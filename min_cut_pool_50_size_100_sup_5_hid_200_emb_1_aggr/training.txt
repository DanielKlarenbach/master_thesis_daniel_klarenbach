Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 0.3897920250892639
Batch 2/64 loss: 0.3540802001953125
Batch 3/64 loss: 0.3383496403694153
Batch 4/64 loss: 0.3313714265823364
Batch 5/64 loss: 0.3287805914878845
Batch 6/64 loss: 0.325819194316864
Batch 7/64 loss: 0.31964391469955444
Batch 8/64 loss: 0.3201514482498169
Batch 9/64 loss: 0.3173055648803711
Batch 10/64 loss: 0.3164551258087158
Batch 11/64 loss: 0.3118577003479004
Batch 12/64 loss: 0.31601184606552124
Batch 13/64 loss: 0.3117561340332031
Batch 14/64 loss: 0.3035318851470947
Batch 15/64 loss: 0.3127075433731079
Batch 16/64 loss: 0.3046596646308899
Batch 17/64 loss: 0.3073934316635132
Batch 18/64 loss: 0.31099724769592285
Batch 19/64 loss: 0.30380773544311523
Batch 20/64 loss: 0.30505967140197754
Batch 21/64 loss: 0.3055580258369446
Batch 22/64 loss: 0.3092087507247925
Batch 23/64 loss: 0.30917888879776
Batch 24/64 loss: 0.3087407350540161
Batch 25/64 loss: 0.30354058742523193
Batch 26/64 loss: 0.30661821365356445
Batch 27/64 loss: 0.3044545650482178
Batch 28/64 loss: 0.29657530784606934
Batch 29/64 loss: 0.30166327953338623
Batch 30/64 loss: 0.3037745952606201
Batch 31/64 loss: 0.29784631729125977
Batch 32/64 loss: 0.3016486167907715
Batch 33/64 loss: 0.29928576946258545
Batch 34/64 loss: 0.30044853687286377
Batch 35/64 loss: 0.3022502064704895
Batch 36/64 loss: 0.3031158447265625
Batch 37/64 loss: 0.29845166206359863
Batch 38/64 loss: 0.29688143730163574
Batch 39/64 loss: 0.29581135511398315
Batch 40/64 loss: 0.29924190044403076
Batch 41/64 loss: 0.29576146602630615
Batch 42/64 loss: 0.2877925634384155
Batch 43/64 loss: 0.28855788707733154
Batch 44/64 loss: 0.2956274747848511
Batch 45/64 loss: 0.2930634617805481
Batch 46/64 loss: 0.2943272590637207
Batch 47/64 loss: 0.3001238703727722
Batch 48/64 loss: 0.2915443181991577
Batch 49/64 loss: 0.30204838514328003
Batch 50/64 loss: 0.28528720140457153
Batch 51/64 loss: 0.29578524827957153
Batch 52/64 loss: 0.2956031560897827
Batch 53/64 loss: 0.29558634757995605
Batch 54/64 loss: 0.2871851921081543
Batch 55/64 loss: 0.2942178249359131
Batch 56/64 loss: 0.2885449528694153
Batch 57/64 loss: 0.29256224632263184
Batch 58/64 loss: 0.2886955738067627
Batch 59/64 loss: 0.285369873046875
Batch 60/64 loss: 0.29439955949783325
Batch 61/64 loss: 0.2918568253517151
Batch 62/64 loss: 0.29176974296569824
Batch 63/64 loss: 0.2879083752632141
Batch 64/64 loss: 0.2935221195220947
Epoch 1  Train loss: 0.30458959972157196  Val loss: 0.30558457898929764
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 0.296481728553772
Batch 2/64 loss: 0.28723418712615967
Batch 3/64 loss: 0.2921425700187683
Batch 4/64 loss: 0.286518931388855
Batch 5/64 loss: 0.2871111035346985
Batch 6/64 loss: 0.2815260887145996
Batch 7/64 loss: 0.28887850046157837
Batch 8/64 loss: 0.29047155380249023
Batch 9/64 loss: 0.2930036783218384
Batch 10/64 loss: 0.28329741954803467
Batch 11/64 loss: 0.2780439257621765
Batch 12/64 loss: 0.28345274925231934
Batch 13/64 loss: 0.2927698493003845
Batch 14/64 loss: 0.29087674617767334
Batch 15/64 loss: 0.2851516604423523
Batch 16/64 loss: 0.28225141763687134
Batch 17/64 loss: 0.2864036560058594
Batch 18/64 loss: 0.2834261655807495
Batch 19/64 loss: 0.2930760383605957
Batch 20/64 loss: 0.2885832190513611
Batch 21/64 loss: 0.28620970249176025
Batch 22/64 loss: 0.28361910581588745
Batch 23/64 loss: 0.2826994061470032
Batch 24/64 loss: 0.29241031408309937
Batch 25/64 loss: 0.2945849895477295
Batch 26/64 loss: 0.2803594470024109
Batch 27/64 loss: 0.2852308750152588
Batch 28/64 loss: 0.2690390944480896
Batch 29/64 loss: 0.2757081985473633
Batch 30/64 loss: 0.278273344039917
Batch 31/64 loss: 0.2813936471939087
Batch 32/64 loss: 0.26992344856262207
Batch 33/64 loss: 0.2867008447647095
Batch 34/64 loss: 0.2793571352958679
Batch 35/64 loss: 0.279230535030365
Batch 36/64 loss: 0.28975749015808105
Batch 37/64 loss: 0.2749311923980713
Batch 38/64 loss: 0.2872990369796753
Batch 39/64 loss: 0.2743210196495056
Batch 40/64 loss: 0.2890596389770508
Batch 41/64 loss: 0.28043603897094727
Batch 42/64 loss: 0.28610169887542725
Batch 43/64 loss: 0.28035902976989746
Batch 44/64 loss: 0.2861022353172302
Batch 45/64 loss: 0.2720184326171875
Batch 46/64 loss: 0.27579188346862793
Batch 47/64 loss: 0.27640366554260254
Batch 48/64 loss: 0.2776087522506714
Batch 49/64 loss: 0.2885751724243164
Batch 50/64 loss: 0.2796206474304199
Batch 51/64 loss: 0.281230628490448
Batch 52/64 loss: 0.2769505977630615
Batch 53/64 loss: 0.2811640501022339
Batch 54/64 loss: 0.27265167236328125
Batch 55/64 loss: 0.28077948093414307
Batch 56/64 loss: 0.2784475088119507
Batch 57/64 loss: 0.27164483070373535
Batch 58/64 loss: 0.2705036997795105
Batch 59/64 loss: 0.275606632232666
Batch 60/64 loss: 0.28062158823013306
Batch 61/64 loss: 0.27725154161453247
Batch 62/64 loss: 0.27390187978744507
Batch 63/64 loss: 0.2688256502151489
Batch 64/64 loss: 0.2694087624549866
Epoch 2  Train loss: 0.28212491834864895  Val loss: 0.28025277783370917
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 0.2766876816749573
Batch 2/64 loss: 0.28062760829925537
Batch 3/64 loss: 0.2737370729446411
Batch 4/64 loss: 0.2759694457054138
Batch 5/64 loss: 0.2694554328918457
Batch 6/64 loss: 0.27641069889068604
Batch 7/64 loss: 0.27881377935409546
Batch 8/64 loss: 0.2759639024734497
Batch 9/64 loss: 0.2740844488143921
Batch 10/64 loss: 0.2654191255569458
Batch 11/64 loss: 0.2763741612434387
Batch 12/64 loss: 0.27640295028686523
Batch 13/64 loss: 0.277726411819458
Batch 14/64 loss: 0.2733916640281677
Batch 15/64 loss: 0.2754812240600586
Batch 16/64 loss: 0.26873886585235596
Batch 17/64 loss: 0.2796410322189331
Batch 18/64 loss: 0.2826547622680664
Batch 19/64 loss: 0.26064616441726685
Batch 20/64 loss: 0.27418190240859985
Batch 21/64 loss: 0.26988983154296875
Batch 22/64 loss: 0.2643422484397888
Batch 23/64 loss: 0.2837337255477905
Batch 24/64 loss: 0.2779543399810791
Batch 25/64 loss: 0.27155590057373047
Batch 26/64 loss: 0.27187079191207886
Batch 27/64 loss: 0.27455735206604004
Batch 28/64 loss: 0.28629517555236816
Batch 29/64 loss: 0.2690867781639099
Batch 30/64 loss: 0.2717454433441162
Batch 31/64 loss: 0.270587682723999
Batch 32/64 loss: 0.26704782247543335
Batch 33/64 loss: 0.2736257314682007
Batch 34/64 loss: 0.2750898599624634
Batch 35/64 loss: 0.26789045333862305
Batch 36/64 loss: 0.27571427822113037
Batch 37/64 loss: 0.2670741677284241
Batch 38/64 loss: 0.27634990215301514
Batch 39/64 loss: 0.2684544324874878
Batch 40/64 loss: 0.2703477144241333
Batch 41/64 loss: 0.2807425260543823
Batch 42/64 loss: 0.282027006149292
Batch 43/64 loss: 0.27835553884506226
Batch 44/64 loss: 0.27657026052474976
Batch 45/64 loss: 0.27181029319763184
Batch 46/64 loss: 0.2717522978782654
Batch 47/64 loss: 0.2669888138771057
Batch 48/64 loss: 0.2794429063796997
Batch 49/64 loss: 0.2642817497253418
Batch 50/64 loss: 0.25931715965270996
Batch 51/64 loss: 0.2676575183868408
Batch 52/64 loss: 0.26523858308792114
Batch 53/64 loss: 0.2602919936180115
Batch 54/64 loss: 0.27171462774276733
Batch 55/64 loss: 0.2649286985397339
Batch 56/64 loss: 0.26397305727005005
Batch 57/64 loss: 0.2747989892959595
Batch 58/64 loss: 0.27769482135772705
Batch 59/64 loss: 0.26335036754608154
Batch 60/64 loss: 0.27153480052948
Batch 61/64 loss: 0.26598358154296875
Batch 62/64 loss: 0.26789385080337524
Batch 63/64 loss: 0.26058685779571533
Batch 64/64 loss: 0.2667512893676758
Epoch 3  Train loss: 0.27219798798654593  Val loss: 0.27384809775860447
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 0.2730480432510376
Batch 2/64 loss: 0.27540123462677
Batch 3/64 loss: 0.27222609519958496
Batch 4/64 loss: 0.268907368183136
Batch 5/64 loss: 0.2653884291648865
Batch 6/64 loss: 0.26267874240875244
Batch 7/64 loss: 0.2691025733947754
Batch 8/64 loss: 0.27457642555236816
Batch 9/64 loss: 0.2675235867500305
Batch 10/64 loss: 0.27682948112487793
Batch 11/64 loss: 0.25562024116516113
Batch 12/64 loss: 0.2619647979736328
Batch 13/64 loss: 0.27082741260528564
Batch 14/64 loss: 0.25988519191741943
Batch 15/64 loss: 0.2620255947113037
Batch 16/64 loss: 0.2572205066680908
Batch 17/64 loss: 0.25272059440612793
Batch 18/64 loss: 0.2745146155357361
Batch 19/64 loss: 0.2643214464187622
Batch 20/64 loss: 0.27318716049194336
Batch 21/64 loss: 0.27323734760284424
Batch 22/64 loss: 0.2556877136230469
Batch 23/64 loss: 0.2754976153373718
Batch 24/64 loss: 0.26383256912231445
Batch 25/64 loss: 0.2643674612045288
Batch 26/64 loss: 0.26103347539901733
Batch 27/64 loss: 0.26296454668045044
Batch 28/64 loss: 0.26109158992767334
Batch 29/64 loss: 0.2751516103744507
Batch 30/64 loss: 0.27259790897369385
Batch 31/64 loss: 0.2684617042541504
Batch 32/64 loss: 0.26900148391723633
Batch 33/64 loss: 0.26946741342544556
Batch 34/64 loss: 0.2611422538757324
Batch 35/64 loss: 0.2722848653793335
Batch 36/64 loss: 0.26982611417770386
Batch 37/64 loss: 0.2603403925895691
Batch 38/64 loss: 0.25744563341140747
Batch 39/64 loss: 0.2690674066543579
Batch 40/64 loss: 0.2614785432815552
Batch 41/64 loss: 0.25559568405151367
Batch 42/64 loss: 0.25955730676651
Batch 43/64 loss: 0.2589869499206543
Batch 44/64 loss: 0.2545393705368042
Batch 45/64 loss: 0.2636919617652893
Batch 46/64 loss: 0.26163244247436523
Batch 47/64 loss: 0.25385814905166626
Batch 48/64 loss: 0.258889377117157
Batch 49/64 loss: 0.2613102197647095
Batch 50/64 loss: 0.267897367477417
Batch 51/64 loss: 0.2608366012573242
Batch 52/64 loss: 0.2681143879890442
Batch 53/64 loss: 0.25902801752090454
Batch 54/64 loss: 0.25196409225463867
Batch 55/64 loss: 0.25695401430130005
Batch 56/64 loss: 0.2647460103034973
Batch 57/64 loss: 0.26473110914230347
Batch 58/64 loss: 0.263724148273468
Batch 59/64 loss: 0.2728121280670166
Batch 60/64 loss: 0.262931227684021
Batch 61/64 loss: 0.2458990216255188
Batch 62/64 loss: 0.2559394836425781
Batch 63/64 loss: 0.2588663101196289
Batch 64/64 loss: 0.2496543526649475
Epoch 4  Train loss: 0.2640265621390997  Val loss: 0.26525288185303153
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 0.2579481601715088
Batch 2/64 loss: 0.2567662000656128
Batch 3/64 loss: 0.2558610439300537
Batch 4/64 loss: 0.26448214054107666
Batch 5/64 loss: 0.26742613315582275
Batch 6/64 loss: 0.2548677921295166
Batch 7/64 loss: 0.26093196868896484
Batch 8/64 loss: 0.24873095750808716
Batch 9/64 loss: 0.2547668218612671
Batch 10/64 loss: 0.26728928089141846
Batch 11/64 loss: 0.2571295499801636
Batch 12/64 loss: 0.25964266061782837
Batch 13/64 loss: 0.2593350410461426
Batch 14/64 loss: 0.25849878787994385
Batch 15/64 loss: 0.26652300357818604
Batch 16/64 loss: 0.23687338829040527
Batch 17/64 loss: 0.25384408235549927
Batch 18/64 loss: 0.250888466835022
Batch 19/64 loss: 0.2523375153541565
Batch 20/64 loss: 0.2523072361946106
Batch 21/64 loss: 0.26175111532211304
Batch 22/64 loss: 0.2620675563812256
Batch 23/64 loss: 0.2541869878768921
Batch 24/64 loss: 0.2589517831802368
Batch 25/64 loss: 0.2557094693183899
Batch 26/64 loss: 0.24972081184387207
Batch 27/64 loss: 0.2487795352935791
Batch 28/64 loss: 0.25409674644470215
Batch 29/64 loss: 0.2611749768257141
Batch 30/64 loss: 0.26103222370147705
Batch 31/64 loss: 0.2612372040748596
Batch 32/64 loss: 0.2512553334236145
Batch 33/64 loss: 0.2619937062263489
Batch 34/64 loss: 0.25756871700286865
Batch 35/64 loss: 0.259928822517395
Batch 36/64 loss: 0.25499576330184937
Batch 37/64 loss: 0.25693124532699585
Batch 38/64 loss: 0.2559083104133606
Batch 39/64 loss: 0.26494044065475464
Batch 40/64 loss: 0.2554330825805664
Batch 41/64 loss: 0.2649538516998291
Batch 42/64 loss: 0.2537767291069031
Batch 43/64 loss: 0.257468581199646
Batch 44/64 loss: 0.27529317140579224
Batch 45/64 loss: 0.25337380170822144
Batch 46/64 loss: 0.2512863874435425
Batch 47/64 loss: 0.24634647369384766
Batch 48/64 loss: 0.2651480436325073
Batch 49/64 loss: 0.26321303844451904
Batch 50/64 loss: 0.256492018699646
Batch 51/64 loss: 0.25667524337768555
Batch 52/64 loss: 0.260087251663208
Batch 53/64 loss: 0.25912415981292725
Batch 54/64 loss: 0.25617361068725586
Batch 55/64 loss: 0.2556154131889343
Batch 56/64 loss: 0.25358617305755615
Batch 57/64 loss: 0.25846827030181885
Batch 58/64 loss: 0.26379549503326416
Batch 59/64 loss: 0.2479913830757141
Batch 60/64 loss: 0.2550737261772156
Batch 61/64 loss: 0.24885034561157227
Batch 62/64 loss: 0.2510252594947815
Batch 63/64 loss: 0.2473219633102417
Batch 64/64 loss: 0.2523458003997803
Epoch 5  Train loss: 0.2568551185084324  Val loss: 0.255699419688523
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 0.24453866481781006
Batch 2/64 loss: 0.24872398376464844
Batch 3/64 loss: 0.24645811319351196
Batch 4/64 loss: 0.2520798444747925
Batch 5/64 loss: 0.24860721826553345
Batch 6/64 loss: 0.2485005259513855
Batch 7/64 loss: 0.24708503484725952
Batch 8/64 loss: 0.24030393362045288
Batch 9/64 loss: 0.254940927028656
Batch 10/64 loss: 0.2441893219947815
Batch 11/64 loss: 0.2481839656829834
Batch 12/64 loss: 0.2604702115058899
Batch 13/64 loss: 0.2544269561767578
Batch 14/64 loss: 0.2530696392059326
Batch 15/64 loss: 0.2551296353340149
Batch 16/64 loss: 0.2469463348388672
Batch 17/64 loss: 0.24162834882736206
Batch 18/64 loss: 0.24756604433059692
Batch 19/64 loss: 0.25330817699432373
Batch 20/64 loss: 0.25280171632766724
Batch 21/64 loss: 0.24812191724777222
Batch 22/64 loss: 0.2510194778442383
Batch 23/64 loss: 0.24607980251312256
Batch 24/64 loss: 0.25322824716567993
Batch 25/64 loss: 0.24954605102539062
Batch 26/64 loss: 0.2531365752220154
Batch 27/64 loss: 0.2530340552330017
Batch 28/64 loss: 0.2572474479675293
Batch 29/64 loss: 0.25504493713378906
Batch 30/64 loss: 0.25257301330566406
Batch 31/64 loss: 0.2663625478744507
Batch 32/64 loss: 0.25346899032592773
Batch 33/64 loss: 0.2521336078643799
Batch 34/64 loss: 0.24789559841156006
Batch 35/64 loss: 0.24378979206085205
Batch 36/64 loss: 0.24163532257080078
Batch 37/64 loss: 0.2518358826637268
Batch 38/64 loss: 0.24887174367904663
Batch 39/64 loss: 0.25815868377685547
Batch 40/64 loss: 0.2467656135559082
Batch 41/64 loss: 0.24164462089538574
Batch 42/64 loss: 0.24803608655929565
Batch 43/64 loss: 0.26374804973602295
Batch 44/64 loss: 0.24851906299591064
Batch 45/64 loss: 0.24910175800323486
Batch 46/64 loss: 0.24712657928466797
Batch 47/64 loss: 0.2440943717956543
Batch 48/64 loss: 0.25427496433258057
Batch 49/64 loss: 0.24761152267456055
Batch 50/64 loss: 0.24456334114074707
Batch 51/64 loss: 0.2606395483016968
Batch 52/64 loss: 0.24048244953155518
Batch 53/64 loss: 0.24415898323059082
Batch 54/64 loss: 0.2507922053337097
Batch 55/64 loss: 0.24539858102798462
Batch 56/64 loss: 0.2508000135421753
Batch 57/64 loss: 0.25295257568359375
Batch 58/64 loss: 0.23545455932617188
Batch 59/64 loss: 0.25378739833831787
Batch 60/64 loss: 0.2516244649887085
Batch 61/64 loss: 0.24241328239440918
Batch 62/64 loss: 0.2470763921737671
Batch 63/64 loss: 0.24579280614852905
Batch 64/64 loss: 0.25042927265167236
Epoch 6  Train loss: 0.24967566237730138  Val loss: 0.2523294179300262
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 0.24557608366012573
Batch 2/64 loss: 0.24198079109191895
Batch 3/64 loss: 0.26278024911880493
Batch 4/64 loss: 0.2470705509185791
Batch 5/64 loss: 0.24672293663024902
Batch 6/64 loss: 0.24807047843933105
Batch 7/64 loss: 0.2514721751213074
Batch 8/64 loss: 0.2443021535873413
Batch 9/64 loss: 0.25545746088027954
Batch 10/64 loss: 0.24685025215148926
Batch 11/64 loss: 0.23921126127243042
Batch 12/64 loss: 0.254657506942749
Batch 13/64 loss: 0.24651503562927246
Batch 14/64 loss: 0.22582745552062988
Batch 15/64 loss: 0.2417447566986084
Batch 16/64 loss: 0.2390683889389038
Batch 17/64 loss: 0.24793916940689087
Batch 18/64 loss: 0.23622488975524902
Batch 19/64 loss: 0.23552405834197998
Batch 20/64 loss: 0.25122904777526855
Batch 21/64 loss: 0.23593813180923462
Batch 22/64 loss: 0.24282199144363403
Batch 23/64 loss: 0.23866891860961914
Batch 24/64 loss: 0.24090611934661865
Batch 25/64 loss: 0.2536548376083374
Batch 26/64 loss: 0.2417285442352295
Batch 27/64 loss: 0.23354530334472656
Batch 28/64 loss: 0.24904471635818481
Batch 29/64 loss: 0.23224055767059326
Batch 30/64 loss: 0.23787498474121094
Batch 31/64 loss: 0.23122823238372803
Batch 32/64 loss: 0.2479003667831421
Batch 33/64 loss: 0.23904216289520264
Batch 34/64 loss: 0.23422539234161377
Batch 35/64 loss: 0.24568676948547363
Batch 36/64 loss: 0.234635591506958
Batch 37/64 loss: 0.23398584127426147
Batch 38/64 loss: 0.24739468097686768
Batch 39/64 loss: 0.2590171694755554
Batch 40/64 loss: 0.25828492641448975
Batch 41/64 loss: 0.23774993419647217
Batch 42/64 loss: 0.24819839000701904
Batch 43/64 loss: 0.23339486122131348
Batch 44/64 loss: 0.23264485597610474
Batch 45/64 loss: 0.23930174112319946
Batch 46/64 loss: 0.2524137496948242
Batch 47/64 loss: 0.24838364124298096
Batch 48/64 loss: 0.23132479190826416
Batch 49/64 loss: 0.24946463108062744
Batch 50/64 loss: 0.2515283226966858
Batch 51/64 loss: 0.2297263741493225
Batch 52/64 loss: 0.24485504627227783
Batch 53/64 loss: 0.23967891931533813
Batch 54/64 loss: 0.2365579605102539
Batch 55/64 loss: 0.23911559581756592
Batch 56/64 loss: 0.23754405975341797
Batch 57/64 loss: 0.23559707403182983
Batch 58/64 loss: 0.24596095085144043
Batch 59/64 loss: 0.24998676776885986
Batch 60/64 loss: 0.25573498010635376
Batch 61/64 loss: 0.2382274866104126
Batch 62/64 loss: 0.2344954013824463
Batch 63/64 loss: 0.24905574321746826
Batch 64/64 loss: 0.2387852668762207
Epoch 7  Train loss: 0.24291890462239582  Val loss: 0.2444786255302298
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 0.24685931205749512
Batch 2/64 loss: 0.25158512592315674
Batch 3/64 loss: 0.24210429191589355
Batch 4/64 loss: 0.24316537380218506
Batch 5/64 loss: 0.24404144287109375
Batch 6/64 loss: 0.2417595386505127
Batch 7/64 loss: 0.2497020959854126
Batch 8/64 loss: 0.2412722110748291
Batch 9/64 loss: 0.2426013946533203
Batch 10/64 loss: 0.2430916428565979
Batch 11/64 loss: 0.23491573333740234
Batch 12/64 loss: 0.24676287174224854
Batch 13/64 loss: 0.2348746657371521
Batch 14/64 loss: 0.2295616865158081
Batch 15/64 loss: 0.24159270524978638
Batch 16/64 loss: 0.23790466785430908
Batch 17/64 loss: 0.25253093242645264
Batch 18/64 loss: 0.2386094331741333
Batch 19/64 loss: 0.23279798030853271
Batch 20/64 loss: 0.23381507396697998
Batch 21/64 loss: 0.21756714582443237
Batch 22/64 loss: 0.23164451122283936
Batch 23/64 loss: 0.23031067848205566
Batch 24/64 loss: 0.23668599128723145
Batch 25/64 loss: 0.22934919595718384
Batch 26/64 loss: 0.23967504501342773
Batch 27/64 loss: 0.23569893836975098
Batch 28/64 loss: 0.25056183338165283
Batch 29/64 loss: 0.24049484729766846
Batch 30/64 loss: 0.243139386177063
Batch 31/64 loss: 0.23428845405578613
Batch 32/64 loss: 0.23080790042877197
Batch 33/64 loss: 0.2362690567970276
Batch 34/64 loss: 0.24064385890960693
Batch 35/64 loss: 0.2285926342010498
Batch 36/64 loss: 0.23147523403167725
Batch 37/64 loss: 0.23686617612838745
Batch 38/64 loss: 0.23693645000457764
Batch 39/64 loss: 0.22393226623535156
Batch 40/64 loss: 0.2282838225364685
Batch 41/64 loss: 0.2217579483985901
Batch 42/64 loss: 0.24359583854675293
Batch 43/64 loss: 0.23291170597076416
Batch 44/64 loss: 0.23421865701675415
Batch 45/64 loss: 0.24175816774368286
Batch 46/64 loss: 0.2333904504776001
Batch 47/64 loss: 0.2434769868850708
Batch 48/64 loss: 0.24122703075408936
Batch 49/64 loss: 0.23301208019256592
Batch 50/64 loss: 0.2469448447227478
Batch 51/64 loss: 0.23936569690704346
Batch 52/64 loss: 0.24101245403289795
Batch 53/64 loss: 0.25392305850982666
Batch 54/64 loss: 0.2324836254119873
Batch 55/64 loss: 0.23488211631774902
Batch 56/64 loss: 0.2383175492286682
Batch 57/64 loss: 0.23318171501159668
Batch 58/64 loss: 0.22812581062316895
Batch 59/64 loss: 0.24394762516021729
Batch 60/64 loss: 0.24757802486419678
Batch 61/64 loss: 0.24147915840148926
Batch 62/64 loss: 0.23304343223571777
Batch 63/64 loss: 0.23733794689178467
Batch 64/64 loss: 0.23034369945526123
Epoch 8  Train loss: 0.23784315773085052  Val loss: 0.2437560548077744
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 0.24430763721466064
Batch 2/64 loss: 0.22623395919799805
Batch 3/64 loss: 0.23831433057785034
Batch 4/64 loss: 0.2268158197402954
Batch 5/64 loss: 0.23138904571533203
Batch 6/64 loss: 0.22817867994308472
Batch 7/64 loss: 0.2450416088104248
Batch 8/64 loss: 0.23742413520812988
Batch 9/64 loss: 0.22257626056671143
Batch 10/64 loss: 0.22943150997161865
Batch 11/64 loss: 0.249140202999115
Batch 12/64 loss: 0.23227083683013916
Batch 13/64 loss: 0.23352015018463135
Batch 14/64 loss: 0.22858357429504395
Batch 15/64 loss: 0.235118567943573
Batch 16/64 loss: 0.23808151483535767
Batch 17/64 loss: 0.2343200445175171
Batch 18/64 loss: 0.23771828413009644
Batch 19/64 loss: 0.2288733720779419
Batch 20/64 loss: 0.22084778547286987
Batch 21/64 loss: 0.23290586471557617
Batch 22/64 loss: 0.21986573934555054
Batch 23/64 loss: 0.2379612922668457
Batch 24/64 loss: 0.22586989402770996
Batch 25/64 loss: 0.22772252559661865
Batch 26/64 loss: 0.23159879446029663
Batch 27/64 loss: 0.2364417314529419
Batch 28/64 loss: 0.2317901849746704
Batch 29/64 loss: 0.22827911376953125
Batch 30/64 loss: 0.2269376516342163
Batch 31/64 loss: 0.2220463752746582
Batch 32/64 loss: 0.23349523544311523
Batch 33/64 loss: 0.23754847049713135
Batch 34/64 loss: 0.22711533308029175
Batch 35/64 loss: 0.2207128405570984
Batch 36/64 loss: 0.23412352800369263
Batch 37/64 loss: 0.24528425931930542
Batch 38/64 loss: 0.23982453346252441
Batch 39/64 loss: 0.23200589418411255
Batch 40/64 loss: 0.22877776622772217
Batch 41/64 loss: 0.22193753719329834
Batch 42/64 loss: 0.22826510667800903
Batch 43/64 loss: 0.22500312328338623
Batch 44/64 loss: 0.23896145820617676
Batch 45/64 loss: 0.23561763763427734
Batch 46/64 loss: 0.22890472412109375
Batch 47/64 loss: 0.2519751787185669
Batch 48/64 loss: 0.22618699073791504
Batch 49/64 loss: 0.23019254207611084
Batch 50/64 loss: 0.2257370948791504
Batch 51/64 loss: 0.23909413814544678
Batch 52/64 loss: 0.22294598817825317
Batch 53/64 loss: 0.22929733991622925
Batch 54/64 loss: 0.22894209623336792
Batch 55/64 loss: 0.22344642877578735
Batch 56/64 loss: 0.21039867401123047
Batch 57/64 loss: 0.23706328868865967
Batch 58/64 loss: 0.22623515129089355
Batch 59/64 loss: 0.23712730407714844
Batch 60/64 loss: 0.22152650356292725
Batch 61/64 loss: 0.22719120979309082
Batch 62/64 loss: 0.23525381088256836
Batch 63/64 loss: 0.2219095230102539
Batch 64/64 loss: 0.2414756417274475
Epoch 9  Train loss: 0.23125982635161457  Val loss: 0.24020154369655752
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 0.21536314487457275
Batch 2/64 loss: 0.23686563968658447
Batch 3/64 loss: 0.23458194732666016
Batch 4/64 loss: 0.2258141040802002
Batch 5/64 loss: 0.23997902870178223
Batch 6/64 loss: 0.21594011783599854
Batch 7/64 loss: 0.23215878009796143
Batch 8/64 loss: 0.23012840747833252
Batch 9/64 loss: 0.24133968353271484
Batch 10/64 loss: 0.2246551513671875
Batch 11/64 loss: 0.22006356716156006
Batch 12/64 loss: 0.21277344226837158
Batch 13/64 loss: 0.22934240102767944
Batch 14/64 loss: 0.23676788806915283
Batch 15/64 loss: 0.23353862762451172
Batch 16/64 loss: 0.2188558578491211
Batch 17/64 loss: 0.22885239124298096
Batch 18/64 loss: 0.22277522087097168
Batch 19/64 loss: 0.23002886772155762
Batch 20/64 loss: 0.2262306809425354
Batch 21/64 loss: 0.2231210470199585
Batch 22/64 loss: 0.23456811904907227
Batch 23/64 loss: 0.23146522045135498
Batch 24/64 loss: 0.24217760562896729
Batch 25/64 loss: 0.23103070259094238
Batch 26/64 loss: 0.22844886779785156
Batch 27/64 loss: 0.2297438383102417
Batch 28/64 loss: 0.2255731225013733
Batch 29/64 loss: 0.22503697872161865
Batch 30/64 loss: 0.23666000366210938
Batch 31/64 loss: 0.2303769588470459
Batch 32/64 loss: 0.22878271341323853
Batch 33/64 loss: 0.21987807750701904
Batch 34/64 loss: 0.22950035333633423
Batch 35/64 loss: 0.23133772611618042
Batch 36/64 loss: 0.22853803634643555
Batch 37/64 loss: 0.2200605869293213
Batch 38/64 loss: 0.22600436210632324
Batch 39/64 loss: 0.2241731882095337
Batch 40/64 loss: 0.22483742237091064
Batch 41/64 loss: 0.22841179370880127
Batch 42/64 loss: 0.2461247444152832
Batch 43/64 loss: 0.2283586859703064
Batch 44/64 loss: 0.236547589302063
Batch 45/64 loss: 0.22294604778289795
Batch 46/64 loss: 0.23523372411727905
Batch 47/64 loss: 0.22763276100158691
Batch 48/64 loss: 0.23629456758499146
Batch 49/64 loss: 0.23351454734802246
Batch 50/64 loss: 0.2066425085067749
Batch 51/64 loss: 0.2087838053703308
Batch 52/64 loss: 0.23117870092391968
Batch 53/64 loss: 0.2173604965209961
Batch 54/64 loss: 0.2195514440536499
Batch 55/64 loss: 0.22672873735427856
Batch 56/64 loss: 0.2194240689277649
Batch 57/64 loss: 0.21279847621917725
Batch 58/64 loss: 0.2206885814666748
Batch 59/64 loss: 0.21264255046844482
Batch 60/64 loss: 0.22479987144470215
Batch 61/64 loss: 0.2174195647239685
Batch 62/64 loss: 0.2133263349533081
Batch 63/64 loss: 0.2316097617149353
Batch 64/64 loss: 0.2030315399169922
Epoch 10  Train loss: 0.22647314352147718  Val loss: 0.23014569343979827
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: 0.23333525657653809
Batch 2/64 loss: 0.2325257658958435
Batch 3/64 loss: 0.21210038661956787
Batch 4/64 loss: 0.21342992782592773
Batch 5/64 loss: 0.22704648971557617
Batch 6/64 loss: 0.22189831733703613
Batch 7/64 loss: 0.23427116870880127
Batch 8/64 loss: 0.23128747940063477
Batch 9/64 loss: 0.22448158264160156
Batch 10/64 loss: 0.1974603533744812
Batch 11/64 loss: 0.22949928045272827
Batch 12/64 loss: 0.21933531761169434
Batch 13/64 loss: 0.21522605419158936
Batch 14/64 loss: 0.22282958030700684
Batch 15/64 loss: 0.22054648399353027
Batch 16/64 loss: 0.21242910623550415
Batch 17/64 loss: 0.2366194725036621
Batch 18/64 loss: 0.20627433061599731
Batch 19/64 loss: 0.2314615249633789
Batch 20/64 loss: 0.21008813381195068
Batch 21/64 loss: 0.21771299839019775
Batch 22/64 loss: 0.2077881097793579
Batch 23/64 loss: 0.20289355516433716
Batch 24/64 loss: 0.22881388664245605
Batch 25/64 loss: 0.2177909016609192
Batch 26/64 loss: 0.22332370281219482
Batch 27/64 loss: 0.20999670028686523
Batch 28/64 loss: 0.2269466519355774
Batch 29/64 loss: 0.2320547103881836
Batch 30/64 loss: 0.22947239875793457
Batch 31/64 loss: 0.2334694266319275
Batch 32/64 loss: 0.22279387712478638
Batch 33/64 loss: 0.2194231152534485
Batch 34/64 loss: 0.2311922311782837
Batch 35/64 loss: 0.2256854772567749
Batch 36/64 loss: 0.21200549602508545
Batch 37/64 loss: 0.21643924713134766
Batch 38/64 loss: 0.22018134593963623
Batch 39/64 loss: 0.21985018253326416
Batch 40/64 loss: 0.21676009893417358
Batch 41/64 loss: 0.23854845762252808
Batch 42/64 loss: 0.23446667194366455
Batch 43/64 loss: 0.2333049774169922
Batch 44/64 loss: 0.22381848096847534
Batch 45/64 loss: 0.23013389110565186
Batch 46/64 loss: 0.22588348388671875
Batch 47/64 loss: 0.2179604172706604
Batch 48/64 loss: 0.2247350811958313
Batch 49/64 loss: 0.2101549506187439
Batch 50/64 loss: 0.22266316413879395
Batch 51/64 loss: 0.21328586339950562
Batch 52/64 loss: 0.21394741535186768
Batch 53/64 loss: 0.22123026847839355
Batch 54/64 loss: 0.22789186239242554
Batch 55/64 loss: 0.2185823917388916
Batch 56/64 loss: 0.20467138290405273
Batch 57/64 loss: 0.2229253053665161
Batch 58/64 loss: 0.21945464611053467
Batch 59/64 loss: 0.2161186933517456
Batch 60/64 loss: 0.2219027280807495
Batch 61/64 loss: 0.2021576166152954
Batch 62/64 loss: 0.2150360345840454
Batch 63/64 loss: 0.22431010007858276
Batch 64/64 loss: 0.2214568853378296
Epoch 11  Train loss: 0.22111398706249163  Val loss: 0.23250755964685552
Epoch 12
-------------------------------
Batch 1/64 loss: 0.21831119060516357
Batch 2/64 loss: 0.20384883880615234
Batch 3/64 loss: 0.2313603162765503
Batch 4/64 loss: 0.22293692827224731
Batch 5/64 loss: 0.2290557622909546
Batch 6/64 loss: 0.21659719944000244
Batch 7/64 loss: 0.22551167011260986
Batch 8/64 loss: 0.2242891788482666
Batch 9/64 loss: 0.20916295051574707
Batch 10/64 loss: 0.20868664979934692
Batch 11/64 loss: 0.2054201364517212
Batch 12/64 loss: 0.20984363555908203
Batch 13/64 loss: 0.20859694480895996
Batch 14/64 loss: 0.20891469717025757
Batch 15/64 loss: 0.2206527590751648
Batch 16/64 loss: 0.22319406270980835
Batch 17/64 loss: 0.22225356101989746
Batch 18/64 loss: 0.20710456371307373
Batch 19/64 loss: 0.22538483142852783
Batch 20/64 loss: 0.21338576078414917
Batch 21/64 loss: 0.21931254863739014
Batch 22/64 loss: 0.2358633279800415
Batch 23/64 loss: 0.2154707908630371
Batch 24/64 loss: 0.22021269798278809
Batch 25/64 loss: 0.2112581729888916
Batch 26/64 loss: 0.2008776068687439
Batch 27/64 loss: 0.22518253326416016
Batch 28/64 loss: 0.22278666496276855
Batch 29/64 loss: 0.2153010368347168
Batch 30/64 loss: 0.22858303785324097
Batch 31/64 loss: 0.2050999402999878
Batch 32/64 loss: 0.2108703851699829
Batch 33/64 loss: 0.21206116676330566
Batch 34/64 loss: 0.22767651081085205
Batch 35/64 loss: 0.22225332260131836
Batch 36/64 loss: 0.22115910053253174
Batch 37/64 loss: 0.2184433937072754
Batch 38/64 loss: 0.20820635557174683
Batch 39/64 loss: 0.21638596057891846
Batch 40/64 loss: 0.21254271268844604
Batch 41/64 loss: 0.20524346828460693
Batch 42/64 loss: 0.20878171920776367
Batch 43/64 loss: 0.21393632888793945
Batch 44/64 loss: 0.22138464450836182
Batch 45/64 loss: 0.21734297275543213
Batch 46/64 loss: 0.21007269620895386
Batch 47/64 loss: 0.2095128297805786
Batch 48/64 loss: 0.2170344591140747
Batch 49/64 loss: 0.21040046215057373
Batch 50/64 loss: 0.21541523933410645
Batch 51/64 loss: 0.22201961278915405
Batch 52/64 loss: 0.21753764152526855
Batch 53/64 loss: 0.22069215774536133
Batch 54/64 loss: 0.21524333953857422
Batch 55/64 loss: 0.2161715030670166
Batch 56/64 loss: 0.22098171710968018
Batch 57/64 loss: 0.22089028358459473
Batch 58/64 loss: 0.21819519996643066
Batch 59/64 loss: 0.21361494064331055
Batch 60/64 loss: 0.21954894065856934
Batch 61/64 loss: 0.21956110000610352
Batch 62/64 loss: 0.22935110330581665
Batch 63/64 loss: 0.21263593435287476
Batch 64/64 loss: 0.22455060482025146
Epoch 12  Train loss: 0.21691043376922609  Val loss: 0.219194852609405
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 0.22373580932617188
Batch 2/64 loss: 0.1975177526473999
Batch 3/64 loss: 0.19991111755371094
Batch 4/64 loss: 0.2081359624862671
Batch 5/64 loss: 0.2130710482597351
Batch 6/64 loss: 0.21487224102020264
Batch 7/64 loss: 0.21188557147979736
Batch 8/64 loss: 0.21462655067443848
Batch 9/64 loss: 0.20407605171203613
Batch 10/64 loss: 0.20737677812576294
Batch 11/64 loss: 0.20625710487365723
Batch 12/64 loss: 0.20212393999099731
Batch 13/64 loss: 0.2080588936805725
Batch 14/64 loss: 0.20269817113876343
Batch 15/64 loss: 0.21698468923568726
Batch 16/64 loss: 0.20447325706481934
Batch 17/64 loss: 0.215814471244812
Batch 18/64 loss: 0.2265183925628662
Batch 19/64 loss: 0.20789849758148193
Batch 20/64 loss: 0.20671933889389038
Batch 21/64 loss: 0.21258938312530518
Batch 22/64 loss: 0.21848487854003906
Batch 23/64 loss: 0.21869730949401855
Batch 24/64 loss: 0.23053473234176636
Batch 25/64 loss: 0.21664488315582275
Batch 26/64 loss: 0.22078388929367065
Batch 27/64 loss: 0.2159271240234375
Batch 28/64 loss: 0.21723788976669312
Batch 29/64 loss: 0.2241041660308838
Batch 30/64 loss: 0.2028522491455078
Batch 31/64 loss: 0.20632052421569824
Batch 32/64 loss: 0.22125661373138428
Batch 33/64 loss: 0.208237886428833
Batch 34/64 loss: 0.211511492729187
Batch 35/64 loss: 0.20879924297332764
Batch 36/64 loss: 0.20759367942810059
Batch 37/64 loss: 0.21531295776367188
Batch 38/64 loss: 0.21296310424804688
Batch 39/64 loss: 0.21121060848236084
Batch 40/64 loss: 0.2077416181564331
Batch 41/64 loss: 0.19958257675170898
Batch 42/64 loss: 0.2201738953590393
Batch 43/64 loss: 0.20466065406799316
Batch 44/64 loss: 0.19808471202850342
Batch 45/64 loss: 0.20894145965576172
Batch 46/64 loss: 0.217262864112854
Batch 47/64 loss: 0.20327377319335938
Batch 48/64 loss: 0.22098731994628906
Batch 49/64 loss: 0.21645140647888184
Batch 50/64 loss: 0.21765530109405518
Batch 51/64 loss: 0.19816583395004272
Batch 52/64 loss: 0.19404995441436768
Batch 53/64 loss: 0.20023572444915771
Batch 54/64 loss: 0.21703088283538818
Batch 55/64 loss: 0.2076675295829773
Batch 56/64 loss: 0.2088770866394043
Batch 57/64 loss: 0.21050572395324707
Batch 58/64 loss: 0.21151947975158691
Batch 59/64 loss: 0.20733541250228882
Batch 60/64 loss: 0.20527982711791992
Batch 61/64 loss: 0.21414732933044434
Batch 62/64 loss: 0.21367090940475464
Batch 63/64 loss: 0.20258235931396484
Batch 64/64 loss: 0.2000032663345337
Epoch 13  Train loss: 0.210662028836269  Val loss: 0.22376244727688557
Epoch 14
-------------------------------
Batch 1/64 loss: 0.2149929404258728
Batch 2/64 loss: 0.205455482006073
Batch 3/64 loss: 0.19832605123519897
Batch 4/64 loss: 0.2067967653274536
Batch 5/64 loss: 0.19743764400482178
Batch 6/64 loss: 0.19528114795684814
Batch 7/64 loss: 0.2108839750289917
Batch 8/64 loss: 0.205563485622406
Batch 9/64 loss: 0.2154751420021057
Batch 10/64 loss: 0.2096768617630005
Batch 11/64 loss: 0.20267093181610107
Batch 12/64 loss: 0.19633615016937256
Batch 13/64 loss: 0.20974689722061157
Batch 14/64 loss: 0.19818782806396484
Batch 15/64 loss: 0.18404912948608398
Batch 16/64 loss: 0.20837849378585815
Batch 17/64 loss: 0.20757973194122314
Batch 18/64 loss: 0.20267945528030396
Batch 19/64 loss: 0.21013736724853516
Batch 20/64 loss: 0.1954917311668396
Batch 21/64 loss: 0.21109110116958618
Batch 22/64 loss: 0.20991671085357666
Batch 23/64 loss: 0.19808417558670044
Batch 24/64 loss: 0.2270074486732483
Batch 25/64 loss: 0.2161492109298706
Batch 26/64 loss: 0.2059236764907837
Batch 27/64 loss: 0.20248925685882568
Batch 28/64 loss: 0.2093033790588379
Batch 29/64 loss: 0.21037179231643677
Batch 30/64 loss: 0.20551025867462158
Batch 31/64 loss: 0.22914189100265503
Batch 32/64 loss: 0.21543282270431519
Batch 33/64 loss: 0.21459805965423584
Batch 34/64 loss: 0.2257472276687622
Batch 35/64 loss: 0.19744324684143066
Batch 36/64 loss: 0.21633172035217285
Batch 37/64 loss: 0.2050786018371582
Batch 38/64 loss: 0.1981053352355957
Batch 39/64 loss: 0.20984405279159546
Batch 40/64 loss: 0.20152878761291504
Batch 41/64 loss: 0.20697587728500366
Batch 42/64 loss: 0.19816887378692627
Batch 43/64 loss: 0.2228935956954956
Batch 44/64 loss: 0.18921083211898804
Batch 45/64 loss: 0.20587366819381714
Batch 46/64 loss: 0.20620965957641602
Batch 47/64 loss: 0.2166646122932434
Batch 48/64 loss: 0.2031635046005249
Batch 49/64 loss: 0.2081841230392456
Batch 50/64 loss: 0.21778202056884766
Batch 51/64 loss: 0.20533251762390137
Batch 52/64 loss: 0.2059723138809204
Batch 53/64 loss: 0.19437015056610107
Batch 54/64 loss: 0.21545946598052979
Batch 55/64 loss: 0.20584845542907715
Batch 56/64 loss: 0.19843435287475586
Batch 57/64 loss: 0.1875826120376587
Batch 58/64 loss: 0.21652686595916748
Batch 59/64 loss: 0.1913939118385315
Batch 60/64 loss: 0.22424423694610596
Batch 61/64 loss: 0.2071230411529541
Batch 62/64 loss: 0.22441649436950684
Batch 63/64 loss: 0.21135234832763672
Batch 64/64 loss: 0.2008158564567566
Epoch 14  Train loss: 0.2070281003035751  Val loss: 0.21726174842041382
Saving best model, epoch: 14
Epoch 15
-------------------------------
Batch 1/64 loss: 0.20131993293762207
Batch 2/64 loss: 0.22023987770080566
Batch 3/64 loss: 0.18968510627746582
Batch 4/64 loss: 0.20764029026031494
Batch 5/64 loss: 0.21417081356048584
Batch 6/64 loss: 0.18855035305023193
Batch 7/64 loss: 0.2142651081085205
Batch 8/64 loss: 0.21124351024627686
Batch 9/64 loss: 0.20785009860992432
Batch 10/64 loss: 0.20205950736999512
Batch 11/64 loss: 0.2064661979675293
Batch 12/64 loss: 0.2110692262649536
Batch 13/64 loss: 0.21386337280273438
Batch 14/64 loss: 0.22390216588974
Batch 15/64 loss: 0.2032318115234375
Batch 16/64 loss: 0.1887592077255249
Batch 17/64 loss: 0.19684237241744995
Batch 18/64 loss: 0.2096385359764099
Batch 19/64 loss: 0.21012389659881592
Batch 20/64 loss: 0.20773792266845703
Batch 21/64 loss: 0.20107579231262207
Batch 22/64 loss: 0.18488633632659912
Batch 23/64 loss: 0.18296539783477783
Batch 24/64 loss: 0.20976102352142334
Batch 25/64 loss: 0.20345449447631836
Batch 26/64 loss: 0.20163869857788086
Batch 27/64 loss: 0.2067064642906189
Batch 28/64 loss: 0.2028583288192749
Batch 29/64 loss: 0.18984538316726685
Batch 30/64 loss: 0.21514856815338135
Batch 31/64 loss: 0.22487890720367432
Batch 32/64 loss: 0.208204984664917
Batch 33/64 loss: 0.19721341133117676
Batch 34/64 loss: 0.20559072494506836
Batch 35/64 loss: 0.20277518033981323
Batch 36/64 loss: 0.20262563228607178
Batch 37/64 loss: 0.18815040588378906
Batch 38/64 loss: 0.200484037399292
Batch 39/64 loss: 0.201943039894104
Batch 40/64 loss: 0.2191711664199829
Batch 41/64 loss: 0.21452999114990234
Batch 42/64 loss: 0.21553564071655273
Batch 43/64 loss: 0.19563663005828857
Batch 44/64 loss: 0.208615243434906
Batch 45/64 loss: 0.19986534118652344
Batch 46/64 loss: 0.19411391019821167
Batch 47/64 loss: 0.2042282223701477
Batch 48/64 loss: 0.19997870922088623
Batch 49/64 loss: 0.18694162368774414
Batch 50/64 loss: 0.19527918100357056
Batch 51/64 loss: 0.18883907794952393
Batch 52/64 loss: 0.1980975866317749
Batch 53/64 loss: 0.18574130535125732
Batch 54/64 loss: 0.17892402410507202
Batch 55/64 loss: 0.20356833934783936
Batch 56/64 loss: 0.18924367427825928
Batch 57/64 loss: 0.2045089602470398
Batch 58/64 loss: 0.18936586380004883
Batch 59/64 loss: 0.19382339715957642
Batch 60/64 loss: 0.18788152933120728
Batch 61/64 loss: 0.1941346526145935
Batch 62/64 loss: 0.20229917764663696
Batch 63/64 loss: 0.19172918796539307
Batch 64/64 loss: 0.1952947974205017
Epoch 15  Train loss: 0.20152768083647185  Val loss: 0.20587906476967932
Saving best model, epoch: 15
Epoch 16
-------------------------------
Batch 1/64 loss: 0.18932265043258667
Batch 2/64 loss: 0.18840593099594116
Batch 3/64 loss: 0.1880556344985962
Batch 4/64 loss: 0.20358824729919434
Batch 5/64 loss: 0.17713886499404907
Batch 6/64 loss: 0.19186186790466309
Batch 7/64 loss: 0.1966637372970581
Batch 8/64 loss: 0.20134097337722778
Batch 9/64 loss: 0.21975547075271606
Batch 10/64 loss: 0.20589399337768555
Batch 11/64 loss: 0.18878918886184692
Batch 12/64 loss: 0.18800711631774902
Batch 13/64 loss: 0.2186293601989746
Batch 14/64 loss: 0.19627320766448975
Batch 15/64 loss: 0.19749391078948975
Batch 16/64 loss: 0.1955554485321045
Batch 17/64 loss: 0.18136608600616455
Batch 18/64 loss: 0.21249449253082275
Batch 19/64 loss: 0.21446692943572998
Batch 20/64 loss: 0.18339389562606812
Batch 21/64 loss: 0.19666486978530884
Batch 22/64 loss: 0.21360456943511963
Batch 23/64 loss: 0.19950544834136963
Batch 24/64 loss: 0.22335171699523926
Batch 25/64 loss: 0.20542776584625244
Batch 26/64 loss: 0.20610129833221436
Batch 27/64 loss: 0.19480466842651367
Batch 28/64 loss: 0.17359048128128052
Batch 29/64 loss: 0.1870729923248291
Batch 30/64 loss: 0.20303374528884888
Batch 31/64 loss: 0.19011139869689941
Batch 32/64 loss: 0.19868022203445435
Batch 33/64 loss: 0.2026270031929016
Batch 34/64 loss: 0.1926773190498352
Batch 35/64 loss: 0.18958008289337158
Batch 36/64 loss: 0.20754218101501465
Batch 37/64 loss: 0.18532121181488037
Batch 38/64 loss: 0.204767107963562
Batch 39/64 loss: 0.19105327129364014
Batch 40/64 loss: 0.18661653995513916
Batch 41/64 loss: 0.20024031400680542
Batch 42/64 loss: 0.21399837732315063
Batch 43/64 loss: 0.20096051692962646
Batch 44/64 loss: 0.21987032890319824
Batch 45/64 loss: 0.19634783267974854
Batch 46/64 loss: 0.1922711730003357
Batch 47/64 loss: 0.20172089338302612
Batch 48/64 loss: 0.20054250955581665
Batch 49/64 loss: 0.20808279514312744
Batch 50/64 loss: 0.19571083784103394
Batch 51/64 loss: 0.20260190963745117
Batch 52/64 loss: 0.18509423732757568
Batch 53/64 loss: 0.19659817218780518
Batch 54/64 loss: 0.1843661665916443
Batch 55/64 loss: 0.18417656421661377
Batch 56/64 loss: 0.17025995254516602
Batch 57/64 loss: 0.20488113164901733
Batch 58/64 loss: 0.18674182891845703
Batch 59/64 loss: 0.18760091066360474
Batch 60/64 loss: 0.18986237049102783
Batch 61/64 loss: 0.20774918794631958
Batch 62/64 loss: 0.2066655158996582
Batch 63/64 loss: 0.21383768320083618
Batch 64/64 loss: 0.19041228294372559
Epoch 16  Train loss: 0.19739013840170466  Val loss: 0.2067756818741867
Epoch 17
-------------------------------
Batch 1/64 loss: 0.18657225370407104
Batch 2/64 loss: 0.17715513706207275
Batch 3/64 loss: 0.19674986600875854
Batch 4/64 loss: 0.20942217111587524
Batch 5/64 loss: 0.1896948218345642
Batch 6/64 loss: 0.1768086552619934
Batch 7/64 loss: 0.19306015968322754
Batch 8/64 loss: 0.21965539455413818
Batch 9/64 loss: 0.18719148635864258
Batch 10/64 loss: 0.19785702228546143
Batch 11/64 loss: 0.19410282373428345
Batch 12/64 loss: 0.20569700002670288
Batch 13/64 loss: 0.19337767362594604
Batch 14/64 loss: 0.19426918029785156
Batch 15/64 loss: 0.1820572018623352
Batch 16/64 loss: 0.18848419189453125
Batch 17/64 loss: 0.18304789066314697
Batch 18/64 loss: 0.20530807971954346
Batch 19/64 loss: 0.17964565753936768
Batch 20/64 loss: 0.1834193468093872
Batch 21/64 loss: 0.20004570484161377
Batch 22/64 loss: 0.19334512948989868
Batch 23/64 loss: 0.19511663913726807
Batch 24/64 loss: 0.197307288646698
Batch 25/64 loss: 0.1949690580368042
Batch 26/64 loss: 0.17951977252960205
Batch 27/64 loss: 0.20036107301712036
Batch 28/64 loss: 0.19456511735916138
Batch 29/64 loss: 0.1991562843322754
Batch 30/64 loss: 0.17816030979156494
Batch 31/64 loss: 0.18887537717819214
Batch 32/64 loss: 0.1910971999168396
Batch 33/64 loss: 0.1894761323928833
Batch 34/64 loss: 0.1937037706375122
Batch 35/64 loss: 0.19065016508102417
Batch 36/64 loss: 0.20845282077789307
Batch 37/64 loss: 0.19445312023162842
Batch 38/64 loss: 0.1937270164489746
Batch 39/64 loss: 0.20004421472549438
Batch 40/64 loss: 0.19124442338943481
Batch 41/64 loss: 0.1913011074066162
Batch 42/64 loss: 0.17322975397109985
Batch 43/64 loss: 0.1969008445739746
Batch 44/64 loss: 0.19531822204589844
Batch 45/64 loss: 0.19459855556488037
Batch 46/64 loss: 0.1911400556564331
Batch 47/64 loss: 0.20415335893630981
Batch 48/64 loss: 0.18465089797973633
Batch 49/64 loss: 0.19245922565460205
Batch 50/64 loss: 0.1786050796508789
Batch 51/64 loss: 0.18639451265335083
Batch 52/64 loss: 0.19528454542160034
Batch 53/64 loss: 0.21497076749801636
Batch 54/64 loss: 0.20472681522369385
Batch 55/64 loss: 0.20227038860321045
Batch 56/64 loss: 0.2037290334701538
Batch 57/64 loss: 0.2039179801940918
Batch 58/64 loss: 0.1778407096862793
Batch 59/64 loss: 0.1944931149482727
Batch 60/64 loss: 0.17874598503112793
Batch 61/64 loss: 0.18886280059814453
Batch 62/64 loss: 0.1853102445602417
Batch 63/64 loss: 0.17480599880218506
Batch 64/64 loss: 0.1829589605331421
Epoch 17  Train loss: 0.1924513858907363  Val loss: 0.2021850279926025
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 0.1886676549911499
Batch 2/64 loss: 0.1945720911026001
Batch 3/64 loss: 0.19753670692443848
Batch 4/64 loss: 0.18896889686584473
Batch 5/64 loss: 0.17640173435211182
Batch 6/64 loss: 0.18817222118377686
Batch 7/64 loss: 0.1830761432647705
Batch 8/64 loss: 0.17961084842681885
Batch 9/64 loss: 0.2004392147064209
Batch 10/64 loss: 0.18475323915481567
Batch 11/64 loss: 0.1940542459487915
Batch 12/64 loss: 0.18536245822906494
Batch 13/64 loss: 0.19221198558807373
Batch 14/64 loss: 0.18732506036758423
Batch 15/64 loss: 0.1753225326538086
Batch 16/64 loss: 0.1886008381843567
Batch 17/64 loss: 0.20341157913208008
Batch 18/64 loss: 0.19283521175384521
Batch 19/64 loss: 0.1649641990661621
Batch 20/64 loss: 0.1938382387161255
Batch 21/64 loss: 0.18900853395462036
Batch 22/64 loss: 0.17884433269500732
Batch 23/64 loss: 0.20562744140625
Batch 24/64 loss: 0.1828618049621582
Batch 25/64 loss: 0.19229739904403687
Batch 26/64 loss: 0.20094692707061768
Batch 27/64 loss: 0.17800945043563843
Batch 28/64 loss: 0.1867232322692871
Batch 29/64 loss: 0.1929483413696289
Batch 30/64 loss: 0.1888393759727478
Batch 31/64 loss: 0.188595712184906
Batch 32/64 loss: 0.17932724952697754
Batch 33/64 loss: 0.19043707847595215
Batch 34/64 loss: 0.18132507801055908
Batch 35/64 loss: 0.18877893686294556
Batch 36/64 loss: 0.17986953258514404
Batch 37/64 loss: 0.18300414085388184
Batch 38/64 loss: 0.19154292345046997
Batch 39/64 loss: 0.17444169521331787
Batch 40/64 loss: 0.16720134019851685
Batch 41/64 loss: 0.18006908893585205
Batch 42/64 loss: 0.19074773788452148
Batch 43/64 loss: 0.1648300290107727
Batch 44/64 loss: 0.19818788766860962
Batch 45/64 loss: 0.16293704509735107
Batch 46/64 loss: 0.1795285940170288
Batch 47/64 loss: 0.20328933000564575
Batch 48/64 loss: 0.19048357009887695
Batch 49/64 loss: 0.17501592636108398
Batch 50/64 loss: 0.20453208684921265
Batch 51/64 loss: 0.179834246635437
Batch 52/64 loss: 0.18739712238311768
Batch 53/64 loss: 0.17167633771896362
Batch 54/64 loss: 0.17876148223876953
Batch 55/64 loss: 0.1720641851425171
Batch 56/64 loss: 0.1879211664199829
Batch 57/64 loss: 0.20719915628433228
Batch 58/64 loss: 0.18270862102508545
Batch 59/64 loss: 0.20546716451644897
Batch 60/64 loss: 0.18384146690368652
Batch 61/64 loss: 0.19666707515716553
Batch 62/64 loss: 0.17961353063583374
Batch 63/64 loss: 0.17672938108444214
Batch 64/64 loss: 0.16370868682861328
Epoch 18  Train loss: 0.18608689214669022  Val loss: 0.19385999154389108
Saving best model, epoch: 18
Epoch 19
-------------------------------
Batch 1/64 loss: 0.18189787864685059
Batch 2/64 loss: 0.17458391189575195
Batch 3/64 loss: 0.18588507175445557
Batch 4/64 loss: 0.18512988090515137
Batch 5/64 loss: 0.1984879970550537
Batch 6/64 loss: 0.1876920461654663
Batch 7/64 loss: 0.16618800163269043
Batch 8/64 loss: 0.20264267921447754
Batch 9/64 loss: 0.1708589792251587
Batch 10/64 loss: 0.19447201490402222
Batch 11/64 loss: 0.15114784240722656
Batch 12/64 loss: 0.1746211051940918
Batch 13/64 loss: 0.16496360301971436
Batch 14/64 loss: 0.1838926076889038
Batch 15/64 loss: 0.18409007787704468
Batch 16/64 loss: 0.1800859570503235
Batch 17/64 loss: 0.16456598043441772
Batch 18/64 loss: 0.17869269847869873
Batch 19/64 loss: 0.18716448545455933
Batch 20/64 loss: 0.1875685453414917
Batch 21/64 loss: 0.20371800661087036
Batch 22/64 loss: 0.16858220100402832
Batch 23/64 loss: 0.17352831363677979
Batch 24/64 loss: 0.17781078815460205
Batch 25/64 loss: 0.1752033233642578
Batch 26/64 loss: 0.17491471767425537
Batch 27/64 loss: 0.17810845375061035
Batch 28/64 loss: 0.18275368213653564
Batch 29/64 loss: 0.17676889896392822
Batch 30/64 loss: 0.1985548734664917
Batch 31/64 loss: 0.1761089563369751
Batch 32/64 loss: 0.18127834796905518
Batch 33/64 loss: 0.17684489488601685
Batch 34/64 loss: 0.16010534763336182
Batch 35/64 loss: 0.1888582706451416
Batch 36/64 loss: 0.1779913306236267
Batch 37/64 loss: 0.17782461643218994
Batch 38/64 loss: 0.17943161725997925
Batch 39/64 loss: 0.1915750503540039
Batch 40/64 loss: 0.1938745379447937
Batch 41/64 loss: 0.1831151247024536
Batch 42/64 loss: 0.17951661348342896
Batch 43/64 loss: 0.18221056461334229
Batch 44/64 loss: 0.18165314197540283
Batch 45/64 loss: 0.18103986978530884
Batch 46/64 loss: 0.1848168969154358
Batch 47/64 loss: 0.18836629390716553
Batch 48/64 loss: 0.19141483306884766
Batch 49/64 loss: 0.1711411476135254
Batch 50/64 loss: 0.18129253387451172
Batch 51/64 loss: 0.18339240550994873
Batch 52/64 loss: 0.17666852474212646
Batch 53/64 loss: 0.17685580253601074
Batch 54/64 loss: 0.1970003843307495
Batch 55/64 loss: 0.19410741329193115
Batch 56/64 loss: 0.20455443859100342
Batch 57/64 loss: 0.17613589763641357
Batch 58/64 loss: 0.2219071388244629
Batch 59/64 loss: 0.18113309144973755
Batch 60/64 loss: 0.163643479347229
Batch 61/64 loss: 0.16867190599441528
Batch 62/64 loss: 0.168121337890625
Batch 63/64 loss: 0.16956567764282227
Batch 64/64 loss: 0.17709481716156006
Epoch 19  Train loss: 0.1812958937065274  Val loss: 0.1964566654356075
Epoch 20
-------------------------------
Batch 1/64 loss: 0.16237103939056396
Batch 2/64 loss: 0.17992287874221802
Batch 3/64 loss: 0.16531705856323242
Batch 4/64 loss: 0.1695699691772461
Batch 5/64 loss: 0.1840648651123047
Batch 6/64 loss: 0.18609392642974854
Batch 7/64 loss: 0.1843937635421753
Batch 8/64 loss: 0.17737919092178345
Batch 9/64 loss: 0.18395733833312988
Batch 10/64 loss: 0.18052494525909424
Batch 11/64 loss: 0.19951194524765015
Batch 12/64 loss: 0.20309364795684814
Batch 13/64 loss: 0.1697983741760254
Batch 14/64 loss: 0.2097061276435852
Batch 15/64 loss: 0.17148476839065552
Batch 16/64 loss: 0.1652994155883789
Batch 17/64 loss: 0.173264741897583
Batch 18/64 loss: 0.1778630018234253
Batch 19/64 loss: 0.15565073490142822
Batch 20/64 loss: 0.18499130010604858
Batch 21/64 loss: 0.17380130290985107
Batch 22/64 loss: 0.1633813977241516
Batch 23/64 loss: 0.16064423322677612
Batch 24/64 loss: 0.1670076847076416
Batch 25/64 loss: 0.18550533056259155
Batch 26/64 loss: 0.17246770858764648
Batch 27/64 loss: 0.18393206596374512
Batch 28/64 loss: 0.15648722648620605
Batch 29/64 loss: 0.16256535053253174
Batch 30/64 loss: 0.1656932830810547
Batch 31/64 loss: 0.18440163135528564
Batch 32/64 loss: 0.19354921579360962
Batch 33/64 loss: 0.18226808309555054
Batch 34/64 loss: 0.1911962628364563
Batch 35/64 loss: 0.17104756832122803
Batch 36/64 loss: 0.18746531009674072
Batch 37/64 loss: 0.1904810667037964
Batch 38/64 loss: 0.17631447315216064
Batch 39/64 loss: 0.16121459007263184
Batch 40/64 loss: 0.164556622505188
Batch 41/64 loss: 0.1802840232849121
Batch 42/64 loss: 0.18849867582321167
Batch 43/64 loss: 0.17681431770324707
Batch 44/64 loss: 0.181194007396698
Batch 45/64 loss: 0.18914055824279785
Batch 46/64 loss: 0.16419923305511475
Batch 47/64 loss: 0.17185115814208984
Batch 48/64 loss: 0.17119359970092773
Batch 49/64 loss: 0.19295179843902588
Batch 50/64 loss: 0.18230056762695312
Batch 51/64 loss: 0.15146636962890625
Batch 52/64 loss: 0.20124447345733643
Batch 53/64 loss: 0.16360795497894287
Batch 54/64 loss: 0.16241967678070068
Batch 55/64 loss: 0.1680523157119751
Batch 56/64 loss: 0.19024670124053955
Batch 57/64 loss: 0.17253881692886353
Batch 58/64 loss: 0.20973163843154907
Batch 59/64 loss: 0.17371898889541626
Batch 60/64 loss: 0.1852731704711914
Batch 61/64 loss: 0.19039469957351685
Batch 62/64 loss: 0.1680823564529419
Batch 63/64 loss: 0.18666613101959229
Batch 64/64 loss: 0.1634383201599121
Epoch 20  Train loss: 0.1776108143376369  Val loss: 0.19601257027629315
Epoch 21
-------------------------------
Batch 1/64 loss: 0.17346036434173584
Batch 2/64 loss: 0.1865738034248352
Batch 3/64 loss: 0.170751690864563
Batch 4/64 loss: 0.17773973941802979
Batch 5/64 loss: 0.17308109998703003
Batch 6/64 loss: 0.18188774585723877
Batch 7/64 loss: 0.18545901775360107
Batch 8/64 loss: 0.17627191543579102
Batch 9/64 loss: 0.1745462417602539
Batch 10/64 loss: 0.1792818307876587
Batch 11/64 loss: 0.1713828444480896
Batch 12/64 loss: 0.18536031246185303
Batch 13/64 loss: 0.16797196865081787
Batch 14/64 loss: 0.16257470846176147
Batch 15/64 loss: 0.17906296253204346
Batch 16/64 loss: 0.17609000205993652
Batch 17/64 loss: 0.16280192136764526
Batch 18/64 loss: 0.19221079349517822
Batch 19/64 loss: 0.17431044578552246
Batch 20/64 loss: 0.14908182621002197
Batch 21/64 loss: 0.16506540775299072
Batch 22/64 loss: 0.1448981761932373
Batch 23/64 loss: 0.17610645294189453
Batch 24/64 loss: 0.18698018789291382
Batch 25/64 loss: 0.17510563135147095
Batch 26/64 loss: 0.1574186086654663
Batch 27/64 loss: 0.17347323894500732
Batch 28/64 loss: 0.1886894702911377
Batch 29/64 loss: 0.17053663730621338
Batch 30/64 loss: 0.1674211025238037
Batch 31/64 loss: 0.16344070434570312
Batch 32/64 loss: 0.18063175678253174
Batch 33/64 loss: 0.18437784910202026
Batch 34/64 loss: 0.1857236623764038
Batch 35/64 loss: 0.18902075290679932
Batch 36/64 loss: 0.15963959693908691
Batch 37/64 loss: 0.17070448398590088
Batch 38/64 loss: 0.17444467544555664
Batch 39/64 loss: 0.16025561094284058
Batch 40/64 loss: 0.1895388960838318
Batch 41/64 loss: 0.18149864673614502
Batch 42/64 loss: 0.1498502492904663
Batch 43/64 loss: 0.15668344497680664
Batch 44/64 loss: 0.1647283434867859
Batch 45/64 loss: 0.19115006923675537
Batch 46/64 loss: 0.17198455333709717
Batch 47/64 loss: 0.17596721649169922
Batch 48/64 loss: 0.16349244117736816
Batch 49/64 loss: 0.18261992931365967
Batch 50/64 loss: 0.18665409088134766
Batch 51/64 loss: 0.16743290424346924
Batch 52/64 loss: 0.16575860977172852
Batch 53/64 loss: 0.178367018699646
Batch 54/64 loss: 0.17488718032836914
Batch 55/64 loss: 0.1670612096786499
Batch 56/64 loss: 0.1708076000213623
Batch 57/64 loss: 0.17283445596694946
Batch 58/64 loss: 0.19628256559371948
Batch 59/64 loss: 0.16556447744369507
Batch 60/64 loss: 0.14196395874023438
Batch 61/64 loss: 0.17220783233642578
Batch 62/64 loss: 0.15956372022628784
Batch 63/64 loss: 0.17070215940475464
Batch 64/64 loss: 0.1840009093284607
Epoch 21  Train loss: 0.17301078427071664  Val loss: 0.1865262157728582
Saving best model, epoch: 21
Epoch 22
-------------------------------
Batch 1/64 loss: 0.16541171073913574
Batch 2/64 loss: 0.1609419584274292
Batch 3/64 loss: 0.1690499186515808
Batch 4/64 loss: 0.1677117943763733
Batch 5/64 loss: 0.16392946243286133
Batch 6/64 loss: 0.1702132225036621
Batch 7/64 loss: 0.16957688331604004
Batch 8/64 loss: 0.16874021291732788
Batch 9/64 loss: 0.14346706867218018
Batch 10/64 loss: 0.16365212202072144
Batch 11/64 loss: 0.1715221405029297
Batch 12/64 loss: 0.1697019338607788
Batch 13/64 loss: 0.17477935552597046
Batch 14/64 loss: 0.1568455696105957
Batch 15/64 loss: 0.15561437606811523
Batch 16/64 loss: 0.1613602638244629
Batch 17/64 loss: 0.17496490478515625
Batch 18/64 loss: 0.17393016815185547
Batch 19/64 loss: 0.1634153127670288
Batch 20/64 loss: 0.1682223081588745
Batch 21/64 loss: 0.15324079990386963
Batch 22/64 loss: 0.17842906713485718
Batch 23/64 loss: 0.18744421005249023
Batch 24/64 loss: 0.15955770015716553
Batch 25/64 loss: 0.180930495262146
Batch 26/64 loss: 0.1654912233352661
Batch 27/64 loss: 0.17255693674087524
Batch 28/64 loss: 0.15096229314804077
Batch 29/64 loss: 0.1638277769088745
Batch 30/64 loss: 0.18509292602539062
Batch 31/64 loss: 0.15980184078216553
Batch 32/64 loss: 0.15295636653900146
Batch 33/64 loss: 0.18462246656417847
Batch 34/64 loss: 0.16627556085586548
Batch 35/64 loss: 0.17386865615844727
Batch 36/64 loss: 0.16845953464508057
Batch 37/64 loss: 0.1815246343612671
Batch 38/64 loss: 0.16965973377227783
Batch 39/64 loss: 0.19113785028457642
Batch 40/64 loss: 0.17855924367904663
Batch 41/64 loss: 0.1754114031791687
Batch 42/64 loss: 0.18241143226623535
Batch 43/64 loss: 0.17414361238479614
Batch 44/64 loss: 0.16136771440505981
Batch 45/64 loss: 0.16788572072982788
Batch 46/64 loss: 0.15250670909881592
Batch 47/64 loss: 0.17105519771575928
Batch 48/64 loss: 0.16409456729888916
Batch 49/64 loss: 0.16411590576171875
Batch 50/64 loss: 0.1744920015335083
Batch 51/64 loss: 0.1581364870071411
Batch 52/64 loss: 0.16462969779968262
Batch 53/64 loss: 0.15212774276733398
Batch 54/64 loss: 0.17946100234985352
Batch 55/64 loss: 0.15916723012924194
Batch 56/64 loss: 0.16484355926513672
Batch 57/64 loss: 0.16017937660217285
Batch 58/64 loss: 0.15444612503051758
Batch 59/64 loss: 0.1567365527153015
Batch 60/64 loss: 0.16802000999450684
Batch 61/64 loss: 0.15426474809646606
Batch 62/64 loss: 0.16777712106704712
Batch 63/64 loss: 0.16050875186920166
Batch 64/64 loss: 0.14119315147399902
Epoch 22  Train loss: 0.16676278488308777  Val loss: 0.18550567446705402
Saving best model, epoch: 22
Epoch 23
-------------------------------
Batch 1/64 loss: 0.1645970344543457
Batch 2/64 loss: 0.1555495262145996
Batch 3/64 loss: 0.18801474571228027
Batch 4/64 loss: 0.17207121849060059
Batch 5/64 loss: 0.16354990005493164
Batch 6/64 loss: 0.1683226227760315
Batch 7/64 loss: 0.16531282663345337
Batch 8/64 loss: 0.14708441495895386
Batch 9/64 loss: 0.15967035293579102
Batch 10/64 loss: 0.15177124738693237
Batch 11/64 loss: 0.1572474241256714
Batch 12/64 loss: 0.1625317931175232
Batch 13/64 loss: 0.16121095418930054
Batch 14/64 loss: 0.169741690158844
Batch 15/64 loss: 0.1573941707611084
Batch 16/64 loss: 0.16523092985153198
Batch 17/64 loss: 0.15967965126037598
Batch 18/64 loss: 0.15927654504776
Batch 19/64 loss: 0.16775548458099365
Batch 20/64 loss: 0.1665317416191101
Batch 21/64 loss: 0.16000306606292725
Batch 22/64 loss: 0.16840380430221558
Batch 23/64 loss: 0.17068064212799072
Batch 24/64 loss: 0.1715129017829895
Batch 25/64 loss: 0.15234732627868652
Batch 26/64 loss: 0.15228700637817383
Batch 27/64 loss: 0.15441930294036865
Batch 28/64 loss: 0.16254723072052002
Batch 29/64 loss: 0.17766010761260986
Batch 30/64 loss: 0.17199504375457764
Batch 31/64 loss: 0.1714063286781311
Batch 32/64 loss: 0.14684468507766724
Batch 33/64 loss: 0.17065858840942383
Batch 34/64 loss: 0.1766929030418396
Batch 35/64 loss: 0.1574099063873291
Batch 36/64 loss: 0.16169095039367676
Batch 37/64 loss: 0.1453670859336853
Batch 38/64 loss: 0.15582406520843506
Batch 39/64 loss: 0.15841925144195557
Batch 40/64 loss: 0.16379070281982422
Batch 41/64 loss: 0.1697479486465454
Batch 42/64 loss: 0.15508782863616943
Batch 43/64 loss: 0.15002179145812988
Batch 44/64 loss: 0.16860002279281616
Batch 45/64 loss: 0.14655804634094238
Batch 46/64 loss: 0.1725839376449585
Batch 47/64 loss: 0.1371920108795166
Batch 48/64 loss: 0.17750656604766846
Batch 49/64 loss: 0.16161108016967773
Batch 50/64 loss: 0.1445941925048828
Batch 51/64 loss: 0.15958356857299805
Batch 52/64 loss: 0.14696753025054932
Batch 53/64 loss: 0.15525484085083008
Batch 54/64 loss: 0.16333240270614624
Batch 55/64 loss: 0.15784257650375366
Batch 56/64 loss: 0.1558402180671692
Batch 57/64 loss: 0.1566983461380005
Batch 58/64 loss: 0.17422938346862793
Batch 59/64 loss: 0.17847466468811035
Batch 60/64 loss: 0.17252308130264282
Batch 61/64 loss: 0.15072393417358398
Batch 62/64 loss: 0.17075800895690918
Batch 63/64 loss: 0.16426146030426025
Batch 64/64 loss: 0.14430177211761475
Epoch 23  Train loss: 0.16173686186472574  Val loss: 0.17536869430050409
Saving best model, epoch: 23
Epoch 24
-------------------------------
Batch 1/64 loss: 0.14289480447769165
Batch 2/64 loss: 0.13993090391159058
Batch 3/64 loss: 0.1502804160118103
Batch 4/64 loss: 0.16268140077590942
Batch 5/64 loss: 0.1789628267288208
Batch 6/64 loss: 0.17440927028656006
Batch 7/64 loss: 0.1860564947128296
Batch 8/64 loss: 0.14724552631378174
Batch 9/64 loss: 0.14986777305603027
Batch 10/64 loss: 0.16425514221191406
Batch 11/64 loss: 0.15188056230545044
Batch 12/64 loss: 0.16294682025909424
Batch 13/64 loss: 0.17586660385131836
Batch 14/64 loss: 0.16684472560882568
Batch 15/64 loss: 0.16269135475158691
Batch 16/64 loss: 0.1755892038345337
Batch 17/64 loss: 0.15895092487335205
Batch 18/64 loss: 0.1619037389755249
Batch 19/64 loss: 0.1553698182106018
Batch 20/64 loss: 0.14894801378250122
Batch 21/64 loss: 0.14869242906570435
Batch 22/64 loss: 0.16494512557983398
Batch 23/64 loss: 0.15493714809417725
Batch 24/64 loss: 0.1579992175102234
Batch 25/64 loss: 0.15195947885513306
Batch 26/64 loss: 0.14565205574035645
Batch 27/64 loss: 0.15220355987548828
Batch 28/64 loss: 0.16910910606384277
Batch 29/64 loss: 0.16708707809448242
Batch 30/64 loss: 0.14413470029830933
Batch 31/64 loss: 0.14078670740127563
Batch 32/64 loss: 0.16506803035736084
Batch 33/64 loss: 0.1439276933670044
Batch 34/64 loss: 0.17702245712280273
Batch 35/64 loss: 0.1643465757369995
Batch 36/64 loss: 0.15019738674163818
Batch 37/64 loss: 0.1596146821975708
Batch 38/64 loss: 0.14968478679656982
Batch 39/64 loss: 0.1511690616607666
Batch 40/64 loss: 0.17325371503829956
Batch 41/64 loss: 0.17982244491577148
Batch 42/64 loss: 0.17776745557785034
Batch 43/64 loss: 0.15932857990264893
Batch 44/64 loss: 0.16992980241775513
Batch 45/64 loss: 0.1608465313911438
Batch 46/64 loss: 0.18695974349975586
Batch 47/64 loss: 0.1637561321258545
Batch 48/64 loss: 0.16722595691680908
Batch 49/64 loss: 0.16937124729156494
Batch 50/64 loss: 0.17848670482635498
Batch 51/64 loss: 0.1600726842880249
Batch 52/64 loss: 0.16615164279937744
Batch 53/64 loss: 0.15017789602279663
Batch 54/64 loss: 0.17274898290634155
Batch 55/64 loss: 0.14714771509170532
Batch 56/64 loss: 0.14687496423721313
Batch 57/64 loss: 0.16622936725616455
Batch 58/64 loss: 0.13399279117584229
Batch 59/64 loss: 0.14995276927947998
Batch 60/64 loss: 0.16548281908035278
Batch 61/64 loss: 0.1412608027458191
Batch 62/64 loss: 0.13587331771850586
Batch 63/64 loss: 0.16661441326141357
Batch 64/64 loss: 0.14253443479537964
Epoch 24  Train loss: 0.1595661633154925  Val loss: 0.17316517223607225
Saving best model, epoch: 24
Epoch 25
-------------------------------
Batch 1/64 loss: 0.16368675231933594
Batch 2/64 loss: 0.16108715534210205
Batch 3/64 loss: 0.14749610424041748
Batch 4/64 loss: 0.14760541915893555
Batch 5/64 loss: 0.14528363943099976
Batch 6/64 loss: 0.16216224431991577
Batch 7/64 loss: 0.1596512794494629
Batch 8/64 loss: 0.1573229432106018
Batch 9/64 loss: 0.14284038543701172
Batch 10/64 loss: 0.14233016967773438
Batch 11/64 loss: 0.16653484106063843
Batch 12/64 loss: 0.1566290259361267
Batch 13/64 loss: 0.15525531768798828
Batch 14/64 loss: 0.16659361124038696
Batch 15/64 loss: 0.15891218185424805
Batch 16/64 loss: 0.13104987144470215
Batch 17/64 loss: 0.13058006763458252
Batch 18/64 loss: 0.16469693183898926
Batch 19/64 loss: 0.15536844730377197
Batch 20/64 loss: 0.15880614519119263
Batch 21/64 loss: 0.15541326999664307
Batch 22/64 loss: 0.1595684289932251
Batch 23/64 loss: 0.14728033542633057
Batch 24/64 loss: 0.14052748680114746
Batch 25/64 loss: 0.14872759580612183
Batch 26/64 loss: 0.16171562671661377
Batch 27/64 loss: 0.14544415473937988
Batch 28/64 loss: 0.13726407289505005
Batch 29/64 loss: 0.1609363555908203
Batch 30/64 loss: 0.15259450674057007
Batch 31/64 loss: 0.16273540258407593
Batch 32/64 loss: 0.14126694202423096
Batch 33/64 loss: 0.15510505437850952
Batch 34/64 loss: 0.15255647897720337
Batch 35/64 loss: 0.14163422584533691
Batch 36/64 loss: 0.14264893531799316
Batch 37/64 loss: 0.15521645545959473
Batch 38/64 loss: 0.1366586685180664
Batch 39/64 loss: 0.1582210659980774
Batch 40/64 loss: 0.15672588348388672
Batch 41/64 loss: 0.15423941612243652
Batch 42/64 loss: 0.14799296855926514
Batch 43/64 loss: 0.14794385433197021
Batch 44/64 loss: 0.14551734924316406
Batch 45/64 loss: 0.1433262825012207
Batch 46/64 loss: 0.16137152910232544
Batch 47/64 loss: 0.1409921646118164
Batch 48/64 loss: 0.14545851945877075
Batch 49/64 loss: 0.16228461265563965
Batch 50/64 loss: 0.1575137972831726
Batch 51/64 loss: 0.13962191343307495
Batch 52/64 loss: 0.15787750482559204
Batch 53/64 loss: 0.1638008952140808
Batch 54/64 loss: 0.1353151798248291
Batch 55/64 loss: 0.14738667011260986
Batch 56/64 loss: 0.14301490783691406
Batch 57/64 loss: 0.1354672908782959
Batch 58/64 loss: 0.17389726638793945
Batch 59/64 loss: 0.14800000190734863
Batch 60/64 loss: 0.15848010778427124
Batch 61/64 loss: 0.1370311975479126
Batch 62/64 loss: 0.14856815338134766
Batch 63/64 loss: 0.13959944248199463
Batch 64/64 loss: 0.13871091604232788
Epoch 25  Train loss: 0.15097831674650602  Val loss: 0.16943007646147737
Saving best model, epoch: 25
Epoch 26
-------------------------------
Batch 1/64 loss: 0.14932608604431152
Batch 2/64 loss: 0.15114939212799072
Batch 3/64 loss: 0.1422433853149414
Batch 4/64 loss: 0.14375537633895874
Batch 5/64 loss: 0.12958234548568726
Batch 6/64 loss: 0.1500406265258789
Batch 7/64 loss: 0.15358179807662964
Batch 8/64 loss: 0.14962470531463623
Batch 9/64 loss: 0.15327668190002441
Batch 10/64 loss: 0.1449120044708252
Batch 11/64 loss: 0.1491628885269165
Batch 12/64 loss: 0.170110821723938
Batch 13/64 loss: 0.15623462200164795
Batch 14/64 loss: 0.1388353705406189
Batch 15/64 loss: 0.1437842845916748
Batch 16/64 loss: 0.1620546579360962
Batch 17/64 loss: 0.15136820077896118
Batch 18/64 loss: 0.1443190574645996
Batch 19/64 loss: 0.1451130509376526
Batch 20/64 loss: 0.15909916162490845
Batch 21/64 loss: 0.15001118183135986
Batch 22/64 loss: 0.15678858757019043
Batch 23/64 loss: 0.17062854766845703
Batch 24/64 loss: 0.12905406951904297
Batch 25/64 loss: 0.13454145193099976
Batch 26/64 loss: 0.1267760992050171
Batch 27/64 loss: 0.16316020488739014
Batch 28/64 loss: 0.15904605388641357
Batch 29/64 loss: 0.12581783533096313
Batch 30/64 loss: 0.13432157039642334
Batch 31/64 loss: 0.139939546585083
Batch 32/64 loss: 0.11881303787231445
Batch 33/64 loss: 0.14509248733520508
Batch 34/64 loss: 0.1603609323501587
Batch 35/64 loss: 0.15119481086730957
Batch 36/64 loss: 0.13872456550598145
Batch 37/64 loss: 0.1421424150466919
Batch 38/64 loss: 0.14906370639801025
Batch 39/64 loss: 0.14010822772979736
Batch 40/64 loss: 0.16361945867538452
Batch 41/64 loss: 0.17313683032989502
Batch 42/64 loss: 0.14954841136932373
Batch 43/64 loss: 0.12088054418563843
Batch 44/64 loss: 0.1350039839744568
Batch 45/64 loss: 0.13491100072860718
Batch 46/64 loss: 0.1629112958908081
Batch 47/64 loss: 0.14353132247924805
Batch 48/64 loss: 0.14068496227264404
Batch 49/64 loss: 0.14882850646972656
Batch 50/64 loss: 0.15511298179626465
Batch 51/64 loss: 0.17084074020385742
Batch 52/64 loss: 0.15839308500289917
Batch 53/64 loss: 0.16019713878631592
Batch 54/64 loss: 0.12887287139892578
Batch 55/64 loss: 0.1410466432571411
Batch 56/64 loss: 0.1451479196548462
Batch 57/64 loss: 0.13781970739364624
Batch 58/64 loss: 0.1188347339630127
Batch 59/64 loss: 0.15818679332733154
Batch 60/64 loss: 0.1435554027557373
Batch 61/64 loss: 0.12952017784118652
Batch 62/64 loss: 0.16445207595825195
Batch 63/64 loss: 0.14453887939453125
Batch 64/64 loss: 0.15452957153320312
Epoch 26  Train loss: 0.14695894110436533  Val loss: 0.16553862377540352
Saving best model, epoch: 26
Epoch 27
-------------------------------
Batch 1/64 loss: 0.14410269260406494
Batch 2/64 loss: 0.13040077686309814
Batch 3/64 loss: 0.14653605222702026
Batch 4/64 loss: 0.12830215692520142
Batch 5/64 loss: 0.12598538398742676
Batch 6/64 loss: 0.12040525674819946
Batch 7/64 loss: 0.14841562509536743
Batch 8/64 loss: 0.12569522857666016
Batch 9/64 loss: 0.13542062044143677
Batch 10/64 loss: 0.1723945140838623
Batch 11/64 loss: 0.14643651247024536
Batch 12/64 loss: 0.12458902597427368
Batch 13/64 loss: 0.1487002968788147
Batch 14/64 loss: 0.127993643283844
Batch 15/64 loss: 0.11606103181838989
Batch 16/64 loss: 0.1578763723373413
Batch 17/64 loss: 0.12932276725769043
Batch 18/64 loss: 0.1455325484275818
Batch 19/64 loss: 0.1279643177986145
Batch 20/64 loss: 0.14899873733520508
Batch 21/64 loss: 0.15079891681671143
Batch 22/64 loss: 0.14786803722381592
Batch 23/64 loss: 0.13394594192504883
Batch 24/64 loss: 0.12126791477203369
Batch 25/64 loss: 0.17197269201278687
Batch 26/64 loss: 0.1452186107635498
Batch 27/64 loss: 0.14880454540252686
Batch 28/64 loss: 0.1272532343864441
Batch 29/64 loss: 0.11324077844619751
Batch 30/64 loss: 0.1472930908203125
Batch 31/64 loss: 0.15407288074493408
Batch 32/64 loss: 0.14645028114318848
Batch 33/64 loss: 0.1455121636390686
Batch 34/64 loss: 0.1380510926246643
Batch 35/64 loss: 0.138241708278656
Batch 36/64 loss: 0.12443459033966064
Batch 37/64 loss: 0.13576793670654297
Batch 38/64 loss: 0.13611018657684326
Batch 39/64 loss: 0.12665700912475586
Batch 40/64 loss: 0.11464190483093262
Batch 41/64 loss: 0.15926587581634521
Batch 42/64 loss: 0.15011972188949585
Batch 43/64 loss: 0.1282874345779419
Batch 44/64 loss: 0.14631623029708862
Batch 45/64 loss: 0.13518202304840088
Batch 46/64 loss: 0.14403396844863892
Batch 47/64 loss: 0.15645724534988403
Batch 48/64 loss: 0.15849041938781738
Batch 49/64 loss: 0.14278572797775269
Batch 50/64 loss: 0.13751298189163208
Batch 51/64 loss: 0.1422474980354309
Batch 52/64 loss: 0.11964452266693115
Batch 53/64 loss: 0.1518864631652832
Batch 54/64 loss: 0.14219385385513306
Batch 55/64 loss: 0.16991102695465088
Batch 56/64 loss: 0.11082953214645386
Batch 57/64 loss: 0.134560227394104
Batch 58/64 loss: 0.14347445964813232
Batch 59/64 loss: 0.1510167121887207
Batch 60/64 loss: 0.16610503196716309
Batch 61/64 loss: 0.1693781018257141
Batch 62/64 loss: 0.12433743476867676
Batch 63/64 loss: 0.13970154523849487
Batch 64/64 loss: 0.16168177127838135
Epoch 27  Train loss: 0.14060763050528133  Val loss: 0.1739570064233341
Epoch 28
-------------------------------
Batch 1/64 loss: 0.17039495706558228
Batch 2/64 loss: 0.1366490125656128
Batch 3/64 loss: 0.1531980037689209
Batch 4/64 loss: 0.1323455572128296
Batch 5/64 loss: 0.13894051313400269
Batch 6/64 loss: 0.16691571474075317
Batch 7/64 loss: 0.15207785367965698
Batch 8/64 loss: 0.14833378791809082
Batch 9/64 loss: 0.10828143358230591
Batch 10/64 loss: 0.16790002584457397
Batch 11/64 loss: 0.17472314834594727
Batch 12/64 loss: 0.14622730016708374
Batch 13/64 loss: 0.17154312133789062
Batch 14/64 loss: 0.1459924578666687
Batch 15/64 loss: 0.14611268043518066
Batch 16/64 loss: 0.1309187412261963
Batch 17/64 loss: 0.142319917678833
Batch 18/64 loss: 0.14292556047439575
Batch 19/64 loss: 0.16470396518707275
Batch 20/64 loss: 0.12144976854324341
Batch 21/64 loss: 0.12773704528808594
Batch 22/64 loss: 0.11226826906204224
Batch 23/64 loss: 0.1160007119178772
Batch 24/64 loss: 0.1320343017578125
Batch 25/64 loss: 0.13966071605682373
Batch 26/64 loss: 0.1481112837791443
Batch 27/64 loss: 0.13565915822982788
Batch 28/64 loss: 0.14489054679870605
Batch 29/64 loss: 0.1285339593887329
Batch 30/64 loss: 0.1349961757659912
Batch 31/64 loss: 0.14112210273742676
Batch 32/64 loss: 0.1264122724533081
Batch 33/64 loss: 0.135769784450531
Batch 34/64 loss: 0.14425218105316162
Batch 35/64 loss: 0.1203424334526062
Batch 36/64 loss: 0.13254237174987793
Batch 37/64 loss: 0.1470077633857727
Batch 38/64 loss: 0.12876391410827637
Batch 39/64 loss: 0.13507699966430664
Batch 40/64 loss: 0.14289438724517822
Batch 41/64 loss: 0.126991868019104
Batch 42/64 loss: 0.1424037218093872
Batch 43/64 loss: 0.16204369068145752
Batch 44/64 loss: 0.13333630561828613
Batch 45/64 loss: 0.13766342401504517
Batch 46/64 loss: 0.13800954818725586
Batch 47/64 loss: 0.13011223077774048
Batch 48/64 loss: 0.13461941480636597
Batch 49/64 loss: 0.1370096206665039
Batch 50/64 loss: 0.13241428136825562
Batch 51/64 loss: 0.1578240990638733
Batch 52/64 loss: 0.13507986068725586
Batch 53/64 loss: 0.13896024227142334
Batch 54/64 loss: 0.1467309594154358
Batch 55/64 loss: 0.13053154945373535
Batch 56/64 loss: 0.14040327072143555
Batch 57/64 loss: 0.11558836698532104
Batch 58/64 loss: 0.13839930295944214
Batch 59/64 loss: 0.15509033203125
Batch 60/64 loss: 0.13949495553970337
Batch 61/64 loss: 0.1363506317138672
Batch 62/64 loss: 0.13683867454528809
Batch 63/64 loss: 0.15000993013381958
Batch 64/64 loss: 0.13540410995483398
Epoch 28  Train loss: 0.14013316491070915  Val loss: 0.15797505243537352
Saving best model, epoch: 28
Epoch 29
-------------------------------
Batch 1/64 loss: 0.15458059310913086
Batch 2/64 loss: 0.14397180080413818
Batch 3/64 loss: 0.1484737992286682
Batch 4/64 loss: 0.14036661386489868
Batch 5/64 loss: 0.1456478238105774
Batch 6/64 loss: 0.1354055404663086
Batch 7/64 loss: 0.14074182510375977
Batch 8/64 loss: 0.12767064571380615
Batch 9/64 loss: 0.13053035736083984
Batch 10/64 loss: 0.12189102172851562
Batch 11/64 loss: 0.1226186752319336
Batch 12/64 loss: 0.15100985765457153
Batch 13/64 loss: 0.138003408908844
Batch 14/64 loss: 0.13543564081192017
Batch 15/64 loss: 0.1552274227142334
Batch 16/64 loss: 0.14658474922180176
Batch 17/64 loss: 0.11475241184234619
Batch 18/64 loss: 0.13438516855239868
Batch 19/64 loss: 0.15332728624343872
Batch 20/64 loss: 0.14391320943832397
Batch 21/64 loss: 0.12481272220611572
Batch 22/64 loss: 0.1380963921546936
Batch 23/64 loss: 0.1286221742630005
Batch 24/64 loss: 0.14206886291503906
Batch 25/64 loss: 0.15317583084106445
Batch 26/64 loss: 0.14671814441680908
Batch 27/64 loss: 0.12780100107192993
Batch 28/64 loss: 0.13549840450286865
Batch 29/64 loss: 0.1335676908493042
Batch 30/64 loss: 0.14736407995224
Batch 31/64 loss: 0.12433898448944092
Batch 32/64 loss: 0.13606280088424683
Batch 33/64 loss: 0.13605785369873047
Batch 34/64 loss: 0.15842097997665405
Batch 35/64 loss: 0.145704448223114
Batch 36/64 loss: 0.10067886114120483
Batch 37/64 loss: 0.12581992149353027
Batch 38/64 loss: 0.12439483404159546
Batch 39/64 loss: 0.12224245071411133
Batch 40/64 loss: 0.12195980548858643
Batch 41/64 loss: 0.15963959693908691
Batch 42/64 loss: 0.13910752534866333
Batch 43/64 loss: 0.12911218404769897
Batch 44/64 loss: 0.15252649784088135
Batch 45/64 loss: 0.13295131921768188
Batch 46/64 loss: 0.13608062267303467
Batch 47/64 loss: 0.14401572942733765
Batch 48/64 loss: 0.13827967643737793
Batch 49/64 loss: 0.12905669212341309
Batch 50/64 loss: 0.11310428380966187
Batch 51/64 loss: 0.13355755805969238
Batch 52/64 loss: 0.1307123303413391
Batch 53/64 loss: 0.11531460285186768
Batch 54/64 loss: 0.1273592710494995
Batch 55/64 loss: 0.12708258628845215
Batch 56/64 loss: 0.11133384704589844
Batch 57/64 loss: 0.14863276481628418
Batch 58/64 loss: 0.1284857988357544
Batch 59/64 loss: 0.11645889282226562
Batch 60/64 loss: 0.12636220455169678
Batch 61/64 loss: 0.1522749662399292
Batch 62/64 loss: 0.1292216181755066
Batch 63/64 loss: 0.11707842350006104
Batch 64/64 loss: 0.11894822120666504
Epoch 29  Train loss: 0.1346651647605148  Val loss: 0.15926995187280923
Epoch 30
-------------------------------
Batch 1/64 loss: 0.1203991174697876
Batch 2/64 loss: 0.1375584602355957
Batch 3/64 loss: 0.1146116852760315
Batch 4/64 loss: 0.1425563097000122
Batch 5/64 loss: 0.11153578758239746
Batch 6/64 loss: 0.11847329139709473
Batch 7/64 loss: 0.11083918809890747
Batch 8/64 loss: 0.12025398015975952
Batch 9/64 loss: 0.13177263736724854
Batch 10/64 loss: 0.12348604202270508
Batch 11/64 loss: 0.15509575605392456
Batch 12/64 loss: 0.1318712830543518
Batch 13/64 loss: 0.11479812860488892
Batch 14/64 loss: 0.11223292350769043
Batch 15/64 loss: 0.14003491401672363
Batch 16/64 loss: 0.1238701343536377
Batch 17/64 loss: 0.14094364643096924
Batch 18/64 loss: 0.12564373016357422
Batch 19/64 loss: 0.14190441370010376
Batch 20/64 loss: 0.14529073238372803
Batch 21/64 loss: 0.13794267177581787
Batch 22/64 loss: 0.15968215465545654
Batch 23/64 loss: 0.13560926914215088
Batch 24/64 loss: 0.15182191133499146
Batch 25/64 loss: 0.11724281311035156
Batch 26/64 loss: 0.12011390924453735
Batch 27/64 loss: 0.13123023509979248
Batch 28/64 loss: 0.1111220121383667
Batch 29/64 loss: 0.11625909805297852
Batch 30/64 loss: 0.13635027408599854
Batch 31/64 loss: 0.14725065231323242
Batch 32/64 loss: 0.10545510053634644
Batch 33/64 loss: 0.1322973370552063
Batch 34/64 loss: 0.11514228582382202
Batch 35/64 loss: 0.12687420845031738
Batch 36/64 loss: 0.12122511863708496
Batch 37/64 loss: 0.1430373191833496
Batch 38/64 loss: 0.13243567943572998
Batch 39/64 loss: 0.1258372664451599
Batch 40/64 loss: 0.12141227722167969
Batch 41/64 loss: 0.1267910599708557
Batch 42/64 loss: 0.14285194873809814
Batch 43/64 loss: 0.1383494734764099
Batch 44/64 loss: 0.14944994449615479
Batch 45/64 loss: 0.1544315218925476
Batch 46/64 loss: 0.12625157833099365
Batch 47/64 loss: 0.13373112678527832
Batch 48/64 loss: 0.12003982067108154
Batch 49/64 loss: 0.11998254060745239
Batch 50/64 loss: 0.15238851308822632
Batch 51/64 loss: 0.13592159748077393
Batch 52/64 loss: 0.0997043251991272
Batch 53/64 loss: 0.13527238368988037
Batch 54/64 loss: 0.13766586780548096
Batch 55/64 loss: 0.1543407440185547
Batch 56/64 loss: 0.13886266946792603
Batch 57/64 loss: 0.12792932987213135
Batch 58/64 loss: 0.1243896484375
Batch 59/64 loss: 0.1262710690498352
Batch 60/64 loss: 0.1280454397201538
Batch 61/64 loss: 0.13702630996704102
Batch 62/64 loss: 0.13168907165527344
Batch 63/64 loss: 0.14064151048660278
Batch 64/64 loss: 0.1233288049697876
Epoch 30  Train loss: 0.13060451535617604  Val loss: 0.14527202307973122
Saving best model, epoch: 30
Epoch 31
-------------------------------
Batch 1/64 loss: 0.14627069234848022
Batch 2/64 loss: 0.14124196767807007
Batch 3/64 loss: 0.12003296613693237
Batch 4/64 loss: 0.11116915941238403
Batch 5/64 loss: 0.13857656717300415
Batch 6/64 loss: 0.10662466287612915
Batch 7/64 loss: 0.13531285524368286
Batch 8/64 loss: 0.13431322574615479
Batch 9/64 loss: 0.1445685625076294
Batch 10/64 loss: 0.10887795686721802
Batch 11/64 loss: 0.1357100009918213
Batch 12/64 loss: 0.15732789039611816
Batch 13/64 loss: 0.1504468321800232
Batch 14/64 loss: 0.14860862493515015
Batch 15/64 loss: 0.12485027313232422
Batch 16/64 loss: 0.13067936897277832
Batch 17/64 loss: 0.13313943147659302
Batch 18/64 loss: 0.12213903665542603
Batch 19/64 loss: 0.12468993663787842
Batch 20/64 loss: 0.12441092729568481
Batch 21/64 loss: 0.1422882080078125
Batch 22/64 loss: 0.13632571697235107
Batch 23/64 loss: 0.1232571005821228
Batch 24/64 loss: 0.1181802749633789
Batch 25/64 loss: 0.1254369020462036
Batch 26/64 loss: 0.11470484733581543
Batch 27/64 loss: 0.12156558036804199
Batch 28/64 loss: 0.10770028829574585
Batch 29/64 loss: 0.11750650405883789
Batch 30/64 loss: 0.12768733501434326
Batch 31/64 loss: 0.10730856657028198
Batch 32/64 loss: 0.15574514865875244
Batch 33/64 loss: 0.12541413307189941
Batch 34/64 loss: 0.12278705835342407
Batch 35/64 loss: 0.12451696395874023
Batch 36/64 loss: 0.11464869976043701
Batch 37/64 loss: 0.14633488655090332
Batch 38/64 loss: 0.12858623266220093
Batch 39/64 loss: 0.1392667293548584
Batch 40/64 loss: 0.13980525732040405
Batch 41/64 loss: 0.11039817333221436
Batch 42/64 loss: 0.13292652368545532
Batch 43/64 loss: 0.1448211669921875
Batch 44/64 loss: 0.12551110982894897
Batch 45/64 loss: 0.13236302137374878
Batch 46/64 loss: 0.0966414213180542
Batch 47/64 loss: 0.116604745388031
Batch 48/64 loss: 0.12136191129684448
Batch 49/64 loss: 0.11766362190246582
Batch 50/64 loss: 0.13661831617355347
Batch 51/64 loss: 0.1247488260269165
Batch 52/64 loss: 0.12682414054870605
Batch 53/64 loss: 0.10580098628997803
Batch 54/64 loss: 0.1233782172203064
Batch 55/64 loss: 0.13671106100082397
Batch 56/64 loss: 0.12805593013763428
Batch 57/64 loss: 0.11676353216171265
Batch 58/64 loss: 0.1186862587928772
Batch 59/64 loss: 0.10778850317001343
Batch 60/64 loss: 0.11815804243087769
Batch 61/64 loss: 0.13255304098129272
Batch 62/64 loss: 0.14869409799575806
Batch 63/64 loss: 0.11017990112304688
Batch 64/64 loss: 0.11933630704879761
Epoch 31  Train loss: 0.12707156316906798  Val loss: 0.1630967392544566
Epoch 32
-------------------------------
Batch 1/64 loss: 0.1476612091064453
Batch 2/64 loss: 0.1447884440422058
Batch 3/64 loss: 0.12051624059677124
Batch 4/64 loss: 0.1275230050086975
Batch 5/64 loss: 0.1107248067855835
Batch 6/64 loss: 0.1328972578048706
Batch 7/64 loss: 0.13781291246414185
Batch 8/64 loss: 0.12819868326187134
Batch 9/64 loss: 0.11930805444717407
Batch 10/64 loss: 0.13511383533477783
Batch 11/64 loss: 0.13117831945419312
Batch 12/64 loss: 0.12676191329956055
Batch 13/64 loss: 0.13096171617507935
Batch 14/64 loss: 0.1365872025489807
Batch 15/64 loss: 0.12962329387664795
Batch 16/64 loss: 0.10336172580718994
Batch 17/64 loss: 0.1380389928817749
Batch 18/64 loss: 0.15236669778823853
Batch 19/64 loss: 0.12075620889663696
Batch 20/64 loss: 0.11727988719940186
Batch 21/64 loss: 0.1324617862701416
Batch 22/64 loss: 0.1050037145614624
Batch 23/64 loss: 0.11668944358825684
Batch 24/64 loss: 0.09942257404327393
Batch 25/64 loss: 0.11321240663528442
Batch 26/64 loss: 0.12762874364852905
Batch 27/64 loss: 0.11273378133773804
Batch 28/64 loss: 0.1315300464630127
Batch 29/64 loss: 0.14240103960037231
Batch 30/64 loss: 0.11959147453308105
Batch 31/64 loss: 0.12383949756622314
Batch 32/64 loss: 0.13587725162506104
Batch 33/64 loss: 0.10577166080474854
Batch 34/64 loss: 0.1310056447982788
Batch 35/64 loss: 0.12919080257415771
Batch 36/64 loss: 0.1453750729560852
Batch 37/64 loss: 0.13181072473526
Batch 38/64 loss: 0.12256848812103271
Batch 39/64 loss: 0.12781751155853271
Batch 40/64 loss: 0.11795461177825928
Batch 41/64 loss: 0.14941179752349854
Batch 42/64 loss: 0.11143839359283447
Batch 43/64 loss: 0.11165666580200195
Batch 44/64 loss: 0.13947933912277222
Batch 45/64 loss: 0.11894077062606812
Batch 46/64 loss: 0.12313616275787354
Batch 47/64 loss: 0.10710012912750244
Batch 48/64 loss: 0.13808196783065796
Batch 49/64 loss: 0.13115614652633667
Batch 50/64 loss: 0.11350226402282715
Batch 51/64 loss: 0.12965363264083862
Batch 52/64 loss: 0.12364709377288818
Batch 53/64 loss: 0.13133323192596436
Batch 54/64 loss: 0.12649816274642944
Batch 55/64 loss: 0.11553913354873657
Batch 56/64 loss: 0.11084461212158203
Batch 57/64 loss: 0.10006272792816162
Batch 58/64 loss: 0.1242140531539917
Batch 59/64 loss: 0.13120949268341064
Batch 60/64 loss: 0.1159408688545227
Batch 61/64 loss: 0.1306682825088501
Batch 62/64 loss: 0.09929579496383667
Batch 63/64 loss: 0.10229885578155518
Batch 64/64 loss: 0.13145077228546143
Epoch 32  Train loss: 0.12465951910205916  Val loss: 0.14940856392031274
Epoch 33
-------------------------------
Batch 1/64 loss: 0.1034388542175293
Batch 2/64 loss: 0.11439347267150879
Batch 3/64 loss: 0.10501831769943237
Batch 4/64 loss: 0.14473921060562134
Batch 5/64 loss: 0.10411703586578369
Batch 6/64 loss: 0.11643725633621216
Batch 7/64 loss: 0.10127431154251099
Batch 8/64 loss: 0.12434589862823486
Batch 9/64 loss: 0.13461095094680786
Batch 10/64 loss: 0.12071841955184937
Batch 11/64 loss: 0.1331031322479248
Batch 12/64 loss: 0.10754728317260742
Batch 13/64 loss: 0.10983610153198242
Batch 14/64 loss: 0.12869417667388916
Batch 15/64 loss: 0.12080782651901245
Batch 16/64 loss: 0.11578172445297241
Batch 17/64 loss: 0.10548722743988037
Batch 18/64 loss: 0.10635101795196533
Batch 19/64 loss: 0.10242843627929688
Batch 20/64 loss: 0.12923896312713623
Batch 21/64 loss: 0.12016260623931885
Batch 22/64 loss: 0.12599468231201172
Batch 23/64 loss: 0.10517293214797974
Batch 24/64 loss: 0.1327875852584839
Batch 25/64 loss: 0.13390189409255981
Batch 26/64 loss: 0.1029977798461914
Batch 27/64 loss: 0.1349257230758667
Batch 28/64 loss: 0.1061168909072876
Batch 29/64 loss: 0.1326225996017456
Batch 30/64 loss: 0.11890745162963867
Batch 31/64 loss: 0.12166154384613037
Batch 32/64 loss: 0.13418877124786377
Batch 33/64 loss: 0.14704471826553345
Batch 34/64 loss: 0.11499345302581787
Batch 35/64 loss: 0.13894033432006836
Batch 36/64 loss: 0.14022064208984375
Batch 37/64 loss: 0.12043142318725586
Batch 38/64 loss: 0.13072198629379272
Batch 39/64 loss: 0.1334540843963623
Batch 40/64 loss: 0.10488641262054443
Batch 41/64 loss: 0.12115955352783203
Batch 42/64 loss: 0.1241254210472107
Batch 43/64 loss: 0.14286160469055176
Batch 44/64 loss: 0.09987348318099976
Batch 45/64 loss: 0.12632238864898682
Batch 46/64 loss: 0.12187981605529785
Batch 47/64 loss: 0.11272585391998291
Batch 48/64 loss: 0.10389953851699829
Batch 49/64 loss: 0.1442124843597412
Batch 50/64 loss: 0.09243690967559814
Batch 51/64 loss: 0.12861627340316772
Batch 52/64 loss: 0.11417168378829956
Batch 53/64 loss: 0.1048729419708252
Batch 54/64 loss: 0.10104411840438843
Batch 55/64 loss: 0.11648935079574585
Batch 56/64 loss: 0.13041901588439941
Batch 57/64 loss: 0.08516043424606323
Batch 58/64 loss: 0.11937320232391357
Batch 59/64 loss: 0.16334468126296997
Batch 60/64 loss: 0.12177878618240356
Batch 61/64 loss: 0.12850069999694824
Batch 62/64 loss: 0.12803411483764648
Batch 63/64 loss: 0.11301344633102417
Batch 64/64 loss: 0.10313659906387329
Epoch 33  Train loss: 0.1200026883798487  Val loss: 0.13752503636776378
Saving best model, epoch: 33
Epoch 34
-------------------------------
Batch 1/64 loss: 0.1046152114868164
Batch 2/64 loss: 0.12126648426055908
Batch 3/64 loss: 0.11954641342163086
Batch 4/64 loss: 0.10262817144393921
Batch 5/64 loss: 0.12108194828033447
Batch 6/64 loss: 0.10651087760925293
Batch 7/64 loss: 0.11711233854293823
Batch 8/64 loss: 0.11955475807189941
Batch 9/64 loss: 0.0972098708152771
Batch 10/64 loss: 0.11661577224731445
Batch 11/64 loss: 0.12827610969543457
Batch 12/64 loss: 0.11303955316543579
Batch 13/64 loss: 0.08885747194290161
Batch 14/64 loss: 0.12988978624343872
Batch 15/64 loss: 0.11789017915725708
Batch 16/64 loss: 0.12957912683486938
Batch 17/64 loss: 0.13001704216003418
Batch 18/64 loss: 0.12450516223907471
Batch 19/64 loss: 0.11900138854980469
Batch 20/64 loss: 0.11948907375335693
Batch 21/64 loss: 0.0978841781616211
Batch 22/64 loss: 0.13484495878219604
Batch 23/64 loss: 0.13588249683380127
Batch 24/64 loss: 0.1261616349220276
Batch 25/64 loss: 0.11173206567764282
Batch 26/64 loss: 0.11485469341278076
Batch 27/64 loss: 0.1142200231552124
Batch 28/64 loss: 0.09918344020843506
Batch 29/64 loss: 0.1252117156982422
Batch 30/64 loss: 0.1075170636177063
Batch 31/64 loss: 0.09708893299102783
Batch 32/64 loss: 0.1108473539352417
Batch 33/64 loss: 0.12950587272644043
Batch 34/64 loss: 0.12473738193511963
Batch 35/64 loss: 0.11740994453430176
Batch 36/64 loss: 0.11016875505447388
Batch 37/64 loss: 0.10901445150375366
Batch 38/64 loss: 0.11488842964172363
Batch 39/64 loss: 0.1249774694442749
Batch 40/64 loss: 0.11897814273834229
Batch 41/64 loss: 0.13423389196395874
Batch 42/64 loss: 0.10818397998809814
Batch 43/64 loss: 0.10500890016555786
Batch 44/64 loss: 0.14157980680465698
Batch 45/64 loss: 0.11689114570617676
Batch 46/64 loss: 0.11547577381134033
Batch 47/64 loss: 0.12141603231430054
Batch 48/64 loss: 0.13713884353637695
Batch 49/64 loss: 0.12370342016220093
Batch 50/64 loss: 0.12280988693237305
Batch 51/64 loss: 0.12189698219299316
Batch 52/64 loss: 0.11196696758270264
Batch 53/64 loss: 0.12541568279266357
Batch 54/64 loss: 0.12290215492248535
Batch 55/64 loss: 0.13692736625671387
Batch 56/64 loss: 0.13406193256378174
Batch 57/64 loss: 0.13350015878677368
Batch 58/64 loss: 0.1086958646774292
Batch 59/64 loss: 0.12636798620224
Batch 60/64 loss: 0.09805107116699219
Batch 61/64 loss: 0.1299654245376587
Batch 62/64 loss: 0.13141655921936035
Batch 63/64 loss: 0.11418688297271729
Batch 64/64 loss: 0.08841639757156372
Epoch 34  Train loss: 0.11827301581700643  Val loss: 0.13829253055795362
Epoch 35
-------------------------------
Batch 1/64 loss: 0.11728614568710327
Batch 2/64 loss: 0.12958204746246338
Batch 3/64 loss: 0.11502200365066528
Batch 4/64 loss: 0.1186566948890686
Batch 5/64 loss: 0.1155095100402832
Batch 6/64 loss: 0.11110740900039673
Batch 7/64 loss: 0.13440734148025513
Batch 8/64 loss: 0.08870625495910645
Batch 9/64 loss: 0.14539730548858643
Batch 10/64 loss: 0.14209461212158203
Batch 11/64 loss: 0.08587926626205444
Batch 12/64 loss: 0.12067168951034546
Batch 13/64 loss: 0.10057675838470459
Batch 14/64 loss: 0.1258934736251831
Batch 15/64 loss: 0.08947092294692993
Batch 16/64 loss: 0.12169045209884644
Batch 17/64 loss: 0.10534429550170898
Batch 18/64 loss: 0.12476438283920288
Batch 19/64 loss: 0.14237982034683228
Batch 20/64 loss: 0.09813570976257324
Batch 21/64 loss: 0.10961788892745972
Batch 22/64 loss: 0.10873252153396606
Batch 23/64 loss: 0.1309676170349121
Batch 24/64 loss: 0.12068867683410645
Batch 25/64 loss: 0.09775364398956299
Batch 26/64 loss: 0.12083804607391357
Batch 27/64 loss: 0.10060733556747437
Batch 28/64 loss: 0.10313451290130615
Batch 29/64 loss: 0.128373384475708
Batch 30/64 loss: 0.14194250106811523
Batch 31/64 loss: 0.09352266788482666
Batch 32/64 loss: 0.13357126712799072
Batch 33/64 loss: 0.1323150396347046
Batch 34/64 loss: 0.09675455093383789
Batch 35/64 loss: 0.1058465838432312
Batch 36/64 loss: 0.11844831705093384
Batch 37/64 loss: 0.10975521802902222
Batch 38/64 loss: 0.10124588012695312
Batch 39/64 loss: 0.12598061561584473
Batch 40/64 loss: 0.1318732500076294
Batch 41/64 loss: 0.1080317497253418
Batch 42/64 loss: 0.1302744746208191
Batch 43/64 loss: 0.10518002510070801
Batch 44/64 loss: 0.12169969081878662
Batch 45/64 loss: 0.12488597631454468
Batch 46/64 loss: 0.11484068632125854
Batch 47/64 loss: 0.1090509295463562
Batch 48/64 loss: 0.11175322532653809
Batch 49/64 loss: 0.11669224500656128
Batch 50/64 loss: 0.13766705989837646
Batch 51/64 loss: 0.1063196063041687
Batch 52/64 loss: 0.10748952627182007
Batch 53/64 loss: 0.11019372940063477
Batch 54/64 loss: 0.11123025417327881
Batch 55/64 loss: 0.13068556785583496
Batch 56/64 loss: 0.11942470073699951
Batch 57/64 loss: 0.11634939908981323
Batch 58/64 loss: 0.09353184700012207
Batch 59/64 loss: 0.1193244457244873
Batch 60/64 loss: 0.1184391975402832
Batch 61/64 loss: 0.12783700227737427
Batch 62/64 loss: 0.09217822551727295
Batch 63/64 loss: 0.11469191312789917
Batch 64/64 loss: 0.09257632493972778
Epoch 35  Train loss: 0.11547842095879947  Val loss: 0.14328882968712509
Epoch 36
-------------------------------
Batch 1/64 loss: 0.11664390563964844
Batch 2/64 loss: 0.12481606006622314
Batch 3/64 loss: 0.07837420701980591
Batch 4/64 loss: 0.09160709381103516
Batch 5/64 loss: 0.10931587219238281
Batch 6/64 loss: 0.10811364650726318
Batch 7/64 loss: 0.14300787448883057
Batch 8/64 loss: 0.07879489660263062
Batch 9/64 loss: 0.1091189980506897
Batch 10/64 loss: 0.12933588027954102
Batch 11/64 loss: 0.1329495906829834
Batch 12/64 loss: 0.12429112195968628
Batch 13/64 loss: 0.11860895156860352
Batch 14/64 loss: 0.11397624015808105
Batch 15/64 loss: 0.10433226823806763
Batch 16/64 loss: 0.12482738494873047
Batch 17/64 loss: 0.1231570839881897
Batch 18/64 loss: 0.11134076118469238
Batch 19/64 loss: 0.09940379858016968
Batch 20/64 loss: 0.09701216220855713
Batch 21/64 loss: 0.11180377006530762
Batch 22/64 loss: 0.11803144216537476
Batch 23/64 loss: 0.12192535400390625
Batch 24/64 loss: 0.10547137260437012
Batch 25/64 loss: 0.12659496068954468
Batch 26/64 loss: 0.09533178806304932
Batch 27/64 loss: 0.09773916006088257
Batch 28/64 loss: 0.12165701389312744
Batch 29/64 loss: 0.14493519067764282
Batch 30/64 loss: 0.10402315855026245
Batch 31/64 loss: 0.10103988647460938
Batch 32/64 loss: 0.09097427129745483
Batch 33/64 loss: 0.09281116724014282
Batch 34/64 loss: 0.09289658069610596
Batch 35/64 loss: 0.13746535778045654
Batch 36/64 loss: 0.12830162048339844
Batch 37/64 loss: 0.11286437511444092
Batch 38/64 loss: 0.1379460096359253
Batch 39/64 loss: 0.1062624454498291
Batch 40/64 loss: 0.12054729461669922
Batch 41/64 loss: 0.11175620555877686
Batch 42/64 loss: 0.12635594606399536
Batch 43/64 loss: 0.09671807289123535
Batch 44/64 loss: 0.11132985353469849
Batch 45/64 loss: 0.1122702956199646
Batch 46/64 loss: 0.11175501346588135
Batch 47/64 loss: 0.10716933012008667
Batch 48/64 loss: 0.10033535957336426
Batch 49/64 loss: 0.115028977394104
Batch 50/64 loss: 0.11485499143600464
Batch 51/64 loss: 0.12054365873336792
Batch 52/64 loss: 0.09447270631790161
Batch 53/64 loss: 0.11264139413833618
Batch 54/64 loss: 0.09914803504943848
Batch 55/64 loss: 0.10583597421646118
Batch 56/64 loss: 0.11214476823806763
Batch 57/64 loss: 0.11409163475036621
Batch 58/64 loss: 0.11980390548706055
Batch 59/64 loss: 0.12577056884765625
Batch 60/64 loss: 0.11085987091064453
Batch 61/64 loss: 0.1306818127632141
Batch 62/64 loss: 0.10695725679397583
Batch 63/64 loss: 0.12490785121917725
Batch 64/64 loss: 0.16185927391052246
Epoch 36  Train loss: 0.11316828166737276  Val loss: 0.1451569256913621
Epoch 37
-------------------------------
Batch 1/64 loss: 0.10178321599960327
Batch 2/64 loss: 0.12059992551803589
Batch 3/64 loss: 0.10307735204696655
Batch 4/64 loss: 0.08912217617034912
Batch 5/64 loss: 0.12069433927536011
Batch 6/64 loss: 0.12675082683563232
Batch 7/64 loss: 0.10672754049301147
Batch 8/64 loss: 0.13555264472961426
Batch 9/64 loss: 0.10030913352966309
Batch 10/64 loss: 0.13219976425170898
Batch 11/64 loss: 0.09983205795288086
Batch 12/64 loss: 0.12308776378631592
Batch 13/64 loss: 0.11726868152618408
Batch 14/64 loss: 0.11350655555725098
Batch 15/64 loss: 0.12164962291717529
Batch 16/64 loss: 0.10443824529647827
Batch 17/64 loss: 0.09579157829284668
Batch 18/64 loss: 0.13371354341506958
Batch 19/64 loss: 0.12059783935546875
Batch 20/64 loss: 0.10359185934066772
Batch 21/64 loss: 0.11917203664779663
Batch 22/64 loss: 0.11406981945037842
Batch 23/64 loss: 0.10801708698272705
Batch 24/64 loss: 0.11859250068664551
Batch 25/64 loss: 0.09695649147033691
Batch 26/64 loss: 0.10774338245391846
Batch 27/64 loss: 0.11627036333084106
Batch 28/64 loss: 0.1021004319190979
Batch 29/64 loss: 0.11935943365097046
Batch 30/64 loss: 0.1087692379951477
Batch 31/64 loss: 0.11817562580108643
Batch 32/64 loss: 0.13988357782363892
Batch 33/64 loss: 0.12102168798446655
Batch 34/64 loss: 0.10101151466369629
Batch 35/64 loss: 0.08118271827697754
Batch 36/64 loss: 0.10820251703262329
Batch 37/64 loss: 0.11271500587463379
Batch 38/64 loss: 0.11040449142456055
Batch 39/64 loss: 0.11695140600204468
Batch 40/64 loss: 0.10737168788909912
Batch 41/64 loss: 0.11451387405395508
Batch 42/64 loss: 0.10639482736587524
Batch 43/64 loss: 0.1090548038482666
Batch 44/64 loss: 0.10743099451065063
Batch 45/64 loss: 0.09928065538406372
Batch 46/64 loss: 0.1280631422996521
Batch 47/64 loss: 0.09972244501113892
Batch 48/64 loss: 0.10343784093856812
Batch 49/64 loss: 0.11219966411590576
Batch 50/64 loss: 0.11973172426223755
Batch 51/64 loss: 0.11047786474227905
Batch 52/64 loss: 0.09970653057098389
Batch 53/64 loss: 0.10282981395721436
Batch 54/64 loss: 0.10384571552276611
Batch 55/64 loss: 0.11487531661987305
Batch 56/64 loss: 0.126572847366333
Batch 57/64 loss: 0.11865323781967163
Batch 58/64 loss: 0.09963005781173706
Batch 59/64 loss: 0.11349338293075562
Batch 60/64 loss: 0.12279552221298218
Batch 61/64 loss: 0.13844168186187744
Batch 62/64 loss: 0.11272478103637695
Batch 63/64 loss: 0.11986428499221802
Batch 64/64 loss: 0.11555999517440796
Epoch 37  Train loss: 0.112449830419877  Val loss: 0.1525072225999996
Epoch 38
-------------------------------
Batch 1/64 loss: 0.12781625986099243
Batch 2/64 loss: 0.11801224946975708
Batch 3/64 loss: 0.11546623706817627
Batch 4/64 loss: 0.12657976150512695
Batch 5/64 loss: 0.08700555562973022
Batch 6/64 loss: 0.10593819618225098
Batch 7/64 loss: 0.10842788219451904
Batch 8/64 loss: 0.09678530693054199
Batch 9/64 loss: 0.10697925090789795
Batch 10/64 loss: 0.12553685903549194
Batch 11/64 loss: 0.11685001850128174
Batch 12/64 loss: 0.12423491477966309
Batch 13/64 loss: 0.0874943733215332
Batch 14/64 loss: 0.12725114822387695
Batch 15/64 loss: 0.08810174465179443
Batch 16/64 loss: 0.11959719657897949
Batch 17/64 loss: 0.12499713897705078
Batch 18/64 loss: 0.13498657941818237
Batch 19/64 loss: 0.10742807388305664
Batch 20/64 loss: 0.11005783081054688
Batch 21/64 loss: 0.09305775165557861
Batch 22/64 loss: 0.1077110767364502
Batch 23/64 loss: 0.10896182060241699
Batch 24/64 loss: 0.08125078678131104
Batch 25/64 loss: 0.1067584753036499
Batch 26/64 loss: 0.12066274881362915
Batch 27/64 loss: 0.12820827960968018
Batch 28/64 loss: 0.12610220909118652
Batch 29/64 loss: 0.11433011293411255
Batch 30/64 loss: 0.09509134292602539
Batch 31/64 loss: 0.11531311273574829
Batch 32/64 loss: 0.09304296970367432
Batch 33/64 loss: 0.08886229991912842
Batch 34/64 loss: 0.128301739692688
Batch 35/64 loss: 0.08602690696716309
Batch 36/64 loss: 0.09972584247589111
Batch 37/64 loss: 0.11187112331390381
Batch 38/64 loss: 0.11715847253799438
Batch 39/64 loss: 0.1257813572883606
Batch 40/64 loss: 0.10251712799072266
Batch 41/64 loss: 0.11585551500320435
Batch 42/64 loss: 0.13087129592895508
Batch 43/64 loss: 0.10618460178375244
Batch 44/64 loss: 0.10413849353790283
Batch 45/64 loss: 0.11255908012390137
Batch 46/64 loss: 0.09096688032150269
Batch 47/64 loss: 0.10887360572814941
Batch 48/64 loss: 0.10673433542251587
Batch 49/64 loss: 0.09487271308898926
Batch 50/64 loss: 0.09319549798965454
Batch 51/64 loss: 0.12093514204025269
Batch 52/64 loss: 0.10252726078033447
Batch 53/64 loss: 0.12033700942993164
Batch 54/64 loss: 0.09488892555236816
Batch 55/64 loss: 0.12103843688964844
Batch 56/64 loss: 0.11386924982070923
Batch 57/64 loss: 0.10349440574645996
Batch 58/64 loss: 0.09331011772155762
Batch 59/64 loss: 0.11759275197982788
Batch 60/64 loss: 0.11133801937103271
Batch 61/64 loss: 0.104664146900177
Batch 62/64 loss: 0.08393537998199463
Batch 63/64 loss: 0.10525751113891602
Batch 64/64 loss: 0.10625267028808594
Epoch 38  Train loss: 0.10897901198443244  Val loss: 0.13071227053186737
Saving best model, epoch: 38
Epoch 39
-------------------------------
Batch 1/64 loss: 0.1117783784866333
Batch 2/64 loss: 0.12031644582748413
Batch 3/64 loss: 0.09154051542282104
Batch 4/64 loss: 0.10310417413711548
Batch 5/64 loss: 0.10098069906234741
Batch 6/64 loss: 0.07549518346786499
Batch 7/64 loss: 0.10904419422149658
Batch 8/64 loss: 0.09512412548065186
Batch 9/64 loss: 0.1299905776977539
Batch 10/64 loss: 0.09392380714416504
Batch 11/64 loss: 0.09879541397094727
Batch 12/64 loss: 0.11587262153625488
Batch 13/64 loss: 0.09998583793640137
Batch 14/64 loss: 0.1088641881942749
Batch 15/64 loss: 0.10178941488265991
Batch 16/64 loss: 0.09313327074050903
Batch 17/64 loss: 0.10649728775024414
Batch 18/64 loss: 0.10260820388793945
Batch 19/64 loss: 0.09341943264007568
Batch 20/64 loss: 0.10382908582687378
Batch 21/64 loss: 0.11346435546875
Batch 22/64 loss: 0.10793524980545044
Batch 23/64 loss: 0.13296520709991455
Batch 24/64 loss: 0.10877865552902222
Batch 25/64 loss: 0.144764244556427
Batch 26/64 loss: 0.1137591004371643
Batch 27/64 loss: 0.0953400731086731
Batch 28/64 loss: 0.10207599401473999
Batch 29/64 loss: 0.10965651273727417
Batch 30/64 loss: 0.1249622106552124
Batch 31/64 loss: 0.12168151140213013
Batch 32/64 loss: 0.07829320430755615
Batch 33/64 loss: 0.11666733026504517
Batch 34/64 loss: 0.12373125553131104
Batch 35/64 loss: 0.10181128978729248
Batch 36/64 loss: 0.11578881740570068
Batch 37/64 loss: 0.09499549865722656
Batch 38/64 loss: 0.09258139133453369
Batch 39/64 loss: 0.084983229637146
Batch 40/64 loss: 0.11797237396240234
Batch 41/64 loss: 0.09852355718612671
Batch 42/64 loss: 0.11944597959518433
Batch 43/64 loss: 0.0901913046836853
Batch 44/64 loss: 0.12830257415771484
Batch 45/64 loss: 0.10907721519470215
Batch 46/64 loss: 0.12470483779907227
Batch 47/64 loss: 0.10913002490997314
Batch 48/64 loss: 0.0934518575668335
Batch 49/64 loss: 0.10682845115661621
Batch 50/64 loss: 0.09913581609725952
Batch 51/64 loss: 0.08650755882263184
Batch 52/64 loss: 0.12301135063171387
Batch 53/64 loss: 0.08333384990692139
Batch 54/64 loss: 0.08910471200942993
Batch 55/64 loss: 0.10522699356079102
Batch 56/64 loss: 0.09988677501678467
Batch 57/64 loss: 0.09107917547225952
Batch 58/64 loss: 0.0956684947013855
Batch 59/64 loss: 0.10247516632080078
Batch 60/64 loss: 0.08091527223587036
Batch 61/64 loss: 0.12094181776046753
Batch 62/64 loss: 0.10057395696640015
Batch 63/64 loss: 0.09542477130889893
Batch 64/64 loss: 0.09657526016235352
Epoch 39  Train loss: 0.10484193446589452  Val loss: 0.12776763701356972
Saving best model, epoch: 39
Epoch 40
-------------------------------
Batch 1/64 loss: 0.10455828905105591
Batch 2/64 loss: 0.0627705454826355
Batch 3/64 loss: 0.09975647926330566
Batch 4/64 loss: 0.10052955150604248
Batch 5/64 loss: 0.09494352340698242
Batch 6/64 loss: 0.08449280261993408
Batch 7/64 loss: 0.12046873569488525
Batch 8/64 loss: 0.13492155075073242
Batch 9/64 loss: 0.1333438754081726
Batch 10/64 loss: 0.09124457836151123
Batch 11/64 loss: 0.09522360563278198
Batch 12/64 loss: 0.10963600873947144
Batch 13/64 loss: 0.10082244873046875
Batch 14/64 loss: 0.10450130701065063
Batch 15/64 loss: 0.10406899452209473
Batch 16/64 loss: 0.08796936273574829
Batch 17/64 loss: 0.10933548212051392
Batch 18/64 loss: 0.08781105279922485
Batch 19/64 loss: 0.11760103702545166
Batch 20/64 loss: 0.13119232654571533
Batch 21/64 loss: 0.10687029361724854
Batch 22/64 loss: 0.1144556999206543
Batch 23/64 loss: 0.10673820972442627
Batch 24/64 loss: 0.10345399379730225
Batch 25/64 loss: 0.07520103454589844
Batch 26/64 loss: 0.09549200534820557
Batch 27/64 loss: 0.09334564208984375
Batch 28/64 loss: 0.10781443119049072
Batch 29/64 loss: 0.12055933475494385
Batch 30/64 loss: 0.09721261262893677
Batch 31/64 loss: 0.1088058352470398
Batch 32/64 loss: 0.10551518201828003
Batch 33/64 loss: 0.11963444948196411
Batch 34/64 loss: 0.07498669624328613
Batch 35/64 loss: 0.10181492567062378
Batch 36/64 loss: 0.09753209352493286
Batch 37/64 loss: 0.111966073513031
Batch 38/64 loss: 0.1044541597366333
Batch 39/64 loss: 0.10053902864456177
Batch 40/64 loss: 0.11734676361083984
Batch 41/64 loss: 0.11142343282699585
Batch 42/64 loss: 0.09924072027206421
Batch 43/64 loss: 0.08545142412185669
Batch 44/64 loss: 0.10950994491577148
Batch 45/64 loss: 0.08781653642654419
Batch 46/64 loss: 0.10553818941116333
Batch 47/64 loss: 0.10252445936203003
Batch 48/64 loss: 0.10246312618255615
Batch 49/64 loss: 0.11145079135894775
Batch 50/64 loss: 0.12143921852111816
Batch 51/64 loss: 0.10725373029708862
Batch 52/64 loss: 0.10038620233535767
Batch 53/64 loss: 0.08572840690612793
Batch 54/64 loss: 0.09597349166870117
Batch 55/64 loss: 0.11091077327728271
Batch 56/64 loss: 0.12352460622787476
Batch 57/64 loss: 0.09290385246276855
Batch 58/64 loss: 0.10787302255630493
Batch 59/64 loss: 0.11517369747161865
Batch 60/64 loss: 0.10825443267822266
Batch 61/64 loss: 0.1142687201499939
Batch 62/64 loss: 0.10017186403274536
Batch 63/64 loss: 0.09522908926010132
Batch 64/64 loss: 0.095272958278656
Epoch 40  Train loss: 0.10360618782978431  Val loss: 0.12828598153550191
Epoch 41
-------------------------------
Batch 1/64 loss: 0.06535285711288452
Batch 2/64 loss: 0.10530030727386475
Batch 3/64 loss: 0.11310946941375732
Batch 4/64 loss: 0.09850168228149414
Batch 5/64 loss: 0.12208616733551025
Batch 6/64 loss: 0.08011454343795776
Batch 7/64 loss: 0.11286520957946777
Batch 8/64 loss: 0.08908921480178833
Batch 9/64 loss: 0.10153472423553467
Batch 10/64 loss: 0.11867934465408325
Batch 11/64 loss: 0.10241687297821045
Batch 12/64 loss: 0.10817217826843262
Batch 13/64 loss: 0.1056894063949585
Batch 14/64 loss: 0.11801153421401978
Batch 15/64 loss: 0.08540737628936768
Batch 16/64 loss: 0.09645992517471313
Batch 17/64 loss: 0.09374606609344482
Batch 18/64 loss: 0.09632658958435059
Batch 19/64 loss: 0.08484452962875366
Batch 20/64 loss: 0.1069294810295105
Batch 21/64 loss: 0.06554728746414185
Batch 22/64 loss: 0.09327155351638794
Batch 23/64 loss: 0.10453420877456665
Batch 24/64 loss: 0.12483346462249756
Batch 25/64 loss: 0.09529203176498413
Batch 26/64 loss: 0.09458017349243164
Batch 27/64 loss: 0.10709774494171143
Batch 28/64 loss: 0.09798228740692139
Batch 29/64 loss: 0.11348605155944824
Batch 30/64 loss: 0.1190367341041565
Batch 31/64 loss: 0.09706616401672363
Batch 32/64 loss: 0.11671948432922363
Batch 33/64 loss: 0.10657978057861328
Batch 34/64 loss: 0.10904335975646973
Batch 35/64 loss: 0.11522024869918823
Batch 36/64 loss: 0.10434675216674805
Batch 37/64 loss: 0.10299843549728394
Batch 38/64 loss: 0.11469405889511108
Batch 39/64 loss: 0.11124694347381592
Batch 40/64 loss: 0.12205898761749268
Batch 41/64 loss: 0.10280400514602661
Batch 42/64 loss: 0.10761404037475586
Batch 43/64 loss: 0.09568464756011963
Batch 44/64 loss: 0.09017789363861084
Batch 45/64 loss: 0.11424744129180908
Batch 46/64 loss: 0.10950273275375366
Batch 47/64 loss: 0.13028699159622192
Batch 48/64 loss: 0.10180294513702393
Batch 49/64 loss: 0.09278452396392822
Batch 50/64 loss: 0.08175450563430786
Batch 51/64 loss: 0.0909777283668518
Batch 52/64 loss: 0.07688570022583008
Batch 53/64 loss: 0.10449552536010742
Batch 54/64 loss: 0.10397303104400635
Batch 55/64 loss: 0.10228097438812256
Batch 56/64 loss: 0.11287575960159302
Batch 57/64 loss: 0.11124587059020996
Batch 58/64 loss: 0.10847675800323486
Batch 59/64 loss: 0.10932052135467529
Batch 60/64 loss: 0.08174824714660645
Batch 61/64 loss: 0.09175264835357666
Batch 62/64 loss: 0.10846024751663208
Batch 63/64 loss: 0.07822096347808838
Batch 64/64 loss: 0.07627081871032715
Epoch 41  Train loss: 0.10172313802382525  Val loss: 0.12516052145318887
Saving best model, epoch: 41
Epoch 42
-------------------------------
Batch 1/64 loss: 0.11763769388198853
Batch 2/64 loss: 0.07552409172058105
Batch 3/64 loss: 0.09946674108505249
Batch 4/64 loss: 0.10157990455627441
Batch 5/64 loss: 0.09793061017990112
Batch 6/64 loss: 0.09444725513458252
Batch 7/64 loss: 0.08961033821105957
Batch 8/64 loss: 0.08734464645385742
Batch 9/64 loss: 0.06789380311965942
Batch 10/64 loss: 0.09687173366546631
Batch 11/64 loss: 0.10958153009414673
Batch 12/64 loss: 0.07702058553695679
Batch 13/64 loss: 0.1286056637763977
Batch 14/64 loss: 0.10185849666595459
Batch 15/64 loss: 0.11385530233383179
Batch 16/64 loss: 0.12107890844345093
Batch 17/64 loss: 0.07767164707183838
Batch 18/64 loss: 0.09535938501358032
Batch 19/64 loss: 0.12300342321395874
Batch 20/64 loss: 0.11412137746810913
Batch 21/64 loss: 0.09320014715194702
Batch 22/64 loss: 0.09492099285125732
Batch 23/64 loss: 0.10576647520065308
Batch 24/64 loss: 0.08871346712112427
Batch 25/64 loss: 0.06572163105010986
Batch 26/64 loss: 0.11558079719543457
Batch 27/64 loss: 0.10223335027694702
Batch 28/64 loss: 0.08276891708374023
Batch 29/64 loss: 0.10393750667572021
Batch 30/64 loss: 0.11376285552978516
Batch 31/64 loss: 0.08887976408004761
Batch 32/64 loss: 0.1168472170829773
Batch 33/64 loss: 0.10032665729522705
Batch 34/64 loss: 0.10580319166183472
Batch 35/64 loss: 0.09976798295974731
Batch 36/64 loss: 0.12174654006958008
Batch 37/64 loss: 0.0874066948890686
Batch 38/64 loss: 0.11298561096191406
Batch 39/64 loss: 0.08891397714614868
Batch 40/64 loss: 0.11294889450073242
Batch 41/64 loss: 0.0747520923614502
Batch 42/64 loss: 0.10083979368209839
Batch 43/64 loss: 0.091846764087677
Batch 44/64 loss: 0.09371340274810791
Batch 45/64 loss: 0.10443323850631714
Batch 46/64 loss: 0.09411287307739258
Batch 47/64 loss: 0.07621854543685913
Batch 48/64 loss: 0.08851099014282227
Batch 49/64 loss: 0.10621404647827148
Batch 50/64 loss: 0.08555501699447632
Batch 51/64 loss: 0.11190325021743774
Batch 52/64 loss: 0.10078716278076172
Batch 53/64 loss: 0.09431284666061401
Batch 54/64 loss: 0.0950194001197815
Batch 55/64 loss: 0.08726716041564941
Batch 56/64 loss: 0.10151147842407227
Batch 57/64 loss: 0.1037026047706604
Batch 58/64 loss: 0.1021079421043396
Batch 59/64 loss: 0.0914301872253418
Batch 60/64 loss: 0.09965074062347412
Batch 61/64 loss: 0.08730745315551758
Batch 62/64 loss: 0.10098755359649658
Batch 63/64 loss: 0.10757476091384888
Batch 64/64 loss: 0.09983807802200317
Epoch 42  Train loss: 0.09831111641491161  Val loss: 0.12108788129799965
Saving best model, epoch: 42
Epoch 43
-------------------------------
Batch 1/64 loss: 0.10600638389587402
Batch 2/64 loss: 0.0914275050163269
Batch 3/64 loss: 0.0913621187210083
Batch 4/64 loss: 0.10259628295898438
Batch 5/64 loss: 0.08072996139526367
Batch 6/64 loss: 0.13007867336273193
Batch 7/64 loss: 0.07649070024490356
Batch 8/64 loss: 0.10796141624450684
Batch 9/64 loss: 0.08319211006164551
Batch 10/64 loss: 0.09957456588745117
Batch 11/64 loss: 0.10367006063461304
Batch 12/64 loss: 0.08081352710723877
Batch 13/64 loss: 0.11042928695678711
Batch 14/64 loss: 0.10068398714065552
Batch 15/64 loss: 0.08514100313186646
Batch 16/64 loss: 0.09554684162139893
Batch 17/64 loss: 0.08962750434875488
Batch 18/64 loss: 0.07909804582595825
Batch 19/64 loss: 0.09315288066864014
Batch 20/64 loss: 0.09130817651748657
Batch 21/64 loss: 0.08633077144622803
Batch 22/64 loss: 0.08289575576782227
Batch 23/64 loss: 0.0803447961807251
Batch 24/64 loss: 0.08432847261428833
Batch 25/64 loss: 0.09287667274475098
Batch 26/64 loss: 0.08827930688858032
Batch 27/64 loss: 0.08833813667297363
Batch 28/64 loss: 0.10235333442687988
Batch 29/64 loss: 0.08113092184066772
Batch 30/64 loss: 0.13089978694915771
Batch 31/64 loss: 0.10350555181503296
Batch 32/64 loss: 0.07747316360473633
Batch 33/64 loss: 0.10856068134307861
Batch 34/64 loss: 0.08226180076599121
Batch 35/64 loss: 0.10770118236541748
Batch 36/64 loss: 0.07806044816970825
Batch 37/64 loss: 0.1093975305557251
Batch 38/64 loss: 0.08966171741485596
Batch 39/64 loss: 0.07146334648132324
Batch 40/64 loss: 0.08200347423553467
Batch 41/64 loss: 0.08043664693832397
Batch 42/64 loss: 0.11010861396789551
Batch 43/64 loss: 0.09099721908569336
Batch 44/64 loss: 0.09261184930801392
Batch 45/64 loss: 0.0923314094543457
Batch 46/64 loss: 0.0932990312576294
Batch 47/64 loss: 0.09338641166687012
Batch 48/64 loss: 0.09949827194213867
Batch 49/64 loss: 0.10184890031814575
Batch 50/64 loss: 0.10713541507720947
Batch 51/64 loss: 0.10644161701202393
Batch 52/64 loss: 0.1080595850944519
Batch 53/64 loss: 0.11294466257095337
Batch 54/64 loss: 0.09501755237579346
Batch 55/64 loss: 0.12069451808929443
Batch 56/64 loss: 0.12212896347045898
Batch 57/64 loss: 0.11649340391159058
Batch 58/64 loss: 0.0804629921913147
Batch 59/64 loss: 0.11248695850372314
Batch 60/64 loss: 0.1118323802947998
Batch 61/64 loss: 0.0908660888671875
Batch 62/64 loss: 0.1128278374671936
Batch 63/64 loss: 0.11588037014007568
Batch 64/64 loss: 0.0982198715209961
Epoch 43  Train loss: 0.0967562899870031  Val loss: 0.1322255093617128
Epoch 44
-------------------------------
Batch 1/64 loss: 0.1262645721435547
Batch 2/64 loss: 0.11336839199066162
Batch 3/64 loss: 0.1024286150932312
Batch 4/64 loss: 0.11810922622680664
Batch 5/64 loss: 0.08551108837127686
Batch 6/64 loss: 0.09113442897796631
Batch 7/64 loss: 0.076931893825531
Batch 8/64 loss: 0.09972155094146729
Batch 9/64 loss: 0.06949031352996826
Batch 10/64 loss: 0.0969739556312561
Batch 11/64 loss: 0.09148228168487549
Batch 12/64 loss: 0.08708137273788452
Batch 13/64 loss: 0.09671652317047119
Batch 14/64 loss: 0.09807074069976807
Batch 15/64 loss: 0.09245014190673828
Batch 16/64 loss: 0.09988182783126831
Batch 17/64 loss: 0.08198267221450806
Batch 18/64 loss: 0.10456889867782593
Batch 19/64 loss: 0.1016547679901123
Batch 20/64 loss: 0.09449684619903564
Batch 21/64 loss: 0.10443627834320068
Batch 22/64 loss: 0.11407506465911865
Batch 23/64 loss: 0.09209197759628296
Batch 24/64 loss: 0.11266618967056274
Batch 25/64 loss: 0.07861852645874023
Batch 26/64 loss: 0.08589434623718262
Batch 27/64 loss: 0.1124160885810852
Batch 28/64 loss: 0.09342211484909058
Batch 29/64 loss: 0.09337210655212402
Batch 30/64 loss: 0.08891934156417847
Batch 31/64 loss: 0.10184520483016968
Batch 32/64 loss: 0.0874212384223938
Batch 33/64 loss: 0.11350780725479126
Batch 34/64 loss: 0.09784114360809326
Batch 35/64 loss: 0.0557863712310791
Batch 36/64 loss: 0.08176416158676147
Batch 37/64 loss: 0.09620380401611328
Batch 38/64 loss: 0.1000748872756958
Batch 39/64 loss: 0.06568944454193115
Batch 40/64 loss: 0.09871023893356323
Batch 41/64 loss: 0.1017383337020874
Batch 42/64 loss: 0.08074814081192017
Batch 43/64 loss: 0.09558230638504028
Batch 44/64 loss: 0.07588505744934082
Batch 45/64 loss: 0.08811730146408081
Batch 46/64 loss: 0.0930970311164856
Batch 47/64 loss: 0.08266997337341309
Batch 48/64 loss: 0.08254861831665039
Batch 49/64 loss: 0.09912019968032837
Batch 50/64 loss: 0.0794215202331543
Batch 51/64 loss: 0.08306723833084106
Batch 52/64 loss: 0.10767602920532227
Batch 53/64 loss: 0.11394459009170532
Batch 54/64 loss: 0.105815589427948
Batch 55/64 loss: 0.12238603830337524
Batch 56/64 loss: 0.11505347490310669
Batch 57/64 loss: 0.07532519102096558
Batch 58/64 loss: 0.09307760000228882
Batch 59/64 loss: 0.09587007761001587
Batch 60/64 loss: 0.1069861650466919
Batch 61/64 loss: 0.07756108045578003
Batch 62/64 loss: 0.1311388611793518
Batch 63/64 loss: 0.10667431354522705
Batch 64/64 loss: 0.11891824007034302
Epoch 44  Train loss: 0.09577678208257638  Val loss: 0.1439048883431556
Epoch 45
-------------------------------
Batch 1/64 loss: 0.0895012617111206
Batch 2/64 loss: 0.0712507963180542
Batch 3/64 loss: 0.11048352718353271
Batch 4/64 loss: 0.10413753986358643
Batch 5/64 loss: 0.0917426347732544
Batch 6/64 loss: 0.09103244543075562
Batch 7/64 loss: 0.06260907649993896
Batch 8/64 loss: 0.08311992883682251
Batch 9/64 loss: 0.08999514579772949
Batch 10/64 loss: 0.11592388153076172
Batch 11/64 loss: 0.10368317365646362
Batch 12/64 loss: 0.10629695653915405
Batch 13/64 loss: 0.06467074155807495
Batch 14/64 loss: 0.11126083135604858
Batch 15/64 loss: 0.10438448190689087
Batch 16/64 loss: 0.09712529182434082
Batch 17/64 loss: 0.0967530608177185
Batch 18/64 loss: 0.09527933597564697
Batch 19/64 loss: 0.09064555168151855
Batch 20/64 loss: 0.09483593702316284
Batch 21/64 loss: 0.11481952667236328
Batch 22/64 loss: 0.11132258176803589
Batch 23/64 loss: 0.06904482841491699
Batch 24/64 loss: 0.09273332357406616
Batch 25/64 loss: 0.08596587181091309
Batch 26/64 loss: 0.0960763692855835
Batch 27/64 loss: 0.09818124771118164
Batch 28/64 loss: 0.10019147396087646
Batch 29/64 loss: 0.08629626035690308
Batch 30/64 loss: 0.13335645198822021
Batch 31/64 loss: 0.09699434041976929
Batch 32/64 loss: 0.06617462635040283
Batch 33/64 loss: 0.08072006702423096
Batch 34/64 loss: 0.12381905317306519
Batch 35/64 loss: 0.09917140007019043
Batch 36/64 loss: 0.06778043508529663
Batch 37/64 loss: 0.09278631210327148
Batch 38/64 loss: 0.07717150449752808
Batch 39/64 loss: 0.07076650857925415
Batch 40/64 loss: 0.09541016817092896
Batch 41/64 loss: 0.07919842004776001
Batch 42/64 loss: 0.07982075214385986
Batch 43/64 loss: 0.10287058353424072
Batch 44/64 loss: 0.11416679620742798
Batch 45/64 loss: 0.11062902212142944
Batch 46/64 loss: 0.08974254131317139
Batch 47/64 loss: 0.06694275140762329
Batch 48/64 loss: 0.0971231460571289
Batch 49/64 loss: 0.07442599534988403
Batch 50/64 loss: 0.08305621147155762
Batch 51/64 loss: 0.08080083131790161
Batch 52/64 loss: 0.09047019481658936
Batch 53/64 loss: 0.09672892093658447
Batch 54/64 loss: 0.10451453924179077
Batch 55/64 loss: 0.07959151268005371
Batch 56/64 loss: 0.08980631828308105
Batch 57/64 loss: 0.10445982217788696
Batch 58/64 loss: 0.10827267169952393
Batch 59/64 loss: 0.10890358686447144
Batch 60/64 loss: 0.08584672212600708
Batch 61/64 loss: 0.09991788864135742
Batch 62/64 loss: 0.09973084926605225
Batch 63/64 loss: 0.07845079898834229
Batch 64/64 loss: 0.09594619274139404
Epoch 45  Train loss: 0.09303442309884465  Val loss: 0.13316874569633982
Epoch 46
-------------------------------
Batch 1/64 loss: 0.06296426057815552
Batch 2/64 loss: 0.12875336408615112
Batch 3/64 loss: 0.08871203660964966
Batch 4/64 loss: 0.10531395673751831
Batch 5/64 loss: 0.09092956781387329
Batch 6/64 loss: 0.08457612991333008
Batch 7/64 loss: 0.10936951637268066
Batch 8/64 loss: 0.09040820598602295
Batch 9/64 loss: 0.07659691572189331
Batch 10/64 loss: 0.09267628192901611
Batch 11/64 loss: 0.08122968673706055
Batch 12/64 loss: 0.09848684072494507
Batch 13/64 loss: 0.0794217586517334
Batch 14/64 loss: 0.08956313133239746
Batch 15/64 loss: 0.09239673614501953
Batch 16/64 loss: 0.07325935363769531
Batch 17/64 loss: 0.08318322896957397
Batch 18/64 loss: 0.07735037803649902
Batch 19/64 loss: 0.08047294616699219
Batch 20/64 loss: 0.09011185169219971
Batch 21/64 loss: 0.09229272603988647
Batch 22/64 loss: 0.08213019371032715
Batch 23/64 loss: 0.11241894960403442
Batch 24/64 loss: 0.09533488750457764
Batch 25/64 loss: 0.11283260583877563
Batch 26/64 loss: 0.08560353517532349
Batch 27/64 loss: 0.09330803155899048
Batch 28/64 loss: 0.08736622333526611
Batch 29/64 loss: 0.08912461996078491
Batch 30/64 loss: 0.09504401683807373
Batch 31/64 loss: 0.08476197719573975
Batch 32/64 loss: 0.10364657640457153
Batch 33/64 loss: 0.07920873165130615
Batch 34/64 loss: 0.09163284301757812
Batch 35/64 loss: 0.09454697370529175
Batch 36/64 loss: 0.07856249809265137
Batch 37/64 loss: 0.09417867660522461
Batch 38/64 loss: 0.08456927537918091
Batch 39/64 loss: 0.09747165441513062
Batch 40/64 loss: 0.06764864921569824
Batch 41/64 loss: 0.11182940006256104
Batch 42/64 loss: 0.08366858959197998
Batch 43/64 loss: 0.08425372838973999
Batch 44/64 loss: 0.07272201776504517
Batch 45/64 loss: 0.09644579887390137
Batch 46/64 loss: 0.09311157464981079
Batch 47/64 loss: 0.09612154960632324
Batch 48/64 loss: 0.11143136024475098
Batch 49/64 loss: 0.08526664972305298
Batch 50/64 loss: 0.11874985694885254
Batch 51/64 loss: 0.12699639797210693
Batch 52/64 loss: 0.08442807197570801
Batch 53/64 loss: 0.12352389097213745
Batch 54/64 loss: 0.10777610540390015
Batch 55/64 loss: 0.09891355037689209
Batch 56/64 loss: 0.09312093257904053
Batch 57/64 loss: 0.09559214115142822
Batch 58/64 loss: 0.09662449359893799
Batch 59/64 loss: 0.09113693237304688
Batch 60/64 loss: 0.09321004152297974
Batch 61/64 loss: 0.11425507068634033
Batch 62/64 loss: 0.08242297172546387
Batch 63/64 loss: 0.11170697212219238
Batch 64/64 loss: 0.08563965559005737
Epoch 46  Train loss: 0.09309800208783617  Val loss: 0.13766992215028742
Epoch 47
-------------------------------
Batch 1/64 loss: 0.11457312107086182
Batch 2/64 loss: 0.08754980564117432
Batch 3/64 loss: 0.08690750598907471
Batch 4/64 loss: 0.09432238340377808
Batch 5/64 loss: 0.055935025215148926
Batch 6/64 loss: 0.08752167224884033
Batch 7/64 loss: 0.09491300582885742
Batch 8/64 loss: 0.10949724912643433
Batch 9/64 loss: 0.07180953025817871
Batch 10/64 loss: 0.1120215654373169
Batch 11/64 loss: 0.10442888736724854
Batch 12/64 loss: 0.09044796228408813
Batch 13/64 loss: 0.06942892074584961
Batch 14/64 loss: 0.1155623197555542
Batch 15/64 loss: 0.0873342752456665
Batch 16/64 loss: 0.11465728282928467
Batch 17/64 loss: 0.07676070928573608
Batch 18/64 loss: 0.10205656290054321
Batch 19/64 loss: 0.11548477411270142
Batch 20/64 loss: 0.10252159833908081
Batch 21/64 loss: 0.07741773128509521
Batch 22/64 loss: 0.08510535955429077
Batch 23/64 loss: 0.08662629127502441
Batch 24/64 loss: 0.09688615798950195
Batch 25/64 loss: 0.07503879070281982
Batch 26/64 loss: 0.09226053953170776
Batch 27/64 loss: 0.06748825311660767
Batch 28/64 loss: 0.09295892715454102
Batch 29/64 loss: 0.1064724326133728
Batch 30/64 loss: 0.08477616310119629
Batch 31/64 loss: 0.08713746070861816
Batch 32/64 loss: 0.09857356548309326
Batch 33/64 loss: 0.06274187564849854
Batch 34/64 loss: 0.08739101886749268
Batch 35/64 loss: 0.0963544249534607
Batch 36/64 loss: 0.08109545707702637
Batch 37/64 loss: 0.09848523139953613
Batch 38/64 loss: 0.08569198846817017
Batch 39/64 loss: 0.06052982807159424
Batch 40/64 loss: 0.11765038967132568
Batch 41/64 loss: 0.10004180669784546
Batch 42/64 loss: 0.07735347747802734
Batch 43/64 loss: 0.0944833755493164
Batch 44/64 loss: 0.10524529218673706
Batch 45/64 loss: 0.08593189716339111
Batch 46/64 loss: 0.09104526042938232
Batch 47/64 loss: 0.08656841516494751
Batch 48/64 loss: 0.08014786243438721
Batch 49/64 loss: 0.0901724100112915
Batch 50/64 loss: 0.07826817035675049
Batch 51/64 loss: 0.07894176244735718
Batch 52/64 loss: 0.10193806886672974
Batch 53/64 loss: 0.06562674045562744
Batch 54/64 loss: 0.07843548059463501
Batch 55/64 loss: 0.07592117786407471
Batch 56/64 loss: 0.0572279691696167
Batch 57/64 loss: 0.10681253671646118
Batch 58/64 loss: 0.08573257923126221
Batch 59/64 loss: 0.10507643222808838
Batch 60/64 loss: 0.0918160080909729
Batch 61/64 loss: 0.09385651350021362
Batch 62/64 loss: 0.11015063524246216
Batch 63/64 loss: 0.10981404781341553
Batch 64/64 loss: 0.10472273826599121
Epoch 47  Train loss: 0.09040887870040594  Val loss: 0.12846795164842376
Epoch 48
-------------------------------
Batch 1/64 loss: 0.0728495717048645
Batch 2/64 loss: 0.08994686603546143
Batch 3/64 loss: 0.07704031467437744
Batch 4/64 loss: 0.07351946830749512
Batch 5/64 loss: 0.08103305101394653
Batch 6/64 loss: 0.08295273780822754
Batch 7/64 loss: 0.0826001763343811
Batch 8/64 loss: 0.1087273359298706
Batch 9/64 loss: 0.09430265426635742
Batch 10/64 loss: 0.08211386203765869
Batch 11/64 loss: 0.08429139852523804
Batch 12/64 loss: 0.10206067562103271
Batch 13/64 loss: 0.09560668468475342
Batch 14/64 loss: 0.10143482685089111
Batch 15/64 loss: 0.11337649822235107
Batch 16/64 loss: 0.06047159433364868
Batch 17/64 loss: 0.08657181262969971
Batch 18/64 loss: 0.10856437683105469
Batch 19/64 loss: 0.07799559831619263
Batch 20/64 loss: 0.062030017375946045
Batch 21/64 loss: 0.07809913158416748
Batch 22/64 loss: 0.10837304592132568
Batch 23/64 loss: 0.11191272735595703
Batch 24/64 loss: 0.08084237575531006
Batch 25/64 loss: 0.10284531116485596
Batch 26/64 loss: 0.10218894481658936
Batch 27/64 loss: 0.09848880767822266
Batch 28/64 loss: 0.10519683361053467
Batch 29/64 loss: 0.12408077716827393
Batch 30/64 loss: 0.09034037590026855
Batch 31/64 loss: 0.08312582969665527
Batch 32/64 loss: 0.06579041481018066
Batch 33/64 loss: 0.07062530517578125
Batch 34/64 loss: 0.09422904253005981
Batch 35/64 loss: 0.10500073432922363
Batch 36/64 loss: 0.08410453796386719
Batch 37/64 loss: 0.08163052797317505
Batch 38/64 loss: 0.07907122373580933
Batch 39/64 loss: 0.08922159671783447
Batch 40/64 loss: 0.08111470937728882
Batch 41/64 loss: 0.0847928524017334
Batch 42/64 loss: 0.08965486288070679
Batch 43/64 loss: 0.10116952657699585
Batch 44/64 loss: 0.07083147764205933
Batch 45/64 loss: 0.07779669761657715
Batch 46/64 loss: 0.10305941104888916
Batch 47/64 loss: 0.09557902812957764
Batch 48/64 loss: 0.06149035692214966
Batch 49/64 loss: 0.09980762004852295
Batch 50/64 loss: 0.08309847116470337
Batch 51/64 loss: 0.05948317050933838
Batch 52/64 loss: 0.09861022233963013
Batch 53/64 loss: 0.07466018199920654
Batch 54/64 loss: 0.0928499698638916
Batch 55/64 loss: 0.10528218746185303
Batch 56/64 loss: 0.10831087827682495
Batch 57/64 loss: 0.08795881271362305
Batch 58/64 loss: 0.0839037299156189
Batch 59/64 loss: 0.07515233755111694
Batch 60/64 loss: 0.08770453929901123
Batch 61/64 loss: 0.07049071788787842
Batch 62/64 loss: 0.09420430660247803
Batch 63/64 loss: 0.07493305206298828
Batch 64/64 loss: 0.07735186815261841
Epoch 48  Train loss: 0.08804094253801832  Val loss: 0.12330142308756248
Epoch 49
-------------------------------
Batch 1/64 loss: 0.061511456966400146
Batch 2/64 loss: 0.11211490631103516
Batch 3/64 loss: 0.07258772850036621
Batch 4/64 loss: 0.06775873899459839
Batch 5/64 loss: 0.07802510261535645
Batch 6/64 loss: 0.08421081304550171
Batch 7/64 loss: 0.0903085470199585
Batch 8/64 loss: 0.0853375792503357
Batch 9/64 loss: 0.06759387254714966
Batch 10/64 loss: 0.0816308856010437
Batch 11/64 loss: 0.0984647274017334
Batch 12/64 loss: 0.07678329944610596
Batch 13/64 loss: 0.07215392589569092
Batch 14/64 loss: 0.06600040197372437
Batch 15/64 loss: 0.0771070122718811
Batch 16/64 loss: 0.09575235843658447
Batch 17/64 loss: 0.11513042449951172
Batch 18/64 loss: 0.07695209980010986
Batch 19/64 loss: 0.09128338098526001
Batch 20/64 loss: 0.09777218103408813
Batch 21/64 loss: 0.08032286167144775
Batch 22/64 loss: 0.09199678897857666
Batch 23/64 loss: 0.09253370761871338
Batch 24/64 loss: 0.08696794509887695
Batch 25/64 loss: 0.07530629634857178
Batch 26/64 loss: 0.08615130186080933
Batch 27/64 loss: 0.09019869565963745
Batch 28/64 loss: 0.08280909061431885
Batch 29/64 loss: 0.06468832492828369
Batch 30/64 loss: 0.07622909545898438
Batch 31/64 loss: 0.08359205722808838
Batch 32/64 loss: 0.07757365703582764
Batch 33/64 loss: 0.06431668996810913
Batch 34/64 loss: 0.08271223306655884
Batch 35/64 loss: 0.07559317350387573
Batch 36/64 loss: 0.10118794441223145
Batch 37/64 loss: 0.10343605279922485
Batch 38/64 loss: 0.07795637845993042
Batch 39/64 loss: 0.07858884334564209
Batch 40/64 loss: 0.10595804452896118
Batch 41/64 loss: 0.08754265308380127
Batch 42/64 loss: 0.10659003257751465
Batch 43/64 loss: 0.076404869556427
Batch 44/64 loss: 0.09611940383911133
Batch 45/64 loss: 0.08869659900665283
Batch 46/64 loss: 0.10897666215896606
Batch 47/64 loss: 0.10285866260528564
Batch 48/64 loss: 0.08910191059112549
Batch 49/64 loss: 0.07342183589935303
Batch 50/64 loss: 0.1125408411026001
Batch 51/64 loss: 0.08670681715011597
Batch 52/64 loss: 0.10573709011077881
Batch 53/64 loss: 0.10339641571044922
Batch 54/64 loss: 0.09220641851425171
Batch 55/64 loss: 0.09386420249938965
Batch 56/64 loss: 0.06845051050186157
Batch 57/64 loss: 0.08970504999160767
Batch 58/64 loss: 0.08271968364715576
Batch 59/64 loss: 0.07973361015319824
Batch 60/64 loss: 0.07734352350234985
Batch 61/64 loss: 0.08540487289428711
Batch 62/64 loss: 0.05491870641708374
Batch 63/64 loss: 0.0727120041847229
Batch 64/64 loss: 0.11324745416641235
Epoch 49  Train loss: 0.08575194653342752  Val loss: 0.11902731718476285
Saving best model, epoch: 49
Epoch 50
-------------------------------
Batch 1/64 loss: 0.09388041496276855
Batch 2/64 loss: 0.06345558166503906
Batch 3/64 loss: 0.08143585920333862
Batch 4/64 loss: 0.06527793407440186
Batch 5/64 loss: 0.09619992971420288
Batch 6/64 loss: 0.10200202465057373
Batch 7/64 loss: 0.08253610134124756
Batch 8/64 loss: 0.12063086032867432
Batch 9/64 loss: 0.09385234117507935
Batch 10/64 loss: 0.09724676609039307
Batch 11/64 loss: 0.09800034761428833
Batch 12/64 loss: 0.08215290307998657
Batch 13/64 loss: 0.07371139526367188
Batch 14/64 loss: 0.06356406211853027
Batch 15/64 loss: 0.07515019178390503
Batch 16/64 loss: 0.07623803615570068
Batch 17/64 loss: 0.07167446613311768
Batch 18/64 loss: 0.07664573192596436
Batch 19/64 loss: 0.0927165150642395
Batch 20/64 loss: 0.1145932674407959
Batch 21/64 loss: 0.10248333215713501
Batch 22/64 loss: 0.08658748865127563
Batch 23/64 loss: 0.07657331228256226
Batch 24/64 loss: 0.09777319431304932
Batch 25/64 loss: 0.05570173263549805
Batch 26/64 loss: 0.08016002178192139
Batch 27/64 loss: 0.07269763946533203
Batch 28/64 loss: 0.06861436367034912
Batch 29/64 loss: 0.08506441116333008
Batch 30/64 loss: 0.0943918228149414
Batch 31/64 loss: 0.09038674831390381
Batch 32/64 loss: 0.09214770793914795
Batch 33/64 loss: 0.07954424619674683
Batch 34/64 loss: 0.0900716781616211
Batch 35/64 loss: 0.07474493980407715
Batch 36/64 loss: 0.09575986862182617
Batch 37/64 loss: 0.09637242555618286
Batch 38/64 loss: 0.06621932983398438
Batch 39/64 loss: 0.0861133337020874
Batch 40/64 loss: 0.0928412675857544
Batch 41/64 loss: 0.09506750106811523
Batch 42/64 loss: 0.06557035446166992
Batch 43/64 loss: 0.05698871612548828
Batch 44/64 loss: 0.0806436538696289
Batch 45/64 loss: 0.08409690856933594
Batch 46/64 loss: 0.09535014629364014
Batch 47/64 loss: 0.09088754653930664
Batch 48/64 loss: 0.08249247074127197
Batch 49/64 loss: 0.06194061040878296
Batch 50/64 loss: 0.09732532501220703
Batch 51/64 loss: 0.09370887279510498
Batch 52/64 loss: 0.05870562791824341
Batch 53/64 loss: 0.05639636516571045
Batch 54/64 loss: 0.10377007722854614
Batch 55/64 loss: 0.07703322172164917
Batch 56/64 loss: 0.09127241373062134
Batch 57/64 loss: 0.09643203020095825
Batch 58/64 loss: 0.09865337610244751
Batch 59/64 loss: 0.07923519611358643
Batch 60/64 loss: 0.09728717803955078
Batch 61/64 loss: 0.08466535806655884
Batch 62/64 loss: 0.07818400859832764
Batch 63/64 loss: 0.10556155443191528
Batch 64/64 loss: 0.06306153535842896
Epoch 50  Train loss: 0.08445142367306878  Val loss: 0.11953668332181845
Epoch 51
-------------------------------
Batch 1/64 loss: 0.08502542972564697
Batch 2/64 loss: 0.09579455852508545
Batch 3/64 loss: 0.06322890520095825
Batch 4/64 loss: 0.09080404043197632
Batch 5/64 loss: 0.09813237190246582
Batch 6/64 loss: 0.07264542579650879
Batch 7/64 loss: 0.07918530702590942
Batch 8/64 loss: 0.07678383588790894
Batch 9/64 loss: 0.08336043357849121
Batch 10/64 loss: 0.0835757851600647
Batch 11/64 loss: 0.07502102851867676
Batch 12/64 loss: 0.05800354480743408
Batch 13/64 loss: 0.07680165767669678
Batch 14/64 loss: 0.08553999662399292
Batch 15/64 loss: 0.07769787311553955
Batch 16/64 loss: 0.11976903676986694
Batch 17/64 loss: 0.08104723691940308
Batch 18/64 loss: 0.08766663074493408
Batch 19/64 loss: 0.066952645778656
Batch 20/64 loss: 0.09201681613922119
Batch 21/64 loss: 0.07671165466308594
Batch 22/64 loss: 0.05577981472015381
Batch 23/64 loss: 0.08118653297424316
Batch 24/64 loss: 0.062078893184661865
Batch 25/64 loss: 0.05884796380996704
Batch 26/64 loss: 0.07021307945251465
Batch 27/64 loss: 0.11758935451507568
Batch 28/64 loss: 0.07444429397583008
Batch 29/64 loss: 0.0667838454246521
Batch 30/64 loss: 0.09706348180770874
Batch 31/64 loss: 0.08027476072311401
Batch 32/64 loss: 0.10592645406723022
Batch 33/64 loss: 0.0566403865814209
Batch 34/64 loss: 0.08252090215682983
Batch 35/64 loss: 0.09201735258102417
Batch 36/64 loss: 0.0788840651512146
Batch 37/64 loss: 0.1009359359741211
Batch 38/64 loss: 0.06849616765975952
Batch 39/64 loss: 0.07068169116973877
Batch 40/64 loss: 0.07271993160247803
Batch 41/64 loss: 0.06314629316329956
Batch 42/64 loss: 0.11442697048187256
Batch 43/64 loss: 0.11986249685287476
Batch 44/64 loss: 0.08786451816558838
Batch 45/64 loss: 0.07707470655441284
Batch 46/64 loss: 0.09734523296356201
Batch 47/64 loss: 0.07989442348480225
Batch 48/64 loss: 0.09150636196136475
Batch 49/64 loss: 0.08791458606719971
Batch 50/64 loss: 0.0788540244102478
Batch 51/64 loss: 0.06341832876205444
Batch 52/64 loss: 0.06548810005187988
Batch 53/64 loss: 0.0859498381614685
Batch 54/64 loss: 0.1176411509513855
Batch 55/64 loss: 0.08424991369247437
Batch 56/64 loss: 0.10002082586288452
Batch 57/64 loss: 0.08057069778442383
Batch 58/64 loss: 0.1205834150314331
Batch 59/64 loss: 0.07177937030792236
Batch 60/64 loss: 0.05563545227050781
Batch 61/64 loss: 0.10374033451080322
Batch 62/64 loss: 0.0829472541809082
Batch 63/64 loss: 0.09302932024002075
Batch 64/64 loss: 0.07310646772384644
Epoch 51  Train loss: 0.08308427590949863  Val loss: 0.11390094802142009
Saving best model, epoch: 51
Epoch 52
-------------------------------
Batch 1/64 loss: 0.06526023149490356
Batch 2/64 loss: 0.06985068321228027
Batch 3/64 loss: 0.07775306701660156
Batch 4/64 loss: 0.08361947536468506
Batch 5/64 loss: 0.1267688274383545
Batch 6/64 loss: 0.07808691263198853
Batch 7/64 loss: 0.10615652799606323
Batch 8/64 loss: 0.07322824001312256
Batch 9/64 loss: 0.0914463996887207
Batch 10/64 loss: 0.0616002082824707
Batch 11/64 loss: 0.07391846179962158
Batch 12/64 loss: 0.0763964056968689
Batch 13/64 loss: 0.07258594036102295
Batch 14/64 loss: 0.10267913341522217
Batch 15/64 loss: 0.07529586553573608
Batch 16/64 loss: 0.07105863094329834
Batch 17/64 loss: 0.05760008096694946
Batch 18/64 loss: 0.09460830688476562
Batch 19/64 loss: 0.07923030853271484
Batch 20/64 loss: 0.06432509422302246
Batch 21/64 loss: 0.07084989547729492
Batch 22/64 loss: 0.06218159198760986
Batch 23/64 loss: 0.08363330364227295
Batch 24/64 loss: 0.11231040954589844
Batch 25/64 loss: 0.09607052803039551
Batch 26/64 loss: 0.10204780101776123
Batch 27/64 loss: 0.04860574007034302
Batch 28/64 loss: 0.08569777011871338
Batch 29/64 loss: 0.07677453756332397
Batch 30/64 loss: 0.07646304368972778
Batch 31/64 loss: 0.08717542886734009
Batch 32/64 loss: 0.07805132865905762
Batch 33/64 loss: 0.08799248933792114
Batch 34/64 loss: 0.08804821968078613
Batch 35/64 loss: 0.09741044044494629
Batch 36/64 loss: 0.07489848136901855
Batch 37/64 loss: 0.0671200156211853
Batch 38/64 loss: 0.0954885482788086
Batch 39/64 loss: 0.09628534317016602
Batch 40/64 loss: 0.07559084892272949
Batch 41/64 loss: 0.06585264205932617
Batch 42/64 loss: 0.0552901029586792
Batch 43/64 loss: 0.08476060628890991
Batch 44/64 loss: 0.09688353538513184
Batch 45/64 loss: 0.10024815797805786
Batch 46/64 loss: 0.08439391851425171
Batch 47/64 loss: 0.06210988759994507
Batch 48/64 loss: 0.08335632085800171
Batch 49/64 loss: 0.07857578992843628
Batch 50/64 loss: 0.08157533407211304
Batch 51/64 loss: 0.10009735822677612
Batch 52/64 loss: 0.09236347675323486
Batch 53/64 loss: 0.08174788951873779
Batch 54/64 loss: 0.09583491086959839
Batch 55/64 loss: 0.07466667890548706
Batch 56/64 loss: 0.09872704744338989
Batch 57/64 loss: 0.08518588542938232
Batch 58/64 loss: 0.07969927787780762
Batch 59/64 loss: 0.07434135675430298
Batch 60/64 loss: 0.07862842082977295
Batch 61/64 loss: 0.07677018642425537
Batch 62/64 loss: 0.08575332164764404
Batch 63/64 loss: 0.0878075361251831
Batch 64/64 loss: 0.0808212161064148
Epoch 52  Train loss: 0.08203059014152078  Val loss: 0.10714074242155987
Saving best model, epoch: 52
Epoch 53
-------------------------------
Batch 1/64 loss: 0.08904910087585449
Batch 2/64 loss: 0.11140996217727661
Batch 3/64 loss: 0.08696842193603516
Batch 4/64 loss: 0.07187151908874512
Batch 5/64 loss: 0.0678454041481018
Batch 6/64 loss: 0.09626835584640503
Batch 7/64 loss: 0.08029013872146606
Batch 8/64 loss: 0.0967477560043335
Batch 9/64 loss: 0.047834038734436035
Batch 10/64 loss: 0.096748948097229
Batch 11/64 loss: 0.05739206075668335
Batch 12/64 loss: 0.07905447483062744
Batch 13/64 loss: 0.07914012670516968
Batch 14/64 loss: 0.08413046598434448
Batch 15/64 loss: 0.06979095935821533
Batch 16/64 loss: 0.10815948247909546
Batch 17/64 loss: 0.07356494665145874
Batch 18/64 loss: 0.07996910810470581
Batch 19/64 loss: 0.07151544094085693
Batch 20/64 loss: 0.056970298290252686
Batch 21/64 loss: 0.08120042085647583
Batch 22/64 loss: 0.07576721906661987
Batch 23/64 loss: 0.07654052972793579
Batch 24/64 loss: 0.06050604581832886
Batch 25/64 loss: 0.08522748947143555
Batch 26/64 loss: 0.04558044672012329
Batch 27/64 loss: 0.10148680210113525
Batch 28/64 loss: 0.07510942220687866
Batch 29/64 loss: 0.08285850286483765
Batch 30/64 loss: 0.09467834234237671
Batch 31/64 loss: 0.096077561378479
Batch 32/64 loss: 0.067909836769104
Batch 33/64 loss: 0.08991771936416626
Batch 34/64 loss: 0.08419537544250488
Batch 35/64 loss: 0.08118224143981934
Batch 36/64 loss: 0.08812892436981201
Batch 37/64 loss: 0.0728340744972229
Batch 38/64 loss: 0.11766642332077026
Batch 39/64 loss: 0.08043026924133301
Batch 40/64 loss: 0.0956379771232605
Batch 41/64 loss: 0.06528639793395996
Batch 42/64 loss: 0.07838255167007446
Batch 43/64 loss: 0.0732201337814331
Batch 44/64 loss: 0.09174466133117676
Batch 45/64 loss: 0.08384889364242554
Batch 46/64 loss: 0.05897742509841919
Batch 47/64 loss: 0.09621322154998779
Batch 48/64 loss: 0.05105847120285034
Batch 49/64 loss: 0.06663298606872559
Batch 50/64 loss: 0.041756391525268555
Batch 51/64 loss: 0.08062577247619629
Batch 52/64 loss: 0.087624192237854
Batch 53/64 loss: 0.08552151918411255
Batch 54/64 loss: 0.06686341762542725
Batch 55/64 loss: 0.08002841472625732
Batch 56/64 loss: 0.06849724054336548
Batch 57/64 loss: 0.058480143547058105
Batch 58/64 loss: 0.08923602104187012
Batch 59/64 loss: 0.08716917037963867
Batch 60/64 loss: 0.06497299671173096
Batch 61/64 loss: 0.057046473026275635
Batch 62/64 loss: 0.09705066680908203
Batch 63/64 loss: 0.060580313205718994
Batch 64/64 loss: 0.08031457662582397
Epoch 53  Train loss: 0.0785680006532108  Val loss: 0.12235233423226478
Epoch 54
-------------------------------
Batch 1/64 loss: 0.08716130256652832
Batch 2/64 loss: 0.07273787260055542
Batch 3/64 loss: 0.08678543567657471
Batch 4/64 loss: 0.06777733564376831
Batch 5/64 loss: 0.07257199287414551
Batch 6/64 loss: 0.064750075340271
Batch 7/64 loss: 0.06681954860687256
Batch 8/64 loss: 0.07212316989898682
Batch 9/64 loss: 0.05428934097290039
Batch 10/64 loss: 0.07762068510055542
Batch 11/64 loss: 0.08229970932006836
Batch 12/64 loss: 0.07441699504852295
Batch 13/64 loss: 0.08243066072463989
Batch 14/64 loss: 0.07350170612335205
Batch 15/64 loss: 0.08374130725860596
Batch 16/64 loss: 0.09673970937728882
Batch 17/64 loss: 0.0881805419921875
Batch 18/64 loss: 0.07540613412857056
Batch 19/64 loss: 0.07670629024505615
Batch 20/64 loss: 0.09024786949157715
Batch 21/64 loss: 0.06975173950195312
Batch 22/64 loss: 0.0757683515548706
Batch 23/64 loss: 0.08101761341094971
Batch 24/64 loss: 0.08451372385025024
Batch 25/64 loss: 0.060398876667022705
Batch 26/64 loss: 0.0653994083404541
Batch 27/64 loss: 0.07684749364852905
Batch 28/64 loss: 0.08861982822418213
Batch 29/64 loss: 0.06942307949066162
Batch 30/64 loss: 0.08783608675003052
Batch 31/64 loss: 0.07519304752349854
Batch 32/64 loss: 0.10133659839630127
Batch 33/64 loss: 0.07959854602813721
Batch 34/64 loss: 0.07425010204315186
Batch 35/64 loss: 0.08395838737487793
Batch 36/64 loss: 0.0689954161643982
Batch 37/64 loss: 0.07201230525970459
Batch 38/64 loss: 0.05677783489227295
Batch 39/64 loss: 0.07478350400924683
Batch 40/64 loss: 0.09739261865615845
Batch 41/64 loss: 0.09243714809417725
Batch 42/64 loss: 0.06124448776245117
Batch 43/64 loss: 0.07725882530212402
Batch 44/64 loss: 0.05961662530899048
Batch 45/64 loss: 0.06386959552764893
Batch 46/64 loss: 0.07445818185806274
Batch 47/64 loss: 0.0989421010017395
Batch 48/64 loss: 0.09043145179748535
Batch 49/64 loss: 0.0961524248123169
Batch 50/64 loss: 0.059913456439971924
Batch 51/64 loss: 0.04876977205276489
Batch 52/64 loss: 0.075550377368927
Batch 53/64 loss: 0.10618603229522705
Batch 54/64 loss: 0.06713783740997314
Batch 55/64 loss: 0.071399986743927
Batch 56/64 loss: 0.07312935590744019
Batch 57/64 loss: 0.0937647819519043
Batch 58/64 loss: 0.07338005304336548
Batch 59/64 loss: 0.05364722013473511
Batch 60/64 loss: 0.07039570808410645
Batch 61/64 loss: 0.09876662492752075
Batch 62/64 loss: 0.12914717197418213
Batch 63/64 loss: 0.05665898323059082
Batch 64/64 loss: 0.08383941650390625
Epoch 54  Train loss: 0.07757364721859203  Val loss: 0.11491939906811796
Epoch 55
-------------------------------
Batch 1/64 loss: 0.08052217960357666
Batch 2/64 loss: 0.11902981996536255
Batch 3/64 loss: 0.0359034538269043
Batch 4/64 loss: 0.08566570281982422
Batch 5/64 loss: 0.08578729629516602
Batch 6/64 loss: 0.04632443189620972
Batch 7/64 loss: 0.07456117868423462
Batch 8/64 loss: 0.041433632373809814
Batch 9/64 loss: 0.057614684104919434
Batch 10/64 loss: 0.08112376928329468
Batch 11/64 loss: 0.08471637964248657
Batch 12/64 loss: 0.06462907791137695
Batch 13/64 loss: 0.048126280307769775
Batch 14/64 loss: 0.11174768209457397
Batch 15/64 loss: 0.06053680181503296
Batch 16/64 loss: 0.06884264945983887
Batch 17/64 loss: 0.06976789236068726
Batch 18/64 loss: 0.08399474620819092
Batch 19/64 loss: 0.06740564107894897
Batch 20/64 loss: 0.07913285493850708
Batch 21/64 loss: 0.08125090599060059
Batch 22/64 loss: 0.0457761287689209
Batch 23/64 loss: 0.07484155893325806
Batch 24/64 loss: 0.07360529899597168
Batch 25/64 loss: 0.06414139270782471
Batch 26/64 loss: 0.10366678237915039
Batch 27/64 loss: 0.06899726390838623
Batch 28/64 loss: 0.07631611824035645
Batch 29/64 loss: 0.07523202896118164
Batch 30/64 loss: 0.06141883134841919
Batch 31/64 loss: 0.04641026258468628
Batch 32/64 loss: 0.05004775524139404
Batch 33/64 loss: 0.0706663727760315
Batch 34/64 loss: 0.06652671098709106
Batch 35/64 loss: 0.10989207029342651
Batch 36/64 loss: 0.07039666175842285
Batch 37/64 loss: 0.08951890468597412
Batch 38/64 loss: 0.06705707311630249
Batch 39/64 loss: 0.0717078447341919
Batch 40/64 loss: 0.09065574407577515
Batch 41/64 loss: 0.1054067611694336
Batch 42/64 loss: 0.110981285572052
Batch 43/64 loss: 0.07717061042785645
Batch 44/64 loss: 0.0794517993927002
Batch 45/64 loss: 0.07145726680755615
Batch 46/64 loss: 0.08992236852645874
Batch 47/64 loss: 0.07267886400222778
Batch 48/64 loss: 0.08396416902542114
Batch 49/64 loss: 0.08153867721557617
Batch 50/64 loss: 0.07987308502197266
Batch 51/64 loss: 0.07596248388290405
Batch 52/64 loss: 0.09759968519210815
Batch 53/64 loss: 0.05536484718322754
Batch 54/64 loss: 0.09124714136123657
Batch 55/64 loss: 0.07834303379058838
Batch 56/64 loss: 0.103878915309906
Batch 57/64 loss: 0.08034682273864746
Batch 58/64 loss: 0.09994316101074219
Batch 59/64 loss: 0.0688239336013794
Batch 60/64 loss: 0.08312809467315674
Batch 61/64 loss: 0.0817750096321106
Batch 62/64 loss: 0.07125794887542725
Batch 63/64 loss: 0.06442135572433472
Batch 64/64 loss: 0.052068233489990234
Epoch 55  Train loss: 0.07605616251627605  Val loss: 0.11781400585502284
Epoch 56
-------------------------------
Batch 1/64 loss: 0.051566243171691895
Batch 2/64 loss: 0.0685722827911377
Batch 3/64 loss: 0.06886047124862671
Batch 4/64 loss: 0.0634647011756897
Batch 5/64 loss: 0.08043724298477173
Batch 6/64 loss: 0.09147477149963379
Batch 7/64 loss: 0.1042327880859375
Batch 8/64 loss: 0.0872158408164978
Batch 9/64 loss: 0.09332031011581421
Batch 10/64 loss: 0.07653844356536865
Batch 11/64 loss: 0.06724506616592407
Batch 12/64 loss: 0.08337712287902832
Batch 13/64 loss: 0.0749940276145935
Batch 14/64 loss: 0.06412661075592041
Batch 15/64 loss: 0.0591699481010437
Batch 16/64 loss: 0.07404345273971558
Batch 17/64 loss: 0.07948899269104004
Batch 18/64 loss: 0.057312607765197754
Batch 19/64 loss: 0.028831005096435547
Batch 20/64 loss: 0.07451868057250977
Batch 21/64 loss: 0.06710684299468994
Batch 22/64 loss: 0.0801311731338501
Batch 23/64 loss: 0.04134726524353027
Batch 24/64 loss: 0.08017516136169434
Batch 25/64 loss: 0.08729338645935059
Batch 26/64 loss: 0.0941820740699768
Batch 27/64 loss: 0.07903528213500977
Batch 28/64 loss: 0.056391894817352295
Batch 29/64 loss: 0.0747339129447937
Batch 30/64 loss: 0.06411278247833252
Batch 31/64 loss: 0.07835531234741211
Batch 32/64 loss: 0.0883752703666687
Batch 33/64 loss: 0.07906776666641235
Batch 34/64 loss: 0.08817559480667114
Batch 35/64 loss: 0.09055030345916748
Batch 36/64 loss: 0.09409737586975098
Batch 37/64 loss: 0.06511837244033813
Batch 38/64 loss: 0.07546281814575195
Batch 39/64 loss: 0.06967657804489136
Batch 40/64 loss: 0.07025694847106934
Batch 41/64 loss: 0.06482815742492676
Batch 42/64 loss: 0.06430071592330933
Batch 43/64 loss: 0.0657469630241394
Batch 44/64 loss: 0.07803422212600708
Batch 45/64 loss: 0.07422256469726562
Batch 46/64 loss: 0.07591509819030762
Batch 47/64 loss: 0.05330252647399902
Batch 48/64 loss: 0.09321510791778564
Batch 49/64 loss: 0.09737133979797363
Batch 50/64 loss: 0.0833318829536438
Batch 51/64 loss: 0.08370661735534668
Batch 52/64 loss: 0.06006312370300293
Batch 53/64 loss: 0.08041781187057495
Batch 54/64 loss: 0.06600242853164673
Batch 55/64 loss: 0.06696772575378418
Batch 56/64 loss: 0.09378683567047119
Batch 57/64 loss: 0.09249508380889893
Batch 58/64 loss: 0.07831722497940063
Batch 59/64 loss: 0.06993865966796875
Batch 60/64 loss: 0.0631447434425354
Batch 61/64 loss: 0.08776688575744629
Batch 62/64 loss: 0.07345438003540039
Batch 63/64 loss: 0.0720868706703186
Batch 64/64 loss: 0.06657308340072632
Epoch 56  Train loss: 0.07467895699482338  Val loss: 0.10413304739391681
Saving best model, epoch: 56
Epoch 57
-------------------------------
Batch 1/64 loss: 0.06057405471801758
Batch 2/64 loss: 0.05211317539215088
Batch 3/64 loss: 0.07351958751678467
Batch 4/64 loss: 0.06003236770629883
Batch 5/64 loss: 0.06569671630859375
Batch 6/64 loss: 0.06554949283599854
Batch 7/64 loss: 0.06384772062301636
Batch 8/64 loss: 0.07042676210403442
Batch 9/64 loss: 0.06890749931335449
Batch 10/64 loss: 0.05815553665161133
Batch 11/64 loss: 0.047634124755859375
Batch 12/64 loss: 0.07303917407989502
Batch 13/64 loss: 0.07871830463409424
Batch 14/64 loss: 0.08225774765014648
Batch 15/64 loss: 0.10145491361618042
Batch 16/64 loss: 0.058513522148132324
Batch 17/64 loss: 0.08211863040924072
Batch 18/64 loss: 0.07039093971252441
Batch 19/64 loss: 0.08663439750671387
Batch 20/64 loss: 0.06466925144195557
Batch 21/64 loss: 0.08094757795333862
Batch 22/64 loss: 0.06270968914031982
Batch 23/64 loss: 0.046489834785461426
Batch 24/64 loss: 0.08605784177780151
Batch 25/64 loss: 0.07222771644592285
Batch 26/64 loss: 0.06409704685211182
Batch 27/64 loss: 0.06758803129196167
Batch 28/64 loss: 0.06326234340667725
Batch 29/64 loss: 0.07816624641418457
Batch 30/64 loss: 0.07751715183258057
Batch 31/64 loss: 0.09181922674179077
Batch 32/64 loss: 0.04038667678833008
Batch 33/64 loss: 0.1137891411781311
Batch 34/64 loss: 0.06281352043151855
Batch 35/64 loss: 0.08312809467315674
Batch 36/64 loss: 0.058299720287323
Batch 37/64 loss: 0.06838291883468628
Batch 38/64 loss: 0.06584423780441284
Batch 39/64 loss: 0.0800093412399292
Batch 40/64 loss: 0.11171412467956543
Batch 41/64 loss: 0.05780428647994995
Batch 42/64 loss: 0.06780791282653809
Batch 43/64 loss: 0.06972187757492065
Batch 44/64 loss: 0.06950068473815918
Batch 45/64 loss: 0.053417086601257324
Batch 46/64 loss: 0.0699152946472168
Batch 47/64 loss: 0.06748086214065552
Batch 48/64 loss: 0.07960963249206543
Batch 49/64 loss: 0.06831693649291992
Batch 50/64 loss: 0.09232157468795776
Batch 51/64 loss: 0.07277941703796387
Batch 52/64 loss: 0.08310306072235107
Batch 53/64 loss: 0.07330644130706787
Batch 54/64 loss: 0.07459598779678345
Batch 55/64 loss: 0.06778973340988159
Batch 56/64 loss: 0.09313476085662842
Batch 57/64 loss: 0.0652153491973877
Batch 58/64 loss: 0.08417832851409912
Batch 59/64 loss: 0.06892597675323486
Batch 60/64 loss: 0.057044029235839844
Batch 61/64 loss: 0.06075555086135864
Batch 62/64 loss: 0.06059563159942627
Batch 63/64 loss: 0.09118419885635376
Batch 64/64 loss: 0.06630712747573853
Epoch 57  Train loss: 0.07149395077836279  Val loss: 0.10873947008368895
Epoch 58
-------------------------------
Batch 1/64 loss: 0.05749469995498657
Batch 2/64 loss: 0.06144803762435913
Batch 3/64 loss: 0.0585939884185791
Batch 4/64 loss: 0.07394534349441528
Batch 5/64 loss: 0.03262662887573242
Batch 6/64 loss: 0.07608342170715332
Batch 7/64 loss: 0.0629040002822876
Batch 8/64 loss: 0.063629150390625
Batch 9/64 loss: 0.06551027297973633
Batch 10/64 loss: 0.08279317617416382
Batch 11/64 loss: 0.08018690347671509
Batch 12/64 loss: 0.0686730146408081
Batch 13/64 loss: 0.06525039672851562
Batch 14/64 loss: 0.07628858089447021
Batch 15/64 loss: 0.08587461709976196
Batch 16/64 loss: 0.0727948546409607
Batch 17/64 loss: 0.07648682594299316
Batch 18/64 loss: 0.05547595024108887
Batch 19/64 loss: 0.09064829349517822
Batch 20/64 loss: 0.09676623344421387
Batch 21/64 loss: 0.07441025972366333
Batch 22/64 loss: 0.08288103342056274
Batch 23/64 loss: 0.06350243091583252
Batch 24/64 loss: 0.05744439363479614
Batch 25/64 loss: 0.08303618431091309
Batch 26/64 loss: 0.05828648805618286
Batch 27/64 loss: 0.0900694727897644
Batch 28/64 loss: 0.0708126425743103
Batch 29/64 loss: 0.08753746747970581
Batch 30/64 loss: 0.10408914089202881
Batch 31/64 loss: 0.07373541593551636
Batch 32/64 loss: 0.07534527778625488
Batch 33/64 loss: 0.053720712661743164
Batch 34/64 loss: 0.07689648866653442
Batch 35/64 loss: 0.09049379825592041
Batch 36/64 loss: 0.059005916118621826
Batch 37/64 loss: 0.0684022307395935
Batch 38/64 loss: 0.0690341591835022
Batch 39/64 loss: 0.059868454933166504
Batch 40/64 loss: 0.05141991376876831
Batch 41/64 loss: 0.10793143510818481
Batch 42/64 loss: 0.05274873971939087
Batch 43/64 loss: 0.0681077241897583
Batch 44/64 loss: 0.07919740676879883
Batch 45/64 loss: 0.05823856592178345
Batch 46/64 loss: 0.05861389636993408
Batch 47/64 loss: 0.051921308040618896
Batch 48/64 loss: 0.054260313510894775
Batch 49/64 loss: 0.05965161323547363
Batch 50/64 loss: 0.06060647964477539
Batch 51/64 loss: 0.06231188774108887
Batch 52/64 loss: 0.05864894390106201
Batch 53/64 loss: 0.09797447919845581
Batch 54/64 loss: 0.06608235836029053
Batch 55/64 loss: 0.06784927845001221
Batch 56/64 loss: 0.08339196443557739
Batch 57/64 loss: 0.12649089097976685
Batch 58/64 loss: 0.08284038305282593
Batch 59/64 loss: 0.07237279415130615
Batch 60/64 loss: 0.07451951503753662
Batch 61/64 loss: 0.068195641040802
Batch 62/64 loss: 0.05058014392852783
Batch 63/64 loss: 0.06524670124053955
Batch 64/64 loss: 0.08705669641494751
Epoch 58  Train loss: 0.07131829425400378  Val loss: 0.11524185855773716
Epoch 59
-------------------------------
Batch 1/64 loss: 0.06745076179504395
Batch 2/64 loss: 0.05265617370605469
Batch 3/64 loss: 0.05785012245178223
Batch 4/64 loss: 0.0827401876449585
Batch 5/64 loss: 0.04673147201538086
Batch 6/64 loss: 0.07144248485565186
Batch 7/64 loss: 0.06950885057449341
Batch 8/64 loss: 0.060172975063323975
Batch 9/64 loss: 0.07939815521240234
Batch 10/64 loss: 0.07894402742385864
Batch 11/64 loss: 0.0776100754737854
Batch 12/64 loss: 0.08985012769699097
Batch 13/64 loss: 0.0511474609375
Batch 14/64 loss: 0.08025360107421875
Batch 15/64 loss: 0.08408856391906738
Batch 16/64 loss: 0.054821550846099854
Batch 17/64 loss: 0.07377421855926514
Batch 18/64 loss: 0.09749412536621094
Batch 19/64 loss: 0.0862988829612732
Batch 20/64 loss: 0.08428752422332764
Batch 21/64 loss: 0.056631624698638916
Batch 22/64 loss: 0.07990390062332153
Batch 23/64 loss: 0.0689241886138916
Batch 24/64 loss: 0.0817636251449585
Batch 25/64 loss: 0.051165223121643066
Batch 26/64 loss: 0.07637304067611694
Batch 27/64 loss: 0.07534289360046387
Batch 28/64 loss: 0.07312703132629395
Batch 29/64 loss: 0.055703580379486084
Batch 30/64 loss: 0.061014771461486816
Batch 31/64 loss: 0.06060385704040527
Batch 32/64 loss: 0.06390255689620972
Batch 33/64 loss: 0.07649719715118408
Batch 34/64 loss: 0.060741543769836426
Batch 35/64 loss: 0.05436354875564575
Batch 36/64 loss: 0.053592562675476074
Batch 37/64 loss: 0.07709330320358276
Batch 38/64 loss: 0.05147051811218262
Batch 39/64 loss: 0.08739900588989258
Batch 40/64 loss: 0.09323620796203613
Batch 41/64 loss: 0.08429741859436035
Batch 42/64 loss: 0.05894327163696289
Batch 43/64 loss: 0.06468379497528076
Batch 44/64 loss: 0.05151355266571045
Batch 45/64 loss: 0.06765329837799072
Batch 46/64 loss: 0.06539970636367798
Batch 47/64 loss: 0.06175112724304199
Batch 48/64 loss: 0.0659748911857605
Batch 49/64 loss: 0.07911854982376099
Batch 50/64 loss: 0.06883054971694946
Batch 51/64 loss: 0.0685197114944458
Batch 52/64 loss: 0.05951458215713501
Batch 53/64 loss: 0.0452384352684021
Batch 54/64 loss: 0.08278721570968628
Batch 55/64 loss: 0.05538284778594971
Batch 56/64 loss: 0.06590837240219116
Batch 57/64 loss: 0.07874691486358643
Batch 58/64 loss: 0.0741506814956665
Batch 59/64 loss: 0.05070394277572632
Batch 60/64 loss: 0.07963156700134277
Batch 61/64 loss: 0.07791155576705933
Batch 62/64 loss: 0.06590801477432251
Batch 63/64 loss: 0.0557096004486084
Batch 64/64 loss: 0.09804320335388184
Epoch 59  Train loss: 0.06913229063445446  Val loss: 0.10621306023646876
Epoch 60
-------------------------------
Batch 1/64 loss: 0.06599724292755127
Batch 2/64 loss: 0.038092613220214844
Batch 3/64 loss: 0.041708290576934814
Batch 4/64 loss: 0.07496470212936401
Batch 5/64 loss: 0.07580506801605225
Batch 6/64 loss: 0.06328922510147095
Batch 7/64 loss: 0.05292046070098877
Batch 8/64 loss: 0.07142508029937744
Batch 9/64 loss: 0.07204526662826538
Batch 10/64 loss: 0.07868903875350952
Batch 11/64 loss: 0.04727756977081299
Batch 12/64 loss: 0.08952975273132324
Batch 13/64 loss: 0.08110624551773071
Batch 14/64 loss: 0.11502188444137573
Batch 15/64 loss: 0.07782119512557983
Batch 16/64 loss: 0.07819479703903198
Batch 17/64 loss: 0.07689905166625977
Batch 18/64 loss: 0.047267913818359375
Batch 19/64 loss: 0.053011953830718994
Batch 20/64 loss: 0.07313793897628784
Batch 21/64 loss: 0.0645378828048706
Batch 22/64 loss: 0.06244301795959473
Batch 23/64 loss: 0.048651158809661865
Batch 24/64 loss: 0.07487112283706665
Batch 25/64 loss: 0.05764108896255493
Batch 26/64 loss: 0.05329793691635132
Batch 27/64 loss: 0.06870889663696289
Batch 28/64 loss: 0.08517563343048096
Batch 29/64 loss: 0.06329107284545898
Batch 30/64 loss: 0.09279137849807739
Batch 31/64 loss: 0.06992518901824951
Batch 32/64 loss: 0.060638368129730225
Batch 33/64 loss: 0.03910320997238159
Batch 34/64 loss: 0.05718064308166504
Batch 35/64 loss: 0.08273506164550781
Batch 36/64 loss: 0.07887125015258789
Batch 37/64 loss: 0.0784764289855957
Batch 38/64 loss: 0.06718635559082031
Batch 39/64 loss: 0.07216620445251465
Batch 40/64 loss: 0.08463633060455322
Batch 41/64 loss: 0.05868887901306152
Batch 42/64 loss: 0.04413241147994995
Batch 43/64 loss: 0.04387819766998291
Batch 44/64 loss: 0.08380872011184692
Batch 45/64 loss: 0.062235236167907715
Batch 46/64 loss: 0.09210336208343506
Batch 47/64 loss: 0.08890706300735474
Batch 48/64 loss: 0.04088759422302246
Batch 49/64 loss: 0.07100450992584229
Batch 50/64 loss: 0.09877884387969971
Batch 51/64 loss: 0.06266450881958008
Batch 52/64 loss: 0.07497721910476685
Batch 53/64 loss: 0.03477358818054199
Batch 54/64 loss: 0.07499516010284424
Batch 55/64 loss: 0.08479487895965576
Batch 56/64 loss: 0.06433874368667603
Batch 57/64 loss: 0.052230119705200195
Batch 58/64 loss: 0.052162766456604004
Batch 59/64 loss: 0.05516105890274048
Batch 60/64 loss: 0.07027888298034668
Batch 61/64 loss: 0.07974278926849365
Batch 62/64 loss: 0.10253411531448364
Batch 63/64 loss: 0.09150391817092896
Batch 64/64 loss: 0.06812220811843872
Epoch 60  Train loss: 0.06858365091623045  Val loss: 0.10538316367008432
Epoch 61
-------------------------------
Batch 1/64 loss: 0.0645551085472107
Batch 2/64 loss: 0.06756895780563354
Batch 3/64 loss: 0.06468331813812256
Batch 4/64 loss: 0.07533180713653564
Batch 5/64 loss: 0.06385970115661621
Batch 6/64 loss: 0.03908902406692505
Batch 7/64 loss: 0.0573917031288147
Batch 8/64 loss: 0.10121023654937744
Batch 9/64 loss: 0.05577760934829712
Batch 10/64 loss: 0.08175015449523926
Batch 11/64 loss: 0.058213114738464355
Batch 12/64 loss: 0.08513379096984863
Batch 13/64 loss: 0.0747177004814148
Batch 14/64 loss: 0.040424644947052
Batch 15/64 loss: 0.08575433492660522
Batch 16/64 loss: 0.09738433361053467
Batch 17/64 loss: 0.047846853733062744
Batch 18/64 loss: 0.055570125579833984
Batch 19/64 loss: 0.06316936016082764
Batch 20/64 loss: 0.08983606100082397
Batch 21/64 loss: 0.04927051067352295
Batch 22/64 loss: 0.069735586643219
Batch 23/64 loss: 0.04771411418914795
Batch 24/64 loss: 0.04451078176498413
Batch 25/64 loss: 0.06872230768203735
Batch 26/64 loss: 0.07514709234237671
Batch 27/64 loss: 0.05330371856689453
Batch 28/64 loss: 0.08552813529968262
Batch 29/64 loss: 0.07332718372344971
Batch 30/64 loss: 0.08730196952819824
Batch 31/64 loss: 0.05785113573074341
Batch 32/64 loss: 0.0758931040763855
Batch 33/64 loss: 0.06026887893676758
Batch 34/64 loss: 0.07352203130722046
Batch 35/64 loss: 0.08368635177612305
Batch 36/64 loss: 0.04487031698226929
Batch 37/64 loss: 0.06799960136413574
Batch 38/64 loss: 0.044744789600372314
Batch 39/64 loss: 0.08302056789398193
Batch 40/64 loss: 0.0757286548614502
Batch 41/64 loss: 0.09332937002182007
Batch 42/64 loss: 0.07018673419952393
Batch 43/64 loss: 0.10469722747802734
Batch 44/64 loss: 0.09754198789596558
Batch 45/64 loss: 0.09196722507476807
Batch 46/64 loss: 0.07516992092132568
Batch 47/64 loss: 0.03634768724441528
Batch 48/64 loss: 0.0664183497428894
Batch 49/64 loss: 0.041038453578948975
Batch 50/64 loss: 0.06121635437011719
Batch 51/64 loss: 0.05600416660308838
Batch 52/64 loss: 0.0539318323135376
Batch 53/64 loss: 0.05487900972366333
Batch 54/64 loss: 0.08856606483459473
Batch 55/64 loss: 0.05210322141647339
Batch 56/64 loss: 0.06223505735397339
Batch 57/64 loss: 0.09164589643478394
Batch 58/64 loss: 0.06733828783035278
Batch 59/64 loss: 0.07830464839935303
Batch 60/64 loss: 0.062324702739715576
Batch 61/64 loss: 0.033226847648620605
Batch 62/64 loss: 0.08653044700622559
Batch 63/64 loss: 0.08276033401489258
Batch 64/64 loss: 0.07899266481399536
Epoch 61  Train loss: 0.06836742115955727  Val loss: 0.10106300385956912
Saving best model, epoch: 61
Epoch 62
-------------------------------
Batch 1/64 loss: 0.050253868103027344
Batch 2/64 loss: 0.07883620262145996
Batch 3/64 loss: 0.06610649824142456
Batch 4/64 loss: 0.0882483720779419
Batch 5/64 loss: 0.02800130844116211
Batch 6/64 loss: 0.06771683692932129
Batch 7/64 loss: 0.08022785186767578
Batch 8/64 loss: 0.060239195823669434
Batch 9/64 loss: 0.06340742111206055
Batch 10/64 loss: 0.05250239372253418
Batch 11/64 loss: 0.040450215339660645
Batch 12/64 loss: 0.052195191383361816
Batch 13/64 loss: 0.0772172212600708
Batch 14/64 loss: 0.07020020484924316
Batch 15/64 loss: 0.07079726457595825
Batch 16/64 loss: 0.062252938747406006
Batch 17/64 loss: 0.07099545001983643
Batch 18/64 loss: 0.06274241209030151
Batch 19/64 loss: 0.05608326196670532
Batch 20/64 loss: 0.0754585862159729
Batch 21/64 loss: 0.052415668964385986
Batch 22/64 loss: 0.06096768379211426
Batch 23/64 loss: 0.06747353076934814
Batch 24/64 loss: 0.05270487070083618
Batch 25/64 loss: 0.07117146253585815
Batch 26/64 loss: 0.06630957126617432
Batch 27/64 loss: 0.06952524185180664
Batch 28/64 loss: 0.06967300176620483
Batch 29/64 loss: 0.06122934818267822
Batch 30/64 loss: 0.06922495365142822
Batch 31/64 loss: 0.05345571041107178
Batch 32/64 loss: 0.0659227967262268
Batch 33/64 loss: 0.05826777219772339
Batch 34/64 loss: 0.0670543909072876
Batch 35/64 loss: 0.07537281513214111
Batch 36/64 loss: 0.07176148891448975
Batch 37/64 loss: 0.08365589380264282
Batch 38/64 loss: 0.059237122535705566
Batch 39/64 loss: 0.046176791191101074
Batch 40/64 loss: 0.06563246250152588
Batch 41/64 loss: 0.058617353439331055
Batch 42/64 loss: 0.042986929416656494
Batch 43/64 loss: 0.08318382501602173
Batch 44/64 loss: 0.06647598743438721
Batch 45/64 loss: 0.07169318199157715
Batch 46/64 loss: 0.06337594985961914
Batch 47/64 loss: 0.07716143131256104
Batch 48/64 loss: 0.08537662029266357
Batch 49/64 loss: 0.06750327348709106
Batch 50/64 loss: 0.10888671875
Batch 51/64 loss: 0.04079294204711914
Batch 52/64 loss: 0.08821600675582886
Batch 53/64 loss: 0.050939321517944336
Batch 54/64 loss: 0.07109814882278442
Batch 55/64 loss: 0.07780903577804565
Batch 56/64 loss: 0.07212209701538086
Batch 57/64 loss: 0.06044572591781616
Batch 58/64 loss: 0.05468392372131348
Batch 59/64 loss: 0.06353890895843506
Batch 60/64 loss: 0.06300652027130127
Batch 61/64 loss: 0.05475020408630371
Batch 62/64 loss: 0.06247556209564209
Batch 63/64 loss: 0.07092946767807007
Batch 64/64 loss: 0.06746876239776611
Epoch 62  Train loss: 0.06537781930437275  Val loss: 0.11052992683915339
Epoch 63
-------------------------------
Batch 1/64 loss: 0.06828796863555908
Batch 2/64 loss: 0.07103139162063599
Batch 3/64 loss: 0.04884231090545654
Batch 4/64 loss: 0.04725390672683716
Batch 5/64 loss: 0.04902845621109009
Batch 6/64 loss: 0.06200695037841797
Batch 7/64 loss: 0.07501327991485596
Batch 8/64 loss: 0.07322466373443604
Batch 9/64 loss: 0.08916562795639038
Batch 10/64 loss: 0.05806189775466919
Batch 11/64 loss: 0.06511306762695312
Batch 12/64 loss: 0.06451421976089478
Batch 13/64 loss: 0.04398137331008911
Batch 14/64 loss: 0.07074207067489624
Batch 15/64 loss: 0.06386101245880127
Batch 16/64 loss: 0.08212149143218994
Batch 17/64 loss: 0.0539783239364624
Batch 18/64 loss: 0.07948869466781616
Batch 19/64 loss: 0.06438088417053223
Batch 20/64 loss: 0.05663961172103882
Batch 21/64 loss: 0.06745380163192749
Batch 22/64 loss: 0.06123495101928711
Batch 23/64 loss: 0.06096881628036499
Batch 24/64 loss: 0.06473445892333984
Batch 25/64 loss: 0.059233903884887695
Batch 26/64 loss: 0.0473780632019043
Batch 27/64 loss: 0.06561106443405151
Batch 28/64 loss: 0.06667280197143555
Batch 29/64 loss: 0.0640343427658081
Batch 30/64 loss: 0.05624419450759888
Batch 31/64 loss: 0.05559182167053223
Batch 32/64 loss: 0.08158218860626221
Batch 33/64 loss: 0.06319433450698853
Batch 34/64 loss: 0.07125884294509888
Batch 35/64 loss: 0.0542905330657959
Batch 36/64 loss: 0.06753838062286377
Batch 37/64 loss: 0.045381784439086914
Batch 38/64 loss: 0.04904216527938843
Batch 39/64 loss: 0.06598430871963501
Batch 40/64 loss: 0.03903961181640625
Batch 41/64 loss: 0.08249390125274658
Batch 42/64 loss: 0.06304949522018433
Batch 43/64 loss: 0.07614260911941528
Batch 44/64 loss: 0.07369887828826904
Batch 45/64 loss: 0.08325451612472534
Batch 46/64 loss: 0.06336277723312378
Batch 47/64 loss: 0.03488105535507202
Batch 48/64 loss: 0.03630721569061279
Batch 49/64 loss: 0.07962226867675781
Batch 50/64 loss: 0.07010841369628906
Batch 51/64 loss: 0.06909376382827759
Batch 52/64 loss: 0.08815371990203857
Batch 53/64 loss: 0.051464200019836426
Batch 54/64 loss: 0.062305450439453125
Batch 55/64 loss: 0.06759494543075562
Batch 56/64 loss: 0.07609564065933228
Batch 57/64 loss: 0.04811716079711914
Batch 58/64 loss: 0.049485623836517334
Batch 59/64 loss: 0.058679819107055664
Batch 60/64 loss: 0.08303111791610718
Batch 61/64 loss: 0.07594114542007446
Batch 62/64 loss: 0.08842599391937256
Batch 63/64 loss: 0.07921409606933594
Batch 64/64 loss: 0.04510152339935303
Epoch 63  Train loss: 0.06411854940302232  Val loss: 0.12345848624239263
Epoch 64
-------------------------------
Batch 1/64 loss: 0.06308186054229736
Batch 2/64 loss: 0.06683611869812012
Batch 3/64 loss: 0.05841594934463501
Batch 4/64 loss: 0.07876336574554443
Batch 5/64 loss: 0.07817971706390381
Batch 6/64 loss: 0.07317858934402466
Batch 7/64 loss: 0.06374818086624146
Batch 8/64 loss: 0.06197923421859741
Batch 9/64 loss: 0.09206050634384155
Batch 10/64 loss: 0.054476261138916016
Batch 11/64 loss: 0.09627032279968262
Batch 12/64 loss: 0.05393993854522705
Batch 13/64 loss: 0.06967443227767944
Batch 14/64 loss: 0.05849736928939819
Batch 15/64 loss: 0.045248985290527344
Batch 16/64 loss: 0.05215191841125488
Batch 17/64 loss: 0.06085097789764404
Batch 18/64 loss: 0.058081626892089844
Batch 19/64 loss: 0.062409818172454834
Batch 20/64 loss: 0.051134467124938965
Batch 21/64 loss: 0.08269679546356201
Batch 22/64 loss: 0.05987262725830078
Batch 23/64 loss: 0.04675722122192383
Batch 24/64 loss: 0.08556663990020752
Batch 25/64 loss: 0.05889922380447388
Batch 26/64 loss: 0.059417665004730225
Batch 27/64 loss: 0.06764072179794312
Batch 28/64 loss: 0.05422663688659668
Batch 29/64 loss: 0.06602621078491211
Batch 30/64 loss: 0.05921691656112671
Batch 31/64 loss: 0.07739502191543579
Batch 32/64 loss: 0.05144017934799194
Batch 33/64 loss: 0.06207150220870972
Batch 34/64 loss: 0.057110369205474854
Batch 35/64 loss: 0.04946225881576538
Batch 36/64 loss: 0.04507845640182495
Batch 37/64 loss: 0.040877461433410645
Batch 38/64 loss: 0.0691450834274292
Batch 39/64 loss: 0.07868808507919312
Batch 40/64 loss: 0.04120093584060669
Batch 41/64 loss: 0.04748350381851196
Batch 42/64 loss: 0.07929885387420654
Batch 43/64 loss: 0.08157521486282349
Batch 44/64 loss: 0.03970462083816528
Batch 45/64 loss: 0.07698196172714233
Batch 46/64 loss: 0.052702367305755615
Batch 47/64 loss: 0.049669623374938965
Batch 48/64 loss: 0.051711440086364746
Batch 49/64 loss: 0.09080290794372559
Batch 50/64 loss: 0.09277093410491943
Batch 51/64 loss: 0.07120311260223389
Batch 52/64 loss: 0.05910778045654297
Batch 53/64 loss: 0.05064094066619873
Batch 54/64 loss: 0.06473791599273682
Batch 55/64 loss: 0.07338780164718628
Batch 56/64 loss: 0.12074989080429077
Batch 57/64 loss: 0.07338035106658936
Batch 58/64 loss: 0.06853663921356201
Batch 59/64 loss: 0.046632349491119385
Batch 60/64 loss: 0.08555841445922852
Batch 61/64 loss: 0.05563509464263916
Batch 62/64 loss: 0.07001066207885742
Batch 63/64 loss: 0.028362810611724854
Batch 64/64 loss: 0.06093907356262207
Epoch 64  Train loss: 0.06412735920326383  Val loss: 0.10646351854416103
Epoch 65
-------------------------------
Batch 1/64 loss: 0.058735013008117676
Batch 2/64 loss: 0.03189849853515625
Batch 3/64 loss: 0.04088270664215088
Batch 4/64 loss: 0.08513051271438599
Batch 5/64 loss: 0.04708719253540039
Batch 6/64 loss: 0.046790361404418945
Batch 7/64 loss: 0.05043375492095947
Batch 8/64 loss: 0.0650097131729126
Batch 9/64 loss: 0.07359433174133301
Batch 10/64 loss: 0.07514113187789917
Batch 11/64 loss: 0.08797329664230347
Batch 12/64 loss: 0.06733286380767822
Batch 13/64 loss: 0.07177871465682983
Batch 14/64 loss: 0.06140846014022827
Batch 15/64 loss: 0.06402194499969482
Batch 16/64 loss: 0.03679150342941284
Batch 17/64 loss: 0.0741545557975769
Batch 18/64 loss: 0.058602988719940186
Batch 19/64 loss: 0.05705171823501587
Batch 20/64 loss: 0.060400962829589844
Batch 21/64 loss: 0.060498058795928955
Batch 22/64 loss: 0.060809552669525146
Batch 23/64 loss: 0.07195281982421875
Batch 24/64 loss: 0.04256027936935425
Batch 25/64 loss: 0.05211210250854492
Batch 26/64 loss: 0.065621018409729
Batch 27/64 loss: 0.08142554759979248
Batch 28/64 loss: 0.061542809009552
Batch 29/64 loss: 0.04031217098236084
Batch 30/64 loss: 0.051220476627349854
Batch 31/64 loss: 0.05451786518096924
Batch 32/64 loss: 0.052259862422943115
Batch 33/64 loss: 0.06725913286209106
Batch 34/64 loss: 0.07420456409454346
Batch 35/64 loss: 0.05792027711868286
Batch 36/64 loss: 0.09300321340560913
Batch 37/64 loss: 0.06416189670562744
Batch 38/64 loss: 0.07592862844467163
Batch 39/64 loss: 0.07057321071624756
Batch 40/64 loss: 0.04876750707626343
Batch 41/64 loss: 0.0526089072227478
Batch 42/64 loss: 0.050253212451934814
Batch 43/64 loss: 0.06688326597213745
Batch 44/64 loss: 0.09992164373397827
Batch 45/64 loss: 0.05818367004394531
Batch 46/64 loss: 0.0600123405456543
Batch 47/64 loss: 0.08799916505813599
Batch 48/64 loss: 0.05223238468170166
Batch 49/64 loss: 0.07032054662704468
Batch 50/64 loss: 0.06582140922546387
Batch 51/64 loss: 0.051277875900268555
Batch 52/64 loss: 0.07572352886199951
Batch 53/64 loss: 0.06289327144622803
Batch 54/64 loss: 0.04966002702713013
Batch 55/64 loss: 0.09084433317184448
Batch 56/64 loss: 0.03635048866271973
Batch 57/64 loss: 0.06537652015686035
Batch 58/64 loss: 0.06548666954040527
Batch 59/64 loss: 0.041412532329559326
Batch 60/64 loss: 0.05645155906677246
Batch 61/64 loss: 0.07052987813949585
Batch 62/64 loss: 0.06259506940841675
Batch 63/64 loss: 0.025696635246276855
Batch 64/64 loss: 0.07481402158737183
Epoch 65  Train loss: 0.061733594828960944  Val loss: 0.11504416777096253
Epoch 66
-------------------------------
Batch 1/64 loss: 0.06695526838302612
Batch 2/64 loss: 0.03582340478897095
Batch 3/64 loss: 0.07185101509094238
Batch 4/64 loss: 0.058600544929504395
Batch 5/64 loss: 0.0463489294052124
Batch 6/64 loss: 0.08057242631912231
Batch 7/64 loss: 0.05010688304901123
Batch 8/64 loss: 0.04711270332336426
Batch 9/64 loss: 0.05288243293762207
Batch 10/64 loss: 0.03267335891723633
Batch 11/64 loss: 0.05804109573364258
Batch 12/64 loss: 0.04252129793167114
Batch 13/64 loss: 0.07168513536453247
Batch 14/64 loss: 0.05251574516296387
Batch 15/64 loss: 0.05718672275543213
Batch 16/64 loss: 0.055872201919555664
Batch 17/64 loss: 0.083018958568573
Batch 18/64 loss: 0.06753736734390259
Batch 19/64 loss: 0.04115712642669678
Batch 20/64 loss: 0.07617628574371338
Batch 21/64 loss: 0.03882479667663574
Batch 22/64 loss: 0.07138973474502563
Batch 23/64 loss: 0.0562363862991333
Batch 24/64 loss: 0.06704074144363403
Batch 25/64 loss: 0.05842745304107666
Batch 26/64 loss: 0.04682266712188721
Batch 27/64 loss: 0.03579401969909668
Batch 28/64 loss: 0.058380722999572754
Batch 29/64 loss: 0.05258047580718994
Batch 30/64 loss: 0.06137728691101074
Batch 31/64 loss: 0.037895917892456055
Batch 32/64 loss: 0.09116065502166748
Batch 33/64 loss: 0.05183756351470947
Batch 34/64 loss: 0.10523796081542969
Batch 35/64 loss: 0.07339900732040405
Batch 36/64 loss: 0.05757540464401245
Batch 37/64 loss: 0.0711064338684082
Batch 38/64 loss: 0.07536721229553223
Batch 39/64 loss: 0.0571209192276001
Batch 40/64 loss: 0.05459129810333252
Batch 41/64 loss: 0.05796051025390625
Batch 42/64 loss: 0.053621113300323486
Batch 43/64 loss: 0.056462764739990234
Batch 44/64 loss: 0.07482385635375977
Batch 45/64 loss: 0.0798153281211853
Batch 46/64 loss: 0.0651700496673584
Batch 47/64 loss: 0.06920373439788818
Batch 48/64 loss: 0.07613909244537354
Batch 49/64 loss: 0.05067276954650879
Batch 50/64 loss: 0.06568485498428345
Batch 51/64 loss: 0.038771092891693115
Batch 52/64 loss: 0.012837350368499756
Batch 53/64 loss: 0.043068647384643555
Batch 54/64 loss: 0.06605792045593262
Batch 55/64 loss: 0.07611805200576782
Batch 56/64 loss: 0.05793583393096924
Batch 57/64 loss: 0.05172622203826904
Batch 58/64 loss: 0.07086390256881714
Batch 59/64 loss: 0.07951569557189941
Batch 60/64 loss: 0.1023244857788086
Batch 61/64 loss: 0.03630375862121582
Batch 62/64 loss: 0.05173546075820923
Batch 63/64 loss: 0.05216193199157715
Batch 64/64 loss: 0.08136534690856934
Epoch 66  Train loss: 0.05993418039060106  Val loss: 0.11650769796568095
Epoch 67
-------------------------------
Batch 1/64 loss: 0.06249338388442993
Batch 2/64 loss: 0.05371445417404175
Batch 3/64 loss: 0.06442868709564209
Batch 4/64 loss: 0.0675627589225769
Batch 5/64 loss: 0.06760966777801514
Batch 6/64 loss: 0.05662328004837036
Batch 7/64 loss: 0.0759427547454834
Batch 8/64 loss: 0.07918083667755127
Batch 9/64 loss: 0.029443979263305664
Batch 10/64 loss: 0.04487264156341553
Batch 11/64 loss: 0.08976393938064575
Batch 12/64 loss: 0.08864718675613403
Batch 13/64 loss: 0.051728785037994385
Batch 14/64 loss: 0.06762254238128662
Batch 15/64 loss: 0.06645351648330688
Batch 16/64 loss: 0.0664447546005249
Batch 17/64 loss: 0.07001900672912598
Batch 18/64 loss: 0.06950092315673828
Batch 19/64 loss: 0.05508005619049072
Batch 20/64 loss: 0.05975764989852905
Batch 21/64 loss: 0.0921679139137268
Batch 22/64 loss: 0.06899142265319824
Batch 23/64 loss: 0.06340247392654419
Batch 24/64 loss: 0.05187636613845825
Batch 25/64 loss: 0.047113239765167236
Batch 26/64 loss: 0.05630987882614136
Batch 27/64 loss: 0.042344868183135986
Batch 28/64 loss: 0.023787379264831543
Batch 29/64 loss: 0.05675637722015381
Batch 30/64 loss: 0.057021915912628174
Batch 31/64 loss: 0.0378798246383667
Batch 32/64 loss: 0.0649794340133667
Batch 33/64 loss: 0.06280183792114258
Batch 34/64 loss: 0.04046815633773804
Batch 35/64 loss: 0.059101223945617676
Batch 36/64 loss: 0.05065184831619263
Batch 37/64 loss: 0.041729509830474854
Batch 38/64 loss: 0.04984855651855469
Batch 39/64 loss: 0.07180404663085938
Batch 40/64 loss: 0.053985655307769775
Batch 41/64 loss: 0.0742032527923584
Batch 42/64 loss: 0.05596029758453369
Batch 43/64 loss: 0.06079673767089844
Batch 44/64 loss: 0.07237154245376587
Batch 45/64 loss: 0.07198917865753174
Batch 46/64 loss: 0.06826424598693848
Batch 47/64 loss: 0.06989800930023193
Batch 48/64 loss: 0.07980066537857056
Batch 49/64 loss: 0.04393380880355835
Batch 50/64 loss: 0.05475252866744995
Batch 51/64 loss: 0.04617595672607422
Batch 52/64 loss: 0.06365382671356201
Batch 53/64 loss: 0.07855820655822754
Batch 54/64 loss: 0.05488383769989014
Batch 55/64 loss: 0.07937467098236084
Batch 56/64 loss: 0.044230878353118896
Batch 57/64 loss: 0.03587138652801514
Batch 58/64 loss: 0.07388544082641602
Batch 59/64 loss: 0.04564213752746582
Batch 60/64 loss: 0.05674338340759277
Batch 61/64 loss: 0.04737997055053711
Batch 62/64 loss: 0.06514215469360352
Batch 63/64 loss: 0.054489970207214355
Batch 64/64 loss: 0.042031049728393555
Epoch 67  Train loss: 0.059755891912123736  Val loss: 0.09167857186491137
Saving best model, epoch: 67
Epoch 68
-------------------------------
Batch 1/64 loss: 0.0421905517578125
Batch 2/64 loss: 0.05717575550079346
Batch 3/64 loss: 0.08902609348297119
Batch 4/64 loss: 0.05599409341812134
Batch 5/64 loss: 0.031199395656585693
Batch 6/64 loss: 0.0629308819770813
Batch 7/64 loss: 0.043620765209198
Batch 8/64 loss: 0.049746811389923096
Batch 9/64 loss: 0.04971867799758911
Batch 10/64 loss: 0.04187178611755371
Batch 11/64 loss: 0.05967170000076294
Batch 12/64 loss: 0.04600667953491211
Batch 13/64 loss: 0.05617135763168335
Batch 14/64 loss: 0.07193905115127563
Batch 15/64 loss: 0.04951101541519165
Batch 16/64 loss: 0.023043334484100342
Batch 17/64 loss: 0.0524287223815918
Batch 18/64 loss: 0.05704641342163086
Batch 19/64 loss: 0.07489681243896484
Batch 20/64 loss: 0.034317851066589355
Batch 21/64 loss: 0.0591583251953125
Batch 22/64 loss: 0.05769538879394531
Batch 23/64 loss: 0.04904365539550781
Batch 24/64 loss: 0.07711100578308105
Batch 25/64 loss: 0.08689725399017334
Batch 26/64 loss: 0.052912354469299316
Batch 27/64 loss: 0.09056729078292847
Batch 28/64 loss: 0.046671509742736816
Batch 29/64 loss: 0.04829668998718262
Batch 30/64 loss: 0.0627661943435669
Batch 31/64 loss: 0.046277761459350586
Batch 32/64 loss: 0.05555635690689087
Batch 33/64 loss: 0.06567656993865967
Batch 34/64 loss: 0.0539705753326416
Batch 35/64 loss: 0.06437665224075317
Batch 36/64 loss: 0.054986000061035156
Batch 37/64 loss: 0.07253247499465942
Batch 38/64 loss: 0.06602412462234497
Batch 39/64 loss: 0.03395801782608032
Batch 40/64 loss: 0.06264859437942505
Batch 41/64 loss: 0.06748729944229126
Batch 42/64 loss: 0.055341243743896484
Batch 43/64 loss: 0.05886948108673096
Batch 44/64 loss: 0.07235568761825562
Batch 45/64 loss: 0.059343814849853516
Batch 46/64 loss: 0.04641878604888916
Batch 47/64 loss: 0.0703054666519165
Batch 48/64 loss: 0.04549682140350342
Batch 49/64 loss: 0.05901223421096802
Batch 50/64 loss: 0.04511302709579468
Batch 51/64 loss: 0.08145439624786377
Batch 52/64 loss: 0.05024230480194092
Batch 53/64 loss: 0.056060850620269775
Batch 54/64 loss: 0.04239422082901001
Batch 55/64 loss: 0.05493408441543579
Batch 56/64 loss: 0.05516791343688965
Batch 57/64 loss: 0.06465393304824829
Batch 58/64 loss: 0.05228161811828613
Batch 59/64 loss: 0.06457126140594482
Batch 60/64 loss: 0.09261411428451538
Batch 61/64 loss: 0.059999704360961914
Batch 62/64 loss: 0.04684185981750488
Batch 63/64 loss: 0.08604490756988525
Batch 64/64 loss: 0.08358287811279297
Epoch 68  Train loss: 0.058122772328993856  Val loss: 0.10197748924858381
Epoch 69
-------------------------------
Batch 1/64 loss: 0.04942411184310913
Batch 2/64 loss: 0.07304751873016357
Batch 3/64 loss: 0.047309160232543945
Batch 4/64 loss: 0.04938548803329468
Batch 5/64 loss: 0.043711304664611816
Batch 6/64 loss: 0.043678104877471924
Batch 7/64 loss: 0.02888387441635132
Batch 8/64 loss: 0.09912192821502686
Batch 9/64 loss: 0.059285879135131836
Batch 10/64 loss: 0.03857994079589844
Batch 11/64 loss: 0.04778546094894409
Batch 12/64 loss: 0.032483458518981934
Batch 13/64 loss: 0.04833024740219116
Batch 14/64 loss: 0.05654400587081909
Batch 15/64 loss: 0.07324224710464478
Batch 16/64 loss: 0.04282468557357788
Batch 17/64 loss: 0.1000550389289856
Batch 18/64 loss: 0.07779937982559204
Batch 19/64 loss: 0.053717076778411865
Batch 20/64 loss: 0.06743180751800537
Batch 21/64 loss: 0.09826231002807617
Batch 22/64 loss: 0.06703650951385498
Batch 23/64 loss: 0.06136929988861084
Batch 24/64 loss: 0.06058001518249512
Batch 25/64 loss: 0.06561136245727539
Batch 26/64 loss: 0.05823683738708496
Batch 27/64 loss: 0.06305503845214844
Batch 28/64 loss: 0.07532012462615967
Batch 29/64 loss: 0.07028281688690186
Batch 30/64 loss: 0.06414777040481567
Batch 31/64 loss: 0.041889965534210205
Batch 32/64 loss: 0.0600970983505249
Batch 33/64 loss: 0.07327735424041748
Batch 34/64 loss: 0.04754161834716797
Batch 35/64 loss: 0.07708019018173218
Batch 36/64 loss: 0.06403475999832153
Batch 37/64 loss: 0.0580902099609375
Batch 38/64 loss: 0.04845726490020752
Batch 39/64 loss: 0.06595540046691895
Batch 40/64 loss: 0.06755876541137695
Batch 41/64 loss: 0.03850686550140381
Batch 42/64 loss: 0.060747742652893066
Batch 43/64 loss: 0.05474144220352173
Batch 44/64 loss: 0.04750525951385498
Batch 45/64 loss: 0.04947495460510254
Batch 46/64 loss: 0.05149334669113159
Batch 47/64 loss: 0.05202341079711914
Batch 48/64 loss: 0.08701145648956299
Batch 49/64 loss: 0.05376321077346802
Batch 50/64 loss: 0.058480679988861084
Batch 51/64 loss: 0.04267984628677368
Batch 52/64 loss: 0.08404284715652466
Batch 53/64 loss: 0.06959080696105957
Batch 54/64 loss: 0.05331110954284668
Batch 55/64 loss: 0.042822420597076416
Batch 56/64 loss: 0.058101534843444824
Batch 57/64 loss: 0.03870350122451782
Batch 58/64 loss: 0.029623210430145264
Batch 59/64 loss: 0.05302298069000244
Batch 60/64 loss: 0.08037102222442627
Batch 61/64 loss: 0.061563730239868164
Batch 62/64 loss: 0.06863898038864136
Batch 63/64 loss: 0.03490257263183594
Batch 64/64 loss: 0.044243693351745605
Epoch 69  Train loss: 0.05842872367185705  Val loss: 0.09949376247183155
Epoch 70
-------------------------------
Batch 1/64 loss: 0.04234212636947632
Batch 2/64 loss: 0.04784846305847168
Batch 3/64 loss: 0.07994735240936279
Batch 4/64 loss: 0.057252347469329834
Batch 5/64 loss: 0.03452634811401367
Batch 6/64 loss: 0.047670841217041016
Batch 7/64 loss: 0.06320589780807495
Batch 8/64 loss: 0.04300284385681152
Batch 9/64 loss: 0.059677183628082275
Batch 10/64 loss: 0.04723012447357178
Batch 11/64 loss: 0.03902697563171387
Batch 12/64 loss: 0.07055366039276123
Batch 13/64 loss: 0.05249500274658203
Batch 14/64 loss: 0.05879491567611694
Batch 15/64 loss: 0.056731462478637695
Batch 16/64 loss: 0.061393558979034424
Batch 17/64 loss: 0.032844722270965576
Batch 18/64 loss: 0.06268948316574097
Batch 19/64 loss: 0.0750741958618164
Batch 20/64 loss: 0.0868825912475586
Batch 21/64 loss: 0.06387847661972046
Batch 22/64 loss: 0.07310020923614502
Batch 23/64 loss: 0.021599888801574707
Batch 24/64 loss: 0.025754809379577637
Batch 25/64 loss: 0.03592216968536377
Batch 26/64 loss: 0.07508701086044312
Batch 27/64 loss: 0.04875463247299194
Batch 28/64 loss: 0.057688891887664795
Batch 29/64 loss: 0.02925485372543335
Batch 30/64 loss: 0.048863768577575684
Batch 31/64 loss: 0.09603655338287354
Batch 32/64 loss: 0.04566723108291626
Batch 33/64 loss: 0.057740986347198486
Batch 34/64 loss: 0.07350939512252808
Batch 35/64 loss: 0.06817162036895752
Batch 36/64 loss: 0.04616767168045044
Batch 37/64 loss: 0.06879961490631104
Batch 38/64 loss: 0.06725496053695679
Batch 39/64 loss: 0.04515337944030762
Batch 40/64 loss: 0.050751447677612305
Batch 41/64 loss: 0.053222715854644775
Batch 42/64 loss: 0.04820823669433594
Batch 43/64 loss: 0.06676620244979858
Batch 44/64 loss: 0.03006768226623535
Batch 45/64 loss: 0.07707071304321289
Batch 46/64 loss: 0.07591956853866577
Batch 47/64 loss: 0.04686933755874634
Batch 48/64 loss: 0.06070917844772339
Batch 49/64 loss: 0.0349421501159668
Batch 50/64 loss: 0.04144918918609619
Batch 51/64 loss: 0.053268253803253174
Batch 52/64 loss: 0.05432307720184326
Batch 53/64 loss: 0.07579803466796875
Batch 54/64 loss: 0.022870004177093506
Batch 55/64 loss: 0.043415725231170654
Batch 56/64 loss: 0.07772189378738403
Batch 57/64 loss: 0.030568838119506836
Batch 58/64 loss: 0.06632262468338013
Batch 59/64 loss: 0.07936376333236694
Batch 60/64 loss: 0.051381707191467285
Batch 61/64 loss: 0.06100338697433472
Batch 62/64 loss: 0.07391160726547241
Batch 63/64 loss: 0.06199914216995239
Batch 64/64 loss: 0.06589555740356445
Epoch 70  Train loss: 0.055763801873898976  Val loss: 0.09704726258504022
Epoch 71
-------------------------------
Batch 1/64 loss: 0.03353238105773926
Batch 2/64 loss: 0.042383432388305664
Batch 3/64 loss: 0.044352173805236816
Batch 4/64 loss: 0.05442154407501221
Batch 5/64 loss: 0.07484638690948486
Batch 6/64 loss: 0.03326314687728882
Batch 7/64 loss: 0.039154648780822754
Batch 8/64 loss: 0.058554768562316895
Batch 9/64 loss: 0.05344390869140625
Batch 10/64 loss: 0.06095612049102783
Batch 11/64 loss: 0.06027275323867798
Batch 12/64 loss: 0.07001316547393799
Batch 13/64 loss: 0.07068359851837158
Batch 14/64 loss: 0.05645346641540527
Batch 15/64 loss: 0.07509762048721313
Batch 16/64 loss: 0.0442582368850708
Batch 17/64 loss: 0.08883005380630493
Batch 18/64 loss: 0.039076924324035645
Batch 19/64 loss: 0.05759340524673462
Batch 20/64 loss: 0.05085289478302002
Batch 21/64 loss: 0.062456727027893066
Batch 22/64 loss: 0.048239290714263916
Batch 23/64 loss: 0.03477275371551514
Batch 24/64 loss: 0.06435400247573853
Batch 25/64 loss: 0.07495516538619995
Batch 26/64 loss: 0.060688793659210205
Batch 27/64 loss: 0.08410000801086426
Batch 28/64 loss: 0.07538348436355591
Batch 29/64 loss: 0.025600731372833252
Batch 30/64 loss: 0.036258041858673096
Batch 31/64 loss: 0.03975534439086914
Batch 32/64 loss: 0.05760359764099121
Batch 33/64 loss: 0.08722811937332153
Batch 34/64 loss: 0.07475364208221436
Batch 35/64 loss: 0.04147779941558838
Batch 36/64 loss: 0.04493933916091919
Batch 37/64 loss: 0.05760389566421509
Batch 38/64 loss: 0.07500350475311279
Batch 39/64 loss: 0.06097996234893799
Batch 40/64 loss: 0.05978989601135254
Batch 41/64 loss: 0.05366039276123047
Batch 42/64 loss: 0.04537343978881836
Batch 43/64 loss: 0.03815704584121704
Batch 44/64 loss: 0.046482861042022705
Batch 45/64 loss: 0.05441927909851074
Batch 46/64 loss: 0.08420431613922119
Batch 47/64 loss: 0.050086021423339844
Batch 48/64 loss: 0.07448792457580566
Batch 49/64 loss: 0.0579981803894043
Batch 50/64 loss: 0.0532260537147522
Batch 51/64 loss: 0.07573765516281128
Batch 52/64 loss: 0.032217204570770264
Batch 53/64 loss: 0.06620389223098755
Batch 54/64 loss: 0.07168489694595337
Batch 55/64 loss: 0.0382651686668396
Batch 56/64 loss: 0.05428212881088257
Batch 57/64 loss: 0.06281697750091553
Batch 58/64 loss: 0.05091822147369385
Batch 59/64 loss: 0.056369006633758545
Batch 60/64 loss: 0.041717350482940674
Batch 61/64 loss: 0.08252191543579102
Batch 62/64 loss: 0.027250945568084717
Batch 63/64 loss: 0.04246389865875244
Batch 64/64 loss: 0.029032468795776367
Epoch 71  Train loss: 0.05578515856873755  Val loss: 0.09795090320593712
Epoch 72
-------------------------------
Batch 1/64 loss: 0.045072078704833984
Batch 2/64 loss: 0.04688382148742676
Batch 3/64 loss: 0.041278958320617676
Batch 4/64 loss: 0.07418012619018555
Batch 5/64 loss: 0.07676035165786743
Batch 6/64 loss: 0.06276071071624756
Batch 7/64 loss: 0.05945003032684326
Batch 8/64 loss: 0.04146295785903931
Batch 9/64 loss: 0.0403217077255249
Batch 10/64 loss: 0.03887450695037842
Batch 11/64 loss: 0.02505505084991455
Batch 12/64 loss: 0.05158877372741699
Batch 13/64 loss: 0.05208313465118408
Batch 14/64 loss: 0.046600520610809326
Batch 15/64 loss: 0.05460011959075928
Batch 16/64 loss: 0.05066126585006714
Batch 17/64 loss: 0.040834009647369385
Batch 18/64 loss: 0.05648040771484375
Batch 19/64 loss: 0.051693737506866455
Batch 20/64 loss: 0.043271422386169434
Batch 21/64 loss: 0.04772216081619263
Batch 22/64 loss: 0.042860984802246094
Batch 23/64 loss: 0.051205337047576904
Batch 24/64 loss: 0.06510031223297119
Batch 25/64 loss: 0.06716799736022949
Batch 26/64 loss: 0.06112372875213623
Batch 27/64 loss: 0.05095893144607544
Batch 28/64 loss: 0.05131500959396362
Batch 29/64 loss: 0.04986560344696045
Batch 30/64 loss: 0.04154002666473389
Batch 31/64 loss: 0.05558532476425171
Batch 32/64 loss: 0.05141258239746094
Batch 33/64 loss: 0.04978668689727783
Batch 34/64 loss: 0.04864192008972168
Batch 35/64 loss: 0.05135762691497803
Batch 36/64 loss: 0.05932462215423584
Batch 37/64 loss: 0.03657656908035278
Batch 38/64 loss: 0.0762985348701477
Batch 39/64 loss: 0.07633990049362183
Batch 40/64 loss: 0.05380821228027344
Batch 41/64 loss: 0.063507080078125
Batch 42/64 loss: 0.08218085765838623
Batch 43/64 loss: 0.054574549198150635
Batch 44/64 loss: 0.05580848455429077
Batch 45/64 loss: 0.061193108558654785
Batch 46/64 loss: 0.08162951469421387
Batch 47/64 loss: 0.050267934799194336
Batch 48/64 loss: 0.052959978580474854
Batch 49/64 loss: 0.059231340885162354
Batch 50/64 loss: 0.048025667667388916
Batch 51/64 loss: 0.08156222105026245
Batch 52/64 loss: 0.0594559907913208
Batch 53/64 loss: 0.05616486072540283
Batch 54/64 loss: 0.03541886806488037
Batch 55/64 loss: 0.0336604118347168
Batch 56/64 loss: 0.056936681270599365
Batch 57/64 loss: 0.03074181079864502
Batch 58/64 loss: 0.06158936023712158
Batch 59/64 loss: 0.039051711559295654
Batch 60/64 loss: 0.051506757736206055
Batch 61/64 loss: 0.0541190505027771
Batch 62/64 loss: 0.04981684684753418
Batch 63/64 loss: 0.05474448204040527
Batch 64/64 loss: 0.048590660095214844
Epoch 72  Train loss: 0.05330974634955911  Val loss: 0.10110551856227756
Epoch 73
-------------------------------
Batch 1/64 loss: 0.05356842279434204
Batch 2/64 loss: 0.056697726249694824
Batch 3/64 loss: 0.06314557790756226
Batch 4/64 loss: 0.05856060981750488
Batch 5/64 loss: 0.02761685848236084
Batch 6/64 loss: 0.03484511375427246
Batch 7/64 loss: 0.06385171413421631
Batch 8/64 loss: 0.06557208299636841
Batch 9/64 loss: 0.09325605630874634
Batch 10/64 loss: 0.0673409104347229
Batch 11/64 loss: 0.043120384216308594
Batch 12/64 loss: 0.048175930976867676
Batch 13/64 loss: 0.02205634117126465
Batch 14/64 loss: 0.06274640560150146
Batch 15/64 loss: 0.07236170768737793
Batch 16/64 loss: 0.02617788314819336
Batch 17/64 loss: 0.0427057147026062
Batch 18/64 loss: 0.07404065132141113
Batch 19/64 loss: 0.057152390480041504
Batch 20/64 loss: 0.05552476644515991
Batch 21/64 loss: 0.04187566041946411
Batch 22/64 loss: 0.041228532791137695
Batch 23/64 loss: 0.05900013446807861
Batch 24/64 loss: 0.07321023941040039
Batch 25/64 loss: 0.0701403021812439
Batch 26/64 loss: 0.05721968412399292
Batch 27/64 loss: 0.04937314987182617
Batch 28/64 loss: 0.09645295143127441
Batch 29/64 loss: 0.05061638355255127
Batch 30/64 loss: 0.07728791236877441
Batch 31/64 loss: 0.05588042736053467
Batch 32/64 loss: 0.08009588718414307
Batch 33/64 loss: 0.043522000312805176
Batch 34/64 loss: 0.04307889938354492
Batch 35/64 loss: 0.042181432247161865
Batch 36/64 loss: 0.03169608116149902
Batch 37/64 loss: 0.03644496202468872
Batch 38/64 loss: 0.05788838863372803
Batch 39/64 loss: 0.07946056127548218
Batch 40/64 loss: 0.06197124719619751
Batch 41/64 loss: 0.04200786352157593
Batch 42/64 loss: 0.033554255962371826
Batch 43/64 loss: 0.04859954118728638
Batch 44/64 loss: 0.0261688232421875
Batch 45/64 loss: 0.04108405113220215
Batch 46/64 loss: 0.05148953199386597
Batch 47/64 loss: 0.0628710389137268
Batch 48/64 loss: 0.0286940336227417
Batch 49/64 loss: 0.04138016700744629
Batch 50/64 loss: 0.05413144826889038
Batch 51/64 loss: 0.07235109806060791
Batch 52/64 loss: 0.06170487403869629
Batch 53/64 loss: 0.05042058229446411
Batch 54/64 loss: 0.03238821029663086
Batch 55/64 loss: 0.037885963916778564
Batch 56/64 loss: 0.0747535228729248
Batch 57/64 loss: 0.06173807382583618
Batch 58/64 loss: 0.09995824098587036
Batch 59/64 loss: 0.040199220180511475
Batch 60/64 loss: 0.050170302391052246
Batch 61/64 loss: 0.03969013690948486
Batch 62/64 loss: 0.04739093780517578
Batch 63/64 loss: 0.07467931509017944
Batch 64/64 loss: 0.04975801706314087
Epoch 73  Train loss: 0.05408269540936339  Val loss: 0.09111455318444374
Saving best model, epoch: 73
Epoch 74
-------------------------------
Batch 1/64 loss: 0.034620583057403564
Batch 2/64 loss: 0.06529176235198975
Batch 3/64 loss: 0.07090157270431519
Batch 4/64 loss: 0.059906721115112305
Batch 5/64 loss: 0.03840351104736328
Batch 6/64 loss: 0.054048895835876465
Batch 7/64 loss: 0.05220162868499756
Batch 8/64 loss: 0.07399344444274902
Batch 9/64 loss: 0.023186147212982178
Batch 10/64 loss: 0.02202165126800537
Batch 11/64 loss: 0.0713357925415039
Batch 12/64 loss: 0.058556556701660156
Batch 13/64 loss: 0.038404226303100586
Batch 14/64 loss: 0.039374589920043945
Batch 15/64 loss: 0.0343896746635437
Batch 16/64 loss: 0.028285503387451172
Batch 17/64 loss: 0.036041855812072754
Batch 18/64 loss: 0.037457823753356934
Batch 19/64 loss: 0.06334125995635986
Batch 20/64 loss: 0.08748352527618408
Batch 21/64 loss: 0.033981919288635254
Batch 22/64 loss: 0.05040556192398071
Batch 23/64 loss: 0.05127447843551636
Batch 24/64 loss: 0.06293284893035889
Batch 25/64 loss: 0.056831300258636475
Batch 26/64 loss: 0.03581970930099487
Batch 27/64 loss: 0.05706411600112915
Batch 28/64 loss: 0.06564056873321533
Batch 29/64 loss: 0.06309157609939575
Batch 30/64 loss: 0.047854602336883545
Batch 31/64 loss: 0.03499990701675415
Batch 32/64 loss: 0.06764805316925049
Batch 33/64 loss: 0.06577152013778687
Batch 34/64 loss: 0.05321604013442993
Batch 35/64 loss: 0.04771298170089722
Batch 36/64 loss: 0.03402376174926758
Batch 37/64 loss: 0.07325762510299683
Batch 38/64 loss: 0.04620623588562012
Batch 39/64 loss: 0.06641125679016113
Batch 40/64 loss: 0.06105029582977295
Batch 41/64 loss: 0.04265636205673218
Batch 42/64 loss: 0.042668044567108154
Batch 43/64 loss: 0.06899499893188477
Batch 44/64 loss: 0.04821521043777466
Batch 45/64 loss: 0.04833793640136719
Batch 46/64 loss: 0.05230981111526489
Batch 47/64 loss: 0.042453229427337646
Batch 48/64 loss: 0.05654257535934448
Batch 49/64 loss: 0.06262201070785522
Batch 50/64 loss: 0.05533343553543091
Batch 51/64 loss: 0.04803597927093506
Batch 52/64 loss: 0.07006466388702393
Batch 53/64 loss: 0.046895742416381836
Batch 54/64 loss: 0.04565131664276123
Batch 55/64 loss: 0.07140278816223145
Batch 56/64 loss: 0.06483256816864014
Batch 57/64 loss: 0.05955725908279419
Batch 58/64 loss: 0.0359346866607666
Batch 59/64 loss: 0.04937922954559326
Batch 60/64 loss: 0.07236099243164062
Batch 61/64 loss: 0.04148662090301514
Batch 62/64 loss: 0.039591073989868164
Batch 63/64 loss: 0.039606571197509766
Batch 64/64 loss: 0.012837111949920654
Epoch 74  Train loss: 0.05143532589370129  Val loss: 0.0969493460819074
Epoch 75
-------------------------------
Batch 1/64 loss: 0.06500875949859619
Batch 2/64 loss: 0.04719424247741699
Batch 3/64 loss: 0.03235471248626709
Batch 4/64 loss: 0.044161319732666016
Batch 5/64 loss: 0.03302711248397827
Batch 6/64 loss: 0.032125115394592285
Batch 7/64 loss: 0.0672866702079773
Batch 8/64 loss: 0.05374729633331299
Batch 9/64 loss: 0.0136793851852417
Batch 10/64 loss: 0.042141616344451904
Batch 11/64 loss: 0.05917555093765259
Batch 12/64 loss: 0.04752689599990845
Batch 13/64 loss: 0.05517709255218506
Batch 14/64 loss: 0.050568461418151855
Batch 15/64 loss: 0.06233632564544678
Batch 16/64 loss: 0.0569416880607605
Batch 17/64 loss: 0.033555805683135986
Batch 18/64 loss: 0.025990843772888184
Batch 19/64 loss: 0.06319516897201538
Batch 20/64 loss: 0.03897637128829956
Batch 21/64 loss: 0.03814685344696045
Batch 22/64 loss: 0.05067574977874756
Batch 23/64 loss: 0.07050979137420654
Batch 24/64 loss: 0.061794817447662354
Batch 25/64 loss: 0.0374714732170105
Batch 26/64 loss: 0.05895364284515381
Batch 27/64 loss: 0.03136920928955078
Batch 28/64 loss: 0.04993325471878052
Batch 29/64 loss: 0.04598432779312134
Batch 30/64 loss: 0.04729431867599487
Batch 31/64 loss: 0.06698918342590332
Batch 32/64 loss: 0.0539589524269104
Batch 33/64 loss: 0.05143171548843384
Batch 34/64 loss: 0.04105794429779053
Batch 35/64 loss: 0.04819685220718384
Batch 36/64 loss: 0.0705270767211914
Batch 37/64 loss: 0.056437671184539795
Batch 38/64 loss: 0.04546189308166504
Batch 39/64 loss: 0.059983134269714355
Batch 40/64 loss: 0.05059492588043213
Batch 41/64 loss: 0.052590250968933105
Batch 42/64 loss: 0.09410327672958374
Batch 43/64 loss: 0.07658928632736206
Batch 44/64 loss: 0.05903667211532593
Batch 45/64 loss: 0.05299443006515503
Batch 46/64 loss: 0.042797207832336426
Batch 47/64 loss: 0.05574476718902588
Batch 48/64 loss: 0.05244642496109009
Batch 49/64 loss: 0.0369834303855896
Batch 50/64 loss: 0.01566493511199951
Batch 51/64 loss: 0.021481692790985107
Batch 52/64 loss: 0.0678640604019165
Batch 53/64 loss: 0.056040823459625244
Batch 54/64 loss: 0.06056159734725952
Batch 55/64 loss: 0.04084193706512451
Batch 56/64 loss: 0.05947446823120117
Batch 57/64 loss: 0.05369192361831665
Batch 58/64 loss: 0.06629586219787598
Batch 59/64 loss: 0.035919249057769775
Batch 60/64 loss: 0.05813944339752197
Batch 61/64 loss: 0.06354087591171265
Batch 62/64 loss: 0.07855772972106934
Batch 63/64 loss: 0.06474965810775757
Batch 64/64 loss: 0.09501504898071289
Epoch 75  Train loss: 0.05173826685138777  Val loss: 0.09192544201395356
Epoch 76
-------------------------------
Batch 1/64 loss: 0.07794499397277832
Batch 2/64 loss: 0.04160338640213013
Batch 3/64 loss: 0.07183998823165894
Batch 4/64 loss: 0.03895300626754761
Batch 5/64 loss: 0.05618959665298462
Batch 6/64 loss: 0.03628730773925781
Batch 7/64 loss: 0.07095992565155029
Batch 8/64 loss: 0.04001086950302124
Batch 9/64 loss: 0.04869765043258667
Batch 10/64 loss: 0.07337349653244019
Batch 11/64 loss: 0.045539259910583496
Batch 12/64 loss: 0.04269057512283325
Batch 13/64 loss: 0.02813267707824707
Batch 14/64 loss: 0.04708653688430786
Batch 15/64 loss: 0.03662675619125366
Batch 16/64 loss: 0.05328333377838135
Batch 17/64 loss: 0.022809743881225586
Batch 18/64 loss: 0.05853378772735596
Batch 19/64 loss: 0.04818040132522583
Batch 20/64 loss: 0.07515347003936768
Batch 21/64 loss: 0.05384647846221924
Batch 22/64 loss: 0.05649244785308838
Batch 23/64 loss: 0.046949923038482666
Batch 24/64 loss: 0.04319947957992554
Batch 25/64 loss: 0.059042274951934814
Batch 26/64 loss: 0.0681200623512268
Batch 27/64 loss: 0.0618206262588501
Batch 28/64 loss: 0.06302964687347412
Batch 29/64 loss: 0.05671346187591553
Batch 30/64 loss: 0.017325162887573242
Batch 31/64 loss: 0.05129420757293701
Batch 32/64 loss: 0.06834757328033447
Batch 33/64 loss: 0.053151488304138184
Batch 34/64 loss: 0.04851818084716797
Batch 35/64 loss: 0.049146294593811035
Batch 36/64 loss: 0.0583343505859375
Batch 37/64 loss: 0.030254244804382324
Batch 38/64 loss: 0.047725796699523926
Batch 39/64 loss: 0.059308648109436035
Batch 40/64 loss: 0.05713373422622681
Batch 41/64 loss: 0.02835214138031006
Batch 42/64 loss: 0.0745089054107666
Batch 43/64 loss: 0.06195920705795288
Batch 44/64 loss: 0.03170311450958252
Batch 45/64 loss: 0.056847453117370605
Batch 46/64 loss: 0.02768254280090332
Batch 47/64 loss: 0.03561532497406006
Batch 48/64 loss: 0.06059575080871582
Batch 49/64 loss: 0.0385320782661438
Batch 50/64 loss: 0.027473270893096924
Batch 51/64 loss: 0.04987967014312744
Batch 52/64 loss: 0.0359196662902832
Batch 53/64 loss: 0.02565169334411621
Batch 54/64 loss: 0.034090399742126465
Batch 55/64 loss: 0.038087427616119385
Batch 56/64 loss: 0.031964778900146484
Batch 57/64 loss: 0.04870384931564331
Batch 58/64 loss: 0.051584482192993164
Batch 59/64 loss: 0.06542408466339111
Batch 60/64 loss: 0.0642818808555603
Batch 61/64 loss: 0.040435850620269775
Batch 62/64 loss: 0.05347847938537598
Batch 63/64 loss: 0.056904137134552
Batch 64/64 loss: 0.051926612854003906
Epoch 76  Train loss: 0.04929054110657935  Val loss: 0.10374881844340321
Epoch 77
-------------------------------
Batch 1/64 loss: 0.06062471866607666
Batch 2/64 loss: 0.044281840324401855
Batch 3/64 loss: 0.041125714778900146
Batch 4/64 loss: 0.061389803886413574
Batch 5/64 loss: 0.06244766712188721
Batch 6/64 loss: 0.028295695781707764
Batch 7/64 loss: 0.04299294948577881
Batch 8/64 loss: 0.03918510675430298
Batch 9/64 loss: 0.031735122203826904
Batch 10/64 loss: 0.06371253728866577
Batch 11/64 loss: 0.041526734828948975
Batch 12/64 loss: 0.05936020612716675
Batch 13/64 loss: 0.04425382614135742
Batch 14/64 loss: 0.03371018171310425
Batch 15/64 loss: 0.04968893527984619
Batch 16/64 loss: 0.03913712501525879
Batch 17/64 loss: 0.04888796806335449
Batch 18/64 loss: 0.05310696363449097
Batch 19/64 loss: 0.023891985416412354
Batch 20/64 loss: 0.0429646372795105
Batch 21/64 loss: 0.055846452713012695
Batch 22/64 loss: 0.020312190055847168
Batch 23/64 loss: 0.045941293239593506
Batch 24/64 loss: 0.02297288179397583
Batch 25/64 loss: 0.03689754009246826
Batch 26/64 loss: 0.04991215467453003
Batch 27/64 loss: 0.045540690422058105
Batch 28/64 loss: 0.04138636589050293
Batch 29/64 loss: 0.028696417808532715
Batch 30/64 loss: 0.0566716194152832
Batch 31/64 loss: 0.04419982433319092
Batch 32/64 loss: 0.034439682960510254
Batch 33/64 loss: 0.06375324726104736
Batch 34/64 loss: 0.05338627099990845
Batch 35/64 loss: 0.042182981967926025
Batch 36/64 loss: 0.04905533790588379
Batch 37/64 loss: 0.03372490406036377
Batch 38/64 loss: 0.05909430980682373
Batch 39/64 loss: 0.0685509443283081
Batch 40/64 loss: 0.058975934982299805
Batch 41/64 loss: 0.06112790107727051
Batch 42/64 loss: 0.03857457637786865
Batch 43/64 loss: 0.06396210193634033
Batch 44/64 loss: 0.05089205503463745
Batch 45/64 loss: 0.06344056129455566
Batch 46/64 loss: 0.049486756324768066
Batch 47/64 loss: 0.05815368890762329
Batch 48/64 loss: 0.05414217710494995
Batch 49/64 loss: 0.06431573629379272
Batch 50/64 loss: 0.042567551136016846
Batch 51/64 loss: 0.03612649440765381
Batch 52/64 loss: 0.05955827236175537
Batch 53/64 loss: 0.04736262559890747
Batch 54/64 loss: 0.051180362701416016
Batch 55/64 loss: 0.03806585073471069
Batch 56/64 loss: 0.04061704874038696
Batch 57/64 loss: 0.049202680587768555
Batch 58/64 loss: 0.06449145078659058
Batch 59/64 loss: 0.063595712184906
Batch 60/64 loss: 0.07813996076583862
Batch 61/64 loss: 0.03128373622894287
Batch 62/64 loss: 0.05805706977844238
Batch 63/64 loss: 0.053974270820617676
Batch 64/64 loss: 0.022356808185577393
Epoch 77  Train loss: 0.04798348253848506  Val loss: 0.08969513902959135
Saving best model, epoch: 77
Epoch 78
-------------------------------
Batch 1/64 loss: 0.058417320251464844
Batch 2/64 loss: 0.058222055435180664
Batch 3/64 loss: 0.06610369682312012
Batch 4/64 loss: 0.02670377492904663
Batch 5/64 loss: 0.06976652145385742
Batch 6/64 loss: 0.02954179048538208
Batch 7/64 loss: 0.015476584434509277
Batch 8/64 loss: 0.04708188772201538
Batch 9/64 loss: 0.06721168756484985
Batch 10/64 loss: 0.041480958461761475
Batch 11/64 loss: 0.034040987491607666
Batch 12/64 loss: 0.04981106519699097
Batch 13/64 loss: 0.04988652467727661
Batch 14/64 loss: 0.021978795528411865
Batch 15/64 loss: 0.03302377462387085
Batch 16/64 loss: 0.04182213544845581
Batch 17/64 loss: 0.05600738525390625
Batch 18/64 loss: 0.0309450626373291
Batch 19/64 loss: 0.0441402792930603
Batch 20/64 loss: 0.05680191516876221
Batch 21/64 loss: 0.04338008165359497
Batch 22/64 loss: 0.04181087017059326
Batch 23/64 loss: 0.08602184057235718
Batch 24/64 loss: 0.02042543888092041
Batch 25/64 loss: 0.028349995613098145
Batch 26/64 loss: 0.0524325966835022
Batch 27/64 loss: 0.02550053596496582
Batch 28/64 loss: 0.08178412914276123
Batch 29/64 loss: 0.03297734260559082
Batch 30/64 loss: 0.047140657901763916
Batch 31/64 loss: 0.05649697780609131
Batch 32/64 loss: 0.028793692588806152
Batch 33/64 loss: 0.06920534372329712
Batch 34/64 loss: 0.06623756885528564
Batch 35/64 loss: 0.055519044399261475
Batch 36/64 loss: 0.03529471158981323
Batch 37/64 loss: 0.044125497341156006
Batch 38/64 loss: 0.026139438152313232
Batch 39/64 loss: 0.05356931686401367
Batch 40/64 loss: 0.03498709201812744
Batch 41/64 loss: 0.0474247932434082
Batch 42/64 loss: 0.023359060287475586
Batch 43/64 loss: 0.05518686771392822
Batch 44/64 loss: 0.05081731081008911
Batch 45/64 loss: 0.04966932535171509
Batch 46/64 loss: 0.043551087379455566
Batch 47/64 loss: 0.05239689350128174
Batch 48/64 loss: 0.03813290596008301
Batch 49/64 loss: 0.034989356994628906
Batch 50/64 loss: 0.05043911933898926
Batch 51/64 loss: 0.06791955232620239
Batch 52/64 loss: 0.057899653911590576
Batch 53/64 loss: 0.054843246936798096
Batch 54/64 loss: 0.04670405387878418
Batch 55/64 loss: 0.03111398220062256
Batch 56/64 loss: 0.04222595691680908
Batch 57/64 loss: 0.06503504514694214
Batch 58/64 loss: 0.06153339147567749
Batch 59/64 loss: 0.04269760847091675
Batch 60/64 loss: 0.03633195161819458
Batch 61/64 loss: 0.065513014793396
Batch 62/64 loss: 0.029277801513671875
Batch 63/64 loss: 0.028232574462890625
Batch 64/64 loss: 0.021106600761413574
Epoch 78  Train loss: 0.045800484395494646  Val loss: 0.0915469824243657
Epoch 79
-------------------------------
Batch 1/64 loss: 0.06713354587554932
Batch 2/64 loss: 0.051987648010253906
Batch 3/64 loss: 0.033917248249053955
Batch 4/64 loss: 0.031010150909423828
Batch 5/64 loss: 0.011594057083129883
Batch 6/64 loss: 0.03874671459197998
Batch 7/64 loss: 0.04821294546127319
Batch 8/64 loss: 0.07794857025146484
Batch 9/64 loss: 0.038200199604034424
Batch 10/64 loss: 0.03677165508270264
Batch 11/64 loss: 0.04032111167907715
Batch 12/64 loss: 0.05158287286758423
Batch 13/64 loss: 0.03252631425857544
Batch 14/64 loss: 0.03604018688201904
Batch 15/64 loss: 0.05991905927658081
Batch 16/64 loss: 0.039789438247680664
Batch 17/64 loss: 0.03196150064468384
Batch 18/64 loss: 0.030919373035430908
Batch 19/64 loss: 0.03184705972671509
Batch 20/64 loss: 0.04100614786148071
Batch 21/64 loss: 0.040994346141815186
Batch 22/64 loss: 0.03446239233016968
Batch 23/64 loss: 0.037825942039489746
Batch 24/64 loss: 0.038797974586486816
Batch 25/64 loss: 0.06917810440063477
Batch 26/64 loss: 0.04583209753036499
Batch 27/64 loss: 0.03532814979553223
Batch 28/64 loss: 0.04125070571899414
Batch 29/64 loss: 0.03680765628814697
Batch 30/64 loss: 0.023833632469177246
Batch 31/64 loss: 0.06090366840362549
Batch 32/64 loss: 0.03870910406112671
Batch 33/64 loss: 0.05862849950790405
Batch 34/64 loss: 0.03810936212539673
Batch 35/64 loss: 0.07089918851852417
Batch 36/64 loss: 0.0488705039024353
Batch 37/64 loss: 0.05591166019439697
Batch 38/64 loss: 0.030237197875976562
Batch 39/64 loss: 0.06649702787399292
Batch 40/64 loss: 0.055459678173065186
Batch 41/64 loss: 0.03029775619506836
Batch 42/64 loss: 0.01973891258239746
Batch 43/64 loss: 0.0673363208770752
Batch 44/64 loss: 0.04612654447555542
Batch 45/64 loss: 0.05347567796707153
Batch 46/64 loss: 0.061830997467041016
Batch 47/64 loss: 0.06330037117004395
Batch 48/64 loss: 0.020053505897521973
Batch 49/64 loss: 0.045433759689331055
Batch 50/64 loss: 0.057250142097473145
Batch 51/64 loss: 0.05444896221160889
Batch 52/64 loss: 0.04446971416473389
Batch 53/64 loss: 0.05235397815704346
Batch 54/64 loss: 0.05228567123413086
Batch 55/64 loss: 0.07899904251098633
Batch 56/64 loss: 0.05412691831588745
Batch 57/64 loss: 0.06337004899978638
Batch 58/64 loss: 0.0745469331741333
Batch 59/64 loss: 0.0507240891456604
Batch 60/64 loss: 0.04098093509674072
Batch 61/64 loss: 0.023323357105255127
Batch 62/64 loss: 0.07027602195739746
Batch 63/64 loss: 0.05659729242324829
Batch 64/64 loss: 0.03569525480270386
Epoch 79  Train loss: 0.04655829153808893  Val loss: 0.097026453804724
Epoch 80
-------------------------------
Batch 1/64 loss: 0.05657839775085449
Batch 2/64 loss: 0.04272109270095825
Batch 3/64 loss: 0.04789471626281738
Batch 4/64 loss: 0.03405356407165527
Batch 5/64 loss: 0.04057997465133667
Batch 6/64 loss: 0.03916686773300171
Batch 7/64 loss: 0.05761080980300903
Batch 8/64 loss: 0.03213590383529663
Batch 9/64 loss: 0.02529698610305786
Batch 10/64 loss: 0.053633689880371094
Batch 11/64 loss: 0.03226959705352783
Batch 12/64 loss: 0.05774843692779541
Batch 13/64 loss: 0.043132781982421875
Batch 14/64 loss: 0.042266786098480225
Batch 15/64 loss: 0.048689186573028564
Batch 16/64 loss: 0.047638893127441406
Batch 17/64 loss: 0.02421814203262329
Batch 18/64 loss: 0.036887288093566895
Batch 19/64 loss: 0.03994452953338623
Batch 20/64 loss: 0.05922353267669678
Batch 21/64 loss: 0.041702449321746826
Batch 22/64 loss: 0.045497000217437744
Batch 23/64 loss: 0.02428513765335083
Batch 24/64 loss: 0.04958420991897583
Batch 25/64 loss: 0.0288693904876709
Batch 26/64 loss: 0.06306713819503784
Batch 27/64 loss: 0.03594255447387695
Batch 28/64 loss: 0.043166935443878174
Batch 29/64 loss: 0.037352561950683594
Batch 30/64 loss: 0.05017542839050293
Batch 31/64 loss: 0.036357998847961426
Batch 32/64 loss: 0.06279021501541138
Batch 33/64 loss: 0.04833132028579712
Batch 34/64 loss: 0.07166910171508789
Batch 35/64 loss: 0.05534261465072632
Batch 36/64 loss: 0.02318328619003296
Batch 37/64 loss: 0.02998340129852295
Batch 38/64 loss: 0.03366351127624512
Batch 39/64 loss: 0.02894914150238037
Batch 40/64 loss: 0.055483222007751465
Batch 41/64 loss: 0.03599816560745239
Batch 42/64 loss: 0.03137218952178955
Batch 43/64 loss: 0.02361053228378296
Batch 44/64 loss: 0.05372881889343262
Batch 45/64 loss: 0.05698990821838379
Batch 46/64 loss: 0.07226192951202393
Batch 47/64 loss: 0.059245526790618896
Batch 48/64 loss: 0.03837841749191284
Batch 49/64 loss: 0.06262189149856567
Batch 50/64 loss: 0.02857828140258789
Batch 51/64 loss: 0.05627930164337158
Batch 52/64 loss: 0.032083988189697266
Batch 53/64 loss: 0.03932148218154907
Batch 54/64 loss: 0.033305227756500244
Batch 55/64 loss: 0.03499561548233032
Batch 56/64 loss: 0.032360970973968506
Batch 57/64 loss: 0.04326444864273071
Batch 58/64 loss: 0.06445562839508057
Batch 59/64 loss: 0.06539022922515869
Batch 60/64 loss: 0.04427027702331543
Batch 61/64 loss: 0.06409454345703125
Batch 62/64 loss: 0.07777845859527588
Batch 63/64 loss: 0.037253618240356445
Batch 64/64 loss: 0.039456963539123535
Epoch 80  Train loss: 0.04461678383397121  Val loss: 0.08575232680310908
Saving best model, epoch: 80
Epoch 81
-------------------------------
Batch 1/64 loss: 0.06680935621261597
Batch 2/64 loss: 0.03609222173690796
Batch 3/64 loss: 0.04862457513809204
Batch 4/64 loss: 0.03181600570678711
Batch 5/64 loss: 0.02700650691986084
Batch 6/64 loss: 0.04337185621261597
Batch 7/64 loss: 0.052000463008880615
Batch 8/64 loss: 0.031661033630371094
Batch 9/64 loss: 0.016732990741729736
Batch 10/64 loss: 0.06499242782592773
Batch 11/64 loss: 0.042439937591552734
Batch 12/64 loss: 0.04517662525177002
Batch 13/64 loss: 0.04947692155838013
Batch 14/64 loss: 0.04573166370391846
Batch 15/64 loss: 0.026713252067565918
Batch 16/64 loss: 0.05983006954193115
Batch 17/64 loss: 0.03820812702178955
Batch 18/64 loss: 0.05497771501541138
Batch 19/64 loss: 0.030065298080444336
Batch 20/64 loss: 0.06806105375289917
Batch 21/64 loss: 0.046491265296936035
Batch 22/64 loss: 0.0579569935798645
Batch 23/64 loss: 0.05510371923446655
Batch 24/64 loss: 0.07570427656173706
Batch 25/64 loss: 0.06741732358932495
Batch 26/64 loss: 0.03567194938659668
Batch 27/64 loss: 0.045143187046051025
Batch 28/64 loss: 0.03543120622634888
Batch 29/64 loss: 0.06098651885986328
Batch 30/64 loss: 0.0402873158454895
Batch 31/64 loss: 0.07631582021713257
Batch 32/64 loss: 0.014031529426574707
Batch 33/64 loss: 0.03643757104873657
Batch 34/64 loss: 0.03954094648361206
Batch 35/64 loss: 0.06233173608779907
Batch 36/64 loss: 0.03883880376815796
Batch 37/64 loss: 0.06808876991271973
Batch 38/64 loss: 0.047327280044555664
Batch 39/64 loss: 0.03614699840545654
Batch 40/64 loss: 0.010928213596343994
Batch 41/64 loss: 0.04364246129989624
Batch 42/64 loss: 0.04729104042053223
Batch 43/64 loss: 0.026772260665893555
Batch 44/64 loss: 0.04863548278808594
Batch 45/64 loss: 0.0542026162147522
Batch 46/64 loss: 0.05288463830947876
Batch 47/64 loss: 0.04218697547912598
Batch 48/64 loss: 0.027001380920410156
Batch 49/64 loss: 0.0630185604095459
Batch 50/64 loss: 0.04356741905212402
Batch 51/64 loss: 0.045819759368896484
Batch 52/64 loss: 0.02725231647491455
Batch 53/64 loss: 0.034123241901397705
Batch 54/64 loss: 0.04866516590118408
Batch 55/64 loss: 0.039084017276763916
Batch 56/64 loss: 0.01120525598526001
Batch 57/64 loss: 0.04951608180999756
Batch 58/64 loss: 0.04155421257019043
Batch 59/64 loss: 0.04305112361907959
Batch 60/64 loss: 0.02211284637451172
Batch 61/64 loss: 0.03388690948486328
Batch 62/64 loss: 0.030100464820861816
Batch 63/64 loss: 0.027558088302612305
Batch 64/64 loss: 0.041451334953308105
Epoch 81  Train loss: 0.04335984856474633  Val loss: 0.0950305539717789
Epoch 82
-------------------------------
Batch 1/64 loss: 0.048983991146087646
Batch 2/64 loss: 0.0413970947265625
Batch 3/64 loss: 0.0736493468284607
Batch 4/64 loss: 0.03829681873321533
Batch 5/64 loss: 0.029578804969787598
Batch 6/64 loss: 0.0576099157333374
Batch 7/64 loss: 0.04920530319213867
Batch 8/64 loss: 0.03490030765533447
Batch 9/64 loss: 0.042882442474365234
Batch 10/64 loss: 0.04120069742202759
Batch 11/64 loss: 0.04924964904785156
Batch 12/64 loss: 0.03441041707992554
Batch 13/64 loss: 0.06681913137435913
Batch 14/64 loss: 0.045814335346221924
Batch 15/64 loss: 0.07026779651641846
Batch 16/64 loss: 0.06116068363189697
Batch 17/64 loss: 0.041912734508514404
Batch 18/64 loss: 0.030988693237304688
Batch 19/64 loss: 0.03550910949707031
Batch 20/64 loss: 0.038073599338531494
Batch 21/64 loss: 0.05276894569396973
Batch 22/64 loss: 0.04951608180999756
Batch 23/64 loss: 0.031125545501708984
Batch 24/64 loss: 0.04317331314086914
Batch 25/64 loss: 0.05056887865066528
Batch 26/64 loss: 0.048082709312438965
Batch 27/64 loss: 0.06325262784957886
Batch 28/64 loss: 0.031017422676086426
Batch 29/64 loss: 0.049556851387023926
Batch 30/64 loss: 0.04227781295776367
Batch 31/64 loss: 0.032668113708496094
Batch 32/64 loss: 0.03308528661727905
Batch 33/64 loss: 0.05562931299209595
Batch 34/64 loss: 0.044867873191833496
Batch 35/64 loss: 0.040174901485443115
Batch 36/64 loss: 0.07017719745635986
Batch 37/64 loss: 0.02753889560699463
Batch 38/64 loss: 0.07055819034576416
Batch 39/64 loss: 0.04732996225357056
Batch 40/64 loss: 0.029231369495391846
Batch 41/64 loss: 0.07187926769256592
Batch 42/64 loss: 0.04522639513015747
Batch 43/64 loss: 0.012725591659545898
Batch 44/64 loss: 0.03709232807159424
Batch 45/64 loss: 0.039624154567718506
Batch 46/64 loss: 0.06493008136749268
Batch 47/64 loss: 0.04330664873123169
Batch 48/64 loss: 0.018795490264892578
Batch 49/64 loss: 0.01551353931427002
Batch 50/64 loss: 0.04358571767807007
Batch 51/64 loss: 0.05650395154953003
Batch 52/64 loss: 0.03669470548629761
Batch 53/64 loss: 0.0064681172370910645
Batch 54/64 loss: 0.05972212553024292
Batch 55/64 loss: 0.03565835952758789
Batch 56/64 loss: 0.03986084461212158
Batch 57/64 loss: 0.015453517436981201
Batch 58/64 loss: 0.02956300973892212
Batch 59/64 loss: 0.04766714572906494
Batch 60/64 loss: 0.050009965896606445
Batch 61/64 loss: 0.042142510414123535
Batch 62/64 loss: 0.05668783187866211
Batch 63/64 loss: 0.010963797569274902
Batch 64/64 loss: 0.032802581787109375
Epoch 82  Train loss: 0.04312453643948424  Val loss: 0.08291530670578946
Saving best model, epoch: 82
Epoch 83
-------------------------------
Batch 1/64 loss: 0.01997154951095581
Batch 2/64 loss: 0.05091375112533569
Batch 3/64 loss: 0.036388516426086426
Batch 4/64 loss: 0.009990334510803223
Batch 5/64 loss: 0.030908286571502686
Batch 6/64 loss: 0.03837460279464722
Batch 7/64 loss: 0.013700604438781738
Batch 8/64 loss: 0.03691965341567993
Batch 9/64 loss: 0.03269737958908081
Batch 10/64 loss: 0.04612952470779419
Batch 11/64 loss: 0.041596055030822754
Batch 12/64 loss: 0.01890695095062256
Batch 13/64 loss: 0.04775130748748779
Batch 14/64 loss: 0.026318132877349854
Batch 15/64 loss: 0.046505510807037354
Batch 16/64 loss: 0.04790562391281128
Batch 17/64 loss: 0.03075873851776123
Batch 18/64 loss: 0.03019624948501587
Batch 19/64 loss: 0.028617143630981445
Batch 20/64 loss: 0.07103586196899414
Batch 21/64 loss: 0.025133848190307617
Batch 22/64 loss: 0.06319105625152588
Batch 23/64 loss: 0.05931907892227173
Batch 24/64 loss: 0.04200059175491333
Batch 25/64 loss: 0.026886582374572754
Batch 26/64 loss: 0.03417038917541504
Batch 27/64 loss: 0.03467792272567749
Batch 28/64 loss: 0.04022216796875
Batch 29/64 loss: 0.06350070238113403
Batch 30/64 loss: 0.035390496253967285
Batch 31/64 loss: 0.05192089080810547
Batch 32/64 loss: 0.05197101831436157
Batch 33/64 loss: 0.05904078483581543
Batch 34/64 loss: 0.02881854772567749
Batch 35/64 loss: 0.024608254432678223
Batch 36/64 loss: 0.03706848621368408
Batch 37/64 loss: 0.03098618984222412
Batch 38/64 loss: 0.06031423807144165
Batch 39/64 loss: 0.07170724868774414
Batch 40/64 loss: 0.031237125396728516
Batch 41/64 loss: 0.04697859287261963
Batch 42/64 loss: 0.048305630683898926
Batch 43/64 loss: 0.030335962772369385
Batch 44/64 loss: 0.045167744159698486
Batch 45/64 loss: 0.05736345052719116
Batch 46/64 loss: 0.07037335634231567
Batch 47/64 loss: 0.02764451503753662
Batch 48/64 loss: 0.045219600200653076
Batch 49/64 loss: 0.04882997274398804
Batch 50/64 loss: 0.04659467935562134
Batch 51/64 loss: 0.02951192855834961
Batch 52/64 loss: 0.027346789836883545
Batch 53/64 loss: 0.05896735191345215
Batch 54/64 loss: 0.012919485569000244
Batch 55/64 loss: 0.024900317192077637
Batch 56/64 loss: 0.06488418579101562
Batch 57/64 loss: 0.04119765758514404
Batch 58/64 loss: 0.03488516807556152
Batch 59/64 loss: 0.053971290588378906
Batch 60/64 loss: 0.028128862380981445
Batch 61/64 loss: 0.008670806884765625
Batch 62/64 loss: 0.05129849910736084
Batch 63/64 loss: 0.05678129196166992
Batch 64/64 loss: 0.05265402793884277
Epoch 83  Train loss: 0.040431671516568055  Val loss: 0.09128475598862901
Epoch 84
-------------------------------
Batch 1/64 loss: 0.04420185089111328
Batch 2/64 loss: 0.02202504873275757
Batch 3/64 loss: 0.025018692016601562
Batch 4/64 loss: 0.05708932876586914
Batch 5/64 loss: 0.02489030361175537
Batch 6/64 loss: -0.0054892897605896
Batch 7/64 loss: 0.058796226978302
Batch 8/64 loss: 0.01753056049346924
Batch 9/64 loss: 0.028831005096435547
Batch 10/64 loss: 0.01682579517364502
Batch 11/64 loss: 0.05396026372909546
Batch 12/64 loss: 0.03385859727859497
Batch 13/64 loss: 0.03824269771575928
Batch 14/64 loss: 0.04989755153656006
Batch 15/64 loss: 0.02918320894241333
Batch 16/64 loss: 0.04271578788757324
Batch 17/64 loss: 0.024525344371795654
Batch 18/64 loss: 0.03798013925552368
Batch 19/64 loss: 0.05868488550186157
Batch 20/64 loss: 0.04533982276916504
Batch 21/64 loss: 0.03218334913253784
Batch 22/64 loss: 0.02279496192932129
Batch 23/64 loss: 0.06600737571716309
Batch 24/64 loss: 0.03477680683135986
Batch 25/64 loss: 0.0395125150680542
Batch 26/64 loss: 0.04708278179168701
Batch 27/64 loss: 0.04238331317901611
Batch 28/64 loss: 0.029361367225646973
Batch 29/64 loss: 0.04869115352630615
Batch 30/64 loss: 0.0773729681968689
Batch 31/64 loss: 0.04085695743560791
Batch 32/64 loss: 0.05682927370071411
Batch 33/64 loss: 0.022114038467407227
Batch 34/64 loss: 0.04606670141220093
Batch 35/64 loss: 0.02169966697692871
Batch 36/64 loss: 0.07235062122344971
Batch 37/64 loss: 0.04674100875854492
Batch 38/64 loss: 0.05663251876831055
Batch 39/64 loss: 0.0199013352394104
Batch 40/64 loss: 0.035465896129608154
Batch 41/64 loss: 0.04259693622589111
Batch 42/64 loss: 0.03608804941177368
Batch 43/64 loss: 0.02752685546875
Batch 44/64 loss: 0.04499584436416626
Batch 45/64 loss: 0.05007201433181763
Batch 46/64 loss: 0.018574655055999756
Batch 47/64 loss: 0.05766463279724121
Batch 48/64 loss: 0.025973200798034668
Batch 49/64 loss: 0.028280198574066162
Batch 50/64 loss: 0.03499782085418701
Batch 51/64 loss: 0.05279994010925293
Batch 52/64 loss: 0.013874530792236328
Batch 53/64 loss: 0.04248034954071045
Batch 54/64 loss: 0.04519498348236084
Batch 55/64 loss: 0.0389026403427124
Batch 56/64 loss: 0.04223203659057617
Batch 57/64 loss: 0.046404480934143066
Batch 58/64 loss: 0.050490617752075195
Batch 59/64 loss: 0.02970564365386963
Batch 60/64 loss: 0.03160744905471802
Batch 61/64 loss: 0.03257995843887329
Batch 62/64 loss: 0.027610361576080322
Batch 63/64 loss: 0.052610158920288086
Batch 64/64 loss: 0.04091709852218628
Epoch 84  Train loss: 0.03869621356328328  Val loss: 0.0814228428598122
Saving best model, epoch: 84
Epoch 85
-------------------------------
Batch 1/64 loss: 0.04162949323654175
Batch 2/64 loss: 0.0557895302772522
Batch 3/64 loss: 0.023737430572509766
Batch 4/64 loss: 0.04306226968765259
Batch 5/64 loss: 0.017073988914489746
Batch 6/64 loss: 0.039446473121643066
Batch 7/64 loss: 0.05634725093841553
Batch 8/64 loss: 0.04797697067260742
Batch 9/64 loss: 0.07314449548721313
Batch 10/64 loss: 0.034334003925323486
Batch 11/64 loss: 0.02562171220779419
Batch 12/64 loss: 0.0317385196685791
Batch 13/64 loss: 0.059253573417663574
Batch 14/64 loss: 0.010540544986724854
Batch 15/64 loss: 0.03843343257904053
Batch 16/64 loss: 0.030543982982635498
Batch 17/64 loss: 0.057135820388793945
Batch 18/64 loss: 0.044971585273742676
Batch 19/64 loss: 0.014131247997283936
Batch 20/64 loss: 0.020206809043884277
Batch 21/64 loss: 0.03218042850494385
Batch 22/64 loss: 0.03712165355682373
Batch 23/64 loss: 0.02272355556488037
Batch 24/64 loss: 0.02224212884902954
Batch 25/64 loss: 0.0546494722366333
Batch 26/64 loss: 0.03684675693511963
Batch 27/64 loss: 0.053388118743896484
Batch 28/64 loss: 0.059848785400390625
Batch 29/64 loss: 0.03378307819366455
Batch 30/64 loss: 0.05472683906555176
Batch 31/64 loss: 0.011564254760742188
Batch 32/64 loss: 0.03984558582305908
Batch 33/64 loss: 0.012361764907836914
Batch 34/64 loss: 0.08958250284194946
Batch 35/64 loss: 0.031253933906555176
Batch 36/64 loss: 0.021843791007995605
Batch 37/64 loss: 0.03149235248565674
Batch 38/64 loss: 0.04884946346282959
Batch 39/64 loss: 0.05929189920425415
Batch 40/64 loss: 0.042528510093688965
Batch 41/64 loss: 0.03878265619277954
Batch 42/64 loss: 0.0360109806060791
Batch 43/64 loss: 0.05462503433227539
Batch 44/64 loss: 0.02308046817779541
Batch 45/64 loss: 0.032593607902526855
Batch 46/64 loss: 0.027181565761566162
Batch 47/64 loss: 0.03692251443862915
Batch 48/64 loss: 0.020555615425109863
Batch 49/64 loss: 0.06647813320159912
Batch 50/64 loss: 0.0774882435798645
Batch 51/64 loss: 0.041242361068725586
Batch 52/64 loss: 0.022873997688293457
Batch 53/64 loss: 0.030153393745422363
Batch 54/64 loss: 0.035661935806274414
Batch 55/64 loss: 0.029751360416412354
Batch 56/64 loss: 0.02751702070236206
Batch 57/64 loss: 0.04509854316711426
Batch 58/64 loss: 0.03018498420715332
Batch 59/64 loss: 0.05789744853973389
Batch 60/64 loss: 0.05341339111328125
Batch 61/64 loss: 0.052545368671417236
Batch 62/64 loss: 0.04063868522644043
Batch 63/64 loss: 0.02152073383331299
Batch 64/64 loss: 0.00944375991821289
Epoch 85  Train loss: 0.03875364509283328  Val loss: 0.08353132501090925
Epoch 86
-------------------------------
Batch 1/64 loss: -0.0058882832527160645
Batch 2/64 loss: 0.04581737518310547
Batch 3/64 loss: 0.0481342077255249
Batch 4/64 loss: 0.043744564056396484
Batch 5/64 loss: 0.02717602252960205
Batch 6/64 loss: 0.060819387435913086
Batch 7/64 loss: 0.03797423839569092
Batch 8/64 loss: 0.021908998489379883
Batch 9/64 loss: 0.046243906021118164
Batch 10/64 loss: 0.01487356424331665
Batch 11/64 loss: 0.0028003454208374023
Batch 12/64 loss: 0.012063264846801758
Batch 13/64 loss: 0.022315621376037598
Batch 14/64 loss: 0.06363457441329956
Batch 15/64 loss: 0.048472046852111816
Batch 16/64 loss: 0.048960089683532715
Batch 17/64 loss: 0.028381049633026123
Batch 18/64 loss: 0.03820997476577759
Batch 19/64 loss: 0.05233156681060791
Batch 20/64 loss: 0.05334347486495972
Batch 21/64 loss: 0.07248306274414062
Batch 22/64 loss: 0.04369455575942993
Batch 23/64 loss: 0.06056469678878784
Batch 24/64 loss: 0.04096907377243042
Batch 25/64 loss: 0.04519832134246826
Batch 26/64 loss: 0.042781949043273926
Batch 27/64 loss: 0.06908029317855835
Batch 28/64 loss: 0.0752374529838562
Batch 29/64 loss: 0.03227376937866211
Batch 30/64 loss: 0.04083096981048584
Batch 31/64 loss: 0.03524118661880493
Batch 32/64 loss: 0.015306353569030762
Batch 33/64 loss: 0.017753422260284424
Batch 34/64 loss: 0.02816474437713623
Batch 35/64 loss: 0.03664350509643555
Batch 36/64 loss: 0.056504130363464355
Batch 37/64 loss: 0.0672922134399414
Batch 38/64 loss: 0.028302669525146484
Batch 39/64 loss: 0.04194784164428711
Batch 40/64 loss: 0.02924412488937378
Batch 41/64 loss: 0.03397965431213379
Batch 42/64 loss: 0.04279059171676636
Batch 43/64 loss: 0.047411322593688965
Batch 44/64 loss: 0.057575345039367676
Batch 45/64 loss: 0.044663190841674805
Batch 46/64 loss: 0.03131413459777832
Batch 47/64 loss: 0.05439108610153198
Batch 48/64 loss: 0.03239256143569946
Batch 49/64 loss: 0.031494736671447754
Batch 50/64 loss: 0.020743250846862793
Batch 51/64 loss: 0.011591970920562744
Batch 52/64 loss: 0.025168538093566895
Batch 53/64 loss: 0.03848922252655029
Batch 54/64 loss: 0.007649481296539307
Batch 55/64 loss: 0.05007529258728027
Batch 56/64 loss: 0.03981286287307739
Batch 57/64 loss: 0.01803797483444214
Batch 58/64 loss: 0.04629307985305786
Batch 59/64 loss: 0.05499047040939331
Batch 60/64 loss: 0.03459244966506958
Batch 61/64 loss: 0.0360865592956543
Batch 62/64 loss: 0.05142521858215332
Batch 63/64 loss: 0.04628020524978638
Batch 64/64 loss: 0.051670610904693604
Epoch 86  Train loss: 0.03900913701337926  Val loss: 0.11425721604389832
Epoch 87
-------------------------------
Batch 1/64 loss: 0.003929436206817627
Batch 2/64 loss: 0.07628053426742554
Batch 3/64 loss: 0.05153387784957886
Batch 4/64 loss: 0.04555433988571167
Batch 5/64 loss: 0.04965412616729736
Batch 6/64 loss: 0.04061329364776611
Batch 7/64 loss: 0.0253942608833313
Batch 8/64 loss: 0.03348720073699951
Batch 9/64 loss: 0.03437507152557373
Batch 10/64 loss: 0.048165202140808105
Batch 11/64 loss: 0.07624650001525879
Batch 12/64 loss: 0.013426721096038818
Batch 13/64 loss: 0.0206412672996521
Batch 14/64 loss: 0.021380066871643066
Batch 15/64 loss: 0.05248236656188965
Batch 16/64 loss: 0.01566249132156372
Batch 17/64 loss: 0.06128215789794922
Batch 18/64 loss: 0.04711151123046875
Batch 19/64 loss: 0.019118845462799072
Batch 20/64 loss: 0.020323991775512695
Batch 21/64 loss: 0.056891679763793945
Batch 22/64 loss: 0.04853188991546631
Batch 23/64 loss: 0.03579765558242798
Batch 24/64 loss: 0.030504345893859863
Batch 25/64 loss: 0.02220165729522705
Batch 26/64 loss: 0.04584980010986328
Batch 27/64 loss: 0.07137584686279297
Batch 28/64 loss: 0.03471696376800537
Batch 29/64 loss: 0.03751760721206665
Batch 30/64 loss: 0.027411937713623047
Batch 31/64 loss: 0.013034820556640625
Batch 32/64 loss: 0.02063310146331787
Batch 33/64 loss: 0.0728604793548584
Batch 34/64 loss: 0.03899538516998291
Batch 35/64 loss: 0.03586059808731079
Batch 36/64 loss: 0.03002452850341797
Batch 37/64 loss: 0.034403324127197266
Batch 38/64 loss: 0.02239745855331421
Batch 39/64 loss: 0.038363873958587646
Batch 40/64 loss: 0.02025163173675537
Batch 41/64 loss: 0.06261104345321655
Batch 42/64 loss: 0.01070702075958252
Batch 43/64 loss: 0.041114866733551025
Batch 44/64 loss: 0.04528719186782837
Batch 45/64 loss: 0.01811671257019043
Batch 46/64 loss: 0.055167555809020996
Batch 47/64 loss: 0.02458399534225464
Batch 48/64 loss: 0.04028880596160889
Batch 49/64 loss: 0.043752193450927734
Batch 50/64 loss: 0.03406721353530884
Batch 51/64 loss: 0.006175875663757324
Batch 52/64 loss: 0.036662936210632324
Batch 53/64 loss: 0.035703301429748535
Batch 54/64 loss: 0.0551600456237793
Batch 55/64 loss: 0.03287374973297119
Batch 56/64 loss: 0.0425260066986084
Batch 57/64 loss: 0.023496270179748535
Batch 58/64 loss: 0.0808185338973999
Batch 59/64 loss: 0.052335381507873535
Batch 60/64 loss: 0.03407919406890869
Batch 61/64 loss: 0.017233967781066895
Batch 62/64 loss: 0.01982635259628296
Batch 63/64 loss: 0.01921260356903076
Batch 64/64 loss: 0.03348040580749512
Epoch 87  Train loss: 0.03688155249053357  Val loss: 0.08595285239498231
Epoch 88
-------------------------------
Batch 1/64 loss: 0.03392958641052246
Batch 2/64 loss: 0.019715428352355957
Batch 3/64 loss: 0.03868210315704346
Batch 4/64 loss: 0.07368063926696777
Batch 5/64 loss: 0.012941539287567139
Batch 6/64 loss: 0.015703976154327393
Batch 7/64 loss: 0.011185407638549805
Batch 8/64 loss: 0.03083813190460205
Batch 9/64 loss: 0.02838122844696045
Batch 10/64 loss: 0.053159356117248535
Batch 11/64 loss: 0.04397016763687134
Batch 12/64 loss: 0.05102008581161499
Batch 13/64 loss: 0.054288387298583984
Batch 14/64 loss: 0.07311725616455078
Batch 15/64 loss: -0.005237758159637451
Batch 16/64 loss: 0.0380091667175293
Batch 17/64 loss: 0.047788798809051514
Batch 18/64 loss: 0.016723990440368652
Batch 19/64 loss: 0.04538547992706299
Batch 20/64 loss: 0.03128683567047119
Batch 21/64 loss: 0.05321317911148071
Batch 22/64 loss: 0.008083224296569824
Batch 23/64 loss: 0.012320160865783691
Batch 24/64 loss: 0.038286566734313965
Batch 25/64 loss: 0.037414610385894775
Batch 26/64 loss: 0.03176921606063843
Batch 27/64 loss: 0.04692184925079346
Batch 28/64 loss: 0.03966248035430908
Batch 29/64 loss: 0.025794625282287598
Batch 30/64 loss: 0.025344252586364746
Batch 31/64 loss: 0.043717026710510254
Batch 32/64 loss: 0.017359375953674316
Batch 33/64 loss: 0.04614722728729248
Batch 34/64 loss: 0.04164779186248779
Batch 35/64 loss: 0.03470253944396973
Batch 36/64 loss: 0.028031349182128906
Batch 37/64 loss: 0.029790878295898438
Batch 38/64 loss: -0.0007519125938415527
Batch 39/64 loss: 0.04397851228713989
Batch 40/64 loss: 0.01720947027206421
Batch 41/64 loss: 0.0335119366645813
Batch 42/64 loss: 0.052185654640197754
Batch 43/64 loss: 0.025505483150482178
Batch 44/64 loss: 0.02503812313079834
Batch 45/64 loss: 0.0409623384475708
Batch 46/64 loss: 0.053041696548461914
Batch 47/64 loss: 0.00919806957244873
Batch 48/64 loss: 0.02048712968826294
Batch 49/64 loss: 0.014909148216247559
Batch 50/64 loss: 0.04361987113952637
Batch 51/64 loss: 0.056757450103759766
Batch 52/64 loss: 0.022388756275177002
Batch 53/64 loss: 0.0626189112663269
Batch 54/64 loss: 0.030018985271453857
Batch 55/64 loss: 0.08058351278305054
Batch 56/64 loss: 0.055792927742004395
Batch 57/64 loss: 0.0711100697517395
Batch 58/64 loss: 0.04004949331283569
Batch 59/64 loss: 0.024414896965026855
Batch 60/64 loss: 0.047682881355285645
Batch 61/64 loss: 0.012614727020263672
Batch 62/64 loss: 0.04136848449707031
Batch 63/64 loss: 0.06338471174240112
Batch 64/64 loss: 0.022291362285614014
Epoch 88  Train loss: 0.03575178057539697  Val loss: 0.08766074872918145
Epoch 89
-------------------------------
Batch 1/64 loss: 0.0530889630317688
Batch 2/64 loss: 0.011783838272094727
Batch 3/64 loss: 0.04172027111053467
Batch 4/64 loss: 0.03188157081604004
Batch 5/64 loss: 0.025721609592437744
Batch 6/64 loss: 0.03795480728149414
Batch 7/64 loss: 0.035946667194366455
Batch 8/64 loss: 0.01825392246246338
Batch 9/64 loss: 0.02855008840560913
Batch 10/64 loss: 0.04725909233093262
Batch 11/64 loss: 0.023578286170959473
Batch 12/64 loss: 0.0437847375869751
Batch 13/64 loss: 0.017641007900238037
Batch 14/64 loss: 0.033849895000457764
Batch 15/64 loss: -0.00042325258255004883
Batch 16/64 loss: 0.047404348850250244
Batch 17/64 loss: 0.020157277584075928
Batch 18/64 loss: 0.037774860858917236
Batch 19/64 loss: 0.02514129877090454
Batch 20/64 loss: 0.016904056072235107
Batch 21/64 loss: 0.02283787727355957
Batch 22/64 loss: 0.03512829542160034
Batch 23/64 loss: 0.027073144912719727
Batch 24/64 loss: 0.01989966630935669
Batch 25/64 loss: 0.008197605609893799
Batch 26/64 loss: 0.028681278228759766
Batch 27/64 loss: 0.03282660245895386
Batch 28/64 loss: 0.05868113040924072
Batch 29/64 loss: 0.013700246810913086
Batch 30/64 loss: 0.011415958404541016
Batch 31/64 loss: 0.0210912823677063
Batch 32/64 loss: 0.055261075496673584
Batch 33/64 loss: 0.04648250341415405
Batch 34/64 loss: 0.028549134731292725
Batch 35/64 loss: 0.03105086088180542
Batch 36/64 loss: 0.03308063745498657
Batch 37/64 loss: 0.028097093105316162
Batch 38/64 loss: 0.03140014410018921
Batch 39/64 loss: 0.045690059661865234
Batch 40/64 loss: 0.031040847301483154
Batch 41/64 loss: 0.033588528633117676
Batch 42/64 loss: 0.019227445125579834
Batch 43/64 loss: 0.03281289339065552
Batch 44/64 loss: 0.034294068813323975
Batch 45/64 loss: 0.04928123950958252
Batch 46/64 loss: 0.03661668300628662
Batch 47/64 loss: 0.022689104080200195
Batch 48/64 loss: 0.08267360925674438
Batch 49/64 loss: 0.009863734245300293
Batch 50/64 loss: 0.024451017379760742
Batch 51/64 loss: 0.05066537857055664
Batch 52/64 loss: 0.06458908319473267
Batch 53/64 loss: 0.04718458652496338
Batch 54/64 loss: 0.04730331897735596
Batch 55/64 loss: 0.011464357376098633
Batch 56/64 loss: 0.06078469753265381
Batch 57/64 loss: 0.031127572059631348
Batch 58/64 loss: 0.02513265609741211
Batch 59/64 loss: 0.04112362861633301
Batch 60/64 loss: 0.04490959644317627
Batch 61/64 loss: 0.05310952663421631
Batch 62/64 loss: 0.05191010236740112
Batch 63/64 loss: 0.024745702743530273
Batch 64/64 loss: 0.032520413398742676
Epoch 89  Train loss: 0.033444668732437434  Val loss: 0.08228283465113427
Epoch 90
-------------------------------
Batch 1/64 loss: 0.0064313411712646484
Batch 2/64 loss: 0.03566926717758179
Batch 3/64 loss: 0.03164052963256836
Batch 4/64 loss: 0.03683590888977051
Batch 5/64 loss: 0.004840254783630371
Batch 6/64 loss: 0.015070796012878418
Batch 7/64 loss: 0.04326695203781128
Batch 8/64 loss: 0.04582631587982178
Batch 9/64 loss: 0.03535193204879761
Batch 10/64 loss: 0.04190027713775635
Batch 11/64 loss: 0.03302431106567383
Batch 12/64 loss: 0.04677248001098633
Batch 13/64 loss: 0.04727339744567871
Batch 14/64 loss: 0.037292301654815674
Batch 15/64 loss: 0.04377669095993042
Batch 16/64 loss: 0.04789125919342041
Batch 17/64 loss: 0.031713783740997314
Batch 18/64 loss: 0.044917941093444824
Batch 19/64 loss: 0.01642012596130371
Batch 20/64 loss: 0.032978832721710205
Batch 21/64 loss: 0.045562803745269775
Batch 22/64 loss: 0.029956340789794922
Batch 23/64 loss: 0.04198765754699707
Batch 24/64 loss: 0.014589786529541016
Batch 25/64 loss: 0.0332714319229126
Batch 26/64 loss: 0.02723217010498047
Batch 27/64 loss: 0.05148661136627197
Batch 28/64 loss: 0.01610654592514038
Batch 29/64 loss: 0.03961735963821411
Batch 30/64 loss: 0.03706008195877075
Batch 31/64 loss: 0.029034197330474854
Batch 32/64 loss: 0.02034902572631836
Batch 33/64 loss: 0.036407530307769775
Batch 34/64 loss: 0.04178684949874878
Batch 35/64 loss: 0.004450082778930664
Batch 36/64 loss: 0.042407333850860596
Batch 37/64 loss: 0.03212696313858032
Batch 38/64 loss: 0.034195125102996826
Batch 39/64 loss: 0.04595828056335449
Batch 40/64 loss: 0.05962979793548584
Batch 41/64 loss: 0.01941293478012085
Batch 42/64 loss: 0.04701882600784302
Batch 43/64 loss: 0.05694925785064697
Batch 44/64 loss: 0.04187631607055664
Batch 45/64 loss: 0.0389784574508667
Batch 46/64 loss: 0.03682452440261841
Batch 47/64 loss: 0.01375722885131836
Batch 48/64 loss: 0.021315813064575195
Batch 49/64 loss: 0.06296062469482422
Batch 50/64 loss: 0.012539327144622803
Batch 51/64 loss: 0.03677952289581299
Batch 52/64 loss: 0.020613014698028564
Batch 53/64 loss: 0.020605921745300293
Batch 54/64 loss: 0.038449645042419434
Batch 55/64 loss: 0.05297267436981201
Batch 56/64 loss: 0.047363996505737305
Batch 57/64 loss: 0.027397871017456055
Batch 58/64 loss: 0.04099196195602417
Batch 59/64 loss: 0.03601795434951782
Batch 60/64 loss: 0.004226326942443848
Batch 61/64 loss: 0.04370558261871338
Batch 62/64 loss: 0.021573424339294434
Batch 63/64 loss: 0.006178140640258789
Batch 64/64 loss: 0.0038251280784606934
Epoch 90  Train loss: 0.03315276655496335  Val loss: 0.08289096302183223
Epoch 91
-------------------------------
Batch 1/64 loss: -0.0009164810180664062
Batch 2/64 loss: 0.019401252269744873
Batch 3/64 loss: 0.005349278450012207
Batch 4/64 loss: 0.027673959732055664
Batch 5/64 loss: 0.055608510971069336
Batch 6/64 loss: 0.04742121696472168
Batch 7/64 loss: 0.03828251361846924
Batch 8/64 loss: 0.01381993293762207
Batch 9/64 loss: 0.04532444477081299
Batch 10/64 loss: 0.026173293590545654
Batch 11/64 loss: 0.07466566562652588
Batch 12/64 loss: 0.027146995067596436
Batch 13/64 loss: 0.03205215930938721
Batch 14/64 loss: 0.02377164363861084
Batch 15/64 loss: 0.0383458137512207
Batch 16/64 loss: 0.02386760711669922
Batch 17/64 loss: 0.023700177669525146
Batch 18/64 loss: 0.005184471607208252
Batch 19/64 loss: 0.047926247119903564
Batch 20/64 loss: 0.022122859954833984
Batch 21/64 loss: 0.03218281269073486
Batch 22/64 loss: 0.043837547302246094
Batch 23/64 loss: 0.036037445068359375
Batch 24/64 loss: 0.035048842430114746
Batch 25/64 loss: 0.024282336235046387
Batch 26/64 loss: 0.025328457355499268
Batch 27/64 loss: 0.05877918004989624
Batch 28/64 loss: 0.028664112091064453
Batch 29/64 loss: 0.015525639057159424
Batch 30/64 loss: 0.030784249305725098
Batch 31/64 loss: 0.030275046825408936
Batch 32/64 loss: 0.06068998575210571
Batch 33/64 loss: 0.035976946353912354
Batch 34/64 loss: 0.015446245670318604
Batch 35/64 loss: 0.042174458503723145
Batch 36/64 loss: 0.05513894557952881
Batch 37/64 loss: 0.04776656627655029
Batch 38/64 loss: 0.041575491428375244
Batch 39/64 loss: 0.022872447967529297
Batch 40/64 loss: 0.0591733455657959
Batch 41/64 loss: 0.058255672454833984
Batch 42/64 loss: 0.05320167541503906
Batch 43/64 loss: 0.028918683528900146
Batch 44/64 loss: 0.03344237804412842
Batch 45/64 loss: 0.024647116661071777
Batch 46/64 loss: 0.0593489408493042
Batch 47/64 loss: 0.006894409656524658
Batch 48/64 loss: 0.008979439735412598
Batch 49/64 loss: 0.01095658540725708
Batch 50/64 loss: 0.04029345512390137
Batch 51/64 loss: 0.04470604658126831
Batch 52/64 loss: 0.052808165550231934
Batch 53/64 loss: 0.03820997476577759
Batch 54/64 loss: 0.03175473213195801
Batch 55/64 loss: 0.04227942228317261
Batch 56/64 loss: 0.026133835315704346
Batch 57/64 loss: 0.04055929183959961
Batch 58/64 loss: 0.021112680435180664
Batch 59/64 loss: 0.02973353862762451
Batch 60/64 loss: 0.04085588455200195
Batch 61/64 loss: 0.029415488243103027
Batch 62/64 loss: 0.03305536508560181
Batch 63/64 loss: 0.043048202991485596
Batch 64/64 loss: 0.02108907699584961
Epoch 91  Train loss: 0.03377112874797746  Val loss: 0.0752503343464173
Saving best model, epoch: 91
Epoch 92
-------------------------------
Batch 1/64 loss: 0.009629666805267334
Batch 2/64 loss: 0.05165302753448486
Batch 3/64 loss: 0.04413115978240967
Batch 4/64 loss: 0.028778910636901855
Batch 5/64 loss: 0.027747511863708496
Batch 6/64 loss: 0.0387120246887207
Batch 7/64 loss: 0.032434165477752686
Batch 8/64 loss: 0.02825617790222168
Batch 9/64 loss: 0.05079770088195801
Batch 10/64 loss: 0.027921319007873535
Batch 11/64 loss: 0.03639179468154907
Batch 12/64 loss: 0.018509864807128906
Batch 13/64 loss: 0.03380441665649414
Batch 14/64 loss: 0.025370240211486816
Batch 15/64 loss: 0.07841414213180542
Batch 16/64 loss: 0.06617367267608643
Batch 17/64 loss: 0.003173828125
Batch 18/64 loss: 0.05865371227264404
Batch 19/64 loss: 0.035839855670928955
Batch 20/64 loss: 0.01945662498474121
Batch 21/64 loss: 0.00836247205734253
Batch 22/64 loss: 0.05036216974258423
Batch 23/64 loss: 0.02484196424484253
Batch 24/64 loss: 0.04558318853378296
Batch 25/64 loss: 0.00977104902267456
Batch 26/64 loss: 0.07082033157348633
Batch 27/64 loss: 0.008424520492553711
Batch 28/64 loss: 0.04061436653137207
Batch 29/64 loss: 0.02590423822402954
Batch 30/64 loss: 0.03022933006286621
Batch 31/64 loss: 0.020308732986450195
Batch 32/64 loss: 0.02538740634918213
Batch 33/64 loss: 0.03747683763504028
Batch 34/64 loss: 0.017729640007019043
Batch 35/64 loss: 0.029241204261779785
Batch 36/64 loss: 0.028182685375213623
Batch 37/64 loss: 0.004128515720367432
Batch 38/64 loss: 0.01996999979019165
Batch 39/64 loss: 0.01549595594406128
Batch 40/64 loss: 0.016599297523498535
Batch 41/64 loss: 0.017300069332122803
Batch 42/64 loss: 0.03600889444351196
Batch 43/64 loss: 0.010758161544799805
Batch 44/64 loss: 0.020310819149017334
Batch 45/64 loss: 0.011352777481079102
Batch 46/64 loss: 0.0296175479888916
Batch 47/64 loss: 0.02389383316040039
Batch 48/64 loss: 0.05658334493637085
Batch 49/64 loss: 0.06758338212966919
Batch 50/64 loss: 0.03679698705673218
Batch 51/64 loss: 0.0262376070022583
Batch 52/64 loss: 0.03149974346160889
Batch 53/64 loss: 0.014734745025634766
Batch 54/64 loss: 0.04179787635803223
Batch 55/64 loss: 0.0270807147026062
Batch 56/64 loss: 0.03175860643386841
Batch 57/64 loss: 0.02602905035018921
Batch 58/64 loss: 0.027682900428771973
Batch 59/64 loss: 0.03850758075714111
Batch 60/64 loss: 0.033345580101013184
Batch 61/64 loss: 0.02236086130142212
Batch 62/64 loss: 0.05040466785430908
Batch 63/64 loss: 0.0105476975440979
Batch 64/64 loss: 0.05114680528640747
Epoch 92  Train loss: 0.030993525888405593  Val loss: 0.10801621237161643
Epoch 93
-------------------------------
Batch 1/64 loss: 0.0229681134223938
Batch 2/64 loss: 0.029170572757720947
Batch 3/64 loss: 0.01598942279815674
Batch 4/64 loss: 0.04968667030334473
Batch 5/64 loss: 0.05685245990753174
Batch 6/64 loss: 0.040776073932647705
Batch 7/64 loss: 0.009217143058776855
Batch 8/64 loss: 0.0070269107818603516
Batch 9/64 loss: 0.04136693477630615
Batch 10/64 loss: 0.03408801555633545
Batch 11/64 loss: 0.00549769401550293
Batch 12/64 loss: 0.01175546646118164
Batch 13/64 loss: 0.03618967533111572
Batch 14/64 loss: 0.019066989421844482
Batch 15/64 loss: 0.037975966930389404
Batch 16/64 loss: 0.05201363563537598
Batch 17/64 loss: 0.024878263473510742
Batch 18/64 loss: 0.013691544532775879
Batch 19/64 loss: 0.0283890962600708
Batch 20/64 loss: -0.0023851990699768066
Batch 21/64 loss: 0.02818197011947632
Batch 22/64 loss: 0.02765202522277832
Batch 23/64 loss: 0.03468048572540283
Batch 24/64 loss: 0.03398919105529785
Batch 25/64 loss: 0.032023072242736816
Batch 26/64 loss: 0.015773296356201172
Batch 27/64 loss: 0.055287957191467285
Batch 28/64 loss: 0.05367398262023926
Batch 29/64 loss: 0.057546377182006836
Batch 30/64 loss: 0.024763643741607666
Batch 31/64 loss: 0.024203836917877197
Batch 32/64 loss: 0.03434866666793823
Batch 33/64 loss: 0.04085183143615723
Batch 34/64 loss: 0.040360093116760254
Batch 35/64 loss: 0.03827100992202759
Batch 36/64 loss: 0.020219087600708008
Batch 37/64 loss: 0.01589268445968628
Batch 38/64 loss: 0.06194251775741577
Batch 39/64 loss: 0.03861361742019653
Batch 40/64 loss: 0.0274314284324646
Batch 41/64 loss: 0.051899731159210205
Batch 42/64 loss: 0.049497902393341064
Batch 43/64 loss: 0.03413623571395874
Batch 44/64 loss: 0.036589741706848145
Batch 45/64 loss: 0.022214829921722412
Batch 46/64 loss: 0.012457609176635742
Batch 47/64 loss: 0.06289732456207275
Batch 48/64 loss: 0.031810879707336426
Batch 49/64 loss: 0.04452282190322876
Batch 50/64 loss: 0.031466901302337646
Batch 51/64 loss: 0.021120667457580566
Batch 52/64 loss: 0.049488961696624756
Batch 53/64 loss: 0.020769953727722168
Batch 54/64 loss: 0.04462134838104248
Batch 55/64 loss: 0.026707887649536133
Batch 56/64 loss: 0.030738353729248047
Batch 57/64 loss: 0.0409235954284668
Batch 58/64 loss: 0.038154423236846924
Batch 59/64 loss: 0.030608296394348145
Batch 60/64 loss: 0.01683366298675537
Batch 61/64 loss: 0.04375690221786499
Batch 62/64 loss: 0.02425074577331543
Batch 63/64 loss: 0.01337730884552002
Batch 64/64 loss: 0.017411231994628906
Epoch 93  Train loss: 0.031871791914397596  Val loss: 0.09136434068384859
Epoch 94
-------------------------------
Batch 1/64 loss: 0.009402632713317871
Batch 2/64 loss: 0.027760624885559082
Batch 3/64 loss: 0.042702555656433105
Batch 4/64 loss: 0.011525630950927734
Batch 5/64 loss: 0.025341153144836426
Batch 6/64 loss: 0.05671381950378418
Batch 7/64 loss: 0.026032328605651855
Batch 8/64 loss: 0.08228516578674316
Batch 9/64 loss: 0.014413774013519287
Batch 10/64 loss: 0.011708557605743408
Batch 11/64 loss: 0.0157850980758667
Batch 12/64 loss: 0.02537161111831665
Batch 13/64 loss: 0.04751628637313843
Batch 14/64 loss: 0.032953739166259766
Batch 15/64 loss: 0.005084514617919922
Batch 16/64 loss: 0.03921765089035034
Batch 17/64 loss: 0.02230668067932129
Batch 18/64 loss: 0.017325103282928467
Batch 19/64 loss: 0.01568901538848877
Batch 20/64 loss: 0.021295249462127686
Batch 21/64 loss: 0.006288290023803711
Batch 22/64 loss: 0.028461694717407227
Batch 23/64 loss: -0.012409389019012451
Batch 24/64 loss: 0.014355182647705078
Batch 25/64 loss: 0.043207764625549316
Batch 26/64 loss: 0.032501220703125
Batch 27/64 loss: 0.040382981300354004
Batch 28/64 loss: 0.03488504886627197
Batch 29/64 loss: 0.03583526611328125
Batch 30/64 loss: 0.03656792640686035
Batch 31/64 loss: 0.007363677024841309
Batch 32/64 loss: 0.02725428342819214
Batch 33/64 loss: 0.012261927127838135
Batch 34/64 loss: 0.03921401500701904
Batch 35/64 loss: 0.021674633026123047
Batch 36/64 loss: 0.0400729775428772
Batch 37/64 loss: 0.009988486766815186
Batch 38/64 loss: 0.03978240489959717
Batch 39/64 loss: 0.0442240834236145
Batch 40/64 loss: 0.018744170665740967
Batch 41/64 loss: 0.02391970157623291
Batch 42/64 loss: 0.06373631954193115
Batch 43/64 loss: 0.006671249866485596
Batch 44/64 loss: 0.031973958015441895
Batch 45/64 loss: 0.024163365364074707
Batch 46/64 loss: 0.023412466049194336
Batch 47/64 loss: 0.049460530281066895
Batch 48/64 loss: 0.026397287845611572
Batch 49/64 loss: 0.044422686100006104
Batch 50/64 loss: 0.06208401918411255
Batch 51/64 loss: 0.054504990577697754
Batch 52/64 loss: 0.03568124771118164
Batch 53/64 loss: 0.0208432674407959
Batch 54/64 loss: 0.05202305316925049
Batch 55/64 loss: 0.03432995080947876
Batch 56/64 loss: 0.02229142189025879
Batch 57/64 loss: 0.033104538917541504
Batch 58/64 loss: 0.020387232303619385
Batch 59/64 loss: 0.03212392330169678
Batch 60/64 loss: 0.040940046310424805
Batch 61/64 loss: 0.036895155906677246
Batch 62/64 loss: 0.035864055156707764
Batch 63/64 loss: 0.04391521215438843
Batch 64/64 loss: 0.05236762762069702
Epoch 94  Train loss: 0.03026681156719432  Val loss: 0.08339193535014935
Epoch 95
-------------------------------
Batch 1/64 loss: 0.03862440586090088
Batch 2/64 loss: 0.010991454124450684
Batch 3/64 loss: 0.02504485845565796
Batch 4/64 loss: 0.018328607082366943
Batch 5/64 loss: 0.015626609325408936
Batch 6/64 loss: 0.04090017080307007
Batch 7/64 loss: 0.025632739067077637
Batch 8/64 loss: 0.020165979862213135
Batch 9/64 loss: 0.024787962436676025
Batch 10/64 loss: 0.003532111644744873
Batch 11/64 loss: 0.03386878967285156
Batch 12/64 loss: 0.01614922285079956
Batch 13/64 loss: 0.05615031719207764
Batch 14/64 loss: 0.01819378137588501
Batch 15/64 loss: 0.016043126583099365
Batch 16/64 loss: 0.018230199813842773
Batch 17/64 loss: 0.0625072717666626
Batch 18/64 loss: 0.04941582679748535
Batch 19/64 loss: 0.04814445972442627
Batch 20/64 loss: 0.025307416915893555
Batch 21/64 loss: 0.0019426941871643066
Batch 22/64 loss: 0.03209275007247925
Batch 23/64 loss: 0.0454103946685791
Batch 24/64 loss: 0.007867097854614258
Batch 25/64 loss: 0.03472721576690674
Batch 26/64 loss: 0.003120601177215576
Batch 27/64 loss: 0.02845621109008789
Batch 28/64 loss: 0.06361514329910278
Batch 29/64 loss: 0.009540975093841553
Batch 30/64 loss: 0.01989668607711792
Batch 31/64 loss: 0.029487192630767822
Batch 32/64 loss: 0.053131282329559326
Batch 33/64 loss: 0.02730327844619751
Batch 34/64 loss: 0.007611989974975586
Batch 35/64 loss: 0.0366588830947876
Batch 36/64 loss: 0.026062071323394775
Batch 37/64 loss: 0.050855398178100586
Batch 38/64 loss: 0.03773224353790283
Batch 39/64 loss: 0.03754901885986328
Batch 40/64 loss: 0.011637687683105469
Batch 41/64 loss: 0.015116691589355469
Batch 42/64 loss: 0.04380953311920166
Batch 43/64 loss: 0.006446421146392822
Batch 44/64 loss: 0.044847309589385986
Batch 45/64 loss: 0.021656453609466553
Batch 46/64 loss: 0.035672664642333984
Batch 47/64 loss: 0.0035573840141296387
Batch 48/64 loss: 0.06258368492126465
Batch 49/64 loss: 0.03459286689758301
Batch 50/64 loss: 0.024197518825531006
Batch 51/64 loss: 0.015583217144012451
Batch 52/64 loss: 0.02203899621963501
Batch 53/64 loss: 0.03708606958389282
Batch 54/64 loss: 0.023430049419403076
Batch 55/64 loss: 0.033588945865631104
Batch 56/64 loss: 0.06048738956451416
Batch 57/64 loss: 0.02656036615371704
Batch 58/64 loss: 0.023775100708007812
Batch 59/64 loss: 0.023029208183288574
Batch 60/64 loss: 0.023338258266448975
Batch 61/64 loss: 0.019304275512695312
Batch 62/64 loss: 0.047190308570861816
Batch 63/64 loss: 0.025444328784942627
Batch 64/64 loss: 0.05578005313873291
Epoch 95  Train loss: 0.028980646413915297  Val loss: 0.07789774264666632
Epoch 96
-------------------------------
Batch 1/64 loss: 0.028483092784881592
Batch 2/64 loss: 0.02847844362258911
Batch 3/64 loss: 0.010668277740478516
Batch 4/64 loss: 0.024440884590148926
Batch 5/64 loss: 0.03266143798828125
Batch 6/64 loss: 0.0237044095993042
Batch 7/64 loss: 0.05818605422973633
Batch 8/64 loss: 0.03261244297027588
Batch 9/64 loss: -0.014474153518676758
Batch 10/64 loss: 0.02389460802078247
Batch 11/64 loss: 0.0211905837059021
Batch 12/64 loss: 0.020052671432495117
Batch 13/64 loss: 0.030602693557739258
Batch 14/64 loss: 0.014978766441345215
Batch 15/64 loss: 0.019859671592712402
Batch 16/64 loss: -0.0008515715599060059
Batch 17/64 loss: 0.03166741132736206
Batch 18/64 loss: 0.003993690013885498
Batch 19/64 loss: 0.05166029930114746
Batch 20/64 loss: 0.024176180362701416
Batch 21/64 loss: 0.015041947364807129
Batch 22/64 loss: 0.035969674587249756
Batch 23/64 loss: 0.020787417888641357
Batch 24/64 loss: 0.04785948991775513
Batch 25/64 loss: 0.04708951711654663
Batch 26/64 loss: 0.04220980405807495
Batch 27/64 loss: 0.0220680832862854
Batch 28/64 loss: 0.06041437387466431
Batch 29/64 loss: 0.032141685485839844
Batch 30/64 loss: 0.025543391704559326
Batch 31/64 loss: 0.018251776695251465
Batch 32/64 loss: 0.02517777681350708
Batch 33/64 loss: 0.011775016784667969
Batch 34/64 loss: 0.005889773368835449
Batch 35/64 loss: 0.05947422981262207
Batch 36/64 loss: 0.040074706077575684
Batch 37/64 loss: 0.0003725290298461914
Batch 38/64 loss: 0.029771745204925537
Batch 39/64 loss: 0.018768012523651123
Batch 40/64 loss: 0.024921655654907227
Batch 41/64 loss: 0.04799652099609375
Batch 42/64 loss: 0.0028955936431884766
Batch 43/64 loss: 0.004677176475524902
Batch 44/64 loss: 0.045758962631225586
Batch 45/64 loss: 0.030142366886138916
Batch 46/64 loss: 0.01738893985748291
Batch 47/64 loss: 0.02873682975769043
Batch 48/64 loss: 0.016149401664733887
Batch 49/64 loss: 0.004548013210296631
Batch 50/64 loss: 0.0475466251373291
Batch 51/64 loss: 0.044182777404785156
Batch 52/64 loss: -0.003103792667388916
Batch 53/64 loss: 0.035579144954681396
Batch 54/64 loss: 0.04473906755447388
Batch 55/64 loss: 0.02120959758758545
Batch 56/64 loss: 0.04979896545410156
Batch 57/64 loss: 0.050193607807159424
Batch 58/64 loss: 0.044088006019592285
Batch 59/64 loss: 0.018952548503875732
Batch 60/64 loss: 0.01335817575454712
Batch 61/64 loss: 0.022692739963531494
Batch 62/64 loss: 0.04144155979156494
Batch 63/64 loss: 0.04691445827484131
Batch 64/64 loss: 0.019009649753570557
Epoch 96  Train loss: 0.02728992981069228  Val loss: 0.08609899438123933
Epoch 97
-------------------------------
Batch 1/64 loss: 0.021735548973083496
Batch 2/64 loss: 0.006480693817138672
Batch 3/64 loss: 0.022134244441986084
Batch 4/64 loss: 0.017979979515075684
Batch 5/64 loss: 0.001849055290222168
Batch 6/64 loss: 0.02023899555206299
Batch 7/64 loss: 0.037813544273376465
Batch 8/64 loss: 0.035245418548583984
Batch 9/64 loss: 0.021282553672790527
Batch 10/64 loss: 0.0295296311378479
Batch 11/64 loss: 0.010503113269805908
Batch 12/64 loss: 0.01362985372543335
Batch 13/64 loss: 0.03569817543029785
Batch 14/64 loss: 0.01922851800918579
Batch 15/64 loss: 0.004852771759033203
Batch 16/64 loss: 0.038278281688690186
Batch 17/64 loss: 0.025032103061676025
Batch 18/64 loss: 0.048523783683776855
Batch 19/64 loss: 0.022733688354492188
Batch 20/64 loss: 0.010570704936981201
Batch 21/64 loss: 0.019801020622253418
Batch 22/64 loss: 0.03511404991149902
Batch 23/64 loss: 0.01744908094406128
Batch 24/64 loss: 0.04161924123764038
Batch 25/64 loss: 0.021802425384521484
Batch 26/64 loss: 0.05009204149246216
Batch 27/64 loss: 0.05323493480682373
Batch 28/64 loss: 0.03295689821243286
Batch 29/64 loss: 0.053705573081970215
Batch 30/64 loss: 0.009927868843078613
Batch 31/64 loss: -0.0031639933586120605
Batch 32/64 loss: 0.03579890727996826
Batch 33/64 loss: 0.01009368896484375
Batch 34/64 loss: 0.005817234516143799
Batch 35/64 loss: 0.025444447994232178
Batch 36/64 loss: 0.022472143173217773
Batch 37/64 loss: 0.03546363115310669
Batch 38/64 loss: 0.033348917961120605
Batch 39/64 loss: 0.041459858417510986
Batch 40/64 loss: 0.006575584411621094
Batch 41/64 loss: 0.031583309173583984
Batch 42/64 loss: 0.017030537128448486
Batch 43/64 loss: 0.03059011697769165
Batch 44/64 loss: 0.008349180221557617
Batch 45/64 loss: 0.022089123725891113
Batch 46/64 loss: 0.018808722496032715
Batch 47/64 loss: 0.01600348949432373
Batch 48/64 loss: 0.025342106819152832
Batch 49/64 loss: 0.029212474822998047
Batch 50/64 loss: 0.010366082191467285
Batch 51/64 loss: 0.029697895050048828
Batch 52/64 loss: 0.021700799465179443
Batch 53/64 loss: -0.0021898746490478516
Batch 54/64 loss: 0.02222144603729248
Batch 55/64 loss: 0.063132643699646
Batch 56/64 loss: 0.007782101631164551
Batch 57/64 loss: 0.022023439407348633
Batch 58/64 loss: 0.04352015256881714
Batch 59/64 loss: 0.04504978656768799
Batch 60/64 loss: 0.035030245780944824
Batch 61/64 loss: 0.002123892307281494
Batch 62/64 loss: 0.046068012714385986
Batch 63/64 loss: 0.03506249189376831
Batch 64/64 loss: 0.00751495361328125
Epoch 97  Train loss: 0.02476217419493432  Val loss: 0.0845465332372082
Epoch 98
-------------------------------
Batch 1/64 loss: 0.014359831809997559
Batch 2/64 loss: -0.0018640756607055664
Batch 3/64 loss: 0.03132975101470947
Batch 4/64 loss: 0.027452051639556885
Batch 5/64 loss: 0.00889885425567627
Batch 6/64 loss: 0.045687198638916016
Batch 7/64 loss: 0.035493314266204834
Batch 8/64 loss: 0.020683348178863525
Batch 9/64 loss: 0.03046470880508423
Batch 10/64 loss: 0.02303147315979004
Batch 11/64 loss: 0.0296134352684021
Batch 12/64 loss: 0.04675090312957764
Batch 13/64 loss: 0.013304591178894043
Batch 14/64 loss: 0.011111021041870117
Batch 15/64 loss: 0.05374568700790405
Batch 16/64 loss: 0.022403478622436523
Batch 17/64 loss: 0.007642865180969238
Batch 18/64 loss: 0.021308422088623047
Batch 19/64 loss: 0.02136152982711792
Batch 20/64 loss: 0.022567927837371826
Batch 21/64 loss: 0.047551870346069336
Batch 22/64 loss: 0.0019974112510681152
Batch 23/64 loss: 0.029490113258361816
Batch 24/64 loss: 0.030605077743530273
Batch 25/64 loss: -0.0007569193840026855
Batch 26/64 loss: 0.021242856979370117
Batch 27/64 loss: 0.03275752067565918
Batch 28/64 loss: 0.027074456214904785
Batch 29/64 loss: 0.019982874393463135
Batch 30/64 loss: 0.05185699462890625
Batch 31/64 loss: 0.03158235549926758
Batch 32/64 loss: -0.004490971565246582
Batch 33/64 loss: 0.017995119094848633
Batch 34/64 loss: 0.025975406169891357
Batch 35/64 loss: 0.03850078582763672
Batch 36/64 loss: 0.018601655960083008
Batch 37/64 loss: 0.036307692527770996
Batch 38/64 loss: 0.030085623264312744
Batch 39/64 loss: 0.027272582054138184
Batch 40/64 loss: 0.02524876594543457
Batch 41/64 loss: -0.004753589630126953
Batch 42/64 loss: 0.015346825122833252
Batch 43/64 loss: 0.022020578384399414
Batch 44/64 loss: 0.00916677713394165
Batch 45/64 loss: 0.009693801403045654
Batch 46/64 loss: 0.055193305015563965
Batch 47/64 loss: 0.03446918725967407
Batch 48/64 loss: 0.026765525341033936
Batch 49/64 loss: 0.06127047538757324
Batch 50/64 loss: 0.009668469429016113
Batch 51/64 loss: 0.03503823280334473
Batch 52/64 loss: 0.03154712915420532
Batch 53/64 loss: 0.05465453863143921
Batch 54/64 loss: 0.02109229564666748
Batch 55/64 loss: 0.048677146434783936
Batch 56/64 loss: 0.0157623291015625
Batch 57/64 loss: 0.02358788251876831
Batch 58/64 loss: 0.0015932917594909668
Batch 59/64 loss: 0.025075078010559082
Batch 60/64 loss: 0.042378008365631104
Batch 61/64 loss: 0.009749352931976318
Batch 62/64 loss: 0.020691633224487305
Batch 63/64 loss: 0.02576810121536255
Batch 64/64 loss: 0.022722303867340088
Epoch 98  Train loss: 0.025187853972117105  Val loss: 0.09171225504367213
Epoch 99
-------------------------------
Batch 1/64 loss: 0.0587497353553772
Batch 2/64 loss: 0.02757394313812256
Batch 3/64 loss: 0.009473443031311035
Batch 4/64 loss: 0.03082031011581421
Batch 5/64 loss: 0.025548934936523438
Batch 6/64 loss: 0.02805614471435547
Batch 7/64 loss: 0.010788440704345703
Batch 8/64 loss: 0.02250427007675171
Batch 9/64 loss: 0.01561117172241211
Batch 10/64 loss: 0.028421878814697266
Batch 11/64 loss: 0.034082770347595215
Batch 12/64 loss: 0.023414313793182373
Batch 13/64 loss: 0.018354296684265137
Batch 14/64 loss: 0.03420192003250122
Batch 15/64 loss: 0.021733224391937256
Batch 16/64 loss: 0.03229111433029175
Batch 17/64 loss: 0.03751617670059204
Batch 18/64 loss: 0.006144285202026367
Batch 19/64 loss: 0.040125906467437744
Batch 20/64 loss: 0.04259622097015381
Batch 21/64 loss: 0.04619413614273071
Batch 22/64 loss: 0.024555563926696777
Batch 23/64 loss: 0.016923367977142334
Batch 24/64 loss: 0.02606511116027832
Batch 25/64 loss: 0.016856074333190918
Batch 26/64 loss: 0.00975722074508667
Batch 27/64 loss: 0.02202373743057251
Batch 28/64 loss: 0.02801990509033203
Batch 29/64 loss: 0.03134077787399292
Batch 30/64 loss: -0.006709635257720947
Batch 31/64 loss: 0.0218505859375
Batch 32/64 loss: 0.010799825191497803
Batch 33/64 loss: 0.02807694673538208
Batch 34/64 loss: 0.025289416313171387
Batch 35/64 loss: 0.015253603458404541
Batch 36/64 loss: 0.007703125476837158
Batch 37/64 loss: 0.0005995631217956543
Batch 38/64 loss: 0.025465965270996094
Batch 39/64 loss: 0.02129518985748291
Batch 40/64 loss: 0.024289250373840332
Batch 41/64 loss: 0.037375032901763916
Batch 42/64 loss: 0.03310114145278931
Batch 43/64 loss: 0.0024707913398742676
Batch 44/64 loss: 0.015561938285827637
Batch 45/64 loss: 0.0033805370330810547
Batch 46/64 loss: 0.024599909782409668
Batch 47/64 loss: 0.033389389514923096
Batch 48/64 loss: 0.0028616786003112793
Batch 49/64 loss: 0.030263662338256836
Batch 50/64 loss: 0.03240686655044556
Batch 51/64 loss: 0.032714784145355225
Batch 52/64 loss: 0.04176986217498779
Batch 53/64 loss: 0.053284525871276855
Batch 54/64 loss: 0.05090051889419556
Batch 55/64 loss: 0.012406766414642334
Batch 56/64 loss: 0.03873252868652344
Batch 57/64 loss: 0.014291226863861084
Batch 58/64 loss: 0.010573744773864746
Batch 59/64 loss: 0.01782524585723877
Batch 60/64 loss: -0.0014487504959106445
Batch 61/64 loss: 0.030626356601715088
Batch 62/64 loss: 0.01989579200744629
Batch 63/64 loss: 0.036076486110687256
Batch 64/64 loss: 0.014615178108215332
Epoch 99  Train loss: 0.023963602383931477  Val loss: 0.07280265301773228
Saving best model, epoch: 99
Epoch 100
-------------------------------
Batch 1/64 loss: 0.006845235824584961
Batch 2/64 loss: -0.011968433856964111
Batch 3/64 loss: 0.03302484750747681
Batch 4/64 loss: 0.00835198163986206
Batch 5/64 loss: 0.02538532018661499
Batch 6/64 loss: 0.007755577564239502
Batch 7/64 loss: 0.008181989192962646
Batch 8/64 loss: 0.016121387481689453
Batch 9/64 loss: 0.033799707889556885
Batch 10/64 loss: 0.10204517841339111
Batch 11/64 loss: 0.02374476194381714
Batch 12/64 loss: 0.02261495590209961
Batch 13/64 loss: 0.011597871780395508
Batch 14/64 loss: 0.03867262601852417
Batch 15/64 loss: 0.021815240383148193
Batch 16/64 loss: 0.021198511123657227
Batch 17/64 loss: -0.00689852237701416
Batch 18/64 loss: 0.009716212749481201
Batch 19/64 loss: -0.0010668039321899414
Batch 20/64 loss: -0.0033485889434814453
Batch 21/64 loss: 0.030026257038116455
Batch 22/64 loss: 0.028359413146972656
Batch 23/64 loss: 0.011638462543487549
Batch 24/64 loss: 0.03431743383407593
Batch 25/64 loss: 0.011252403259277344
Batch 26/64 loss: 0.02251458168029785
Batch 27/64 loss: 0.028298616409301758
Batch 28/64 loss: 0.012718677520751953
Batch 29/64 loss: 0.020721614360809326
Batch 30/64 loss: 0.023410499095916748
Batch 31/64 loss: 0.04853463172912598
Batch 32/64 loss: 0.001041412353515625
Batch 33/64 loss: 0.03034120798110962
Batch 34/64 loss: 0.027254998683929443
Batch 35/64 loss: 0.021353542804718018
Batch 36/64 loss: 0.04412519931793213
Batch 37/64 loss: 0.03168988227844238
Batch 38/64 loss: 0.024715185165405273
Batch 39/64 loss: 0.05161786079406738
Batch 40/64 loss: 0.030706048011779785
Batch 41/64 loss: 0.027645230293273926
Batch 42/64 loss: 0.026026904582977295
Batch 43/64 loss: 0.033091843128204346
Batch 44/64 loss: 0.029989540576934814
Batch 45/64 loss: 0.014860689640045166
Batch 46/64 loss: 0.050043344497680664
Batch 47/64 loss: 0.024429917335510254
Batch 48/64 loss: 0.02125263214111328
Batch 49/64 loss: 0.013485491275787354
Batch 50/64 loss: 0.032609641551971436
Batch 51/64 loss: 0.01930534839630127
Batch 52/64 loss: 0.05482429265975952
Batch 53/64 loss: 0.014643609523773193
Batch 54/64 loss: 0.02573680877685547
Batch 55/64 loss: 0.032682955265045166
Batch 56/64 loss: 0.014372706413269043
Batch 57/64 loss: 0.041149675846099854
Batch 58/64 loss: -0.0019015669822692871
Batch 59/64 loss: 0.025196313858032227
Batch 60/64 loss: 0.03829526901245117
Batch 61/64 loss: 0.007493853569030762
Batch 62/64 loss: 0.011125028133392334
Batch 63/64 loss: 0.0045171380043029785
Batch 64/64 loss: 0.045271873474121094
Epoch 100  Train loss: 0.02348325579774146  Val loss: 0.0729161521413482
Epoch 101
-------------------------------
Batch 1/64 loss: 0.025044798851013184
Batch 2/64 loss: 0.013971090316772461
Batch 3/64 loss: 0.02720111608505249
Batch 4/64 loss: 0.011003494262695312
Batch 5/64 loss: 0.012624621391296387
Batch 6/64 loss: 0.039659976959228516
Batch 7/64 loss: 0.025490999221801758
Batch 8/64 loss: 0.019928574562072754
Batch 9/64 loss: 0.06646883487701416
Batch 10/64 loss: 0.0326572060585022
Batch 11/64 loss: 0.003565192222595215
Batch 12/64 loss: 0.02063000202178955
Batch 13/64 loss: 0.03877115249633789
Batch 14/64 loss: -0.0003859996795654297
Batch 15/64 loss: 0.006489753723144531
Batch 16/64 loss: 0.033673882484436035
Batch 17/64 loss: 0.02820819616317749
Batch 18/64 loss: 0.003671109676361084
Batch 19/64 loss: -7.969141006469727e-05
Batch 20/64 loss: 0.022547781467437744
Batch 21/64 loss: 0.00843954086303711
Batch 22/64 loss: 0.04285752773284912
Batch 23/64 loss: 0.04658329486846924
Batch 24/64 loss: 0.03159892559051514
Batch 25/64 loss: 0.00743788480758667
Batch 26/64 loss: 0.03007817268371582
Batch 27/64 loss: 0.0036684274673461914
Batch 28/64 loss: 0.008832991123199463
Batch 29/64 loss: 0.04534250497817993
Batch 30/64 loss: -0.010954022407531738
Batch 31/64 loss: 0.03946423530578613
Batch 32/64 loss: 0.011195719242095947
Batch 33/64 loss: 0.0075151920318603516
Batch 34/64 loss: 0.02055460214614868
Batch 35/64 loss: 0.040098369121551514
Batch 36/64 loss: 0.011648416519165039
Batch 37/64 loss: 0.023693084716796875
Batch 38/64 loss: 0.025712192058563232
Batch 39/64 loss: 0.052370667457580566
Batch 40/64 loss: 0.03762108087539673
Batch 41/64 loss: 0.025282740592956543
Batch 42/64 loss: 0.04118800163269043
Batch 43/64 loss: -0.004590213298797607
Batch 44/64 loss: 0.04257237911224365
Batch 45/64 loss: 0.03308194875717163
Batch 46/64 loss: 0.023054778575897217
Batch 47/64 loss: 0.023668289184570312
Batch 48/64 loss: 0.006765604019165039
Batch 49/64 loss: 0.01563340425491333
Batch 50/64 loss: 0.02742105722427368
Batch 51/64 loss: 0.04149937629699707
Batch 52/64 loss: -0.011063992977142334
Batch 53/64 loss: 0.006908893585205078
Batch 54/64 loss: 0.011183679103851318
Batch 55/64 loss: 0.014939963817596436
Batch 56/64 loss: 0.01242983341217041
Batch 57/64 loss: 0.019446849822998047
Batch 58/64 loss: 0.020550966262817383
Batch 59/64 loss: 0.03183150291442871
Batch 60/64 loss: 0.03018850088119507
Batch 61/64 loss: 0.0444796085357666
Batch 62/64 loss: 0.05881547927856445
Batch 63/64 loss: 0.0329129695892334
Batch 64/64 loss: 0.01772165298461914
Epoch 101  Train loss: 0.023159117792166915  Val loss: 0.080812638567895
Epoch 102
-------------------------------
Batch 1/64 loss: 0.028883814811706543
Batch 2/64 loss: 0.014127850532531738
Batch 3/64 loss: -0.01692497730255127
Batch 4/64 loss: 0.04464060068130493
Batch 5/64 loss: 0.015475153923034668
Batch 6/64 loss: 0.011701345443725586
Batch 7/64 loss: 0.021975457668304443
Batch 8/64 loss: 0.055853188037872314
Batch 9/64 loss: 0.011620700359344482
Batch 10/64 loss: -0.0006384849548339844
Batch 11/64 loss: 0.025151550769805908
Batch 12/64 loss: 0.032288193702697754
Batch 13/64 loss: 0.039479970932006836
Batch 14/64 loss: 0.022894978523254395
Batch 15/64 loss: 0.04410290718078613
Batch 16/64 loss: 0.026806294918060303
Batch 17/64 loss: 0.0016465187072753906
Batch 18/64 loss: 0.012752056121826172
Batch 19/64 loss: 0.050875186920166016
Batch 20/64 loss: -0.004095852375030518
Batch 21/64 loss: 0.04858285188674927
Batch 22/64 loss: 0.015315115451812744
Batch 23/64 loss: 0.016716301441192627
Batch 24/64 loss: 0.011737167835235596
Batch 25/64 loss: 0.015320897102355957
Batch 26/64 loss: 0.011123061180114746
Batch 27/64 loss: 0.030970096588134766
Batch 28/64 loss: 0.028106093406677246
Batch 29/64 loss: 0.03461951017379761
Batch 30/64 loss: 0.05476236343383789
Batch 31/64 loss: 0.03591817617416382
Batch 32/64 loss: 0.03708153963088989
Batch 33/64 loss: 0.010194003582000732
Batch 34/64 loss: 4.76837158203125e-07
Batch 35/64 loss: 0.009770035743713379
Batch 36/64 loss: -0.00422060489654541
Batch 37/64 loss: 0.011873364448547363
Batch 38/64 loss: 0.003509044647216797
Batch 39/64 loss: 0.03391575813293457
Batch 40/64 loss: 0.02101379632949829
Batch 41/64 loss: -0.0004512667655944824
Batch 42/64 loss: 0.02736687660217285
Batch 43/64 loss: 0.02795875072479248
Batch 44/64 loss: 0.03160202503204346
Batch 45/64 loss: 0.012496471405029297
Batch 46/64 loss: 0.012069165706634521
Batch 47/64 loss: 0.007347762584686279
Batch 48/64 loss: 0.04255688190460205
Batch 49/64 loss: 0.004289448261260986
Batch 50/64 loss: 0.03410637378692627
Batch 51/64 loss: 0.03411799669265747
Batch 52/64 loss: 0.03162044286727905
Batch 53/64 loss: -0.0015993714332580566
Batch 54/64 loss: 0.014539122581481934
Batch 55/64 loss: 0.019111275672912598
Batch 56/64 loss: 0.016342639923095703
Batch 57/64 loss: 0.013353586196899414
Batch 58/64 loss: 0.03248465061187744
Batch 59/64 loss: 0.011996567249298096
Batch 60/64 loss: 0.03556859493255615
Batch 61/64 loss: 0.024873733520507812
Batch 62/64 loss: 0.015314698219299316
Batch 63/64 loss: 0.04584771394729614
Batch 64/64 loss: -0.003654003143310547
Epoch 102  Train loss: 0.021256457123101927  Val loss: 0.07046860825155199
Saving best model, epoch: 102
Epoch 103
-------------------------------
Batch 1/64 loss: -0.000977635383605957
Batch 2/64 loss: 0.021513700485229492
Batch 3/64 loss: 0.022998392581939697
Batch 4/64 loss: 0.011683344841003418
Batch 5/64 loss: 0.019383728504180908
Batch 6/64 loss: 0.012964069843292236
Batch 7/64 loss: 0.020388245582580566
Batch 8/64 loss: 0.008376598358154297
Batch 9/64 loss: 0.03458434343338013
Batch 10/64 loss: 0.017773687839508057
Batch 11/64 loss: 0.030273377895355225
Batch 12/64 loss: 0.021356523036956787
Batch 13/64 loss: 0.02372211217880249
Batch 14/64 loss: 0.0022842884063720703
Batch 15/64 loss: 0.004937648773193359
Batch 16/64 loss: 0.028724074363708496
Batch 17/64 loss: 0.015805840492248535
Batch 18/64 loss: 0.04319477081298828
Batch 19/64 loss: 0.007189452648162842
Batch 20/64 loss: -0.0015805363655090332
Batch 21/64 loss: 0.02629101276397705
Batch 22/64 loss: -0.006524085998535156
Batch 23/64 loss: 0.019431650638580322
Batch 24/64 loss: 0.02256917953491211
Batch 25/64 loss: 0.01979118585586548
Batch 26/64 loss: 0.042001962661743164
Batch 27/64 loss: 0.009284555912017822
Batch 28/64 loss: 0.01373589038848877
Batch 29/64 loss: 0.016789615154266357
Batch 30/64 loss: 0.008562803268432617
Batch 31/64 loss: 0.01925504207611084
Batch 32/64 loss: 0.021604478359222412
Batch 33/64 loss: 0.013861656188964844
Batch 34/64 loss: 0.04079383611679077
Batch 35/64 loss: 0.029016494750976562
Batch 36/64 loss: 0.038490891456604004
Batch 37/64 loss: 0.0405498743057251
Batch 38/64 loss: 0.013161897659301758
Batch 39/64 loss: 0.027246534824371338
Batch 40/64 loss: 0.04574519395828247
Batch 41/64 loss: 0.04024571180343628
Batch 42/64 loss: 0.00873643159866333
Batch 43/64 loss: 0.019184529781341553
Batch 44/64 loss: 0.03915059566497803
Batch 45/64 loss: 0.02190929651260376
Batch 46/64 loss: 0.019364237785339355
Batch 47/64 loss: 0.0461844801902771
Batch 48/64 loss: 0.04710590839385986
Batch 49/64 loss: -0.0041016340255737305
Batch 50/64 loss: 0.029162228107452393
Batch 51/64 loss: 0.0008330941200256348
Batch 52/64 loss: 0.010410547256469727
Batch 53/64 loss: 0.02698308229446411
Batch 54/64 loss: 0.0045729875564575195
Batch 55/64 loss: 0.024751782417297363
Batch 56/64 loss: 0.003987550735473633
Batch 57/64 loss: 0.03063863515853882
Batch 58/64 loss: 0.0014549493789672852
Batch 59/64 loss: 0.0010268688201904297
Batch 60/64 loss: 0.011560320854187012
Batch 61/64 loss: 0.004140317440032959
Batch 62/64 loss: 0.013263583183288574
Batch 63/64 loss: 0.026241540908813477
Batch 64/64 loss: 0.0323941707611084
Epoch 103  Train loss: 0.019723268583709118  Val loss: 0.06936060849743611
Saving best model, epoch: 103
Epoch 104
-------------------------------
Batch 1/64 loss: 0.0092812180519104
Batch 2/64 loss: 0.0400814414024353
Batch 3/64 loss: 0.010480999946594238
Batch 4/64 loss: 0.021673381328582764
Batch 5/64 loss: 0.033946454524993896
Batch 6/64 loss: 0.0023077726364135742
Batch 7/64 loss: 0.03177732229232788
Batch 8/64 loss: 0.025884151458740234
Batch 9/64 loss: 0.014184296131134033
Batch 10/64 loss: 0.00878065824508667
Batch 11/64 loss: 0.020677804946899414
Batch 12/64 loss: 0.012183725833892822
Batch 13/64 loss: 0.011995673179626465
Batch 14/64 loss: 0.02770388126373291
Batch 15/64 loss: 0.03595161437988281
Batch 16/64 loss: 0.013908147811889648
Batch 17/64 loss: 0.02640700340270996
Batch 18/64 loss: -0.0004159212112426758
Batch 19/64 loss: 0.003394603729248047
Batch 20/64 loss: 0.002949535846710205
Batch 21/64 loss: 0.018031597137451172
Batch 22/64 loss: 0.024649441242218018
Batch 23/64 loss: 0.03411751985549927
Batch 24/64 loss: 0.016856372356414795
Batch 25/64 loss: -0.00972890853881836
Batch 26/64 loss: 0.030955195426940918
Batch 27/64 loss: 0.05239492654800415
Batch 28/64 loss: 0.00025963783264160156
Batch 29/64 loss: 0.021373867988586426
Batch 30/64 loss: 0.0017475485801696777
Batch 31/64 loss: 0.05976998805999756
Batch 32/64 loss: 0.028848707675933838
Batch 33/64 loss: 0.047718048095703125
Batch 34/64 loss: 0.029254555702209473
Batch 35/64 loss: 0.02617788314819336
Batch 36/64 loss: 0.028409361839294434
Batch 37/64 loss: 0.0058321356773376465
Batch 38/64 loss: 0.010618031024932861
Batch 39/64 loss: 0.0009567737579345703
Batch 40/64 loss: 0.0064105987548828125
Batch 41/64 loss: 0.021214783191680908
Batch 42/64 loss: 0.002671539783477783
Batch 43/64 loss: 0.029755592346191406
Batch 44/64 loss: 0.02500736713409424
Batch 45/64 loss: 0.029010653495788574
Batch 46/64 loss: 0.02430671453475952
Batch 47/64 loss: 0.020068585872650146
Batch 48/64 loss: 0.013669192790985107
Batch 49/64 loss: 0.03365910053253174
Batch 50/64 loss: 0.009801983833312988
Batch 51/64 loss: 0.03522694110870361
Batch 52/64 loss: 0.03203117847442627
Batch 53/64 loss: 0.005213141441345215
Batch 54/64 loss: 0.011292517185211182
Batch 55/64 loss: 0.041069209575653076
Batch 56/64 loss: 0.018827199935913086
Batch 57/64 loss: 0.012716591358184814
Batch 58/64 loss: 0.02666604518890381
Batch 59/64 loss: 0.043443918228149414
Batch 60/64 loss: 0.026173174381256104
Batch 61/64 loss: 0.017936766147613525
Batch 62/64 loss: 0.0037081241607666016
Batch 63/64 loss: 0.01628190279006958
Batch 64/64 loss: -0.00561070442199707
Epoch 104  Train loss: 0.020130999883015952  Val loss: 0.07224738106285174
Epoch 105
-------------------------------
Batch 1/64 loss: 0.03507339954376221
Batch 2/64 loss: 0.013874053955078125
Batch 3/64 loss: -0.0110207200050354
Batch 4/64 loss: 0.0446852445602417
Batch 5/64 loss: 0.044621288776397705
Batch 6/64 loss: -0.010037660598754883
Batch 7/64 loss: 0.0266726016998291
Batch 8/64 loss: -0.004550755023956299
Batch 9/64 loss: 0.01620858907699585
Batch 10/64 loss: 0.006592214107513428
Batch 11/64 loss: 0.0512005090713501
Batch 12/64 loss: 0.026830613613128662
Batch 13/64 loss: 0.0023621320724487305
Batch 14/64 loss: -0.006072103977203369
Batch 15/64 loss: 0.02153414487838745
Batch 16/64 loss: 0.022389769554138184
Batch 17/64 loss: 0.011453211307525635
Batch 18/64 loss: 0.014801621437072754
Batch 19/64 loss: 0.020344674587249756
Batch 20/64 loss: 0.02426588535308838
Batch 21/64 loss: 0.04010462760925293
Batch 22/64 loss: -0.00033664703369140625
Batch 23/64 loss: 0.012330412864685059
Batch 24/64 loss: 0.038385868072509766
Batch 25/64 loss: 0.04303598403930664
Batch 26/64 loss: 0.005399823188781738
Batch 27/64 loss: 0.037026405334472656
Batch 28/64 loss: 0.02014613151550293
Batch 29/64 loss: 0.019265472888946533
Batch 30/64 loss: 0.026157259941101074
Batch 31/64 loss: 0.007644295692443848
Batch 32/64 loss: 0.038978636264801025
Batch 33/64 loss: 0.029641687870025635
Batch 34/64 loss: 0.0037602782249450684
Batch 35/64 loss: 0.03914642333984375
Batch 36/64 loss: 0.0259249210357666
Batch 37/64 loss: 0.03490030765533447
Batch 38/64 loss: -0.0037251710891723633
Batch 39/64 loss: 0.0030447840690612793
Batch 40/64 loss: 0.03319889307022095
Batch 41/64 loss: 0.03431069850921631
Batch 42/64 loss: 0.029510140419006348
Batch 43/64 loss: 0.023646891117095947
Batch 44/64 loss: -0.0073844194412231445
Batch 45/64 loss: -0.0006431341171264648
Batch 46/64 loss: 0.011533021926879883
Batch 47/64 loss: 0.004380583763122559
Batch 48/64 loss: 0.0065607428550720215
Batch 49/64 loss: 0.004355907440185547
Batch 50/64 loss: 0.014288127422332764
Batch 51/64 loss: 0.03148871660232544
Batch 52/64 loss: -0.0027478933334350586
Batch 53/64 loss: 0.04523134231567383
Batch 54/64 loss: 0.005831897258758545
Batch 55/64 loss: 0.01821422576904297
Batch 56/64 loss: 0.03214371204376221
Batch 57/64 loss: 0.018651902675628662
Batch 58/64 loss: 0.002175450325012207
Batch 59/64 loss: 0.0035938620567321777
Batch 60/64 loss: 0.040041446685791016
Batch 61/64 loss: 0.0073893070220947266
Batch 62/64 loss: 0.025171518325805664
Batch 63/64 loss: 0.017640769481658936
Batch 64/64 loss: 0.029524385929107666
Epoch 105  Train loss: 0.018710387688057096  Val loss: 0.08473940587945
Epoch 106
-------------------------------
Batch 1/64 loss: 0.019542157649993896
Batch 2/64 loss: 0.04180222749710083
Batch 3/64 loss: 0.0011540651321411133
Batch 4/64 loss: 0.003118276596069336
Batch 5/64 loss: 0.005755066871643066
Batch 6/64 loss: 0.06419140100479126
Batch 7/64 loss: 0.03425264358520508
Batch 8/64 loss: 0.0004055500030517578
Batch 9/64 loss: 0.03713047504425049
Batch 10/64 loss: -0.00850832462310791
Batch 11/64 loss: 0.016225218772888184
Batch 12/64 loss: -0.002885282039642334
Batch 13/64 loss: 0.0018691420555114746
Batch 14/64 loss: 0.00120621919631958
Batch 15/64 loss: 0.021844327449798584
Batch 16/64 loss: 0.013669967651367188
Batch 17/64 loss: -0.00196760892868042
Batch 18/64 loss: 0.023528218269348145
Batch 19/64 loss: -0.013627111911773682
Batch 20/64 loss: -0.014660954475402832
Batch 21/64 loss: 0.016875267028808594
Batch 22/64 loss: 0.0006033182144165039
Batch 23/64 loss: 0.007743716239929199
Batch 24/64 loss: 0.0032825469970703125
Batch 25/64 loss: 0.000983893871307373
Batch 26/64 loss: 0.012188076972961426
Batch 27/64 loss: 0.016364693641662598
Batch 28/64 loss: 0.0332375168800354
Batch 29/64 loss: 0.03448706865310669
Batch 30/64 loss: 0.020702004432678223
Batch 31/64 loss: 0.015468120574951172
Batch 32/64 loss: 0.028139233589172363
Batch 33/64 loss: -0.007710456848144531
Batch 34/64 loss: 0.015032172203063965
Batch 35/64 loss: 0.035874783992767334
Batch 36/64 loss: 0.04121196269989014
Batch 37/64 loss: 0.030503809452056885
Batch 38/64 loss: 0.0015489459037780762
Batch 39/64 loss: -0.0017431974411010742
Batch 40/64 loss: 0.04470479488372803
Batch 41/64 loss: -0.0021335482597351074
Batch 42/64 loss: 0.012480199337005615
Batch 43/64 loss: 0.010522902011871338
Batch 44/64 loss: 0.02330338954925537
Batch 45/64 loss: 0.01936030387878418
Batch 46/64 loss: 0.03787505626678467
Batch 47/64 loss: 0.03256654739379883
Batch 48/64 loss: 0.015545129776000977
Batch 49/64 loss: 0.017611801624298096
Batch 50/64 loss: 0.021309971809387207
Batch 51/64 loss: 0.03661268949508667
Batch 52/64 loss: 0.009989440441131592
Batch 53/64 loss: 0.030314326286315918
Batch 54/64 loss: 0.026910126209259033
Batch 55/64 loss: 0.021892309188842773
Batch 56/64 loss: 0.033035337924957275
Batch 57/64 loss: 0.03585708141326904
Batch 58/64 loss: 0.03666132688522339
Batch 59/64 loss: 0.0020082592964172363
Batch 60/64 loss: 0.0259244441986084
Batch 61/64 loss: 0.03238332271575928
Batch 62/64 loss: 0.05650663375854492
Batch 63/64 loss: -0.01834583282470703
Batch 64/64 loss: 0.026786327362060547
Epoch 106  Train loss: 0.017754116245344575  Val loss: 0.07942403386958276
Epoch 107
-------------------------------
Batch 1/64 loss: 0.017909646034240723
Batch 2/64 loss: 0.02524667978286743
Batch 3/64 loss: 0.01012575626373291
Batch 4/64 loss: 0.03822439908981323
Batch 5/64 loss: 0.015728533267974854
Batch 6/64 loss: 0.016274571418762207
Batch 7/64 loss: 0.03285503387451172
Batch 8/64 loss: 0.006978869438171387
Batch 9/64 loss: 0.031440556049346924
Batch 10/64 loss: -0.00042188167572021484
Batch 11/64 loss: -0.002937793731689453
Batch 12/64 loss: 0.014428555965423584
Batch 13/64 loss: 0.03368234634399414
Batch 14/64 loss: -0.0034828782081604004
Batch 15/64 loss: 0.009134888648986816
Batch 16/64 loss: 0.01706564426422119
Batch 17/64 loss: 0.03393673896789551
Batch 18/64 loss: 0.023368537425994873
Batch 19/64 loss: 0.012445688247680664
Batch 20/64 loss: 0.00030744075775146484
Batch 21/64 loss: 0.024998188018798828
Batch 22/64 loss: 0.0312732458114624
Batch 23/64 loss: 0.029521644115447998
Batch 24/64 loss: 0.018151521682739258
Batch 25/64 loss: 0.015115559101104736
Batch 26/64 loss: 0.003919780254364014
Batch 27/64 loss: 0.02444368600845337
Batch 28/64 loss: 0.01872342824935913
Batch 29/64 loss: 0.011975586414337158
Batch 30/64 loss: 0.014563143253326416
Batch 31/64 loss: 0.04097718000411987
Batch 32/64 loss: 0.01116567850112915
Batch 33/64 loss: 0.017085373401641846
Batch 34/64 loss: 0.008543789386749268
Batch 35/64 loss: 0.024829387664794922
Batch 36/64 loss: -0.010682344436645508
Batch 37/64 loss: -0.016557157039642334
Batch 38/64 loss: 0.02767777442932129
Batch 39/64 loss: -0.008682072162628174
Batch 40/64 loss: -0.014885544776916504
Batch 41/64 loss: 0.02102029323577881
Batch 42/64 loss: 0.0001156926155090332
Batch 43/64 loss: 0.027759194374084473
Batch 44/64 loss: 0.018244147300720215
Batch 45/64 loss: 0.01942455768585205
Batch 46/64 loss: 0.0067130327224731445
Batch 47/64 loss: 0.01855027675628662
Batch 48/64 loss: -0.008237004280090332
Batch 49/64 loss: 0.03593200445175171
Batch 50/64 loss: 0.033145010471343994
Batch 51/64 loss: 0.04045206308364868
Batch 52/64 loss: 0.042098939418792725
Batch 53/64 loss: 0.007846474647521973
Batch 54/64 loss: 0.007919669151306152
Batch 55/64 loss: 0.06907778978347778
Batch 56/64 loss: 0.02023404836654663
Batch 57/64 loss: 0.004021048545837402
Batch 58/64 loss: 0.010463595390319824
Batch 59/64 loss: 0.007340967655181885
Batch 60/64 loss: 0.008477628231048584
Batch 61/64 loss: 0.03357309103012085
Batch 62/64 loss: 0.03273576498031616
Batch 63/64 loss: 0.013880014419555664
Batch 64/64 loss: 0.0028606653213500977
Epoch 107  Train loss: 0.016900438888400208  Val loss: 0.07206300116076912
Epoch 108
-------------------------------
Batch 1/64 loss: 0.021762490272521973
Batch 2/64 loss: 0.031087040901184082
Batch 3/64 loss: 0.029607772827148438
Batch 4/64 loss: 0.020292699337005615
Batch 5/64 loss: 0.008808314800262451
Batch 6/64 loss: 0.04093301296234131
Batch 7/64 loss: 0.01617807149887085
Batch 8/64 loss: 0.011951446533203125
Batch 9/64 loss: 0.019092917442321777
Batch 10/64 loss: 0.040550827980041504
Batch 11/64 loss: -0.0028495192527770996
Batch 12/64 loss: 0.02275395393371582
Batch 13/64 loss: 0.011261224746704102
Batch 14/64 loss: 0.007024407386779785
Batch 15/64 loss: 0.021234869956970215
Batch 16/64 loss: 0.025050818920135498
Batch 17/64 loss: 0.02577155828475952
Batch 18/64 loss: 9.989738464355469e-05
Batch 19/64 loss: 0.008996248245239258
Batch 20/64 loss: -0.0023688077926635742
Batch 21/64 loss: 0.022491276264190674
Batch 22/64 loss: -0.018976271152496338
Batch 23/64 loss: 0.028314054012298584
Batch 24/64 loss: 0.010197877883911133
Batch 25/64 loss: 0.029920876026153564
Batch 26/64 loss: 0.0051885247230529785
Batch 27/64 loss: 0.00901496410369873
Batch 28/64 loss: -0.004250049591064453
Batch 29/64 loss: 0.012282609939575195
Batch 30/64 loss: 0.016672492027282715
Batch 31/64 loss: -0.01676344871520996
Batch 32/64 loss: 0.024437010288238525
Batch 33/64 loss: 0.015470266342163086
Batch 34/64 loss: 0.020835459232330322
Batch 35/64 loss: 0.03987091779708862
Batch 36/64 loss: 0.02977573871612549
Batch 37/64 loss: 0.019068658351898193
Batch 38/64 loss: -0.006901800632476807
Batch 39/64 loss: 0.004016518592834473
Batch 40/64 loss: 0.04042816162109375
Batch 41/64 loss: 0.0568695068359375
Batch 42/64 loss: 0.029796957969665527
Batch 43/64 loss: 0.016641676425933838
Batch 44/64 loss: 0.03178274631500244
Batch 45/64 loss: 0.02186405658721924
Batch 46/64 loss: 0.001499474048614502
Batch 47/64 loss: -0.004035592079162598
Batch 48/64 loss: 0.005778193473815918
Batch 49/64 loss: -0.006317257881164551
Batch 50/64 loss: -0.009608626365661621
Batch 51/64 loss: 0.00022405385971069336
Batch 52/64 loss: 0.0040024518966674805
Batch 53/64 loss: 0.004829347133636475
Batch 54/64 loss: 0.015847325325012207
Batch 55/64 loss: 0.033604979515075684
Batch 56/64 loss: 0.02115452289581299
Batch 57/64 loss: 0.03407245874404907
Batch 58/64 loss: -0.0019658803939819336
Batch 59/64 loss: 0.011726856231689453
Batch 60/64 loss: 0.016746819019317627
Batch 61/64 loss: 0.04268646240234375
Batch 62/64 loss: 0.0450749397277832
Batch 63/64 loss: 0.01718151569366455
Batch 64/64 loss: 0.00826716423034668
Epoch 108  Train loss: 0.016219457925534714  Val loss: 0.07066471531628743
Epoch 109
-------------------------------
Batch 1/64 loss: -0.001983046531677246
Batch 2/64 loss: 0.016865313053131104
Batch 3/64 loss: 0.008731067180633545
Batch 4/64 loss: -0.0017266273498535156
Batch 5/64 loss: 0.009229183197021484
Batch 6/64 loss: 0.028854548931121826
Batch 7/64 loss: -0.0052741169929504395
Batch 8/64 loss: 0.017960667610168457
Batch 9/64 loss: 0.029662787914276123
Batch 10/64 loss: 0.023958563804626465
Batch 11/64 loss: 0.03543633222579956
Batch 12/64 loss: 0.043347954750061035
Batch 13/64 loss: -0.011785507202148438
Batch 14/64 loss: 0.0026764273643493652
Batch 15/64 loss: 0.003307342529296875
Batch 16/64 loss: 0.03545212745666504
Batch 17/64 loss: 0.017316818237304688
Batch 18/64 loss: 0.007631957530975342
Batch 19/64 loss: -0.004020333290100098
Batch 20/64 loss: 0.024837851524353027
Batch 21/64 loss: -0.0011322498321533203
Batch 22/64 loss: 0.03479057550430298
Batch 23/64 loss: 0.028348922729492188
Batch 24/64 loss: 0.00387418270111084
Batch 25/64 loss: 0.020991623401641846
Batch 26/64 loss: 0.028271734714508057
Batch 27/64 loss: 0.01856982707977295
Batch 28/64 loss: 0.005810797214508057
Batch 29/64 loss: 0.017423629760742188
Batch 30/64 loss: -0.0010756254196166992
Batch 31/64 loss: 0.05842512845993042
Batch 32/64 loss: 0.030228078365325928
Batch 33/64 loss: -0.007955849170684814
Batch 34/64 loss: 0.018655061721801758
Batch 35/64 loss: 0.001408219337463379
Batch 36/64 loss: 0.006979405879974365
Batch 37/64 loss: 0.011358320713043213
Batch 38/64 loss: -0.019119977951049805
Batch 39/64 loss: 0.018711388111114502
Batch 40/64 loss: 0.013261973857879639
Batch 41/64 loss: 0.0013763904571533203
Batch 42/64 loss: 0.027357876300811768
Batch 43/64 loss: 0.026694536209106445
Batch 44/64 loss: 0.03929948806762695
Batch 45/64 loss: 0.017332613468170166
Batch 46/64 loss: 0.00517117977142334
Batch 47/64 loss: 0.011486351490020752
Batch 48/64 loss: 0.033743441104888916
Batch 49/64 loss: -0.0025982260704040527
Batch 50/64 loss: 0.005382359027862549
Batch 51/64 loss: 0.016902029514312744
Batch 52/64 loss: 0.004752635955810547
Batch 53/64 loss: 0.024003982543945312
Batch 54/64 loss: 0.02310049533843994
Batch 55/64 loss: 0.02293074131011963
Batch 56/64 loss: 0.004610836505889893
Batch 57/64 loss: 0.0023761391639709473
Batch 58/64 loss: -0.0021824240684509277
Batch 59/64 loss: 0.040929198265075684
Batch 60/64 loss: 0.019344866275787354
Batch 61/64 loss: 0.015787124633789062
Batch 62/64 loss: 0.005456268787384033
Batch 63/64 loss: -0.006231248378753662
Batch 64/64 loss: 0.024943113327026367
Epoch 109  Train loss: 0.014494721094767253  Val loss: 0.07271730449191484
Epoch 110
-------------------------------
Batch 1/64 loss: 0.026599109172821045
Batch 2/64 loss: 0.007352173328399658
Batch 3/64 loss: 0.022969961166381836
Batch 4/64 loss: 0.019244372844696045
Batch 5/64 loss: 0.0036379098892211914
Batch 6/64 loss: 0.01129060983657837
Batch 7/64 loss: -0.00391775369644165
Batch 8/64 loss: 0.004543423652648926
Batch 9/64 loss: 0.00038695335388183594
Batch 10/64 loss: 0.02350860834121704
Batch 11/64 loss: 0.04163461923599243
Batch 12/64 loss: 0.02165853977203369
Batch 13/64 loss: 0.014029741287231445
Batch 14/64 loss: 0.028504908084869385
Batch 15/64 loss: 0.003132641315460205
Batch 16/64 loss: 0.012654304504394531
Batch 17/64 loss: 0.019988179206848145
Batch 18/64 loss: -0.019497394561767578
Batch 19/64 loss: 0.005355119705200195
Batch 20/64 loss: 0.018964290618896484
Batch 21/64 loss: 0.003635585308074951
Batch 22/64 loss: 0.040714263916015625
Batch 23/64 loss: 0.032212257385253906
Batch 24/64 loss: 0.0038988590240478516
Batch 25/64 loss: 0.02964574098587036
Batch 26/64 loss: 0.0025536417961120605
Batch 27/64 loss: 0.012656867504119873
Batch 28/64 loss: 0.01673954725265503
Batch 29/64 loss: -0.006934463977813721
Batch 30/64 loss: -0.0011467337608337402
Batch 31/64 loss: 0.014622747898101807
Batch 32/64 loss: 0.003809332847595215
Batch 33/64 loss: 0.01302480697631836
Batch 34/64 loss: -0.011247217655181885
Batch 35/64 loss: 0.020516037940979004
Batch 36/64 loss: 0.019925475120544434
Batch 37/64 loss: 0.008944034576416016
Batch 38/64 loss: 0.005170166492462158
Batch 39/64 loss: 0.013755500316619873
Batch 40/64 loss: 0.005273342132568359
Batch 41/64 loss: 0.010442972183227539
Batch 42/64 loss: 0.03498029708862305
Batch 43/64 loss: -0.026283740997314453
Batch 44/64 loss: 0.04356759786605835
Batch 45/64 loss: -0.015913844108581543
Batch 46/64 loss: 0.03599834442138672
Batch 47/64 loss: 0.023840606212615967
Batch 48/64 loss: 0.028397858142852783
Batch 49/64 loss: 0.02264106273651123
Batch 50/64 loss: -0.006723225116729736
Batch 51/64 loss: 0.014092862606048584
Batch 52/64 loss: 0.027827322483062744
Batch 53/64 loss: 0.03702437877655029
Batch 54/64 loss: 0.013091683387756348
Batch 55/64 loss: 0.006455421447753906
Batch 56/64 loss: 0.010334908962249756
Batch 57/64 loss: 0.03511679172515869
Batch 58/64 loss: 0.03921914100646973
Batch 59/64 loss: 0.005757331848144531
Batch 60/64 loss: 0.05233597755432129
Batch 61/64 loss: 0.005327999591827393
Batch 62/64 loss: -0.0030994415283203125
Batch 63/64 loss: -0.014410734176635742
Batch 64/64 loss: 0.009875595569610596
Epoch 110  Train loss: 0.013760602006725237  Val loss: 0.07729123957788002
Epoch 111
-------------------------------
Batch 1/64 loss: 0.029413819313049316
Batch 2/64 loss: -0.025736212730407715
Batch 3/64 loss: 0.020627140998840332
Batch 4/64 loss: 0.0257682204246521
Batch 5/64 loss: -0.01153033971786499
Batch 6/64 loss: 0.02473074197769165
Batch 7/64 loss: -0.011598348617553711
Batch 8/64 loss: 0.026621222496032715
Batch 9/64 loss: 0.030402839183807373
Batch 10/64 loss: 0.019141972064971924
Batch 11/64 loss: 0.020727455615997314
Batch 12/64 loss: 0.013733386993408203
Batch 13/64 loss: -0.002318739891052246
Batch 14/64 loss: 0.015999436378479004
Batch 15/64 loss: 0.03806710243225098
Batch 16/64 loss: -0.005509853363037109
Batch 17/64 loss: 0.020377933979034424
Batch 18/64 loss: -0.0024422407150268555
Batch 19/64 loss: -0.0011458396911621094
Batch 20/64 loss: 0.019721150398254395
Batch 21/64 loss: 0.027164876461029053
Batch 22/64 loss: -0.0004807710647583008
Batch 23/64 loss: -0.006369888782501221
Batch 24/64 loss: -0.009634792804718018
Batch 25/64 loss: -0.00019109249114990234
Batch 26/64 loss: 0.016745567321777344
Batch 27/64 loss: 0.03886079788208008
Batch 28/64 loss: 0.007101237773895264
Batch 29/64 loss: 0.007701277732849121
Batch 30/64 loss: 0.01366513967514038
Batch 31/64 loss: 0.03607761859893799
Batch 32/64 loss: -0.012807726860046387
Batch 33/64 loss: 0.007861912250518799
Batch 34/64 loss: 0.04200321435928345
Batch 35/64 loss: 0.030237793922424316
Batch 36/64 loss: 0.027214467525482178
Batch 37/64 loss: 0.02670919895172119
Batch 38/64 loss: 0.022992730140686035
Batch 39/64 loss: -0.011874616146087646
Batch 40/64 loss: -0.0072196125984191895
Batch 41/64 loss: 0.011876702308654785
Batch 42/64 loss: 0.007528245449066162
Batch 43/64 loss: 0.008874773979187012
Batch 44/64 loss: 0.016825079917907715
Batch 45/64 loss: 0.03724324703216553
Batch 46/64 loss: 0.004018843173980713
Batch 47/64 loss: 0.01574122905731201
Batch 48/64 loss: 0.04357707500457764
Batch 49/64 loss: 0.028981029987335205
Batch 50/64 loss: 0.009887993335723877
Batch 51/64 loss: 0.0006785392761230469
Batch 52/64 loss: -0.0005506277084350586
Batch 53/64 loss: 0.011868119239807129
Batch 54/64 loss: 0.002605259418487549
Batch 55/64 loss: -0.0056514739990234375
Batch 56/64 loss: 0.013410568237304688
Batch 57/64 loss: -0.009524643421173096
Batch 58/64 loss: 0.01780831813812256
Batch 59/64 loss: 0.017278194427490234
Batch 60/64 loss: -0.010164439678192139
Batch 61/64 loss: 0.012702643871307373
Batch 62/64 loss: 0.028621912002563477
Batch 63/64 loss: 0.005865275859832764
Batch 64/64 loss: 0.008839130401611328
Epoch 111  Train loss: 0.012187284581801471  Val loss: 0.07607411334604741
Epoch 112
-------------------------------
Batch 1/64 loss: -0.00040608644485473633
Batch 2/64 loss: 0.008491754531860352
Batch 3/64 loss: 0.019017577171325684
Batch 4/64 loss: 0.009331345558166504
Batch 5/64 loss: -0.022797107696533203
Batch 6/64 loss: 0.02419126033782959
Batch 7/64 loss: 0.006022810935974121
Batch 8/64 loss: 0.012585282325744629
Batch 9/64 loss: 0.019901514053344727
Batch 10/64 loss: -0.008737683296203613
Batch 11/64 loss: 0.013731122016906738
Batch 12/64 loss: 0.024717748165130615
Batch 13/64 loss: 0.016293823719024658
Batch 14/64 loss: 0.020351409912109375
Batch 15/64 loss: 0.011158764362335205
Batch 16/64 loss: -0.023545145988464355
Batch 17/64 loss: 0.031868696212768555
Batch 18/64 loss: 0.007919013500213623
Batch 19/64 loss: 0.031986236572265625
Batch 20/64 loss: 0.016466259956359863
Batch 21/64 loss: 0.03984498977661133
Batch 22/64 loss: 0.011606991291046143
Batch 23/64 loss: 0.013534009456634521
Batch 24/64 loss: 0.012238085269927979
Batch 25/64 loss: 0.00871044397354126
Batch 26/64 loss: 0.02066558599472046
Batch 27/64 loss: 0.009521126747131348
Batch 28/64 loss: 0.02491307258605957
Batch 29/64 loss: 0.007239639759063721
Batch 30/64 loss: 0.041410088539123535
Batch 31/64 loss: 0.02851581573486328
Batch 32/64 loss: 0.014695823192596436
Batch 33/64 loss: 0.00649183988571167
Batch 34/64 loss: 0.0015829205513000488
Batch 35/64 loss: -0.0015755891799926758
Batch 36/64 loss: 0.019397616386413574
Batch 37/64 loss: 0.014789402484893799
Batch 38/64 loss: 0.021849751472473145
Batch 39/64 loss: 0.012426614761352539
Batch 40/64 loss: 0.0006064176559448242
Batch 41/64 loss: -0.013623237609863281
Batch 42/64 loss: -0.008913159370422363
Batch 43/64 loss: 0.0008598566055297852
Batch 44/64 loss: -0.0016848444938659668
Batch 45/64 loss: 0.015571892261505127
Batch 46/64 loss: 0.02951294183731079
Batch 47/64 loss: 0.0182722806930542
Batch 48/64 loss: -0.010777771472930908
Batch 49/64 loss: 0.00612252950668335
Batch 50/64 loss: 0.051661908626556396
Batch 51/64 loss: 0.0056122541427612305
Batch 52/64 loss: 0.04776132106781006
Batch 53/64 loss: 0.008073925971984863
Batch 54/64 loss: 0.010435640811920166
Batch 55/64 loss: 0.03340059518814087
Batch 56/64 loss: 0.009541511535644531
Batch 57/64 loss: 0.014233052730560303
Batch 58/64 loss: 0.0021965503692626953
Batch 59/64 loss: 0.00010395050048828125
Batch 60/64 loss: 0.026084423065185547
Batch 61/64 loss: 0.010963916778564453
Batch 62/64 loss: -0.010668456554412842
Batch 63/64 loss: -0.010989785194396973
Batch 64/64 loss: 0.01620584726333618
Epoch 112  Train loss: 0.012124218426498712  Val loss: 0.07436691711039067
Epoch 113
-------------------------------
Batch 1/64 loss: 0.015187382698059082
Batch 2/64 loss: 0.010456681251525879
Batch 3/64 loss: 0.011541664600372314
Batch 4/64 loss: 0.014301598072052002
Batch 5/64 loss: 0.002040684223175049
Batch 6/64 loss: -0.0018172860145568848
Batch 7/64 loss: -0.0030807852745056152
Batch 8/64 loss: 0.020858943462371826
Batch 9/64 loss: 0.02744913101196289
Batch 10/64 loss: -0.000997006893157959
Batch 11/64 loss: 0.008769452571868896
Batch 12/64 loss: 0.004415988922119141
Batch 13/64 loss: 0.004349946975708008
Batch 14/64 loss: 0.02167034149169922
Batch 15/64 loss: -0.006598770618438721
Batch 16/64 loss: 0.008998572826385498
Batch 17/64 loss: 0.0030871033668518066
Batch 18/64 loss: 0.01154094934463501
Batch 19/64 loss: -0.007650196552276611
Batch 20/64 loss: 0.024707674980163574
Batch 21/64 loss: 0.0052016377449035645
Batch 22/64 loss: -0.005653262138366699
Batch 23/64 loss: 0.01598459482192993
Batch 24/64 loss: 0.005399584770202637
Batch 25/64 loss: -0.02286839485168457
Batch 26/64 loss: 0.0006825923919677734
Batch 27/64 loss: 0.010637402534484863
Batch 28/64 loss: 0.0351482629776001
Batch 29/64 loss: 0.011328518390655518
Batch 30/64 loss: 0.04065847396850586
Batch 31/64 loss: 0.03485405445098877
Batch 32/64 loss: 0.007190287113189697
Batch 33/64 loss: 0.03833407163619995
Batch 34/64 loss: 0.015621662139892578
Batch 35/64 loss: 0.008205592632293701
Batch 36/64 loss: 0.03522300720214844
Batch 37/64 loss: 0.007278561592102051
Batch 38/64 loss: 0.00489276647567749
Batch 39/64 loss: 0.011429786682128906
Batch 40/64 loss: 0.03212994337081909
Batch 41/64 loss: -0.0005841851234436035
Batch 42/64 loss: 0.009888172149658203
Batch 43/64 loss: 0.01354604959487915
Batch 44/64 loss: 0.030355870723724365
Batch 45/64 loss: 0.015071392059326172
Batch 46/64 loss: -0.010133743286132812
Batch 47/64 loss: 0.04507189989089966
Batch 48/64 loss: 0.028675973415374756
Batch 49/64 loss: -0.006415367126464844
Batch 50/64 loss: 0.009382069110870361
Batch 51/64 loss: -0.00956493616104126
Batch 52/64 loss: 0.00972127914428711
Batch 53/64 loss: -0.01126164197921753
Batch 54/64 loss: 0.028200209140777588
Batch 55/64 loss: 0.0008736252784729004
Batch 56/64 loss: -0.012839436531066895
Batch 57/64 loss: -0.011026561260223389
Batch 58/64 loss: 0.007357656955718994
Batch 59/64 loss: -3.230571746826172e-05
Batch 60/64 loss: 0.01722884178161621
Batch 61/64 loss: -0.004698514938354492
Batch 62/64 loss: 0.014562010765075684
Batch 63/64 loss: 0.023562133312225342
Batch 64/64 loss: 0.0019574761390686035
Epoch 113  Train loss: 0.010185408825967825  Val loss: 0.06997526613707394
Epoch 114
-------------------------------
Batch 1/64 loss: -0.007174253463745117
Batch 2/64 loss: -0.0006064176559448242
Batch 3/64 loss: 0.025087356567382812
Batch 4/64 loss: 0.008168041706085205
Batch 5/64 loss: 0.004567146301269531
Batch 6/64 loss: 0.017448723316192627
Batch 7/64 loss: -0.00810098648071289
Batch 8/64 loss: -0.03177303075790405
Batch 9/64 loss: -0.0016584396362304688
Batch 10/64 loss: 0.006512820720672607
Batch 11/64 loss: 0.02797609567642212
Batch 12/64 loss: -0.012290120124816895
Batch 13/64 loss: -0.01381218433380127
Batch 14/64 loss: 0.028956711292266846
Batch 15/64 loss: 0.0018149614334106445
Batch 16/64 loss: -0.009530901908874512
Batch 17/64 loss: 0.04239976406097412
Batch 18/64 loss: 0.015347599983215332
Batch 19/64 loss: 0.021506309509277344
Batch 20/64 loss: -0.01902008056640625
Batch 21/64 loss: -0.008673787117004395
Batch 22/64 loss: 0.01920032501220703
Batch 23/64 loss: 0.0014556050300598145
Batch 24/64 loss: 0.0039103031158447266
Batch 25/64 loss: 0.014234542846679688
Batch 26/64 loss: -0.004803299903869629
Batch 27/64 loss: 0.005330681800842285
Batch 28/64 loss: 0.02327418327331543
Batch 29/64 loss: 0.03580951690673828
Batch 30/64 loss: 0.019314587116241455
Batch 31/64 loss: 0.04580813646316528
Batch 32/64 loss: 0.013299107551574707
Batch 33/64 loss: 0.0017841458320617676
Batch 34/64 loss: 0.024195551872253418
Batch 35/64 loss: 0.0035410523414611816
Batch 36/64 loss: 0.0136793851852417
Batch 37/64 loss: 0.024993062019348145
Batch 38/64 loss: 0.03659200668334961
Batch 39/64 loss: 0.025357067584991455
Batch 40/64 loss: 0.022777438163757324
Batch 41/64 loss: 0.0007914304733276367
Batch 42/64 loss: 0.03147464990615845
Batch 43/64 loss: 0.00800168514251709
Batch 44/64 loss: -0.00835561752319336
Batch 45/64 loss: 0.010014355182647705
Batch 46/64 loss: -0.01014941930770874
Batch 47/64 loss: 0.005700230598449707
Batch 48/64 loss: 0.009474873542785645
Batch 49/64 loss: 0.0352749228477478
Batch 50/64 loss: 0.0024824142456054688
Batch 51/64 loss: 0.012823045253753662
Batch 52/64 loss: 0.010814189910888672
Batch 53/64 loss: -0.0012696385383605957
Batch 54/64 loss: 0.016706407070159912
Batch 55/64 loss: 0.019975662231445312
Batch 56/64 loss: 0.021757423877716064
Batch 57/64 loss: 0.02509075403213501
Batch 58/64 loss: -0.0006735324859619141
Batch 59/64 loss: 0.05550265312194824
Batch 60/64 loss: 0.03459155559539795
Batch 61/64 loss: 0.036792218685150146
Batch 62/64 loss: 0.01034015417098999
Batch 63/64 loss: -0.017300903797149658
Batch 64/64 loss: 0.028513729572296143
Epoch 114  Train loss: 0.011735584922865326  Val loss: 0.07225156178589129
Epoch 115
-------------------------------
Batch 1/64 loss: 0.017555415630340576
Batch 2/64 loss: 0.006865441799163818
Batch 3/64 loss: 0.01195615530014038
Batch 4/64 loss: -0.009100615978240967
Batch 5/64 loss: -0.010025084018707275
Batch 6/64 loss: 0.013359248638153076
Batch 7/64 loss: -0.02023404836654663
Batch 8/64 loss: 0.0003502964973449707
Batch 9/64 loss: -0.011486709117889404
Batch 10/64 loss: 0.005065739154815674
Batch 11/64 loss: 0.0187339186668396
Batch 12/64 loss: 0.009087443351745605
Batch 13/64 loss: 0.023759007453918457
Batch 14/64 loss: -0.018694639205932617
Batch 15/64 loss: -5.805492401123047e-05
Batch 16/64 loss: 0.014630794525146484
Batch 17/64 loss: 0.02414339780807495
Batch 18/64 loss: -0.0019213557243347168
Batch 19/64 loss: -0.005051136016845703
Batch 20/64 loss: -0.00908273458480835
Batch 21/64 loss: -0.0011475086212158203
Batch 22/64 loss: 0.0244748592376709
Batch 23/64 loss: 0.005807936191558838
Batch 24/64 loss: 0.03684341907501221
Batch 25/64 loss: 0.03563189506530762
Batch 26/64 loss: 0.012211978435516357
Batch 27/64 loss: 0.020359277725219727
Batch 28/64 loss: 0.017712950706481934
Batch 29/64 loss: 0.014881491661071777
Batch 30/64 loss: 0.02535557746887207
Batch 31/64 loss: 0.02835547924041748
Batch 32/64 loss: 0.009191811084747314
Batch 33/64 loss: 0.020031332969665527
Batch 34/64 loss: 0.03589928150177002
Batch 35/64 loss: 0.024480164051055908
Batch 36/64 loss: 0.012472808361053467
Batch 37/64 loss: -0.004874885082244873
Batch 38/64 loss: 0.01700460910797119
Batch 39/64 loss: 0.01424473524093628
Batch 40/64 loss: 0.02466106414794922
Batch 41/64 loss: 0.02787703275680542
Batch 42/64 loss: 0.028041422367095947
Batch 43/64 loss: 0.002208232879638672
Batch 44/64 loss: -0.009875893592834473
Batch 45/64 loss: -0.007292032241821289
Batch 46/64 loss: 0.0189058780670166
Batch 47/64 loss: -0.0013068318367004395
Batch 48/64 loss: 0.02419722080230713
Batch 49/64 loss: 0.0006394386291503906
Batch 50/64 loss: -0.00027549266815185547
Batch 51/64 loss: 0.022436797618865967
Batch 52/64 loss: -0.003339707851409912
Batch 53/64 loss: 0.01984483003616333
Batch 54/64 loss: -0.0006552934646606445
Batch 55/64 loss: 0.017116963863372803
Batch 56/64 loss: 0.0022333860397338867
Batch 57/64 loss: -0.011594712734222412
Batch 58/64 loss: 0.007147669792175293
Batch 59/64 loss: 0.0024246573448181152
Batch 60/64 loss: 0.0051264166831970215
Batch 61/64 loss: 0.017811894416809082
Batch 62/64 loss: 0.030723929405212402
Batch 63/64 loss: 0.023444533348083496
Batch 64/64 loss: 0.004502058029174805
Epoch 115  Train loss: 0.010237923790426815  Val loss: 0.07227929924771548
Epoch 116
-------------------------------
Batch 1/64 loss: -0.007268130779266357
Batch 2/64 loss: 0.017540335655212402
Batch 3/64 loss: 0.00829857587814331
Batch 4/64 loss: -0.014640092849731445
Batch 5/64 loss: 0.01956874132156372
Batch 6/64 loss: 0.021981120109558105
Batch 7/64 loss: 0.003309488296508789
Batch 8/64 loss: 0.02830195426940918
Batch 9/64 loss: -0.016942918300628662
Batch 10/64 loss: 0.01949441432952881
Batch 11/64 loss: -0.02037990093231201
Batch 12/64 loss: 0.04266023635864258
Batch 13/64 loss: 0.004874169826507568
Batch 14/64 loss: 0.010165631771087646
Batch 15/64 loss: -0.03858840465545654
Batch 16/64 loss: 0.00989389419555664
Batch 17/64 loss: 0.008873343467712402
Batch 18/64 loss: 0.02015441656112671
Batch 19/64 loss: 0.01711636781692505
Batch 20/64 loss: -0.0026988983154296875
Batch 21/64 loss: 0.0007679462432861328
Batch 22/64 loss: 0.0049307942390441895
Batch 23/64 loss: 0.005766808986663818
Batch 24/64 loss: 0.0013989806175231934
Batch 25/64 loss: -0.0026941895484924316
Batch 26/64 loss: 0.027672648429870605
Batch 27/64 loss: 0.006268203258514404
Batch 28/64 loss: 0.02249467372894287
Batch 29/64 loss: 0.012515127658843994
Batch 30/64 loss: 0.004475235939025879
Batch 31/64 loss: -0.0039033889770507812
Batch 32/64 loss: -0.026606976985931396
Batch 33/64 loss: 0.008618295192718506
Batch 34/64 loss: 0.009159326553344727
Batch 35/64 loss: -0.0018572211265563965
Batch 36/64 loss: -0.005654811859130859
Batch 37/64 loss: -0.0036318302154541016
Batch 38/64 loss: -0.007543325424194336
Batch 39/64 loss: -0.0084494948387146
Batch 40/64 loss: 0.005861163139343262
Batch 41/64 loss: 0.035107553005218506
Batch 42/64 loss: 0.01134335994720459
Batch 43/64 loss: -0.02237832546234131
Batch 44/64 loss: 0.010900497436523438
Batch 45/64 loss: 0.006101846694946289
Batch 46/64 loss: 0.02993631362915039
Batch 47/64 loss: -0.005182147026062012
Batch 48/64 loss: 0.015490949153900146
Batch 49/64 loss: 0.004984498023986816
Batch 50/64 loss: 0.05789154767990112
Batch 51/64 loss: 0.026464998722076416
Batch 52/64 loss: 0.02014482021331787
Batch 53/64 loss: 0.006452202796936035
Batch 54/64 loss: 0.03316318988800049
Batch 55/64 loss: 0.021230101585388184
Batch 56/64 loss: 0.009955644607543945
Batch 57/64 loss: 0.006987273693084717
Batch 58/64 loss: 0.024127602577209473
Batch 59/64 loss: 0.02331089973449707
Batch 60/64 loss: 0.012203454971313477
Batch 61/64 loss: 0.011700451374053955
Batch 62/64 loss: 0.02159970998764038
Batch 63/64 loss: 0.0018830299377441406
Batch 64/64 loss: 0.00022745132446289062
Epoch 116  Train loss: 0.008547331305111155  Val loss: 0.09106973253984221
Epoch 117
-------------------------------
Batch 1/64 loss: -0.018786966800689697
Batch 2/64 loss: 0.021320223808288574
Batch 3/64 loss: -0.002121448516845703
Batch 4/64 loss: -0.004377961158752441
Batch 5/64 loss: 0.029021263122558594
Batch 6/64 loss: 0.027944624423980713
Batch 7/64 loss: 0.03577160835266113
Batch 8/64 loss: -0.020781874656677246
Batch 9/64 loss: -0.0010500550270080566
Batch 10/64 loss: 0.008565664291381836
Batch 11/64 loss: -0.012148380279541016
Batch 12/64 loss: 0.029071688652038574
Batch 13/64 loss: -0.006626129150390625
Batch 14/64 loss: 0.0038260817527770996
Batch 15/64 loss: 0.03727364540100098
Batch 16/64 loss: 0.016573190689086914
Batch 17/64 loss: 0.0063901543617248535
Batch 18/64 loss: 0.022224247455596924
Batch 19/64 loss: 0.001111447811126709
Batch 20/64 loss: 0.0007970333099365234
Batch 21/64 loss: -0.004641354084014893
Batch 22/64 loss: 0.006024956703186035
Batch 23/64 loss: -0.002590000629425049
Batch 24/64 loss: 0.005594313144683838
Batch 25/64 loss: 0.01121455430984497
Batch 26/64 loss: 0.026733338832855225
Batch 27/64 loss: 0.0003885626792907715
Batch 28/64 loss: -0.009496569633483887
Batch 29/64 loss: 0.041967570781707764
Batch 30/64 loss: -0.022651970386505127
Batch 31/64 loss: 0.02526259422302246
Batch 32/64 loss: -0.0063132643699646
Batch 33/64 loss: 0.02601301670074463
Batch 34/64 loss: 0.007880687713623047
Batch 35/64 loss: 0.04780995845794678
Batch 36/64 loss: 0.010114789009094238
Batch 37/64 loss: -0.011395096778869629
Batch 38/64 loss: 0.014308273792266846
Batch 39/64 loss: 0.0031365156173706055
Batch 40/64 loss: -0.013318359851837158
Batch 41/64 loss: 0.04174584150314331
Batch 42/64 loss: -0.014036953449249268
Batch 43/64 loss: 0.017982125282287598
Batch 44/64 loss: -0.022703826427459717
Batch 45/64 loss: 0.02491438388824463
Batch 46/64 loss: 0.0043631792068481445
Batch 47/64 loss: 0.00708925724029541
Batch 48/64 loss: 0.004285633563995361
Batch 49/64 loss: -0.0007333159446716309
Batch 50/64 loss: 0.03932178020477295
Batch 51/64 loss: 0.004885196685791016
Batch 52/64 loss: 0.016378164291381836
Batch 53/64 loss: -0.012670040130615234
Batch 54/64 loss: 0.03725355863571167
Batch 55/64 loss: -0.0006626248359680176
Batch 56/64 loss: 0.008633613586425781
Batch 57/64 loss: 0.008827567100524902
Batch 58/64 loss: 0.016702234745025635
Batch 59/64 loss: 0.008516073226928711
Batch 60/64 loss: 0.013562917709350586
Batch 61/64 loss: 0.004627048969268799
Batch 62/64 loss: -0.00021660327911376953
Batch 63/64 loss: 0.03173243999481201
Batch 64/64 loss: -0.0024285316467285156
Epoch 117  Train loss: 0.008910067876180013  Val loss: 0.07922957934874439
Epoch 118
-------------------------------
Batch 1/64 loss: 0.0038703083992004395
Batch 2/64 loss: 0.001623690128326416
Batch 3/64 loss: -0.008036017417907715
Batch 4/64 loss: 0.011728405952453613
Batch 5/64 loss: -0.0024167299270629883
Batch 6/64 loss: 0.019561171531677246
Batch 7/64 loss: 0.050868332386016846
Batch 8/64 loss: 0.010274112224578857
Batch 9/64 loss: 0.014035463333129883
Batch 10/64 loss: 0.02134472131729126
Batch 11/64 loss: 0.0051406025886535645
Batch 12/64 loss: 0.015352249145507812
Batch 13/64 loss: 0.018266379833221436
Batch 14/64 loss: 0.024756431579589844
Batch 15/64 loss: -0.000579535961151123
Batch 16/64 loss: 0.011261820793151855
Batch 17/64 loss: 0.03701961040496826
Batch 18/64 loss: -0.012678861618041992
Batch 19/64 loss: 0.011122643947601318
Batch 20/64 loss: -0.0005543828010559082
Batch 21/64 loss: 0.01900947093963623
Batch 22/64 loss: -0.0028946399688720703
Batch 23/64 loss: -0.020707249641418457
Batch 24/64 loss: 0.005640566349029541
Batch 25/64 loss: -0.0032368898391723633
Batch 26/64 loss: -0.005026519298553467
Batch 27/64 loss: -0.0038106441497802734
Batch 28/64 loss: 0.009234309196472168
Batch 29/64 loss: 0.012722194194793701
Batch 30/64 loss: -0.007262289524078369
Batch 31/64 loss: -0.016260504722595215
Batch 32/64 loss: 0.008109331130981445
Batch 33/64 loss: -0.0008436441421508789
Batch 34/64 loss: -0.01015561819076538
Batch 35/64 loss: -0.01197516918182373
Batch 36/64 loss: 0.006077408790588379
Batch 37/64 loss: 0.005827546119689941
Batch 38/64 loss: -0.008108556270599365
Batch 39/64 loss: -0.0016202926635742188
Batch 40/64 loss: 0.0006469488143920898
Batch 41/64 loss: 0.015458405017852783
Batch 42/64 loss: 0.023099184036254883
Batch 43/64 loss: 0.03671598434448242
Batch 44/64 loss: -0.0012706518173217773
Batch 45/64 loss: 0.019530057907104492
Batch 46/64 loss: -0.0010298490524291992
Batch 47/64 loss: 0.0200883150100708
Batch 48/64 loss: 0.005433142185211182
Batch 49/64 loss: 0.006785333156585693
Batch 50/64 loss: 0.0008869171142578125
Batch 51/64 loss: -0.008440196514129639
Batch 52/64 loss: 0.0016564726829528809
Batch 53/64 loss: 0.027759790420532227
Batch 54/64 loss: 0.00445556640625
Batch 55/64 loss: 0.024231255054473877
Batch 56/64 loss: 0.004482746124267578
Batch 57/64 loss: 0.0029584765434265137
Batch 58/64 loss: 0.009089648723602295
Batch 59/64 loss: 0.031217217445373535
Batch 60/64 loss: -0.013141095638275146
Batch 61/64 loss: -0.013281166553497314
Batch 62/64 loss: -0.009813666343688965
Batch 63/64 loss: -0.0026900768280029297
Batch 64/64 loss: 0.015166044235229492
Epoch 118  Train loss: 0.0063197257472019566  Val loss: 0.07242718656448155
Epoch 119
-------------------------------
Batch 1/64 loss: -0.0035296082496643066
Batch 2/64 loss: -0.003468930721282959
Batch 3/64 loss: -0.024818718433380127
Batch 4/64 loss: -0.014591753482818604
Batch 5/64 loss: -0.013305902481079102
Batch 6/64 loss: 0.048604488372802734
Batch 7/64 loss: 0.0017592310905456543
Batch 8/64 loss: -0.0041381120681762695
Batch 9/64 loss: -0.005705773830413818
Batch 10/64 loss: 0.037433505058288574
Batch 11/64 loss: 0.0034122467041015625
Batch 12/64 loss: -0.004882931709289551
Batch 13/64 loss: 0.004259705543518066
Batch 14/64 loss: 0.013819456100463867
Batch 15/64 loss: 0.0110548734664917
Batch 16/64 loss: 0.005281507968902588
Batch 17/64 loss: 0.010901153087615967
Batch 18/64 loss: -0.024669766426086426
Batch 19/64 loss: 0.004788041114807129
Batch 20/64 loss: -0.03450685739517212
Batch 21/64 loss: 0.021619677543640137
Batch 22/64 loss: 0.05702638626098633
Batch 23/64 loss: 0.021074116230010986
Batch 24/64 loss: 0.03189355134963989
Batch 25/64 loss: 0.03668785095214844
Batch 26/64 loss: -0.0007401704788208008
Batch 27/64 loss: -0.002075493335723877
Batch 28/64 loss: 0.002948760986328125
Batch 29/64 loss: 0.009316444396972656
Batch 30/64 loss: -0.02025526762008667
Batch 31/64 loss: 0.03097224235534668
Batch 32/64 loss: 0.01469278335571289
Batch 33/64 loss: -0.007284939289093018
Batch 34/64 loss: -0.014744579792022705
Batch 35/64 loss: 0.0066446661949157715
Batch 36/64 loss: 0.042930424213409424
Batch 37/64 loss: -0.004109084606170654
Batch 38/64 loss: 0.015149414539337158
Batch 39/64 loss: 0.0008213520050048828
Batch 40/64 loss: 0.005007266998291016
Batch 41/64 loss: -0.008770585060119629
Batch 42/64 loss: 0.00792020559310913
Batch 43/64 loss: 0.02106928825378418
Batch 44/64 loss: 0.0023528337478637695
Batch 45/64 loss: 0.02725958824157715
Batch 46/64 loss: -0.010217666625976562
Batch 47/64 loss: 0.005681514739990234
Batch 48/64 loss: -0.007853269577026367
Batch 49/64 loss: 0.013929128646850586
Batch 50/64 loss: 0.004962801933288574
Batch 51/64 loss: 0.0069921016693115234
Batch 52/64 loss: 0.00585019588470459
Batch 53/64 loss: -0.010542154312133789
Batch 54/64 loss: -0.004072308540344238
Batch 55/64 loss: 0.002547144889831543
Batch 56/64 loss: 0.01650702953338623
Batch 57/64 loss: -0.004004776477813721
Batch 58/64 loss: -0.0038747787475585938
Batch 59/64 loss: 0.022027909755706787
Batch 60/64 loss: -0.005786895751953125
Batch 61/64 loss: 0.011206507682800293
Batch 62/64 loss: 0.0035816431045532227
Batch 63/64 loss: 0.0023540258407592773
Batch 64/64 loss: -0.016315937042236328
Epoch 119  Train loss: 0.005367118237065334  Val loss: 0.06790942834414977
Saving best model, epoch: 119
Epoch 120
-------------------------------
Batch 1/64 loss: -0.00862807035446167
Batch 2/64 loss: 0.022816359996795654
Batch 3/64 loss: -0.01052236557006836
Batch 4/64 loss: -0.027016878128051758
Batch 5/64 loss: 0.03178936243057251
Batch 6/64 loss: 0.024985194206237793
Batch 7/64 loss: 0.012108922004699707
Batch 8/64 loss: 0.0023266077041625977
Batch 9/64 loss: -0.007621288299560547
Batch 10/64 loss: -0.01675856113433838
Batch 11/64 loss: 8.177757263183594e-05
Batch 12/64 loss: -0.0019103288650512695
Batch 13/64 loss: -0.005855560302734375
Batch 14/64 loss: 0.004654407501220703
Batch 15/64 loss: 0.010100007057189941
Batch 16/64 loss: 0.029282093048095703
Batch 17/64 loss: -0.008036315441131592
Batch 18/64 loss: -0.005491733551025391
Batch 19/64 loss: 0.010230839252471924
Batch 20/64 loss: 0.00017249584197998047
Batch 21/64 loss: 0.02013295888900757
Batch 22/64 loss: 0.008842229843139648
Batch 23/64 loss: 0.01827824115753174
Batch 24/64 loss: 0.01535409688949585
Batch 25/64 loss: 0.009325683116912842
Batch 26/64 loss: -0.005458354949951172
Batch 27/64 loss: 0.0033077597618103027
Batch 28/64 loss: 0.027567923069000244
Batch 29/64 loss: 0.004373669624328613
Batch 30/64 loss: 0.02786386013031006
Batch 31/64 loss: -0.025159239768981934
Batch 32/64 loss: 0.01827681064605713
Batch 33/64 loss: 0.0022446513175964355
Batch 34/64 loss: 0.01470416784286499
Batch 35/64 loss: 0.0014303326606750488
Batch 36/64 loss: 0.015659093856811523
Batch 37/64 loss: 0.004406332969665527
Batch 38/64 loss: 0.02024078369140625
Batch 39/64 loss: 0.03079324960708618
Batch 40/64 loss: 0.020376384258270264
Batch 41/64 loss: -0.01724839210510254
Batch 42/64 loss: 0.037828683853149414
Batch 43/64 loss: -0.015719890594482422
Batch 44/64 loss: -0.000970005989074707
Batch 45/64 loss: -0.0001544952392578125
Batch 46/64 loss: 0.0020419955253601074
Batch 47/64 loss: 0.019345104694366455
Batch 48/64 loss: 0.01379537582397461
Batch 49/64 loss: -0.007958471775054932
Batch 50/64 loss: 0.00028651952743530273
Batch 51/64 loss: -0.0042572021484375
Batch 52/64 loss: -0.00374758243560791
Batch 53/64 loss: -0.001676321029663086
Batch 54/64 loss: -0.00020188093185424805
Batch 55/64 loss: 0.012112140655517578
Batch 56/64 loss: 0.02177971601486206
Batch 57/64 loss: 0.005858898162841797
Batch 58/64 loss: 0.018854737281799316
Batch 59/64 loss: 0.02424520254135132
Batch 60/64 loss: 0.014819979667663574
Batch 61/64 loss: 0.006450355052947998
Batch 62/64 loss: -0.001962900161743164
Batch 63/64 loss: -0.010318398475646973
Batch 64/64 loss: 0.03355938196182251
Epoch 120  Train loss: 0.006708083199519737  Val loss: 0.08277668473646813
Epoch 121
-------------------------------
Batch 1/64 loss: 0.029653847217559814
Batch 2/64 loss: -0.01349031925201416
Batch 3/64 loss: -0.004003047943115234
Batch 4/64 loss: 0.0052329301834106445
Batch 5/64 loss: -0.010203123092651367
Batch 6/64 loss: 0.008721888065338135
Batch 7/64 loss: -0.012677311897277832
Batch 8/64 loss: -0.003350555896759033
Batch 9/64 loss: -0.01041114330291748
Batch 10/64 loss: -0.004896640777587891
Batch 11/64 loss: -0.001485288143157959
Batch 12/64 loss: -0.015632569789886475
Batch 13/64 loss: 0.028460144996643066
Batch 14/64 loss: 0.003588378429412842
Batch 15/64 loss: -0.004187464714050293
Batch 16/64 loss: 0.026353538036346436
Batch 17/64 loss: -0.0033648014068603516
Batch 18/64 loss: -0.0031554698944091797
Batch 19/64 loss: -0.003905355930328369
Batch 20/64 loss: 0.0013052821159362793
Batch 21/64 loss: 0.007021546363830566
Batch 22/64 loss: 0.025948703289031982
Batch 23/64 loss: 0.01130533218383789
Batch 24/64 loss: 0.005342364311218262
Batch 25/64 loss: 0.025251328945159912
Batch 26/64 loss: -0.011624455451965332
Batch 27/64 loss: -0.007236123085021973
Batch 28/64 loss: -0.0017660856246948242
Batch 29/64 loss: 0.000892937183380127
Batch 30/64 loss: -0.0198746919631958
Batch 31/64 loss: -0.0007228851318359375
Batch 32/64 loss: -0.024948477745056152
Batch 33/64 loss: 0.009233534336090088
Batch 34/64 loss: 0.017332494258880615
Batch 35/64 loss: 0.004970192909240723
Batch 36/64 loss: 0.0358738899230957
Batch 37/64 loss: -0.011540770530700684
Batch 38/64 loss: 0.020998716354370117
Batch 39/64 loss: 0.031592488288879395
Batch 40/64 loss: 0.01040804386138916
Batch 41/64 loss: -0.008421182632446289
Batch 42/64 loss: 0.041268229484558105
Batch 43/64 loss: 0.012878775596618652
Batch 44/64 loss: -0.02261495590209961
Batch 45/64 loss: 0.0144997239112854
Batch 46/64 loss: -0.014478981494903564
Batch 47/64 loss: 0.026723802089691162
Batch 48/64 loss: -0.008914709091186523
Batch 49/64 loss: 0.02150702476501465
Batch 50/64 loss: 0.005213618278503418
Batch 51/64 loss: -0.0009444355964660645
Batch 52/64 loss: 0.01481175422668457
Batch 53/64 loss: 0.017428994178771973
Batch 54/64 loss: -0.010314106941223145
Batch 55/64 loss: -0.020923972129821777
Batch 56/64 loss: -0.0266876220703125
Batch 57/64 loss: 0.008219718933105469
Batch 58/64 loss: -0.0039044618606567383
Batch 59/64 loss: 0.005187869071960449
Batch 60/64 loss: -0.017294645309448242
Batch 61/64 loss: 0.029257476329803467
Batch 62/64 loss: 0.0018714070320129395
Batch 63/64 loss: 0.018656909465789795
Batch 64/64 loss: 0.00515592098236084
Epoch 121  Train loss: 0.003574967384338379  Val loss: 0.07597035128636048
Epoch 122
-------------------------------
Batch 1/64 loss: 0.03369283676147461
Batch 2/64 loss: 0.008364975452423096
Batch 3/64 loss: 2.3126602172851562e-05
Batch 4/64 loss: 0.001261591911315918
Batch 5/64 loss: -0.013262331485748291
Batch 6/64 loss: 0.006489813327789307
Batch 7/64 loss: 0.055858612060546875
Batch 8/64 loss: -0.016063570976257324
Batch 9/64 loss: -0.008157730102539062
Batch 10/64 loss: -0.042319416999816895
Batch 11/64 loss: -0.01632225513458252
Batch 12/64 loss: -0.0098036527633667
Batch 13/64 loss: 0.003512084484100342
Batch 14/64 loss: 0.03338205814361572
Batch 15/64 loss: -0.01458364725112915
Batch 16/64 loss: 0.036376953125
Batch 17/64 loss: 0.02372664213180542
Batch 18/64 loss: 0.007410883903503418
Batch 19/64 loss: 0.015607237815856934
Batch 20/64 loss: -0.006341695785522461
Batch 21/64 loss: 0.022957563400268555
Batch 22/64 loss: -0.0030944347381591797
Batch 23/64 loss: 0.002934873104095459
Batch 24/64 loss: 0.014982342720031738
Batch 25/64 loss: 0.029518067836761475
Batch 26/64 loss: 0.01728212833404541
Batch 27/64 loss: 0.00037723779678344727
Batch 28/64 loss: 0.01616692543029785
Batch 29/64 loss: 0.0021919608116149902
Batch 30/64 loss: 0.008373022079467773
Batch 31/64 loss: -0.0019229650497436523
Batch 32/64 loss: 0.0021274685859680176
Batch 33/64 loss: 0.00475698709487915
Batch 34/64 loss: 0.018941164016723633
Batch 35/64 loss: -0.005572319030761719
Batch 36/64 loss: -0.014806926250457764
Batch 37/64 loss: 0.007073938846588135
Batch 38/64 loss: 0.035429954528808594
Batch 39/64 loss: -0.02203536033630371
Batch 40/64 loss: 0.001003563404083252
Batch 41/64 loss: -0.02316230535507202
Batch 42/64 loss: 0.02397078275680542
Batch 43/64 loss: 0.025310099124908447
Batch 44/64 loss: -0.0263979434967041
Batch 45/64 loss: 0.045340776443481445
Batch 46/64 loss: 0.01221531629562378
Batch 47/64 loss: 0.02760767936706543
Batch 48/64 loss: -0.005140185356140137
Batch 49/64 loss: 0.0026012063026428223
Batch 50/64 loss: -0.028157949447631836
Batch 51/64 loss: -0.0009451508522033691
Batch 52/64 loss: 0.007827222347259521
Batch 53/64 loss: -0.00850379467010498
Batch 54/64 loss: -0.01032710075378418
Batch 55/64 loss: -0.006091773509979248
Batch 56/64 loss: -0.00857478380203247
Batch 57/64 loss: -0.03801524639129639
Batch 58/64 loss: 0.011381745338439941
Batch 59/64 loss: 0.011427462100982666
Batch 60/64 loss: -0.01598525047302246
Batch 61/64 loss: -0.00025194883346557617
Batch 62/64 loss: 0.0035117268562316895
Batch 63/64 loss: -0.016948461532592773
Batch 64/64 loss: -0.016220569610595703
Epoch 122  Train loss: 0.0032323827930525236  Val loss: 0.06630111519004062
Saving best model, epoch: 122
Epoch 123
-------------------------------
Batch 1/64 loss: 0.014952123165130615
Batch 2/64 loss: -0.011425614356994629
Batch 3/64 loss: -0.007107913494110107
Batch 4/64 loss: -0.013113677501678467
Batch 5/64 loss: -0.011195182800292969
Batch 6/64 loss: 0.0072089433670043945
Batch 7/64 loss: -0.002621173858642578
Batch 8/64 loss: -0.02143073081970215
Batch 9/64 loss: 0.018141508102416992
Batch 10/64 loss: -0.025223076343536377
Batch 11/64 loss: -0.002144157886505127
Batch 12/64 loss: 0.00032401084899902344
Batch 13/64 loss: -0.010682523250579834
Batch 14/64 loss: 0.0023036599159240723
Batch 15/64 loss: -0.0017940402030944824
Batch 16/64 loss: 0.007320225238800049
Batch 17/64 loss: -0.014563679695129395
Batch 18/64 loss: -0.00155562162399292
Batch 19/64 loss: 0.0006204843521118164
Batch 20/64 loss: 0.008078396320343018
Batch 21/64 loss: 0.027112066745758057
Batch 22/64 loss: -0.003221452236175537
Batch 23/64 loss: 0.03082859516143799
Batch 24/64 loss: 0.017854273319244385
Batch 25/64 loss: 0.008962631225585938
Batch 26/64 loss: -0.01234525442123413
Batch 27/64 loss: 0.0017888545989990234
Batch 28/64 loss: -0.007975637912750244
Batch 29/64 loss: -0.007603883743286133
Batch 30/64 loss: 0.00835508108139038
Batch 31/64 loss: 0.006594061851501465
Batch 32/64 loss: -0.02851325273513794
Batch 33/64 loss: -0.009893298149108887
Batch 34/64 loss: 0.005566298961639404
Batch 35/64 loss: 0.007909595966339111
Batch 36/64 loss: -0.0023201704025268555
Batch 37/64 loss: 0.03417348861694336
Batch 38/64 loss: 0.012841522693634033
Batch 39/64 loss: -0.0012328624725341797
Batch 40/64 loss: 0.021802902221679688
Batch 41/64 loss: 0.008233785629272461
Batch 42/64 loss: -0.00287020206451416
Batch 43/64 loss: 0.015262007713317871
Batch 44/64 loss: 0.05088698863983154
Batch 45/64 loss: 0.029255688190460205
Batch 46/64 loss: 0.006684660911560059
Batch 47/64 loss: 0.017866134643554688
Batch 48/64 loss: 0.030650436878204346
Batch 49/64 loss: 0.025867164134979248
Batch 50/64 loss: -0.0011663436889648438
Batch 51/64 loss: -0.00575566291809082
Batch 52/64 loss: 0.006272494792938232
Batch 53/64 loss: 0.00788414478302002
Batch 54/64 loss: 0.021665751934051514
Batch 55/64 loss: 0.019508838653564453
Batch 56/64 loss: 0.012386918067932129
Batch 57/64 loss: 0.008882999420166016
Batch 58/64 loss: 0.02241361141204834
Batch 59/64 loss: -0.017696261405944824
Batch 60/64 loss: -0.004818260669708252
Batch 61/64 loss: -0.012863636016845703
Batch 62/64 loss: -0.0075838565826416016
Batch 63/64 loss: 0.0021626949310302734
Batch 64/64 loss: 0.01091080904006958
Epoch 123  Train loss: 0.004519038808112051  Val loss: 0.07018165346683096
Epoch 124
-------------------------------
Batch 1/64 loss: -0.02000904083251953
Batch 2/64 loss: 0.009512901306152344
Batch 3/64 loss: -0.03208202123641968
Batch 4/64 loss: 0.006547868251800537
Batch 5/64 loss: -0.004233658313751221
Batch 6/64 loss: -0.0038886070251464844
Batch 7/64 loss: -0.0012660622596740723
Batch 8/64 loss: 0.017218410968780518
Batch 9/64 loss: 0.024480879306793213
Batch 10/64 loss: 0.02856987714767456
Batch 11/64 loss: -0.01130521297454834
Batch 12/64 loss: -0.002840876579284668
Batch 13/64 loss: -0.0038785934448242188
Batch 14/64 loss: -0.014227509498596191
Batch 15/64 loss: -0.0006400346755981445
Batch 16/64 loss: 0.019140958786010742
Batch 17/64 loss: -0.004674434661865234
Batch 18/64 loss: 0.010986506938934326
Batch 19/64 loss: 0.003847360610961914
Batch 20/64 loss: 0.025423645973205566
Batch 21/64 loss: -0.0020527243614196777
Batch 22/64 loss: -0.0033918023109436035
Batch 23/64 loss: -0.00365525484085083
Batch 24/64 loss: -0.0038850903511047363
Batch 25/64 loss: 0.00711512565612793
Batch 26/64 loss: -0.0043354034423828125
Batch 27/64 loss: -0.028668344020843506
Batch 28/64 loss: 0.01029980182647705
Batch 29/64 loss: 0.0025190114974975586
Batch 30/64 loss: 0.009024739265441895
Batch 31/64 loss: -0.001064300537109375
Batch 32/64 loss: -0.016949355602264404
Batch 33/64 loss: -0.0003922581672668457
Batch 34/64 loss: -0.00898587703704834
Batch 35/64 loss: -0.007483184337615967
Batch 36/64 loss: -0.024824023246765137
Batch 37/64 loss: 0.000615239143371582
Batch 38/64 loss: 0.011853456497192383
Batch 39/64 loss: 0.01767951250076294
Batch 40/64 loss: 0.033194780349731445
Batch 41/64 loss: -0.023127079010009766
Batch 42/64 loss: -0.017659544944763184
Batch 43/64 loss: -0.02278655767440796
Batch 44/64 loss: -0.010099530220031738
Batch 45/64 loss: 0.035295844078063965
Batch 46/64 loss: -0.002447843551635742
Batch 47/64 loss: 0.019393086433410645
Batch 48/64 loss: 0.0017989873886108398
Batch 49/64 loss: 0.020773887634277344
Batch 50/64 loss: 0.030245065689086914
Batch 51/64 loss: -0.021337509155273438
Batch 52/64 loss: -0.011513888835906982
Batch 53/64 loss: 0.002681732177734375
Batch 54/64 loss: 0.0026258230209350586
Batch 55/64 loss: -0.008115887641906738
Batch 56/64 loss: 0.01119297742843628
Batch 57/64 loss: -0.010495305061340332
Batch 58/64 loss: 0.04694509506225586
Batch 59/64 loss: 0.005057930946350098
Batch 60/64 loss: 0.02283799648284912
Batch 61/64 loss: 0.005278110504150391
Batch 62/64 loss: 0.016574084758758545
Batch 63/64 loss: -0.004408121109008789
Batch 64/64 loss: 0.011204242706298828
Epoch 124  Train loss: 0.002045630473716586  Val loss: 0.08145098219212797
Epoch 125
-------------------------------
Batch 1/64 loss: -0.011210322380065918
Batch 2/64 loss: 0.017526447772979736
Batch 3/64 loss: 0.02149200439453125
Batch 4/64 loss: 0.01978909969329834
Batch 5/64 loss: -0.009286284446716309
Batch 6/64 loss: -0.011878907680511475
Batch 7/64 loss: -0.005701601505279541
Batch 8/64 loss: 0.011020064353942871
Batch 9/64 loss: 0.008917689323425293
Batch 10/64 loss: 0.007264971733093262
Batch 11/64 loss: -0.010201036930084229
Batch 12/64 loss: 0.012180209159851074
Batch 13/64 loss: -0.009710550308227539
Batch 14/64 loss: 0.018731117248535156
Batch 15/64 loss: 0.013591647148132324
Batch 16/64 loss: -0.02463090419769287
Batch 17/64 loss: -0.0001894235610961914
Batch 18/64 loss: 0.008127868175506592
Batch 19/64 loss: -0.007239103317260742
Batch 20/64 loss: -0.01368248462677002
Batch 21/64 loss: -0.002530992031097412
Batch 22/64 loss: -0.014536738395690918
Batch 23/64 loss: -0.010663211345672607
Batch 24/64 loss: 0.023750603199005127
Batch 25/64 loss: -0.002425968647003174
Batch 26/64 loss: 0.009549856185913086
Batch 27/64 loss: 0.005452871322631836
Batch 28/64 loss: 0.030676543712615967
Batch 29/64 loss: 0.005319058895111084
Batch 30/64 loss: 0.02938896417617798
Batch 31/64 loss: -0.020718872547149658
Batch 32/64 loss: 0.012470483779907227
Batch 33/64 loss: 0.0012531280517578125
Batch 34/64 loss: 0.0050427913665771484
Batch 35/64 loss: 0.026246309280395508
Batch 36/64 loss: -0.010660767555236816
Batch 37/64 loss: -0.011717081069946289
Batch 38/64 loss: 0.017213106155395508
Batch 39/64 loss: 0.011573195457458496
Batch 40/64 loss: 0.009556412696838379
Batch 41/64 loss: -0.005971968173980713
Batch 42/64 loss: -0.003939270973205566
Batch 43/64 loss: 0.0102461576461792
Batch 44/64 loss: 0.01267927885055542
Batch 45/64 loss: 0.005993247032165527
Batch 46/64 loss: -0.025045037269592285
Batch 47/64 loss: -0.01775217056274414
Batch 48/64 loss: 0.011909544467926025
Batch 49/64 loss: -0.01584416627883911
Batch 50/64 loss: 0.023816168308258057
Batch 51/64 loss: 0.0019978880882263184
Batch 52/64 loss: 0.008557796478271484
Batch 53/64 loss: -0.018377184867858887
Batch 54/64 loss: -0.014046251773834229
Batch 55/64 loss: 0.000638127326965332
Batch 56/64 loss: 0.0077329277992248535
Batch 57/64 loss: 0.030495285987854004
Batch 58/64 loss: -0.008253991603851318
Batch 59/64 loss: 0.00892859697341919
Batch 60/64 loss: -0.017139792442321777
Batch 61/64 loss: -0.014591991901397705
Batch 62/64 loss: -0.007628083229064941
Batch 63/64 loss: -0.00605010986328125
Batch 64/64 loss: 0.042614102363586426
Epoch 125  Train loss: 0.0023445610906563555  Val loss: 0.05889606885483994
Saving best model, epoch: 125
Epoch 126
-------------------------------
Batch 1/64 loss: 0.003307938575744629
Batch 2/64 loss: -0.0006254911422729492
Batch 3/64 loss: 0.006010770797729492
Batch 4/64 loss: -0.03556168079376221
Batch 5/64 loss: -0.023745417594909668
Batch 6/64 loss: -0.017627060413360596
Batch 7/64 loss: 0.0009748339653015137
Batch 8/64 loss: 0.005834460258483887
Batch 9/64 loss: 0.02475583553314209
Batch 10/64 loss: -0.009711146354675293
Batch 11/64 loss: -0.00409543514251709
Batch 12/64 loss: 0.005409836769104004
Batch 13/64 loss: 0.018384933471679688
Batch 14/64 loss: 0.023871302604675293
Batch 15/64 loss: -0.0038277506828308105
Batch 16/64 loss: -0.01340794563293457
Batch 17/64 loss: -0.03047424554824829
Batch 18/64 loss: 0.008817434310913086
Batch 19/64 loss: 0.0010144710540771484
Batch 20/64 loss: 0.0023598670959472656
Batch 21/64 loss: 0.04418957233428955
Batch 22/64 loss: 0.0018157958984375
Batch 23/64 loss: -0.0027837753295898438
Batch 24/64 loss: -0.0018731355667114258
Batch 25/64 loss: -0.04012173414230347
Batch 26/64 loss: 0.026552259922027588
Batch 27/64 loss: -0.01609259843826294
Batch 28/64 loss: -0.00766068696975708
Batch 29/64 loss: -0.0030218958854675293
Batch 30/64 loss: 0.006211578845977783
Batch 31/64 loss: -0.004252195358276367
Batch 32/64 loss: 0.0477638840675354
Batch 33/64 loss: -0.007665514945983887
Batch 34/64 loss: 0.0003339052200317383
Batch 35/64 loss: 0.009699404239654541
Batch 36/64 loss: -0.009769856929779053
Batch 37/64 loss: 0.0042632222175598145
Batch 38/64 loss: -0.021102726459503174
Batch 39/64 loss: 0.013741493225097656
Batch 40/64 loss: 0.0008782744407653809
Batch 41/64 loss: 0.017316579818725586
Batch 42/64 loss: 0.013102889060974121
Batch 43/64 loss: 0.0011475682258605957
Batch 44/64 loss: 0.0016413331031799316
Batch 45/64 loss: -0.011675596237182617
Batch 46/64 loss: -0.0059781670570373535
Batch 47/64 loss: 0.028051376342773438
Batch 48/64 loss: -0.03201603889465332
Batch 49/64 loss: -0.003306567668914795
Batch 50/64 loss: 0.023326337337493896
Batch 51/64 loss: 0.030252277851104736
Batch 52/64 loss: -0.02178710699081421
Batch 53/64 loss: -0.01708984375
Batch 54/64 loss: -0.005759179592132568
Batch 55/64 loss: -0.006402075290679932
Batch 56/64 loss: -0.012044668197631836
Batch 57/64 loss: 0.00519639253616333
Batch 58/64 loss: 0.005195975303649902
Batch 59/64 loss: 0.01915895938873291
Batch 60/64 loss: 0.0031210780143737793
Batch 61/64 loss: 0.005364537239074707
Batch 62/64 loss: -0.012642264366149902
Batch 63/64 loss: 0.002444624900817871
Batch 64/64 loss: -0.010578334331512451
Epoch 126  Train loss: 0.00033655610739016063  Val loss: 0.0718815203794499
Epoch 127
-------------------------------
Batch 1/64 loss: 0.011315226554870605
Batch 2/64 loss: -0.03404802083969116
Batch 3/64 loss: 0.01344209909439087
Batch 4/64 loss: -0.032635629177093506
Batch 5/64 loss: 0.01668792963027954
Batch 6/64 loss: 0.014973580837249756
Batch 7/64 loss: -0.004295825958251953
Batch 8/64 loss: -0.009145855903625488
Batch 9/64 loss: -0.04530668258666992
Batch 10/64 loss: -0.006828784942626953
Batch 11/64 loss: 0.018380045890808105
Batch 12/64 loss: -0.0008529424667358398
Batch 13/64 loss: -0.027210772037506104
Batch 14/64 loss: 0.01790308952331543
Batch 15/64 loss: 0.01714712381362915
Batch 16/64 loss: 0.00010919570922851562
Batch 17/64 loss: -0.006285727024078369
Batch 18/64 loss: 0.0009188652038574219
Batch 19/64 loss: -0.002559661865234375
Batch 20/64 loss: -0.004482388496398926
Batch 21/64 loss: 0.012212634086608887
Batch 22/64 loss: -0.025022447109222412
Batch 23/64 loss: 0.006242036819458008
Batch 24/64 loss: -0.023701369762420654
Batch 25/64 loss: -0.007744967937469482
Batch 26/64 loss: -0.0026833415031433105
Batch 27/64 loss: 0.019016385078430176
Batch 28/64 loss: 0.04816240072250366
Batch 29/64 loss: -0.0018916130065917969
Batch 30/64 loss: -0.008218169212341309
Batch 31/64 loss: -0.01788043975830078
Batch 32/64 loss: -0.012453079223632812
Batch 33/64 loss: 0.01036304235458374
Batch 34/64 loss: 0.02311861515045166
Batch 35/64 loss: 0.030338406562805176
Batch 36/64 loss: -0.010707557201385498
Batch 37/64 loss: -0.007239818572998047
Batch 38/64 loss: -0.007671535015106201
Batch 39/64 loss: -0.01117253303527832
Batch 40/64 loss: -0.006452620029449463
Batch 41/64 loss: 0.013376891613006592
Batch 42/64 loss: -0.017546892166137695
Batch 43/64 loss: -0.02254021167755127
Batch 44/64 loss: 0.018484830856323242
Batch 45/64 loss: 0.000686943531036377
Batch 46/64 loss: 0.002792835235595703
Batch 47/64 loss: 0.005824685096740723
Batch 48/64 loss: 0.004927992820739746
Batch 49/64 loss: -0.01957803964614868
Batch 50/64 loss: -0.01712101697921753
Batch 51/64 loss: 0.02096867561340332
Batch 52/64 loss: 0.026371359825134277
Batch 53/64 loss: 0.028760552406311035
Batch 54/64 loss: 0.00042700767517089844
Batch 55/64 loss: -0.017515599727630615
Batch 56/64 loss: -0.010663390159606934
Batch 57/64 loss: 0.007205843925476074
Batch 58/64 loss: 0.006152689456939697
Batch 59/64 loss: -0.020484089851379395
Batch 60/64 loss: -0.015862226486206055
Batch 61/64 loss: -0.002359628677368164
Batch 62/64 loss: 0.010516524314880371
Batch 63/64 loss: 0.014654040336608887
Batch 64/64 loss: -0.00858992338180542
Epoch 127  Train loss: -0.0007078238562041638  Val loss: 0.08256130345498573
Epoch 128
-------------------------------
Batch 1/64 loss: 0.023929476737976074
Batch 2/64 loss: -0.0081368088722229
Batch 3/64 loss: -0.015467584133148193
Batch 4/64 loss: -0.012252569198608398
Batch 5/64 loss: 0.011786222457885742
Batch 6/64 loss: -0.0031775832176208496
Batch 7/64 loss: 0.005654096603393555
Batch 8/64 loss: 0.010807693004608154
Batch 9/64 loss: -0.012765586376190186
Batch 10/64 loss: -0.015178382396697998
Batch 11/64 loss: 0.007118105888366699
Batch 12/64 loss: -0.018207788467407227
Batch 13/64 loss: 0.005793511867523193
Batch 14/64 loss: 0.01371455192565918
Batch 15/64 loss: 0.03709888458251953
Batch 16/64 loss: -0.023133039474487305
Batch 17/64 loss: 0.0013614296913146973
Batch 18/64 loss: 0.011900901794433594
Batch 19/64 loss: -0.014021039009094238
Batch 20/64 loss: 0.005407810211181641
Batch 21/64 loss: -0.02872687578201294
Batch 22/64 loss: -0.005968034267425537
Batch 23/64 loss: 0.03595072031021118
Batch 24/64 loss: 0.04316061735153198
Batch 25/64 loss: 0.001276850700378418
Batch 26/64 loss: 0.0005386471748352051
Batch 27/64 loss: 0.0016139745712280273
Batch 28/64 loss: -0.020185112953186035
Batch 29/64 loss: 0.011584579944610596
Batch 30/64 loss: -0.027665793895721436
Batch 31/64 loss: -0.019807636737823486
Batch 32/64 loss: 0.010236382484436035
Batch 33/64 loss: -0.0064542293548583984
Batch 34/64 loss: 0.020556330680847168
Batch 35/64 loss: -0.04139900207519531
Batch 36/64 loss: -0.01261758804321289
Batch 37/64 loss: 0.014257252216339111
Batch 38/64 loss: -0.00359952449798584
Batch 39/64 loss: -0.03915739059448242
Batch 40/64 loss: 0.023715198040008545
Batch 41/64 loss: 0.011913836002349854
Batch 42/64 loss: 0.007355093955993652
Batch 43/64 loss: -0.00162506103515625
Batch 44/64 loss: 0.00740659236907959
Batch 45/64 loss: -0.027373135089874268
Batch 46/64 loss: -0.00324249267578125
Batch 47/64 loss: -0.0240401029586792
Batch 48/64 loss: -0.001411139965057373
Batch 49/64 loss: -0.013357400894165039
Batch 50/64 loss: 0.04370027780532837
Batch 51/64 loss: 0.009432017803192139
Batch 52/64 loss: -0.005560159683227539
Batch 53/64 loss: -0.007718503475189209
Batch 54/64 loss: -0.00823509693145752
Batch 55/64 loss: -0.01489400863647461
Batch 56/64 loss: -0.01018226146697998
Batch 57/64 loss: -0.010315179824829102
Batch 58/64 loss: -0.02893245220184326
Batch 59/64 loss: -0.02115654945373535
Batch 60/64 loss: -0.024262189865112305
Batch 61/64 loss: 0.006577193737030029
Batch 62/64 loss: 0.010431408882141113
Batch 63/64 loss: -0.006897628307342529
Batch 64/64 loss: 0.010128140449523926
Epoch 128  Train loss: -0.0021215555714625936  Val loss: 0.07210879268515151
Epoch 129
-------------------------------
Batch 1/64 loss: -0.025709986686706543
Batch 2/64 loss: 0.011533021926879883
Batch 3/64 loss: -0.008850455284118652
Batch 4/64 loss: -0.00013238191604614258
Batch 5/64 loss: 0.00977468490600586
Batch 6/64 loss: -0.005351006984710693
Batch 7/64 loss: -0.010492384433746338
Batch 8/64 loss: -0.0037807226181030273
Batch 9/64 loss: 0.02413487434387207
Batch 10/64 loss: -0.018504858016967773
Batch 11/64 loss: -0.009081125259399414
Batch 12/64 loss: -0.012063145637512207
Batch 13/64 loss: 0.010946094989776611
Batch 14/64 loss: 0.008562147617340088
Batch 15/64 loss: -0.010000824928283691
Batch 16/64 loss: -3.5762786865234375e-06
Batch 17/64 loss: -0.003972530364990234
Batch 18/64 loss: -0.015610694885253906
Batch 19/64 loss: -0.008383214473724365
Batch 20/64 loss: -0.023844122886657715
Batch 21/64 loss: -0.04944109916687012
Batch 22/64 loss: 0.022464454174041748
Batch 23/64 loss: -0.0069335103034973145
Batch 24/64 loss: -0.00921773910522461
Batch 25/64 loss: 0.007244884967803955
Batch 26/64 loss: -0.012062788009643555
Batch 27/64 loss: -0.007257521152496338
Batch 28/64 loss: -0.010037541389465332
Batch 29/64 loss: -0.007932305335998535
Batch 30/64 loss: -0.01742488145828247
Batch 31/64 loss: 0.0035789012908935547
Batch 32/64 loss: 0.00464475154876709
Batch 33/64 loss: -0.02333986759185791
Batch 34/64 loss: 0.0016532540321350098
Batch 35/64 loss: -0.00489276647567749
Batch 36/64 loss: 0.010983049869537354
Batch 37/64 loss: 0.012929141521453857
Batch 38/64 loss: -0.002526223659515381
Batch 39/64 loss: -0.026954352855682373
Batch 40/64 loss: -0.02437901496887207
Batch 41/64 loss: 0.006800055503845215
Batch 42/64 loss: 0.02238255739212036
Batch 43/64 loss: 0.011255145072937012
Batch 44/64 loss: -0.004115939140319824
Batch 45/64 loss: 0.013543665409088135
Batch 46/64 loss: -0.009982109069824219
Batch 47/64 loss: 0.00782865285873413
Batch 48/64 loss: 0.044581830501556396
Batch 49/64 loss: -0.020051121711730957
Batch 50/64 loss: -0.0006926655769348145
Batch 51/64 loss: -0.007712125778198242
Batch 52/64 loss: -0.0035746097564697266
Batch 53/64 loss: -0.012668490409851074
Batch 54/64 loss: -0.0008317232131958008
Batch 55/64 loss: 0.022015094757080078
Batch 56/64 loss: -0.009528398513793945
Batch 57/64 loss: 0.03147631883621216
Batch 58/64 loss: -0.011927962303161621
Batch 59/64 loss: -0.008759558200836182
Batch 60/64 loss: -0.0010263919830322266
Batch 61/64 loss: -0.009453535079956055
Batch 62/64 loss: 0.00036346912384033203
Batch 63/64 loss: -0.004858672618865967
Batch 64/64 loss: -0.003095090389251709
Epoch 129  Train loss: -0.0027763013746224196  Val loss: 0.07107249322216126
Epoch 130
-------------------------------
Batch 1/64 loss: 0.009485244750976562
Batch 2/64 loss: 0.007204771041870117
Batch 3/64 loss: 0.01966339349746704
Batch 4/64 loss: -0.015663743019104004
Batch 5/64 loss: -0.02165919542312622
Batch 6/64 loss: -0.02182745933532715
Batch 7/64 loss: -0.010247468948364258
Batch 8/64 loss: -0.025241374969482422
Batch 9/64 loss: -0.017178714275360107
Batch 10/64 loss: -0.003686249256134033
Batch 11/64 loss: -0.024481892585754395
Batch 12/64 loss: 0.007097601890563965
Batch 13/64 loss: 0.0005956292152404785
Batch 14/64 loss: 0.008397936820983887
Batch 15/64 loss: -0.001565098762512207
Batch 16/64 loss: -0.00810229778289795
Batch 17/64 loss: 0.0018009543418884277
Batch 18/64 loss: 0.0004200935363769531
Batch 19/64 loss: -0.024673104286193848
Batch 20/64 loss: -0.011004626750946045
Batch 21/64 loss: -0.021306097507476807
Batch 22/64 loss: -0.01940906047821045
Batch 23/64 loss: -0.017446517944335938
Batch 24/64 loss: 0.004227101802825928
Batch 25/64 loss: -0.01984274387359619
Batch 26/64 loss: 0.017934978008270264
Batch 27/64 loss: 0.009727716445922852
Batch 28/64 loss: 0.00163954496383667
Batch 29/64 loss: -0.0013671517372131348
Batch 30/64 loss: 0.009609103202819824
Batch 31/64 loss: 0.045526742935180664
Batch 32/64 loss: -0.004585683345794678
Batch 33/64 loss: -0.023486435413360596
Batch 34/64 loss: 0.02293187379837036
Batch 35/64 loss: -0.01068723201751709
Batch 36/64 loss: -0.010918021202087402
Batch 37/64 loss: 0.013457417488098145
Batch 38/64 loss: -0.005674839019775391
Batch 39/64 loss: -0.01383984088897705
Batch 40/64 loss: -0.009977400302886963
Batch 41/64 loss: -0.027116239070892334
Batch 42/64 loss: -0.020838558673858643
Batch 43/64 loss: -0.014346957206726074
Batch 44/64 loss: 0.0035611987113952637
Batch 45/64 loss: -0.014994502067565918
Batch 46/64 loss: 0.005763888359069824
Batch 47/64 loss: -0.002129673957824707
Batch 48/64 loss: 0.004249691963195801
Batch 49/64 loss: -0.015301108360290527
Batch 50/64 loss: -0.014202296733856201
Batch 51/64 loss: -0.021817505359649658
Batch 52/64 loss: 0.03250396251678467
Batch 53/64 loss: -0.017432689666748047
Batch 54/64 loss: 0.025545120239257812
Batch 55/64 loss: 0.0014818906784057617
Batch 56/64 loss: -0.01922607421875
Batch 57/64 loss: -0.038649916648864746
Batch 58/64 loss: -0.006808042526245117
Batch 59/64 loss: 0.016246497631072998
Batch 60/64 loss: -0.013899087905883789
Batch 61/64 loss: 0.008400559425354004
Batch 62/64 loss: 0.009581208229064941
Batch 63/64 loss: -0.02778226137161255
Batch 64/64 loss: 0.026560068130493164
Epoch 130  Train loss: -0.004571654749851601  Val loss: 0.06934681258250758
Epoch 131
-------------------------------
Batch 1/64 loss: 0.01650136709213257
Batch 2/64 loss: 0.0010717511177062988
Batch 3/64 loss: 0.009741544723510742
Batch 4/64 loss: -0.008358359336853027
Batch 5/64 loss: -0.011924207210540771
Batch 6/64 loss: 0.0025382637977600098
Batch 7/64 loss: 0.001514136791229248
Batch 8/64 loss: -0.006780505180358887
Batch 9/64 loss: -0.010510742664337158
Batch 10/64 loss: -0.005697488784790039
Batch 11/64 loss: 0.007596433162689209
Batch 12/64 loss: -0.02883690595626831
Batch 13/64 loss: -0.0026546716690063477
Batch 14/64 loss: 0.004203379154205322
Batch 15/64 loss: -0.03377723693847656
Batch 16/64 loss: -0.004300057888031006
Batch 17/64 loss: -0.011081814765930176
Batch 18/64 loss: -0.005209505558013916
Batch 19/64 loss: -0.011453390121459961
Batch 20/64 loss: 0.023371219635009766
Batch 21/64 loss: -0.003818690776824951
Batch 22/64 loss: -0.01634538173675537
Batch 23/64 loss: -0.010104477405548096
Batch 24/64 loss: -0.00712662935256958
Batch 25/64 loss: -0.012973189353942871
Batch 26/64 loss: -0.015289366245269775
Batch 27/64 loss: -0.011799454689025879
Batch 28/64 loss: 0.007756233215332031
Batch 29/64 loss: -0.019406914710998535
Batch 30/64 loss: 0.02986443042755127
Batch 31/64 loss: -0.026657462120056152
Batch 32/64 loss: -0.0100175142288208
Batch 33/64 loss: -0.022973716259002686
Batch 34/64 loss: -0.008800387382507324
Batch 35/64 loss: -0.0032721757888793945
Batch 36/64 loss: -0.01592540740966797
Batch 37/64 loss: -0.0064501166343688965
Batch 38/64 loss: -0.006763815879821777
Batch 39/64 loss: -0.004685342311859131
Batch 40/64 loss: -0.015950441360473633
Batch 41/64 loss: 0.00829160213470459
Batch 42/64 loss: 0.012084007263183594
Batch 43/64 loss: 0.010541200637817383
Batch 44/64 loss: 0.023946762084960938
Batch 45/64 loss: 0.0014671087265014648
Batch 46/64 loss: -0.015107452869415283
Batch 47/64 loss: -0.010100007057189941
Batch 48/64 loss: -0.015091419219970703
Batch 49/64 loss: -0.026594698429107666
Batch 50/64 loss: -0.011408567428588867
Batch 51/64 loss: 0.033222854137420654
Batch 52/64 loss: -0.029430150985717773
Batch 53/64 loss: -0.021143615245819092
Batch 54/64 loss: 0.00638657808303833
Batch 55/64 loss: 0.0047332048416137695
Batch 56/64 loss: -0.008060097694396973
Batch 57/64 loss: -0.023556292057037354
Batch 58/64 loss: -0.0032598376274108887
Batch 59/64 loss: -0.0009078383445739746
Batch 60/64 loss: -0.02215898036956787
Batch 61/64 loss: -0.024037837982177734
Batch 62/64 loss: 0.014559805393218994
Batch 63/64 loss: -0.017567932605743408
Batch 64/64 loss: 0.011977136135101318
Epoch 131  Train loss: -0.005631299813588461  Val loss: 0.07176093362860664
Epoch 132
-------------------------------
Batch 1/64 loss: 0.007623076438903809
Batch 2/64 loss: -0.010162115097045898
Batch 3/64 loss: -0.03117358684539795
Batch 4/64 loss: 0.00474780797958374
Batch 5/64 loss: -0.0003978610038757324
Batch 6/64 loss: -0.028882861137390137
Batch 7/64 loss: 0.013810694217681885
Batch 8/64 loss: 0.014765322208404541
Batch 9/64 loss: -0.024636268615722656
Batch 10/64 loss: -0.02184349298477173
Batch 11/64 loss: 0.019906938076019287
Batch 12/64 loss: 0.004556715488433838
Batch 13/64 loss: -0.02422332763671875
Batch 14/64 loss: -0.025766491889953613
Batch 15/64 loss: -0.04093611240386963
Batch 16/64 loss: -0.007697045803070068
Batch 17/64 loss: -0.0036499500274658203
Batch 18/64 loss: -0.04150807857513428
Batch 19/64 loss: -0.010688602924346924
Batch 20/64 loss: 0.029458820819854736
Batch 21/64 loss: -0.016019761562347412
Batch 22/64 loss: -0.011204421520233154
Batch 23/64 loss: -0.006724357604980469
Batch 24/64 loss: 0.0013033747673034668
Batch 25/64 loss: 0.003312349319458008
Batch 26/64 loss: -0.018437683582305908
Batch 27/64 loss: -0.014253437519073486
Batch 28/64 loss: -0.013897359371185303
Batch 29/64 loss: -0.007293820381164551
Batch 30/64 loss: -0.0022788047790527344
Batch 31/64 loss: -0.010752618312835693
Batch 32/64 loss: -0.003400087356567383
Batch 33/64 loss: -0.021746516227722168
Batch 34/64 loss: -0.020247578620910645
Batch 35/64 loss: 0.0007030963897705078
Batch 36/64 loss: -0.0016376376152038574
Batch 37/64 loss: -0.004305839538574219
Batch 38/64 loss: 0.0015195608139038086
Batch 39/64 loss: 0.01696944236755371
Batch 40/64 loss: -0.003102898597717285
Batch 41/64 loss: 0.0068999528884887695
Batch 42/64 loss: -0.009637892246246338
Batch 43/64 loss: 0.0030726194381713867
Batch 44/64 loss: 0.01146930456161499
Batch 45/64 loss: 0.025508761405944824
Batch 46/64 loss: -0.007541537284851074
Batch 47/64 loss: -0.0016058683395385742
Batch 48/64 loss: -0.013701081275939941
Batch 49/64 loss: 0.0072977542877197266
Batch 50/64 loss: 0.0011900067329406738
Batch 51/64 loss: -0.013916611671447754
Batch 52/64 loss: -0.019795536994934082
Batch 53/64 loss: 0.00032019615173339844
Batch 54/64 loss: -0.01934283971786499
Batch 55/64 loss: -0.0035579800605773926
Batch 56/64 loss: 0.018540263175964355
Batch 57/64 loss: 0.01524341106414795
Batch 58/64 loss: -0.026863276958465576
Batch 59/64 loss: 0.005952656269073486
Batch 60/64 loss: -0.0013686418533325195
Batch 61/64 loss: -0.006080508232116699
Batch 62/64 loss: -0.003632664680480957
Batch 63/64 loss: -0.006875932216644287
Batch 64/64 loss: -0.019632816314697266
Epoch 132  Train loss: -0.005668101591222426  Val loss: 0.0669493970182753
Epoch 133
-------------------------------
Batch 1/64 loss: -0.020970821380615234
Batch 2/64 loss: -0.0036205649375915527
Batch 3/64 loss: -0.0183374285697937
Batch 4/64 loss: 0.0004202723503112793
Batch 5/64 loss: -0.01687067747116089
Batch 6/64 loss: -0.015667319297790527
Batch 7/64 loss: -0.04162400960922241
Batch 8/64 loss: -0.03240764141082764
Batch 9/64 loss: 0.0036861300468444824
Batch 10/64 loss: -0.022295653820037842
Batch 11/64 loss: -0.02136087417602539
Batch 12/64 loss: -0.035746097564697266
Batch 13/64 loss: -0.002674281597137451
Batch 14/64 loss: -0.014336824417114258
Batch 15/64 loss: -0.018243849277496338
Batch 16/64 loss: -0.016332149505615234
Batch 17/64 loss: -0.03712886571884155
Batch 18/64 loss: 0.014937877655029297
Batch 19/64 loss: 0.004775941371917725
Batch 20/64 loss: -0.02916550636291504
Batch 21/64 loss: -0.018027544021606445
Batch 22/64 loss: 0.03608286380767822
Batch 23/64 loss: -0.016183912754058838
Batch 24/64 loss: -0.01895463466644287
Batch 25/64 loss: -0.024194419384002686
Batch 26/64 loss: 0.00684279203414917
Batch 27/64 loss: -0.014800429344177246
Batch 28/64 loss: -0.01556330919265747
Batch 29/64 loss: -0.0062773823738098145
Batch 30/64 loss: 0.014106035232543945
Batch 31/64 loss: -0.005753636360168457
Batch 32/64 loss: -0.004912912845611572
Batch 33/64 loss: 0.012972772121429443
Batch 34/64 loss: -0.01875770092010498
Batch 35/64 loss: 0.01929652690887451
Batch 36/64 loss: -0.006900370121002197
Batch 37/64 loss: -0.0016842484474182129
Batch 38/64 loss: -0.0390055775642395
Batch 39/64 loss: -0.019345521926879883
Batch 40/64 loss: -0.01730901002883911
Batch 41/64 loss: -0.0074800848960876465
Batch 42/64 loss: -0.01340019702911377
Batch 43/64 loss: -0.0033912062644958496
Batch 44/64 loss: -0.001447439193725586
Batch 45/64 loss: -0.005977928638458252
Batch 46/64 loss: 0.0012996196746826172
Batch 47/64 loss: 0.008778572082519531
Batch 48/64 loss: 0.03986680507659912
Batch 49/64 loss: -0.029684841632843018
Batch 50/64 loss: -0.021484375
Batch 51/64 loss: 0.006438553333282471
Batch 52/64 loss: 0.02039027214050293
Batch 53/64 loss: -0.006856203079223633
Batch 54/64 loss: -0.01644235849380493
Batch 55/64 loss: 0.009716391563415527
Batch 56/64 loss: 0.007729828357696533
Batch 57/64 loss: 0.004523515701293945
Batch 58/64 loss: 0.013210594654083252
Batch 59/64 loss: 0.014138281345367432
Batch 60/64 loss: -0.020820021629333496
Batch 61/64 loss: -0.00269162654876709
Batch 62/64 loss: -0.031188607215881348
Batch 63/64 loss: -0.022573351860046387
Batch 64/64 loss: 0.03505057096481323
Epoch 133  Train loss: -0.007723762241064334  Val loss: 0.06068257245001514
Epoch 134
-------------------------------
Batch 1/64 loss: -0.03587663173675537
Batch 2/64 loss: -0.03780031204223633
Batch 3/64 loss: 0.011072278022766113
Batch 4/64 loss: -0.000739753246307373
Batch 5/64 loss: 0.014906167984008789
Batch 6/64 loss: 0.007723093032836914
Batch 7/64 loss: 0.01309061050415039
Batch 8/64 loss: -0.008221268653869629
Batch 9/64 loss: -0.03003084659576416
Batch 10/64 loss: -0.018329262733459473
Batch 11/64 loss: -0.02244645357131958
Batch 12/64 loss: 0.01793438196182251
Batch 13/64 loss: -0.021951675415039062
Batch 14/64 loss: -0.01764655113220215
Batch 15/64 loss: -0.030012786388397217
Batch 16/64 loss: 0.014500439167022705
Batch 17/64 loss: -0.012262940406799316
Batch 18/64 loss: 0.015781939029693604
Batch 19/64 loss: -0.023063838481903076
Batch 20/64 loss: -0.022911429405212402
Batch 21/64 loss: -0.016453981399536133
Batch 22/64 loss: -0.015912532806396484
Batch 23/64 loss: -0.013403654098510742
Batch 24/64 loss: 0.014559268951416016
Batch 25/64 loss: -0.0462685227394104
Batch 26/64 loss: -0.028583288192749023
Batch 27/64 loss: -0.0012492537498474121
Batch 28/64 loss: -0.00263214111328125
Batch 29/64 loss: -0.010689854621887207
Batch 30/64 loss: -0.02061629295349121
Batch 31/64 loss: -0.013808786869049072
Batch 32/64 loss: -0.002026200294494629
Batch 33/64 loss: -0.02065974473953247
Batch 34/64 loss: -0.00933384895324707
Batch 35/64 loss: -0.003142714500427246
Batch 36/64 loss: -0.0130845308303833
Batch 37/64 loss: -0.0231935977935791
Batch 38/64 loss: -0.02527552843093872
Batch 39/64 loss: 0.024816155433654785
Batch 40/64 loss: 0.0010919570922851562
Batch 41/64 loss: 0.008024334907531738
Batch 42/64 loss: 0.011701345443725586
Batch 43/64 loss: -0.007728099822998047
Batch 44/64 loss: -0.014565348625183105
Batch 45/64 loss: -0.025165796279907227
Batch 46/64 loss: -0.016830921173095703
Batch 47/64 loss: 0.009415090084075928
Batch 48/64 loss: 0.0030409693717956543
Batch 49/64 loss: 0.006696105003356934
Batch 50/64 loss: 0.011209726333618164
Batch 51/64 loss: 0.010199785232543945
Batch 52/64 loss: 0.004618048667907715
Batch 53/64 loss: -0.00019496679306030273
Batch 54/64 loss: -0.007594764232635498
Batch 55/64 loss: -0.031143009662628174
Batch 56/64 loss: 0.0035629868507385254
Batch 57/64 loss: 0.03063035011291504
Batch 58/64 loss: -0.018038511276245117
Batch 59/64 loss: 0.0014384984970092773
Batch 60/64 loss: 0.01782912015914917
Batch 61/64 loss: 0.0072716474533081055
Batch 62/64 loss: 0.0005738139152526855
Batch 63/64 loss: -0.017443299293518066
Batch 64/64 loss: -0.000602424144744873
Epoch 134  Train loss: -0.006668182681588566  Val loss: 0.06882053019664541
Epoch 135
-------------------------------
Batch 1/64 loss: -0.03007805347442627
Batch 2/64 loss: -0.04121178388595581
Batch 3/64 loss: -0.014062643051147461
Batch 4/64 loss: 0.011470437049865723
Batch 5/64 loss: -0.017411887645721436
Batch 6/64 loss: 0.014558255672454834
Batch 7/64 loss: -0.021573185920715332
Batch 8/64 loss: -0.019236326217651367
Batch 9/64 loss: -0.0011511445045471191
Batch 10/64 loss: -0.0032559633255004883
Batch 11/64 loss: 0.002146482467651367
Batch 12/64 loss: -0.022938013076782227
Batch 13/64 loss: -0.02505195140838623
Batch 14/64 loss: 0.0024728775024414062
Batch 15/64 loss: 0.012735486030578613
Batch 16/64 loss: -0.015706419944763184
Batch 17/64 loss: -0.014612317085266113
Batch 18/64 loss: 0.02948737144470215
Batch 19/64 loss: 0.009714722633361816
Batch 20/64 loss: 0.012583374977111816
Batch 21/64 loss: 0.029772520065307617
Batch 22/64 loss: -0.001117408275604248
Batch 23/64 loss: -0.016376614570617676
Batch 24/64 loss: -0.01597040891647339
Batch 25/64 loss: -0.005074441432952881
Batch 26/64 loss: -0.02288442850112915
Batch 27/64 loss: 0.005746066570281982
Batch 28/64 loss: 0.00045692920684814453
Batch 29/64 loss: -0.023864567279815674
Batch 30/64 loss: -0.010046541690826416
Batch 31/64 loss: 0.02049487829208374
Batch 32/64 loss: -0.006865024566650391
Batch 33/64 loss: 0.019724726676940918
Batch 34/64 loss: -0.0015947222709655762
Batch 35/64 loss: 0.0073348283767700195
Batch 36/64 loss: -0.007693767547607422
Batch 37/64 loss: -0.02091062068939209
Batch 38/64 loss: -0.019402742385864258
Batch 39/64 loss: -0.007742464542388916
Batch 40/64 loss: -0.020124435424804688
Batch 41/64 loss: -0.02193504571914673
Batch 42/64 loss: -0.01583772897720337
Batch 43/64 loss: 0.012143254280090332
Batch 44/64 loss: -0.027767479419708252
Batch 45/64 loss: 0.03654664754867554
Batch 46/64 loss: -0.00730133056640625
Batch 47/64 loss: 0.004571855068206787
Batch 48/64 loss: -0.01722276210784912
Batch 49/64 loss: -0.023222386837005615
Batch 50/64 loss: 0.026970267295837402
Batch 51/64 loss: 0.0003050565719604492
Batch 52/64 loss: -0.012486577033996582
Batch 53/64 loss: -0.02520698308944702
Batch 54/64 loss: -0.009809017181396484
Batch 55/64 loss: 0.005264461040496826
Batch 56/64 loss: -0.010090172290802002
Batch 57/64 loss: 0.025009095668792725
Batch 58/64 loss: 0.01405942440032959
Batch 59/64 loss: -0.0021647214889526367
Batch 60/64 loss: -0.026183128356933594
Batch 61/64 loss: -0.015530943870544434
Batch 62/64 loss: -0.018575847148895264
Batch 63/64 loss: -0.03451448678970337
Batch 64/64 loss: -0.025408267974853516
Epoch 135  Train loss: -0.0061065673828125  Val loss: 0.08135890632970226
Epoch 136
-------------------------------
Batch 1/64 loss: 0.004273593425750732
Batch 2/64 loss: -0.017848670482635498
Batch 3/64 loss: -0.025970041751861572
Batch 4/64 loss: -0.009172141551971436
Batch 5/64 loss: -0.031700730323791504
Batch 6/64 loss: -0.019564270973205566
Batch 7/64 loss: 1.9252300262451172e-05
Batch 8/64 loss: -0.002198517322540283
Batch 9/64 loss: 0.004388689994812012
Batch 10/64 loss: -0.001658618450164795
Batch 11/64 loss: -0.0004571676254272461
Batch 12/64 loss: -0.02847111225128174
Batch 13/64 loss: -0.030700623989105225
Batch 14/64 loss: 0.003844141960144043
Batch 15/64 loss: 0.00850766897201538
Batch 16/64 loss: -0.003954946994781494
Batch 17/64 loss: -0.015471398830413818
Batch 18/64 loss: 0.03353393077850342
Batch 19/64 loss: -0.010354697704315186
Batch 20/64 loss: -0.040783822536468506
Batch 21/64 loss: 0.00949317216873169
Batch 22/64 loss: -0.0256156325340271
Batch 23/64 loss: -0.032128334045410156
Batch 24/64 loss: 0.014476120471954346
Batch 25/64 loss: -0.01801431179046631
Batch 26/64 loss: 0.001989126205444336
Batch 27/64 loss: -0.010288715362548828
Batch 28/64 loss: -0.014131307601928711
Batch 29/64 loss: 0.005473136901855469
Batch 30/64 loss: -0.007193386554718018
Batch 31/64 loss: 0.0014147162437438965
Batch 32/64 loss: 0.010043323040008545
Batch 33/64 loss: -0.03517180681228638
Batch 34/64 loss: 0.006045818328857422
Batch 35/64 loss: 0.0052793025970458984
Batch 36/64 loss: 0.009839415550231934
Batch 37/64 loss: 0.007370650768280029
Batch 38/64 loss: -0.025969266891479492
Batch 39/64 loss: -0.01706671714782715
Batch 40/64 loss: -0.0020546317100524902
Batch 41/64 loss: -0.0027014613151550293
Batch 42/64 loss: -0.0015963912010192871
Batch 43/64 loss: -0.012444913387298584
Batch 44/64 loss: -0.010157942771911621
Batch 45/64 loss: 0.022856712341308594
Batch 46/64 loss: -0.025852978229522705
Batch 47/64 loss: 0.0019271373748779297
Batch 48/64 loss: -0.008705615997314453
Batch 49/64 loss: 0.010823607444763184
Batch 50/64 loss: -0.011022388935089111
Batch 51/64 loss: -0.028721988201141357
Batch 52/64 loss: -4.565715789794922e-05
Batch 53/64 loss: 0.03044867515563965
Batch 54/64 loss: -0.025968730449676514
Batch 55/64 loss: -0.028172016143798828
Batch 56/64 loss: -0.02682483196258545
Batch 57/64 loss: 0.002070903778076172
Batch 58/64 loss: -0.011092841625213623
Batch 59/64 loss: -0.02285844087600708
Batch 60/64 loss: -0.024054765701293945
Batch 61/64 loss: -0.013195395469665527
Batch 62/64 loss: 0.010772705078125
Batch 63/64 loss: -0.03572404384613037
Batch 64/64 loss: -0.006632030010223389
Epoch 136  Train loss: -0.008080996017830045  Val loss: 0.06536777425058109
Epoch 137
-------------------------------
Batch 1/64 loss: 0.011399924755096436
Batch 2/64 loss: -0.02245330810546875
Batch 3/64 loss: -0.007956922054290771
Batch 4/64 loss: -0.010924100875854492
Batch 5/64 loss: -0.0430757999420166
Batch 6/64 loss: 0.007112443447113037
Batch 7/64 loss: -0.00890427827835083
Batch 8/64 loss: -0.029388904571533203
Batch 9/64 loss: -0.029002785682678223
Batch 10/64 loss: -0.00017762184143066406
Batch 11/64 loss: -0.0040444135665893555
Batch 12/64 loss: -0.022109627723693848
Batch 13/64 loss: -0.03606843948364258
Batch 14/64 loss: -0.015221476554870605
Batch 15/64 loss: -0.03338813781738281
Batch 16/64 loss: -0.004120051860809326
Batch 17/64 loss: -0.007207512855529785
Batch 18/64 loss: 0.005244553089141846
Batch 19/64 loss: -0.013684749603271484
Batch 20/64 loss: 0.020024895668029785
Batch 21/64 loss: -0.006485402584075928
Batch 22/64 loss: -0.02277553081512451
Batch 23/64 loss: 0.01921910047531128
Batch 24/64 loss: -0.018294990062713623
Batch 25/64 loss: -0.02980351448059082
Batch 26/64 loss: -0.03594386577606201
Batch 27/64 loss: -0.012469172477722168
Batch 28/64 loss: -0.014302492141723633
Batch 29/64 loss: -0.039684832096099854
Batch 30/64 loss: -0.01218724250793457
Batch 31/64 loss: -0.034641921520233154
Batch 32/64 loss: 0.01332855224609375
Batch 33/64 loss: 0.006045103073120117
Batch 34/64 loss: -0.03509795665740967
Batch 35/64 loss: -0.023471474647521973
Batch 36/64 loss: -0.017424583435058594
Batch 37/64 loss: -0.025517284870147705
Batch 38/64 loss: 0.012481331825256348
Batch 39/64 loss: -0.011784732341766357
Batch 40/64 loss: 0.011297285556793213
Batch 41/64 loss: 0.012249231338500977
Batch 42/64 loss: 0.011025667190551758
Batch 43/64 loss: -0.011921346187591553
Batch 44/64 loss: -0.010462343692779541
Batch 45/64 loss: 0.01732635498046875
Batch 46/64 loss: -0.01214379072189331
Batch 47/64 loss: -6.139278411865234e-06
Batch 48/64 loss: 0.00977557897567749
Batch 49/64 loss: -0.017387688159942627
Batch 50/64 loss: -0.00450289249420166
Batch 51/64 loss: 0.016616344451904297
Batch 52/64 loss: -0.012639880180358887
Batch 53/64 loss: -0.017054438591003418
Batch 54/64 loss: -0.025972425937652588
Batch 55/64 loss: 0.0023795366287231445
Batch 56/64 loss: 0.00945669412612915
Batch 57/64 loss: -0.016414225101470947
Batch 58/64 loss: 0.00017726421356201172
Batch 59/64 loss: 0.0024967193603515625
Batch 60/64 loss: -0.014392316341400146
Batch 61/64 loss: -0.008890390396118164
Batch 62/64 loss: -0.01346731185913086
Batch 63/64 loss: -0.0460701584815979
Batch 64/64 loss: -0.01923316717147827
Epoch 137  Train loss: -0.01044245911579506  Val loss: 0.06759048921545756
Epoch 138
-------------------------------
Batch 1/64 loss: -0.028225362300872803
Batch 2/64 loss: -0.026089966297149658
Batch 3/64 loss: 0.04108726978302002
Batch 4/64 loss: 0.03781980276107788
Batch 5/64 loss: -0.006270051002502441
Batch 6/64 loss: -0.009820699691772461
Batch 7/64 loss: -0.01820957660675049
Batch 8/64 loss: -0.021355271339416504
Batch 9/64 loss: -0.03435492515563965
Batch 10/64 loss: -0.004927575588226318
Batch 11/64 loss: -0.012492239475250244
Batch 12/64 loss: -0.0028547048568725586
Batch 13/64 loss: -0.014922142028808594
Batch 14/64 loss: -0.050874412059783936
Batch 15/64 loss: -0.02267247438430786
Batch 16/64 loss: -0.010946333408355713
Batch 17/64 loss: 0.0014395713806152344
Batch 18/64 loss: -0.003075718879699707
Batch 19/64 loss: 0.006931781768798828
Batch 20/64 loss: -0.017081916332244873
Batch 21/64 loss: -0.0236780047416687
Batch 22/64 loss: 0.0016393661499023438
Batch 23/64 loss: -0.00919884443283081
Batch 24/64 loss: -0.0329776406288147
Batch 25/64 loss: -0.025713741779327393
Batch 26/64 loss: -0.02495044469833374
Batch 27/64 loss: -0.02168440818786621
Batch 28/64 loss: -0.012240588665008545
Batch 29/64 loss: 0.017293930053710938
Batch 30/64 loss: -0.013913869857788086
Batch 31/64 loss: -0.005189061164855957
Batch 32/64 loss: -0.0017001628875732422
Batch 33/64 loss: -0.017043769359588623
Batch 34/64 loss: 0.01572108268737793
Batch 35/64 loss: -0.0022029876708984375
Batch 36/64 loss: -0.005204677581787109
Batch 37/64 loss: 0.01509183645248413
Batch 38/64 loss: 0.007293820381164551
Batch 39/64 loss: -0.012704849243164062
Batch 40/64 loss: 0.026487350463867188
Batch 41/64 loss: -0.02345043420791626
Batch 42/64 loss: -0.0009229779243469238
Batch 43/64 loss: -0.011132657527923584
Batch 44/64 loss: -0.0007965564727783203
Batch 45/64 loss: -0.010640382766723633
Batch 46/64 loss: -0.0052651166915893555
Batch 47/64 loss: -0.025331079959869385
Batch 48/64 loss: -0.030374765396118164
Batch 49/64 loss: -0.0006275773048400879
Batch 50/64 loss: -0.020747900009155273
Batch 51/64 loss: 0.005002260208129883
Batch 52/64 loss: -0.02319920063018799
Batch 53/64 loss: -0.0064476728439331055
Batch 54/64 loss: -0.024436771869659424
Batch 55/64 loss: -0.03170496225357056
Batch 56/64 loss: -0.004553556442260742
Batch 57/64 loss: -0.02356696128845215
Batch 58/64 loss: 0.007817864418029785
Batch 59/64 loss: -0.02715134620666504
Batch 60/64 loss: -0.011383295059204102
Batch 61/64 loss: -0.025688230991363525
Batch 62/64 loss: 0.0024868249893188477
Batch 63/64 loss: -0.02994590997695923
Batch 64/64 loss: 0.0031042098999023438
Epoch 138  Train loss: -0.010062789916992188  Val loss: 0.0594705074923145
Epoch 139
-------------------------------
Batch 1/64 loss: -0.00824660062789917
Batch 2/64 loss: -0.01961761713027954
Batch 3/64 loss: 0.0010149478912353516
Batch 4/64 loss: -0.01918572187423706
Batch 5/64 loss: 0.004503130912780762
Batch 6/64 loss: -0.010498285293579102
Batch 7/64 loss: -0.011672616004943848
Batch 8/64 loss: 0.005045592784881592
Batch 9/64 loss: -0.02468341588973999
Batch 10/64 loss: -0.017555713653564453
Batch 11/64 loss: -0.03845322132110596
Batch 12/64 loss: -0.015552759170532227
Batch 13/64 loss: -0.009890556335449219
Batch 14/64 loss: -0.00782233476638794
Batch 15/64 loss: -0.007901370525360107
Batch 16/64 loss: -0.02481013536453247
Batch 17/64 loss: -0.01662755012512207
Batch 18/64 loss: 0.004060983657836914
Batch 19/64 loss: -0.01625537872314453
Batch 20/64 loss: -0.011794030666351318
Batch 21/64 loss: -0.02465277910232544
Batch 22/64 loss: -0.01328188180923462
Batch 23/64 loss: -0.0021840929985046387
Batch 24/64 loss: 0.003486454486846924
Batch 25/64 loss: -0.0011354684829711914
Batch 26/64 loss: -0.01303941011428833
Batch 27/64 loss: -0.023825347423553467
Batch 28/64 loss: 0.014632225036621094
Batch 29/64 loss: -0.005721867084503174
Batch 30/64 loss: -0.02872169017791748
Batch 31/64 loss: 0.0029959678649902344
Batch 32/64 loss: 0.003078758716583252
Batch 33/64 loss: -0.0207136869430542
Batch 34/64 loss: 0.018890023231506348
Batch 35/64 loss: -0.005753159523010254
Batch 36/64 loss: -0.017759501934051514
Batch 37/64 loss: -0.002307593822479248
Batch 38/64 loss: -0.0383983850479126
Batch 39/64 loss: -0.02358025312423706
Batch 40/64 loss: 0.000674903392791748
Batch 41/64 loss: -0.012652099132537842
Batch 42/64 loss: -0.026105523109436035
Batch 43/64 loss: 0.015505075454711914
Batch 44/64 loss: 0.01037830114364624
Batch 45/64 loss: -0.03449970483779907
Batch 46/64 loss: -0.0004850625991821289
Batch 47/64 loss: -0.020210444927215576
Batch 48/64 loss: 0.0026965737342834473
Batch 49/64 loss: -0.019857168197631836
Batch 50/64 loss: -0.029071033000946045
Batch 51/64 loss: -0.01803445816040039
Batch 52/64 loss: -0.01324462890625
Batch 53/64 loss: -0.01685887575149536
Batch 54/64 loss: 0.013195037841796875
Batch 55/64 loss: -0.050183117389678955
Batch 56/64 loss: -0.00036787986755371094
Batch 57/64 loss: 0.00879216194152832
Batch 58/64 loss: -0.03456336259841919
Batch 59/64 loss: 0.013525128364562988
Batch 60/64 loss: -0.015492379665374756
Batch 61/64 loss: -0.04739797115325928
Batch 62/64 loss: 0.015005111694335938
Batch 63/64 loss: 0.00336533784866333
Batch 64/64 loss: -0.01438671350479126
Epoch 139  Train loss: -0.010833105152728511  Val loss: 0.0645547077008539
Epoch 140
-------------------------------
Batch 1/64 loss: 0.00860285758972168
Batch 2/64 loss: -0.007087290287017822
Batch 3/64 loss: -0.020694494247436523
Batch 4/64 loss: -0.02391350269317627
Batch 5/64 loss: -0.01573324203491211
Batch 6/64 loss: -0.007929384708404541
Batch 7/64 loss: -0.03140014410018921
Batch 8/64 loss: -0.009984791278839111
Batch 9/64 loss: 0.004585325717926025
Batch 10/64 loss: -0.01781708002090454
Batch 11/64 loss: -0.001311659812927246
Batch 12/64 loss: -0.008798003196716309
Batch 13/64 loss: -0.0034893155097961426
Batch 14/64 loss: -0.01926720142364502
Batch 15/64 loss: -0.0046083927154541016
Batch 16/64 loss: -0.04097932577133179
Batch 17/64 loss: 0.00270003080368042
Batch 18/64 loss: -0.01829230785369873
Batch 19/64 loss: -0.028255820274353027
Batch 20/64 loss: 0.005692422389984131
Batch 21/64 loss: -0.02487325668334961
Batch 22/64 loss: 0.02983677387237549
Batch 23/64 loss: -0.02068561315536499
Batch 24/64 loss: -0.021080970764160156
Batch 25/64 loss: -0.023289799690246582
Batch 26/64 loss: 0.01751542091369629
Batch 27/64 loss: 0.019847393035888672
Batch 28/64 loss: -0.02744007110595703
Batch 29/64 loss: -0.006101369857788086
Batch 30/64 loss: -0.020150721073150635
Batch 31/64 loss: -0.008482694625854492
Batch 32/64 loss: -0.011530160903930664
Batch 33/64 loss: -0.01173102855682373
Batch 34/64 loss: 0.005882143974304199
Batch 35/64 loss: -0.029807209968566895
Batch 36/64 loss: -0.0008201003074645996
Batch 37/64 loss: -0.035442054271698
Batch 38/64 loss: -0.016388893127441406
Batch 39/64 loss: -0.027301549911499023
Batch 40/64 loss: -0.03575921058654785
Batch 41/64 loss: 0.010453343391418457
Batch 42/64 loss: 0.016228973865509033
Batch 43/64 loss: -0.02768731117248535
Batch 44/64 loss: -0.026205003261566162
Batch 45/64 loss: -0.03761851787567139
Batch 46/64 loss: 0.013159513473510742
Batch 47/64 loss: 0.009953975677490234
Batch 48/64 loss: -0.031048178672790527
Batch 49/64 loss: -0.021018505096435547
Batch 50/64 loss: -0.018433690071105957
Batch 51/64 loss: -0.019561707973480225
Batch 52/64 loss: 0.004223346710205078
Batch 53/64 loss: -0.018269360065460205
Batch 54/64 loss: -0.038146138191223145
Batch 55/64 loss: -0.0059645771980285645
Batch 56/64 loss: 0.005372762680053711
Batch 57/64 loss: -0.03294944763183594
Batch 58/64 loss: 0.013541936874389648
Batch 59/64 loss: -0.028460562229156494
Batch 60/64 loss: -0.01242673397064209
Batch 61/64 loss: -0.034813642501831055
Batch 62/64 loss: -0.0022235512733459473
Batch 63/64 loss: -0.002212226390838623
Batch 64/64 loss: 0.023542702198028564
Epoch 140  Train loss: -0.011799726532954795  Val loss: 0.07214355550680783
Epoch 141
-------------------------------
Batch 1/64 loss: -0.018276989459991455
Batch 2/64 loss: -0.03943568468093872
Batch 3/64 loss: 0.002675354480743408
Batch 4/64 loss: -0.034700751304626465
Batch 5/64 loss: -0.0030992627143859863
Batch 6/64 loss: -0.02402520179748535
Batch 7/64 loss: -0.04358905553817749
Batch 8/64 loss: -0.018047094345092773
Batch 9/64 loss: -0.0027486681938171387
Batch 10/64 loss: 0.007576346397399902
Batch 11/64 loss: -0.015404999256134033
Batch 12/64 loss: -0.0028306245803833008
Batch 13/64 loss: -0.022974014282226562
Batch 14/64 loss: -0.028162002563476562
Batch 15/64 loss: -0.030181050300598145
Batch 16/64 loss: -0.021137654781341553
Batch 17/64 loss: -0.025468885898590088
Batch 18/64 loss: -0.021047353744506836
Batch 19/64 loss: 0.001287698745727539
Batch 20/64 loss: 0.0013448596000671387
Batch 21/64 loss: -0.022988736629486084
Batch 22/64 loss: -0.023328721523284912
Batch 23/64 loss: 0.0006145834922790527
Batch 24/64 loss: 0.020723581314086914
Batch 25/64 loss: 0.01072549819946289
Batch 26/64 loss: 0.016824424266815186
Batch 27/64 loss: -0.012314319610595703
Batch 28/64 loss: -0.019756615161895752
Batch 29/64 loss: -0.013831138610839844
Batch 30/64 loss: -0.001079261302947998
Batch 31/64 loss: -0.007016479969024658
Batch 32/64 loss: -0.012406527996063232
Batch 33/64 loss: 0.011210083961486816
Batch 34/64 loss: -0.0039001107215881348
Batch 35/64 loss: -0.033925414085388184
Batch 36/64 loss: 0.00634688138961792
Batch 37/64 loss: -0.023805201053619385
Batch 38/64 loss: -0.0059836506843566895
Batch 39/64 loss: -0.00010412931442260742
Batch 40/64 loss: 0.009350121021270752
Batch 41/64 loss: -0.016580045223236084
Batch 42/64 loss: 0.014511704444885254
Batch 43/64 loss: 0.00720822811126709
Batch 44/64 loss: -0.02855384349822998
Batch 45/64 loss: -0.021436214447021484
Batch 46/64 loss: -0.024945378303527832
Batch 47/64 loss: -0.010703444480895996
Batch 48/64 loss: 0.003354191780090332
Batch 49/64 loss: -0.010336875915527344
Batch 50/64 loss: -0.014965474605560303
Batch 51/64 loss: -0.03337198495864868
Batch 52/64 loss: -0.02250063419342041
Batch 53/64 loss: -0.020220935344696045
Batch 54/64 loss: -0.015464305877685547
Batch 55/64 loss: -0.014562726020812988
Batch 56/64 loss: 0.005983412265777588
Batch 57/64 loss: -0.03620290756225586
Batch 58/64 loss: -0.013555824756622314
Batch 59/64 loss: -0.025076687335968018
Batch 60/64 loss: -0.025074660778045654
Batch 61/64 loss: 0.0030032992362976074
Batch 62/64 loss: -0.022933602333068848
Batch 63/64 loss: 0.024926066398620605
Batch 64/64 loss: -0.002013564109802246
Epoch 141  Train loss: -0.011637631117128858  Val loss: 0.06503740823555648
Epoch 142
-------------------------------
Batch 1/64 loss: -0.02513200044631958
Batch 2/64 loss: -0.013555288314819336
Batch 3/64 loss: -0.020030856132507324
Batch 4/64 loss: -0.024698734283447266
Batch 5/64 loss: -0.0237007737159729
Batch 6/64 loss: -0.016515076160430908
Batch 7/64 loss: -0.03193002939224243
Batch 8/64 loss: -0.022346556186676025
Batch 9/64 loss: -0.005827367305755615
Batch 10/64 loss: -0.024590492248535156
Batch 11/64 loss: -0.02370142936706543
Batch 12/64 loss: -0.01694542169570923
Batch 13/64 loss: 0.01999223232269287
Batch 14/64 loss: -0.013725519180297852
Batch 15/64 loss: -0.01682138442993164
Batch 16/64 loss: 0.010899186134338379
Batch 17/64 loss: -0.013340294361114502
Batch 18/64 loss: -0.03151220083236694
Batch 19/64 loss: 0.002814650535583496
Batch 20/64 loss: -0.030880510807037354
Batch 21/64 loss: -0.0031316280364990234
Batch 22/64 loss: 0.014249086380004883
Batch 23/64 loss: -0.0013137459754943848
Batch 24/64 loss: 0.007176578044891357
Batch 25/64 loss: -0.007609903812408447
Batch 26/64 loss: -0.029139161109924316
Batch 27/64 loss: -0.004973053932189941
Batch 28/64 loss: -0.010672390460968018
Batch 29/64 loss: -0.009665906429290771
Batch 30/64 loss: -0.0009809136390686035
Batch 31/64 loss: -0.015675008296966553
Batch 32/64 loss: 0.0024306178092956543
Batch 33/64 loss: -0.033006489276885986
Batch 34/64 loss: -0.007999181747436523
Batch 35/64 loss: 0.005212962627410889
Batch 36/64 loss: 0.0012262463569641113
Batch 37/64 loss: -0.020969629287719727
Batch 38/64 loss: -0.02188277244567871
Batch 39/64 loss: -0.019215524196624756
Batch 40/64 loss: -0.0374295711517334
Batch 41/64 loss: -0.016577303409576416
Batch 42/64 loss: -0.04660952091217041
Batch 43/64 loss: 0.02669733762741089
Batch 44/64 loss: -0.02436220645904541
Batch 45/64 loss: -0.027373790740966797
Batch 46/64 loss: -0.038603007793426514
Batch 47/64 loss: -0.022020399570465088
Batch 48/64 loss: 0.01566380262374878
Batch 49/64 loss: -0.016262471675872803
Batch 50/64 loss: -0.018703579902648926
Batch 51/64 loss: 0.00787365436553955
Batch 52/64 loss: -0.020562708377838135
Batch 53/64 loss: -0.03588610887527466
Batch 54/64 loss: -0.025642752647399902
Batch 55/64 loss: -0.016928374767303467
Batch 56/64 loss: -0.02334308624267578
Batch 57/64 loss: 0.02053755521774292
Batch 58/64 loss: -0.015778660774230957
Batch 59/64 loss: -0.0017223954200744629
Batch 60/64 loss: -0.013635575771331787
Batch 61/64 loss: -0.019812464714050293
Batch 62/64 loss: -0.017310738563537598
Batch 63/64 loss: -0.031359732151031494
Batch 64/64 loss: -0.031307220458984375
Epoch 142  Train loss: -0.014119532529045555  Val loss: 0.06257401667919356
Epoch 143
-------------------------------
Batch 1/64 loss: -0.0214616060256958
Batch 2/64 loss: -0.051135241985321045
Batch 3/64 loss: -0.03145807981491089
Batch 4/64 loss: -0.04205930233001709
Batch 5/64 loss: -0.023909389972686768
Batch 6/64 loss: -0.01827263832092285
Batch 7/64 loss: -0.023063957691192627
Batch 8/64 loss: -0.02899456024169922
Batch 9/64 loss: 0.008587658405303955
Batch 10/64 loss: -0.015832960605621338
Batch 11/64 loss: 0.010406255722045898
Batch 12/64 loss: -0.03158259391784668
Batch 13/64 loss: -0.025689661502838135
Batch 14/64 loss: -0.00622248649597168
Batch 15/64 loss: -0.006242573261260986
Batch 16/64 loss: -0.03224480152130127
Batch 17/64 loss: -0.015252411365509033
Batch 18/64 loss: -0.025758445262908936
Batch 19/64 loss: -0.0022897720336914062
Batch 20/64 loss: -0.027718544006347656
Batch 21/64 loss: -0.02575773000717163
Batch 22/64 loss: -0.03222125768661499
Batch 23/64 loss: -0.012927472591400146
Batch 24/64 loss: -0.030319511890411377
Batch 25/64 loss: -0.025957226753234863
Batch 26/64 loss: 0.012495875358581543
Batch 27/64 loss: -0.026916921138763428
Batch 28/64 loss: -0.012523472309112549
Batch 29/64 loss: -0.02623039484024048
Batch 30/64 loss: -0.013049483299255371
Batch 31/64 loss: -0.00413668155670166
Batch 32/64 loss: 0.007733523845672607
Batch 33/64 loss: 0.026924312114715576
Batch 34/64 loss: -0.038825035095214844
Batch 35/64 loss: -0.03296881914138794
Batch 36/64 loss: 0.012039244174957275
Batch 37/64 loss: -0.019260883331298828
Batch 38/64 loss: -0.006761610507965088
Batch 39/64 loss: -0.020517170429229736
Batch 40/64 loss: -0.003461003303527832
Batch 41/64 loss: -0.010117888450622559
Batch 42/64 loss: 0.0050231218338012695
Batch 43/64 loss: 0.0031815767288208008
Batch 44/64 loss: -0.0045983195304870605
Batch 45/64 loss: -0.011006951332092285
Batch 46/64 loss: 0.03080230951309204
Batch 47/64 loss: 0.0007249712944030762
Batch 48/64 loss: -0.020974576473236084
Batch 49/64 loss: -0.017560124397277832
Batch 50/64 loss: -0.009263157844543457
Batch 51/64 loss: -0.02205181121826172
Batch 52/64 loss: -0.007281899452209473
Batch 53/64 loss: 0.009725987911224365
Batch 54/64 loss: -0.03808295726776123
Batch 55/64 loss: -0.012771427631378174
Batch 56/64 loss: -0.004968404769897461
Batch 57/64 loss: 0.01669985055923462
Batch 58/64 loss: -0.01851046085357666
Batch 59/64 loss: -0.034001946449279785
Batch 60/64 loss: 0.054967641830444336
Batch 61/64 loss: -0.029598891735076904
Batch 62/64 loss: -0.00591045618057251
Batch 63/64 loss: 0.01896756887435913
Batch 64/64 loss: 0.00869065523147583
Epoch 143  Train loss: -0.012281177791894651  Val loss: 0.06984154424306863
Epoch 144
-------------------------------
Batch 1/64 loss: -0.006511986255645752
Batch 2/64 loss: 0.029365122318267822
Batch 3/64 loss: -0.014394402503967285
Batch 4/64 loss: -0.021935105323791504
Batch 5/64 loss: -0.010072469711303711
Batch 6/64 loss: -0.04744642972946167
Batch 7/64 loss: 0.004231750965118408
Batch 8/64 loss: -0.02809518575668335
Batch 9/64 loss: -0.037537455558776855
Batch 10/64 loss: -0.019366681575775146
Batch 11/64 loss: -0.03933107852935791
Batch 12/64 loss: -0.03488266468048096
Batch 13/64 loss: -0.011555612087249756
Batch 14/64 loss: 0.029406309127807617
Batch 15/64 loss: -0.019183337688446045
Batch 16/64 loss: 0.0013994574546813965
Batch 17/64 loss: -0.0195467472076416
Batch 18/64 loss: 0.006058394908905029
Batch 19/64 loss: -0.023315787315368652
Batch 20/64 loss: -0.030701518058776855
Batch 21/64 loss: -0.017041146755218506
Batch 22/64 loss: -0.027505040168762207
Batch 23/64 loss: -0.028019964694976807
Batch 24/64 loss: -0.04108971357345581
Batch 25/64 loss: -0.007547914981842041
Batch 26/64 loss: -0.011880159378051758
Batch 27/64 loss: 0.004463791847229004
Batch 28/64 loss: 0.021977245807647705
Batch 29/64 loss: -0.011686205863952637
Batch 30/64 loss: -0.012412011623382568
Batch 31/64 loss: -0.03128790855407715
Batch 32/64 loss: 0.008092284202575684
Batch 33/64 loss: 0.008874773979187012
Batch 34/64 loss: 0.018381118774414062
Batch 35/64 loss: -0.006684303283691406
Batch 36/64 loss: -0.01043689250946045
Batch 37/64 loss: -0.014503717422485352
Batch 38/64 loss: -0.022823572158813477
Batch 39/64 loss: -0.02700638771057129
Batch 40/64 loss: -0.005390167236328125
Batch 41/64 loss: -0.044162869453430176
Batch 42/64 loss: -0.017927885055541992
Batch 43/64 loss: -0.0020821094512939453
Batch 44/64 loss: -0.015531361103057861
Batch 45/64 loss: -0.02332538366317749
Batch 46/64 loss: -0.01916104555130005
Batch 47/64 loss: -0.02569270133972168
Batch 48/64 loss: -0.010436475276947021
Batch 49/64 loss: -0.013617217540740967
Batch 50/64 loss: 0.025750160217285156
Batch 51/64 loss: -0.03264129161834717
Batch 52/64 loss: -0.018834292888641357
Batch 53/64 loss: -0.015871882438659668
Batch 54/64 loss: -0.024518191814422607
Batch 55/64 loss: -0.02477961778640747
Batch 56/64 loss: -0.025894880294799805
Batch 57/64 loss: -0.02018415927886963
Batch 58/64 loss: -0.005835831165313721
Batch 59/64 loss: -0.01849961280822754
Batch 60/64 loss: -0.021012723445892334
Batch 61/64 loss: -0.022786080837249756
Batch 62/64 loss: -0.004958212375640869
Batch 63/64 loss: -0.03068709373474121
Batch 64/64 loss: -0.017276763916015625
Epoch 144  Train loss: -0.014628857257319432  Val loss: 0.07412848685615252
Epoch 145
-------------------------------
Batch 1/64 loss: -0.02427971363067627
Batch 2/64 loss: -0.026810109615325928
Batch 3/64 loss: -0.030338048934936523
Batch 4/64 loss: -0.013656318187713623
Batch 5/64 loss: -0.015045523643493652
Batch 6/64 loss: -0.024008750915527344
Batch 7/64 loss: -0.013579607009887695
Batch 8/64 loss: 0.0001545548439025879
Batch 9/64 loss: 0.014227509498596191
Batch 10/64 loss: -0.002663910388946533
Batch 11/64 loss: -0.04696476459503174
Batch 12/64 loss: -0.026486515998840332
Batch 13/64 loss: -0.0378718376159668
Batch 14/64 loss: -0.030189454555511475
Batch 15/64 loss: -0.008939921855926514
Batch 16/64 loss: -0.03356868028640747
Batch 17/64 loss: -0.043032288551330566
Batch 18/64 loss: -0.005663573741912842
Batch 19/64 loss: -0.01542520523071289
Batch 20/64 loss: -0.02717745304107666
Batch 21/64 loss: -0.031773388385772705
Batch 22/64 loss: -0.010110259056091309
Batch 23/64 loss: -0.0003693699836730957
Batch 24/64 loss: 0.0007306933403015137
Batch 25/64 loss: -0.038673341274261475
Batch 26/64 loss: -0.004684627056121826
Batch 27/64 loss: 0.007024526596069336
Batch 28/64 loss: 0.004221141338348389
Batch 29/64 loss: -0.03815305233001709
Batch 30/64 loss: -0.024239838123321533
Batch 31/64 loss: -0.024167895317077637
Batch 32/64 loss: -0.05022245645523071
Batch 33/64 loss: 0.03365051746368408
Batch 34/64 loss: -0.0037706494331359863
Batch 35/64 loss: -0.009021997451782227
Batch 36/64 loss: -0.015857815742492676
Batch 37/64 loss: -0.021389245986938477
Batch 38/64 loss: -0.03610491752624512
Batch 39/64 loss: -0.03363358974456787
Batch 40/64 loss: -0.03973579406738281
Batch 41/64 loss: -0.023131132125854492
Batch 42/64 loss: -0.03422123193740845
Batch 43/64 loss: 0.01976644992828369
Batch 44/64 loss: -0.03824359178543091
Batch 45/64 loss: -0.011476576328277588
Batch 46/64 loss: 0.000546872615814209
Batch 47/64 loss: 0.015004932880401611
Batch 48/64 loss: -0.02630603313446045
Batch 49/64 loss: -0.03449106216430664
Batch 50/64 loss: 0.008026182651519775
Batch 51/64 loss: 0.0020431876182556152
Batch 52/64 loss: -0.018287360668182373
Batch 53/64 loss: -0.03504270315170288
Batch 54/64 loss: -0.034285783767700195
Batch 55/64 loss: -0.014142394065856934
Batch 56/64 loss: -0.01702702045440674
Batch 57/64 loss: 0.009035050868988037
Batch 58/64 loss: -0.008150994777679443
Batch 59/64 loss: -0.029839932918548584
Batch 60/64 loss: 0.015236258506774902
Batch 61/64 loss: 0.011459946632385254
Batch 62/64 loss: -0.009014248847961426
Batch 63/64 loss: -0.03547245264053345
Batch 64/64 loss: -0.007436275482177734
Epoch 145  Train loss: -0.016332420648313035  Val loss: 0.06316278152859088
Epoch 146
-------------------------------
Batch 1/64 loss: -0.032656073570251465
Batch 2/64 loss: -0.022084474563598633
Batch 3/64 loss: -0.0314328670501709
Batch 4/64 loss: -0.026157081127166748
Batch 5/64 loss: 0.0113677978515625
Batch 6/64 loss: -0.02385491132736206
Batch 7/64 loss: -0.028757691383361816
Batch 8/64 loss: -0.025310277938842773
Batch 9/64 loss: -0.02556246519088745
Batch 10/64 loss: 0.007871687412261963
Batch 11/64 loss: 0.00783771276473999
Batch 12/64 loss: -0.0282517671585083
Batch 13/64 loss: 0.001974821090698242
Batch 14/64 loss: -0.000283658504486084
Batch 15/64 loss: 0.0015925168991088867
Batch 16/64 loss: -0.01835501194000244
Batch 17/64 loss: -0.038505733013153076
Batch 18/64 loss: 0.007728695869445801
Batch 19/64 loss: -0.01273047924041748
Batch 20/64 loss: -0.01794832944869995
Batch 21/64 loss: -0.023142218589782715
Batch 22/64 loss: -0.038255929946899414
Batch 23/64 loss: 0.006712675094604492
Batch 24/64 loss: 0.044675588607788086
Batch 25/64 loss: -0.0138014554977417
Batch 26/64 loss: -0.02504068613052368
Batch 27/64 loss: -0.03713589906692505
Batch 28/64 loss: -0.035246312618255615
Batch 29/64 loss: -0.0010752677917480469
Batch 30/64 loss: -0.020538032054901123
Batch 31/64 loss: 0.016448616981506348
Batch 32/64 loss: -0.004460573196411133
Batch 33/64 loss: -0.025462031364440918
Batch 34/64 loss: -0.013315081596374512
Batch 35/64 loss: -0.022628188133239746
Batch 36/64 loss: -0.0252993106842041
Batch 37/64 loss: -0.036880433559417725
Batch 38/64 loss: -0.032171428203582764
Batch 39/64 loss: -0.027784347534179688
Batch 40/64 loss: -0.03309756517410278
Batch 41/64 loss: -0.0014164447784423828
Batch 42/64 loss: 0.00168609619140625
Batch 43/64 loss: -0.060498058795928955
Batch 44/64 loss: -0.024383068084716797
Batch 45/64 loss: 0.006861865520477295
Batch 46/64 loss: 0.007909595966339111
Batch 47/64 loss: -0.007801413536071777
Batch 48/64 loss: 0.0144004225730896
Batch 49/64 loss: -0.028059720993041992
Batch 50/64 loss: 0.015833377838134766
Batch 51/64 loss: -0.012534022331237793
Batch 52/64 loss: -0.021122992038726807
Batch 53/64 loss: -0.015064418315887451
Batch 54/64 loss: -0.01388394832611084
Batch 55/64 loss: -0.04318493604660034
Batch 56/64 loss: -0.045768797397613525
Batch 57/64 loss: -0.04377937316894531
Batch 58/64 loss: -0.013674616813659668
Batch 59/64 loss: -0.016945600509643555
Batch 60/64 loss: -0.03848612308502197
Batch 61/64 loss: -0.017168521881103516
Batch 62/64 loss: -0.013982832431793213
Batch 63/64 loss: -0.018856287002563477
Batch 64/64 loss: -0.02974945306777954
Epoch 146  Train loss: -0.01652152748668895  Val loss: 0.06680695113447524
Epoch 147
-------------------------------
Batch 1/64 loss: -0.022146940231323242
Batch 2/64 loss: -0.00893467664718628
Batch 3/64 loss: -0.019418001174926758
Batch 4/64 loss: -0.012109458446502686
Batch 5/64 loss: -0.029636144638061523
Batch 6/64 loss: -0.019294261932373047
Batch 7/64 loss: 0.035135865211486816
Batch 8/64 loss: -0.011982262134552002
Batch 9/64 loss: 0.009405374526977539
Batch 10/64 loss: -0.016195833683013916
Batch 11/64 loss: 0.006525158882141113
Batch 12/64 loss: -0.020497798919677734
Batch 13/64 loss: -0.005720853805541992
Batch 14/64 loss: -0.03928607702255249
Batch 15/64 loss: -0.03690892457962036
Batch 16/64 loss: -0.03512614965438843
Batch 17/64 loss: -0.018488585948944092
Batch 18/64 loss: -0.0396694540977478
Batch 19/64 loss: -0.02722752094268799
Batch 20/64 loss: -0.011796832084655762
Batch 21/64 loss: -0.02060830593109131
Batch 22/64 loss: 0.012086689472198486
Batch 23/64 loss: -0.01982361078262329
Batch 24/64 loss: -0.015404105186462402
Batch 25/64 loss: -0.026511013507843018
Batch 26/64 loss: -0.012493371963500977
Batch 27/64 loss: -0.008568227291107178
Batch 28/64 loss: -0.012714505195617676
Batch 29/64 loss: -0.03730738162994385
Batch 30/64 loss: -0.03982996940612793
Batch 31/64 loss: 0.00510936975479126
Batch 32/64 loss: 0.00021469593048095703
Batch 33/64 loss: -0.029937744140625
Batch 34/64 loss: -0.027378201484680176
Batch 35/64 loss: -0.03984642028808594
Batch 36/64 loss: -0.017654478549957275
Batch 37/64 loss: -0.009144246578216553
Batch 38/64 loss: -0.024008989334106445
Batch 39/64 loss: -0.02225261926651001
Batch 40/64 loss: -0.026774883270263672
Batch 41/64 loss: -0.002445518970489502
Batch 42/64 loss: -0.013981103897094727
Batch 43/64 loss: -0.02672111988067627
Batch 44/64 loss: -0.02388739585876465
Batch 45/64 loss: -0.02920091152191162
Batch 46/64 loss: -0.018234729766845703
Batch 47/64 loss: -0.029715299606323242
Batch 48/64 loss: -0.008294463157653809
Batch 49/64 loss: 0.004314839839935303
Batch 50/64 loss: -0.02672111988067627
Batch 51/64 loss: 0.012976765632629395
Batch 52/64 loss: -0.00968635082244873
Batch 53/64 loss: -0.01980578899383545
Batch 54/64 loss: -0.027901053428649902
Batch 55/64 loss: -0.009199023246765137
Batch 56/64 loss: -0.019864797592163086
Batch 57/64 loss: -0.026057422161102295
Batch 58/64 loss: -0.01806110143661499
Batch 59/64 loss: -0.029370605945587158
Batch 60/64 loss: -0.01905691623687744
Batch 61/64 loss: -0.020579814910888672
Batch 62/64 loss: 0.006824672222137451
Batch 63/64 loss: -0.03971362113952637
Batch 64/64 loss: 0.018737316131591797
Epoch 147  Train loss: -0.01688705238641477  Val loss: 0.06200129551576175
Epoch 148
-------------------------------
Batch 1/64 loss: -0.026073038578033447
Batch 2/64 loss: -0.026944458484649658
Batch 3/64 loss: -0.009187877178192139
Batch 4/64 loss: -0.03721576929092407
Batch 5/64 loss: 0.024181723594665527
Batch 6/64 loss: -0.037847936153411865
Batch 7/64 loss: -0.02103203535079956
Batch 8/64 loss: -0.006800293922424316
Batch 9/64 loss: -0.03430664539337158
Batch 10/64 loss: -0.02846086025238037
Batch 11/64 loss: -0.03552234172821045
Batch 12/64 loss: -0.0426487922668457
Batch 13/64 loss: -0.04463613033294678
Batch 14/64 loss: 0.006350398063659668
Batch 15/64 loss: -0.01947575807571411
Batch 16/64 loss: -0.03870129585266113
Batch 17/64 loss: -0.00694119930267334
Batch 18/64 loss: -0.01929473876953125
Batch 19/64 loss: -0.00787341594696045
Batch 20/64 loss: -0.010188400745391846
Batch 21/64 loss: -0.02106386423110962
Batch 22/64 loss: -0.018008947372436523
Batch 23/64 loss: -0.020155012607574463
Batch 24/64 loss: -0.05170327425003052
Batch 25/64 loss: -0.03227752447128296
Batch 26/64 loss: -0.03633701801300049
Batch 27/64 loss: 0.010849535465240479
Batch 28/64 loss: -0.004412949085235596
Batch 29/64 loss: -0.036154747009277344
Batch 30/64 loss: -0.028549790382385254
Batch 31/64 loss: -0.04155993461608887
Batch 32/64 loss: 0.021659433841705322
Batch 33/64 loss: -0.01968967914581299
Batch 34/64 loss: -0.013301849365234375
Batch 35/64 loss: -0.008396625518798828
Batch 36/64 loss: -0.034692585468292236
Batch 37/64 loss: -0.03528439998626709
Batch 38/64 loss: 0.0037000179290771484
Batch 39/64 loss: -0.03139573335647583
Batch 40/64 loss: -0.016956806182861328
Batch 41/64 loss: -0.01788496971130371
Batch 42/64 loss: -0.027693986892700195
Batch 43/64 loss: -0.012661576271057129
Batch 44/64 loss: -0.020275413990020752
Batch 45/64 loss: -0.03829383850097656
Batch 46/64 loss: 0.0025145411491394043
Batch 47/64 loss: -0.027098357677459717
Batch 48/64 loss: -0.02516394853591919
Batch 49/64 loss: -0.007550597190856934
Batch 50/64 loss: 0.014806210994720459
Batch 51/64 loss: -0.01739323139190674
Batch 52/64 loss: -0.011850237846374512
Batch 53/64 loss: 0.006445109844207764
Batch 54/64 loss: 0.016255855560302734
Batch 55/64 loss: -0.019534647464752197
Batch 56/64 loss: -0.02672708034515381
Batch 57/64 loss: -0.026643872261047363
Batch 58/64 loss: -0.03382521867752075
Batch 59/64 loss: -0.04459255933761597
Batch 60/64 loss: -0.007459044456481934
Batch 61/64 loss: -0.040593743324279785
Batch 62/64 loss: -0.002068638801574707
Batch 63/64 loss: -0.021206319332122803
Batch 64/64 loss: -0.0173223614692688
Epoch 148  Train loss: -0.019417065966363046  Val loss: 0.08002566123746105
Epoch 149
-------------------------------
Batch 1/64 loss: -0.021077275276184082
Batch 2/64 loss: -0.026697099208831787
Batch 3/64 loss: 0.0024515390396118164
Batch 4/64 loss: -0.03557479381561279
Batch 5/64 loss: -0.033896446228027344
Batch 6/64 loss: -0.006846427917480469
Batch 7/64 loss: -0.0069923996925354
Batch 8/64 loss: 0.0007565617561340332
Batch 9/64 loss: -0.015036821365356445
Batch 10/64 loss: -0.02122032642364502
Batch 11/64 loss: 0.011685192584991455
Batch 12/64 loss: -0.029020190238952637
Batch 13/64 loss: -0.02765059471130371
Batch 14/64 loss: -0.009775698184967041
Batch 15/64 loss: -0.025847434997558594
Batch 16/64 loss: -0.027368903160095215
Batch 17/64 loss: -0.014072954654693604
Batch 18/64 loss: -0.03187108039855957
Batch 19/64 loss: -0.003537416458129883
Batch 20/64 loss: -0.003645479679107666
Batch 21/64 loss: -0.03062283992767334
Batch 22/64 loss: -0.039943039417266846
Batch 23/64 loss: -0.025651276111602783
Batch 24/64 loss: -0.01572728157043457
Batch 25/64 loss: -0.034545719623565674
Batch 26/64 loss: -0.0406954288482666
Batch 27/64 loss: -0.015333294868469238
Batch 28/64 loss: -0.043491363525390625
Batch 29/64 loss: -0.016553103923797607
Batch 30/64 loss: -0.014251470565795898
Batch 31/64 loss: -0.030198216438293457
Batch 32/64 loss: -0.02656233310699463
Batch 33/64 loss: 0.001652836799621582
Batch 34/64 loss: -0.01988750696182251
Batch 35/64 loss: -0.017345070838928223
Batch 36/64 loss: -0.016828715801239014
Batch 37/64 loss: -0.026011943817138672
Batch 38/64 loss: -0.02734905481338501
Batch 39/64 loss: -0.027099251747131348
Batch 40/64 loss: -0.0060239434242248535
Batch 41/64 loss: -0.015714049339294434
Batch 42/64 loss: -0.018149971961975098
Batch 43/64 loss: -0.0105820894241333
Batch 44/64 loss: -0.02286738157272339
Batch 45/64 loss: -0.038130342960357666
Batch 46/64 loss: -0.03201091289520264
Batch 47/64 loss: -0.031522154808044434
Batch 48/64 loss: -0.010927915573120117
Batch 49/64 loss: 0.007923483848571777
Batch 50/64 loss: -0.023250341415405273
Batch 51/64 loss: -0.019342422485351562
Batch 52/64 loss: -0.030080795288085938
Batch 53/64 loss: -0.02200847864151001
Batch 54/64 loss: -0.02494281530380249
Batch 55/64 loss: -0.04731470346450806
Batch 56/64 loss: -0.008239269256591797
Batch 57/64 loss: -0.005510449409484863
Batch 58/64 loss: -0.028109490871429443
Batch 59/64 loss: -0.025546550750732422
Batch 60/64 loss: -0.007349967956542969
Batch 61/64 loss: -0.028264760971069336
Batch 62/64 loss: -0.04163622856140137
Batch 63/64 loss: -0.029710888862609863
Batch 64/64 loss: -0.012799263000488281
Epoch 149  Train loss: -0.02071520019980038  Val loss: 0.056415121375080646
Saving best model, epoch: 149
Epoch 150
-------------------------------
Batch 1/64 loss: -0.01848214864730835
Batch 2/64 loss: -0.013859868049621582
Batch 3/64 loss: -0.03190743923187256
Batch 4/64 loss: -0.023803412914276123
Batch 5/64 loss: -0.055559396743774414
Batch 6/64 loss: -0.02490973472595215
Batch 7/64 loss: -0.03468447923660278
Batch 8/64 loss: -0.029683470726013184
Batch 9/64 loss: -0.01303035020828247
Batch 10/64 loss: -0.03195476531982422
Batch 11/64 loss: 0.007148087024688721
Batch 12/64 loss: -0.028054475784301758
Batch 13/64 loss: -0.043704986572265625
Batch 14/64 loss: -0.050295889377593994
Batch 15/64 loss: -0.023459553718566895
Batch 16/64 loss: -0.024969041347503662
Batch 17/64 loss: -0.010598957538604736
Batch 18/64 loss: -0.02933824062347412
Batch 19/64 loss: -0.02620929479598999
Batch 20/64 loss: -0.031665682792663574
Batch 21/64 loss: -0.020897388458251953
Batch 22/64 loss: -0.030249714851379395
Batch 23/64 loss: -0.04400157928466797
Batch 24/64 loss: -0.03188145160675049
Batch 25/64 loss: -0.013262510299682617
Batch 26/64 loss: -0.04245030879974365
Batch 27/64 loss: -0.02621215581893921
Batch 28/64 loss: -0.04903459548950195
Batch 29/64 loss: -0.02857762575149536
Batch 30/64 loss: -0.04693084955215454
Batch 31/64 loss: -0.020557820796966553
Batch 32/64 loss: -0.013244271278381348
Batch 33/64 loss: -0.00034481287002563477
Batch 34/64 loss: -0.0012050867080688477
Batch 35/64 loss: 0.02136129140853882
Batch 36/64 loss: 0.004135072231292725
Batch 37/64 loss: -0.04108220338821411
Batch 38/64 loss: -0.014360189437866211
Batch 39/64 loss: -0.018413126468658447
Batch 40/64 loss: -0.025052011013031006
Batch 41/64 loss: -0.01967144012451172
Batch 42/64 loss: -0.014065980911254883
Batch 43/64 loss: -0.030167818069458008
Batch 44/64 loss: -0.004173636436462402
Batch 45/64 loss: -0.024854719638824463
Batch 46/64 loss: -0.04450041055679321
Batch 47/64 loss: -0.0294610857963562
Batch 48/64 loss: -0.002414882183074951
Batch 49/64 loss: -0.02745974063873291
Batch 50/64 loss: -0.019867658615112305
Batch 51/64 loss: -0.0283735990524292
Batch 52/64 loss: -0.036091148853302
Batch 53/64 loss: -0.014790058135986328
Batch 54/64 loss: -0.0170859694480896
Batch 55/64 loss: -0.013672173023223877
Batch 56/64 loss: 0.00729060173034668
Batch 57/64 loss: -0.009694695472717285
Batch 58/64 loss: -0.010077476501464844
Batch 59/64 loss: -0.025863349437713623
Batch 60/64 loss: -0.008311212062835693
Batch 61/64 loss: -0.029620230197906494
Batch 62/64 loss: -0.026959598064422607
Batch 63/64 loss: -0.00790029764175415
Batch 64/64 loss: -0.01429075002670288
Epoch 150  Train loss: -0.02242806401907229  Val loss: 0.0602682669138171
Epoch 151
-------------------------------
Batch 1/64 loss: 0.014303386211395264
Batch 2/64 loss: -0.01773834228515625
Batch 3/64 loss: -0.034140825271606445
Batch 4/64 loss: -0.0020219087600708008
Batch 5/64 loss: -0.04568135738372803
Batch 6/64 loss: -0.03415423631668091
Batch 7/64 loss: -0.0366055965423584
Batch 8/64 loss: -0.04002523422241211
Batch 9/64 loss: 0.010099053382873535
Batch 10/64 loss: -0.012465894222259521
Batch 11/64 loss: -0.052047789096832275
Batch 12/64 loss: -0.02439713478088379
Batch 13/64 loss: -0.04180783033370972
Batch 14/64 loss: 0.021492183208465576
Batch 15/64 loss: -0.03115004301071167
Batch 16/64 loss: -0.04265183210372925
Batch 17/64 loss: -0.03637510538101196
Batch 18/64 loss: -0.0022546052932739258
Batch 19/64 loss: -0.024192392826080322
Batch 20/64 loss: -0.04610937833786011
Batch 21/64 loss: -0.018324315547943115
Batch 22/64 loss: -0.02575010061264038
Batch 23/64 loss: 0.005469620227813721
Batch 24/64 loss: -0.039079904556274414
Batch 25/64 loss: -0.02145230770111084
Batch 26/64 loss: 0.01392596960067749
Batch 27/64 loss: -0.022529661655426025
Batch 28/64 loss: -0.03334683179855347
Batch 29/64 loss: 0.01692885160446167
Batch 30/64 loss: -0.021643519401550293
Batch 31/64 loss: -0.02650827169418335
Batch 32/64 loss: -0.02837049961090088
Batch 33/64 loss: -0.00904989242553711
Batch 34/64 loss: -0.029646754264831543
Batch 35/64 loss: -0.02105247974395752
Batch 36/64 loss: -0.03680884838104248
Batch 37/64 loss: -0.01998072862625122
Batch 38/64 loss: -0.025889575481414795
Batch 39/64 loss: -0.004779219627380371
Batch 40/64 loss: -0.026940464973449707
Batch 41/64 loss: -0.012056887149810791
Batch 42/64 loss: 0.0127028226852417
Batch 43/64 loss: -0.05250817537307739
Batch 44/64 loss: -0.04212045669555664
Batch 45/64 loss: -0.03906029462814331
Batch 46/64 loss: -0.015412986278533936
Batch 47/64 loss: -0.015707015991210938
Batch 48/64 loss: -0.027812421321868896
Batch 49/64 loss: -0.008646249771118164
Batch 50/64 loss: -0.01666492223739624
Batch 51/64 loss: -0.02915668487548828
Batch 52/64 loss: 0.0009164810180664062
Batch 53/64 loss: -0.026280224323272705
Batch 54/64 loss: -0.033368587493896484
Batch 55/64 loss: -0.030870258808135986
Batch 56/64 loss: -0.00913459062576294
Batch 57/64 loss: 0.0026493072509765625
Batch 58/64 loss: 0.0074721574783325195
Batch 59/64 loss: -0.026786625385284424
Batch 60/64 loss: -0.007773876190185547
Batch 61/64 loss: -0.036533355712890625
Batch 62/64 loss: -0.006936311721801758
Batch 63/64 loss: -0.023858189582824707
Batch 64/64 loss: -0.05482238531112671
Epoch 151  Train loss: -0.020875575729444915  Val loss: 0.05668350736709805
Epoch 152
-------------------------------
Batch 1/64 loss: -0.04583853483200073
Batch 2/64 loss: -0.01585477590560913
Batch 3/64 loss: -0.026854276657104492
Batch 4/64 loss: -0.037630438804626465
Batch 5/64 loss: -0.041993916034698486
Batch 6/64 loss: -0.01608407497406006
Batch 7/64 loss: -0.03829091787338257
Batch 8/64 loss: 0.016449272632598877
Batch 9/64 loss: -0.020635128021240234
Batch 10/64 loss: -0.0053136348724365234
Batch 11/64 loss: -0.03585612773895264
Batch 12/64 loss: -0.020808875560760498
Batch 13/64 loss: -0.00857234001159668
Batch 14/64 loss: -0.029611408710479736
Batch 15/64 loss: -0.02257704734802246
Batch 16/64 loss: -0.03340810537338257
Batch 17/64 loss: -0.04442566633224487
Batch 18/64 loss: -0.021642982959747314
Batch 19/64 loss: -0.0012230277061462402
Batch 20/64 loss: -0.0342637300491333
Batch 21/64 loss: 0.003273606300354004
Batch 22/64 loss: -0.046607136726379395
Batch 23/64 loss: -0.011187970638275146
Batch 24/64 loss: -0.043609023094177246
Batch 25/64 loss: -0.03168070316314697
Batch 26/64 loss: -0.036747753620147705
Batch 27/64 loss: -0.023068904876708984
Batch 28/64 loss: -0.0641583800315857
Batch 29/64 loss: -0.015794098377227783
Batch 30/64 loss: -0.04335379600524902
Batch 31/64 loss: -0.026745080947875977
Batch 32/64 loss: -0.025412440299987793
Batch 33/64 loss: -0.010626673698425293
Batch 34/64 loss: -0.038895249366760254
Batch 35/64 loss: 0.0002726316452026367
Batch 36/64 loss: 0.006420254707336426
Batch 37/64 loss: -0.014991402626037598
Batch 38/64 loss: -0.015147507190704346
Batch 39/64 loss: -0.03751176595687866
Batch 40/64 loss: -0.019656002521514893
Batch 41/64 loss: -0.025035083293914795
Batch 42/64 loss: -0.01608794927597046
Batch 43/64 loss: -0.04095053672790527
Batch 44/64 loss: -0.02914130687713623
Batch 45/64 loss: -0.031478822231292725
Batch 46/64 loss: 0.0014386177062988281
Batch 47/64 loss: -0.020816922187805176
Batch 48/64 loss: -0.020397543907165527
Batch 49/64 loss: -0.033277273178100586
Batch 50/64 loss: -0.014223575592041016
Batch 51/64 loss: -0.006081283092498779
Batch 52/64 loss: -0.007523000240325928
Batch 53/64 loss: 0.020757198333740234
Batch 54/64 loss: -0.03477901220321655
Batch 55/64 loss: 0.005033552646636963
Batch 56/64 loss: -0.003397345542907715
Batch 57/64 loss: -0.014468669891357422
Batch 58/64 loss: 0.01885688304901123
Batch 59/64 loss: -0.013069629669189453
Batch 60/64 loss: -0.028989791870117188
Batch 61/64 loss: -0.0386541485786438
Batch 62/64 loss: -0.03377532958984375
Batch 63/64 loss: 0.007893919944763184
Batch 64/64 loss: -0.015987038612365723
Epoch 152  Train loss: -0.021173654350579954  Val loss: 0.06593275377430867
Epoch 153
-------------------------------
Batch 1/64 loss: -0.0361291766166687
Batch 2/64 loss: -0.02986985445022583
Batch 3/64 loss: -0.02910083532333374
Batch 4/64 loss: -0.004619240760803223
Batch 5/64 loss: 0.010790646076202393
Batch 6/64 loss: -0.04145956039428711
Batch 7/64 loss: -0.03211545944213867
Batch 8/64 loss: -0.02062976360321045
Batch 9/64 loss: -0.05201232433319092
Batch 10/64 loss: -0.01822340488433838
Batch 11/64 loss: -0.024055302143096924
Batch 12/64 loss: -0.05614471435546875
Batch 13/64 loss: -0.009738564491271973
Batch 14/64 loss: -0.01905888319015503
Batch 15/64 loss: -0.04359090328216553
Batch 16/64 loss: -0.0344356894493103
Batch 17/64 loss: -0.05848228931427002
Batch 18/64 loss: -0.01159137487411499
Batch 19/64 loss: 0.005396842956542969
Batch 20/64 loss: -0.0055694580078125
Batch 21/64 loss: -0.012603759765625
Batch 22/64 loss: -0.02698540687561035
Batch 23/64 loss: -0.04907876253128052
Batch 24/64 loss: -0.023888587951660156
Batch 25/64 loss: -0.007786870002746582
Batch 26/64 loss: 0.002996087074279785
Batch 27/64 loss: -0.02960968017578125
Batch 28/64 loss: -0.012103557586669922
Batch 29/64 loss: -0.02162456512451172
Batch 30/64 loss: -0.02617466449737549
Batch 31/64 loss: -0.03391671180725098
Batch 32/64 loss: -0.038238525390625
Batch 33/64 loss: -0.01867389678955078
Batch 34/64 loss: -0.027427077293395996
Batch 35/64 loss: -0.02196866273880005
Batch 36/64 loss: -0.031529784202575684
Batch 37/64 loss: -0.04512244462966919
Batch 38/64 loss: -0.026748478412628174
Batch 39/64 loss: -0.018023312091827393
Batch 40/64 loss: -0.02130711078643799
Batch 41/64 loss: -0.016540586948394775
Batch 42/64 loss: -0.019632577896118164
Batch 43/64 loss: -0.003758251667022705
Batch 44/64 loss: -0.013745009899139404
Batch 45/64 loss: 0.0016230940818786621
Batch 46/64 loss: -0.04698765277862549
Batch 47/64 loss: -0.03892195224761963
Batch 48/64 loss: -0.03771847486495972
Batch 49/64 loss: -0.03272652626037598
Batch 50/64 loss: -0.019395947456359863
Batch 51/64 loss: -0.03889000415802002
Batch 52/64 loss: -0.04108142852783203
Batch 53/64 loss: -0.013334691524505615
Batch 54/64 loss: -0.020686566829681396
Batch 55/64 loss: -0.03854250907897949
Batch 56/64 loss: -0.04414337873458862
Batch 57/64 loss: -0.01979207992553711
Batch 58/64 loss: -0.000538170337677002
Batch 59/64 loss: -0.00556260347366333
Batch 60/64 loss: -0.021179020404815674
Batch 61/64 loss: -0.008671760559082031
Batch 62/64 loss: -0.00580364465713501
Batch 63/64 loss: 0.012970328330993652
Batch 64/64 loss: 0.004662752151489258
Epoch 153  Train loss: -0.02305909699084712  Val loss: 0.06368308337693362
Epoch 154
-------------------------------
Batch 1/64 loss: -0.05305856466293335
Batch 2/64 loss: -0.026722490787506104
Batch 3/64 loss: -0.008943796157836914
Batch 4/64 loss: -0.015256822109222412
Batch 5/64 loss: -0.01491326093673706
Batch 6/64 loss: -0.04903346300125122
Batch 7/64 loss: -0.03913521766662598
Batch 8/64 loss: -0.008096873760223389
Batch 9/64 loss: -0.018771350383758545
Batch 10/64 loss: -0.04259747266769409
Batch 11/64 loss: -0.021550893783569336
Batch 12/64 loss: -0.0316733717918396
Batch 13/64 loss: -0.05230569839477539
Batch 14/64 loss: -0.01686382293701172
Batch 15/64 loss: -0.046447157859802246
Batch 16/64 loss: -0.025771379470825195
Batch 17/64 loss: 0.012488126754760742
Batch 18/64 loss: -0.026683330535888672
Batch 19/64 loss: -0.0020069479942321777
Batch 20/64 loss: -0.0012770891189575195
Batch 21/64 loss: -0.02249729633331299
Batch 22/64 loss: -0.03385293483734131
Batch 23/64 loss: -0.030314326286315918
Batch 24/64 loss: -0.06844037771224976
Batch 25/64 loss: -0.00701218843460083
Batch 26/64 loss: -0.0020133256912231445
Batch 27/64 loss: -0.023335576057434082
Batch 28/64 loss: -0.03454512357711792
Batch 29/64 loss: -0.007124066352844238
Batch 30/64 loss: -0.02332282066345215
Batch 31/64 loss: -0.026059865951538086
Batch 32/64 loss: -0.010504961013793945
Batch 33/64 loss: -0.02181762456893921
Batch 34/64 loss: -0.05023771524429321
Batch 35/64 loss: 0.006601512432098389
Batch 36/64 loss: -0.03158378601074219
Batch 37/64 loss: -0.036837100982666016
Batch 38/64 loss: -0.02366805076599121
Batch 39/64 loss: -0.011874854564666748
Batch 40/64 loss: -0.008428215980529785
Batch 41/64 loss: -0.03801620006561279
Batch 42/64 loss: -0.03679990768432617
Batch 43/64 loss: -0.011277496814727783
Batch 44/64 loss: -0.03362172842025757
Batch 45/64 loss: -0.04638230800628662
Batch 46/64 loss: -0.02243119478225708
Batch 47/64 loss: -0.022766947746276855
Batch 48/64 loss: -0.048927128314971924
Batch 49/64 loss: 0.006501913070678711
Batch 50/64 loss: -0.02076280117034912
Batch 51/64 loss: -0.031018435955047607
Batch 52/64 loss: -0.03987836837768555
Batch 53/64 loss: -0.030178070068359375
Batch 54/64 loss: -0.017139017581939697
Batch 55/64 loss: -0.018357396125793457
Batch 56/64 loss: -0.023294925689697266
Batch 57/64 loss: -0.0036740899085998535
Batch 58/64 loss: -0.042166948318481445
Batch 59/64 loss: -0.02701956033706665
Batch 60/64 loss: -0.05208289623260498
Batch 61/64 loss: -0.03898119926452637
Batch 62/64 loss: -0.02385944128036499
Batch 63/64 loss: -0.008150160312652588
Batch 64/64 loss: -0.02602618932723999
Epoch 154  Train loss: -0.02518107493718465  Val loss: 0.07141844021905329
Epoch 155
-------------------------------
Batch 1/64 loss: -0.025306880474090576
Batch 2/64 loss: -0.01869809627532959
Batch 3/64 loss: -0.05145949125289917
Batch 4/64 loss: -0.03931403160095215
Batch 5/64 loss: -0.06386071443557739
Batch 6/64 loss: -0.008943796157836914
Batch 7/64 loss: 0.006782650947570801
Batch 8/64 loss: -0.04345440864562988
Batch 9/64 loss: 0.02940535545349121
Batch 10/64 loss: -0.010367989540100098
Batch 11/64 loss: -0.034122467041015625
Batch 12/64 loss: -0.03536355495452881
Batch 13/64 loss: -0.02505171298980713
Batch 14/64 loss: -0.023739516735076904
Batch 15/64 loss: -0.029551267623901367
Batch 16/64 loss: -0.030394315719604492
Batch 17/64 loss: 0.0024130940437316895
Batch 18/64 loss: -0.019005656242370605
Batch 19/64 loss: -0.04819720983505249
Batch 20/64 loss: -0.04735255241394043
Batch 21/64 loss: -0.02257668972015381
Batch 22/64 loss: -0.02025848627090454
Batch 23/64 loss: -0.03739023208618164
Batch 24/64 loss: -0.031097114086151123
Batch 25/64 loss: 0.007799506187438965
Batch 26/64 loss: -0.022565066814422607
Batch 27/64 loss: -0.01044762134552002
Batch 28/64 loss: -0.040643513202667236
Batch 29/64 loss: -0.0319516658782959
Batch 30/64 loss: -0.04798978567123413
Batch 31/64 loss: -0.011341452598571777
Batch 32/64 loss: -0.02846205234527588
Batch 33/64 loss: -0.03592073917388916
Batch 34/64 loss: -0.00885552167892456
Batch 35/64 loss: -0.025139689445495605
Batch 36/64 loss: -0.004709005355834961
Batch 37/64 loss: -0.02331852912902832
Batch 38/64 loss: -0.012744307518005371
Batch 39/64 loss: -0.02705204486846924
Batch 40/64 loss: -0.030122220516204834
Batch 41/64 loss: -0.05155104398727417
Batch 42/64 loss: -0.038769662380218506
Batch 43/64 loss: -0.04161173105239868
Batch 44/64 loss: -0.03947293758392334
Batch 45/64 loss: -0.02858680486679077
Batch 46/64 loss: 0.0042389631271362305
Batch 47/64 loss: -0.027866840362548828
Batch 48/64 loss: -0.01350092887878418
Batch 49/64 loss: -0.009031176567077637
Batch 50/64 loss: -0.028464913368225098
Batch 51/64 loss: -0.011961102485656738
Batch 52/64 loss: -0.02676182985305786
Batch 53/64 loss: -0.02344822883605957
Batch 54/64 loss: 0.004483461380004883
Batch 55/64 loss: -0.02094399929046631
Batch 56/64 loss: -0.0163571834564209
Batch 57/64 loss: -0.025364220142364502
Batch 58/64 loss: -0.010937273502349854
Batch 59/64 loss: -0.029947876930236816
Batch 60/64 loss: -0.0022399425506591797
Batch 61/64 loss: -0.05531555414199829
Batch 62/64 loss: -0.03056347370147705
Batch 63/64 loss: -0.023084402084350586
Batch 64/64 loss: -0.029617011547088623
Epoch 155  Train loss: -0.02430808240292119  Val loss: 0.05889968814718764
Epoch 156
-------------------------------
Batch 1/64 loss: -0.005370199680328369
Batch 2/64 loss: -0.04073667526245117
Batch 3/64 loss: -0.03313291072845459
Batch 4/64 loss: -0.014710664749145508
Batch 5/64 loss: -0.0401153564453125
Batch 6/64 loss: -0.033527672290802
Batch 7/64 loss: -0.031223177909851074
Batch 8/64 loss: -0.01240462064743042
Batch 9/64 loss: -0.001881420612335205
Batch 10/64 loss: -0.03783524036407471
Batch 11/64 loss: -0.02218317985534668
Batch 12/64 loss: -0.009197890758514404
Batch 13/64 loss: -0.04418224096298218
Batch 14/64 loss: -0.03409886360168457
Batch 15/64 loss: -0.022551000118255615
Batch 16/64 loss: -0.008925676345825195
Batch 17/64 loss: 0.0025597214698791504
Batch 18/64 loss: -0.020484328269958496
Batch 19/64 loss: -0.03758418560028076
Batch 20/64 loss: -0.03304779529571533
Batch 21/64 loss: -0.04160654544830322
Batch 22/64 loss: -0.047608256340026855
Batch 23/64 loss: -0.05825996398925781
Batch 24/64 loss: -0.04665267467498779
Batch 25/64 loss: -0.0008041262626647949
Batch 26/64 loss: -0.055596113204956055
Batch 27/64 loss: 0.016579508781433105
Batch 28/64 loss: -0.02181023359298706
Batch 29/64 loss: -0.0058016180992126465
Batch 30/64 loss: -0.040193796157836914
Batch 31/64 loss: 0.0019191503524780273
Batch 32/64 loss: 0.01289588212966919
Batch 33/64 loss: -0.031920433044433594
Batch 34/64 loss: -0.03778886795043945
Batch 35/64 loss: -0.017163455486297607
Batch 36/64 loss: -0.038160085678100586
Batch 37/64 loss: -0.026365041732788086
Batch 38/64 loss: -0.018740415573120117
Batch 39/64 loss: -0.03567099571228027
Batch 40/64 loss: -0.038066864013671875
Batch 41/64 loss: -0.00997394323348999
Batch 42/64 loss: -0.037357449531555176
Batch 43/64 loss: -0.03669828176498413
Batch 44/64 loss: -0.02105480432510376
Batch 45/64 loss: -0.028910160064697266
Batch 46/64 loss: -0.024772346019744873
Batch 47/64 loss: -0.04179072380065918
Batch 48/64 loss: -0.05149376392364502
Batch 49/64 loss: -0.03702002763748169
Batch 50/64 loss: -0.03359031677246094
Batch 51/64 loss: -0.02779287099838257
Batch 52/64 loss: -0.04835331439971924
Batch 53/64 loss: -0.03161567449569702
Batch 54/64 loss: -0.03639805316925049
Batch 55/64 loss: -0.016101419925689697
Batch 56/64 loss: -0.023583948612213135
Batch 57/64 loss: -0.01867198944091797
Batch 58/64 loss: -0.03286278247833252
Batch 59/64 loss: -0.014771342277526855
Batch 60/64 loss: -0.011831283569335938
Batch 61/64 loss: -0.04828256368637085
Batch 62/64 loss: -0.008329212665557861
Batch 63/64 loss: -0.01711130142211914
Batch 64/64 loss: -0.020893514156341553
Epoch 156  Train loss: -0.026438996604844636  Val loss: 0.06341924761578799
Epoch 157
-------------------------------
Batch 1/64 loss: -0.024031996726989746
Batch 2/64 loss: -0.05101567506790161
Batch 3/64 loss: -0.030735373497009277
Batch 4/64 loss: -0.033215105533599854
Batch 5/64 loss: -0.04383742809295654
Batch 6/64 loss: -0.0015423297882080078
Batch 7/64 loss: -0.032329678535461426
Batch 8/64 loss: -0.034342944622039795
Batch 9/64 loss: -0.0220491886138916
Batch 10/64 loss: -0.022815585136413574
Batch 11/64 loss: -0.01163870096206665
Batch 12/64 loss: -0.00038051605224609375
Batch 13/64 loss: -0.0028197169303894043
Batch 14/64 loss: -0.04592007398605347
Batch 15/64 loss: -0.021205544471740723
Batch 16/64 loss: -0.015117168426513672
Batch 17/64 loss: -0.010775089263916016
Batch 18/64 loss: -0.04209744930267334
Batch 19/64 loss: -0.024562954902648926
Batch 20/64 loss: -0.033943772315979004
Batch 21/64 loss: -0.004407286643981934
Batch 22/64 loss: -0.036827921867370605
Batch 23/64 loss: -0.020194172859191895
Batch 24/64 loss: -0.011630058288574219
Batch 25/64 loss: -0.031007707118988037
Batch 26/64 loss: -0.01816737651824951
Batch 27/64 loss: -0.007278263568878174
Batch 28/64 loss: -0.015726745128631592
Batch 29/64 loss: -0.009118795394897461
Batch 30/64 loss: -0.020885348320007324
Batch 31/64 loss: -0.021684706211090088
Batch 32/64 loss: -0.03900027275085449
Batch 33/64 loss: -0.036548733711242676
Batch 34/64 loss: -0.04661059379577637
Batch 35/64 loss: -0.023273706436157227
Batch 36/64 loss: -0.025792956352233887
Batch 37/64 loss: -0.04241818189620972
Batch 38/64 loss: -0.012019455432891846
Batch 39/64 loss: -0.03740870952606201
Batch 40/64 loss: -0.026729226112365723
Batch 41/64 loss: -0.036457955837249756
Batch 42/64 loss: -0.026262283325195312
Batch 43/64 loss: -0.013934016227722168
Batch 44/64 loss: -0.024767398834228516
Batch 45/64 loss: -0.02951723337173462
Batch 46/64 loss: -0.023509740829467773
Batch 47/64 loss: -0.02784174680709839
Batch 48/64 loss: -0.03984677791595459
Batch 49/64 loss: -0.032160282135009766
Batch 50/64 loss: -0.01930481195449829
Batch 51/64 loss: -0.04365730285644531
Batch 52/64 loss: -0.011298000812530518
Batch 53/64 loss: -0.04200863838195801
Batch 54/64 loss: -0.017241239547729492
Batch 55/64 loss: -0.0014666318893432617
Batch 56/64 loss: -0.012142062187194824
Batch 57/64 loss: -0.009835302829742432
Batch 58/64 loss: -0.009766876697540283
Batch 59/64 loss: -0.026975631713867188
Batch 60/64 loss: -0.0252377986907959
Batch 61/64 loss: -0.028225302696228027
Batch 62/64 loss: -0.012747645378112793
Batch 63/64 loss: -0.042142391204833984
Batch 64/64 loss: -0.022549927234649658
Epoch 157  Train loss: -0.024539074944514853  Val loss: 0.06441460094091409
Epoch 158
-------------------------------
Batch 1/64 loss: -0.020403802394866943
Batch 2/64 loss: -0.04745984077453613
Batch 3/64 loss: -0.01929086446762085
Batch 4/64 loss: -0.029680252075195312
Batch 5/64 loss: -0.052301228046417236
Batch 6/64 loss: -0.02178865671157837
Batch 7/64 loss: -0.012039899826049805
Batch 8/64 loss: -0.03423994779586792
Batch 9/64 loss: -0.0028256773948669434
Batch 10/64 loss: -0.014776349067687988
Batch 11/64 loss: -0.023658394813537598
Batch 12/64 loss: -0.037122368812561035
Batch 13/64 loss: -0.05150043964385986
Batch 14/64 loss: -0.007477760314941406
Batch 15/64 loss: -0.02654176950454712
Batch 16/64 loss: -0.01024329662322998
Batch 17/64 loss: -0.010479569435119629
Batch 18/64 loss: -0.04404228925704956
Batch 19/64 loss: -0.052971601486206055
Batch 20/64 loss: -0.007757067680358887
Batch 21/64 loss: 0.0055245161056518555
Batch 22/64 loss: -0.011814594268798828
Batch 23/64 loss: -0.0417860746383667
Batch 24/64 loss: -0.04221034049987793
Batch 25/64 loss: -0.04746401309967041
Batch 26/64 loss: -0.03127098083496094
Batch 27/64 loss: -0.047048211097717285
Batch 28/64 loss: -0.03842449188232422
Batch 29/64 loss: -0.04493075609207153
Batch 30/64 loss: -0.0417027473449707
Batch 31/64 loss: -0.02324730157852173
Batch 32/64 loss: -0.051857590675354004
Batch 33/64 loss: -0.022005438804626465
Batch 34/64 loss: -0.022590339183807373
Batch 35/64 loss: -0.028538167476654053
Batch 36/64 loss: -0.020537734031677246
Batch 37/64 loss: -0.02063685655593872
Batch 38/64 loss: -0.04607957601547241
Batch 39/64 loss: -0.03949713706970215
Batch 40/64 loss: 0.015803873538970947
Batch 41/64 loss: -0.048620760440826416
Batch 42/64 loss: -0.02456223964691162
Batch 43/64 loss: 0.013131201267242432
Batch 44/64 loss: -0.003316223621368408
Batch 45/64 loss: -0.010311305522918701
Batch 46/64 loss: -0.008537769317626953
Batch 47/64 loss: -0.03525125980377197
Batch 48/64 loss: -0.01537632942199707
Batch 49/64 loss: -0.02815002202987671
Batch 50/64 loss: 0.03195929527282715
Batch 51/64 loss: -0.03371429443359375
Batch 52/64 loss: 0.005593419075012207
Batch 53/64 loss: -0.04128098487854004
Batch 54/64 loss: -0.022690176963806152
Batch 55/64 loss: -0.06638199090957642
Batch 56/64 loss: -0.022643566131591797
Batch 57/64 loss: -0.04250490665435791
Batch 58/64 loss: -0.0378342866897583
Batch 59/64 loss: -0.019672393798828125
Batch 60/64 loss: -0.01533585786819458
Batch 61/64 loss: -0.03873473405838013
Batch 62/64 loss: -0.01137077808380127
Batch 63/64 loss: -0.03199821710586548
Batch 64/64 loss: -0.02306431531906128
Epoch 158  Train loss: -0.025910862053141876  Val loss: 0.06013785358966421
Epoch 159
-------------------------------
Batch 1/64 loss: -0.040337443351745605
Batch 2/64 loss: -0.016584396362304688
Batch 3/64 loss: -0.03308868408203125
Batch 4/64 loss: -0.04141724109649658
Batch 5/64 loss: 0.040510356426239014
Batch 6/64 loss: -0.022873640060424805
Batch 7/64 loss: -0.04458802938461304
Batch 8/64 loss: 0.0014780163764953613
Batch 9/64 loss: -0.027143001556396484
Batch 10/64 loss: -0.03851652145385742
Batch 11/64 loss: -0.04524827003479004
Batch 12/64 loss: -0.038797974586486816
Batch 13/64 loss: -0.04358261823654175
Batch 14/64 loss: -0.017736613750457764
Batch 15/64 loss: -0.026755094528198242
Batch 16/64 loss: -0.04208493232727051
Batch 17/64 loss: -0.021636486053466797
Batch 18/64 loss: -0.003842473030090332
Batch 19/64 loss: -0.0327143669128418
Batch 20/64 loss: -0.03357541561126709
Batch 21/64 loss: -0.014090955257415771
Batch 22/64 loss: -0.02812260389328003
Batch 23/64 loss: -0.04819297790527344
Batch 24/64 loss: -0.04668456315994263
Batch 25/64 loss: -0.031833529472351074
Batch 26/64 loss: -0.004565238952636719
Batch 27/64 loss: -0.053151488304138184
Batch 28/64 loss: -0.03980278968811035
Batch 29/64 loss: -0.01438671350479126
Batch 30/64 loss: -0.02178370952606201
Batch 31/64 loss: -0.034061312675476074
Batch 32/64 loss: -0.01342862844467163
Batch 33/64 loss: -0.01990342140197754
Batch 34/64 loss: -0.05619889497756958
Batch 35/64 loss: -0.03761929273605347
Batch 36/64 loss: -0.028446197509765625
Batch 37/64 loss: -0.008351564407348633
Batch 38/64 loss: -0.03559613227844238
Batch 39/64 loss: 0.0016470551490783691
Batch 40/64 loss: -0.008476018905639648
Batch 41/64 loss: -0.038072407245635986
Batch 42/64 loss: -0.01333993673324585
Batch 43/64 loss: -0.024916410446166992
Batch 44/64 loss: -0.021637141704559326
Batch 45/64 loss: -0.046178579330444336
Batch 46/64 loss: 0.021774590015411377
Batch 47/64 loss: -0.049515485763549805
Batch 48/64 loss: -0.033696115016937256
Batch 49/64 loss: -0.04573523998260498
Batch 50/64 loss: -0.0050234198570251465
Batch 51/64 loss: -0.026346981525421143
Batch 52/64 loss: -0.04024612903594971
Batch 53/64 loss: -0.025956392288208008
Batch 54/64 loss: -0.05582106113433838
Batch 55/64 loss: -0.04236048460006714
Batch 56/64 loss: -0.04602968692779541
Batch 57/64 loss: -0.044282376766204834
Batch 58/64 loss: -0.019584715366363525
Batch 59/64 loss: -0.03096747398376465
Batch 60/64 loss: 0.006289660930633545
Batch 61/64 loss: 0.0029390454292297363
Batch 62/64 loss: -0.02875804901123047
Batch 63/64 loss: -0.012070417404174805
Batch 64/64 loss: -0.022875666618347168
Epoch 159  Train loss: -0.026796482591068045  Val loss: 0.05681477133760747
Epoch 160
-------------------------------
Batch 1/64 loss: -0.036512136459350586
Batch 2/64 loss: -0.02862548828125
Batch 3/64 loss: -0.06088268756866455
Batch 4/64 loss: -0.020219922065734863
Batch 5/64 loss: -0.009738802909851074
Batch 6/64 loss: -0.030953526496887207
Batch 7/64 loss: -0.036841511726379395
Batch 8/64 loss: -0.0051316022872924805
Batch 9/64 loss: 0.0032966136932373047
Batch 10/64 loss: -0.032007575035095215
Batch 11/64 loss: 0.002865433692932129
Batch 12/64 loss: -0.039917588233947754
Batch 13/64 loss: -0.002824068069458008
Batch 14/64 loss: -0.012356042861938477
Batch 15/64 loss: -0.011577486991882324
Batch 16/64 loss: -0.05628448724746704
Batch 17/64 loss: 6.604194641113281e-05
Batch 18/64 loss: -0.020909428596496582
Batch 19/64 loss: -0.005358636379241943
Batch 20/64 loss: -0.04441004991531372
Batch 21/64 loss: -0.009814798831939697
Batch 22/64 loss: -0.04870748519897461
Batch 23/64 loss: -0.03482753038406372
Batch 24/64 loss: -0.044763386249542236
Batch 25/64 loss: -0.01354837417602539
Batch 26/64 loss: -0.022109389305114746
Batch 27/64 loss: -0.0394933819770813
Batch 28/64 loss: -0.049300551414489746
Batch 29/64 loss: -0.052210330963134766
Batch 30/64 loss: -0.04202026128768921
Batch 31/64 loss: -0.0454670786857605
Batch 32/64 loss: -0.007598519325256348
Batch 33/64 loss: -0.038957834243774414
Batch 34/64 loss: -0.01174086332321167
Batch 35/64 loss: -0.02079707384109497
Batch 36/64 loss: -0.02256321907043457
Batch 37/64 loss: -0.04621011018753052
Batch 38/64 loss: -0.02055668830871582
Batch 39/64 loss: -0.024221181869506836
Batch 40/64 loss: -0.04900538921356201
Batch 41/64 loss: -0.04699820280075073
Batch 42/64 loss: -0.01869809627532959
Batch 43/64 loss: -0.034795403480529785
Batch 44/64 loss: -0.03203427791595459
Batch 45/64 loss: -0.03939378261566162
Batch 46/64 loss: -0.04335832595825195
Batch 47/64 loss: -0.04401677846908569
Batch 48/64 loss: -0.032140493392944336
Batch 49/64 loss: -0.014706909656524658
Batch 50/64 loss: 0.0004957318305969238
Batch 51/64 loss: -0.03179746866226196
Batch 52/64 loss: -0.04295259714126587
Batch 53/64 loss: -0.04195290803909302
Batch 54/64 loss: -0.032697856426239014
Batch 55/64 loss: -0.01319974660873413
Batch 56/64 loss: -0.01805490255355835
Batch 57/64 loss: -0.062303781509399414
Batch 58/64 loss: -0.025741219520568848
Batch 59/64 loss: -0.03699570894241333
Batch 60/64 loss: -0.03159534931182861
Batch 61/64 loss: -0.0352015495300293
Batch 62/64 loss: -0.020236730575561523
Batch 63/64 loss: -0.018147051334381104
Batch 64/64 loss: -0.034719645977020264
Epoch 160  Train loss: -0.028781169302323287  Val loss: 0.07566737944317847
Epoch 161
-------------------------------
Batch 1/64 loss: -0.04788237810134888
Batch 2/64 loss: -0.01886838674545288
Batch 3/64 loss: -0.041676342487335205
Batch 4/64 loss: -0.04874718189239502
Batch 5/64 loss: -0.044229984283447266
Batch 6/64 loss: -0.029143214225769043
Batch 7/64 loss: -0.029771149158477783
Batch 8/64 loss: -0.005700945854187012
Batch 9/64 loss: -0.05410468578338623
Batch 10/64 loss: -0.04535180330276489
Batch 11/64 loss: -0.047464847564697266
Batch 12/64 loss: -0.03198784589767456
Batch 13/64 loss: -0.04422307014465332
Batch 14/64 loss: -0.04896199703216553
Batch 15/64 loss: -0.05327659845352173
Batch 16/64 loss: -0.030609071254730225
Batch 17/64 loss: -0.03705871105194092
Batch 18/64 loss: -0.02323603630065918
Batch 19/64 loss: -0.03570103645324707
Batch 20/64 loss: -0.014988183975219727
Batch 21/64 loss: -0.018353044986724854
Batch 22/64 loss: -0.02909177541732788
Batch 23/64 loss: -0.036175668239593506
Batch 24/64 loss: -0.020898520946502686
Batch 25/64 loss: -0.01564878225326538
Batch 26/64 loss: -0.03870600461959839
Batch 27/64 loss: -0.0491788387298584
Batch 28/64 loss: -0.029854893684387207
Batch 29/64 loss: -0.0334894061088562
Batch 30/64 loss: -0.03340041637420654
Batch 31/64 loss: -0.02468627691268921
Batch 32/64 loss: -0.02442711591720581
Batch 33/64 loss: -0.031004011631011963
Batch 34/64 loss: -0.039840102195739746
Batch 35/64 loss: -0.030861496925354004
Batch 36/64 loss: -0.01843547821044922
Batch 37/64 loss: -0.0315777063369751
Batch 38/64 loss: -0.035988569259643555
Batch 39/64 loss: -0.03400617837905884
Batch 40/64 loss: -0.020202577114105225
Batch 41/64 loss: -0.01452106237411499
Batch 42/64 loss: -0.037976980209350586
Batch 43/64 loss: -0.04482811689376831
Batch 44/64 loss: 0.003950536251068115
Batch 45/64 loss: -0.018853366374969482
Batch 46/64 loss: -0.03227585554122925
Batch 47/64 loss: -0.008456766605377197
Batch 48/64 loss: -0.014015138149261475
Batch 49/64 loss: -0.04409092664718628
Batch 50/64 loss: -0.005513787269592285
Batch 51/64 loss: -0.05263310670852661
Batch 52/64 loss: -0.03112727403640747
Batch 53/64 loss: 0.010909736156463623
Batch 54/64 loss: -0.015462398529052734
Batch 55/64 loss: -0.021425843238830566
Batch 56/64 loss: -0.026057183742523193
Batch 57/64 loss: -0.018383026123046875
Batch 58/64 loss: -0.05556631088256836
Batch 59/64 loss: 0.02103877067565918
Batch 60/64 loss: -0.05882549285888672
Batch 61/64 loss: -0.019608795642852783
Batch 62/64 loss: -0.011049091815948486
Batch 63/64 loss: -0.036364197731018066
Batch 64/64 loss: -0.028482437133789062
Epoch 161  Train loss: -0.02951071122113396  Val loss: 0.05580480606695221
Saving best model, epoch: 161
Epoch 162
-------------------------------
Batch 1/64 loss: -0.03530168533325195
Batch 2/64 loss: -0.03268158435821533
Batch 3/64 loss: -0.03835654258728027
Batch 4/64 loss: -0.02936553955078125
Batch 5/64 loss: -0.03747004270553589
Batch 6/64 loss: -0.038115620613098145
Batch 7/64 loss: -0.016777753829956055
Batch 8/64 loss: -0.021793782711029053
Batch 9/64 loss: -0.0039446353912353516
Batch 10/64 loss: -0.0501483678817749
Batch 11/64 loss: -0.006152033805847168
Batch 12/64 loss: -0.025794267654418945
Batch 13/64 loss: -0.0065915584564208984
Batch 14/64 loss: -0.014058589935302734
Batch 15/64 loss: -0.019970178604125977
Batch 16/64 loss: -0.050429344177246094
Batch 17/64 loss: -0.021198511123657227
Batch 18/64 loss: -0.007011592388153076
Batch 19/64 loss: -0.0025069117546081543
Batch 20/64 loss: -0.052143752574920654
Batch 21/64 loss: 0.02396094799041748
Batch 22/64 loss: -0.05191504955291748
Batch 23/64 loss: -0.030469536781311035
Batch 24/64 loss: -0.02902507781982422
Batch 25/64 loss: -0.047129273414611816
Batch 26/64 loss: -0.07095539569854736
Batch 27/64 loss: -0.03844153881072998
Batch 28/64 loss: -0.0518723726272583
Batch 29/64 loss: -0.02562689781188965
Batch 30/64 loss: -0.02345883846282959
Batch 31/64 loss: 0.025037705898284912
Batch 32/64 loss: -0.04915618896484375
Batch 33/64 loss: -0.023619532585144043
Batch 34/64 loss: -0.05084604024887085
Batch 35/64 loss: -0.009056925773620605
Batch 36/64 loss: -0.02541440725326538
Batch 37/64 loss: -0.0669780969619751
Batch 38/64 loss: 0.023294925689697266
Batch 39/64 loss: -0.020818889141082764
Batch 40/64 loss: -0.003845393657684326
Batch 41/64 loss: -0.03743851184844971
Batch 42/64 loss: -0.025197148323059082
Batch 43/64 loss: -0.04437363147735596
Batch 44/64 loss: -0.030437052249908447
Batch 45/64 loss: -0.05768084526062012
Batch 46/64 loss: -0.018523693084716797
Batch 47/64 loss: -0.0052918195724487305
Batch 48/64 loss: -0.049298763275146484
Batch 49/64 loss: 0.03830420970916748
Batch 50/64 loss: -0.04290562868118286
Batch 51/64 loss: -0.04147738218307495
Batch 52/64 loss: -0.049225449562072754
Batch 53/64 loss: -0.02656233310699463
Batch 54/64 loss: -0.018506109714508057
Batch 55/64 loss: -0.05419754981994629
Batch 56/64 loss: -0.034681499004364014
Batch 57/64 loss: -0.0335882306098938
Batch 58/64 loss: -0.045750975608825684
Batch 59/64 loss: -0.027644991874694824
Batch 60/64 loss: -0.04647183418273926
Batch 61/64 loss: -0.026071667671203613
Batch 62/64 loss: -0.030306339263916016
Batch 63/64 loss: -0.048277318477630615
Batch 64/64 loss: -0.021685123443603516
Epoch 162  Train loss: -0.028674769869037702  Val loss: 0.06481941622966753
Epoch 163
-------------------------------
Batch 1/64 loss: -0.03922301530838013
Batch 2/64 loss: -0.040234506130218506
Batch 3/64 loss: -0.01440340280532837
Batch 4/64 loss: -0.0284498929977417
Batch 5/64 loss: -0.04171270132064819
Batch 6/64 loss: -0.030354022979736328
Batch 7/64 loss: -0.041772544384002686
Batch 8/64 loss: -0.061060547828674316
Batch 9/64 loss: -0.00514453649520874
Batch 10/64 loss: -0.006655097007751465
Batch 11/64 loss: -0.035876452922821045
Batch 12/64 loss: -0.005360126495361328
Batch 13/64 loss: -0.015316605567932129
Batch 14/64 loss: -0.04835832118988037
Batch 15/64 loss: -0.04120206832885742
Batch 16/64 loss: -0.0757591724395752
Batch 17/64 loss: -0.00038188695907592773
Batch 18/64 loss: -0.007262229919433594
Batch 19/64 loss: -0.05440634489059448
Batch 20/64 loss: -0.03295242786407471
Batch 21/64 loss: -0.0254058837890625
Batch 22/64 loss: -0.03341251611709595
Batch 23/64 loss: -0.03268986940383911
Batch 24/64 loss: -0.03835064172744751
Batch 25/64 loss: -0.039452433586120605
Batch 26/64 loss: -0.030875027179718018
Batch 27/64 loss: -0.041442692279815674
Batch 28/64 loss: -0.05585348606109619
Batch 29/64 loss: -0.04330945014953613
Batch 30/64 loss: -0.030662894248962402
Batch 31/64 loss: -0.05304300785064697
Batch 32/64 loss: -0.011935949325561523
Batch 33/64 loss: -0.03222000598907471
Batch 34/64 loss: 0.003213226795196533
Batch 35/64 loss: -0.02554929256439209
Batch 36/64 loss: -0.042075157165527344
Batch 37/64 loss: -0.03732943534851074
Batch 38/64 loss: -0.01599562168121338
Batch 39/64 loss: -0.03210383653640747
Batch 40/64 loss: -0.0324321985244751
Batch 41/64 loss: -0.03642028570175171
Batch 42/64 loss: -0.036052584648132324
Batch 43/64 loss: -0.021140456199645996
Batch 44/64 loss: -0.05428594350814819
Batch 45/64 loss: -0.04992210865020752
Batch 46/64 loss: -0.03492456674575806
Batch 47/64 loss: -0.05233943462371826
Batch 48/64 loss: -0.029669225215911865
Batch 49/64 loss: -0.048888206481933594
Batch 50/64 loss: -0.011278629302978516
Batch 51/64 loss: -0.003397226333618164
Batch 52/64 loss: -0.0354158878326416
Batch 53/64 loss: -0.011968493461608887
Batch 54/64 loss: -0.02994006872177124
Batch 55/64 loss: -0.050630807876586914
Batch 56/64 loss: -0.021604418754577637
Batch 57/64 loss: -0.024458348751068115
Batch 58/64 loss: -0.03711611032485962
Batch 59/64 loss: -0.04305684566497803
Batch 60/64 loss: -0.04290956258773804
Batch 61/64 loss: -0.03768157958984375
Batch 62/64 loss: -0.04912519454956055
Batch 63/64 loss: -0.018891632556915283
Batch 64/64 loss: 0.0003936290740966797
Epoch 163  Train loss: -0.03227661918191349  Val loss: 0.06039457366228923
Epoch 164
-------------------------------
Batch 1/64 loss: -0.0328403115272522
Batch 2/64 loss: 0.009393453598022461
Batch 3/64 loss: -0.03142726421356201
Batch 4/64 loss: -0.025588035583496094
Batch 5/64 loss: -0.02350163459777832
Batch 6/64 loss: -0.04194772243499756
Batch 7/64 loss: -0.011279106140136719
Batch 8/64 loss: -0.0613093376159668
Batch 9/64 loss: -0.02942413091659546
Batch 10/64 loss: -0.03321772813796997
Batch 11/64 loss: -0.043773531913757324
Batch 12/64 loss: 0.00778353214263916
Batch 13/64 loss: -0.02523040771484375
Batch 14/64 loss: -0.02492225170135498
Batch 15/64 loss: -0.01690131425857544
Batch 16/64 loss: -0.026971399784088135
Batch 17/64 loss: 0.0018057823181152344
Batch 18/64 loss: -0.03924030065536499
Batch 19/64 loss: 0.0021858811378479004
Batch 20/64 loss: -0.04675126075744629
Batch 21/64 loss: -0.03187358379364014
Batch 22/64 loss: -0.0355604887008667
Batch 23/64 loss: -0.0441814661026001
Batch 24/64 loss: -0.04486525058746338
Batch 25/64 loss: -0.042114078998565674
Batch 26/64 loss: -0.040963590145111084
Batch 27/64 loss: -0.0409703254699707
Batch 28/64 loss: -0.05664640665054321
Batch 29/64 loss: -0.0239751935005188
Batch 30/64 loss: -0.03612786531448364
Batch 31/64 loss: -0.06728923320770264
Batch 32/64 loss: -0.04459100961685181
Batch 33/64 loss: -0.030528604984283447
Batch 34/64 loss: -0.015322744846343994
Batch 35/64 loss: -0.044058382511138916
Batch 36/64 loss: -0.032289206981658936
Batch 37/64 loss: -0.021209001541137695
Batch 38/64 loss: -0.03013026714324951
Batch 39/64 loss: -0.05755800008773804
Batch 40/64 loss: -0.03437691926956177
Batch 41/64 loss: -0.03783529996871948
Batch 42/64 loss: -0.04371678829193115
Batch 43/64 loss: -0.019166111946105957
Batch 44/64 loss: -0.05429810285568237
Batch 45/64 loss: -0.045412302017211914
Batch 46/64 loss: 0.025768578052520752
Batch 47/64 loss: -0.04393202066421509
Batch 48/64 loss: -0.02407097816467285
Batch 49/64 loss: -0.05530422925949097
Batch 50/64 loss: -0.024403691291809082
Batch 51/64 loss: -0.03252679109573364
Batch 52/64 loss: -0.03882408142089844
Batch 53/64 loss: -0.02399975061416626
Batch 54/64 loss: -0.008452415466308594
Batch 55/64 loss: -0.034018874168395996
Batch 56/64 loss: -0.037520408630371094
Batch 57/64 loss: -0.05766171216964722
Batch 58/64 loss: -0.020511627197265625
Batch 59/64 loss: -0.0016091465950012207
Batch 60/64 loss: -0.05713999271392822
Batch 61/64 loss: -0.03426271677017212
Batch 62/64 loss: -0.005961418151855469
Batch 63/64 loss: -0.030464589595794678
Batch 64/64 loss: -0.049538373947143555
Epoch 164  Train loss: -0.03153359936732872  Val loss: 0.07332982783465042
Epoch 165
-------------------------------
Batch 1/64 loss: -0.04090392589569092
Batch 2/64 loss: -0.007907509803771973
Batch 3/64 loss: -0.0030083656311035156
Batch 4/64 loss: -0.05058765411376953
Batch 5/64 loss: -0.03794187307357788
Batch 6/64 loss: -0.04669445753097534
Batch 7/64 loss: -0.04826509952545166
Batch 8/64 loss: -0.035186707973480225
Batch 9/64 loss: -0.049366295337677
Batch 10/64 loss: -0.03261286020278931
Batch 11/64 loss: -0.03345763683319092
Batch 12/64 loss: -0.037500739097595215
Batch 13/64 loss: -0.019660234451293945
Batch 14/64 loss: -0.03957545757293701
Batch 15/64 loss: -0.04213249683380127
Batch 16/64 loss: -0.02446281909942627
Batch 17/64 loss: 0.0037651658058166504
Batch 18/64 loss: -0.059304118156433105
Batch 19/64 loss: -0.03835904598236084
Batch 20/64 loss: -0.025951802730560303
Batch 21/64 loss: -0.03609567880630493
Batch 22/64 loss: -0.013187825679779053
Batch 23/64 loss: -0.045880675315856934
Batch 24/64 loss: 0.0005342960357666016
Batch 25/64 loss: -0.024453938007354736
Batch 26/64 loss: -0.03674435615539551
Batch 27/64 loss: -0.030092597007751465
Batch 28/64 loss: 0.0043680667877197266
Batch 29/64 loss: -0.042565107345581055
Batch 30/64 loss: -0.01786649227142334
Batch 31/64 loss: -0.013831675052642822
Batch 32/64 loss: -0.012440860271453857
Batch 33/64 loss: -0.030139625072479248
Batch 34/64 loss: -0.046711623668670654
Batch 35/64 loss: -0.02349454164505005
Batch 36/64 loss: -0.04290437698364258
Batch 37/64 loss: -0.05128169059753418
Batch 38/64 loss: -0.02766430377960205
Batch 39/64 loss: -0.020835816860198975
Batch 40/64 loss: -0.02734506130218506
Batch 41/64 loss: -0.025371253490447998
Batch 42/64 loss: -0.02668905258178711
Batch 43/64 loss: -0.027961313724517822
Batch 44/64 loss: -0.013269305229187012
Batch 45/64 loss: -0.012743711471557617
Batch 46/64 loss: -0.005433559417724609
Batch 47/64 loss: -0.03398609161376953
Batch 48/64 loss: -0.00822216272354126
Batch 49/64 loss: -0.03811115026473999
Batch 50/64 loss: -0.012708067893981934
Batch 51/64 loss: -0.04636788368225098
Batch 52/64 loss: -0.031006455421447754
Batch 53/64 loss: -0.040774405002593994
Batch 54/64 loss: -0.051530539989471436
Batch 55/64 loss: -0.049164533615112305
Batch 56/64 loss: -0.04381757974624634
Batch 57/64 loss: -0.03086775541305542
Batch 58/64 loss: -0.054158926010131836
Batch 59/64 loss: -0.01891648769378662
Batch 60/64 loss: -0.010562539100646973
Batch 61/64 loss: -0.04631459712982178
Batch 62/64 loss: -0.03785580396652222
Batch 63/64 loss: -0.042867302894592285
Batch 64/64 loss: -0.043666064739227295
Epoch 165  Train loss: -0.030575652449738745  Val loss: 0.057425014341819736
Epoch 166
-------------------------------
Batch 1/64 loss: -0.027578413486480713
Batch 2/64 loss: -0.033131182193756104
Batch 3/64 loss: -0.009868860244750977
Batch 4/64 loss: -0.03505599498748779
Batch 5/64 loss: -0.046959757804870605
Batch 6/64 loss: -0.04560929536819458
Batch 7/64 loss: -0.033782362937927246
Batch 8/64 loss: -0.027125775814056396
Batch 9/64 loss: -0.028809189796447754
Batch 10/64 loss: -0.04407036304473877
Batch 11/64 loss: -0.030688345432281494
Batch 12/64 loss: -0.04419916868209839
Batch 13/64 loss: -0.036473214626312256
Batch 14/64 loss: -0.043002188205718994
Batch 15/64 loss: -0.0465126633644104
Batch 16/64 loss: -0.02075296640396118
Batch 17/64 loss: -0.04240894317626953
Batch 18/64 loss: -0.055550217628479004
Batch 19/64 loss: -0.020711183547973633
Batch 20/64 loss: -0.013446629047393799
Batch 21/64 loss: -0.05156588554382324
Batch 22/64 loss: -0.0120924711227417
Batch 23/64 loss: -0.021376073360443115
Batch 24/64 loss: -0.03594779968261719
Batch 25/64 loss: -0.0392833948135376
Batch 26/64 loss: -0.04913824796676636
Batch 27/64 loss: -0.028272151947021484
Batch 28/64 loss: -0.0474892258644104
Batch 29/64 loss: -0.031970083713531494
Batch 30/64 loss: -0.010159015655517578
Batch 31/64 loss: -0.039382100105285645
Batch 32/64 loss: -0.025675415992736816
Batch 33/64 loss: -0.05462229251861572
Batch 34/64 loss: -0.05269813537597656
Batch 35/64 loss: -0.048241496086120605
Batch 36/64 loss: -0.001100301742553711
Batch 37/64 loss: -0.010271728038787842
Batch 38/64 loss: -0.01853048801422119
Batch 39/64 loss: -0.01710224151611328
Batch 40/64 loss: -0.03944581747055054
Batch 41/64 loss: -0.03505659103393555
Batch 42/64 loss: -0.007328152656555176
Batch 43/64 loss: -0.03428983688354492
Batch 44/64 loss: -0.03670001029968262
Batch 45/64 loss: -0.03198552131652832
Batch 46/64 loss: -0.0569993257522583
Batch 47/64 loss: -0.03425121307373047
Batch 48/64 loss: -0.03703927993774414
Batch 49/64 loss: -0.05881458520889282
Batch 50/64 loss: -0.02702033519744873
Batch 51/64 loss: -0.022762060165405273
Batch 52/64 loss: -0.024737119674682617
Batch 53/64 loss: -0.04829072952270508
Batch 54/64 loss: -0.040909767150878906
Batch 55/64 loss: -0.03972548246383667
Batch 56/64 loss: -0.034817397594451904
Batch 57/64 loss: -0.03204244375228882
Batch 58/64 loss: -0.04728597402572632
Batch 59/64 loss: -0.04212278127670288
Batch 60/64 loss: -0.024250388145446777
Batch 61/64 loss: -0.005239903926849365
Batch 62/64 loss: -0.04658275842666626
Batch 63/64 loss: -0.01811903715133667
Batch 64/64 loss: -0.04079717397689819
Epoch 166  Train loss: -0.03352266129325418  Val loss: 0.05525841233656578
Saving best model, epoch: 166
Epoch 167
-------------------------------
Batch 1/64 loss: -0.047967493534088135
Batch 2/64 loss: -0.05635237693786621
Batch 3/64 loss: -0.04305344820022583
Batch 4/64 loss: -0.055958449840545654
Batch 5/64 loss: -0.051125526428222656
Batch 6/64 loss: -0.04283362627029419
Batch 7/64 loss: -0.04464876651763916
Batch 8/64 loss: -0.034331321716308594
Batch 9/64 loss: -0.041295409202575684
Batch 10/64 loss: -0.048554301261901855
Batch 11/64 loss: -0.03324168920516968
Batch 12/64 loss: -0.0499110221862793
Batch 13/64 loss: -0.03375983238220215
Batch 14/64 loss: -0.031098783016204834
Batch 15/64 loss: -0.03962206840515137
Batch 16/64 loss: -0.04409605264663696
Batch 17/64 loss: -0.04407024383544922
Batch 18/64 loss: -0.019013702869415283
Batch 19/64 loss: -0.030006706714630127
Batch 20/64 loss: -0.020029425621032715
Batch 21/64 loss: -0.015031218528747559
Batch 22/64 loss: -0.04819941520690918
Batch 23/64 loss: -0.05599099397659302
Batch 24/64 loss: -0.004513144493103027
Batch 25/64 loss: -0.06976693868637085
Batch 26/64 loss: -0.022646546363830566
Batch 27/64 loss: -0.053724706172943115
Batch 28/64 loss: -0.039781808853149414
Batch 29/64 loss: -0.036161959171295166
Batch 30/64 loss: -0.006072819232940674
Batch 31/64 loss: -0.04198342561721802
Batch 32/64 loss: -0.034203529357910156
Batch 33/64 loss: -0.03246748447418213
Batch 34/64 loss: -0.02246326208114624
Batch 35/64 loss: -0.052884697914123535
Batch 36/64 loss: -0.027872025966644287
Batch 37/64 loss: -0.0037568211555480957
Batch 38/64 loss: -0.035280704498291016
Batch 39/64 loss: -0.017650067806243896
Batch 40/64 loss: -0.04280203580856323
Batch 41/64 loss: -0.03768765926361084
Batch 42/64 loss: -0.024325788021087646
Batch 43/64 loss: -0.02347666025161743
Batch 44/64 loss: -0.03381723165512085
Batch 45/64 loss: -0.021858274936676025
Batch 46/64 loss: -0.039245426654815674
Batch 47/64 loss: -0.036581337451934814
Batch 48/64 loss: -0.03331923484802246
Batch 49/64 loss: -0.04916083812713623
Batch 50/64 loss: -0.05204540491104126
Batch 51/64 loss: -0.039256155490875244
Batch 52/64 loss: -0.03219914436340332
Batch 53/64 loss: -0.02349144220352173
Batch 54/64 loss: -0.038979530334472656
Batch 55/64 loss: -0.031940340995788574
Batch 56/64 loss: -0.01577591896057129
Batch 57/64 loss: -0.04431462287902832
Batch 58/64 loss: -0.023181438446044922
Batch 59/64 loss: -0.021367251873016357
Batch 60/64 loss: -0.028445661067962646
Batch 61/64 loss: -0.009413957595825195
Batch 62/64 loss: -0.04037189483642578
Batch 63/64 loss: -0.013400554656982422
Batch 64/64 loss: -0.031054675579071045
Epoch 167  Train loss: -0.034685029469284354  Val loss: 0.05752384375870433
Epoch 168
-------------------------------
Batch 1/64 loss: -0.04935169219970703
Batch 2/64 loss: -0.040220558643341064
Batch 3/64 loss: -0.058193087577819824
Batch 4/64 loss: -0.04752546548843384
Batch 5/64 loss: -0.013592123985290527
Batch 6/64 loss: -0.047109007835388184
Batch 7/64 loss: -0.022055447101593018
Batch 8/64 loss: 0.0034328699111938477
Batch 9/64 loss: -0.02275991439819336
Batch 10/64 loss: -0.045907020568847656
Batch 11/64 loss: -0.04103583097457886
Batch 12/64 loss: -0.01695317029953003
Batch 13/64 loss: -0.047529518604278564
Batch 14/64 loss: -0.044523000717163086
Batch 15/64 loss: -0.031372249126434326
Batch 16/64 loss: 0.003078639507293701
Batch 17/64 loss: -0.05232119560241699
Batch 18/64 loss: -0.05473971366882324
Batch 19/64 loss: -0.013800382614135742
Batch 20/64 loss: -0.045757412910461426
Batch 21/64 loss: -0.03732830286026001
Batch 22/64 loss: -0.023111283779144287
Batch 23/64 loss: -0.06927591562271118
Batch 24/64 loss: -0.008369982242584229
Batch 25/64 loss: -0.017444074153900146
Batch 26/64 loss: -0.0369112491607666
Batch 27/64 loss: -0.009247422218322754
Batch 28/64 loss: -0.023647427558898926
Batch 29/64 loss: 0.009550988674163818
Batch 30/64 loss: -0.04155731201171875
Batch 31/64 loss: -0.05305302143096924
Batch 32/64 loss: -0.01937270164489746
Batch 33/64 loss: -0.040820538997650146
Batch 34/64 loss: 7.450580596923828e-05
Batch 35/64 loss: -0.02695220708847046
Batch 36/64 loss: -0.02716231346130371
Batch 37/64 loss: -0.03858917951583862
Batch 38/64 loss: -0.04497867822647095
Batch 39/64 loss: -0.010143518447875977
Batch 40/64 loss: -0.044646263122558594
Batch 41/64 loss: -0.05303919315338135
Batch 42/64 loss: -0.030346035957336426
Batch 43/64 loss: -0.04908031225204468
Batch 44/64 loss: -0.018126606941223145
Batch 45/64 loss: -0.04878640174865723
Batch 46/64 loss: -0.03585106134414673
Batch 47/64 loss: -0.02680426836013794
Batch 48/64 loss: -0.05145782232284546
Batch 49/64 loss: -0.027505218982696533
Batch 50/64 loss: -0.07335048913955688
Batch 51/64 loss: -0.025982141494750977
Batch 52/64 loss: -0.04824787378311157
Batch 53/64 loss: -0.033289551734924316
Batch 54/64 loss: -0.04142463207244873
Batch 55/64 loss: -0.011323809623718262
Batch 56/64 loss: -0.027144014835357666
Batch 57/64 loss: -0.034373462200164795
Batch 58/64 loss: -0.015544116497039795
Batch 59/64 loss: -0.04251277446746826
Batch 60/64 loss: -0.032373785972595215
Batch 61/64 loss: -0.030717015266418457
Batch 62/64 loss: -0.010782480239868164
Batch 63/64 loss: -0.01538991928100586
Batch 64/64 loss: -0.03825312852859497
Epoch 168  Train loss: -0.03236649433771769  Val loss: 0.07712349920338372
Epoch 169
-------------------------------
Batch 1/64 loss: 0.003584623336791992
Batch 2/64 loss: 0.005610048770904541
Batch 3/64 loss: -0.038440167903900146
Batch 4/64 loss: -0.053726911544799805
Batch 5/64 loss: -0.04483097791671753
Batch 6/64 loss: -0.005636870861053467
Batch 7/64 loss: -0.06000781059265137
Batch 8/64 loss: -0.041113078594207764
Batch 9/64 loss: -0.030172228813171387
Batch 10/64 loss: -0.04302334785461426
Batch 11/64 loss: -0.013365626335144043
Batch 12/64 loss: -0.01435542106628418
Batch 13/64 loss: -0.009157717227935791
Batch 14/64 loss: -0.05162006616592407
Batch 15/64 loss: -0.043483614921569824
Batch 16/64 loss: -0.044865429401397705
Batch 17/64 loss: -0.05635988712310791
Batch 18/64 loss: -0.013155996799468994
Batch 19/64 loss: -0.05183321237564087
Batch 20/64 loss: -0.03594207763671875
Batch 21/64 loss: -0.047110021114349365
Batch 22/64 loss: -0.018425405025482178
Batch 23/64 loss: -0.03456699848175049
Batch 24/64 loss: -0.02741706371307373
Batch 25/64 loss: -0.03129297494888306
Batch 26/64 loss: -0.03674119710922241
Batch 27/64 loss: -0.054674506187438965
Batch 28/64 loss: -0.05181777477264404
Batch 29/64 loss: -0.053012967109680176
Batch 30/64 loss: -0.04055500030517578
Batch 31/64 loss: -0.06312334537506104
Batch 32/64 loss: -0.03284329175949097
Batch 33/64 loss: -0.06556397676467896
Batch 34/64 loss: -0.03206366300582886
Batch 35/64 loss: -0.06216913461685181
Batch 36/64 loss: -0.03740197420120239
Batch 37/64 loss: -0.003132045269012451
Batch 38/64 loss: 0.011764049530029297
Batch 39/64 loss: -0.043584585189819336
Batch 40/64 loss: -0.018061161041259766
Batch 41/64 loss: -0.03228616714477539
Batch 42/64 loss: -0.027930080890655518
Batch 43/64 loss: -0.03420877456665039
Batch 44/64 loss: -0.060640156269073486
Batch 45/64 loss: -0.0409625768661499
Batch 46/64 loss: -0.03497523069381714
Batch 47/64 loss: -0.04306572675704956
Batch 48/64 loss: -0.04409116506576538
Batch 49/64 loss: -0.05592834949493408
Batch 50/64 loss: -0.03422105312347412
Batch 51/64 loss: -0.03557848930358887
Batch 52/64 loss: 0.006566822528839111
Batch 53/64 loss: -0.028349459171295166
Batch 54/64 loss: -0.015543937683105469
Batch 55/64 loss: -0.06925809383392334
Batch 56/64 loss: -0.03299456834793091
Batch 57/64 loss: -0.055111587047576904
Batch 58/64 loss: -0.009355306625366211
Batch 59/64 loss: -0.01742631196975708
Batch 60/64 loss: -0.0405498743057251
Batch 61/64 loss: -0.06902909278869629
Batch 62/64 loss: -0.025744736194610596
Batch 63/64 loss: -0.019126713275909424
Batch 64/64 loss: -0.030358314514160156
Epoch 169  Train loss: -0.034921853682574104  Val loss: 0.05471476001018511
Saving best model, epoch: 169
Epoch 170
-------------------------------
Batch 1/64 loss: -0.028965115547180176
Batch 2/64 loss: -0.04512995481491089
Batch 3/64 loss: -0.040187954902648926
Batch 4/64 loss: -0.045488178730010986
Batch 5/64 loss: -0.048983633518218994
Batch 6/64 loss: -0.051581382751464844
Batch 7/64 loss: -0.009205877780914307
Batch 8/64 loss: -0.05385005474090576
Batch 9/64 loss: -0.0375058650970459
Batch 10/64 loss: -0.04253464937210083
Batch 11/64 loss: -0.052259206771850586
Batch 12/64 loss: -0.037087976932525635
Batch 13/64 loss: -0.032254695892333984
Batch 14/64 loss: -0.022285938262939453
Batch 15/64 loss: -0.016353726387023926
Batch 16/64 loss: -0.02732694149017334
Batch 17/64 loss: -0.033328890800476074
Batch 18/64 loss: -0.06549179553985596
Batch 19/64 loss: -0.05156886577606201
Batch 20/64 loss: -0.05508077144622803
Batch 21/64 loss: -0.04257011413574219
Batch 22/64 loss: -0.012846529483795166
Batch 23/64 loss: -0.0426793098449707
Batch 24/64 loss: -0.03276258707046509
Batch 25/64 loss: -0.026775896549224854
Batch 26/64 loss: -0.02834951877593994
Batch 27/64 loss: -0.038071393966674805
Batch 28/64 loss: -0.0627102255821228
Batch 29/64 loss: -0.05017918348312378
Batch 30/64 loss: -0.03940904140472412
Batch 31/64 loss: -0.030699312686920166
Batch 32/64 loss: -0.044944584369659424
Batch 33/64 loss: -0.018615365028381348
Batch 34/64 loss: -0.05365443229675293
Batch 35/64 loss: -0.043620526790618896
Batch 36/64 loss: -0.0434609055519104
Batch 37/64 loss: -0.05612891912460327
Batch 38/64 loss: -0.0510706901550293
Batch 39/64 loss: -0.009804368019104004
Batch 40/64 loss: -0.06396228075027466
Batch 41/64 loss: -0.014942646026611328
Batch 42/64 loss: -0.03923392295837402
Batch 43/64 loss: -0.03186821937561035
Batch 44/64 loss: -0.0262143611907959
Batch 45/64 loss: -0.03614288568496704
Batch 46/64 loss: -0.046624183654785156
Batch 47/64 loss: -0.05116474628448486
Batch 48/64 loss: -0.026390552520751953
Batch 49/64 loss: -0.013083219528198242
Batch 50/64 loss: -0.0625150203704834
Batch 51/64 loss: -0.00389176607131958
Batch 52/64 loss: -0.0046218037605285645
Batch 53/64 loss: -0.02989262342453003
Batch 54/64 loss: -0.01891881227493286
Batch 55/64 loss: -0.024609088897705078
Batch 56/64 loss: -0.010077357292175293
Batch 57/64 loss: -0.05355095863342285
Batch 58/64 loss: -0.04548847675323486
Batch 59/64 loss: -0.04692864418029785
Batch 60/64 loss: -0.04039490222930908
Batch 61/64 loss: -0.0343780517578125
Batch 62/64 loss: -0.04673349857330322
Batch 63/64 loss: -0.016753077507019043
Batch 64/64 loss: -0.029687881469726562
Epoch 170  Train loss: -0.03663484535965265  Val loss: 0.07840903477160792
Epoch 171
-------------------------------
Batch 1/64 loss: -0.05057835578918457
Batch 2/64 loss: -0.03672212362289429
Batch 3/64 loss: -0.05895507335662842
Batch 4/64 loss: -0.034148454666137695
Batch 5/64 loss: -0.061640262603759766
Batch 6/64 loss: -0.05855715274810791
Batch 7/64 loss: -0.05598568916320801
Batch 8/64 loss: -0.04831111431121826
Batch 9/64 loss: -0.08547079563140869
Batch 10/64 loss: -0.06761026382446289
Batch 11/64 loss: -0.03865116834640503
Batch 12/64 loss: -0.034516334533691406
Batch 13/64 loss: -0.052738308906555176
Batch 14/64 loss: -0.04798680543899536
Batch 15/64 loss: -0.035624802112579346
Batch 16/64 loss: -0.04926091432571411
Batch 17/64 loss: -0.054590582847595215
Batch 18/64 loss: -0.007727444171905518
Batch 19/64 loss: -0.05934321880340576
Batch 20/64 loss: -0.027965128421783447
Batch 21/64 loss: -0.02650386095046997
Batch 22/64 loss: -0.03933435678482056
Batch 23/64 loss: -0.04639613628387451
Batch 24/64 loss: -0.01929384469985962
Batch 25/64 loss: 0.008972406387329102
Batch 26/64 loss: -0.02776336669921875
Batch 27/64 loss: -0.03297281265258789
Batch 28/64 loss: -0.044493257999420166
Batch 29/64 loss: -0.048779964447021484
Batch 30/64 loss: -0.020220816135406494
Batch 31/64 loss: -0.02170276641845703
Batch 32/64 loss: -0.036181092262268066
Batch 33/64 loss: -0.04574054479598999
Batch 34/64 loss: -0.02267313003540039
Batch 35/64 loss: -0.03248465061187744
Batch 36/64 loss: -0.05010092258453369
Batch 37/64 loss: -0.06729334592819214
Batch 38/64 loss: -0.03790688514709473
Batch 39/64 loss: -0.012267589569091797
Batch 40/64 loss: -0.05428457260131836
Batch 41/64 loss: -0.022344231605529785
Batch 42/64 loss: -0.03371536731719971
Batch 43/64 loss: -0.011961936950683594
Batch 44/64 loss: -0.0450972318649292
Batch 45/64 loss: -0.0655066967010498
Batch 46/64 loss: -0.06820273399353027
Batch 47/64 loss: -0.026110172271728516
Batch 48/64 loss: -0.037336885929107666
Batch 49/64 loss: -0.02987748384475708
Batch 50/64 loss: -0.03825509548187256
Batch 51/64 loss: -0.03763669729232788
Batch 52/64 loss: -0.031944215297698975
Batch 53/64 loss: -0.01800161600112915
Batch 54/64 loss: -0.019778788089752197
Batch 55/64 loss: -0.05416768789291382
Batch 56/64 loss: -0.024822890758514404
Batch 57/64 loss: -0.0360068678855896
Batch 58/64 loss: -0.027096867561340332
Batch 59/64 loss: -0.034734368324279785
Batch 60/64 loss: -0.027038991451263428
Batch 61/64 loss: -0.028719067573547363
Batch 62/64 loss: -0.006311655044555664
Batch 63/64 loss: -0.041190147399902344
Batch 64/64 loss: -0.04111355543136597
Epoch 171  Train loss: -0.038282327324736354  Val loss: 0.058252324763032576
Epoch 172
-------------------------------
Batch 1/64 loss: -0.04507017135620117
Batch 2/64 loss: -0.06007671356201172
Batch 3/64 loss: -0.04648876190185547
Batch 4/64 loss: -0.054896414279937744
Batch 5/64 loss: -0.010627448558807373
Batch 6/64 loss: -0.04691970348358154
Batch 7/64 loss: -0.04909563064575195
Batch 8/64 loss: -0.03709208965301514
Batch 9/64 loss: -0.021328210830688477
Batch 10/64 loss: -0.03492015600204468
Batch 11/64 loss: -0.03716230392456055
Batch 12/64 loss: -0.00690847635269165
Batch 13/64 loss: -0.044740915298461914
Batch 14/64 loss: -0.010958671569824219
Batch 15/64 loss: -0.03576195240020752
Batch 16/64 loss: -0.022453129291534424
Batch 17/64 loss: -0.05778908729553223
Batch 18/64 loss: -0.028666138648986816
Batch 19/64 loss: -0.04252380132675171
Batch 20/64 loss: -0.034301578998565674
Batch 21/64 loss: -0.047530949115753174
Batch 22/64 loss: -0.010695040225982666
Batch 23/64 loss: -0.0616723895072937
Batch 24/64 loss: 0.021898746490478516
Batch 25/64 loss: -0.021824896335601807
Batch 26/64 loss: -0.04165983200073242
Batch 27/64 loss: -0.02946329116821289
Batch 28/64 loss: -0.03479909896850586
Batch 29/64 loss: -0.03220003843307495
Batch 30/64 loss: -0.04966968297958374
Batch 31/64 loss: -0.024759411811828613
Batch 32/64 loss: -0.04638242721557617
Batch 33/64 loss: -0.026808083057403564
Batch 34/64 loss: -0.019313454627990723
Batch 35/64 loss: -0.0647515058517456
Batch 36/64 loss: -0.07062315940856934
Batch 37/64 loss: -0.026112914085388184
Batch 38/64 loss: -0.046386778354644775
Batch 39/64 loss: -0.016864001750946045
Batch 40/64 loss: -0.01302492618560791
Batch 41/64 loss: -0.06317257881164551
Batch 42/64 loss: -0.06445527076721191
Batch 43/64 loss: -0.0689542293548584
Batch 44/64 loss: -0.06829923391342163
Batch 45/64 loss: -0.039646148681640625
Batch 46/64 loss: -0.021817445755004883
Batch 47/64 loss: -0.05391514301300049
Batch 48/64 loss: -0.019736051559448242
Batch 49/64 loss: -0.04500788450241089
Batch 50/64 loss: -0.023760974407196045
Batch 51/64 loss: -0.05740320682525635
Batch 52/64 loss: -0.048307180404663086
Batch 53/64 loss: -0.029980778694152832
Batch 54/64 loss: -0.05987060070037842
Batch 55/64 loss: -0.014540910720825195
Batch 56/64 loss: -0.04393893480300903
Batch 57/64 loss: -0.04626941680908203
Batch 58/64 loss: -0.032932281494140625
Batch 59/64 loss: -0.0277557373046875
Batch 60/64 loss: -0.029782652854919434
Batch 61/64 loss: -0.06673824787139893
Batch 62/64 loss: -0.044825851917266846
Batch 63/64 loss: -0.04202544689178467
Batch 64/64 loss: -0.04486793279647827
Epoch 172  Train loss: -0.038230770943211576  Val loss: 0.06400162923786648
Epoch 173
-------------------------------
Batch 1/64 loss: -0.026998281478881836
Batch 2/64 loss: -0.013349771499633789
Batch 3/64 loss: -0.04699814319610596
Batch 4/64 loss: -0.02436649799346924
Batch 5/64 loss: -0.031404972076416016
Batch 6/64 loss: -0.05463218688964844
Batch 7/64 loss: -0.0261613130569458
Batch 8/64 loss: -0.06044894456863403
Batch 9/64 loss: -0.0696377158164978
Batch 10/64 loss: -0.017950892448425293
Batch 11/64 loss: -0.049588561058044434
Batch 12/64 loss: -0.033618032932281494
Batch 13/64 loss: -0.05294674634933472
Batch 14/64 loss: -0.0443652868270874
Batch 15/64 loss: -0.009868621826171875
Batch 16/64 loss: -0.05092984437942505
Batch 17/64 loss: -0.026076972484588623
Batch 18/64 loss: -0.017374634742736816
Batch 19/64 loss: -0.017604410648345947
Batch 20/64 loss: -0.02868705987930298
Batch 21/64 loss: -0.06016337871551514
Batch 22/64 loss: -0.054705142974853516
Batch 23/64 loss: -0.0573086142539978
Batch 24/64 loss: -0.05612325668334961
Batch 25/64 loss: -0.05710101127624512
Batch 26/64 loss: -0.041470885276794434
Batch 27/64 loss: -0.003552377223968506
Batch 28/64 loss: -0.04503530263900757
Batch 29/64 loss: -0.04141569137573242
Batch 30/64 loss: -0.08374303579330444
Batch 31/64 loss: -0.041114866733551025
Batch 32/64 loss: 0.005250394344329834
Batch 33/64 loss: -0.05150175094604492
Batch 34/64 loss: -0.04539942741394043
Batch 35/64 loss: -0.03755295276641846
Batch 36/64 loss: -0.03811907768249512
Batch 37/64 loss: -0.06116950511932373
Batch 38/64 loss: -0.02273184061050415
Batch 39/64 loss: -0.050728559494018555
Batch 40/64 loss: -0.024357736110687256
Batch 41/64 loss: -0.05201679468154907
Batch 42/64 loss: -0.04532575607299805
Batch 43/64 loss: -0.04683089256286621
Batch 44/64 loss: -0.07638543844223022
Batch 45/64 loss: -0.05905044078826904
Batch 46/64 loss: -0.042569756507873535
Batch 47/64 loss: -0.05391806364059448
Batch 48/64 loss: -0.028948962688446045
Batch 49/64 loss: -0.016354084014892578
Batch 50/64 loss: -0.040056586265563965
Batch 51/64 loss: -0.02088254690170288
Batch 52/64 loss: -0.04390919208526611
Batch 53/64 loss: -0.03833460807800293
Batch 54/64 loss: -0.033903419971466064
Batch 55/64 loss: 0.007209956645965576
Batch 56/64 loss: -0.01199418306350708
Batch 57/64 loss: -0.04019802808761597
Batch 58/64 loss: -0.02334451675415039
Batch 59/64 loss: -0.018148183822631836
Batch 60/64 loss: -0.07028734683990479
Batch 61/64 loss: -0.02631068229675293
Batch 62/64 loss: -0.05425482988357544
Batch 63/64 loss: -0.058765411376953125
Batch 64/64 loss: -0.0382537841796875
Epoch 173  Train loss: -0.03912663553275314  Val loss: 0.05521624358658938
Epoch 174
-------------------------------
Batch 1/64 loss: -0.05039757490158081
Batch 2/64 loss: -0.011654198169708252
Batch 3/64 loss: -0.08257436752319336
Batch 4/64 loss: -0.0522955060005188
Batch 5/64 loss: -0.025239229202270508
Batch 6/64 loss: -0.04865604639053345
Batch 7/64 loss: -0.02894127368927002
Batch 8/64 loss: 0.0027565956115722656
Batch 9/64 loss: -0.06488049030303955
Batch 10/64 loss: -0.06052732467651367
Batch 11/64 loss: -0.035735905170440674
Batch 12/64 loss: -0.04108273983001709
Batch 13/64 loss: -0.06692236661911011
Batch 14/64 loss: -0.03825896978378296
Batch 15/64 loss: -0.02264559268951416
Batch 16/64 loss: -0.04246491193771362
Batch 17/64 loss: -0.035965561866760254
Batch 18/64 loss: -0.05949443578720093
Batch 19/64 loss: -0.05606156587600708
Batch 20/64 loss: -0.015430271625518799
Batch 21/64 loss: -0.009481668472290039
Batch 22/64 loss: -0.04704493284225464
Batch 23/64 loss: -0.02397763729095459
Batch 24/64 loss: -0.03818929195404053
Batch 25/64 loss: -0.07435458898544312
Batch 26/64 loss: -0.053662657737731934
Batch 27/64 loss: -0.030979514122009277
Batch 28/64 loss: -0.058218955993652344
Batch 29/64 loss: -0.014999985694885254
Batch 30/64 loss: -0.03364628553390503
Batch 31/64 loss: -0.02514326572418213
Batch 32/64 loss: -0.018646955490112305
Batch 33/64 loss: -0.029734313488006592
Batch 34/64 loss: -0.023111462593078613
Batch 35/64 loss: -0.03309208154678345
Batch 36/64 loss: -0.01849365234375
Batch 37/64 loss: -0.025504469871520996
Batch 38/64 loss: -0.051669538021087646
Batch 39/64 loss: -0.06759291887283325
Batch 40/64 loss: -0.038481950759887695
Batch 41/64 loss: -0.03362470865249634
Batch 42/64 loss: -0.04468071460723877
Batch 43/64 loss: -0.04307389259338379
Batch 44/64 loss: -0.03367209434509277
Batch 45/64 loss: -0.03510254621505737
Batch 46/64 loss: -0.04396629333496094
Batch 47/64 loss: -0.056314706802368164
Batch 48/64 loss: -0.015308797359466553
Batch 49/64 loss: -0.06344753503799438
Batch 50/64 loss: -0.01952582597732544
Batch 51/64 loss: -0.01949983835220337
Batch 52/64 loss: -0.03883349895477295
Batch 53/64 loss: -0.05049538612365723
Batch 54/64 loss: -0.056984782218933105
Batch 55/64 loss: -0.040836453437805176
Batch 56/64 loss: -0.041355013847351074
Batch 57/64 loss: -0.008499383926391602
Batch 58/64 loss: -0.042594969272613525
Batch 59/64 loss: -0.07008355855941772
Batch 60/64 loss: -0.053256988525390625
Batch 61/64 loss: -0.05814081430435181
Batch 62/64 loss: -0.0327991247177124
Batch 63/64 loss: -0.05489844083786011
Batch 64/64 loss: -0.0334477424621582
Epoch 174  Train loss: -0.03975810256658816  Val loss: 0.05406678419342566
Saving best model, epoch: 174
Epoch 175
-------------------------------
Batch 1/64 loss: -0.03729122877120972
Batch 2/64 loss: -0.06967467069625854
Batch 3/64 loss: -0.052362799644470215
Batch 4/64 loss: -0.05853754281997681
Batch 5/64 loss: -0.03790843486785889
Batch 6/64 loss: -0.04291445016860962
Batch 7/64 loss: -0.06539011001586914
Batch 8/64 loss: -0.06653451919555664
Batch 9/64 loss: 0.004449784755706787
Batch 10/64 loss: -0.049529314041137695
Batch 11/64 loss: -0.04648619890213013
Batch 12/64 loss: -0.008319079875946045
Batch 13/64 loss: -0.032648444175720215
Batch 14/64 loss: -0.03564643859863281
Batch 15/64 loss: -0.04141402244567871
Batch 16/64 loss: -0.02798926830291748
Batch 17/64 loss: -0.061634361743927
Batch 18/64 loss: -0.023473501205444336
Batch 19/64 loss: -0.025912821292877197
Batch 20/64 loss: -0.04494112730026245
Batch 21/64 loss: -0.030249714851379395
Batch 22/64 loss: -0.012643873691558838
Batch 23/64 loss: -0.06582337617874146
Batch 24/64 loss: -0.008998692035675049
Batch 25/64 loss: -0.0014787912368774414
Batch 26/64 loss: -0.03431510925292969
Batch 27/64 loss: -0.053939223289489746
Batch 28/64 loss: -0.03635871410369873
Batch 29/64 loss: -0.03522694110870361
Batch 30/64 loss: -0.023689866065979004
Batch 31/64 loss: -0.01316523551940918
Batch 32/64 loss: -0.0181657075881958
Batch 33/64 loss: -0.06370651721954346
Batch 34/64 loss: -0.04215574264526367
Batch 35/64 loss: -0.0319446325302124
Batch 36/64 loss: -0.04685640335083008
Batch 37/64 loss: -0.05821728706359863
Batch 38/64 loss: -0.05497598648071289
Batch 39/64 loss: -0.04960530996322632
Batch 40/64 loss: -0.02819007635116577
Batch 41/64 loss: -0.0517573356628418
Batch 42/64 loss: -0.02420186996459961
Batch 43/64 loss: -0.0516735315322876
Batch 44/64 loss: -0.02811485528945923
Batch 45/64 loss: -0.020574867725372314
Batch 46/64 loss: -0.0366935133934021
Batch 47/64 loss: -0.04341691732406616
Batch 48/64 loss: -0.0024449825286865234
Batch 49/64 loss: -0.03490591049194336
Batch 50/64 loss: -0.04059576988220215
Batch 51/64 loss: -0.0453985333442688
Batch 52/64 loss: -0.06309568881988525
Batch 53/64 loss: 0.002477407455444336
Batch 54/64 loss: -0.03421670198440552
Batch 55/64 loss: -0.048611223697662354
Batch 56/64 loss: -0.046314120292663574
Batch 57/64 loss: -0.02974492311477661
Batch 58/64 loss: -0.05096566677093506
Batch 59/64 loss: -0.042566537857055664
Batch 60/64 loss: -0.027668476104736328
Batch 61/64 loss: -0.05713003873825073
Batch 62/64 loss: -0.03372025489807129
Batch 63/64 loss: -0.06237369775772095
Batch 64/64 loss: -0.053388237953186035
Epoch 175  Train loss: -0.03839395981208951  Val loss: 0.058018094690394977
Epoch 176
-------------------------------
Batch 1/64 loss: -0.03630232810974121
Batch 2/64 loss: -0.04764842987060547
Batch 3/64 loss: -0.0386539101600647
Batch 4/64 loss: -0.041570425033569336
Batch 5/64 loss: -0.07041364908218384
Batch 6/64 loss: -0.0805518627166748
Batch 7/64 loss: -0.07880055904388428
Batch 8/64 loss: -0.05692023038864136
Batch 9/64 loss: -0.047580063343048096
Batch 10/64 loss: -0.043131113052368164
Batch 11/64 loss: -0.03411799669265747
Batch 12/64 loss: -0.049784064292907715
Batch 13/64 loss: -0.04762709140777588
Batch 14/64 loss: -0.00731426477432251
Batch 15/64 loss: 0.0005855560302734375
Batch 16/64 loss: -0.050993263721466064
Batch 17/64 loss: -0.012616634368896484
Batch 18/64 loss: -0.02190464735031128
Batch 19/64 loss: -0.051124751567840576
Batch 20/64 loss: -0.01846480369567871
Batch 21/64 loss: -0.06777775287628174
Batch 22/64 loss: -0.02813267707824707
Batch 23/64 loss: -0.07025563716888428
Batch 24/64 loss: -0.0627068281173706
Batch 25/64 loss: -0.04181391000747681
Batch 26/64 loss: -0.03646349906921387
Batch 27/64 loss: -0.04383724927902222
Batch 28/64 loss: -0.03328132629394531
Batch 29/64 loss: -0.06696116924285889
Batch 30/64 loss: -0.050216495990753174
Batch 31/64 loss: -0.03984951972961426
Batch 32/64 loss: -0.06211668252944946
Batch 33/64 loss: -0.026033878326416016
Batch 34/64 loss: -0.04070812463760376
Batch 35/64 loss: -0.047272443771362305
Batch 36/64 loss: -0.035965144634246826
Batch 37/64 loss: -0.060781896114349365
Batch 38/64 loss: -0.029789328575134277
Batch 39/64 loss: -0.05350661277770996
Batch 40/64 loss: -0.017781734466552734
Batch 41/64 loss: -0.02493387460708618
Batch 42/64 loss: -0.06572967767715454
Batch 43/64 loss: -0.05431097745895386
Batch 44/64 loss: -0.052631258964538574
Batch 45/64 loss: -0.030417680740356445
Batch 46/64 loss: -0.03599202632904053
Batch 47/64 loss: -0.009572982788085938
Batch 48/64 loss: 0.014357268810272217
Batch 49/64 loss: -0.023932993412017822
Batch 50/64 loss: -0.03748667240142822
Batch 51/64 loss: -0.06207764148712158
Batch 52/64 loss: -0.029954195022583008
Batch 53/64 loss: -0.042481958866119385
Batch 54/64 loss: -0.030488252639770508
Batch 55/64 loss: -0.03549039363861084
Batch 56/64 loss: -0.03225994110107422
Batch 57/64 loss: -0.06059211492538452
Batch 58/64 loss: -0.03106057643890381
Batch 59/64 loss: -0.02780395746231079
Batch 60/64 loss: -0.0451885461807251
Batch 61/64 loss: -0.02503758668899536
Batch 62/64 loss: -0.015525579452514648
Batch 63/64 loss: -0.029632747173309326
Batch 64/64 loss: -0.028729379177093506
Epoch 176  Train loss: -0.040156515205607694  Val loss: 0.07665146339390286
Epoch 177
-------------------------------
Batch 1/64 loss: -0.005926549434661865
Batch 2/64 loss: -0.03593921661376953
Batch 3/64 loss: -0.06472069025039673
Batch 4/64 loss: -0.029919564723968506
Batch 5/64 loss: -0.024451076984405518
Batch 6/64 loss: -0.05950087308883667
Batch 7/64 loss: -0.03643381595611572
Batch 8/64 loss: -0.04787546396255493
Batch 9/64 loss: -0.03453803062438965
Batch 10/64 loss: -0.05453997850418091
Batch 11/64 loss: 0.0007216334342956543
Batch 12/64 loss: -0.0540691614151001
Batch 13/64 loss: -0.0505366325378418
Batch 14/64 loss: -0.03895217180252075
Batch 15/64 loss: -0.06426513195037842
Batch 16/64 loss: -0.03844732046127319
Batch 17/64 loss: -0.025411009788513184
Batch 18/64 loss: -0.0246201753616333
Batch 19/64 loss: -0.012301802635192871
Batch 20/64 loss: -0.04745596647262573
Batch 21/64 loss: -0.07144540548324585
Batch 22/64 loss: -0.02838832139968872
Batch 23/64 loss: -0.059545278549194336
Batch 24/64 loss: -0.046170949935913086
Batch 25/64 loss: -0.052029967308044434
Batch 26/64 loss: -0.06256645917892456
Batch 27/64 loss: -0.03611040115356445
Batch 28/64 loss: -0.03883737325668335
Batch 29/64 loss: -0.06254082918167114
Batch 30/64 loss: -0.07791978120803833
Batch 31/64 loss: -0.04968303442001343
Batch 32/64 loss: -0.033955276012420654
Batch 33/64 loss: -0.05962550640106201
Batch 34/64 loss: -0.029115140438079834
Batch 35/64 loss: -0.04558110237121582
Batch 36/64 loss: -0.035625576972961426
Batch 37/64 loss: -0.04203438758850098
Batch 38/64 loss: -0.05426311492919922
Batch 39/64 loss: -0.024205684661865234
Batch 40/64 loss: -0.04300034046173096
Batch 41/64 loss: -0.049682021141052246
Batch 42/64 loss: -0.04880756139755249
Batch 43/64 loss: -0.015428364276885986
Batch 44/64 loss: -0.03564095497131348
Batch 45/64 loss: -0.06385451555252075
Batch 46/64 loss: -0.019607603549957275
Batch 47/64 loss: -0.039240121841430664
Batch 48/64 loss: -0.057411789894104004
Batch 49/64 loss: -0.05242544412612915
Batch 50/64 loss: -0.0361667275428772
Batch 51/64 loss: -0.008057892322540283
Batch 52/64 loss: -0.03056889772415161
Batch 53/64 loss: -0.010646462440490723
Batch 54/64 loss: -0.03971266746520996
Batch 55/64 loss: -0.04015302658081055
Batch 56/64 loss: -0.022826313972473145
Batch 57/64 loss: -0.019844353199005127
Batch 58/64 loss: -0.051653385162353516
Batch 59/64 loss: 0.014370441436767578
Batch 60/64 loss: -0.01578497886657715
Batch 61/64 loss: -0.0669097900390625
Batch 62/64 loss: -0.03714495897293091
Batch 63/64 loss: -0.050511717796325684
Batch 64/64 loss: -0.01903092861175537
Epoch 177  Train loss: -0.03943230170829623  Val loss: 0.06343226273035266
Epoch 178
-------------------------------
Batch 1/64 loss: -0.023415327072143555
Batch 2/64 loss: -0.03598892688751221
Batch 3/64 loss: -0.052758097648620605
Batch 4/64 loss: -0.027904391288757324
Batch 5/64 loss: -0.026184678077697754
Batch 6/64 loss: -0.04314088821411133
Batch 7/64 loss: -0.05314528942108154
Batch 8/64 loss: -0.0685378909111023
Batch 9/64 loss: -0.05342674255371094
Batch 10/64 loss: -0.05569756031036377
Batch 11/64 loss: -0.024610459804534912
Batch 12/64 loss: -0.01663076877593994
Batch 13/64 loss: -0.03596174716949463
Batch 14/64 loss: -0.04443246126174927
Batch 15/64 loss: -0.012436926364898682
Batch 16/64 loss: -0.04183626174926758
Batch 17/64 loss: 0.010906815528869629
Batch 18/64 loss: -0.06811791658401489
Batch 19/64 loss: -0.06814664602279663
Batch 20/64 loss: -0.021880924701690674
Batch 21/64 loss: -0.052627742290496826
Batch 22/64 loss: -0.03349989652633667
Batch 23/64 loss: -0.03685355186462402
Batch 24/64 loss: -0.0529971718788147
Batch 25/64 loss: -0.08039391040802002
Batch 26/64 loss: -0.03837478160858154
Batch 27/64 loss: -0.03373003005981445
Batch 28/64 loss: -0.06656008958816528
Batch 29/64 loss: 0.0029397010803222656
Batch 30/64 loss: -0.0651090145111084
Batch 31/64 loss: -0.03515666723251343
Batch 32/64 loss: -0.03168773651123047
Batch 33/64 loss: -0.04039120674133301
Batch 34/64 loss: -0.06539207696914673
Batch 35/64 loss: -0.05742710828781128
Batch 36/64 loss: -0.0375291109085083
Batch 37/64 loss: -0.052549004554748535
Batch 38/64 loss: -0.030864834785461426
Batch 39/64 loss: -0.03425854444503784
Batch 40/64 loss: -0.036484718322753906
Batch 41/64 loss: -0.04081845283508301
Batch 42/64 loss: -0.04286789894104004
Batch 43/64 loss: -0.035043954849243164
Batch 44/64 loss: -0.046461284160614014
Batch 45/64 loss: -0.07419991493225098
Batch 46/64 loss: -0.009382963180541992
Batch 47/64 loss: -0.05131250619888306
Batch 48/64 loss: -0.011083543300628662
Batch 49/64 loss: -0.02853327989578247
Batch 50/64 loss: -0.05409514904022217
Batch 51/64 loss: -0.04503178596496582
Batch 52/64 loss: -0.04179728031158447
Batch 53/64 loss: -0.04857313632965088
Batch 54/64 loss: -0.021826565265655518
Batch 55/64 loss: -0.04576212167739868
Batch 56/64 loss: -0.02249664068222046
Batch 57/64 loss: -0.04073566198348999
Batch 58/64 loss: -0.032093167304992676
Batch 59/64 loss: -0.046833038330078125
Batch 60/64 loss: -0.05846017599105835
Batch 61/64 loss: -0.0751989483833313
Batch 62/64 loss: -0.0407567024230957
Batch 63/64 loss: -0.0542827844619751
Batch 64/64 loss: -0.06005018949508667
Epoch 178  Train loss: -0.04164673230227302  Val loss: 0.05732390274296921
Epoch 179
-------------------------------
Batch 1/64 loss: -0.06923717260360718
Batch 2/64 loss: -0.05500650405883789
Batch 3/64 loss: -0.0671471357345581
Batch 4/64 loss: -0.025558650493621826
Batch 5/64 loss: 0.012651562690734863
Batch 6/64 loss: -0.06070679426193237
Batch 7/64 loss: -0.05672484636306763
Batch 8/64 loss: -0.04548913240432739
Batch 9/64 loss: -0.008894741535186768
Batch 10/64 loss: -0.03810495138168335
Batch 11/64 loss: -0.06258535385131836
Batch 12/64 loss: -0.050253093242645264
Batch 13/64 loss: -0.06720072031021118
Batch 14/64 loss: -0.03756308555603027
Batch 15/64 loss: -0.044020235538482666
Batch 16/64 loss: -0.05503612756729126
Batch 17/64 loss: -0.05607926845550537
Batch 18/64 loss: -0.039599716663360596
Batch 19/64 loss: -0.05923306941986084
Batch 20/64 loss: -0.051183223724365234
Batch 21/64 loss: -0.04446136951446533
Batch 22/64 loss: -0.04046499729156494
Batch 23/64 loss: -0.03309887647628784
Batch 24/64 loss: -0.04385685920715332
Batch 25/64 loss: -0.02239614725112915
Batch 26/64 loss: -0.04406249523162842
Batch 27/64 loss: -0.05571770668029785
Batch 28/64 loss: -0.05000180006027222
Batch 29/64 loss: -0.037496328353881836
Batch 30/64 loss: -0.041677236557006836
Batch 31/64 loss: -0.02267378568649292
Batch 32/64 loss: -0.04160994291305542
Batch 33/64 loss: -0.03999769687652588
Batch 34/64 loss: -0.04075300693511963
Batch 35/64 loss: -0.03122079372406006
Batch 36/64 loss: -0.04906958341598511
Batch 37/64 loss: -0.0625753402709961
Batch 38/64 loss: -0.02506089210510254
Batch 39/64 loss: -0.06306487321853638
Batch 40/64 loss: -0.05067145824432373
Batch 41/64 loss: -0.046140849590301514
Batch 42/64 loss: -0.03323400020599365
Batch 43/64 loss: -0.03562629222869873
Batch 44/64 loss: -0.026660621166229248
Batch 45/64 loss: -0.060145556926727295
Batch 46/64 loss: -0.0406041145324707
Batch 47/64 loss: -0.05659675598144531
Batch 48/64 loss: -0.015887558460235596
Batch 49/64 loss: -0.006151318550109863
Batch 50/64 loss: -0.04038208723068237
Batch 51/64 loss: -0.04701066017150879
Batch 52/64 loss: -0.05321979522705078
Batch 53/64 loss: -0.03543853759765625
Batch 54/64 loss: -0.04493355751037598
Batch 55/64 loss: -0.06407713890075684
Batch 56/64 loss: -0.013440608978271484
Batch 57/64 loss: -0.033515751361846924
Batch 58/64 loss: -0.05453157424926758
Batch 59/64 loss: -0.06567353010177612
Batch 60/64 loss: -0.051332712173461914
Batch 61/64 loss: -0.052111923694610596
Batch 62/64 loss: -0.028709888458251953
Batch 63/64 loss: -0.03932380676269531
Batch 64/64 loss: -0.009816944599151611
Epoch 179  Train loss: -0.042808075278413064  Val loss: 0.057672033604887346
Epoch 180
-------------------------------
Batch 1/64 loss: -0.007984280586242676
Batch 2/64 loss: -0.0441664457321167
Batch 3/64 loss: -0.062103986740112305
Batch 4/64 loss: -0.043161988258361816
Batch 5/64 loss: -0.032519638538360596
Batch 6/64 loss: -0.025664210319519043
Batch 7/64 loss: -0.02432161569595337
Batch 8/64 loss: -0.06683176755905151
Batch 9/64 loss: -0.04438728094100952
Batch 10/64 loss: -0.03173011541366577
Batch 11/64 loss: -0.024247825145721436
Batch 12/64 loss: -0.04986774921417236
Batch 13/64 loss: -0.0755121111869812
Batch 14/64 loss: -0.04000246524810791
Batch 15/64 loss: -0.019136130809783936
Batch 16/64 loss: -0.05773979425430298
Batch 17/64 loss: -0.04695284366607666
Batch 18/64 loss: -0.05754595994949341
Batch 19/64 loss: -0.04856353998184204
Batch 20/64 loss: -0.057847559452056885
Batch 21/64 loss: -0.036345720291137695
Batch 22/64 loss: -0.05232882499694824
Batch 23/64 loss: -0.018974125385284424
Batch 24/64 loss: -0.03666651248931885
Batch 25/64 loss: -0.05782878398895264
Batch 26/64 loss: -0.0527459979057312
Batch 27/64 loss: -0.057411909103393555
Batch 28/64 loss: -0.05914735794067383
Batch 29/64 loss: -0.04003751277923584
Batch 30/64 loss: -0.05719602108001709
Batch 31/64 loss: -0.03812748193740845
Batch 32/64 loss: -0.056386590003967285
Batch 33/64 loss: -0.06311476230621338
Batch 34/64 loss: -0.04266512393951416
Batch 35/64 loss: -0.039548635482788086
Batch 36/64 loss: -0.024873197078704834
Batch 37/64 loss: -0.042410314083099365
Batch 38/64 loss: -0.03767383098602295
Batch 39/64 loss: -0.05202007293701172
Batch 40/64 loss: -0.03863346576690674
Batch 41/64 loss: -0.06323504447937012
Batch 42/64 loss: -0.04791605472564697
Batch 43/64 loss: -0.042332470417022705
Batch 44/64 loss: -0.050445616245269775
Batch 45/64 loss: -0.02899867296218872
Batch 46/64 loss: -0.0657084584236145
Batch 47/64 loss: -0.044844985008239746
Batch 48/64 loss: -0.048357486724853516
Batch 49/64 loss: -0.04224395751953125
Batch 50/64 loss: -0.03763842582702637
Batch 51/64 loss: -0.035883963108062744
Batch 52/64 loss: -0.07314014434814453
Batch 53/64 loss: -0.05715912580490112
Batch 54/64 loss: -0.06931102275848389
Batch 55/64 loss: -0.06307291984558105
Batch 56/64 loss: -0.048082947731018066
Batch 57/64 loss: -0.03170371055603027
Batch 58/64 loss: -0.047229886054992676
Batch 59/64 loss: -0.05190014839172363
Batch 60/64 loss: -0.039093852043151855
Batch 61/64 loss: -0.03174477815628052
Batch 62/64 loss: -0.03036731481552124
Batch 63/64 loss: -0.029949963092803955
Batch 64/64 loss: -0.018619418144226074
Epoch 180  Train loss: -0.0448431225383983  Val loss: 0.0622106403419652
Epoch 181
-------------------------------
Batch 1/64 loss: -0.06271982192993164
Batch 2/64 loss: -0.021979033946990967
Batch 3/64 loss: -0.010641753673553467
Batch 4/64 loss: -0.05496305227279663
Batch 5/64 loss: -0.011278688907623291
Batch 6/64 loss: -0.07377028465270996
Batch 7/64 loss: -0.045479536056518555
Batch 8/64 loss: -0.043352365493774414
Batch 9/64 loss: -0.015080630779266357
Batch 10/64 loss: -0.029410481452941895
Batch 11/64 loss: -0.04109114408493042
Batch 12/64 loss: -0.03526550531387329
Batch 13/64 loss: -0.07870686054229736
Batch 14/64 loss: -0.05838334560394287
Batch 15/64 loss: -0.038484156131744385
Batch 16/64 loss: -0.03878343105316162
Batch 17/64 loss: -0.05332779884338379
Batch 18/64 loss: -0.06409770250320435
Batch 19/64 loss: -0.03178972005844116
Batch 20/64 loss: -0.038741886615753174
Batch 21/64 loss: -0.0394977331161499
Batch 22/64 loss: -0.006386935710906982
Batch 23/64 loss: -0.0502811074256897
Batch 24/64 loss: -0.03784835338592529
Batch 25/64 loss: -0.07429385185241699
Batch 26/64 loss: -0.023304104804992676
Batch 27/64 loss: -0.055415987968444824
Batch 28/64 loss: -0.019444942474365234
Batch 29/64 loss: -0.05917620658874512
Batch 30/64 loss: -0.04965341091156006
Batch 31/64 loss: -0.050182342529296875
Batch 32/64 loss: -0.06448233127593994
Batch 33/64 loss: -0.04735410213470459
Batch 34/64 loss: -0.05389571189880371
Batch 35/64 loss: -0.03997480869293213
Batch 36/64 loss: -0.05130434036254883
Batch 37/64 loss: -0.05622047185897827
Batch 38/64 loss: -0.0692446231842041
Batch 39/64 loss: -0.07557988166809082
Batch 40/64 loss: -0.020113646984100342
Batch 41/64 loss: -0.05067640542984009
Batch 42/64 loss: -0.04272860288619995
Batch 43/64 loss: -0.06966370344161987
Batch 44/64 loss: -0.0375480055809021
Batch 45/64 loss: -0.012524306774139404
Batch 46/64 loss: -0.0773857831954956
Batch 47/64 loss: -0.03847384452819824
Batch 48/64 loss: -0.049737393856048584
Batch 49/64 loss: -0.031239807605743408
Batch 50/64 loss: -0.04216426610946655
Batch 51/64 loss: -0.04370725154876709
Batch 52/64 loss: -0.02006453275680542
Batch 53/64 loss: -0.043295204639434814
Batch 54/64 loss: -0.03721761703491211
Batch 55/64 loss: -0.032753944396972656
Batch 56/64 loss: -0.05106973648071289
Batch 57/64 loss: -0.05896669626235962
Batch 58/64 loss: -0.043810486793518066
Batch 59/64 loss: -0.05031299591064453
Batch 60/64 loss: -0.045555710792541504
Batch 61/64 loss: -0.043050527572631836
Batch 62/64 loss: -0.05934619903564453
Batch 63/64 loss: -0.012316107749938965
Batch 64/64 loss: -0.0557788610458374
Epoch 181  Train loss: -0.044336397975098855  Val loss: 0.052830509098944385
Saving best model, epoch: 181
Epoch 182
-------------------------------
Batch 1/64 loss: -0.036201298236846924
Batch 2/64 loss: -0.05187404155731201
Batch 3/64 loss: -0.05254465341567993
Batch 4/64 loss: -0.019792795181274414
Batch 5/64 loss: -0.04105257987976074
Batch 6/64 loss: -0.05284851789474487
Batch 7/64 loss: -0.05392813682556152
Batch 8/64 loss: -0.043181538581848145
Batch 9/64 loss: -0.03541302680969238
Batch 10/64 loss: -0.026131033897399902
Batch 11/64 loss: -0.043478310108184814
Batch 12/64 loss: -0.041022539138793945
Batch 13/64 loss: -0.04874372482299805
Batch 14/64 loss: -0.05130910873413086
Batch 15/64 loss: -0.053403258323669434
Batch 16/64 loss: -0.04202687740325928
Batch 17/64 loss: -0.04762768745422363
Batch 18/64 loss: -0.010359883308410645
Batch 19/64 loss: -0.035746216773986816
Batch 20/64 loss: -0.04882842302322388
Batch 21/64 loss: -0.05733680725097656
Batch 22/64 loss: -0.05318814516067505
Batch 23/64 loss: -0.05668342113494873
Batch 24/64 loss: -0.03923523426055908
Batch 25/64 loss: -0.032286882400512695
Batch 26/64 loss: -0.044150710105895996
Batch 27/64 loss: -0.020884990692138672
Batch 28/64 loss: -0.07373911142349243
Batch 29/64 loss: -0.05753827095031738
Batch 30/64 loss: -0.04358094930648804
Batch 31/64 loss: -0.02342700958251953
Batch 32/64 loss: -0.04546821117401123
Batch 33/64 loss: -0.08636218309402466
Batch 34/64 loss: -0.03794831037521362
Batch 35/64 loss: -0.059046149253845215
Batch 36/64 loss: -0.07642197608947754
Batch 37/64 loss: -0.0342785120010376
Batch 38/64 loss: -0.05278122425079346
Batch 39/64 loss: -0.08176559209823608
Batch 40/64 loss: -0.060856640338897705
Batch 41/64 loss: -0.03875386714935303
Batch 42/64 loss: -0.020920276641845703
Batch 43/64 loss: -0.041646480560302734
Batch 44/64 loss: -0.039278268814086914
Batch 45/64 loss: -0.048241257667541504
Batch 46/64 loss: -0.03190857172012329
Batch 47/64 loss: -0.030390024185180664
Batch 48/64 loss: -0.03820270299911499
Batch 49/64 loss: -0.02939969301223755
Batch 50/64 loss: -0.048464059829711914
Batch 51/64 loss: -0.0401572585105896
Batch 52/64 loss: -0.046937763690948486
Batch 53/64 loss: -0.03046417236328125
Batch 54/64 loss: -0.042574405670166016
Batch 55/64 loss: -0.027457356452941895
Batch 56/64 loss: -0.05112791061401367
Batch 57/64 loss: -0.02875608205795288
Batch 58/64 loss: -0.04391765594482422
Batch 59/64 loss: -0.02154618501663208
Batch 60/64 loss: -0.06482619047164917
Batch 61/64 loss: -0.03943347930908203
Batch 62/64 loss: -0.049581170082092285
Batch 63/64 loss: -0.0493694543838501
Batch 64/64 loss: -0.05886256694793701
Epoch 182  Train loss: -0.04423521873997707  Val loss: 0.062039492875849665
Epoch 183
-------------------------------
Batch 1/64 loss: -0.08324027061462402
Batch 2/64 loss: -0.050420165061950684
Batch 3/64 loss: -0.04874610900878906
Batch 4/64 loss: -0.04425531625747681
Batch 5/64 loss: -0.023634016513824463
Batch 6/64 loss: -0.05066496133804321
Batch 7/64 loss: -0.057184040546417236
Batch 8/64 loss: -0.04863440990447998
Batch 9/64 loss: -0.05953294038772583
Batch 10/64 loss: -0.07488435506820679
Batch 11/64 loss: -0.06553608179092407
Batch 12/64 loss: -0.024113893508911133
Batch 13/64 loss: -0.051334500312805176
Batch 14/64 loss: -0.020720839500427246
Batch 15/64 loss: -0.043352365493774414
Batch 16/64 loss: -0.051079511642456055
Batch 17/64 loss: -0.04249817132949829
Batch 18/64 loss: -0.01902914047241211
Batch 19/64 loss: -0.05206543207168579
Batch 20/64 loss: -0.057301998138427734
Batch 21/64 loss: -0.05858480930328369
Batch 22/64 loss: -0.05183148384094238
Batch 23/64 loss: -0.042362332344055176
Batch 24/64 loss: -0.04545015096664429
Batch 25/64 loss: -0.07289552688598633
Batch 26/64 loss: -0.06308400630950928
Batch 27/64 loss: -0.0034407973289489746
Batch 28/64 loss: -0.04036432504653931
Batch 29/64 loss: -0.051320552825927734
Batch 30/64 loss: -0.06371235847473145
Batch 31/64 loss: -0.0637393593788147
Batch 32/64 loss: -0.0317155122756958
Batch 33/64 loss: -0.022120237350463867
Batch 34/64 loss: -0.034665584564208984
Batch 35/64 loss: -0.022198498249053955
Batch 36/64 loss: -0.04486489295959473
Batch 37/64 loss: -0.025734543800354004
Batch 38/64 loss: -0.05305880308151245
Batch 39/64 loss: -0.0392148494720459
Batch 40/64 loss: -0.06236422061920166
Batch 41/64 loss: -0.04709470272064209
Batch 42/64 loss: -0.0464400053024292
Batch 43/64 loss: -0.04092419147491455
Batch 44/64 loss: -0.050061821937561035
Batch 45/64 loss: -0.011469781398773193
Batch 46/64 loss: -0.04539179801940918
Batch 47/64 loss: -0.05161398649215698
Batch 48/64 loss: -0.028954923152923584
Batch 49/64 loss: -0.018609344959259033
Batch 50/64 loss: -0.04523193836212158
Batch 51/64 loss: -0.052022695541381836
Batch 52/64 loss: -0.03987884521484375
Batch 53/64 loss: -0.015862882137298584
Batch 54/64 loss: -0.04328286647796631
Batch 55/64 loss: -0.06186962127685547
Batch 56/64 loss: -0.03804963827133179
Batch 57/64 loss: -0.015408754348754883
Batch 58/64 loss: -0.05785191059112549
Batch 59/64 loss: -0.04702037572860718
Batch 60/64 loss: -0.018926382064819336
Batch 61/64 loss: -0.039119839668273926
Batch 62/64 loss: -0.05068928003311157
Batch 63/64 loss: -0.0629950761795044
Batch 64/64 loss: 0.0023047327995300293
Epoch 183  Train loss: -0.04373323099285949  Val loss: 0.058140726023932915
Epoch 184
-------------------------------
Batch 1/64 loss: -0.03198111057281494
Batch 2/64 loss: -0.01787811517715454
Batch 3/64 loss: -0.035622239112854004
Batch 4/64 loss: -0.03079211711883545
Batch 5/64 loss: -0.017676949501037598
Batch 6/64 loss: -0.05365431308746338
Batch 7/64 loss: -0.0470736026763916
Batch 8/64 loss: -0.061460137367248535
Batch 9/64 loss: -0.054717838764190674
Batch 10/64 loss: -0.04382777214050293
Batch 11/64 loss: -0.050694406032562256
Batch 12/64 loss: -0.057647883892059326
Batch 13/64 loss: -0.059717655181884766
Batch 14/64 loss: -0.04287075996398926
Batch 15/64 loss: -0.04917639493942261
Batch 16/64 loss: -0.05046719312667847
Batch 17/64 loss: -0.06466490030288696
Batch 18/64 loss: -0.06771451234817505
Batch 19/64 loss: -0.05551379919052124
Batch 20/64 loss: -0.032415926456451416
Batch 21/64 loss: -0.017996907234191895
Batch 22/64 loss: -0.03968173265457153
Batch 23/64 loss: -0.059306800365448
Batch 24/64 loss: -0.08299970626831055
Batch 25/64 loss: -0.06094670295715332
Batch 26/64 loss: -0.030276477336883545
Batch 27/64 loss: -0.05740487575531006
Batch 28/64 loss: -0.022551536560058594
Batch 29/64 loss: -0.04152041673660278
Batch 30/64 loss: -0.037103474140167236
Batch 31/64 loss: -0.02498394250869751
Batch 32/64 loss: -0.04500627517700195
Batch 33/64 loss: -0.05562114715576172
Batch 34/64 loss: -0.05363178253173828
Batch 35/64 loss: -0.05739247798919678
Batch 36/64 loss: -0.06672674417495728
Batch 37/64 loss: -0.02927839756011963
Batch 38/64 loss: -0.01864004135131836
Batch 39/64 loss: -0.0511438250541687
Batch 40/64 loss: -0.03759503364562988
Batch 41/64 loss: -0.04434609413146973
Batch 42/64 loss: -0.05848062038421631
Batch 43/64 loss: -0.017226099967956543
Batch 44/64 loss: -0.03613412380218506
Batch 45/64 loss: -0.08476114273071289
Batch 46/64 loss: -0.04693794250488281
Batch 47/64 loss: -0.033078134059906006
Batch 48/64 loss: -0.06785237789154053
Batch 49/64 loss: -0.06185424327850342
Batch 50/64 loss: -0.04311180114746094
Batch 51/64 loss: -0.05347597599029541
Batch 52/64 loss: -0.03235512971878052
Batch 53/64 loss: -0.052403032779693604
Batch 54/64 loss: -0.060576796531677246
Batch 55/64 loss: -0.04494202136993408
Batch 56/64 loss: -0.04845559597015381
Batch 57/64 loss: -0.010730624198913574
Batch 58/64 loss: -0.05366283655166626
Batch 59/64 loss: -0.05306839942932129
Batch 60/64 loss: -0.061104774475097656
Batch 61/64 loss: -0.046101272106170654
Batch 62/64 loss: -0.02380204200744629
Batch 63/64 loss: -0.03656971454620361
Batch 64/64 loss: -0.04659903049468994
Epoch 184  Train loss: -0.04582519204008813  Val loss: 0.0792121971186084
Epoch 185
-------------------------------
Batch 1/64 loss: -0.07499098777770996
Batch 2/64 loss: -0.061024367809295654
Batch 3/64 loss: 0.004582107067108154
Batch 4/64 loss: -0.02310580015182495
Batch 5/64 loss: -0.06552278995513916
Batch 6/64 loss: -0.03561359643936157
Batch 7/64 loss: -0.054521381855010986
Batch 8/64 loss: -0.027193129062652588
Batch 9/64 loss: -0.030461430549621582
Batch 10/64 loss: -0.05793344974517822
Batch 11/64 loss: -0.03910475969314575
Batch 12/64 loss: -0.05069994926452637
Batch 13/64 loss: -0.02905052900314331
Batch 14/64 loss: -0.05292487144470215
Batch 15/64 loss: -0.05209028720855713
Batch 16/64 loss: -0.07370942831039429
Batch 17/64 loss: -0.04895061254501343
Batch 18/64 loss: -0.08917909860610962
Batch 19/64 loss: -0.03033125400543213
Batch 20/64 loss: -0.05785214900970459
Batch 21/64 loss: -0.030628204345703125
Batch 22/64 loss: -0.06479120254516602
Batch 23/64 loss: -0.059734046459198
Batch 24/64 loss: -0.04309433698654175
Batch 25/64 loss: -0.024701237678527832
Batch 26/64 loss: -0.02372145652770996
Batch 27/64 loss: -0.02778416872024536
Batch 28/64 loss: -0.062061309814453125
Batch 29/64 loss: -0.013184726238250732
Batch 30/64 loss: -0.022326111793518066
Batch 31/64 loss: -0.07249993085861206
Batch 32/64 loss: -0.03150707483291626
Batch 33/64 loss: -0.013173997402191162
Batch 34/64 loss: -0.05646252632141113
Batch 35/64 loss: -0.007833242416381836
Batch 36/64 loss: -0.04850614070892334
Batch 37/64 loss: -0.05252063274383545
Batch 38/64 loss: -0.0666971206665039
Batch 39/64 loss: -0.04441934823989868
Batch 40/64 loss: -0.012990772724151611
Batch 41/64 loss: -0.036644816398620605
Batch 42/64 loss: -0.06785809993743896
Batch 43/64 loss: -0.054976582527160645
Batch 44/64 loss: -0.06743848323822021
Batch 45/64 loss: -0.0019472837448120117
Batch 46/64 loss: -0.039777159690856934
Batch 47/64 loss: -0.033255159854888916
Batch 48/64 loss: -0.044667959213256836
Batch 49/64 loss: -0.06565266847610474
Batch 50/64 loss: -0.06094706058502197
Batch 51/64 loss: -0.04323720932006836
Batch 52/64 loss: -0.05153089761734009
Batch 53/64 loss: -0.05072665214538574
Batch 54/64 loss: -0.04410058259963989
Batch 55/64 loss: -0.05065256357192993
Batch 56/64 loss: -0.07011353969573975
Batch 57/64 loss: -0.08384937047958374
Batch 58/64 loss: -0.0835798978805542
Batch 59/64 loss: -0.0660901665687561
Batch 60/64 loss: -0.03468966484069824
Batch 61/64 loss: -0.02557581663131714
Batch 62/64 loss: -0.05580633878707886
Batch 63/64 loss: -0.033174991607666016
Batch 64/64 loss: -0.03621488809585571
Epoch 185  Train loss: -0.04583167819415822  Val loss: 0.110269410298862
Epoch 186
-------------------------------
Batch 1/64 loss: -0.046342551708221436
Batch 2/64 loss: -0.01560896635055542
Batch 3/64 loss: -0.06096458435058594
Batch 4/64 loss: -0.0697677731513977
Batch 5/64 loss: -0.04288095235824585
Batch 6/64 loss: -0.041634559631347656
Batch 7/64 loss: -0.05995810031890869
Batch 8/64 loss: -0.04887127876281738
Batch 9/64 loss: -0.017112016677856445
Batch 10/64 loss: -0.05655425786972046
Batch 11/64 loss: -0.06537449359893799
Batch 12/64 loss: -0.03747737407684326
Batch 13/64 loss: -0.04370677471160889
Batch 14/64 loss: -0.04802274703979492
Batch 15/64 loss: -0.05128747224807739
Batch 16/64 loss: -0.06175333261489868
Batch 17/64 loss: -0.029592812061309814
Batch 18/64 loss: -0.0651392936706543
Batch 19/64 loss: -0.05705052614212036
Batch 20/64 loss: -0.06058776378631592
Batch 21/64 loss: -0.06917273998260498
Batch 22/64 loss: -0.037709951400756836
Batch 23/64 loss: -0.05939292907714844
Batch 24/64 loss: -0.06390100717544556
Batch 25/64 loss: -0.05773746967315674
Batch 26/64 loss: -0.056711554527282715
Batch 27/64 loss: -0.03853154182434082
Batch 28/64 loss: -0.029106855392456055
Batch 29/64 loss: -0.024602174758911133
Batch 30/64 loss: -0.0759325623512268
Batch 31/64 loss: -0.03547614812850952
Batch 32/64 loss: -0.0462685227394104
Batch 33/64 loss: -0.040629565715789795
Batch 34/64 loss: -0.07592469453811646
Batch 35/64 loss: -0.05207479000091553
Batch 36/64 loss: -0.036382853984832764
Batch 37/64 loss: -0.03755456209182739
Batch 38/64 loss: -0.04851841926574707
Batch 39/64 loss: -0.0785987377166748
Batch 40/64 loss: -0.07235372066497803
Batch 41/64 loss: -0.048054516315460205
Batch 42/64 loss: -0.06436306238174438
Batch 43/64 loss: -0.013872981071472168
Batch 44/64 loss: -0.07061856985092163
Batch 45/64 loss: -0.04393261671066284
Batch 46/64 loss: -0.05332601070404053
Batch 47/64 loss: -0.04547560214996338
Batch 48/64 loss: -0.0368955135345459
Batch 49/64 loss: -0.027317166328430176
Batch 50/64 loss: -0.06455832719802856
Batch 51/64 loss: -0.02661144733428955
Batch 52/64 loss: -0.03616875410079956
Batch 53/64 loss: -0.046290814876556396
Batch 54/64 loss: -0.06068229675292969
Batch 55/64 loss: -0.08036136627197266
Batch 56/64 loss: -0.03464043140411377
Batch 57/64 loss: -0.04401040077209473
Batch 58/64 loss: -0.07060658931732178
Batch 59/64 loss: -0.05120950937271118
Batch 60/64 loss: -0.06074327230453491
Batch 61/64 loss: -0.022153019905090332
Batch 62/64 loss: -0.05584961175918579
Batch 63/64 loss: -0.05736875534057617
Batch 64/64 loss: -0.04354572296142578
Epoch 186  Train loss: -0.04963197427637437  Val loss: 0.05833498452537248
Epoch 187
-------------------------------
Batch 1/64 loss: -0.05619800090789795
Batch 2/64 loss: -0.030054926872253418
Batch 3/64 loss: -0.03648930788040161
Batch 4/64 loss: -0.03883767127990723
Batch 5/64 loss: -0.05437493324279785
Batch 6/64 loss: -0.03181052207946777
Batch 7/64 loss: -0.07226115465164185
Batch 8/64 loss: -0.042480528354644775
Batch 9/64 loss: -0.05308520793914795
Batch 10/64 loss: -0.026037216186523438
Batch 11/64 loss: -0.05559748411178589
Batch 12/64 loss: -0.06460875272750854
Batch 13/64 loss: -0.06315863132476807
Batch 14/64 loss: -0.054787278175354004
Batch 15/64 loss: -0.02874577045440674
Batch 16/64 loss: -0.015162348747253418
Batch 17/64 loss: -0.06688350439071655
Batch 18/64 loss: -0.05755507946014404
Batch 19/64 loss: -0.043729186058044434
Batch 20/64 loss: -0.057230472564697266
Batch 21/64 loss: -0.044241905212402344
Batch 22/64 loss: -0.03788524866104126
Batch 23/64 loss: -0.041976749897003174
Batch 24/64 loss: -0.0447002649307251
Batch 25/64 loss: -0.04906123876571655
Batch 26/64 loss: -0.027420103549957275
Batch 27/64 loss: -0.08046633005142212
Batch 28/64 loss: -0.05695265531539917
Batch 29/64 loss: -0.05408835411071777
Batch 30/64 loss: -0.040986835956573486
Batch 31/64 loss: -0.02237558364868164
Batch 32/64 loss: -0.06429862976074219
Batch 33/64 loss: -0.07077878713607788
Batch 34/64 loss: -0.043816566467285156
Batch 35/64 loss: -0.026478052139282227
Batch 36/64 loss: -0.06079459190368652
Batch 37/64 loss: -0.06037163734436035
Batch 38/64 loss: -0.03350663185119629
Batch 39/64 loss: -0.04935520887374878
Batch 40/64 loss: -0.06036490201950073
Batch 41/64 loss: -0.06816422939300537
Batch 42/64 loss: -0.06940734386444092
Batch 43/64 loss: -0.05086326599121094
Batch 44/64 loss: -0.05141347646713257
Batch 45/64 loss: -0.021639227867126465
Batch 46/64 loss: -0.012965083122253418
Batch 47/64 loss: -0.03458434343338013
Batch 48/64 loss: -0.02437412738800049
Batch 49/64 loss: -0.04414576292037964
Batch 50/64 loss: -0.04215902090072632
Batch 51/64 loss: -0.07638216018676758
Batch 52/64 loss: -0.06155204772949219
Batch 53/64 loss: -0.06839108467102051
Batch 54/64 loss: -0.06116926670074463
Batch 55/64 loss: -0.0655221939086914
Batch 56/64 loss: -0.05710721015930176
Batch 57/64 loss: -0.04473954439163208
Batch 58/64 loss: -0.04627835750579834
Batch 59/64 loss: -0.05072832107543945
Batch 60/64 loss: -0.036148250102996826
Batch 61/64 loss: -0.06367939710617065
Batch 62/64 loss: -0.03563344478607178
Batch 63/64 loss: -0.06779325008392334
Batch 64/64 loss: -0.05988854169845581
Epoch 187  Train loss: -0.048921805035834216  Val loss: 0.06179379761423852
Epoch 188
-------------------------------
Batch 1/64 loss: -0.062117934226989746
Batch 2/64 loss: -0.0522456169128418
Batch 3/64 loss: -0.0779646635055542
Batch 4/64 loss: -0.06316483020782471
Batch 5/64 loss: -0.03239816427230835
Batch 6/64 loss: -0.01731252670288086
Batch 7/64 loss: -0.04739964008331299
Batch 8/64 loss: -0.07155722379684448
Batch 9/64 loss: -0.021276116371154785
Batch 10/64 loss: -0.05157119035720825
Batch 11/64 loss: -0.04541432857513428
Batch 12/64 loss: -0.05875444412231445
Batch 13/64 loss: -0.06468355655670166
Batch 14/64 loss: -0.0708540678024292
Batch 15/64 loss: -0.07021212577819824
Batch 16/64 loss: -0.03825497627258301
Batch 17/64 loss: -0.0524938702583313
Batch 18/64 loss: -0.029984712600708008
Batch 19/64 loss: -0.029816031455993652
Batch 20/64 loss: -0.04545098543167114
Batch 21/64 loss: -0.01028221845626831
Batch 22/64 loss: -0.05254155397415161
Batch 23/64 loss: -0.057617783546447754
Batch 24/64 loss: -0.025320768356323242
Batch 25/64 loss: -0.02581918239593506
Batch 26/64 loss: -0.04864472150802612
Batch 27/64 loss: -0.0135270357131958
Batch 28/64 loss: -0.038533568382263184
Batch 29/64 loss: -0.04309070110321045
Batch 30/64 loss: -0.021950364112854004
Batch 31/64 loss: -0.04566091299057007
Batch 32/64 loss: -0.046904027462005615
Batch 33/64 loss: -0.07074844837188721
Batch 34/64 loss: -0.028708577156066895
Batch 35/64 loss: -0.03871661424636841
Batch 36/64 loss: -0.06987667083740234
Batch 37/64 loss: -0.04624819755554199
Batch 38/64 loss: -0.07156133651733398
Batch 39/64 loss: -0.03291541337966919
Batch 40/64 loss: -0.04979825019836426
Batch 41/64 loss: -0.05458563566207886
Batch 42/64 loss: -0.08426910638809204
Batch 43/64 loss: -0.04404383897781372
Batch 44/64 loss: -0.035547733306884766
Batch 45/64 loss: -0.02840667963027954
Batch 46/64 loss: -0.05780971050262451
Batch 47/64 loss: -0.0563468337059021
Batch 48/64 loss: -0.048281073570251465
Batch 49/64 loss: -0.05922985076904297
Batch 50/64 loss: -0.06286871433258057
Batch 51/64 loss: -0.05618560314178467
Batch 52/64 loss: -0.031632959842681885
Batch 53/64 loss: -0.062326252460479736
Batch 54/64 loss: -0.04881864786148071
Batch 55/64 loss: -0.02180933952331543
Batch 56/64 loss: -0.019198060035705566
Batch 57/64 loss: -0.03240090608596802
Batch 58/64 loss: -0.03837937116622925
Batch 59/64 loss: -0.04345130920410156
Batch 60/64 loss: -0.045119643211364746
Batch 61/64 loss: -0.05019253492355347
Batch 62/64 loss: -0.06659489870071411
Batch 63/64 loss: -0.06834971904754639
Batch 64/64 loss: -0.04273873567581177
Epoch 188  Train loss: -0.04689091490764244  Val loss: 0.05078350248205703
Saving best model, epoch: 188
Epoch 189
-------------------------------
Batch 1/64 loss: -0.054380059242248535
Batch 2/64 loss: -0.06494677066802979
Batch 3/64 loss: -0.05988049507141113
Batch 4/64 loss: -0.05272620916366577
Batch 5/64 loss: -0.07341992855072021
Batch 6/64 loss: -0.05649912357330322
Batch 7/64 loss: -0.02856731414794922
Batch 8/64 loss: -0.08080637454986572
Batch 9/64 loss: -0.05118703842163086
Batch 10/64 loss: -0.058013737201690674
Batch 11/64 loss: -0.06838583946228027
Batch 12/64 loss: -0.04546642303466797
Batch 13/64 loss: -0.08389341831207275
Batch 14/64 loss: -0.059023916721343994
Batch 15/64 loss: -0.056118667125701904
Batch 16/64 loss: -0.05353063344955444
Batch 17/64 loss: -0.043357253074645996
Batch 18/64 loss: 0.004220843315124512
Batch 19/64 loss: -0.08765840530395508
Batch 20/64 loss: -0.05887115001678467
Batch 21/64 loss: -0.003915071487426758
Batch 22/64 loss: -0.04713249206542969
Batch 23/64 loss: -0.02973717451095581
Batch 24/64 loss: -0.07767462730407715
Batch 25/64 loss: -0.07642096281051636
Batch 26/64 loss: -0.04650139808654785
Batch 27/64 loss: -0.04005885124206543
Batch 28/64 loss: -0.04129612445831299
Batch 29/64 loss: -0.05252581834793091
Batch 30/64 loss: -0.056856751441955566
Batch 31/64 loss: -0.05857962369918823
Batch 32/64 loss: -0.061479151248931885
Batch 33/64 loss: -0.06445485353469849
Batch 34/64 loss: -0.023387789726257324
Batch 35/64 loss: -0.04777127504348755
Batch 36/64 loss: -0.05601227283477783
Batch 37/64 loss: -0.05670684576034546
Batch 38/64 loss: -0.031108975410461426
Batch 39/64 loss: -0.04235959053039551
Batch 40/64 loss: -0.053891539573669434
Batch 41/64 loss: -0.04255169630050659
Batch 42/64 loss: -0.07061374187469482
Batch 43/64 loss: -0.07028239965438843
Batch 44/64 loss: -0.022949397563934326
Batch 45/64 loss: -0.032663702964782715
Batch 46/64 loss: -0.057721078395843506
Batch 47/64 loss: -0.07163739204406738
Batch 48/64 loss: -0.030847668647766113
Batch 49/64 loss: -0.08070248365402222
Batch 50/64 loss: -0.0547177791595459
Batch 51/64 loss: -0.036411821842193604
Batch 52/64 loss: -0.0533410906791687
Batch 53/64 loss: -0.03694438934326172
Batch 54/64 loss: -0.05072379112243652
Batch 55/64 loss: -0.04101109504699707
Batch 56/64 loss: -0.04373568296432495
Batch 57/64 loss: -0.04641669988632202
Batch 58/64 loss: -0.018185734748840332
Batch 59/64 loss: -0.04137098789215088
Batch 60/64 loss: -0.053587257862091064
Batch 61/64 loss: -0.06691223382949829
Batch 62/64 loss: -0.04456162452697754
Batch 63/64 loss: -0.08026343584060669
Batch 64/64 loss: -0.08344560861587524
Epoch 189  Train loss: -0.05193914502274757  Val loss: 0.05781622379506167
Epoch 190
-------------------------------
Batch 1/64 loss: -0.03223097324371338
Batch 2/64 loss: -0.02580881118774414
Batch 3/64 loss: -0.08706009387969971
Batch 4/64 loss: -0.06788969039916992
Batch 5/64 loss: 0.000961005687713623
Batch 6/64 loss: -0.004552066326141357
Batch 7/64 loss: -0.07095634937286377
Batch 8/64 loss: -0.06254732608795166
Batch 9/64 loss: -0.06338036060333252
Batch 10/64 loss: -0.07711511850357056
Batch 11/64 loss: -0.05931323766708374
Batch 12/64 loss: -0.07412773370742798
Batch 13/64 loss: -0.07457494735717773
Batch 14/64 loss: -0.032961905002593994
Batch 15/64 loss: -0.038454413414001465
Batch 16/64 loss: -0.09328687191009521
Batch 17/64 loss: -0.051496028900146484
Batch 18/64 loss: -0.018731534481048584
Batch 19/64 loss: -0.04258120059967041
Batch 20/64 loss: -0.03679347038269043
Batch 21/64 loss: -0.04198646545410156
Batch 22/64 loss: -0.037909090518951416
Batch 23/64 loss: -0.05917251110076904
Batch 24/64 loss: -0.06969410181045532
Batch 25/64 loss: -0.03174400329589844
Batch 26/64 loss: -0.027411222457885742
Batch 27/64 loss: -0.04511445760726929
Batch 28/64 loss: -0.05273914337158203
Batch 29/64 loss: -0.05569267272949219
Batch 30/64 loss: -0.07836270332336426
Batch 31/64 loss: -0.04843860864639282
Batch 32/64 loss: -0.05588144063949585
Batch 33/64 loss: -0.06036585569381714
Batch 34/64 loss: -0.05895596742630005
Batch 35/64 loss: -0.05242854356765747
Batch 36/64 loss: -0.07683658599853516
Batch 37/64 loss: -0.06753528118133545
Batch 38/64 loss: -0.04772365093231201
Batch 39/64 loss: -0.04193079471588135
Batch 40/64 loss: -0.06608277559280396
Batch 41/64 loss: -0.007585465908050537
Batch 42/64 loss: -0.06741929054260254
Batch 43/64 loss: -0.0651240348815918
Batch 44/64 loss: -0.031181693077087402
Batch 45/64 loss: -0.04808080196380615
Batch 46/64 loss: -0.05204516649246216
Batch 47/64 loss: -0.027753353118896484
Batch 48/64 loss: -0.04533374309539795
Batch 49/64 loss: -0.06485962867736816
Batch 50/64 loss: -0.06345957517623901
Batch 51/64 loss: -0.08293807506561279
Batch 52/64 loss: -0.05770236253738403
Batch 53/64 loss: -0.057789504528045654
Batch 54/64 loss: -0.06511485576629639
Batch 55/64 loss: -0.04352349042892456
Batch 56/64 loss: -0.05378222465515137
Batch 57/64 loss: -0.05661332607269287
Batch 58/64 loss: -0.07261300086975098
Batch 59/64 loss: -0.011081516742706299
Batch 60/64 loss: -0.05571389198303223
Batch 61/64 loss: -0.06666481494903564
Batch 62/64 loss: -0.037707626819610596
Batch 63/64 loss: -0.06900560855865479
Batch 64/64 loss: -0.08672285079956055
Epoch 190  Train loss: -0.05265948538686715  Val loss: 0.05892672714908508
Epoch 191
-------------------------------
Batch 1/64 loss: -0.061197519302368164
Batch 2/64 loss: -0.054433584213256836
Batch 3/64 loss: -0.06414544582366943
Batch 4/64 loss: -0.03108084201812744
Batch 5/64 loss: -0.007953166961669922
Batch 6/64 loss: -0.061354637145996094
Batch 7/64 loss: -0.0715261697769165
Batch 8/64 loss: -0.06757265329360962
Batch 9/64 loss: -0.053703904151916504
Batch 10/64 loss: -0.06757593154907227
Batch 11/64 loss: -0.06694036722183228
Batch 12/64 loss: -0.025194644927978516
Batch 13/64 loss: -0.04822057485580444
Batch 14/64 loss: -0.013741135597229004
Batch 15/64 loss: -0.041636526584625244
Batch 16/64 loss: -0.08917510509490967
Batch 17/64 loss: -0.05214393138885498
Batch 18/64 loss: -0.05354946851730347
Batch 19/64 loss: -0.06035768985748291
Batch 20/64 loss: -0.04726666212081909
Batch 21/64 loss: -0.05937141180038452
Batch 22/64 loss: -0.027857542037963867
Batch 23/64 loss: -0.05029261112213135
Batch 24/64 loss: -0.0776292085647583
Batch 25/64 loss: -0.05649036169052124
Batch 26/64 loss: -0.022500157356262207
Batch 27/64 loss: -0.0771746039390564
Batch 28/64 loss: -0.04553794860839844
Batch 29/64 loss: -0.044544339179992676
Batch 30/64 loss: -0.0369412899017334
Batch 31/64 loss: -0.022699713706970215
Batch 32/64 loss: -0.039659738540649414
Batch 33/64 loss: -0.052998900413513184
Batch 34/64 loss: -0.04106944799423218
Batch 35/64 loss: -0.0382271409034729
Batch 36/64 loss: -0.06790721416473389
Batch 37/64 loss: -0.024629592895507812
Batch 38/64 loss: -0.04616546630859375
Batch 39/64 loss: -0.05196201801300049
Batch 40/64 loss: -0.05015301704406738
Batch 41/64 loss: -0.04214900732040405
Batch 42/64 loss: -0.034321606159210205
Batch 43/64 loss: -0.036759376525878906
Batch 44/64 loss: -0.07551980018615723
Batch 45/64 loss: -0.07705920934677124
Batch 46/64 loss: -0.06695389747619629
Batch 47/64 loss: -0.059832870960235596
Batch 48/64 loss: -0.05282187461853027
Batch 49/64 loss: -0.07088238000869751
Batch 50/64 loss: -0.053772032260894775
Batch 51/64 loss: -0.05904436111450195
Batch 52/64 loss: -0.049357712268829346
Batch 53/64 loss: -0.057476162910461426
Batch 54/64 loss: -0.034665822982788086
Batch 55/64 loss: -0.029085159301757812
Batch 56/64 loss: -0.054917216300964355
Batch 57/64 loss: -0.07311433553695679
Batch 58/64 loss: -0.04201972484588623
Batch 59/64 loss: -0.06433254480361938
Batch 60/64 loss: -0.04090380668640137
Batch 61/64 loss: -0.01665860414505005
Batch 62/64 loss: -0.05490303039550781
Batch 63/64 loss: -0.05462193489074707
Batch 64/64 loss: -0.0725400447845459
Epoch 191  Train loss: -0.05063779120351754  Val loss: 0.05292003793814748
Epoch 192
-------------------------------
Batch 1/64 loss: -0.06917417049407959
Batch 2/64 loss: -0.04661685228347778
Batch 3/64 loss: -0.03740561008453369
Batch 4/64 loss: -0.05748295783996582
Batch 5/64 loss: -0.04910784959793091
Batch 6/64 loss: -0.014431297779083252
Batch 7/64 loss: -0.07745903730392456
Batch 8/64 loss: -0.02729976177215576
Batch 9/64 loss: -0.05569791793823242
Batch 10/64 loss: -0.05581545829772949
Batch 11/64 loss: -0.015905797481536865
Batch 12/64 loss: -0.06530612707138062
Batch 13/64 loss: -0.06271010637283325
Batch 14/64 loss: -0.05907285213470459
Batch 15/64 loss: -0.0540308952331543
Batch 16/64 loss: -0.05393761396408081
Batch 17/64 loss: -0.06235390901565552
Batch 18/64 loss: -0.07552683353424072
Batch 19/64 loss: -0.057361066341400146
Batch 20/64 loss: -0.04856312274932861
Batch 21/64 loss: -0.0399554967880249
Batch 22/64 loss: -0.04362767934799194
Batch 23/64 loss: -0.05120658874511719
Batch 24/64 loss: -0.05153524875640869
Batch 25/64 loss: -0.07404708862304688
Batch 26/64 loss: -0.06213116645812988
Batch 27/64 loss: -0.06587100028991699
Batch 28/64 loss: -0.050638794898986816
Batch 29/64 loss: -0.034779489040374756
Batch 30/64 loss: -0.030148565769195557
Batch 31/64 loss: -0.033223509788513184
Batch 32/64 loss: -0.0414242148399353
Batch 33/64 loss: -0.044904232025146484
Batch 34/64 loss: -0.02640700340270996
Batch 35/64 loss: -0.07035624980926514
Batch 36/64 loss: -0.06366539001464844
Batch 37/64 loss: -0.07279050350189209
Batch 38/64 loss: -0.06843066215515137
Batch 39/64 loss: -0.02551513910293579
Batch 40/64 loss: -0.0425494909286499
Batch 41/64 loss: -0.015026509761810303
Batch 42/64 loss: -0.028328657150268555
Batch 43/64 loss: -0.05806076526641846
Batch 44/64 loss: -0.07552415132522583
Batch 45/64 loss: -0.0444071888923645
Batch 46/64 loss: -0.045136094093322754
Batch 47/64 loss: -0.06445801258087158
Batch 48/64 loss: -0.06300944089889526
Batch 49/64 loss: -0.03796738386154175
Batch 50/64 loss: -0.04905587434768677
Batch 51/64 loss: -0.0342717170715332
Batch 52/64 loss: -0.08031082153320312
Batch 53/64 loss: -0.03679168224334717
Batch 54/64 loss: -0.06190598011016846
Batch 55/64 loss: -0.08548307418823242
Batch 56/64 loss: -0.05470317602157593
Batch 57/64 loss: -0.05276226997375488
Batch 58/64 loss: -0.05246454477310181
Batch 59/64 loss: -0.0361708402633667
Batch 60/64 loss: -0.02668130397796631
Batch 61/64 loss: -0.04896950721740723
Batch 62/64 loss: -0.05741184949874878
Batch 63/64 loss: -0.06109815835952759
Batch 64/64 loss: -0.06080281734466553
Epoch 192  Train loss: -0.05104420185089111  Val loss: 0.05463463170421902
Epoch 193
-------------------------------
Batch 1/64 loss: -0.061212778091430664
Batch 2/64 loss: -0.0714040994644165
Batch 3/64 loss: -0.02734607458114624
Batch 4/64 loss: -0.04228925704956055
Batch 5/64 loss: -0.03243666887283325
Batch 6/64 loss: -0.06091111898422241
Batch 7/64 loss: -0.053558170795440674
Batch 8/64 loss: -0.04608356952667236
Batch 9/64 loss: -0.08460724353790283
Batch 10/64 loss: -0.08733421564102173
Batch 11/64 loss: -0.08917200565338135
Batch 12/64 loss: -0.07077211141586304
Batch 13/64 loss: -0.019731223583221436
Batch 14/64 loss: -0.052374422550201416
Batch 15/64 loss: -0.04917246103286743
Batch 16/64 loss: -0.03321397304534912
Batch 17/64 loss: -0.06179094314575195
Batch 18/64 loss: -0.04444241523742676
Batch 19/64 loss: -0.06282913684844971
Batch 20/64 loss: -0.022873401641845703
Batch 21/64 loss: -0.042696237564086914
Batch 22/64 loss: -0.0709577202796936
Batch 23/64 loss: -0.06546664237976074
Batch 24/64 loss: -0.046938955783843994
Batch 25/64 loss: -0.04502153396606445
Batch 26/64 loss: -0.06265336275100708
Batch 27/64 loss: -0.05239206552505493
Batch 28/64 loss: -0.03695958852767944
Batch 29/64 loss: -0.045558929443359375
Batch 30/64 loss: -0.05469638109207153
Batch 31/64 loss: -0.06168121099472046
Batch 32/64 loss: -0.049576640129089355
Batch 33/64 loss: -0.05399376153945923
Batch 34/64 loss: -0.03633224964141846
Batch 35/64 loss: -0.04284250736236572
Batch 36/64 loss: -0.07365548610687256
Batch 37/64 loss: -0.02971118688583374
Batch 38/64 loss: -0.07592153549194336
Batch 39/64 loss: -0.018991708755493164
Batch 40/64 loss: -0.036224544048309326
Batch 41/64 loss: -0.04785275459289551
Batch 42/64 loss: -0.07108688354492188
Batch 43/64 loss: -0.026235580444335938
Batch 44/64 loss: -0.08770567178726196
Batch 45/64 loss: -0.05464226007461548
Batch 46/64 loss: -0.03065502643585205
Batch 47/64 loss: -0.031596362590789795
Batch 48/64 loss: -0.0486220121383667
Batch 49/64 loss: -0.04960334300994873
Batch 50/64 loss: -0.028864264488220215
Batch 51/64 loss: -0.03281372785568237
Batch 52/64 loss: -0.04746806621551514
Batch 53/64 loss: -0.06734752655029297
Batch 54/64 loss: -0.06656807661056519
Batch 55/64 loss: -0.060373783111572266
Batch 56/64 loss: -0.053393661975860596
Batch 57/64 loss: -0.058512091636657715
Batch 58/64 loss: -0.06931591033935547
Batch 59/64 loss: -0.060008347034454346
Batch 60/64 loss: -0.05801337957382202
Batch 61/64 loss: -0.06850755214691162
Batch 62/64 loss: -0.07326632738113403
Batch 63/64 loss: -0.07025539875030518
Batch 64/64 loss: -0.032437801361083984
Epoch 193  Train loss: -0.052750806247486785  Val loss: 0.0555850361630679
Epoch 194
-------------------------------
Batch 1/64 loss: -0.03682827949523926
Batch 2/64 loss: -0.06875324249267578
Batch 3/64 loss: -0.04391700029373169
Batch 4/64 loss: -0.04923248291015625
Batch 5/64 loss: -0.014827072620391846
Batch 6/64 loss: -0.07044607400894165
Batch 7/64 loss: -0.057830095291137695
Batch 8/64 loss: -0.079775869846344
Batch 9/64 loss: -0.05837887525558472
Batch 10/64 loss: -0.058982908725738525
Batch 11/64 loss: -0.07552725076675415
Batch 12/64 loss: -0.07437628507614136
Batch 13/64 loss: -0.05939966440200806
Batch 14/64 loss: -0.06174588203430176
Batch 15/64 loss: -0.04953283071517944
Batch 16/64 loss: -0.05268353223800659
Batch 17/64 loss: -0.042539358139038086
Batch 18/64 loss: -0.037336111068725586
Batch 19/64 loss: -0.040922343730926514
Batch 20/64 loss: -0.07918453216552734
Batch 21/64 loss: -0.05626487731933594
Batch 22/64 loss: -0.048285484313964844
Batch 23/64 loss: -0.05469679832458496
Batch 24/64 loss: -0.08637881278991699
Batch 25/64 loss: -0.05924898386001587
Batch 26/64 loss: -0.06179308891296387
Batch 27/64 loss: -0.010156691074371338
Batch 28/64 loss: -0.025187373161315918
Batch 29/64 loss: -0.0727047324180603
Batch 30/64 loss: -0.08001905679702759
Batch 31/64 loss: -0.06614464521408081
Batch 32/64 loss: -0.054499030113220215
Batch 33/64 loss: -0.0876007080078125
Batch 34/64 loss: -0.041291654109954834
Batch 35/64 loss: -0.050051212310791016
Batch 36/64 loss: -0.04658550024032593
Batch 37/64 loss: -0.04558819532394409
Batch 38/64 loss: -0.039015889167785645
Batch 39/64 loss: -0.05230140686035156
Batch 40/64 loss: -0.06553477048873901
Batch 41/64 loss: -0.029503226280212402
Batch 42/64 loss: -0.029996812343597412
Batch 43/64 loss: -0.029043078422546387
Batch 44/64 loss: -0.04508793354034424
Batch 45/64 loss: -0.03554189205169678
Batch 46/64 loss: -0.07525962591171265
Batch 47/64 loss: -0.04307752847671509
Batch 48/64 loss: -0.03831702470779419
Batch 49/64 loss: -0.03402101993560791
Batch 50/64 loss: -0.06323850154876709
Batch 51/64 loss: -0.053731322288513184
Batch 52/64 loss: -0.09282088279724121
Batch 53/64 loss: -0.06768441200256348
Batch 54/64 loss: -0.0689535140991211
Batch 55/64 loss: -0.05837750434875488
Batch 56/64 loss: -0.029271602630615234
Batch 57/64 loss: -0.091488778591156
Batch 58/64 loss: -0.0662124752998352
Batch 59/64 loss: -0.06627297401428223
Batch 60/64 loss: -0.02068507671356201
Batch 61/64 loss: -0.020826518535614014
Batch 62/64 loss: -0.06462746858596802
Batch 63/64 loss: -0.028719007968902588
Batch 64/64 loss: -0.013662338256835938
Epoch 194  Train loss: -0.05299723195094688  Val loss: 0.0531290517639868
Epoch 195
-------------------------------
Batch 1/64 loss: -0.05169552564620972
Batch 2/64 loss: -0.08936470746994019
Batch 3/64 loss: -0.04240208864212036
Batch 4/64 loss: -0.06740999221801758
Batch 5/64 loss: -0.05096399784088135
Batch 6/64 loss: -0.09171026945114136
Batch 7/64 loss: -0.05392718315124512
Batch 8/64 loss: -0.01631021499633789
Batch 9/64 loss: -0.05274933576583862
Batch 10/64 loss: -0.06218808889389038
Batch 11/64 loss: -0.0059719085693359375
Batch 12/64 loss: -0.06343317031860352
Batch 13/64 loss: -0.06671243906021118
Batch 14/64 loss: -0.0697622299194336
Batch 15/64 loss: -0.06005716323852539
Batch 16/64 loss: -0.035608887672424316
Batch 17/64 loss: -0.05159604549407959
Batch 18/64 loss: -0.04938685894012451
Batch 19/64 loss: -0.05199289321899414
Batch 20/64 loss: -0.04223579168319702
Batch 21/64 loss: -0.04907524585723877
Batch 22/64 loss: -0.042044997215270996
Batch 23/64 loss: -0.05113852024078369
Batch 24/64 loss: -0.03751087188720703
Batch 25/64 loss: -0.06390571594238281
Batch 26/64 loss: -0.054485201835632324
Batch 27/64 loss: -0.07097947597503662
Batch 28/64 loss: -0.07207536697387695
Batch 29/64 loss: -0.03968799114227295
Batch 30/64 loss: -0.06882590055465698
Batch 31/64 loss: -0.04755133390426636
Batch 32/64 loss: -0.07721960544586182
Batch 33/64 loss: -0.07720720767974854
Batch 34/64 loss: -0.07179546356201172
Batch 35/64 loss: -0.05519723892211914
Batch 36/64 loss: -0.04423028230667114
Batch 37/64 loss: -0.06515884399414062
Batch 38/64 loss: -0.053556859493255615
Batch 39/64 loss: -0.040362656116485596
Batch 40/64 loss: -0.07929199934005737
Batch 41/64 loss: -0.08499503135681152
Batch 42/64 loss: -0.0879371166229248
Batch 43/64 loss: -0.06489789485931396
Batch 44/64 loss: -0.07880270481109619
Batch 45/64 loss: -0.01216888427734375
Batch 46/64 loss: -0.0694434642791748
Batch 47/64 loss: -0.041069865226745605
Batch 48/64 loss: -0.05410921573638916
Batch 49/64 loss: -0.01678621768951416
Batch 50/64 loss: -0.03829169273376465
Batch 51/64 loss: -0.028545022010803223
Batch 52/64 loss: -0.046930789947509766
Batch 53/64 loss: -0.022566616535186768
Batch 54/64 loss: -0.0639607310295105
Batch 55/64 loss: -0.07840442657470703
Batch 56/64 loss: -0.05013185739517212
Batch 57/64 loss: -0.04302525520324707
Batch 58/64 loss: -0.04205554723739624
Batch 59/64 loss: -0.05683636665344238
Batch 60/64 loss: -0.05318373441696167
Batch 61/64 loss: -0.065421462059021
Batch 62/64 loss: -0.05889475345611572
Batch 63/64 loss: -0.029815316200256348
Batch 64/64 loss: -0.07973247766494751
Epoch 195  Train loss: -0.05466435913946115  Val loss: 0.0578726370310046
Epoch 196
-------------------------------
Batch 1/64 loss: -0.025606632232666016
Batch 2/64 loss: -0.0948190689086914
Batch 3/64 loss: -0.08940726518630981
Batch 4/64 loss: -0.046855628490448
Batch 5/64 loss: -0.060936033725738525
Batch 6/64 loss: -0.053414106369018555
Batch 7/64 loss: -0.08632421493530273
Batch 8/64 loss: -0.07036834955215454
Batch 9/64 loss: -0.06303811073303223
Batch 10/64 loss: -0.07007044553756714
Batch 11/64 loss: -0.044869303703308105
Batch 12/64 loss: -0.0687708854675293
Batch 13/64 loss: -0.023157238960266113
Batch 14/64 loss: -0.07290446758270264
Batch 15/64 loss: -0.04730898141860962
Batch 16/64 loss: -0.011560797691345215
Batch 17/64 loss: -0.07000327110290527
Batch 18/64 loss: -0.06687742471694946
Batch 19/64 loss: -0.04145258665084839
Batch 20/64 loss: -0.04925107955932617
Batch 21/64 loss: -0.05445551872253418
Batch 22/64 loss: -0.03374171257019043
Batch 23/64 loss: -0.06445688009262085
Batch 24/64 loss: -0.03758448362350464
Batch 25/64 loss: -0.03706562519073486
Batch 26/64 loss: -0.07865071296691895
Batch 27/64 loss: -0.061048805713653564
Batch 28/64 loss: -0.0717085599899292
Batch 29/64 loss: -0.057826101779937744
Batch 30/64 loss: -0.07587391138076782
Batch 31/64 loss: -0.05527663230895996
Batch 32/64 loss: -0.06284952163696289
Batch 33/64 loss: -0.05850517749786377
Batch 34/64 loss: -0.058373451232910156
Batch 35/64 loss: -0.06314492225646973
Batch 36/64 loss: -0.02189880609512329
Batch 37/64 loss: -0.04872941970825195
Batch 38/64 loss: -0.07139217853546143
Batch 39/64 loss: -0.02065986394882202
Batch 40/64 loss: -0.06799066066741943
Batch 41/64 loss: -0.03591197729110718
Batch 42/64 loss: -0.04984605312347412
Batch 43/64 loss: -0.04514998197555542
Batch 44/64 loss: -0.0578421950340271
Batch 45/64 loss: -0.038736045360565186
Batch 46/64 loss: -0.04932373762130737
Batch 47/64 loss: -0.04155707359313965
Batch 48/64 loss: -0.05690455436706543
Batch 49/64 loss: -0.048583805561065674
Batch 50/64 loss: -0.08553880453109741
Batch 51/64 loss: -0.07445728778839111
Batch 52/64 loss: -0.07604700326919556
Batch 53/64 loss: -0.056607067584991455
Batch 54/64 loss: -0.06477904319763184
Batch 55/64 loss: -0.04860323667526245
Batch 56/64 loss: -0.04599809646606445
Batch 57/64 loss: -0.07058626413345337
Batch 58/64 loss: -0.04535001516342163
Batch 59/64 loss: -0.06450545787811279
Batch 60/64 loss: -0.05052995681762695
Batch 61/64 loss: -0.049476683139801025
Batch 62/64 loss: -0.07790130376815796
Batch 63/64 loss: -0.0375637412071228
Batch 64/64 loss: -0.07378405332565308
Epoch 196  Train loss: -0.056241039435068765  Val loss: 0.05786267895878795
Epoch 197
-------------------------------
Batch 1/64 loss: -0.07471728324890137
Batch 2/64 loss: -0.07971459627151489
Batch 3/64 loss: -0.07539600133895874
Batch 4/64 loss: -0.07003539800643921
Batch 5/64 loss: -0.07300758361816406
Batch 6/64 loss: -0.04403090476989746
Batch 7/64 loss: -0.0935826301574707
Batch 8/64 loss: -0.03459525108337402
Batch 9/64 loss: -0.047502994537353516
Batch 10/64 loss: -0.07322371006011963
Batch 11/64 loss: -0.03848707675933838
Batch 12/64 loss: -0.02810591459274292
Batch 13/64 loss: -0.07105177640914917
Batch 14/64 loss: -0.06024956703186035
Batch 15/64 loss: -0.046953558921813965
Batch 16/64 loss: -0.05429774522781372
Batch 17/64 loss: -0.057291269302368164
Batch 18/64 loss: -0.05691951513290405
Batch 19/64 loss: -0.02231287956237793
Batch 20/64 loss: -0.03598511219024658
Batch 21/64 loss: -0.051434099674224854
Batch 22/64 loss: -0.05454224348068237
Batch 23/64 loss: -0.06146836280822754
Batch 24/64 loss: -0.02613002061843872
Batch 25/64 loss: -0.060717642307281494
Batch 26/64 loss: -0.08494889736175537
Batch 27/64 loss: -0.06596702337265015
Batch 28/64 loss: -0.06009864807128906
Batch 29/64 loss: -0.06212759017944336
Batch 30/64 loss: -0.05942034721374512
Batch 31/64 loss: -0.06818598508834839
Batch 32/64 loss: -0.06822973489761353
Batch 33/64 loss: -0.04483109712600708
Batch 34/64 loss: -0.08111822605133057
Batch 35/64 loss: -0.04312264919281006
Batch 36/64 loss: -0.08033114671707153
Batch 37/64 loss: -0.05214667320251465
Batch 38/64 loss: -0.0928717851638794
Batch 39/64 loss: -0.05216991901397705
Batch 40/64 loss: -0.006039023399353027
Batch 41/64 loss: -0.05483514070510864
Batch 42/64 loss: -0.06452816724777222
Batch 43/64 loss: -0.04422640800476074
Batch 44/64 loss: -0.06365066766738892
Batch 45/64 loss: -0.028331875801086426
Batch 46/64 loss: -0.03473067283630371
Batch 47/64 loss: -0.009161829948425293
Batch 48/64 loss: -0.02049356698989868
Batch 49/64 loss: -0.07190263271331787
Batch 50/64 loss: -0.0676724910736084
Batch 51/64 loss: -0.057100892066955566
Batch 52/64 loss: -0.06990706920623779
Batch 53/64 loss: -0.05792587995529175
Batch 54/64 loss: -0.0846053957939148
Batch 55/64 loss: -0.06722617149353027
Batch 56/64 loss: -0.04913187026977539
Batch 57/64 loss: -0.07543396949768066
Batch 58/64 loss: -0.045313894748687744
Batch 59/64 loss: -0.06435525417327881
Batch 60/64 loss: -0.03432822227478027
Batch 61/64 loss: -0.03118419647216797
Batch 62/64 loss: -0.062458157539367676
Batch 63/64 loss: -0.058704495429992676
Batch 64/64 loss: -0.05803322792053223
Epoch 197  Train loss: -0.05606424668255974  Val loss: 0.07608049595888537
Epoch 198
-------------------------------
Batch 1/64 loss: -0.04410034418106079
Batch 2/64 loss: -0.0493931770324707
Batch 3/64 loss: -0.06076931953430176
Batch 4/64 loss: -0.041726112365722656
Batch 5/64 loss: -0.07653701305389404
Batch 6/64 loss: -0.05282425880432129
Batch 7/64 loss: -0.048800528049468994
Batch 8/64 loss: -0.05361872911453247
Batch 9/64 loss: -0.04966163635253906
Batch 10/64 loss: -0.021819114685058594
Batch 11/64 loss: -0.0641932487487793
Batch 12/64 loss: -0.044467031955718994
Batch 13/64 loss: -0.06722515821456909
Batch 14/64 loss: -0.042103588581085205
Batch 15/64 loss: -0.08216631412506104
Batch 16/64 loss: -0.06600689888000488
Batch 17/64 loss: -0.08035451173782349
Batch 18/64 loss: -0.036458611488342285
Batch 19/64 loss: -0.04155170917510986
Batch 20/64 loss: -0.048317134380340576
Batch 21/64 loss: -0.08507239818572998
Batch 22/64 loss: -0.05145448446273804
Batch 23/64 loss: -0.07791608572006226
Batch 24/64 loss: -0.06594717502593994
Batch 25/64 loss: -0.05753183364868164
Batch 26/64 loss: -0.04026442766189575
Batch 27/64 loss: -0.028388798236846924
Batch 28/64 loss: -0.04989957809448242
Batch 29/64 loss: -0.07196825742721558
Batch 30/64 loss: -0.04264998435974121
Batch 31/64 loss: -0.03173220157623291
Batch 32/64 loss: -0.06965351104736328
Batch 33/64 loss: -0.05017179250717163
Batch 34/64 loss: -0.05541449785232544
Batch 35/64 loss: -0.046133220195770264
Batch 36/64 loss: -0.08130371570587158
Batch 37/64 loss: -0.07299304008483887
Batch 38/64 loss: -0.055620670318603516
Batch 39/64 loss: -0.023185312747955322
Batch 40/64 loss: -0.07939094305038452
Batch 41/64 loss: -0.08793461322784424
Batch 42/64 loss: -0.03495526313781738
Batch 43/64 loss: -0.0757979154586792
Batch 44/64 loss: -0.06752485036849976
Batch 45/64 loss: -0.07559323310852051
Batch 46/64 loss: -0.031430959701538086
Batch 47/64 loss: -0.047937989234924316
Batch 48/64 loss: -0.05131852626800537
Batch 49/64 loss: -0.044913411140441895
Batch 50/64 loss: -0.04395246505737305
Batch 51/64 loss: -0.08405929803848267
Batch 52/64 loss: -0.05842643976211548
Batch 53/64 loss: -0.04653817415237427
Batch 54/64 loss: -0.04174041748046875
Batch 55/64 loss: -0.06894832849502563
Batch 56/64 loss: -0.07450807094573975
Batch 57/64 loss: -0.02734553813934326
Batch 58/64 loss: -0.0289536714553833
Batch 59/64 loss: -0.0648924708366394
Batch 60/64 loss: -0.0823480486869812
Batch 61/64 loss: -0.08518987894058228
Batch 62/64 loss: -0.030650794506072998
Batch 63/64 loss: -0.026310861110687256
Batch 64/64 loss: -0.05483508110046387
Epoch 198  Train loss: -0.055391119975669714  Val loss: 0.06162473016588139
Epoch 199
-------------------------------
Batch 1/64 loss: -0.06694352626800537
Batch 2/64 loss: -0.027341842651367188
Batch 3/64 loss: -0.08385133743286133
Batch 4/64 loss: -0.05801421403884888
Batch 5/64 loss: -0.056872010231018066
Batch 6/64 loss: -0.07048678398132324
Batch 7/64 loss: -0.044084787368774414
Batch 8/64 loss: -0.061920106410980225
Batch 9/64 loss: -0.07148319482803345
Batch 10/64 loss: -0.05885970592498779
Batch 11/64 loss: -0.02789449691772461
Batch 12/64 loss: -0.06678950786590576
Batch 13/64 loss: -0.043208956718444824
Batch 14/64 loss: -0.044319331645965576
Batch 15/64 loss: -0.05035024881362915
Batch 16/64 loss: -0.023063063621520996
Batch 17/64 loss: -0.03994488716125488
Batch 18/64 loss: -0.05972105264663696
Batch 19/64 loss: -0.061998963356018066
Batch 20/64 loss: -0.05258220434188843
Batch 21/64 loss: -0.02895963191986084
Batch 22/64 loss: -0.06329548358917236
Batch 23/64 loss: -0.06740480661392212
Batch 24/64 loss: -0.052793800830841064
Batch 25/64 loss: -0.07208633422851562
Batch 26/64 loss: -0.05027204751968384
Batch 27/64 loss: -0.07076025009155273
Batch 28/64 loss: -0.0653454065322876
Batch 29/64 loss: -0.051401734352111816
Batch 30/64 loss: -0.03571408987045288
Batch 31/64 loss: -0.07915753126144409
Batch 32/64 loss: -0.07233333587646484
Batch 33/64 loss: -0.05181068181991577
Batch 34/64 loss: -0.05726045370101929
Batch 35/64 loss: -0.0737072229385376
Batch 36/64 loss: -0.08687913417816162
Batch 37/64 loss: -0.07348352670669556
Batch 38/64 loss: -0.0037619471549987793
Batch 39/64 loss: -0.08938384056091309
Batch 40/64 loss: -0.07362425327301025
Batch 41/64 loss: -0.05084490776062012
Batch 42/64 loss: -0.048516809940338135
Batch 43/64 loss: -0.05198389291763306
Batch 44/64 loss: -0.036744534969329834
Batch 45/64 loss: -0.023881494998931885
Batch 46/64 loss: -0.07477766275405884
Batch 47/64 loss: -0.06546425819396973
Batch 48/64 loss: -0.018482983112335205
Batch 49/64 loss: -0.022164523601531982
Batch 50/64 loss: -0.07035434246063232
Batch 51/64 loss: -0.059978365898132324
Batch 52/64 loss: -0.05895310640335083
Batch 53/64 loss: -0.07257986068725586
Batch 54/64 loss: -0.06303524971008301
Batch 55/64 loss: -0.05615508556365967
Batch 56/64 loss: -0.06769651174545288
Batch 57/64 loss: -0.05566805601119995
Batch 58/64 loss: -0.07560503482818604
Batch 59/64 loss: -0.06688195466995239
Batch 60/64 loss: -0.07183486223220825
Batch 61/64 loss: -0.03460538387298584
Batch 62/64 loss: -0.03818029165267944
Batch 63/64 loss: -0.07974296808242798
Batch 64/64 loss: -0.06308257579803467
Epoch 199  Train loss: -0.056480153869180115  Val loss: 0.05721025716807834
Epoch 200
-------------------------------
Batch 1/64 loss: -0.10195255279541016
Batch 2/64 loss: -0.05420488119125366
Batch 3/64 loss: -0.0922122597694397
Batch 4/64 loss: -0.06459164619445801
Batch 5/64 loss: 0.00028377771377563477
Batch 6/64 loss: -0.07851254940032959
Batch 7/64 loss: -0.0557364821434021
Batch 8/64 loss: -0.07939356565475464
Batch 9/64 loss: -0.06662487983703613
Batch 10/64 loss: -0.07269465923309326
Batch 11/64 loss: -0.05173617601394653
Batch 12/64 loss: -0.05214911699295044
Batch 13/64 loss: -0.039116859436035156
Batch 14/64 loss: -0.0692451000213623
Batch 15/64 loss: -0.06832629442214966
Batch 16/64 loss: -0.086719810962677
Batch 17/64 loss: -0.05387610197067261
Batch 18/64 loss: -0.04936659336090088
Batch 19/64 loss: -0.030617237091064453
Batch 20/64 loss: -0.07666265964508057
Batch 21/64 loss: -0.059894680976867676
Batch 22/64 loss: -0.06963109970092773
Batch 23/64 loss: -0.0673290491104126
Batch 24/64 loss: -0.0684502124786377
Batch 25/64 loss: -0.059149861335754395
Batch 26/64 loss: -0.052982449531555176
Batch 27/64 loss: -0.040693461894989014
Batch 28/64 loss: -0.04883074760437012
Batch 29/64 loss: -0.0562170147895813
Batch 30/64 loss: -0.05474865436553955
Batch 31/64 loss: -0.050678253173828125
Batch 32/64 loss: -0.05869019031524658
Batch 33/64 loss: -0.04240447282791138
Batch 34/64 loss: -0.05038905143737793
Batch 35/64 loss: -0.07430803775787354
Batch 36/64 loss: -0.029555559158325195
Batch 37/64 loss: -0.052426695823669434
Batch 38/64 loss: -0.07947361469268799
Batch 39/64 loss: -0.0134505033493042
Batch 40/64 loss: -0.08648192882537842
Batch 41/64 loss: -0.0007387399673461914
Batch 42/64 loss: -0.09193593263626099
Batch 43/64 loss: -0.07415050268173218
Batch 44/64 loss: -0.06296968460083008
Batch 45/64 loss: -0.03694266080856323
Batch 46/64 loss: -0.05861586332321167
Batch 47/64 loss: -0.05508887767791748
Batch 48/64 loss: -0.04469180107116699
Batch 49/64 loss: -0.07977014780044556
Batch 50/64 loss: -0.03318125009536743
Batch 51/64 loss: -0.01780545711517334
Batch 52/64 loss: -0.06380754709243774
Batch 53/64 loss: -0.06818336248397827
Batch 54/64 loss: -0.02469104528427124
Batch 55/64 loss: -0.06441998481750488
Batch 56/64 loss: -0.04526597261428833
Batch 57/64 loss: -0.06569552421569824
Batch 58/64 loss: -0.07256698608398438
Batch 59/64 loss: -0.06046581268310547
Batch 60/64 loss: -0.05347132682800293
Batch 61/64 loss: -0.07519024610519409
Batch 62/64 loss: -0.047223567962646484
Batch 63/64 loss: -0.05978989601135254
Batch 64/64 loss: -0.036920905113220215
Epoch 200  Train loss: -0.05715441657047646  Val loss: 0.06055756819616888
Epoch 201
-------------------------------
Batch 1/64 loss: -0.0807415246963501
Batch 2/64 loss: -0.07420814037322998
Batch 3/64 loss: -0.04455101490020752
Batch 4/64 loss: -0.03619939088821411
Batch 5/64 loss: -0.054324328899383545
Batch 6/64 loss: -0.07610732316970825
Batch 7/64 loss: -0.05602240562438965
Batch 8/64 loss: -0.09086525440216064
Batch 9/64 loss: -0.03791600465774536
Batch 10/64 loss: -0.04537147283554077
Batch 11/64 loss: -0.06773781776428223
Batch 12/64 loss: -0.06939786672592163
Batch 13/64 loss: -0.03973257541656494
Batch 14/64 loss: -0.0695151686668396
Batch 15/64 loss: -0.06652474403381348
Batch 16/64 loss: -0.06721019744873047
Batch 17/64 loss: -0.04930150508880615
Batch 18/64 loss: -0.05647033452987671
Batch 19/64 loss: -0.07923614978790283
Batch 20/64 loss: -0.08599501848220825
Batch 21/64 loss: -0.044468462467193604
Batch 22/64 loss: -0.07139062881469727
Batch 23/64 loss: -0.02275395393371582
Batch 24/64 loss: -0.045336008071899414
Batch 25/64 loss: -0.07024145126342773
Batch 26/64 loss: -0.07728028297424316
Batch 27/64 loss: -0.019699454307556152
Batch 28/64 loss: -0.08631610870361328
Batch 29/64 loss: -0.03378188610076904
Batch 30/64 loss: -0.032265424728393555
Batch 31/64 loss: -0.04334104061126709
Batch 32/64 loss: -0.0582004189491272
Batch 33/64 loss: -0.03148162364959717
Batch 34/64 loss: -0.057592034339904785
Batch 35/64 loss: -0.02405226230621338
Batch 36/64 loss: -0.04222995042800903
Batch 37/64 loss: -0.057065606117248535
Batch 38/64 loss: -0.03832197189331055
Batch 39/64 loss: -0.07003569602966309
Batch 40/64 loss: -0.019931316375732422
Batch 41/64 loss: -0.060847699642181396
Batch 42/64 loss: -0.0752720832824707
Batch 43/64 loss: -0.05048036575317383
Batch 44/64 loss: -0.04143601655960083
Batch 45/64 loss: -0.07780683040618896
Batch 46/64 loss: -0.06850522756576538
Batch 47/64 loss: -0.054892122745513916
Batch 48/64 loss: -0.07147961854934692
Batch 49/64 loss: -0.06440520286560059
Batch 50/64 loss: -0.05297541618347168
Batch 51/64 loss: -0.05972003936767578
Batch 52/64 loss: -0.07959473133087158
Batch 53/64 loss: -0.04602551460266113
Batch 54/64 loss: -0.0779256820678711
Batch 55/64 loss: -0.05551958084106445
Batch 56/64 loss: -0.06176304817199707
Batch 57/64 loss: -0.05419957637786865
Batch 58/64 loss: -0.0421333909034729
Batch 59/64 loss: -0.06756502389907837
Batch 60/64 loss: -0.020536720752716064
Batch 61/64 loss: -0.022970616817474365
Batch 62/64 loss: -0.06738615036010742
Batch 63/64 loss: -0.05369091033935547
Batch 64/64 loss: -0.05150496959686279
Epoch 201  Train loss: -0.05582704497318642  Val loss: 0.057873312550312056
Epoch 202
-------------------------------
Batch 1/64 loss: -0.025184154510498047
Batch 2/64 loss: -0.0636974573135376
Batch 3/64 loss: -0.06925380229949951
Batch 4/64 loss: -0.054024338722229004
Batch 5/64 loss: -0.06210511922836304
Batch 6/64 loss: -0.05621105432510376
Batch 7/64 loss: -0.03632926940917969
Batch 8/64 loss: -0.049701690673828125
Batch 9/64 loss: -0.04267686605453491
Batch 10/64 loss: -0.041734397411346436
Batch 11/64 loss: -0.025716066360473633
Batch 12/64 loss: -0.06362086534500122
Batch 13/64 loss: -0.06683617830276489
Batch 14/64 loss: -0.06967854499816895
Batch 15/64 loss: -0.07094830274581909
Batch 16/64 loss: -0.09110367298126221
Batch 17/64 loss: -0.03847348690032959
Batch 18/64 loss: -0.06796222925186157
Batch 19/64 loss: -0.06413304805755615
Batch 20/64 loss: -0.06912440061569214
Batch 21/64 loss: -0.04506915807723999
Batch 22/64 loss: -0.06807953119277954
Batch 23/64 loss: -0.0609930157661438
Batch 24/64 loss: -0.030751526355743408
Batch 25/64 loss: -0.06812620162963867
Batch 26/64 loss: -0.08159279823303223
Batch 27/64 loss: -0.045995354652404785
Batch 28/64 loss: -0.05443012714385986
Batch 29/64 loss: -0.07548606395721436
Batch 30/64 loss: -0.05508220195770264
Batch 31/64 loss: -0.06589078903198242
Batch 32/64 loss: -0.05946570634841919
Batch 33/64 loss: -0.09787231683731079
Batch 34/64 loss: -0.08785510063171387
Batch 35/64 loss: -0.08010315895080566
Batch 36/64 loss: -0.08076637983322144
Batch 37/64 loss: -0.06671440601348877
Batch 38/64 loss: -0.07788681983947754
Batch 39/64 loss: -0.0583268404006958
Batch 40/64 loss: -0.07151985168457031
Batch 41/64 loss: -0.045475125312805176
Batch 42/64 loss: -0.05878591537475586
Batch 43/64 loss: -0.08681529760360718
Batch 44/64 loss: -0.04391896724700928
Batch 45/64 loss: -0.009340763092041016
Batch 46/64 loss: -0.04642373323440552
Batch 47/64 loss: -0.060318827629089355
Batch 48/64 loss: -0.06677895784378052
Batch 49/64 loss: -0.0686902403831482
Batch 50/64 loss: -0.026929736137390137
Batch 51/64 loss: -0.055257201194763184
Batch 52/64 loss: -0.03564620018005371
Batch 53/64 loss: -0.0459141731262207
Batch 54/64 loss: -0.06164395809173584
Batch 55/64 loss: -0.0760384202003479
Batch 56/64 loss: -0.06501555442810059
Batch 57/64 loss: -0.06710934638977051
Batch 58/64 loss: -0.04304087162017822
Batch 59/64 loss: -0.03432607650756836
Batch 60/64 loss: -0.059052109718322754
Batch 61/64 loss: -0.08020412921905518
Batch 62/64 loss: -0.06087595224380493
Batch 63/64 loss: -0.0661994218826294
Batch 64/64 loss: -0.049580156803131104
Epoch 202  Train loss: -0.05900405318129296  Val loss: 0.05511549821833974
Epoch 203
-------------------------------
Batch 1/64 loss: -0.06732553243637085
Batch 2/64 loss: -0.053675055503845215
Batch 3/64 loss: -0.0644407868385315
Batch 4/64 loss: -0.07002913951873779
Batch 5/64 loss: -0.07748466730117798
Batch 6/64 loss: -0.026408374309539795
Batch 7/64 loss: -0.059217214584350586
Batch 8/64 loss: -0.061400413513183594
Batch 9/64 loss: -0.08403074741363525
Batch 10/64 loss: -0.05917942523956299
Batch 11/64 loss: -0.07593172788619995
Batch 12/64 loss: -0.036585092544555664
Batch 13/64 loss: -0.08622586727142334
Batch 14/64 loss: -0.07800561189651489
Batch 15/64 loss: -0.07035094499588013
Batch 16/64 loss: -0.04015243053436279
Batch 17/64 loss: -0.08086395263671875
Batch 18/64 loss: -0.04072439670562744
Batch 19/64 loss: -0.07126826047897339
Batch 20/64 loss: -0.07082587480545044
Batch 21/64 loss: -0.03904759883880615
Batch 22/64 loss: -0.05889320373535156
Batch 23/64 loss: -0.0463489294052124
Batch 24/64 loss: -0.06561189889907837
Batch 25/64 loss: -0.04671800136566162
Batch 26/64 loss: -0.04837924242019653
Batch 27/64 loss: -0.03799855709075928
Batch 28/64 loss: -0.05297660827636719
Batch 29/64 loss: -0.05105710029602051
Batch 30/64 loss: -0.029954969882965088
Batch 31/64 loss: -0.007805883884429932
Batch 32/64 loss: -0.060485124588012695
Batch 33/64 loss: -0.059706151485443115
Batch 34/64 loss: -0.10220116376876831
Batch 35/64 loss: -0.07049179077148438
Batch 36/64 loss: -0.06186175346374512
Batch 37/64 loss: -0.08403408527374268
Batch 38/64 loss: -0.03760266304016113
Batch 39/64 loss: -0.04493492841720581
Batch 40/64 loss: -0.06436812877655029
Batch 41/64 loss: -0.06698215007781982
Batch 42/64 loss: -0.025783061981201172
Batch 43/64 loss: -0.08184289932250977
Batch 44/64 loss: -0.06564491987228394
Batch 45/64 loss: -0.07093721628189087
Batch 46/64 loss: -0.07220923900604248
Batch 47/64 loss: -0.051458775997161865
Batch 48/64 loss: -0.05082458257675171
Batch 49/64 loss: -0.0390782356262207
Batch 50/64 loss: -0.05998712778091431
Batch 51/64 loss: -0.05434262752532959
Batch 52/64 loss: -0.04639232158660889
Batch 53/64 loss: -0.056884825229644775
Batch 54/64 loss: -0.04526388645172119
Batch 55/64 loss: -0.0731348991394043
Batch 56/64 loss: -0.08992272615432739
Batch 57/64 loss: -0.07706993818283081
Batch 58/64 loss: -0.06119471788406372
Batch 59/64 loss: -0.022042274475097656
Batch 60/64 loss: -0.08059322834014893
Batch 61/64 loss: -0.07786816358566284
Batch 62/64 loss: -0.07557576894760132
Batch 63/64 loss: -0.04603242874145508
Batch 64/64 loss: -0.08720922470092773
Epoch 203  Train loss: -0.0596247252295999  Val loss: 0.05303262496731945
Epoch 204
-------------------------------
Batch 1/64 loss: -0.060613930225372314
Batch 2/64 loss: -0.04623138904571533
Batch 3/64 loss: -0.05892127752304077
Batch 4/64 loss: -0.0692751407623291
Batch 5/64 loss: -0.07824236154556274
Batch 6/64 loss: -0.07099920511245728
Batch 7/64 loss: -0.040747642517089844
Batch 8/64 loss: -0.09644484519958496
Batch 9/64 loss: -0.037284135818481445
Batch 10/64 loss: -0.06226903200149536
Batch 11/64 loss: -0.08805108070373535
Batch 12/64 loss: -0.07125633955001831
Batch 13/64 loss: -0.057271361351013184
Batch 14/64 loss: -0.054532527923583984
Batch 15/64 loss: -0.05729550123214722
Batch 16/64 loss: -0.06891483068466187
Batch 17/64 loss: -0.056787073612213135
Batch 18/64 loss: -0.07735741138458252
Batch 19/64 loss: -0.0604284405708313
Batch 20/64 loss: -0.06970024108886719
Batch 21/64 loss: -0.04964756965637207
Batch 22/64 loss: -0.054430365562438965
Batch 23/64 loss: -0.02207571268081665
Batch 24/64 loss: -0.06346273422241211
Batch 25/64 loss: -0.06226909160614014
Batch 26/64 loss: -0.09497976303100586
Batch 27/64 loss: -0.037992238998413086
Batch 28/64 loss: -0.058174967765808105
Batch 29/64 loss: -0.0667073130607605
Batch 30/64 loss: -0.04111438989639282
Batch 31/64 loss: -0.0841749906539917
Batch 32/64 loss: -0.03710418939590454
Batch 33/64 loss: -0.0646708607673645
Batch 34/64 loss: -0.08734023571014404
Batch 35/64 loss: -0.061338961124420166
Batch 36/64 loss: -0.05658191442489624
Batch 37/64 loss: -0.08519500494003296
Batch 38/64 loss: -0.03223085403442383
Batch 39/64 loss: -0.0546228289604187
Batch 40/64 loss: -0.0514225959777832
Batch 41/64 loss: -0.07221311330795288
Batch 42/64 loss: -0.08364808559417725
Batch 43/64 loss: -0.08840203285217285
Batch 44/64 loss: -0.07730734348297119
Batch 45/64 loss: -0.07708346843719482
Batch 46/64 loss: -0.07161056995391846
Batch 47/64 loss: -0.07818853855133057
Batch 48/64 loss: -0.02001279592514038
Batch 49/64 loss: -0.05712765455245972
Batch 50/64 loss: -0.09577041864395142
Batch 51/64 loss: -0.06571674346923828
Batch 52/64 loss: -0.08243453502655029
Batch 53/64 loss: -0.04090315103530884
Batch 54/64 loss: -0.06519770622253418
Batch 55/64 loss: -0.05441939830780029
Batch 56/64 loss: -0.0492214560508728
Batch 57/64 loss: -0.08077073097229004
Batch 58/64 loss: -0.06795305013656616
Batch 59/64 loss: -0.006556570529937744
Batch 60/64 loss: -0.05383288860321045
Batch 61/64 loss: -0.03913217782974243
Batch 62/64 loss: -0.09229660034179688
Batch 63/64 loss: -0.05592548847198486
Batch 64/64 loss: -0.042002737522125244
Epoch 204  Train loss: -0.06204528504726933  Val loss: 0.07693643422470879
Epoch 205
-------------------------------
Batch 1/64 loss: -0.056800663471221924
Batch 2/64 loss: -0.041271209716796875
Batch 3/64 loss: -0.07031649351119995
Batch 4/64 loss: -0.07557189464569092
Batch 5/64 loss: -0.05417931079864502
Batch 6/64 loss: -0.04710984230041504
Batch 7/64 loss: -0.0913846492767334
Batch 8/64 loss: -0.05950450897216797
Batch 9/64 loss: -0.05360966920852661
Batch 10/64 loss: -0.06673794984817505
Batch 11/64 loss: -0.09090018272399902
Batch 12/64 loss: -0.06558763980865479
Batch 13/64 loss: -0.05283278226852417
Batch 14/64 loss: -0.09439617395401001
Batch 15/64 loss: -0.05310279130935669
Batch 16/64 loss: -0.07391786575317383
Batch 17/64 loss: -0.07506579160690308
Batch 18/64 loss: -0.04687631130218506
Batch 19/64 loss: -0.06193339824676514
Batch 20/64 loss: -0.06402415037155151
Batch 21/64 loss: -0.047711730003356934
Batch 22/64 loss: -0.05775076150894165
Batch 23/64 loss: -0.05749309062957764
Batch 24/64 loss: -0.042256832122802734
Batch 25/64 loss: -0.048592209815979004
Batch 26/64 loss: -0.0400620698928833
Batch 27/64 loss: -0.0665971040725708
Batch 28/64 loss: -0.05486500263214111
Batch 29/64 loss: -0.03616225719451904
Batch 30/64 loss: -0.06221824884414673
Batch 31/64 loss: -0.06384974718093872
Batch 32/64 loss: -0.06701791286468506
Batch 33/64 loss: -0.02515721321105957
Batch 34/64 loss: -0.07424956560134888
Batch 35/64 loss: -0.020336031913757324
Batch 36/64 loss: -0.06660324335098267
Batch 37/64 loss: -0.06717038154602051
Batch 38/64 loss: -0.061041831970214844
Batch 39/64 loss: -0.05866706371307373
Batch 40/64 loss: -0.06320023536682129
Batch 41/64 loss: -0.057901978492736816
Batch 42/64 loss: -0.09523409605026245
Batch 43/64 loss: -0.06166183948516846
Batch 44/64 loss: -0.08940654993057251
Batch 45/64 loss: -0.06658279895782471
Batch 46/64 loss: -0.039708852767944336
Batch 47/64 loss: -0.025926172733306885
Batch 48/64 loss: -0.07341599464416504
Batch 49/64 loss: -0.08196008205413818
Batch 50/64 loss: -0.03457140922546387
Batch 51/64 loss: -0.04507803916931152
Batch 52/64 loss: -0.07998353242874146
Batch 53/64 loss: -0.06117802858352661
Batch 54/64 loss: -0.06259304285049438
Batch 55/64 loss: -0.07819908857345581
Batch 56/64 loss: -0.0690380334854126
Batch 57/64 loss: -0.06274533271789551
Batch 58/64 loss: -0.04190933704376221
Batch 59/64 loss: -0.06402528285980225
Batch 60/64 loss: -0.07167637348175049
Batch 61/64 loss: -0.0663905143737793
Batch 62/64 loss: -0.06695997714996338
Batch 63/64 loss: -0.062113940715789795
Batch 64/64 loss: -0.05615442991256714
Epoch 205  Train loss: -0.06080787345474842  Val loss: 0.05577764662680347
Epoch 206
-------------------------------
Batch 1/64 loss: -0.029748737812042236
Batch 2/64 loss: -0.08759653568267822
Batch 3/64 loss: -0.05147969722747803
Batch 4/64 loss: -0.04096996784210205
Batch 5/64 loss: -0.0677717924118042
Batch 6/64 loss: -0.07086038589477539
Batch 7/64 loss: -0.07813411951065063
Batch 8/64 loss: -0.05456358194351196
Batch 9/64 loss: -0.042258620262145996
Batch 10/64 loss: -0.07563108205795288
Batch 11/64 loss: -0.06405764818191528
Batch 12/64 loss: -0.06564486026763916
Batch 13/64 loss: -0.053635358810424805
Batch 14/64 loss: -0.08116084337234497
Batch 15/64 loss: -0.04236644506454468
Batch 16/64 loss: -0.03329813480377197
Batch 17/64 loss: -0.05063599348068237
Batch 18/64 loss: -0.07095903158187866
Batch 19/64 loss: -0.08150428533554077
Batch 20/64 loss: -0.06877601146697998
Batch 21/64 loss: -0.08201450109481812
Batch 22/64 loss: -0.062478721141815186
Batch 23/64 loss: -0.07458579540252686
Batch 24/64 loss: -0.06913614273071289
Batch 25/64 loss: -0.07474815845489502
Batch 26/64 loss: -0.06628519296646118
Batch 27/64 loss: -0.014169931411743164
Batch 28/64 loss: -0.054796040058135986
Batch 29/64 loss: -0.07473635673522949
Batch 30/64 loss: -0.052799761295318604
Batch 31/64 loss: -0.07146561145782471
Batch 32/64 loss: -0.03792881965637207
Batch 33/64 loss: -0.05037510395050049
Batch 34/64 loss: -0.07225406169891357
Batch 35/64 loss: -0.08543205261230469
Batch 36/64 loss: -0.0759270191192627
Batch 37/64 loss: -0.06944715976715088
Batch 38/64 loss: -0.05975979566574097
Batch 39/64 loss: -0.05711466073989868
Batch 40/64 loss: -0.0752679705619812
Batch 41/64 loss: -0.05080223083496094
Batch 42/64 loss: -0.0581282377243042
Batch 43/64 loss: -0.04305011034011841
Batch 44/64 loss: -0.07585698366165161
Batch 45/64 loss: -0.041319966316223145
Batch 46/64 loss: -0.06938517093658447
Batch 47/64 loss: -0.06564247608184814
Batch 48/64 loss: -0.06845855712890625
Batch 49/64 loss: -0.04888266324996948
Batch 50/64 loss: -0.03649592399597168
Batch 51/64 loss: -0.08390301465988159
Batch 52/64 loss: -0.07359373569488525
Batch 53/64 loss: -0.059993863105773926
Batch 54/64 loss: -0.06600302457809448
Batch 55/64 loss: -0.05746859312057495
Batch 56/64 loss: -0.060550808906555176
Batch 57/64 loss: -0.04759794473648071
Batch 58/64 loss: -0.029735565185546875
Batch 59/64 loss: -0.06310802698135376
Batch 60/64 loss: -0.0599365234375
Batch 61/64 loss: -0.044103920459747314
Batch 62/64 loss: -0.03653991222381592
Batch 63/64 loss: -0.09398019313812256
Batch 64/64 loss: -0.045041024684906006
Epoch 206  Train loss: -0.06055049732619641  Val loss: 0.05590807387918951
Epoch 207
-------------------------------
Batch 1/64 loss: -0.043749988079071045
Batch 2/64 loss: -0.053243160247802734
Batch 3/64 loss: -0.08822685480117798
Batch 4/64 loss: -0.06902998685836792
Batch 5/64 loss: -0.03471565246582031
Batch 6/64 loss: -0.08382993936538696
Batch 7/64 loss: -0.09138995409011841
Batch 8/64 loss: -0.0672616958618164
Batch 9/64 loss: -0.0629657506942749
Batch 10/64 loss: -0.07606852054595947
Batch 11/64 loss: -0.057951509952545166
Batch 12/64 loss: -0.04802083969116211
Batch 13/64 loss: -0.08393007516860962
Batch 14/64 loss: -0.06897193193435669
Batch 15/64 loss: -0.08434218168258667
Batch 16/64 loss: -0.09324383735656738
Batch 17/64 loss: -0.07605946063995361
Batch 18/64 loss: -0.0798911452293396
Batch 19/64 loss: -0.04818946123123169
Batch 20/64 loss: -0.041250526905059814
Batch 21/64 loss: -0.06375455856323242
Batch 22/64 loss: -0.023794829845428467
Batch 23/64 loss: -0.038644254207611084
Batch 24/64 loss: -0.05698847770690918
Batch 25/64 loss: -0.045622289180755615
Batch 26/64 loss: -0.061378657817840576
Batch 27/64 loss: -0.056387126445770264
Batch 28/64 loss: -0.06168413162231445
Batch 29/64 loss: -0.07181441783905029
Batch 30/64 loss: -0.03461480140686035
Batch 31/64 loss: -0.06863635778427124
Batch 32/64 loss: -0.017964184284210205
Batch 33/64 loss: -0.0844603180885315
Batch 34/64 loss: -0.061969637870788574
Batch 35/64 loss: -0.08258271217346191
Batch 36/64 loss: -0.061876535415649414
Batch 37/64 loss: -0.07716226577758789
Batch 38/64 loss: -0.05396759510040283
Batch 39/64 loss: -0.042383551597595215
Batch 40/64 loss: -0.09761315584182739
Batch 41/64 loss: -0.06708031892776489
Batch 42/64 loss: -0.0887075662612915
Batch 43/64 loss: -0.04596388339996338
Batch 44/64 loss: -0.07873040437698364
Batch 45/64 loss: -0.04477119445800781
Batch 46/64 loss: -0.08560317754745483
Batch 47/64 loss: -0.06176018714904785
Batch 48/64 loss: -0.0832815170288086
Batch 49/64 loss: -0.05215984582901001
Batch 50/64 loss: -0.08803325891494751
Batch 51/64 loss: -0.05917483568191528
Batch 52/64 loss: -0.0812833309173584
Batch 53/64 loss: -0.036435842514038086
Batch 54/64 loss: -0.06255996227264404
Batch 55/64 loss: -0.06514108180999756
Batch 56/64 loss: -0.06537741422653198
Batch 57/64 loss: -0.039802491664886475
Batch 58/64 loss: -0.05248075723648071
Batch 59/64 loss: -0.040154457092285156
Batch 60/64 loss: -0.03361058235168457
Batch 61/64 loss: -0.05511057376861572
Batch 62/64 loss: -0.07592809200286865
Batch 63/64 loss: -0.0805850625038147
Batch 64/64 loss: -0.020341157913208008
Epoch 207  Train loss: -0.06234704372929592  Val loss: 0.05536617794397361
Epoch 208
-------------------------------
Batch 1/64 loss: -0.07374978065490723
Batch 2/64 loss: -0.029677748680114746
Batch 3/64 loss: -0.07787740230560303
Batch 4/64 loss: -0.10187649726867676
Batch 5/64 loss: -0.08936744928359985
Batch 6/64 loss: -0.08158212900161743
Batch 7/64 loss: -0.07681578397750854
Batch 8/64 loss: -0.08466315269470215
Batch 9/64 loss: -0.05111438035964966
Batch 10/64 loss: -0.06488430500030518
Batch 11/64 loss: -0.09777861833572388
Batch 12/64 loss: -0.09019792079925537
Batch 13/64 loss: -0.050282180309295654
Batch 14/64 loss: -0.05806499719619751
Batch 15/64 loss: -0.05758112668991089
Batch 16/64 loss: -0.08028125762939453
Batch 17/64 loss: -0.0835418701171875
Batch 18/64 loss: -0.05524611473083496
Batch 19/64 loss: -0.04297912120819092
Batch 20/64 loss: -0.0622289776802063
Batch 21/64 loss: -0.0410919189453125
Batch 22/64 loss: -0.10188931226730347
Batch 23/64 loss: -0.08060717582702637
Batch 24/64 loss: -0.056710004806518555
Batch 25/64 loss: -0.06664085388183594
Batch 26/64 loss: -0.029338479042053223
Batch 27/64 loss: -0.04622924327850342
Batch 28/64 loss: -0.03304070234298706
Batch 29/64 loss: -0.09423458576202393
Batch 30/64 loss: -0.07096785306930542
Batch 31/64 loss: -0.06366801261901855
Batch 32/64 loss: -0.07300323247909546
Batch 33/64 loss: -0.06334155797958374
Batch 34/64 loss: -0.04474085569381714
Batch 35/64 loss: -0.07122749090194702
Batch 36/64 loss: -0.06409019231796265
Batch 37/64 loss: -0.03204178810119629
Batch 38/64 loss: -0.09434223175048828
Batch 39/64 loss: -0.07537513971328735
Batch 40/64 loss: -0.05696868896484375
Batch 41/64 loss: -0.022179603576660156
Batch 42/64 loss: -0.0702635645866394
Batch 43/64 loss: -0.08046907186508179
Batch 44/64 loss: -0.016100764274597168
Batch 45/64 loss: -0.08946114778518677
Batch 46/64 loss: -0.07492589950561523
Batch 47/64 loss: -0.09293699264526367
Batch 48/64 loss: -0.08412784337997437
Batch 49/64 loss: -0.0531960129737854
Batch 50/64 loss: -0.06033807992935181
Batch 51/64 loss: -0.06967592239379883
Batch 52/64 loss: -0.06289219856262207
Batch 53/64 loss: -0.0048272013664245605
Batch 54/64 loss: -0.07630950212478638
Batch 55/64 loss: -0.05460095405578613
Batch 56/64 loss: -0.067604660987854
Batch 57/64 loss: -0.08530718088150024
Batch 58/64 loss: -0.05486762523651123
Batch 59/64 loss: -0.08256697654724121
Batch 60/64 loss: -0.07586288452148438
Batch 61/64 loss: -0.0768880844116211
Batch 62/64 loss: -0.025400936603546143
Batch 63/64 loss: -0.03566789627075195
Batch 64/64 loss: -0.04260748624801636
Epoch 208  Train loss: -0.06459241965237786  Val loss: 0.06313218120037485
Epoch 209
-------------------------------
Batch 1/64 loss: -0.09276425838470459
Batch 2/64 loss: -0.08056670427322388
Batch 3/64 loss: -0.07657623291015625
Batch 4/64 loss: -0.04196822643280029
Batch 5/64 loss: -0.062429189682006836
Batch 6/64 loss: -0.0364227294921875
Batch 7/64 loss: -0.06713652610778809
Batch 8/64 loss: -0.0725250244140625
Batch 9/64 loss: -0.030478835105895996
Batch 10/64 loss: -0.05134671926498413
Batch 11/64 loss: 0.005119919776916504
Batch 12/64 loss: -0.04996192455291748
Batch 13/64 loss: -0.08306217193603516
Batch 14/64 loss: -0.07981252670288086
Batch 15/64 loss: -0.07191509008407593
Batch 16/64 loss: -0.06450635194778442
Batch 17/64 loss: -0.03669995069503784
Batch 18/64 loss: -0.03869962692260742
Batch 19/64 loss: -0.054940879344940186
Batch 20/64 loss: -0.06419181823730469
Batch 21/64 loss: -0.056912899017333984
Batch 22/64 loss: -0.03717947006225586
Batch 23/64 loss: -0.09180569648742676
Batch 24/64 loss: -0.03126406669616699
Batch 25/64 loss: -0.07314759492874146
Batch 26/64 loss: -0.05775916576385498
Batch 27/64 loss: -0.06583654880523682
Batch 28/64 loss: -0.07299298048019409
Batch 29/64 loss: -0.07102644443511963
Batch 30/64 loss: -0.08185887336730957
Batch 31/64 loss: -0.055458664894104004
Batch 32/64 loss: -0.04350632429122925
Batch 33/64 loss: -0.06321263313293457
Batch 34/64 loss: -0.04926800727844238
Batch 35/64 loss: -0.10184431076049805
Batch 36/64 loss: -0.0929635763168335
Batch 37/64 loss: -0.06813812255859375
Batch 38/64 loss: -0.055396974086761475
Batch 39/64 loss: -0.059020042419433594
Batch 40/64 loss: -0.07133889198303223
Batch 41/64 loss: -0.06529426574707031
Batch 42/64 loss: -0.05373215675354004
Batch 43/64 loss: -0.055310726165771484
Batch 44/64 loss: -0.09566593170166016
Batch 45/64 loss: -0.046325087547302246
Batch 46/64 loss: -0.0852041244506836
Batch 47/64 loss: -0.05383706092834473
Batch 48/64 loss: -0.02357548475265503
Batch 49/64 loss: -0.02462327480316162
Batch 50/64 loss: -0.06605362892150879
Batch 51/64 loss: -0.06563085317611694
Batch 52/64 loss: -0.038758933544158936
Batch 53/64 loss: -0.0889132022857666
Batch 54/64 loss: -0.05735582113265991
Batch 55/64 loss: -0.03397393226623535
Batch 56/64 loss: -0.07649987936019897
Batch 57/64 loss: -0.07528352737426758
Batch 58/64 loss: -0.08159565925598145
Batch 59/64 loss: -0.08713889122009277
Batch 60/64 loss: -0.08555567264556885
Batch 61/64 loss: -0.0420072078704834
Batch 62/64 loss: -0.09050202369689941
Batch 63/64 loss: -0.05752664804458618
Batch 64/64 loss: -0.06451207399368286
Epoch 209  Train loss: -0.06195394829207776  Val loss: 0.05402120814700307
Epoch 210
-------------------------------
Batch 1/64 loss: -0.05466049909591675
Batch 2/64 loss: -0.05299830436706543
Batch 3/64 loss: -0.07359927892684937
Batch 4/64 loss: -0.04805147647857666
Batch 5/64 loss: -0.07012057304382324
Batch 6/64 loss: -0.04589498043060303
Batch 7/64 loss: -0.060840070247650146
Batch 8/64 loss: -0.062175214290618896
Batch 9/64 loss: -0.08809888362884521
Batch 10/64 loss: -0.08460891246795654
Batch 11/64 loss: -0.09283667802810669
Batch 12/64 loss: -0.09140366315841675
Batch 13/64 loss: -0.07375955581665039
Batch 14/64 loss: -0.048601388931274414
Batch 15/64 loss: -0.03698933124542236
Batch 16/64 loss: -0.06081491708755493
Batch 17/64 loss: -0.05552798509597778
Batch 18/64 loss: -0.03825193643569946
Batch 19/64 loss: -0.056392133235931396
Batch 20/64 loss: -0.06179887056350708
Batch 21/64 loss: -0.07934123277664185
Batch 22/64 loss: -0.024795711040496826
Batch 23/64 loss: -0.0824744701385498
Batch 24/64 loss: -0.09059619903564453
Batch 25/64 loss: -0.06811028718948364
Batch 26/64 loss: -0.06802260875701904
Batch 27/64 loss: -0.05294978618621826
Batch 28/64 loss: -0.05349689722061157
Batch 29/64 loss: -0.059062063694000244
Batch 30/64 loss: -0.08153444528579712
Batch 31/64 loss: -0.04318875074386597
Batch 32/64 loss: -0.06790882349014282
Batch 33/64 loss: -0.09152323007583618
Batch 34/64 loss: -0.04556894302368164
Batch 35/64 loss: -0.07588469982147217
Batch 36/64 loss: -0.07452988624572754
Batch 37/64 loss: -0.07552129030227661
Batch 38/64 loss: -0.06820577383041382
Batch 39/64 loss: -0.0717775821685791
Batch 40/64 loss: -0.08357560634613037
Batch 41/64 loss: -0.024975478649139404
Batch 42/64 loss: -0.06697243452072144
Batch 43/64 loss: -0.05604368448257446
Batch 44/64 loss: -0.05150437355041504
Batch 45/64 loss: -0.06906384229660034
Batch 46/64 loss: -0.07703185081481934
Batch 47/64 loss: -0.05410444736480713
Batch 48/64 loss: -0.06469279527664185
Batch 49/64 loss: -0.09900546073913574
Batch 50/64 loss: -0.09058642387390137
Batch 51/64 loss: -0.05665147304534912
Batch 52/64 loss: -0.06717151403427124
Batch 53/64 loss: -0.08691829442977905
Batch 54/64 loss: -0.04069405794143677
Batch 55/64 loss: -0.08766055107116699
Batch 56/64 loss: -0.0899350643157959
Batch 57/64 loss: -0.04060518741607666
Batch 58/64 loss: -0.05391108989715576
Batch 59/64 loss: -0.011789560317993164
Batch 60/64 loss: -0.08841097354888916
Batch 61/64 loss: -0.020391762256622314
Batch 62/64 loss: -0.08752274513244629
Batch 63/64 loss: -0.07534646987915039
Batch 64/64 loss: -0.051344096660614014
Epoch 210  Train loss: -0.06454887133018643  Val loss: 0.09128147240766545
Epoch 211
-------------------------------
Batch 1/64 loss: -0.0669664740562439
Batch 2/64 loss: -0.07036334276199341
Batch 3/64 loss: -0.07690107822418213
Batch 4/64 loss: -0.06912004947662354
Batch 5/64 loss: -0.07742452621459961
Batch 6/64 loss: -0.08406251668930054
Batch 7/64 loss: -0.056461215019226074
Batch 8/64 loss: -0.036980509757995605
Batch 9/64 loss: -0.06066948175430298
Batch 10/64 loss: -0.08098739385604858
Batch 11/64 loss: -0.0874224305152893
Batch 12/64 loss: -0.064949631690979
Batch 13/64 loss: -0.0934402346611023
Batch 14/64 loss: -0.05039411783218384
Batch 15/64 loss: -0.09430229663848877
Batch 16/64 loss: -0.07680857181549072
Batch 17/64 loss: -0.08493256568908691
Batch 18/64 loss: -0.04468250274658203
Batch 19/64 loss: -0.027887165546417236
Batch 20/64 loss: -0.06251227855682373
Batch 21/64 loss: -0.07815611362457275
Batch 22/64 loss: -0.05911731719970703
Batch 23/64 loss: -0.044172048568725586
Batch 24/64 loss: -0.07458668947219849
Batch 25/64 loss: -0.08270514011383057
Batch 26/64 loss: -0.06381821632385254
Batch 27/64 loss: -0.0693773627281189
Batch 28/64 loss: -0.08460372686386108
Batch 29/64 loss: -0.06033921241760254
Batch 30/64 loss: -0.036329448223114014
Batch 31/64 loss: -0.0565953254699707
Batch 32/64 loss: -0.06662851572036743
Batch 33/64 loss: -0.07620072364807129
Batch 34/64 loss: -0.03732335567474365
Batch 35/64 loss: -0.0737115740776062
Batch 36/64 loss: -0.0369342565536499
Batch 37/64 loss: -0.03041863441467285
Batch 38/64 loss: -0.05037575960159302
Batch 39/64 loss: -0.052223920822143555
Batch 40/64 loss: -0.04924452304840088
Batch 41/64 loss: -0.041461825370788574
Batch 42/64 loss: -0.05252808332443237
Batch 43/64 loss: -0.056234538555145264
Batch 44/64 loss: -0.045877933502197266
Batch 45/64 loss: -0.07210636138916016
Batch 46/64 loss: -0.06483912467956543
Batch 47/64 loss: -0.047674715518951416
Batch 48/64 loss: -0.027083218097686768
Batch 49/64 loss: -0.07092666625976562
Batch 50/64 loss: -0.026447653770446777
Batch 51/64 loss: -0.054263949394226074
Batch 52/64 loss: -0.10374641418457031
Batch 53/64 loss: -0.054065823554992676
Batch 54/64 loss: -0.054551124572753906
Batch 55/64 loss: -0.09697359800338745
Batch 56/64 loss: -0.04549813270568848
Batch 57/64 loss: -0.07205390930175781
Batch 58/64 loss: -0.05242711305618286
Batch 59/64 loss: -0.09300708770751953
Batch 60/64 loss: -0.06076216697692871
Batch 61/64 loss: -0.09032225608825684
Batch 62/64 loss: -0.06688868999481201
Batch 63/64 loss: -0.08608108758926392
Batch 64/64 loss: -0.00954502820968628
Epoch 211  Train loss: -0.06263702733843934  Val loss: 0.05378285408839328
Epoch 212
-------------------------------
Batch 1/64 loss: -0.05725991725921631
Batch 2/64 loss: -0.09246975183486938
Batch 3/64 loss: -0.07113432884216309
Batch 4/64 loss: -0.06771773099899292
Batch 5/64 loss: -0.07575863599777222
Batch 6/64 loss: -0.08342742919921875
Batch 7/64 loss: -0.04553765058517456
Batch 8/64 loss: -0.057837605476379395
Batch 9/64 loss: -0.07117927074432373
Batch 10/64 loss: -0.05348128080368042
Batch 11/64 loss: -0.05337846279144287
Batch 12/64 loss: -0.06442528963088989
Batch 13/64 loss: -0.05344480276107788
Batch 14/64 loss: -0.11056196689605713
Batch 15/64 loss: -0.05684483051300049
Batch 16/64 loss: -0.07412862777709961
Batch 17/64 loss: -0.06814265251159668
Batch 18/64 loss: -0.030643045902252197
Batch 19/64 loss: -0.04868006706237793
Batch 20/64 loss: -0.05507540702819824
Batch 21/64 loss: -0.045982182025909424
Batch 22/64 loss: -0.05575007200241089
Batch 23/64 loss: -0.053674161434173584
Batch 24/64 loss: -0.026413798332214355
Batch 25/64 loss: -0.10239666700363159
Batch 26/64 loss: -0.04358208179473877
Batch 27/64 loss: -0.07439815998077393
Batch 28/64 loss: -0.03157311677932739
Batch 29/64 loss: -0.052428483963012695
Batch 30/64 loss: -0.07481139898300171
Batch 31/64 loss: -0.10002851486206055
Batch 32/64 loss: -0.06396257877349854
Batch 33/64 loss: -0.02712428569793701
Batch 34/64 loss: -0.059891581535339355
Batch 35/64 loss: -0.06856471300125122
Batch 36/64 loss: -0.04462718963623047
Batch 37/64 loss: -0.048798441886901855
Batch 38/64 loss: -0.08813047409057617
Batch 39/64 loss: -0.06700146198272705
Batch 40/64 loss: -0.05787009000778198
Batch 41/64 loss: -0.048836588859558105
Batch 42/64 loss: -0.08711498975753784
Batch 43/64 loss: -0.057112038135528564
Batch 44/64 loss: -0.055031657218933105
Batch 45/64 loss: -0.056909024715423584
Batch 46/64 loss: -0.058474957942962646
Batch 47/64 loss: -0.07617342472076416
Batch 48/64 loss: -0.08076459169387817
Batch 49/64 loss: -0.06252968311309814
Batch 50/64 loss: -0.0819401741027832
Batch 51/64 loss: -0.07510441541671753
Batch 52/64 loss: -0.0634315013885498
Batch 53/64 loss: -0.04606205224990845
Batch 54/64 loss: -0.06122267246246338
Batch 55/64 loss: -0.05198413133621216
Batch 56/64 loss: -0.06312412023544312
Batch 57/64 loss: -0.04773777723312378
Batch 58/64 loss: -0.10387939214706421
Batch 59/64 loss: -0.08816933631896973
Batch 60/64 loss: -0.0277097225189209
Batch 61/64 loss: -0.06742632389068604
Batch 62/64 loss: -0.055339813232421875
Batch 63/64 loss: -0.06406962871551514
Batch 64/64 loss: -0.07068061828613281
Epoch 212  Train loss: -0.06292230101192699  Val loss: 0.0565800281734401
Epoch 213
-------------------------------
Batch 1/64 loss: -0.07983177900314331
Batch 2/64 loss: -0.03560441732406616
Batch 3/64 loss: -0.04848074913024902
Batch 4/64 loss: -0.07391387224197388
Batch 5/64 loss: -0.03322559595108032
Batch 6/64 loss: -0.07192331552505493
Batch 7/64 loss: -0.07566273212432861
Batch 8/64 loss: -0.09474313259124756
Batch 9/64 loss: -0.07533520460128784
Batch 10/64 loss: -0.07679098844528198
Batch 11/64 loss: -0.03211396932601929
Batch 12/64 loss: -0.06275522708892822
Batch 13/64 loss: -0.09635686874389648
Batch 14/64 loss: -0.08160001039505005
Batch 15/64 loss: -0.06324112415313721
Batch 16/64 loss: -0.08220803737640381
Batch 17/64 loss: -0.08647167682647705
Batch 18/64 loss: -0.08551526069641113
Batch 19/64 loss: -0.08048486709594727
Batch 20/64 loss: -0.07904690504074097
Batch 21/64 loss: -0.04876822233200073
Batch 22/64 loss: -0.0741267204284668
Batch 23/64 loss: -0.0846412181854248
Batch 24/64 loss: -0.047486305236816406
Batch 25/64 loss: -0.07737624645233154
Batch 26/64 loss: -0.07818710803985596
Batch 27/64 loss: -0.047185540199279785
Batch 28/64 loss: -0.0701596736907959
Batch 29/64 loss: -0.060856759548187256
Batch 30/64 loss: -0.06435835361480713
Batch 31/64 loss: -0.05362415313720703
Batch 32/64 loss: -0.0980309247970581
Batch 33/64 loss: -0.10165756940841675
Batch 34/64 loss: -0.04874420166015625
Batch 35/64 loss: -0.06798583269119263
Batch 36/64 loss: -0.08610492944717407
Batch 37/64 loss: -0.08878147602081299
Batch 38/64 loss: -0.08882415294647217
Batch 39/64 loss: -0.08108949661254883
Batch 40/64 loss: -0.07022809982299805
Batch 41/64 loss: -0.09939384460449219
Batch 42/64 loss: -0.04850351810455322
Batch 43/64 loss: -0.04452669620513916
Batch 44/64 loss: -0.04784327745437622
Batch 45/64 loss: -0.038361430168151855
Batch 46/64 loss: -0.08593648672103882
Batch 47/64 loss: -0.0888633131980896
Batch 48/64 loss: -0.06411266326904297
Batch 49/64 loss: -0.05607396364212036
Batch 50/64 loss: -0.0792006254196167
Batch 51/64 loss: -0.0465126633644104
Batch 52/64 loss: -0.07224345207214355
Batch 53/64 loss: -0.0874016284942627
Batch 54/64 loss: -0.04874897003173828
Batch 55/64 loss: -0.042084455490112305
Batch 56/64 loss: -0.06702065467834473
Batch 57/64 loss: -0.06061899662017822
Batch 58/64 loss: -0.030431687831878662
Batch 59/64 loss: -0.0840960144996643
Batch 60/64 loss: -0.06536471843719482
Batch 61/64 loss: -0.07495999336242676
Batch 62/64 loss: -0.08443719148635864
Batch 63/64 loss: -0.0794605016708374
Batch 64/64 loss: -0.08394759893417358
Epoch 213  Train loss: -0.0692185123761495  Val loss: 0.05412595788228143
Epoch 214
-------------------------------
Batch 1/64 loss: -0.11397683620452881
Batch 2/64 loss: -0.05261087417602539
Batch 3/64 loss: -0.09897243976593018
Batch 4/64 loss: -0.06958699226379395
Batch 5/64 loss: -0.06009674072265625
Batch 6/64 loss: -0.10131537914276123
Batch 7/64 loss: -0.09002679586410522
Batch 8/64 loss: -0.09497439861297607
Batch 9/64 loss: -0.077114999294281
Batch 10/64 loss: -0.047814249992370605
Batch 11/64 loss: -0.06352770328521729
Batch 12/64 loss: -0.0676572322845459
Batch 13/64 loss: -0.07355797290802002
Batch 14/64 loss: -0.06709426641464233
Batch 15/64 loss: -0.05551314353942871
Batch 16/64 loss: -0.0781291127204895
Batch 17/64 loss: -0.07907336950302124
Batch 18/64 loss: -0.05420476198196411
Batch 19/64 loss: -0.03549003601074219
Batch 20/64 loss: -0.08364880084991455
Batch 21/64 loss: -0.07783788442611694
Batch 22/64 loss: -0.08207875490188599
Batch 23/64 loss: -0.0781562328338623
Batch 24/64 loss: -0.07815384864807129
Batch 25/64 loss: -0.05105787515640259
Batch 26/64 loss: -0.06935328245162964
Batch 27/64 loss: -0.07345539331436157
Batch 28/64 loss: -0.09124952554702759
Batch 29/64 loss: -0.06614500284194946
Batch 30/64 loss: -0.07638579607009888
Batch 31/64 loss: -0.06983506679534912
Batch 32/64 loss: -0.08731430768966675
Batch 33/64 loss: -0.060356855392456055
Batch 34/64 loss: -0.10209935903549194
Batch 35/64 loss: -0.06760019063949585
Batch 36/64 loss: -0.05129504203796387
Batch 37/64 loss: -0.08607208728790283
Batch 38/64 loss: -0.05976361036300659
Batch 39/64 loss: -0.06305551528930664
Batch 40/64 loss: -0.05128800868988037
Batch 41/64 loss: -0.07929086685180664
Batch 42/64 loss: -0.06654477119445801
Batch 43/64 loss: -0.06071954965591431
Batch 44/64 loss: -0.08913087844848633
Batch 45/64 loss: -0.057888805866241455
Batch 46/64 loss: -0.04179328680038452
Batch 47/64 loss: -0.05695384740829468
Batch 48/64 loss: -0.03889232873916626
Batch 49/64 loss: -0.056722402572631836
Batch 50/64 loss: -0.053497493267059326
Batch 51/64 loss: -0.09299647808074951
Batch 52/64 loss: -0.05448579788208008
Batch 53/64 loss: -0.060273826122283936
Batch 54/64 loss: -0.03379422426223755
Batch 55/64 loss: -0.05324375629425049
Batch 56/64 loss: -0.05412489175796509
Batch 57/64 loss: -0.04620552062988281
Batch 58/64 loss: -0.08435297012329102
Batch 59/64 loss: -0.08440452814102173
Batch 60/64 loss: -0.1002269983291626
Batch 61/64 loss: -0.04530817270278931
Batch 62/64 loss: -0.03245902061462402
Batch 63/64 loss: -0.06722700595855713
Batch 64/64 loss: -0.03713393211364746
Epoch 214  Train loss: -0.06816200181549671  Val loss: 0.0868722823067629
Epoch 215
-------------------------------
Batch 1/64 loss: -0.0990174412727356
Batch 2/64 loss: -0.061475515365600586
Batch 3/64 loss: -0.06813949346542358
Batch 4/64 loss: -0.09525704383850098
Batch 5/64 loss: -0.07922106981277466
Batch 6/64 loss: -0.10282695293426514
Batch 7/64 loss: -0.013646185398101807
Batch 8/64 loss: -0.04932898283004761
Batch 9/64 loss: -0.10081422328948975
Batch 10/64 loss: -0.0672721266746521
Batch 11/64 loss: -0.043463051319122314
Batch 12/64 loss: -0.09264320135116577
Batch 13/64 loss: -0.1033252477645874
Batch 14/64 loss: -0.07839208841323853
Batch 15/64 loss: -0.07992327213287354
Batch 16/64 loss: -0.08072608709335327
Batch 17/64 loss: -0.07466882467269897
Batch 18/64 loss: -0.08507180213928223
Batch 19/64 loss: -0.025270521640777588
Batch 20/64 loss: -0.07713449001312256
Batch 21/64 loss: -0.015694618225097656
Batch 22/64 loss: -0.09501922130584717
Batch 23/64 loss: -0.08949553966522217
Batch 24/64 loss: -0.0627967119216919
Batch 25/64 loss: -0.05867791175842285
Batch 26/64 loss: -0.06202977895736694
Batch 27/64 loss: -0.07063114643096924
Batch 28/64 loss: -0.05095779895782471
Batch 29/64 loss: -0.08294755220413208
Batch 30/64 loss: -0.05849301815032959
Batch 31/64 loss: -0.07267892360687256
Batch 32/64 loss: -0.049238383769989014
Batch 33/64 loss: -0.08072853088378906
Batch 34/64 loss: -0.052965760231018066
Batch 35/64 loss: -0.0792575478553772
Batch 36/64 loss: -0.06992107629776001
Batch 37/64 loss: -0.0770639181137085
Batch 38/64 loss: -0.06668269634246826
Batch 39/64 loss: -0.10886532068252563
Batch 40/64 loss: -0.09246128797531128
Batch 41/64 loss: -0.0511249303817749
Batch 42/64 loss: -0.051782071590423584
Batch 43/64 loss: -0.06006884574890137
Batch 44/64 loss: -0.04520750045776367
Batch 45/64 loss: -0.07960373163223267
Batch 46/64 loss: -0.07849454879760742
Batch 47/64 loss: -0.054706037044525146
Batch 48/64 loss: -0.06618344783782959
Batch 49/64 loss: -0.05290961265563965
Batch 50/64 loss: -0.03562724590301514
Batch 51/64 loss: -0.03537344932556152
Batch 52/64 loss: -0.07143449783325195
Batch 53/64 loss: -0.07986021041870117
Batch 54/64 loss: -0.02227962017059326
Batch 55/64 loss: -0.07270097732543945
Batch 56/64 loss: -0.08111882209777832
Batch 57/64 loss: -0.07054561376571655
Batch 58/64 loss: -0.06340444087982178
Batch 59/64 loss: -0.04912865161895752
Batch 60/64 loss: -0.058795034885406494
Batch 61/64 loss: -0.059829533100128174
Batch 62/64 loss: -0.04177016019821167
Batch 63/64 loss: -0.09066814184188843
Batch 64/64 loss: -0.06354743242263794
Epoch 215  Train loss: -0.06736473826801076  Val loss: 0.0617935012296303
Epoch 216
-------------------------------
Batch 1/64 loss: -0.055273354053497314
Batch 2/64 loss: -0.08203774690628052
Batch 3/64 loss: -0.07513415813446045
Batch 4/64 loss: -0.049841463565826416
Batch 5/64 loss: -0.02863675355911255
Batch 6/64 loss: -0.07163578271865845
Batch 7/64 loss: -0.06956326961517334
Batch 8/64 loss: -0.08932554721832275
Batch 9/64 loss: -0.06832265853881836
Batch 10/64 loss: -0.06874632835388184
Batch 11/64 loss: -0.08169585466384888
Batch 12/64 loss: -0.04698300361633301
Batch 13/64 loss: -0.08394646644592285
Batch 14/64 loss: -0.08334267139434814
Batch 15/64 loss: -0.05619311332702637
Batch 16/64 loss: -0.07973289489746094
Batch 17/64 loss: -0.08155298233032227
Batch 18/64 loss: -0.07659900188446045
Batch 19/64 loss: -0.055433809757232666
Batch 20/64 loss: -0.09164488315582275
Batch 21/64 loss: -0.09872668981552124
Batch 22/64 loss: -0.0824514627456665
Batch 23/64 loss: -0.0553165078163147
Batch 24/64 loss: -0.08090734481811523
Batch 25/64 loss: -0.09448409080505371
Batch 26/64 loss: -0.04135030508041382
Batch 27/64 loss: -0.06484061479568481
Batch 28/64 loss: -0.040868401527404785
Batch 29/64 loss: -0.08489084243774414
Batch 30/64 loss: -0.07280468940734863
Batch 31/64 loss: -0.08138316869735718
Batch 32/64 loss: -0.05457824468612671
Batch 33/64 loss: -0.08474862575531006
Batch 34/64 loss: -0.08239048719406128
Batch 35/64 loss: -0.06796211004257202
Batch 36/64 loss: -0.08132123947143555
Batch 37/64 loss: -0.059120774269104004
Batch 38/64 loss: -0.07797306776046753
Batch 39/64 loss: -0.060686469078063965
Batch 40/64 loss: -0.08599674701690674
Batch 41/64 loss: -0.048581838607788086
Batch 42/64 loss: -0.04047703742980957
Batch 43/64 loss: -0.05324023962020874
Batch 44/64 loss: -0.07201051712036133
Batch 45/64 loss: -0.0556827187538147
Batch 46/64 loss: -0.0568578839302063
Batch 47/64 loss: -0.083193838596344
Batch 48/64 loss: -0.06471604108810425
Batch 49/64 loss: -0.06458812952041626
Batch 50/64 loss: -0.02234470844268799
Batch 51/64 loss: -0.09969586133956909
Batch 52/64 loss: -0.09008342027664185
Batch 53/64 loss: -0.0494767427444458
Batch 54/64 loss: -0.049498796463012695
Batch 55/64 loss: -0.09201377630233765
Batch 56/64 loss: -0.04191887378692627
Batch 57/64 loss: -0.0630335807800293
Batch 58/64 loss: -0.04125934839248657
Batch 59/64 loss: -0.07755565643310547
Batch 60/64 loss: -0.03804570436477661
Batch 61/64 loss: -0.052154362201690674
Batch 62/64 loss: -0.0723455548286438
Batch 63/64 loss: -0.06096893548965454
Batch 64/64 loss: -0.05731558799743652
Epoch 216  Train loss: -0.06715566878225289  Val loss: 0.054907731583847624
Epoch 217
-------------------------------
Batch 1/64 loss: -0.08486080169677734
Batch 2/64 loss: -0.07132309675216675
Batch 3/64 loss: -0.08559036254882812
Batch 4/64 loss: -0.04120981693267822
Batch 5/64 loss: -0.07981294393539429
Batch 6/64 loss: -0.08337914943695068
Batch 7/64 loss: -0.07650983333587646
Batch 8/64 loss: -0.01924121379852295
Batch 9/64 loss: -0.08762335777282715
Batch 10/64 loss: -0.07753902673721313
Batch 11/64 loss: -0.07189315557479858
Batch 12/64 loss: -0.05758625268936157
Batch 13/64 loss: -0.06411314010620117
Batch 14/64 loss: -0.04648244380950928
Batch 15/64 loss: -0.08583927154541016
Batch 16/64 loss: -0.08626699447631836
Batch 17/64 loss: -0.09671038389205933
Batch 18/64 loss: -0.06087011098861694
Batch 19/64 loss: -0.09683042764663696
Batch 20/64 loss: -0.040333688259124756
Batch 21/64 loss: -0.05121338367462158
Batch 22/64 loss: -0.07136189937591553
Batch 23/64 loss: -0.05324941873550415
Batch 24/64 loss: -0.09229141473770142
Batch 25/64 loss: -0.04215693473815918
Batch 26/64 loss: -0.06028604507446289
Batch 27/64 loss: -0.06694304943084717
Batch 28/64 loss: -0.04907459020614624
Batch 29/64 loss: -0.07665109634399414
Batch 30/64 loss: -0.050647616386413574
Batch 31/64 loss: -0.09516263008117676
Batch 32/64 loss: -0.08391767740249634
Batch 33/64 loss: -0.06709122657775879
Batch 34/64 loss: -0.07429653406143188
Batch 35/64 loss: -0.060030996799468994
Batch 36/64 loss: -0.0807155966758728
Batch 37/64 loss: -0.05692249536514282
Batch 38/64 loss: -0.060254037380218506
Batch 39/64 loss: -0.03603130578994751
Batch 40/64 loss: -0.06994163990020752
Batch 41/64 loss: -0.0685078501701355
Batch 42/64 loss: -0.04752153158187866
Batch 43/64 loss: -0.07024478912353516
Batch 44/64 loss: -0.03587353229522705
Batch 45/64 loss: -0.05645662546157837
Batch 46/64 loss: -0.05781280994415283
Batch 47/64 loss: -0.08637213706970215
Batch 48/64 loss: -0.09327858686447144
Batch 49/64 loss: -0.04532235860824585
Batch 50/64 loss: -0.09509378671646118
Batch 51/64 loss: -0.08243060111999512
Batch 52/64 loss: -0.07183551788330078
Batch 53/64 loss: -0.04238969087600708
Batch 54/64 loss: -0.0915682315826416
Batch 55/64 loss: -0.0523301362991333
Batch 56/64 loss: -0.07451659440994263
Batch 57/64 loss: -0.05130791664123535
Batch 58/64 loss: -0.06285381317138672
Batch 59/64 loss: -0.0927894115447998
Batch 60/64 loss: -0.08053874969482422
Batch 61/64 loss: -0.05722177028656006
Batch 62/64 loss: -0.07026630640029907
Batch 63/64 loss: -0.064846932888031
Batch 64/64 loss: -0.05162012577056885
Epoch 217  Train loss: -0.06748784055896834  Val loss: 0.06631835260751731
Epoch 218
-------------------------------
Batch 1/64 loss: -0.055133819580078125
Batch 2/64 loss: -0.08369225263595581
Batch 3/64 loss: -0.09756886959075928
Batch 4/64 loss: -0.08354783058166504
Batch 5/64 loss: -0.09724044799804688
Batch 6/64 loss: -0.06285214424133301
Batch 7/64 loss: -0.08483946323394775
Batch 8/64 loss: -0.051536381244659424
Batch 9/64 loss: -0.08058905601501465
Batch 10/64 loss: -0.0553320050239563
Batch 11/64 loss: -0.09351277351379395
Batch 12/64 loss: -0.054439425468444824
Batch 13/64 loss: -0.10585141181945801
Batch 14/64 loss: -0.09091895818710327
Batch 15/64 loss: -0.05522650480270386
Batch 16/64 loss: -0.09306490421295166
Batch 17/64 loss: -0.02915138006210327
Batch 18/64 loss: -0.05201297998428345
Batch 19/64 loss: -0.06300199031829834
Batch 20/64 loss: -0.07887142896652222
Batch 21/64 loss: -0.07079505920410156
Batch 22/64 loss: -0.05285525321960449
Batch 23/64 loss: -0.060423851013183594
Batch 24/64 loss: -0.08110237121582031
Batch 25/64 loss: -0.016466379165649414
Batch 26/64 loss: -0.061867713928222656
Batch 27/64 loss: -0.10076701641082764
Batch 28/64 loss: -0.06676959991455078
Batch 29/64 loss: -0.08476561307907104
Batch 30/64 loss: -0.06819283962249756
Batch 31/64 loss: -0.0735940933227539
Batch 32/64 loss: -0.05180704593658447
Batch 33/64 loss: -0.09389793872833252
Batch 34/64 loss: -0.08165085315704346
Batch 35/64 loss: -0.06697022914886475
Batch 36/64 loss: -0.10980218648910522
Batch 37/64 loss: -0.07858413457870483
Batch 38/64 loss: -0.05683934688568115
Batch 39/64 loss: -0.0871056318283081
Batch 40/64 loss: -0.030173301696777344
Batch 41/64 loss: -0.025408267974853516
Batch 42/64 loss: -0.05422419309616089
Batch 43/64 loss: -0.014291524887084961
Batch 44/64 loss: -0.05863451957702637
Batch 45/64 loss: -0.060628294944763184
Batch 46/64 loss: -0.035655081272125244
Batch 47/64 loss: -0.05404520034790039
Batch 48/64 loss: -0.07555609941482544
Batch 49/64 loss: -0.084297776222229
Batch 50/64 loss: -0.09198075532913208
Batch 51/64 loss: -0.05005300045013428
Batch 52/64 loss: -0.06069529056549072
Batch 53/64 loss: -0.061857521533966064
Batch 54/64 loss: -0.04271245002746582
Batch 55/64 loss: -0.06664085388183594
Batch 56/64 loss: -0.07513952255249023
Batch 57/64 loss: -0.07732647657394409
Batch 58/64 loss: -0.05638009309768677
Batch 59/64 loss: -0.07424342632293701
Batch 60/64 loss: -0.08302909135818481
Batch 61/64 loss: -0.07065975666046143
Batch 62/64 loss: -0.057011187076568604
Batch 63/64 loss: -0.0662926435470581
Batch 64/64 loss: -0.05776923894882202
Epoch 218  Train loss: -0.06749654026592479  Val loss: 0.05133188446772467
Epoch 219
-------------------------------
Batch 1/64 loss: -0.07795554399490356
Batch 2/64 loss: -0.052715003490448
Batch 3/64 loss: -0.10616886615753174
Batch 4/64 loss: -0.09821438789367676
Batch 5/64 loss: -0.07174605131149292
Batch 6/64 loss: -0.09060090780258179
Batch 7/64 loss: -0.10484397411346436
Batch 8/64 loss: -0.06211739778518677
Batch 9/64 loss: -0.07554221153259277
Batch 10/64 loss: -0.08560574054718018
Batch 11/64 loss: -0.06271916627883911
Batch 12/64 loss: -0.07173919677734375
Batch 13/64 loss: -0.08949542045593262
Batch 14/64 loss: -0.08762329816818237
Batch 15/64 loss: -0.05048149824142456
Batch 16/64 loss: -0.05288422107696533
Batch 17/64 loss: -0.08041292428970337
Batch 18/64 loss: -0.06939834356307983
Batch 19/64 loss: -0.07649636268615723
Batch 20/64 loss: -0.08727794885635376
Batch 21/64 loss: -0.07942724227905273
Batch 22/64 loss: -0.06384789943695068
Batch 23/64 loss: -0.0519406795501709
Batch 24/64 loss: -0.02383023500442505
Batch 25/64 loss: -0.04725992679595947
Batch 26/64 loss: -0.06239438056945801
Batch 27/64 loss: -0.0769495964050293
Batch 28/64 loss: -0.07812720537185669
Batch 29/64 loss: -0.06531959772109985
Batch 30/64 loss: -0.029087722301483154
Batch 31/64 loss: -0.07095563411712646
Batch 32/64 loss: -0.07206231355667114
Batch 33/64 loss: -0.05620509386062622
Batch 34/64 loss: -0.06648331880569458
Batch 35/64 loss: -0.07640337944030762
Batch 36/64 loss: -0.06326329708099365
Batch 37/64 loss: -0.06999439001083374
Batch 38/64 loss: -0.07900732755661011
Batch 39/64 loss: -0.08683323860168457
Batch 40/64 loss: -0.07679718732833862
Batch 41/64 loss: -0.06072360277175903
Batch 42/64 loss: -0.07821303606033325
Batch 43/64 loss: -0.05333292484283447
Batch 44/64 loss: -0.056985318660736084
Batch 45/64 loss: -0.05750548839569092
Batch 46/64 loss: -0.03716123104095459
Batch 47/64 loss: -0.045108675956726074
Batch 48/64 loss: -0.07439219951629639
Batch 49/64 loss: -0.06764954328536987
Batch 50/64 loss: -0.06807368993759155
Batch 51/64 loss: -0.06621396541595459
Batch 52/64 loss: -0.064869225025177
Batch 53/64 loss: -0.07773077487945557
Batch 54/64 loss: -0.0568888783454895
Batch 55/64 loss: -0.07057350873947144
Batch 56/64 loss: -0.0929574966430664
Batch 57/64 loss: -0.07997709512710571
Batch 58/64 loss: -0.062395453453063965
Batch 59/64 loss: -0.08817118406295776
Batch 60/64 loss: -0.04766666889190674
Batch 61/64 loss: -0.05469679832458496
Batch 62/64 loss: -0.09789973497390747
Batch 63/64 loss: -0.075783371925354
Batch 64/64 loss: -0.051908254623413086
Epoch 219  Train loss: -0.06939810491075703  Val loss: 0.062150724360213655
Epoch 220
-------------------------------
Batch 1/64 loss: -0.07025396823883057
Batch 2/64 loss: -0.05959576368331909
Batch 3/64 loss: -0.07907015085220337
Batch 4/64 loss: -0.08152395486831665
Batch 5/64 loss: -0.0645214319229126
Batch 6/64 loss: -0.06871253252029419
Batch 7/64 loss: -0.08790677785873413
Batch 8/64 loss: -0.1038510799407959
Batch 9/64 loss: -0.06247389316558838
Batch 10/64 loss: -0.09127181768417358
Batch 11/64 loss: -0.06344711780548096
Batch 12/64 loss: -0.0780630111694336
Batch 13/64 loss: -0.05400991439819336
Batch 14/64 loss: -0.06088721752166748
Batch 15/64 loss: -0.11302286386489868
Batch 16/64 loss: -0.0587763786315918
Batch 17/64 loss: -0.061551809310913086
Batch 18/64 loss: -0.04642081260681152
Batch 19/64 loss: -0.06229513883590698
Batch 20/64 loss: -0.056304097175598145
Batch 21/64 loss: -0.0877043604850769
Batch 22/64 loss: -0.0635424256324768
Batch 23/64 loss: -0.06393676996231079
Batch 24/64 loss: -0.06284332275390625
Batch 25/64 loss: -0.050029456615448
Batch 26/64 loss: -0.1022985577583313
Batch 27/64 loss: -0.07379776239395142
Batch 28/64 loss: -0.08056151866912842
Batch 29/64 loss: -0.08833122253417969
Batch 30/64 loss: -0.07832503318786621
Batch 31/64 loss: -0.05027198791503906
Batch 32/64 loss: -0.05071169137954712
Batch 33/64 loss: -0.08595055341720581
Batch 34/64 loss: -0.04683339595794678
Batch 35/64 loss: -0.07061988115310669
Batch 36/64 loss: -0.07880568504333496
Batch 37/64 loss: -0.07226055860519409
Batch 38/64 loss: -0.05872511863708496
Batch 39/64 loss: -0.07862907648086548
Batch 40/64 loss: -0.08857429027557373
Batch 41/64 loss: -0.031416237354278564
Batch 42/64 loss: -0.0913657546043396
Batch 43/64 loss: -0.08948951959609985
Batch 44/64 loss: -0.03707695007324219
Batch 45/64 loss: -0.03718382120132446
Batch 46/64 loss: -0.05488932132720947
Batch 47/64 loss: -0.08965378999710083
Batch 48/64 loss: -0.08992689847946167
Batch 49/64 loss: -0.05491727590560913
Batch 50/64 loss: -0.08464151620864868
Batch 51/64 loss: -0.10569530725479126
Batch 52/64 loss: -0.047338664531707764
Batch 53/64 loss: -0.060829102993011475
Batch 54/64 loss: -0.09400361776351929
Batch 55/64 loss: -0.06698399782180786
Batch 56/64 loss: -0.07468545436859131
Batch 57/64 loss: -0.09325718879699707
Batch 58/64 loss: -0.09351325035095215
Batch 59/64 loss: -0.06954836845397949
Batch 60/64 loss: -0.055652737617492676
Batch 61/64 loss: -0.03623521327972412
Batch 62/64 loss: -0.051965296268463135
Batch 63/64 loss: -0.07037699222564697
Batch 64/64 loss: -0.05517852306365967
Epoch 220  Train loss: -0.07025478540682326  Val loss: 0.054024641661299874
Epoch 221
-------------------------------
Batch 1/64 loss: -0.07705295085906982
Batch 2/64 loss: -0.07479697465896606
Batch 3/64 loss: -0.053304195404052734
Batch 4/64 loss: -0.07424616813659668
Batch 5/64 loss: -0.05872386693954468
Batch 6/64 loss: -0.0005620718002319336
Batch 7/64 loss: -0.09408587217330933
Batch 8/64 loss: -0.0724790096282959
Batch 9/64 loss: -0.05067789554595947
Batch 10/64 loss: -0.08942669630050659
Batch 11/64 loss: -0.03519022464752197
Batch 12/64 loss: -0.0690685510635376
Batch 13/64 loss: -0.10036927461624146
Batch 14/64 loss: -0.07924318313598633
Batch 15/64 loss: -0.10128629207611084
Batch 16/64 loss: -0.08827465772628784
Batch 17/64 loss: -0.09138178825378418
Batch 18/64 loss: -0.08387595415115356
Batch 19/64 loss: -0.06083124876022339
Batch 20/64 loss: -0.07932102680206299
Batch 21/64 loss: -0.053640007972717285
Batch 22/64 loss: -0.0804365873336792
Batch 23/64 loss: -0.07743501663208008
Batch 24/64 loss: -0.10523927211761475
Batch 25/64 loss: -0.04212307929992676
Batch 26/64 loss: -0.08415812253952026
Batch 27/64 loss: -0.08901071548461914
Batch 28/64 loss: -0.09517562389373779
Batch 29/64 loss: -0.06021392345428467
Batch 30/64 loss: -0.06612014770507812
Batch 31/64 loss: -0.08563673496246338
Batch 32/64 loss: -0.07164180278778076
Batch 33/64 loss: -0.04130631685256958
Batch 34/64 loss: -0.0636109709739685
Batch 35/64 loss: -0.07264840602874756
Batch 36/64 loss: -0.06236541271209717
Batch 37/64 loss: -0.06280326843261719
Batch 38/64 loss: -0.05710124969482422
Batch 39/64 loss: -0.08110904693603516
Batch 40/64 loss: -0.052824318408966064
Batch 41/64 loss: -0.08016222715377808
Batch 42/64 loss: -0.06603091955184937
Batch 43/64 loss: -0.09153616428375244
Batch 44/64 loss: -0.08841848373413086
Batch 45/64 loss: -0.028971493244171143
Batch 46/64 loss: -0.07761716842651367
Batch 47/64 loss: -0.0931132435798645
Batch 48/64 loss: -0.08806633949279785
Batch 49/64 loss: -0.07402366399765015
Batch 50/64 loss: -0.059578657150268555
Batch 51/64 loss: -0.03512907028198242
Batch 52/64 loss: -0.053295671939849854
Batch 53/64 loss: -0.06196075677871704
Batch 54/64 loss: -0.06220114231109619
Batch 55/64 loss: -0.04165524244308472
Batch 56/64 loss: -0.08146721124649048
Batch 57/64 loss: -0.06259649991989136
Batch 58/64 loss: -0.07450056076049805
Batch 59/64 loss: -0.04097700119018555
Batch 60/64 loss: -0.09495019912719727
Batch 61/64 loss: -0.06902223825454712
Batch 62/64 loss: -0.08583199977874756
Batch 63/64 loss: -0.084841787815094
Batch 64/64 loss: -0.06019800901412964
Epoch 221  Train loss: -0.07027284982157689  Val loss: 0.05834684339175929
Epoch 222
-------------------------------
Batch 1/64 loss: -0.05432617664337158
Batch 2/64 loss: -0.09043467044830322
Batch 3/64 loss: -0.0347212553024292
Batch 4/64 loss: -0.07451200485229492
Batch 5/64 loss: -0.09442025423049927
Batch 6/64 loss: -0.06451553106307983
Batch 7/64 loss: -0.06984865665435791
Batch 8/64 loss: -0.09111040830612183
Batch 9/64 loss: -0.05377209186553955
Batch 10/64 loss: -0.08425188064575195
Batch 11/64 loss: -0.034873008728027344
Batch 12/64 loss: -0.08771651983261108
Batch 13/64 loss: -0.04913979768753052
Batch 14/64 loss: -0.09027040004730225
Batch 15/64 loss: -0.07495671510696411
Batch 16/64 loss: -0.05549699068069458
Batch 17/64 loss: -0.09039747714996338
Batch 18/64 loss: -0.05486267805099487
Batch 19/64 loss: -0.06130480766296387
Batch 20/64 loss: -0.07211542129516602
Batch 21/64 loss: -0.08348017930984497
Batch 22/64 loss: -0.08508694171905518
Batch 23/64 loss: -0.1060945987701416
Batch 24/64 loss: -0.05928105115890503
Batch 25/64 loss: -0.0749402642250061
Batch 26/64 loss: -0.05602604150772095
Batch 27/64 loss: -0.07974809408187866
Batch 28/64 loss: -0.07455652952194214
Batch 29/64 loss: -0.07250863313674927
Batch 30/64 loss: -0.05530714988708496
Batch 31/64 loss: -0.0782654881477356
Batch 32/64 loss: -0.10724616050720215
Batch 33/64 loss: -0.09737539291381836
Batch 34/64 loss: -0.054673075675964355
Batch 35/64 loss: -0.0908089280128479
Batch 36/64 loss: -0.056721627712249756
Batch 37/64 loss: -0.05412018299102783
Batch 38/64 loss: -0.07768267393112183
Batch 39/64 loss: -0.06674331426620483
Batch 40/64 loss: -0.08822262287139893
Batch 41/64 loss: -0.04959738254547119
Batch 42/64 loss: -0.03669416904449463
Batch 43/64 loss: -0.0774073600769043
Batch 44/64 loss: -0.1077008843421936
Batch 45/64 loss: -0.053415656089782715
Batch 46/64 loss: -0.06772875785827637
Batch 47/64 loss: -0.07654821872711182
Batch 48/64 loss: -0.08862441778182983
Batch 49/64 loss: -0.05242323875427246
Batch 50/64 loss: -0.05009979009628296
Batch 51/64 loss: -0.07961750030517578
Batch 52/64 loss: -0.06075143814086914
Batch 53/64 loss: -0.059221744537353516
Batch 54/64 loss: -0.08509761095046997
Batch 55/64 loss: -0.09563767910003662
Batch 56/64 loss: -0.07324117422103882
Batch 57/64 loss: -0.053880274295806885
Batch 58/64 loss: -0.08763754367828369
Batch 59/64 loss: -0.07115811109542847
Batch 60/64 loss: -0.05807030200958252
Batch 61/64 loss: -0.07086026668548584
Batch 62/64 loss: -0.06599396467208862
Batch 63/64 loss: -0.0902249813079834
Batch 64/64 loss: -0.06773066520690918
Epoch 222  Train loss: -0.07159790057761996  Val loss: 0.06368490449341711
Epoch 223
-------------------------------
Batch 1/64 loss: -0.08426892757415771
Batch 2/64 loss: -0.07716667652130127
Batch 3/64 loss: -0.08728933334350586
Batch 4/64 loss: -0.04723566770553589
Batch 5/64 loss: -0.06650996208190918
Batch 6/64 loss: -0.0861748456954956
Batch 7/64 loss: -0.08234333992004395
Batch 8/64 loss: -0.08994680643081665
Batch 9/64 loss: -0.09612101316452026
Batch 10/64 loss: -0.08336889743804932
Batch 11/64 loss: -0.05929279327392578
Batch 12/64 loss: -0.08338069915771484
Batch 13/64 loss: -0.07062190771102905
Batch 14/64 loss: -0.0852823257446289
Batch 15/64 loss: -0.049316227436065674
Batch 16/64 loss: -0.10229372978210449
Batch 17/64 loss: -0.06811881065368652
Batch 18/64 loss: -0.06440907716751099
Batch 19/64 loss: -0.0756235122680664
Batch 20/64 loss: -0.058358073234558105
Batch 21/64 loss: -0.07972192764282227
Batch 22/64 loss: -0.07426339387893677
Batch 23/64 loss: -0.09002417325973511
Batch 24/64 loss: -0.10251009464263916
Batch 25/64 loss: -0.10344469547271729
Batch 26/64 loss: -0.06250876188278198
Batch 27/64 loss: -0.04314243793487549
Batch 28/64 loss: -0.07139664888381958
Batch 29/64 loss: -0.09151798486709595
Batch 30/64 loss: -0.06607866287231445
Batch 31/64 loss: -0.08489835262298584
Batch 32/64 loss: -0.06659847497940063
Batch 33/64 loss: -0.08707040548324585
Batch 34/64 loss: -0.06396281719207764
Batch 35/64 loss: -0.05515366792678833
Batch 36/64 loss: -0.07564663887023926
Batch 37/64 loss: -0.07967674732208252
Batch 38/64 loss: -0.06109929084777832
Batch 39/64 loss: -0.07977080345153809
Batch 40/64 loss: -0.0436859130859375
Batch 41/64 loss: -0.0909833312034607
Batch 42/64 loss: -0.06343889236450195
Batch 43/64 loss: -0.0710260272026062
Batch 44/64 loss: -0.061028242111206055
Batch 45/64 loss: -0.07248401641845703
Batch 46/64 loss: -0.07324349880218506
Batch 47/64 loss: -0.08918821811676025
Batch 48/64 loss: -0.08912456035614014
Batch 49/64 loss: -0.05498373508453369
Batch 50/64 loss: -0.07052606344223022
Batch 51/64 loss: -0.09462684392929077
Batch 52/64 loss: -0.054489195346832275
Batch 53/64 loss: -0.05784177780151367
Batch 54/64 loss: -0.012774467468261719
Batch 55/64 loss: -0.07860451936721802
Batch 56/64 loss: -0.06010860204696655
Batch 57/64 loss: -0.0947563648223877
Batch 58/64 loss: -0.09273701906204224
Batch 59/64 loss: -0.08758097887039185
Batch 60/64 loss: -0.06287235021591187
Batch 61/64 loss: -0.07639217376708984
Batch 62/64 loss: -0.07605278491973877
Batch 63/64 loss: -0.06574463844299316
Batch 64/64 loss: -0.032979726791381836
Epoch 223  Train loss: -0.07332764887342266  Val loss: 0.06436935917208694
Epoch 224
-------------------------------
Batch 1/64 loss: -0.08999401330947876
Batch 2/64 loss: -0.08093434572219849
Batch 3/64 loss: -0.09266799688339233
Batch 4/64 loss: -0.08085280656814575
Batch 5/64 loss: -0.1009901762008667
Batch 6/64 loss: -0.05820333957672119
Batch 7/64 loss: -0.0849151611328125
Batch 8/64 loss: -0.025886237621307373
Batch 9/64 loss: -0.08769804239273071
Batch 10/64 loss: -0.06648385524749756
Batch 11/64 loss: -0.08575296401977539
Batch 12/64 loss: -0.0713011622428894
Batch 13/64 loss: -0.08496201038360596
Batch 14/64 loss: -0.08091509342193604
Batch 15/64 loss: -0.05653887987136841
Batch 16/64 loss: -0.07554411888122559
Batch 17/64 loss: -0.05563807487487793
Batch 18/64 loss: -0.07768368721008301
Batch 19/64 loss: -0.09491467475891113
Batch 20/64 loss: -0.05965149402618408
Batch 21/64 loss: -0.07354384660720825
Batch 22/64 loss: -0.08717989921569824
Batch 23/64 loss: -0.059344470500946045
Batch 24/64 loss: -0.0853813886642456
Batch 25/64 loss: -0.037250399589538574
Batch 26/64 loss: -0.08417904376983643
Batch 27/64 loss: -0.07222962379455566
Batch 28/64 loss: -0.07841378450393677
Batch 29/64 loss: -0.06352120637893677
Batch 30/64 loss: -0.0987047553062439
Batch 31/64 loss: -0.08061492443084717
Batch 32/64 loss: -0.11489373445510864
Batch 33/64 loss: -0.0860433578491211
Batch 34/64 loss: -0.06620943546295166
Batch 35/64 loss: -0.08026868104934692
Batch 36/64 loss: -0.09945625066757202
Batch 37/64 loss: -0.0733826756477356
Batch 38/64 loss: -0.04963386058807373
Batch 39/64 loss: -0.07966387271881104
Batch 40/64 loss: -0.0700387954711914
Batch 41/64 loss: -0.03709608316421509
Batch 42/64 loss: -0.05426514148712158
Batch 43/64 loss: -0.07264167070388794
Batch 44/64 loss: -0.03372669219970703
Batch 45/64 loss: -0.07286405563354492
Batch 46/64 loss: -0.09361815452575684
Batch 47/64 loss: -0.09108340740203857
Batch 48/64 loss: -0.09318387508392334
Batch 49/64 loss: -0.05165278911590576
Batch 50/64 loss: -0.06621676683425903
Batch 51/64 loss: -0.09502077102661133
Batch 52/64 loss: -0.08832234144210815
Batch 53/64 loss: -0.061493635177612305
Batch 54/64 loss: -0.07759130001068115
Batch 55/64 loss: -0.06181305646896362
Batch 56/64 loss: -0.06786644458770752
Batch 57/64 loss: -0.08340221643447876
Batch 58/64 loss: -0.06594479084014893
Batch 59/64 loss: -0.05255621671676636
Batch 60/64 loss: -0.09406858682632446
Batch 61/64 loss: -0.09986215829849243
Batch 62/64 loss: -0.07206881046295166
Batch 63/64 loss: -0.04998713731765747
Batch 64/64 loss: -0.07256656885147095
Epoch 224  Train loss: -0.07438828501046872  Val loss: 0.06406529498673796
Epoch 225
-------------------------------
Batch 1/64 loss: -0.07434284687042236
Batch 2/64 loss: -0.06596404314041138
Batch 3/64 loss: -0.04212319850921631
Batch 4/64 loss: -0.11161291599273682
Batch 5/64 loss: -0.0929986834526062
Batch 6/64 loss: -0.05859637260437012
Batch 7/64 loss: -0.08992242813110352
Batch 8/64 loss: -0.044113874435424805
Batch 9/64 loss: -0.06140249967575073
Batch 10/64 loss: -0.08209460973739624
Batch 11/64 loss: -0.09241169691085815
Batch 12/64 loss: -0.05510115623474121
Batch 13/64 loss: -0.032930076122283936
Batch 14/64 loss: -0.06381958723068237
Batch 15/64 loss: -0.0806850790977478
Batch 16/64 loss: -0.09496736526489258
Batch 17/64 loss: -0.054361045360565186
Batch 18/64 loss: -0.07586926221847534
Batch 19/64 loss: -0.08365476131439209
Batch 20/64 loss: -0.10197681188583374
Batch 21/64 loss: -0.020351827144622803
Batch 22/64 loss: -0.10961633920669556
Batch 23/64 loss: -0.07804632186889648
Batch 24/64 loss: -0.0902300477027893
Batch 25/64 loss: -0.02246922254562378
Batch 26/64 loss: -0.07215452194213867
Batch 27/64 loss: -0.09930497407913208
Batch 28/64 loss: -0.06685149669647217
Batch 29/64 loss: -0.07902109622955322
Batch 30/64 loss: -0.08161020278930664
Batch 31/64 loss: -0.07400870323181152
Batch 32/64 loss: -0.06885677576065063
Batch 33/64 loss: -0.08540761470794678
Batch 34/64 loss: -0.11005616188049316
Batch 35/64 loss: -0.08518815040588379
Batch 36/64 loss: -0.08441245555877686
Batch 37/64 loss: -0.0825343132019043
Batch 38/64 loss: -0.06640756130218506
Batch 39/64 loss: -0.06968778371810913
Batch 40/64 loss: -0.05922579765319824
Batch 41/64 loss: -0.07165205478668213
Batch 42/64 loss: -0.07521402835845947
Batch 43/64 loss: -0.06751179695129395
Batch 44/64 loss: -0.084933340549469
Batch 45/64 loss: -0.06560748815536499
Batch 46/64 loss: -0.07004755735397339
Batch 47/64 loss: -0.04881197214126587
Batch 48/64 loss: -0.07659697532653809
Batch 49/64 loss: -0.05986452102661133
Batch 50/64 loss: -0.06756603717803955
Batch 51/64 loss: -0.10000181198120117
Batch 52/64 loss: -0.08081567287445068
Batch 53/64 loss: -0.043273329734802246
Batch 54/64 loss: -0.058773934841156006
Batch 55/64 loss: -0.06982421875
Batch 56/64 loss: -0.07748514413833618
Batch 57/64 loss: -0.07392674684524536
Batch 58/64 loss: -0.10390275716781616
Batch 59/64 loss: -0.043467164039611816
Batch 60/64 loss: -0.08548271656036377
Batch 61/64 loss: -0.0739588737487793
Batch 62/64 loss: -0.0403975248336792
Batch 63/64 loss: -0.06499379873275757
Batch 64/64 loss: -0.1029958724975586
Epoch 225  Train loss: -0.07287444787866929  Val loss: 0.06057553909898214
Epoch 226
-------------------------------
Batch 1/64 loss: -0.0890151858329773
Batch 2/64 loss: -0.06563293933868408
Batch 3/64 loss: -0.04722392559051514
Batch 4/64 loss: -0.10386139154434204
Batch 5/64 loss: -0.07717186212539673
Batch 6/64 loss: -0.061654090881347656
Batch 7/64 loss: -0.06527626514434814
Batch 8/64 loss: -0.03355693817138672
Batch 9/64 loss: -0.08318877220153809
Batch 10/64 loss: -0.09525907039642334
Batch 11/64 loss: -0.07058167457580566
Batch 12/64 loss: -0.04419910907745361
Batch 13/64 loss: -0.09016048908233643
Batch 14/64 loss: -0.0580098032951355
Batch 15/64 loss: -0.09525918960571289
Batch 16/64 loss: -0.07491999864578247
Batch 17/64 loss: -0.047031939029693604
Batch 18/64 loss: -0.06555777788162231
Batch 19/64 loss: -0.102306067943573
Batch 20/64 loss: -0.051175475120544434
Batch 21/64 loss: -0.10005611181259155
Batch 22/64 loss: -0.049323439598083496
Batch 23/64 loss: -0.07882839441299438
Batch 24/64 loss: -0.06116241216659546
Batch 25/64 loss: -0.09614968299865723
Batch 26/64 loss: -0.08518481254577637
Batch 27/64 loss: -0.0825660228729248
Batch 28/64 loss: -0.10587859153747559
Batch 29/64 loss: -0.07410162687301636
Batch 30/64 loss: -0.0763654112815857
Batch 31/64 loss: -0.05200272798538208
Batch 32/64 loss: -0.07359051704406738
Batch 33/64 loss: -0.1071237325668335
Batch 34/64 loss: -0.06258213520050049
Batch 35/64 loss: -0.10124480724334717
Batch 36/64 loss: -0.058110713958740234
Batch 37/64 loss: -0.09439623355865479
Batch 38/64 loss: -0.058406949043273926
Batch 39/64 loss: -0.07200342416763306
Batch 40/64 loss: -0.060309767723083496
Batch 41/64 loss: -0.09898418188095093
Batch 42/64 loss: -0.09102141857147217
Batch 43/64 loss: -0.08668935298919678
Batch 44/64 loss: -0.07891440391540527
Batch 45/64 loss: -0.06762981414794922
Batch 46/64 loss: -0.08717364072799683
Batch 47/64 loss: -0.059560418128967285
Batch 48/64 loss: -0.050251126289367676
Batch 49/64 loss: -0.08823835849761963
Batch 50/64 loss: -0.08805632591247559
Batch 51/64 loss: -0.08140826225280762
Batch 52/64 loss: -0.06696826219558716
Batch 53/64 loss: -0.06783920526504517
Batch 54/64 loss: -0.08605468273162842
Batch 55/64 loss: -0.07494276762008667
Batch 56/64 loss: -0.04925817251205444
Batch 57/64 loss: -0.07082575559616089
Batch 58/64 loss: -0.05354112386703491
Batch 59/64 loss: -0.0651705265045166
Batch 60/64 loss: -0.06856220960617065
Batch 61/64 loss: -0.07693535089492798
Batch 62/64 loss: -0.08692878484725952
Batch 63/64 loss: -0.0686301589012146
Batch 64/64 loss: -0.05981159210205078
Epoch 226  Train loss: -0.07417839181189444  Val loss: 0.059259563172396106
Epoch 227
-------------------------------
Batch 1/64 loss: -0.09065139293670654
Batch 2/64 loss: -0.05718493461608887
Batch 3/64 loss: -0.09319525957107544
Batch 4/64 loss: -0.09375548362731934
Batch 5/64 loss: -0.07446587085723877
Batch 6/64 loss: -0.08269739151000977
Batch 7/64 loss: -0.07414233684539795
Batch 8/64 loss: -0.09799498319625854
Batch 9/64 loss: -0.03027665615081787
Batch 10/64 loss: -0.10589843988418579
Batch 11/64 loss: -0.04819750785827637
Batch 12/64 loss: -0.07734715938568115
Batch 13/64 loss: -0.06284797191619873
Batch 14/64 loss: -0.08289194107055664
Batch 15/64 loss: -0.09558165073394775
Batch 16/64 loss: -0.08692556619644165
Batch 17/64 loss: -0.060292720794677734
Batch 18/64 loss: -0.08529424667358398
Batch 19/64 loss: -0.09833455085754395
Batch 20/64 loss: -0.07131582498550415
Batch 21/64 loss: -0.0542072057723999
Batch 22/64 loss: -0.07909023761749268
Batch 23/64 loss: -0.09507626295089722
Batch 24/64 loss: -0.06206488609313965
Batch 25/64 loss: -0.10263341665267944
Batch 26/64 loss: -0.046746134757995605
Batch 27/64 loss: -0.06739318370819092
Batch 28/64 loss: -0.0799490213394165
Batch 29/64 loss: -0.07254362106323242
Batch 30/64 loss: -0.09843367338180542
Batch 31/64 loss: -0.0906141996383667
Batch 32/64 loss: -0.09289520978927612
Batch 33/64 loss: -0.0511818528175354
Batch 34/64 loss: -0.10213226079940796
Batch 35/64 loss: -0.04652392864227295
Batch 36/64 loss: -0.089743971824646
Batch 37/64 loss: -0.11472928524017334
Batch 38/64 loss: -0.05660015344619751
Batch 39/64 loss: -0.06756830215454102
Batch 40/64 loss: -0.054487526416778564
Batch 41/64 loss: -0.07769668102264404
Batch 42/64 loss: -0.09777772426605225
Batch 43/64 loss: -0.0998077392578125
Batch 44/64 loss: -0.05208086967468262
Batch 45/64 loss: -0.08616876602172852
Batch 46/64 loss: -0.08739060163497925
Batch 47/64 loss: -0.05757474899291992
Batch 48/64 loss: -0.06524032354354858
Batch 49/64 loss: -0.05323821306228638
Batch 50/64 loss: -0.057544052600860596
Batch 51/64 loss: -0.05839967727661133
Batch 52/64 loss: -0.08144271373748779
Batch 53/64 loss: -0.096790611743927
Batch 54/64 loss: -0.06459707021713257
Batch 55/64 loss: -0.011393904685974121
Batch 56/64 loss: -0.08278626203536987
Batch 57/64 loss: -0.10026007890701294
Batch 58/64 loss: -0.07831716537475586
Batch 59/64 loss: -0.06093567609786987
Batch 60/64 loss: -0.04410356283187866
Batch 61/64 loss: -0.07518887519836426
Batch 62/64 loss: -0.08187609910964966
Batch 63/64 loss: -0.05299675464630127
Batch 64/64 loss: -0.0919376015663147
Epoch 227  Train loss: -0.07508184465707517  Val loss: 0.05346268517864529
Epoch 228
-------------------------------
Batch 1/64 loss: -0.012328028678894043
Batch 2/64 loss: -0.0849953293800354
Batch 3/64 loss: -0.08986365795135498
Batch 4/64 loss: -0.03729313611984253
Batch 5/64 loss: -0.0829242467880249
Batch 6/64 loss: -0.09385567903518677
Batch 7/64 loss: -0.0292510986328125
Batch 8/64 loss: -0.04997992515563965
Batch 9/64 loss: -0.10610496997833252
Batch 10/64 loss: -0.04492926597595215
Batch 11/64 loss: -0.029175758361816406
Batch 12/64 loss: -0.07242274284362793
Batch 13/64 loss: -0.05462479591369629
Batch 14/64 loss: -0.07592642307281494
Batch 15/64 loss: -0.07477611303329468
Batch 16/64 loss: -0.07787883281707764
Batch 17/64 loss: -0.06277424097061157
Batch 18/64 loss: -0.06166863441467285
Batch 19/64 loss: -0.08912009000778198
Batch 20/64 loss: -0.10860669612884521
Batch 21/64 loss: -0.08089452981948853
Batch 22/64 loss: -0.05502474308013916
Batch 23/64 loss: -0.059781551361083984
Batch 24/64 loss: -0.09324133396148682
Batch 25/64 loss: -0.09952694177627563
Batch 26/64 loss: -0.09180712699890137
Batch 27/64 loss: -0.09342306852340698
Batch 28/64 loss: -0.08569425344467163
Batch 29/64 loss: -0.08133792877197266
Batch 30/64 loss: -0.06724286079406738
Batch 31/64 loss: -0.07585102319717407
Batch 32/64 loss: -0.09073078632354736
Batch 33/64 loss: -0.08649182319641113
Batch 34/64 loss: -0.06555813550949097
Batch 35/64 loss: -0.05607402324676514
Batch 36/64 loss: -0.06744277477264404
Batch 37/64 loss: -0.04687011241912842
Batch 38/64 loss: -0.07230597734451294
Batch 39/64 loss: -0.10099947452545166
Batch 40/64 loss: -0.07412666082382202
Batch 41/64 loss: -0.0925593376159668
Batch 42/64 loss: -0.09145230054855347
Batch 43/64 loss: -0.08070266246795654
Batch 44/64 loss: -0.10055786371231079
Batch 45/64 loss: -0.05231344699859619
Batch 46/64 loss: -0.08447510004043579
Batch 47/64 loss: -0.03997606039047241
Batch 48/64 loss: -0.07207542657852173
Batch 49/64 loss: -0.1005755066871643
Batch 50/64 loss: -0.08186507225036621
Batch 51/64 loss: -0.08259201049804688
Batch 52/64 loss: -0.10263192653656006
Batch 53/64 loss: -0.08580780029296875
Batch 54/64 loss: -0.09356886148452759
Batch 55/64 loss: -0.09196901321411133
Batch 56/64 loss: -0.07786369323730469
Batch 57/64 loss: -0.06951206922531128
Batch 58/64 loss: -0.09909135103225708
Batch 59/64 loss: -0.015332043170928955
Batch 60/64 loss: -0.10982292890548706
Batch 61/64 loss: -0.07355773448944092
Batch 62/64 loss: -0.10168725252151489
Batch 63/64 loss: -0.08017349243164062
Batch 64/64 loss: -0.06398236751556396
Epoch 228  Train loss: -0.07546783568812351  Val loss: 0.049793706521955146
Saving best model, epoch: 228
Epoch 229
-------------------------------
Batch 1/64 loss: -0.09475970268249512
Batch 2/64 loss: -0.13801556825637817
Batch 3/64 loss: -0.07836383581161499
Batch 4/64 loss: -0.09081685543060303
Batch 5/64 loss: -0.065152108669281
Batch 6/64 loss: -0.09617835283279419
Batch 7/64 loss: -0.04767942428588867
Batch 8/64 loss: -0.057377636432647705
Batch 9/64 loss: -0.09210824966430664
Batch 10/64 loss: -0.09323304891586304
Batch 11/64 loss: -0.07468324899673462
Batch 12/64 loss: -0.09184366464614868
Batch 13/64 loss: -0.08040851354598999
Batch 14/64 loss: -0.09945690631866455
Batch 15/64 loss: -0.10739767551422119
Batch 16/64 loss: -0.0635024905204773
Batch 17/64 loss: -0.0780075192451477
Batch 18/64 loss: -0.0706281065940857
Batch 19/64 loss: -0.05818939208984375
Batch 20/64 loss: -0.06607335805892944
Batch 21/64 loss: -0.06565415859222412
Batch 22/64 loss: -0.06170839071273804
Batch 23/64 loss: -0.02030080556869507
Batch 24/64 loss: -0.06961655616760254
Batch 25/64 loss: -0.05391240119934082
Batch 26/64 loss: -0.07160884141921997
Batch 27/64 loss: -0.06729274988174438
Batch 28/64 loss: -0.09123075008392334
Batch 29/64 loss: -0.06379294395446777
Batch 30/64 loss: -0.06606775522232056
Batch 31/64 loss: -0.08574342727661133
Batch 32/64 loss: -0.04675203561782837
Batch 33/64 loss: -0.08476299047470093
Batch 34/64 loss: -0.06663131713867188
Batch 35/64 loss: -0.06293737888336182
Batch 36/64 loss: -0.06864660978317261
Batch 37/64 loss: -0.09301257133483887
Batch 38/64 loss: -0.10423970222473145
Batch 39/64 loss: -0.08283424377441406
Batch 40/64 loss: -0.07211560010910034
Batch 41/64 loss: -0.08551973104476929
Batch 42/64 loss: -0.09516537189483643
Batch 43/64 loss: -0.09464395046234131
Batch 44/64 loss: -0.09774219989776611
Batch 45/64 loss: -0.06465816497802734
Batch 46/64 loss: -0.0773172378540039
Batch 47/64 loss: -0.09867596626281738
Batch 48/64 loss: -0.07173484563827515
Batch 49/64 loss: -0.04970228672027588
Batch 50/64 loss: -0.08316004276275635
Batch 51/64 loss: -0.08495098352432251
Batch 52/64 loss: -0.07038110494613647
Batch 53/64 loss: -0.10519039630889893
Batch 54/64 loss: -0.036446213722229004
Batch 55/64 loss: -0.07898521423339844
Batch 56/64 loss: -0.08815211057662964
Batch 57/64 loss: -0.05747634172439575
Batch 58/64 loss: -0.08616536855697632
Batch 59/64 loss: -0.09739416837692261
Batch 60/64 loss: -0.0673379898071289
Batch 61/64 loss: -0.07789325714111328
Batch 62/64 loss: -0.07978010177612305
Batch 63/64 loss: -0.032359957695007324
Batch 64/64 loss: -0.07016730308532715
Epoch 229  Train loss: -0.07648933915530934  Val loss: 0.054653855328707354
Epoch 230
-------------------------------
Batch 1/64 loss: -0.09186375141143799
Batch 2/64 loss: -0.09787660837173462
Batch 3/64 loss: -0.06490719318389893
Batch 4/64 loss: -0.10172587633132935
Batch 5/64 loss: -0.0878455638885498
Batch 6/64 loss: -0.10115337371826172
Batch 7/64 loss: -0.002176821231842041
Batch 8/64 loss: -0.05196589231491089
Batch 9/64 loss: -0.07592779397964478
Batch 10/64 loss: -0.06598901748657227
Batch 11/64 loss: -0.097370445728302
Batch 12/64 loss: -0.08453118801116943
Batch 13/64 loss: -0.08337771892547607
Batch 14/64 loss: -0.07601243257522583
Batch 15/64 loss: -0.10504257678985596
Batch 16/64 loss: -0.050893306732177734
Batch 17/64 loss: -0.11246746778488159
Batch 18/64 loss: -0.03330838680267334
Batch 19/64 loss: -0.04053783416748047
Batch 20/64 loss: -0.05502581596374512
Batch 21/64 loss: -0.03315162658691406
Batch 22/64 loss: -0.06873553991317749
Batch 23/64 loss: -0.09984344244003296
Batch 24/64 loss: -0.08061486482620239
Batch 25/64 loss: -0.06264066696166992
Batch 26/64 loss: -0.10017043352127075
Batch 27/64 loss: -0.05201154947280884
Batch 28/64 loss: -0.11002129316329956
Batch 29/64 loss: -0.08070230484008789
Batch 30/64 loss: -0.08462107181549072
Batch 31/64 loss: -0.09757643938064575
Batch 32/64 loss: -0.07703042030334473
Batch 33/64 loss: -0.08644258975982666
Batch 34/64 loss: -0.07824528217315674
Batch 35/64 loss: -0.08580350875854492
Batch 36/64 loss: -0.10096460580825806
Batch 37/64 loss: -0.09048497676849365
Batch 38/64 loss: -0.09701943397521973
Batch 39/64 loss: -0.07667362689971924
Batch 40/64 loss: -0.0960882306098938
Batch 41/64 loss: -0.08974069356918335
Batch 42/64 loss: -0.08109015226364136
Batch 43/64 loss: -0.056658267974853516
Batch 44/64 loss: -0.0727013349533081
Batch 45/64 loss: -0.06611913442611694
Batch 46/64 loss: -0.1040453314781189
Batch 47/64 loss: -0.06006824970245361
Batch 48/64 loss: -0.029377758502960205
Batch 49/64 loss: -0.07620370388031006
Batch 50/64 loss: -0.08486723899841309
Batch 51/64 loss: -0.08306044340133667
Batch 52/64 loss: -0.07680612802505493
Batch 53/64 loss: -0.05481690168380737
Batch 54/64 loss: -0.05073124170303345
Batch 55/64 loss: -0.07849442958831787
Batch 56/64 loss: -0.07140505313873291
Batch 57/64 loss: -0.08404034376144409
Batch 58/64 loss: -0.07469052076339722
Batch 59/64 loss: -0.07791435718536377
Batch 60/64 loss: -0.0550808310508728
Batch 61/64 loss: -0.10673516988754272
Batch 62/64 loss: -0.06641983985900879
Batch 63/64 loss: -0.05899888277053833
Batch 64/64 loss: -0.04848664999008179
Epoch 230  Train loss: -0.07584740344215841  Val loss: 0.06867883889535858
Epoch 231
-------------------------------
Batch 1/64 loss: -0.07728278636932373
Batch 2/64 loss: -0.10383737087249756
Batch 3/64 loss: -0.11526679992675781
Batch 4/64 loss: -0.11282026767730713
Batch 5/64 loss: -0.09315413236618042
Batch 6/64 loss: -0.13129621744155884
Batch 7/64 loss: -0.0536653995513916
Batch 8/64 loss: -0.12290549278259277
Batch 9/64 loss: -0.07400727272033691
Batch 10/64 loss: -0.09142911434173584
Batch 11/64 loss: -0.07610887289047241
Batch 12/64 loss: -0.08847880363464355
Batch 13/64 loss: -0.0683245062828064
Batch 14/64 loss: -0.059025585651397705
Batch 15/64 loss: -0.07829350233078003
Batch 16/64 loss: -0.055937767028808594
Batch 17/64 loss: -0.08214116096496582
Batch 18/64 loss: -0.12415659427642822
Batch 19/64 loss: -0.06955468654632568
Batch 20/64 loss: -0.06511181592941284
Batch 21/64 loss: -0.09983980655670166
Batch 22/64 loss: -0.07415932416915894
Batch 23/64 loss: -0.07211703062057495
Batch 24/64 loss: -0.09735637903213501
Batch 25/64 loss: -0.07489228248596191
Batch 26/64 loss: -0.05506843328475952
Batch 27/64 loss: -0.08965110778808594
Batch 28/64 loss: -0.05626392364501953
Batch 29/64 loss: -0.04232293367385864
Batch 30/64 loss: -0.05260610580444336
Batch 31/64 loss: -0.036504924297332764
Batch 32/64 loss: -0.07905089855194092
Batch 33/64 loss: -0.09329932928085327
Batch 34/64 loss: -0.04234796762466431
Batch 35/64 loss: -0.053192079067230225
Batch 36/64 loss: -0.06856644153594971
Batch 37/64 loss: -0.06551986932754517
Batch 38/64 loss: -0.08708399534225464
Batch 39/64 loss: -0.0761866569519043
Batch 40/64 loss: -0.10158038139343262
Batch 41/64 loss: -0.09743267297744751
Batch 42/64 loss: -0.05684006214141846
Batch 43/64 loss: -0.07966232299804688
Batch 44/64 loss: -0.07925355434417725
Batch 45/64 loss: -0.06155502796173096
Batch 46/64 loss: -0.029085278511047363
Batch 47/64 loss: -0.04537320137023926
Batch 48/64 loss: -0.1078534722328186
Batch 49/64 loss: -0.07534348964691162
Batch 50/64 loss: -0.08469009399414062
Batch 51/64 loss: -0.022402405738830566
Batch 52/64 loss: -0.08732527494430542
Batch 53/64 loss: -0.08471959829330444
Batch 54/64 loss: -0.09021764993667603
Batch 55/64 loss: -0.09231829643249512
Batch 56/64 loss: -0.07623064517974854
Batch 57/64 loss: -0.08262550830841064
Batch 58/64 loss: -0.07164549827575684
Batch 59/64 loss: -0.07054269313812256
Batch 60/64 loss: -0.07511258125305176
Batch 61/64 loss: -0.07085740566253662
Batch 62/64 loss: -0.08683943748474121
Batch 63/64 loss: -0.06663656234741211
Batch 64/64 loss: -0.0904836654663086
Epoch 231  Train loss: -0.0772209176830217  Val loss: 0.054283369038113205
Epoch 232
-------------------------------
Batch 1/64 loss: -0.09095656871795654
Batch 2/64 loss: -0.07508748769760132
Batch 3/64 loss: -0.08461642265319824
Batch 4/64 loss: -0.0924798846244812
Batch 5/64 loss: -0.09015893936157227
Batch 6/64 loss: -0.06645119190216064
Batch 7/64 loss: -0.08615422248840332
Batch 8/64 loss: -0.08139145374298096
Batch 9/64 loss: -0.07869654893875122
Batch 10/64 loss: -0.10549068450927734
Batch 11/64 loss: -0.04096406698226929
Batch 12/64 loss: -0.06013983488082886
Batch 13/64 loss: -0.05461680889129639
Batch 14/64 loss: -0.10461229085922241
Batch 15/64 loss: -0.09630298614501953
Batch 16/64 loss: -0.09664422273635864
Batch 17/64 loss: -0.10240042209625244
Batch 18/64 loss: -0.04717618227005005
Batch 19/64 loss: -0.07452857494354248
Batch 20/64 loss: -0.08830732107162476
Batch 21/64 loss: -0.08927172422409058
Batch 22/64 loss: -0.03651076555252075
Batch 23/64 loss: -0.07487964630126953
Batch 24/64 loss: -0.06373107433319092
Batch 25/64 loss: -0.07636141777038574
Batch 26/64 loss: -0.06747406721115112
Batch 27/64 loss: -0.061598241329193115
Batch 28/64 loss: -0.036056339740753174
Batch 29/64 loss: -0.061332106590270996
Batch 30/64 loss: -0.07598263025283813
Batch 31/64 loss: -0.09164214134216309
Batch 32/64 loss: -0.08039414882659912
Batch 33/64 loss: -0.08708328008651733
Batch 34/64 loss: -0.07431679964065552
Batch 35/64 loss: -0.10333395004272461
Batch 36/64 loss: -0.062394022941589355
Batch 37/64 loss: -0.08054006099700928
Batch 38/64 loss: -0.09197592735290527
Batch 39/64 loss: -0.0907907485961914
Batch 40/64 loss: -0.0687493085861206
Batch 41/64 loss: -0.04990255832672119
Batch 42/64 loss: -0.06000995635986328
Batch 43/64 loss: -0.09602391719818115
Batch 44/64 loss: -0.09975725412368774
Batch 45/64 loss: -0.07138979434967041
Batch 46/64 loss: -0.0751335620880127
Batch 47/64 loss: -0.09381121397018433
Batch 48/64 loss: -0.08987462520599365
Batch 49/64 loss: -0.03753596544265747
Batch 50/64 loss: -0.08261382579803467
Batch 51/64 loss: -0.060441017150878906
Batch 52/64 loss: -0.05980563163757324
Batch 53/64 loss: -0.0700722336769104
Batch 54/64 loss: -0.04799234867095947
Batch 55/64 loss: -0.0759129524230957
Batch 56/64 loss: -0.08023393154144287
Batch 57/64 loss: -0.09279793500900269
Batch 58/64 loss: -0.06487441062927246
Batch 59/64 loss: -0.0769646167755127
Batch 60/64 loss: -0.11925292015075684
Batch 61/64 loss: -0.08178490400314331
Batch 62/64 loss: -0.0960877537727356
Batch 63/64 loss: -0.08374983072280884
Batch 64/64 loss: -0.09587419033050537
Epoch 232  Train loss: -0.07732585598440732  Val loss: 0.053295759400960915
Epoch 233
-------------------------------
Batch 1/64 loss: -0.08342951536178589
Batch 2/64 loss: -0.07876640558242798
Batch 3/64 loss: -0.10113459825515747
Batch 4/64 loss: -0.08044135570526123
Batch 5/64 loss: -0.06946539878845215
Batch 6/64 loss: -0.08151078224182129
Batch 7/64 loss: -0.06769168376922607
Batch 8/64 loss: -0.10314100980758667
Batch 9/64 loss: -0.10270261764526367
Batch 10/64 loss: -0.04969304800033569
Batch 11/64 loss: -0.07482457160949707
Batch 12/64 loss: -0.05262523889541626
Batch 13/64 loss: -0.08413660526275635
Batch 14/64 loss: -0.07177948951721191
Batch 15/64 loss: -0.0746346116065979
Batch 16/64 loss: -0.08127069473266602
Batch 17/64 loss: -0.1066288948059082
Batch 18/64 loss: -0.07338500022888184
Batch 19/64 loss: -0.06728088855743408
Batch 20/64 loss: -0.0849829912185669
Batch 21/64 loss: -0.09999388456344604
Batch 22/64 loss: -0.08996087312698364
Batch 23/64 loss: -0.10153955221176147
Batch 24/64 loss: -0.09175074100494385
Batch 25/64 loss: -0.04393911361694336
Batch 26/64 loss: -0.07468116283416748
Batch 27/64 loss: -0.12097829580307007
Batch 28/64 loss: -0.05174762010574341
Batch 29/64 loss: -0.07237052917480469
Batch 30/64 loss: -0.0912008285522461
Batch 31/64 loss: -0.07862550020217896
Batch 32/64 loss: -0.0884053111076355
Batch 33/64 loss: -0.09435319900512695
Batch 34/64 loss: -0.07891190052032471
Batch 35/64 loss: -0.11330163478851318
Batch 36/64 loss: -0.07300037145614624
Batch 37/64 loss: -0.09982055425643921
Batch 38/64 loss: -0.0780479907989502
Batch 39/64 loss: -0.10186022520065308
Batch 40/64 loss: -0.07613867521286011
Batch 41/64 loss: -0.11884790658950806
Batch 42/64 loss: -0.08635187149047852
Batch 43/64 loss: -0.07360762357711792
Batch 44/64 loss: -0.04904395341873169
Batch 45/64 loss: -0.05762094259262085
Batch 46/64 loss: -0.060743510723114014
Batch 47/64 loss: -0.09686076641082764
Batch 48/64 loss: -0.08163970708847046
Batch 49/64 loss: -0.06364554166793823
Batch 50/64 loss: -0.07570701837539673
Batch 51/64 loss: -0.06960773468017578
Batch 52/64 loss: -0.06271743774414062
Batch 53/64 loss: -0.1091698408126831
Batch 54/64 loss: -0.0842772126197815
Batch 55/64 loss: -0.06564873456954956
Batch 56/64 loss: -0.08747458457946777
Batch 57/64 loss: -0.07556009292602539
Batch 58/64 loss: -0.045806705951690674
Batch 59/64 loss: -0.08731663227081299
Batch 60/64 loss: -0.0668032169342041
Batch 61/64 loss: -0.06786125898361206
Batch 62/64 loss: -0.08250850439071655
Batch 63/64 loss: -0.08202540874481201
Batch 64/64 loss: -0.08966249227523804
Epoch 233  Train loss: -0.08044308143503526  Val loss: 0.0646920597430357
Epoch 234
-------------------------------
Batch 1/64 loss: -0.11973881721496582
Batch 2/64 loss: -0.0924304723739624
Batch 3/64 loss: -0.1022573709487915
Batch 4/64 loss: -0.0632430911064148
Batch 5/64 loss: -0.06061089038848877
Batch 6/64 loss: -0.06252831220626831
Batch 7/64 loss: -0.07516735792160034
Batch 8/64 loss: -0.07251501083374023
Batch 9/64 loss: -0.06694638729095459
Batch 10/64 loss: -0.09755241870880127
Batch 11/64 loss: -0.06550490856170654
Batch 12/64 loss: -0.06374669075012207
Batch 13/64 loss: -0.09690481424331665
Batch 14/64 loss: -0.05712002515792847
Batch 15/64 loss: -0.10105395317077637
Batch 16/64 loss: -0.0715898871421814
Batch 17/64 loss: -0.031425416469573975
Batch 18/64 loss: -0.07863831520080566
Batch 19/64 loss: -0.08052676916122437
Batch 20/64 loss: -0.09466546773910522
Batch 21/64 loss: -0.07044947147369385
Batch 22/64 loss: -0.08818972110748291
Batch 23/64 loss: -0.042546093463897705
Batch 24/64 loss: -0.08987927436828613
Batch 25/64 loss: -0.045211851596832275
Batch 26/64 loss: -0.08881151676177979
Batch 27/64 loss: -0.049677908420562744
Batch 28/64 loss: -0.08606386184692383
Batch 29/64 loss: -0.10356521606445312
Batch 30/64 loss: -0.06755650043487549
Batch 31/64 loss: -0.1187126636505127
Batch 32/64 loss: -0.05009406805038452
Batch 33/64 loss: -0.07998651266098022
Batch 34/64 loss: -0.09073615074157715
Batch 35/64 loss: -0.04400038719177246
Batch 36/64 loss: -0.09453481435775757
Batch 37/64 loss: -0.10301715135574341
Batch 38/64 loss: -0.060843467712402344
Batch 39/64 loss: -0.059363722801208496
Batch 40/64 loss: -0.07851260900497437
Batch 41/64 loss: -0.03468722105026245
Batch 42/64 loss: -0.08433938026428223
Batch 43/64 loss: -0.07564449310302734
Batch 44/64 loss: -0.0687870979309082
Batch 45/64 loss: -0.0839005708694458
Batch 46/64 loss: -0.08649766445159912
Batch 47/64 loss: -0.09074008464813232
Batch 48/64 loss: -0.054647624492645264
Batch 49/64 loss: -0.06482547521591187
Batch 50/64 loss: -0.08379727602005005
Batch 51/64 loss: -0.07921791076660156
Batch 52/64 loss: -0.06211739778518677
Batch 53/64 loss: -0.09537947177886963
Batch 54/64 loss: -0.0909421443939209
Batch 55/64 loss: -0.08579021692276001
Batch 56/64 loss: -0.08986973762512207
Batch 57/64 loss: -0.07446479797363281
Batch 58/64 loss: -0.05239605903625488
Batch 59/64 loss: -0.10773390531539917
Batch 60/64 loss: -0.0934874415397644
Batch 61/64 loss: -0.06195110082626343
Batch 62/64 loss: -0.0738227367401123
Batch 63/64 loss: -0.0555727481842041
Batch 64/64 loss: -0.08247983455657959
Epoch 234  Train loss: -0.07652379250993915  Val loss: 0.05802040288538458
Epoch 235
-------------------------------
Batch 1/64 loss: -0.059575021266937256
Batch 2/64 loss: -0.09994018077850342
Batch 3/64 loss: -0.06569510698318481
Batch 4/64 loss: -0.07953381538391113
Batch 5/64 loss: -0.0500415563583374
Batch 6/64 loss: -0.0587383508682251
Batch 7/64 loss: -0.05901980400085449
Batch 8/64 loss: -0.05878889560699463
Batch 9/64 loss: -0.0837283730506897
Batch 10/64 loss: -0.07412666082382202
Batch 11/64 loss: -0.03620553016662598
Batch 12/64 loss: -0.048761963844299316
Batch 13/64 loss: -0.05672752857208252
Batch 14/64 loss: -0.06924140453338623
Batch 15/64 loss: -0.08813458681106567
Batch 16/64 loss: -0.09432768821716309
Batch 17/64 loss: -0.09243446588516235
Batch 18/64 loss: -0.11013108491897583
Batch 19/64 loss: -0.08956605195999146
Batch 20/64 loss: -0.08175599575042725
Batch 21/64 loss: -0.08935445547103882
Batch 22/64 loss: -0.11510622501373291
Batch 23/64 loss: -0.07674437761306763
Batch 24/64 loss: -0.0914490818977356
Batch 25/64 loss: -0.07165944576263428
Batch 26/64 loss: -0.06916069984436035
Batch 27/64 loss: -0.06273549795150757
Batch 28/64 loss: -0.08320385217666626
Batch 29/64 loss: -0.0820389986038208
Batch 30/64 loss: -0.09595775604248047
Batch 31/64 loss: -0.08316588401794434
Batch 32/64 loss: -0.06076478958129883
Batch 33/64 loss: -0.10148477554321289
Batch 34/64 loss: -0.10408031940460205
Batch 35/64 loss: -0.05611485242843628
Batch 36/64 loss: -0.07107669115066528
Batch 37/64 loss: -0.10107392072677612
Batch 38/64 loss: -0.09005391597747803
Batch 39/64 loss: -0.07339608669281006
Batch 40/64 loss: -0.07875257730484009
Batch 41/64 loss: -0.0960574746131897
Batch 42/64 loss: -0.08789676427841187
Batch 43/64 loss: -0.07961070537567139
Batch 44/64 loss: -0.11148697137832642
Batch 45/64 loss: -0.06078290939331055
Batch 46/64 loss: -0.08496963977813721
Batch 47/64 loss: -0.04638016223907471
Batch 48/64 loss: -0.06784754991531372
Batch 49/64 loss: -0.08980488777160645
Batch 50/64 loss: -0.08736687898635864
Batch 51/64 loss: -0.037894487380981445
Batch 52/64 loss: -0.06980621814727783
Batch 53/64 loss: -0.08853459358215332
Batch 54/64 loss: -0.052947044372558594
Batch 55/64 loss: -0.084869384765625
Batch 56/64 loss: -0.1050879955291748
Batch 57/64 loss: -0.08522677421569824
Batch 58/64 loss: -0.10283589363098145
Batch 59/64 loss: -0.07813698053359985
Batch 60/64 loss: -0.10105746984481812
Batch 61/64 loss: -0.09658443927764893
Batch 62/64 loss: -0.04981982707977295
Batch 63/64 loss: -0.108498215675354
Batch 64/64 loss: -0.07828342914581299
Epoch 235  Train loss: -0.0791539468017279  Val loss: 0.068671006312485
Epoch 236
-------------------------------
Batch 1/64 loss: -0.11888861656188965
Batch 2/64 loss: -0.07286125421524048
Batch 3/64 loss: -0.07802736759185791
Batch 4/64 loss: -0.09471994638442993
Batch 5/64 loss: -0.08670508861541748
Batch 6/64 loss: -0.09996294975280762
Batch 7/64 loss: -0.09978199005126953
Batch 8/64 loss: -0.05210542678833008
Batch 9/64 loss: -0.0920172929763794
Batch 10/64 loss: -0.08782815933227539
Batch 11/64 loss: -0.06255519390106201
Batch 12/64 loss: -0.09102261066436768
Batch 13/64 loss: -0.10956889390945435
Batch 14/64 loss: -0.054308176040649414
Batch 15/64 loss: -0.11222076416015625
Batch 16/64 loss: -0.09490227699279785
Batch 17/64 loss: -0.05430549383163452
Batch 18/64 loss: -0.1141740083694458
Batch 19/64 loss: -0.10727685689926147
Batch 20/64 loss: -0.10160505771636963
Batch 21/64 loss: -0.07101750373840332
Batch 22/64 loss: -0.09805667400360107
Batch 23/64 loss: -0.05465197563171387
Batch 24/64 loss: -0.08333969116210938
Batch 25/64 loss: -0.030771195888519287
Batch 26/64 loss: -0.09098553657531738
Batch 27/64 loss: -0.07107073068618774
Batch 28/64 loss: -0.08691757917404175
Batch 29/64 loss: -0.09243518114089966
Batch 30/64 loss: -0.06815022230148315
Batch 31/64 loss: -0.08303630352020264
Batch 32/64 loss: -0.11160576343536377
Batch 33/64 loss: -0.07668042182922363
Batch 34/64 loss: -0.0875016450881958
Batch 35/64 loss: -0.08111971616744995
Batch 36/64 loss: -0.0792551040649414
Batch 37/64 loss: -0.10325169563293457
Batch 38/64 loss: -0.05059051513671875
Batch 39/64 loss: -0.09321862459182739
Batch 40/64 loss: -0.06761586666107178
Batch 41/64 loss: -0.022816598415374756
Batch 42/64 loss: -0.06299543380737305
Batch 43/64 loss: -0.05929595232009888
Batch 44/64 loss: -0.07288038730621338
Batch 45/64 loss: -0.07628858089447021
Batch 46/64 loss: -0.06927061080932617
Batch 47/64 loss: -0.08454287052154541
Batch 48/64 loss: -0.08772361278533936
Batch 49/64 loss: -0.08394807577133179
Batch 50/64 loss: -0.09278571605682373
Batch 51/64 loss: -0.105915367603302
Batch 52/64 loss: -0.06260049343109131
Batch 53/64 loss: -0.08498251438140869
Batch 54/64 loss: -0.05754387378692627
Batch 55/64 loss: -0.041753172874450684
Batch 56/64 loss: -0.09663563966751099
Batch 57/64 loss: -0.09670597314834595
Batch 58/64 loss: -0.02414470911026001
Batch 59/64 loss: -0.03513646125793457
Batch 60/64 loss: -0.07575321197509766
Batch 61/64 loss: -0.03946638107299805
Batch 62/64 loss: -0.10563230514526367
Batch 63/64 loss: -0.07362288236618042
Batch 64/64 loss: -0.07738697528839111
Epoch 236  Train loss: -0.07900534003388648  Val loss: 0.05873663032177797
Epoch 237
-------------------------------
Batch 1/64 loss: -0.07922065258026123
Batch 2/64 loss: -0.10930728912353516
Batch 3/64 loss: -0.11062049865722656
Batch 4/64 loss: -0.09304273128509521
Batch 5/64 loss: -0.03563535213470459
Batch 6/64 loss: -0.10087728500366211
Batch 7/64 loss: -0.06760179996490479
Batch 8/64 loss: -0.08972644805908203
Batch 9/64 loss: -0.05994504690170288
Batch 10/64 loss: -0.09270703792572021
Batch 11/64 loss: -0.08299434185028076
Batch 12/64 loss: -0.07222020626068115
Batch 13/64 loss: -0.0713571310043335
Batch 14/64 loss: -0.05839437246322632
Batch 15/64 loss: -0.07595992088317871
Batch 16/64 loss: -0.09161156415939331
Batch 17/64 loss: -0.10263854265213013
Batch 18/64 loss: -0.06694823503494263
Batch 19/64 loss: -0.07678872346878052
Batch 20/64 loss: -0.10680866241455078
Batch 21/64 loss: -0.07565629482269287
Batch 22/64 loss: -0.08785641193389893
Batch 23/64 loss: -0.08884274959564209
Batch 24/64 loss: -0.10600024461746216
Batch 25/64 loss: -0.059393882751464844
Batch 26/64 loss: -0.06443798542022705
Batch 27/64 loss: -0.11366438865661621
Batch 28/64 loss: -0.07574582099914551
Batch 29/64 loss: -0.06297409534454346
Batch 30/64 loss: -0.08210098743438721
Batch 31/64 loss: -0.10655176639556885
Batch 32/64 loss: -0.058770179748535156
Batch 33/64 loss: -0.08542537689208984
Batch 34/64 loss: -0.09009993076324463
Batch 35/64 loss: -0.09956479072570801
Batch 36/64 loss: -0.09630757570266724
Batch 37/64 loss: -0.03473109006881714
Batch 38/64 loss: -0.07772451639175415
Batch 39/64 loss: -0.08783864974975586
Batch 40/64 loss: -0.059494972229003906
Batch 41/64 loss: -0.03978234529495239
Batch 42/64 loss: -0.054888904094696045
Batch 43/64 loss: -0.08772790431976318
Batch 44/64 loss: -0.08315610885620117
Batch 45/64 loss: -0.09288346767425537
Batch 46/64 loss: -0.0691102147102356
Batch 47/64 loss: -0.08388715982437134
Batch 48/64 loss: -0.03445124626159668
Batch 49/64 loss: -0.10933846235275269
Batch 50/64 loss: -0.08915174007415771
Batch 51/64 loss: -0.08359402418136597
Batch 52/64 loss: -0.05734509229660034
Batch 53/64 loss: -0.10527002811431885
Batch 54/64 loss: -0.046475231647491455
Batch 55/64 loss: -0.06277096271514893
Batch 56/64 loss: -0.08650881052017212
Batch 57/64 loss: -0.07113200426101685
Batch 58/64 loss: -0.11874282360076904
Batch 59/64 loss: -0.05107313394546509
Batch 60/64 loss: -0.047289490699768066
Batch 61/64 loss: -0.08432334661483765
Batch 62/64 loss: -0.0510898232460022
Batch 63/64 loss: -0.10450232028961182
Batch 64/64 loss: -0.07697588205337524
Epoch 237  Train loss: -0.07889904485029332  Val loss: 0.05631887093442412
Epoch 238
-------------------------------
Batch 1/64 loss: -0.0697171688079834
Batch 2/64 loss: -0.09917408227920532
Batch 3/64 loss: -0.12223148345947266
Batch 4/64 loss: -0.1005849838256836
Batch 5/64 loss: -0.08948332071304321
Batch 6/64 loss: -0.07012218236923218
Batch 7/64 loss: -0.10165369510650635
Batch 8/64 loss: -0.09267354011535645
Batch 9/64 loss: -0.050280988216400146
Batch 10/64 loss: -0.09942877292633057
Batch 11/64 loss: -0.0984601378440857
Batch 12/64 loss: -0.09526163339614868
Batch 13/64 loss: -0.0762147307395935
Batch 14/64 loss: -0.09320032596588135
Batch 15/64 loss: -0.09900474548339844
Batch 16/64 loss: -0.07463634014129639
Batch 17/64 loss: -0.06437408924102783
Batch 18/64 loss: -0.08363962173461914
Batch 19/64 loss: -0.05525106191635132
Batch 20/64 loss: -0.09993141889572144
Batch 21/64 loss: -0.10068774223327637
Batch 22/64 loss: -0.07156455516815186
Batch 23/64 loss: -0.10914218425750732
Batch 24/64 loss: -0.09035229682922363
Batch 25/64 loss: -0.07185393571853638
Batch 26/64 loss: -0.07510596513748169
Batch 27/64 loss: -0.0769612193107605
Batch 28/64 loss: -0.12580513954162598
Batch 29/64 loss: -0.07363969087600708
Batch 30/64 loss: -0.09802472591400146
Batch 31/64 loss: -0.057166099548339844
Batch 32/64 loss: -0.09304690361022949
Batch 33/64 loss: -0.10802197456359863
Batch 34/64 loss: -0.10823619365692139
Batch 35/64 loss: -0.0591813325881958
Batch 36/64 loss: -0.06138789653778076
Batch 37/64 loss: -0.09930825233459473
Batch 38/64 loss: -0.07056695222854614
Batch 39/64 loss: -0.09178018569946289
Batch 40/64 loss: -0.052121877670288086
Batch 41/64 loss: -0.09277766942977905
Batch 42/64 loss: -0.07916730642318726
Batch 43/64 loss: -0.05204671621322632
Batch 44/64 loss: -0.10362863540649414
Batch 45/64 loss: -0.06305944919586182
Batch 46/64 loss: -0.09387898445129395
Batch 47/64 loss: -0.05863964557647705
Batch 48/64 loss: -0.0703691840171814
Batch 49/64 loss: -0.07348877191543579
Batch 50/64 loss: -0.09403789043426514
Batch 51/64 loss: -0.08894610404968262
Batch 52/64 loss: -0.04590201377868652
Batch 53/64 loss: -0.06725329160690308
Batch 54/64 loss: -0.06295931339263916
Batch 55/64 loss: -0.06800413131713867
Batch 56/64 loss: -0.09475088119506836
Batch 57/64 loss: -0.07275575399398804
Batch 58/64 loss: -0.08129924535751343
Batch 59/64 loss: -0.09300673007965088
Batch 60/64 loss: -0.09382373094558716
Batch 61/64 loss: -0.0839812159538269
Batch 62/64 loss: -0.05398339033126831
Batch 63/64 loss: -0.0938538908958435
Batch 64/64 loss: -0.0699378252029419
Epoch 238  Train loss: -0.0825623021406286  Val loss: 0.05671191235997833
Epoch 239
-------------------------------
Batch 1/64 loss: -0.08934718370437622
Batch 2/64 loss: -0.09817236661911011
Batch 3/64 loss: -0.10029500722885132
Batch 4/64 loss: -0.06340688467025757
Batch 5/64 loss: -0.09957921504974365
Batch 6/64 loss: -0.09076565504074097
Batch 7/64 loss: -0.09740382432937622
Batch 8/64 loss: -0.09838604927062988
Batch 9/64 loss: -0.08031964302062988
Batch 10/64 loss: -0.09350162744522095
Batch 11/64 loss: -0.09772622585296631
Batch 12/64 loss: -0.05503445863723755
Batch 13/64 loss: -0.07710105180740356
Batch 14/64 loss: -0.10600793361663818
Batch 15/64 loss: -0.07808053493499756
Batch 16/64 loss: -0.05758601427078247
Batch 17/64 loss: -0.059104740619659424
Batch 18/64 loss: -0.08391392230987549
Batch 19/64 loss: -0.0927969217300415
Batch 20/64 loss: -0.08332580327987671
Batch 21/64 loss: -0.06714892387390137
Batch 22/64 loss: -0.07166874408721924
Batch 23/64 loss: -0.0891878604888916
Batch 24/64 loss: -0.09120196104049683
Batch 25/64 loss: -0.07967120409011841
Batch 26/64 loss: -0.07025718688964844
Batch 27/64 loss: -0.08457684516906738
Batch 28/64 loss: -0.10048902034759521
Batch 29/64 loss: -0.04540061950683594
Batch 30/64 loss: -0.09829044342041016
Batch 31/64 loss: -0.06548166275024414
Batch 32/64 loss: -0.08785438537597656
Batch 33/64 loss: -0.08670806884765625
Batch 34/64 loss: -0.11014223098754883
Batch 35/64 loss: -0.07142722606658936
Batch 36/64 loss: -0.09213685989379883
Batch 37/64 loss: -0.047451138496398926
Batch 38/64 loss: -0.08467936515808105
Batch 39/64 loss: -0.0836038589477539
Batch 40/64 loss: -0.06940865516662598
Batch 41/64 loss: -0.08496379852294922
Batch 42/64 loss: -0.1164013147354126
Batch 43/64 loss: -0.058073461055755615
Batch 44/64 loss: -0.04016375541687012
Batch 45/64 loss: -0.09179335832595825
Batch 46/64 loss: -0.06100046634674072
Batch 47/64 loss: -0.08771371841430664
Batch 48/64 loss: -0.07159113883972168
Batch 49/64 loss: -0.08203071355819702
Batch 50/64 loss: -0.07447081804275513
Batch 51/64 loss: -0.08383339643478394
Batch 52/64 loss: -0.03213024139404297
Batch 53/64 loss: -0.07190203666687012
Batch 54/64 loss: -0.06979000568389893
Batch 55/64 loss: -0.0662001371383667
Batch 56/64 loss: -0.08535414934158325
Batch 57/64 loss: -0.09136366844177246
Batch 58/64 loss: -0.06805157661437988
Batch 59/64 loss: -0.07673436403274536
Batch 60/64 loss: -0.08736705780029297
Batch 61/64 loss: -0.09733468294143677
Batch 62/64 loss: -0.07478934526443481
Batch 63/64 loss: -0.06385165452957153
Batch 64/64 loss: -0.07395952939987183
Epoch 239  Train loss: -0.07985907185311411  Val loss: 0.08444950957478527
Epoch 240
-------------------------------
Batch 1/64 loss: -0.1007421612739563
Batch 2/64 loss: -0.061150193214416504
Batch 3/64 loss: -0.11710375547409058
Batch 4/64 loss: -0.05304419994354248
Batch 5/64 loss: -0.06804347038269043
Batch 6/64 loss: -0.10657018423080444
Batch 7/64 loss: -0.09761399030685425
Batch 8/64 loss: -0.10714030265808105
Batch 9/64 loss: -0.07289421558380127
Batch 10/64 loss: -0.08578914403915405
Batch 11/64 loss: -0.08552497625350952
Batch 12/64 loss: -0.10161072015762329
Batch 13/64 loss: -0.08281111717224121
Batch 14/64 loss: -0.09073007106781006
Batch 15/64 loss: -0.09013336896896362
Batch 16/64 loss: -0.08468133211135864
Batch 17/64 loss: -0.07395708560943604
Batch 18/64 loss: -0.08228647708892822
Batch 19/64 loss: -0.0811237096786499
Batch 20/64 loss: -0.1115524172782898
Batch 21/64 loss: -0.022365450859069824
Batch 22/64 loss: -0.08013653755187988
Batch 23/64 loss: -0.09871232509613037
Batch 24/64 loss: -0.022316932678222656
Batch 25/64 loss: -0.1108245849609375
Batch 26/64 loss: -0.0707816481590271
Batch 27/64 loss: -0.09887480735778809
Batch 28/64 loss: -0.09651488065719604
Batch 29/64 loss: -0.05294990539550781
Batch 30/64 loss: -0.10065370798110962
Batch 31/64 loss: -0.09812885522842407
Batch 32/64 loss: -0.0798196792602539
Batch 33/64 loss: -0.07638084888458252
Batch 34/64 loss: -0.07509076595306396
Batch 35/64 loss: -0.05852639675140381
Batch 36/64 loss: -0.09017699956893921
Batch 37/64 loss: -0.10305023193359375
Batch 38/64 loss: -0.08785474300384521
Batch 39/64 loss: -0.09143579006195068
Batch 40/64 loss: -0.09028446674346924
Batch 41/64 loss: -0.053247690200805664
Batch 42/64 loss: -0.1100504994392395
Batch 43/64 loss: -0.10120564699172974
Batch 44/64 loss: -0.10056126117706299
Batch 45/64 loss: -0.07768255472183228
Batch 46/64 loss: -0.08266341686248779
Batch 47/64 loss: -0.0907827615737915
Batch 48/64 loss: -0.09065103530883789
Batch 49/64 loss: -0.0576443076133728
Batch 50/64 loss: -0.08792388439178467
Batch 51/64 loss: -0.0532841682434082
Batch 52/64 loss: -0.11044824123382568
Batch 53/64 loss: -0.042875826358795166
Batch 54/64 loss: -0.062424421310424805
Batch 55/64 loss: -0.11361932754516602
Batch 56/64 loss: -0.0767064094543457
Batch 57/64 loss: -0.07040524482727051
Batch 58/64 loss: -0.09799659252166748
Batch 59/64 loss: -0.07733970880508423
Batch 60/64 loss: -0.06308233737945557
Batch 61/64 loss: -0.09550559520721436
Batch 62/64 loss: -0.10447055101394653
Batch 63/64 loss: -0.08210021257400513
Batch 64/64 loss: -0.08954721689224243
Epoch 240  Train loss: -0.08359552247851502  Val loss: 0.062036561802080815
Epoch 241
-------------------------------
Batch 1/64 loss: -0.0813477635383606
Batch 2/64 loss: -0.07753771543502808
Batch 3/64 loss: -0.12290513515472412
Batch 4/64 loss: -0.09042215347290039
Batch 5/64 loss: -0.08377909660339355
Batch 6/64 loss: -0.05246007442474365
Batch 7/64 loss: -0.05736660957336426
Batch 8/64 loss: -0.056488871574401855
Batch 9/64 loss: -0.09688961505889893
Batch 10/64 loss: -0.05928611755371094
Batch 11/64 loss: -0.09027725458145142
Batch 12/64 loss: -0.1083717942237854
Batch 13/64 loss: -0.10771727561950684
Batch 14/64 loss: -0.0953066349029541
Batch 15/64 loss: -0.07989376783370972
Batch 16/64 loss: -0.08669328689575195
Batch 17/64 loss: -0.06034278869628906
Batch 18/64 loss: -0.08635002374649048
Batch 19/64 loss: -0.0688624382019043
Batch 20/64 loss: -0.062280356884002686
Batch 21/64 loss: -0.055688679218292236
Batch 22/64 loss: -0.06286370754241943
Batch 23/64 loss: -0.09350645542144775
Batch 24/64 loss: -0.0703166127204895
Batch 25/64 loss: -0.09142833948135376
Batch 26/64 loss: -0.08957582712173462
Batch 27/64 loss: -0.07586967945098877
Batch 28/64 loss: -0.09948772192001343
Batch 29/64 loss: -0.10512691736221313
Batch 30/64 loss: -0.07222980260848999
Batch 31/64 loss: -0.09362995624542236
Batch 32/64 loss: -0.06201678514480591
Batch 33/64 loss: -0.058819711208343506
Batch 34/64 loss: -0.07106059789657593
Batch 35/64 loss: -0.1095350980758667
Batch 36/64 loss: -0.0620078444480896
Batch 37/64 loss: -0.09640169143676758
Batch 38/64 loss: -0.09944695234298706
Batch 39/64 loss: -0.1078609824180603
Batch 40/64 loss: -0.06973683834075928
Batch 41/64 loss: -0.09213244915008545
Batch 42/64 loss: -0.06089341640472412
Batch 43/64 loss: -0.07416671514511108
Batch 44/64 loss: -0.06360024213790894
Batch 45/64 loss: -0.10943013429641724
Batch 46/64 loss: -0.038904547691345215
Batch 47/64 loss: -0.08956015110015869
Batch 48/64 loss: -0.09801268577575684
Batch 49/64 loss: -0.07885909080505371
Batch 50/64 loss: -0.11307990550994873
Batch 51/64 loss: -0.08695733547210693
Batch 52/64 loss: -0.1210629940032959
Batch 53/64 loss: -0.07347023487091064
Batch 54/64 loss: -0.07953792810440063
Batch 55/64 loss: -0.11528134346008301
Batch 56/64 loss: -0.051139235496520996
Batch 57/64 loss: -0.0907357931137085
Batch 58/64 loss: -0.09401804208755493
Batch 59/64 loss: -0.08941012620925903
Batch 60/64 loss: -0.09100925922393799
Batch 61/64 loss: -0.08415758609771729
Batch 62/64 loss: -0.07788443565368652
Batch 63/64 loss: -0.09814471006393433
Batch 64/64 loss: -0.0929376482963562
Epoch 241  Train loss: -0.08333083250943352  Val loss: 0.08902222286794603
Epoch 242
-------------------------------
Batch 1/64 loss: -0.09724748134613037
Batch 2/64 loss: -0.07426440715789795
Batch 3/64 loss: -0.07739889621734619
Batch 4/64 loss: -0.08031225204467773
Batch 5/64 loss: -0.06961840391159058
Batch 6/64 loss: -0.09908914566040039
Batch 7/64 loss: -0.10386216640472412
Batch 8/64 loss: -0.09444928169250488
Batch 9/64 loss: -0.07546263933181763
Batch 10/64 loss: -0.09117496013641357
Batch 11/64 loss: -0.0763390064239502
Batch 12/64 loss: -0.06650537252426147
Batch 13/64 loss: -0.10654592514038086
Batch 14/64 loss: -0.11084961891174316
Batch 15/64 loss: -0.07414442300796509
Batch 16/64 loss: -0.09230637550354004
Batch 17/64 loss: -0.0915936827659607
Batch 18/64 loss: -0.10500305891036987
Batch 19/64 loss: -0.11168462038040161
Batch 20/64 loss: -0.08934664726257324
Batch 21/64 loss: -0.08137929439544678
Batch 22/64 loss: -0.07620197534561157
Batch 23/64 loss: -0.06746268272399902
Batch 24/64 loss: -0.07005476951599121
Batch 25/64 loss: -0.06834542751312256
Batch 26/64 loss: -0.09207504987716675
Batch 27/64 loss: -0.07750415802001953
Batch 28/64 loss: -0.0753900408744812
Batch 29/64 loss: -0.07859545946121216
Batch 30/64 loss: -0.0714867115020752
Batch 31/64 loss: -0.03978842496871948
Batch 32/64 loss: -0.07437807321548462
Batch 33/64 loss: -0.1069301962852478
Batch 34/64 loss: -0.10053586959838867
Batch 35/64 loss: -0.09460711479187012
Batch 36/64 loss: -0.05421966314315796
Batch 37/64 loss: -0.04821646213531494
Batch 38/64 loss: -0.09391331672668457
Batch 39/64 loss: -0.08682286739349365
Batch 40/64 loss: -0.09145963191986084
Batch 41/64 loss: -0.08947890996932983
Batch 42/64 loss: -0.07263433933258057
Batch 43/64 loss: -0.08031868934631348
Batch 44/64 loss: -0.10868579149246216
Batch 45/64 loss: -0.08942663669586182
Batch 46/64 loss: -0.07355928421020508
Batch 47/64 loss: -0.0663297176361084
Batch 48/64 loss: -0.09221529960632324
Batch 49/64 loss: -0.07622420787811279
Batch 50/64 loss: -0.08456438779830933
Batch 51/64 loss: -0.08770275115966797
Batch 52/64 loss: -0.11048614978790283
Batch 53/64 loss: -0.11252826452255249
Batch 54/64 loss: -0.10868042707443237
Batch 55/64 loss: -0.07740974426269531
Batch 56/64 loss: -0.07999730110168457
Batch 57/64 loss: -0.10505062341690063
Batch 58/64 loss: -0.08313572406768799
Batch 59/64 loss: -0.08192873001098633
Batch 60/64 loss: -0.037675559520721436
Batch 61/64 loss: -0.08951890468597412
Batch 62/64 loss: -0.11508554220199585
Batch 63/64 loss: -0.06654512882232666
Batch 64/64 loss: -0.07918959856033325
Epoch 242  Train loss: -0.084472782237857  Val loss: 0.05953961385484414
Epoch 243
-------------------------------
Batch 1/64 loss: -0.11124849319458008
Batch 2/64 loss: -0.0795179009437561
Batch 3/64 loss: -0.09494966268539429
Batch 4/64 loss: -0.06639724969863892
Batch 5/64 loss: -0.030633211135864258
Batch 6/64 loss: -0.09004133939743042
Batch 7/64 loss: -0.11545521020889282
Batch 8/64 loss: -0.08145517110824585
Batch 9/64 loss: -0.09275859594345093
Batch 10/64 loss: -0.11119991540908813
Batch 11/64 loss: -0.11499261856079102
Batch 12/64 loss: -0.0649074912071228
Batch 13/64 loss: -0.10391759872436523
Batch 14/64 loss: -0.05546313524246216
Batch 15/64 loss: -0.11256694793701172
Batch 16/64 loss: -0.07401716709136963
Batch 17/64 loss: -0.11020904779434204
Batch 18/64 loss: -0.015077352523803711
Batch 19/64 loss: -0.11643445491790771
Batch 20/64 loss: -0.06049215793609619
Batch 21/64 loss: -0.10408389568328857
Batch 22/64 loss: -0.10593259334564209
Batch 23/64 loss: -0.08817678689956665
Batch 24/64 loss: -0.10823845863342285
Batch 25/64 loss: -0.07073670625686646
Batch 26/64 loss: -0.11309361457824707
Batch 27/64 loss: -0.07573723793029785
Batch 28/64 loss: -0.07249146699905396
Batch 29/64 loss: -0.07721638679504395
Batch 30/64 loss: -0.09552997350692749
Batch 31/64 loss: -0.08878117799758911
Batch 32/64 loss: -0.09703302383422852
Batch 33/64 loss: -0.08314734697341919
Batch 34/64 loss: -0.1013108491897583
Batch 35/64 loss: -0.13861191272735596
Batch 36/64 loss: -0.05400973558425903
Batch 37/64 loss: -0.09564143419265747
Batch 38/64 loss: -0.11531734466552734
Batch 39/64 loss: -0.07496035099029541
Batch 40/64 loss: -0.07186448574066162
Batch 41/64 loss: -0.0827336311340332
Batch 42/64 loss: -0.09388738870620728
Batch 43/64 loss: -0.06682401895523071
Batch 44/64 loss: -0.07983040809631348
Batch 45/64 loss: -0.12634742259979248
Batch 46/64 loss: -0.10887348651885986
Batch 47/64 loss: -0.06827747821807861
Batch 48/64 loss: -0.10662776231765747
Batch 49/64 loss: -0.0887761116027832
Batch 50/64 loss: -0.07557177543640137
Batch 51/64 loss: -0.08699226379394531
Batch 52/64 loss: -0.04210257530212402
Batch 53/64 loss: -0.07390528917312622
Batch 54/64 loss: -0.10687762498855591
Batch 55/64 loss: -0.10655128955841064
Batch 56/64 loss: -0.06757032871246338
Batch 57/64 loss: -0.08786427974700928
Batch 58/64 loss: -0.09377574920654297
Batch 59/64 loss: -0.0720597505569458
Batch 60/64 loss: -0.11127322912216187
Batch 61/64 loss: -0.05551952123641968
Batch 62/64 loss: -0.1004677414894104
Batch 63/64 loss: -0.0908542275428772
Batch 64/64 loss: -0.08821773529052734
Epoch 243  Train loss: -0.0877392338771446  Val loss: 0.05300686531460162
Epoch 244
-------------------------------
Batch 1/64 loss: -0.11054623126983643
Batch 2/64 loss: -0.09970307350158691
Batch 3/64 loss: -0.0862661600112915
Batch 4/64 loss: -0.08868122100830078
Batch 5/64 loss: -0.09767770767211914
Batch 6/64 loss: -0.10172015428543091
Batch 7/64 loss: -0.0468173623085022
Batch 8/64 loss: -0.0999823808670044
Batch 9/64 loss: -0.05633044242858887
Batch 10/64 loss: -0.1206367015838623
Batch 11/64 loss: -0.08228075504302979
Batch 12/64 loss: -0.09423971176147461
Batch 13/64 loss: -0.08659094572067261
Batch 14/64 loss: -0.11099350452423096
Batch 15/64 loss: -0.08941650390625
Batch 16/64 loss: -0.12375342845916748
Batch 17/64 loss: -0.09747415781021118
Batch 18/64 loss: -0.09876960515975952
Batch 19/64 loss: -0.07941055297851562
Batch 20/64 loss: -0.08444350957870483
Batch 21/64 loss: -0.0932692289352417
Batch 22/64 loss: -0.07373780012130737
Batch 23/64 loss: -0.06697922945022583
Batch 24/64 loss: -0.08142644166946411
Batch 25/64 loss: -0.0781446099281311
Batch 26/64 loss: -0.08274751901626587
Batch 27/64 loss: -0.10775148868560791
Batch 28/64 loss: -0.08606362342834473
Batch 29/64 loss: -0.0718240737915039
Batch 30/64 loss: -0.08696240186691284
Batch 31/64 loss: -0.07806360721588135
Batch 32/64 loss: -0.06996309757232666
Batch 33/64 loss: -0.06313955783843994
Batch 34/64 loss: -0.07414799928665161
Batch 35/64 loss: -0.0915982723236084
Batch 36/64 loss: -0.07763046026229858
Batch 37/64 loss: -0.05887413024902344
Batch 38/64 loss: -0.11776983737945557
Batch 39/64 loss: -0.10025167465209961
Batch 40/64 loss: -0.059547364711761475
Batch 41/64 loss: -0.07627975940704346
Batch 42/64 loss: -0.1312652826309204
Batch 43/64 loss: -0.11372572183609009
Batch 44/64 loss: -0.06362593173980713
Batch 45/64 loss: -0.09334927797317505
Batch 46/64 loss: -0.07068735361099243
Batch 47/64 loss: -0.11879700422286987
Batch 48/64 loss: -0.05488133430480957
Batch 49/64 loss: -0.08178812265396118
Batch 50/64 loss: -0.09353113174438477
Batch 51/64 loss: -0.11075860261917114
Batch 52/64 loss: -0.08128863573074341
Batch 53/64 loss: -0.10076415538787842
Batch 54/64 loss: -0.11619651317596436
Batch 55/64 loss: -0.0685083270072937
Batch 56/64 loss: -0.0783088207244873
Batch 57/64 loss: -0.05919754505157471
Batch 58/64 loss: -0.08267557621002197
Batch 59/64 loss: -0.07207059860229492
Batch 60/64 loss: -0.07951420545578003
Batch 61/64 loss: -0.08783161640167236
Batch 62/64 loss: -0.05752366781234741
Batch 63/64 loss: -0.05668282508850098
Batch 64/64 loss: -0.08826017379760742
Epoch 244  Train loss: -0.08613448890985227  Val loss: 0.05298818652982155
Epoch 245
-------------------------------
Batch 1/64 loss: -0.100466787815094
Batch 2/64 loss: -0.0738821029663086
Batch 3/64 loss: -0.09459984302520752
Batch 4/64 loss: -0.10573452711105347
Batch 5/64 loss: -0.10536885261535645
Batch 6/64 loss: -0.06620460748672485
Batch 7/64 loss: -0.08156847953796387
Batch 8/64 loss: -0.07365280389785767
Batch 9/64 loss: -0.10646653175354004
Batch 10/64 loss: -0.03300893306732178
Batch 11/64 loss: -0.10355007648468018
Batch 12/64 loss: -0.09087169170379639
Batch 13/64 loss: -0.08930045366287231
Batch 14/64 loss: -0.06662219762802124
Batch 15/64 loss: -0.08235305547714233
Batch 16/64 loss: -0.06870794296264648
Batch 17/64 loss: -0.11305642127990723
Batch 18/64 loss: -0.10694503784179688
Batch 19/64 loss: -0.10685265064239502
Batch 20/64 loss: -0.09964644908905029
Batch 21/64 loss: -0.10317385196685791
Batch 22/64 loss: -0.10459542274475098
Batch 23/64 loss: -0.10214126110076904
Batch 24/64 loss: -0.08706420660018921
Batch 25/64 loss: -0.11575925350189209
Batch 26/64 loss: -0.08979201316833496
Batch 27/64 loss: -0.10131001472473145
Batch 28/64 loss: -0.0679289698600769
Batch 29/64 loss: -0.09516209363937378
Batch 30/64 loss: -0.10112929344177246
Batch 31/64 loss: -0.09664469957351685
Batch 32/64 loss: -0.09219980239868164
Batch 33/64 loss: -0.06885737180709839
Batch 34/64 loss: -0.05324709415435791
Batch 35/64 loss: -0.0869409441947937
Batch 36/64 loss: -0.11233019828796387
Batch 37/64 loss: -0.06580847501754761
Batch 38/64 loss: -0.08467233180999756
Batch 39/64 loss: -0.1119534969329834
Batch 40/64 loss: -0.08045780658721924
Batch 41/64 loss: -0.0913093090057373
Batch 42/64 loss: -0.09493839740753174
Batch 43/64 loss: -0.09345197677612305
Batch 44/64 loss: -0.07900232076644897
Batch 45/64 loss: -0.06355583667755127
Batch 46/64 loss: -0.08721506595611572
Batch 47/64 loss: -0.0605696439743042
Batch 48/64 loss: -0.06570494174957275
Batch 49/64 loss: -0.09371829032897949
Batch 50/64 loss: -0.08989971876144409
Batch 51/64 loss: -0.09256213903427124
Batch 52/64 loss: -0.06311798095703125
Batch 53/64 loss: -0.09196722507476807
Batch 54/64 loss: -0.08598756790161133
Batch 55/64 loss: -0.12478744983673096
Batch 56/64 loss: -0.03455841541290283
Batch 57/64 loss: -0.06435561180114746
Batch 58/64 loss: -0.07354915142059326
Batch 59/64 loss: -0.09328830242156982
Batch 60/64 loss: -0.0741797685623169
Batch 61/64 loss: -0.08124208450317383
Batch 62/64 loss: -0.07980543375015259
Batch 63/64 loss: -0.08496248722076416
Batch 64/64 loss: -0.012898504734039307
Epoch 245  Train loss: -0.0857008791437336  Val loss: 0.057017047790317604
Epoch 246
-------------------------------
Batch 1/64 loss: -0.08615267276763916
Batch 2/64 loss: -0.10954546928405762
Batch 3/64 loss: -0.0911489725112915
Batch 4/64 loss: -0.11520242691040039
Batch 5/64 loss: -0.0846167802810669
Batch 6/64 loss: -0.064888596534729
Batch 7/64 loss: -0.10353612899780273
Batch 8/64 loss: -0.10854530334472656
Batch 9/64 loss: -0.10912859439849854
Batch 10/64 loss: -0.1103702187538147
Batch 11/64 loss: -0.07758015394210815
Batch 12/64 loss: -0.07471781969070435
Batch 13/64 loss: -0.10678994655609131
Batch 14/64 loss: -0.10712611675262451
Batch 15/64 loss: -0.0927274227142334
Batch 16/64 loss: -0.09442615509033203
Batch 17/64 loss: -0.06971287727355957
Batch 18/64 loss: -0.10637319087982178
Batch 19/64 loss: -0.05770695209503174
Batch 20/64 loss: -0.06746220588684082
Batch 21/64 loss: -0.12691009044647217
Batch 22/64 loss: -0.08003365993499756
Batch 23/64 loss: -0.085324227809906
Batch 24/64 loss: -0.11606645584106445
Batch 25/64 loss: -0.0865933895111084
Batch 26/64 loss: -0.08431100845336914
Batch 27/64 loss: -0.052361130714416504
Batch 28/64 loss: -0.07591533660888672
Batch 29/64 loss: -0.08634698390960693
Batch 30/64 loss: -0.08560556173324585
Batch 31/64 loss: -0.12249535322189331
Batch 32/64 loss: -0.07765179872512817
Batch 33/64 loss: -0.07117271423339844
Batch 34/64 loss: -0.10894960165023804
Batch 35/64 loss: -0.0760464072227478
Batch 36/64 loss: -0.0986027717590332
Batch 37/64 loss: -0.11195141077041626
Batch 38/64 loss: -0.019015789031982422
Batch 39/64 loss: -0.1173749566078186
Batch 40/64 loss: -0.09366112947463989
Batch 41/64 loss: -0.08812737464904785
Batch 42/64 loss: -0.09000283479690552
Batch 43/64 loss: -0.08647370338439941
Batch 44/64 loss: -0.11383914947509766
Batch 45/64 loss: -0.09893310070037842
Batch 46/64 loss: -0.09811794757843018
Batch 47/64 loss: -0.06722080707550049
Batch 48/64 loss: -0.09674495458602905
Batch 49/64 loss: -0.030779719352722168
Batch 50/64 loss: -0.08166372776031494
Batch 51/64 loss: -0.021839141845703125
Batch 52/64 loss: -0.08128887414932251
Batch 53/64 loss: -0.10930567979812622
Batch 54/64 loss: -0.057338595390319824
Batch 55/64 loss: -0.07724088430404663
Batch 56/64 loss: -0.09012734889984131
Batch 57/64 loss: -0.07348978519439697
Batch 58/64 loss: -0.07055222988128662
Batch 59/64 loss: -0.10294145345687866
Batch 60/64 loss: -0.07655459642410278
Batch 61/64 loss: -0.08902662992477417
Batch 62/64 loss: -0.0494767427444458
Batch 63/64 loss: -0.09845960140228271
Batch 64/64 loss: -0.06469178199768066
Epoch 246  Train loss: -0.08646606277017033  Val loss: 0.06291942149912778
Epoch 247
-------------------------------
Batch 1/64 loss: -0.1122470498085022
Batch 2/64 loss: -0.12398421764373779
Batch 3/64 loss: -0.03893840312957764
Batch 4/64 loss: -0.11369889974594116
Batch 5/64 loss: -0.10723471641540527
Batch 6/64 loss: -0.11667579412460327
Batch 7/64 loss: -0.058288633823394775
Batch 8/64 loss: -0.10122144222259521
Batch 9/64 loss: -0.08212012052536011
Batch 10/64 loss: -0.0863196849822998
Batch 11/64 loss: -0.05849939584732056
Batch 12/64 loss: -0.08190274238586426
Batch 13/64 loss: -0.08502119779586792
Batch 14/64 loss: -0.07370465993881226
Batch 15/64 loss: -0.10318571329116821
Batch 16/64 loss: -0.06377339363098145
Batch 17/64 loss: -0.08202046155929565
Batch 18/64 loss: -0.06674468517303467
Batch 19/64 loss: -0.09052562713623047
Batch 20/64 loss: -0.07279086112976074
Batch 21/64 loss: -0.08849841356277466
Batch 22/64 loss: -0.10101139545440674
Batch 23/64 loss: -0.10348230600357056
Batch 24/64 loss: -0.058727264404296875
Batch 25/64 loss: -0.11914926767349243
Batch 26/64 loss: -0.08662045001983643
Batch 27/64 loss: -0.12962239980697632
Batch 28/64 loss: -0.10658246278762817
Batch 29/64 loss: -0.07130229473114014
Batch 30/64 loss: -0.10560792684555054
Batch 31/64 loss: -0.08267402648925781
Batch 32/64 loss: -0.09604912996292114
Batch 33/64 loss: -0.05040276050567627
Batch 34/64 loss: -0.08777600526809692
Batch 35/64 loss: -0.10736942291259766
Batch 36/64 loss: -0.053930819034576416
Batch 37/64 loss: -0.06304067373275757
Batch 38/64 loss: -0.13080745935440063
Batch 39/64 loss: -0.08503681421279907
Batch 40/64 loss: -0.08887887001037598
Batch 41/64 loss: -0.0865965485572815
Batch 42/64 loss: -0.07797116041183472
Batch 43/64 loss: -0.07277965545654297
Batch 44/64 loss: -0.10112172365188599
Batch 45/64 loss: -0.05496251583099365
Batch 46/64 loss: -0.04600268602371216
Batch 47/64 loss: -0.10150688886642456
Batch 48/64 loss: -0.09792697429656982
Batch 49/64 loss: -0.09620904922485352
Batch 50/64 loss: -0.048922836780548096
Batch 51/64 loss: -0.08716142177581787
Batch 52/64 loss: -0.044685423374176025
Batch 53/64 loss: -0.07125085592269897
Batch 54/64 loss: -0.10676789283752441
Batch 55/64 loss: -0.08047592639923096
Batch 56/64 loss: -0.0905541181564331
Batch 57/64 loss: -0.0925864577293396
Batch 58/64 loss: -0.06002908945083618
Batch 59/64 loss: -0.06916612386703491
Batch 60/64 loss: -0.10011696815490723
Batch 61/64 loss: -0.1054222583770752
Batch 62/64 loss: -0.05861937999725342
Batch 63/64 loss: -0.09558141231536865
Batch 64/64 loss: -0.08821535110473633
Epoch 247  Train loss: -0.0854595567665848  Val loss: 0.0543589632945372
Epoch 248
-------------------------------
Batch 1/64 loss: -0.1014183759689331
Batch 2/64 loss: -0.11120814085006714
Batch 3/64 loss: -0.10092252492904663
Batch 4/64 loss: -0.06800752878189087
Batch 5/64 loss: -0.11963367462158203
Batch 6/64 loss: -0.0921970009803772
Batch 7/64 loss: -0.07904160022735596
Batch 8/64 loss: -0.05695915222167969
Batch 9/64 loss: -0.1022181510925293
Batch 10/64 loss: -0.11629199981689453
Batch 11/64 loss: -0.08501255512237549
Batch 12/64 loss: -0.11527782678604126
Batch 13/64 loss: -0.08916211128234863
Batch 14/64 loss: -0.0926174521446228
Batch 15/64 loss: -0.07690632343292236
Batch 16/64 loss: -0.07445532083511353
Batch 17/64 loss: -0.08471435308456421
Batch 18/64 loss: -0.07458353042602539
Batch 19/64 loss: -0.07074171304702759
Batch 20/64 loss: -0.06808185577392578
Batch 21/64 loss: -0.1097787618637085
Batch 22/64 loss: -0.09787356853485107
Batch 23/64 loss: -0.11782270669937134
Batch 24/64 loss: -0.09033459424972534
Batch 25/64 loss: -0.07969290018081665
Batch 26/64 loss: -0.07227146625518799
Batch 27/64 loss: -0.11483025550842285
Batch 28/64 loss: -0.05133897066116333
Batch 29/64 loss: -0.08675718307495117
Batch 30/64 loss: -0.08294677734375
Batch 31/64 loss: -0.1052047610282898
Batch 32/64 loss: -0.08019828796386719
Batch 33/64 loss: -0.08431267738342285
Batch 34/64 loss: -0.11101293563842773
Batch 35/64 loss: -0.10521596670150757
Batch 36/64 loss: -0.06986665725708008
Batch 37/64 loss: -0.05309188365936279
Batch 38/64 loss: -0.09997701644897461
Batch 39/64 loss: -0.0792575478553772
Batch 40/64 loss: -0.07656067609786987
Batch 41/64 loss: -0.08726537227630615
Batch 42/64 loss: -0.07227981090545654
Batch 43/64 loss: -0.07471346855163574
Batch 44/64 loss: -0.09874492883682251
Batch 45/64 loss: -0.11758923530578613
Batch 46/64 loss: -0.08099192380905151
Batch 47/64 loss: -0.11037284135818481
Batch 48/64 loss: -0.08650094270706177
Batch 49/64 loss: -0.09378319978713989
Batch 50/64 loss: -0.10477274656295776
Batch 51/64 loss: -0.08753740787506104
Batch 52/64 loss: -0.0979565978050232
Batch 53/64 loss: -0.09423971176147461
Batch 54/64 loss: -0.06909865140914917
Batch 55/64 loss: -0.1058957576751709
Batch 56/64 loss: -0.06622451543807983
Batch 57/64 loss: -0.09070509672164917
Batch 58/64 loss: -0.07851988077163696
Batch 59/64 loss: -0.049205899238586426
Batch 60/64 loss: -0.08749139308929443
Batch 61/64 loss: -0.08286023139953613
Batch 62/64 loss: -0.09135973453521729
Batch 63/64 loss: -0.09520095586776733
Batch 64/64 loss: -0.0769650936126709
Epoch 248  Train loss: -0.0882953868192785  Val loss: 0.05701100232265249
Epoch 249
-------------------------------
Batch 1/64 loss: -0.08693206310272217
Batch 2/64 loss: -0.11516457796096802
Batch 3/64 loss: -0.09648960828781128
Batch 4/64 loss: -0.1045156717300415
Batch 5/64 loss: -0.07725286483764648
Batch 6/64 loss: -0.12237197160720825
Batch 7/64 loss: -0.10344141721725464
Batch 8/64 loss: -0.08374249935150146
Batch 9/64 loss: -0.1170356273651123
Batch 10/64 loss: -0.08285379409790039
Batch 11/64 loss: -0.08046543598175049
Batch 12/64 loss: -0.07247030735015869
Batch 13/64 loss: -0.06673264503479004
Batch 14/64 loss: -0.10400676727294922
Batch 15/64 loss: -0.08695095777511597
Batch 16/64 loss: -0.05419260263442993
Batch 17/64 loss: -0.10022342205047607
Batch 18/64 loss: -0.06996005773544312
Batch 19/64 loss: -0.10812991857528687
Batch 20/64 loss: -0.08997273445129395
Batch 21/64 loss: -0.10925096273422241
Batch 22/64 loss: -0.08439326286315918
Batch 23/64 loss: -0.08044642210006714
Batch 24/64 loss: -0.10670280456542969
Batch 25/64 loss: -0.10463201999664307
Batch 26/64 loss: -0.0397486686706543
Batch 27/64 loss: -0.0839163064956665
Batch 28/64 loss: -0.08317095041275024
Batch 29/64 loss: -0.1281341314315796
Batch 30/64 loss: -0.07471752166748047
Batch 31/64 loss: -0.09327292442321777
Batch 32/64 loss: -0.06017988920211792
Batch 33/64 loss: -0.0884253978729248
Batch 34/64 loss: -0.08813422918319702
Batch 35/64 loss: -0.07976078987121582
Batch 36/64 loss: -0.07822287082672119
Batch 37/64 loss: -0.10448884963989258
Batch 38/64 loss: -0.10570037364959717
Batch 39/64 loss: -0.09291517734527588
Batch 40/64 loss: -0.11307328939437866
Batch 41/64 loss: -0.061295509338378906
Batch 42/64 loss: -0.03656589984893799
Batch 43/64 loss: -0.06713783740997314
Batch 44/64 loss: -0.10034066438674927
Batch 45/64 loss: -0.08356082439422607
Batch 46/64 loss: -0.03862106800079346
Batch 47/64 loss: -0.08577537536621094
Batch 48/64 loss: -0.0975792407989502
Batch 49/64 loss: -0.07392096519470215
Batch 50/64 loss: -0.11340242624282837
Batch 51/64 loss: -0.08553266525268555
Batch 52/64 loss: -0.08331286907196045
Batch 53/64 loss: -0.09741103649139404
Batch 54/64 loss: -0.1119346022605896
Batch 55/64 loss: -0.08584237098693848
Batch 56/64 loss: -0.08716762065887451
Batch 57/64 loss: -0.1034475564956665
Batch 58/64 loss: -0.10482132434844971
Batch 59/64 loss: -0.10082405805587769
Batch 60/64 loss: -0.08151769638061523
Batch 61/64 loss: -0.0798758864402771
Batch 62/64 loss: -0.10203284025192261
Batch 63/64 loss: -0.0978095531463623
Batch 64/64 loss: -0.031817734241485596
Epoch 249  Train loss: -0.08824760750228283  Val loss: 0.056888580322265625
Epoch 250
-------------------------------
Batch 1/64 loss: -0.08589237928390503
Batch 2/64 loss: -0.10171759128570557
Batch 3/64 loss: -0.08981883525848389
Batch 4/64 loss: -0.11226511001586914
Batch 5/64 loss: -0.11465567350387573
Batch 6/64 loss: -0.08186572790145874
Batch 7/64 loss: -0.12641167640686035
Batch 8/64 loss: -0.0813981294631958
Batch 9/64 loss: -0.10127341747283936
Batch 10/64 loss: -0.11613893508911133
Batch 11/64 loss: -0.06351232528686523
Batch 12/64 loss: -0.08101469278335571
Batch 13/64 loss: -0.03968691825866699
Batch 14/64 loss: -0.11923789978027344
Batch 15/64 loss: -0.09924036264419556
Batch 16/64 loss: -0.10303390026092529
Batch 17/64 loss: -0.11911851167678833
Batch 18/64 loss: -0.08402997255325317
Batch 19/64 loss: -0.09659814834594727
Batch 20/64 loss: -0.09511089324951172
Batch 21/64 loss: -0.0978270173072815
Batch 22/64 loss: -0.058843135833740234
Batch 23/64 loss: -0.09720152616500854
Batch 24/64 loss: -0.11413204669952393
Batch 25/64 loss: -0.07038450241088867
Batch 26/64 loss: -0.11997759342193604
Batch 27/64 loss: -0.09043616056442261
Batch 28/64 loss: -0.11464154720306396
Batch 29/64 loss: -0.07678854465484619
Batch 30/64 loss: -0.06009697914123535
Batch 31/64 loss: -0.058177947998046875
Batch 32/64 loss: -0.056664466857910156
Batch 33/64 loss: -0.11330515146255493
Batch 34/64 loss: -0.07054466009140015
Batch 35/64 loss: -0.1054719090461731
Batch 36/64 loss: -0.08339983224868774
Batch 37/64 loss: -0.09047025442123413
Batch 38/64 loss: -0.10757094621658325
Batch 39/64 loss: -0.09448468685150146
Batch 40/64 loss: -0.0829460620880127
Batch 41/64 loss: -0.08421647548675537
Batch 42/64 loss: -0.08111178874969482
Batch 43/64 loss: -0.09867477416992188
Batch 44/64 loss: -0.05792689323425293
Batch 45/64 loss: -0.07199954986572266
Batch 46/64 loss: -0.08731919527053833
Batch 47/64 loss: -0.07987433671951294
Batch 48/64 loss: -0.06951391696929932
Batch 49/64 loss: -0.06114184856414795
Batch 50/64 loss: -0.07247257232666016
Batch 51/64 loss: -0.09848666191101074
Batch 52/64 loss: -0.08357983827590942
Batch 53/64 loss: -0.09299373626708984
Batch 54/64 loss: -0.07466602325439453
Batch 55/64 loss: -0.0767863392829895
Batch 56/64 loss: -0.03684377670288086
Batch 57/64 loss: -0.07443857192993164
Batch 58/64 loss: -0.07493406534194946
Batch 59/64 loss: -0.09777438640594482
Batch 60/64 loss: -0.10096794366836548
Batch 61/64 loss: -0.07638269662857056
Batch 62/64 loss: -0.07465928792953491
Batch 63/64 loss: -0.07841044664382935
Batch 64/64 loss: -0.09226131439208984
Epoch 250  Train loss: -0.08705501462899003  Val loss: 0.05504735372320483
Epoch 251
-------------------------------
Batch 1/64 loss: -0.12901723384857178
Batch 2/64 loss: -0.09087789058685303
Batch 3/64 loss: -0.0874791145324707
Batch 4/64 loss: -0.10374701023101807
Batch 5/64 loss: -0.11271482706069946
Batch 6/64 loss: -0.07171738147735596
Batch 7/64 loss: -0.09597116708755493
Batch 8/64 loss: -0.0963173508644104
Batch 9/64 loss: -0.11193382740020752
Batch 10/64 loss: -0.07693243026733398
Batch 11/64 loss: -0.08312541246414185
Batch 12/64 loss: -0.07872319221496582
Batch 13/64 loss: -0.10863125324249268
Batch 14/64 loss: -0.10655510425567627
Batch 15/64 loss: -0.07268673181533813
Batch 16/64 loss: -0.07302576303482056
Batch 17/64 loss: -0.08358490467071533
Batch 18/64 loss: -0.10667383670806885
Batch 19/64 loss: -0.09053140878677368
Batch 20/64 loss: -0.08646714687347412
Batch 21/64 loss: -0.0661994218826294
Batch 22/64 loss: -0.06798547506332397
Batch 23/64 loss: -0.07857829332351685
Batch 24/64 loss: -0.07918804883956909
Batch 25/64 loss: -0.08984583616256714
Batch 26/64 loss: -0.059472620487213135
Batch 27/64 loss: -0.07760125398635864
Batch 28/64 loss: -0.10290366411209106
Batch 29/64 loss: -0.09357112646102905
Batch 30/64 loss: -0.07787680625915527
Batch 31/64 loss: -0.11968141794204712
Batch 32/64 loss: -0.13368266820907593
Batch 33/64 loss: -0.1334337592124939
Batch 34/64 loss: -0.06713354587554932
Batch 35/64 loss: -0.09142130613327026
Batch 36/64 loss: -0.10176891088485718
Batch 37/64 loss: -0.07010871171951294
Batch 38/64 loss: -0.04812365770339966
Batch 39/64 loss: -0.08047103881835938
Batch 40/64 loss: -0.09755384922027588
Batch 41/64 loss: -0.10951411724090576
Batch 42/64 loss: -0.1075482964515686
Batch 43/64 loss: -0.07256931066513062
Batch 44/64 loss: -0.0722193717956543
Batch 45/64 loss: -0.09275215864181519
Batch 46/64 loss: -0.09481430053710938
Batch 47/64 loss: -0.08315730094909668
Batch 48/64 loss: -0.06884181499481201
Batch 49/64 loss: -0.10887807607650757
Batch 50/64 loss: -0.057987868785858154
Batch 51/64 loss: -0.09579169750213623
Batch 52/64 loss: -0.09529882669448853
Batch 53/64 loss: -0.07072532176971436
Batch 54/64 loss: -0.10445743799209595
Batch 55/64 loss: -0.05405759811401367
Batch 56/64 loss: -0.08454883098602295
Batch 57/64 loss: -0.09025382995605469
Batch 58/64 loss: -0.09640669822692871
Batch 59/64 loss: -0.07871454954147339
Batch 60/64 loss: -0.06375110149383545
Batch 61/64 loss: -0.07992452383041382
Batch 62/64 loss: -0.12193465232849121
Batch 63/64 loss: -0.024760186672210693
Batch 64/64 loss: -0.09951257705688477
Epoch 251  Train loss: -0.08795069339228612  Val loss: 0.05768565398311287
Epoch 252
-------------------------------
Batch 1/64 loss: -0.11718833446502686
Batch 2/64 loss: -0.11255991458892822
Batch 3/64 loss: -0.0745888352394104
Batch 4/64 loss: -0.09511542320251465
Batch 5/64 loss: -0.09725874662399292
Batch 6/64 loss: -0.11012119054794312
Batch 7/64 loss: -0.05087709426879883
Batch 8/64 loss: -0.06447446346282959
Batch 9/64 loss: -0.10492396354675293
Batch 10/64 loss: -0.10614770650863647
Batch 11/64 loss: -0.08010059595108032
Batch 12/64 loss: -0.10880893468856812
Batch 13/64 loss: -0.08577722311019897
Batch 14/64 loss: -0.12188351154327393
Batch 15/64 loss: -0.10783177614212036
Batch 16/64 loss: -0.07002711296081543
Batch 17/64 loss: -0.09433865547180176
Batch 18/64 loss: -0.10373860597610474
Batch 19/64 loss: -0.07377249002456665
Batch 20/64 loss: -0.11989396810531616
Batch 21/64 loss: -0.0967530608177185
Batch 22/64 loss: -0.07393461465835571
Batch 23/64 loss: -0.06376451253890991
Batch 24/64 loss: -0.11304402351379395
Batch 25/64 loss: -0.06892180442810059
Batch 26/64 loss: -0.0940900444984436
Batch 27/64 loss: -0.08328211307525635
Batch 28/64 loss: -0.10344582796096802
Batch 29/64 loss: -0.09950250387191772
Batch 30/64 loss: -0.02111947536468506
Batch 31/64 loss: -0.11831605434417725
Batch 32/64 loss: -0.028278708457946777
Batch 33/64 loss: -0.07076632976531982
Batch 34/64 loss: -0.07857877016067505
Batch 35/64 loss: -0.09951311349868774
Batch 36/64 loss: -0.1146743893623352
Batch 37/64 loss: -0.09131717681884766
Batch 38/64 loss: -0.07996463775634766
Batch 39/64 loss: -0.09047150611877441
Batch 40/64 loss: -0.11087280511856079
Batch 41/64 loss: -0.055709779262542725
Batch 42/64 loss: -0.07755124568939209
Batch 43/64 loss: -0.060573577880859375
Batch 44/64 loss: -0.08353912830352783
Batch 45/64 loss: -0.06156742572784424
Batch 46/64 loss: -0.08746886253356934
Batch 47/64 loss: -0.08841484785079956
Batch 48/64 loss: -0.1231566071510315
Batch 49/64 loss: -0.0728156566619873
Batch 50/64 loss: -0.13100945949554443
Batch 51/64 loss: -0.09882885217666626
Batch 52/64 loss: -0.09847337007522583
Batch 53/64 loss: -0.08675515651702881
Batch 54/64 loss: -0.1057119369506836
Batch 55/64 loss: -0.08793318271636963
Batch 56/64 loss: -0.07244992256164551
Batch 57/64 loss: -0.07970231771469116
Batch 58/64 loss: -0.07743030786514282
Batch 59/64 loss: -0.08439981937408447
Batch 60/64 loss: -0.12753582000732422
Batch 61/64 loss: -0.120358407497406
Batch 62/64 loss: -0.0816083550453186
Batch 63/64 loss: -0.08196532726287842
Batch 64/64 loss: -0.08663403987884521
Epoch 252  Train loss: -0.08956823395747765  Val loss: 0.057747710611402374
Epoch 253
-------------------------------
Batch 1/64 loss: -0.10618114471435547
Batch 2/64 loss: -0.11102920770645142
Batch 3/64 loss: -0.08021146059036255
Batch 4/64 loss: -0.09426754713058472
Batch 5/64 loss: -0.12187832593917847
Batch 6/64 loss: -0.09847348928451538
Batch 7/64 loss: -0.11934584379196167
Batch 8/64 loss: -0.11291629076004028
Batch 9/64 loss: -0.10121595859527588
Batch 10/64 loss: -0.11646205186843872
Batch 11/64 loss: -0.12663942575454712
Batch 12/64 loss: -0.09057074785232544
Batch 13/64 loss: -0.11063629388809204
Batch 14/64 loss: -0.12511223554611206
Batch 15/64 loss: -0.10263192653656006
Batch 16/64 loss: -0.10181081295013428
Batch 17/64 loss: -0.13091790676116943
Batch 18/64 loss: -0.08837497234344482
Batch 19/64 loss: -0.11790907382965088
Batch 20/64 loss: -0.08167392015457153
Batch 21/64 loss: -0.07945472002029419
Batch 22/64 loss: -0.11279928684234619
Batch 23/64 loss: -0.11139935255050659
Batch 24/64 loss: -0.09003257751464844
Batch 25/64 loss: -0.09547942876815796
Batch 26/64 loss: -0.08212131261825562
Batch 27/64 loss: -0.06561100482940674
Batch 28/64 loss: -0.12052690982818604
Batch 29/64 loss: -0.08347874879837036
Batch 30/64 loss: -0.08076536655426025
Batch 31/64 loss: -0.0799025297164917
Batch 32/64 loss: -0.09886348247528076
Batch 33/64 loss: -0.0992739200592041
Batch 34/64 loss: -0.10198581218719482
Batch 35/64 loss: -0.10103738307952881
Batch 36/64 loss: -0.08175837993621826
Batch 37/64 loss: -0.09430015087127686
Batch 38/64 loss: -0.08438539505004883
Batch 39/64 loss: -0.1192392110824585
Batch 40/64 loss: -0.08096176385879517
Batch 41/64 loss: -0.0917348861694336
Batch 42/64 loss: -0.08159035444259644
Batch 43/64 loss: -0.0934341549873352
Batch 44/64 loss: -0.12022960186004639
Batch 45/64 loss: -0.04480034112930298
Batch 46/64 loss: -0.08512639999389648
Batch 47/64 loss: -0.08084475994110107
Batch 48/64 loss: -0.08942627906799316
Batch 49/64 loss: -0.01745814085006714
Batch 50/64 loss: -0.10304015874862671
Batch 51/64 loss: -0.08896297216415405
Batch 52/64 loss: -0.1037294864654541
Batch 53/64 loss: -0.08187401294708252
Batch 54/64 loss: -0.11232388019561768
Batch 55/64 loss: -0.06676852703094482
Batch 56/64 loss: -0.09613978862762451
Batch 57/64 loss: -0.0967555046081543
Batch 58/64 loss: -0.06983834505081177
Batch 59/64 loss: -0.0807986855506897
Batch 60/64 loss: -0.06897711753845215
Batch 61/64 loss: -0.07389026880264282
Batch 62/64 loss: -0.06513381004333496
Batch 63/64 loss: -0.06491243839263916
Batch 64/64 loss: -0.05516529083251953
Epoch 253  Train loss: -0.09287528243719363  Val loss: 0.058920535025318056
Epoch 254
-------------------------------
Batch 1/64 loss: -0.07003635168075562
Batch 2/64 loss: -0.07647550106048584
Batch 3/64 loss: -0.09680300951004028
Batch 4/64 loss: -0.08455276489257812
Batch 5/64 loss: -0.08350127935409546
Batch 6/64 loss: -0.1002497673034668
Batch 7/64 loss: -0.05420732498168945
Batch 8/64 loss: -0.07282406091690063
Batch 9/64 loss: -0.08998715877532959
Batch 10/64 loss: -0.07674390077590942
Batch 11/64 loss: -0.0997348427772522
Batch 12/64 loss: -0.0686265230178833
Batch 13/64 loss: -0.12567102909088135
Batch 14/64 loss: -0.12410551309585571
Batch 15/64 loss: -0.12144464254379272
Batch 16/64 loss: -0.08708661794662476
Batch 17/64 loss: -0.06828159093856812
Batch 18/64 loss: -0.1035621166229248
Batch 19/64 loss: -0.11765915155410767
Batch 20/64 loss: -0.12372022867202759
Batch 21/64 loss: -0.07262825965881348
Batch 22/64 loss: -0.08076512813568115
Batch 23/64 loss: -0.05963331460952759
Batch 24/64 loss: -0.09175711870193481
Batch 25/64 loss: -0.07007235288619995
Batch 26/64 loss: -0.10640716552734375
Batch 27/64 loss: -0.10699349641799927
Batch 28/64 loss: -0.09028667211532593
Batch 29/64 loss: -0.09130245447158813
Batch 30/64 loss: -0.08861041069030762
Batch 31/64 loss: -0.09602737426757812
Batch 32/64 loss: -0.09493184089660645
Batch 33/64 loss: -0.08011329174041748
Batch 34/64 loss: -0.10803169012069702
Batch 35/64 loss: -0.08333277702331543
Batch 36/64 loss: -0.10294926166534424
Batch 37/64 loss: -0.10547524690628052
Batch 38/64 loss: -0.0977984070777893
Batch 39/64 loss: -0.09895241260528564
Batch 40/64 loss: -0.0871349573135376
Batch 41/64 loss: -0.09656304121017456
Batch 42/64 loss: -0.11217588186264038
Batch 43/64 loss: -0.0812496542930603
Batch 44/64 loss: -0.11523592472076416
Batch 45/64 loss: -0.08438920974731445
Batch 46/64 loss: -0.11187034845352173
Batch 47/64 loss: -0.11167865991592407
Batch 48/64 loss: -0.10175573825836182
Batch 49/64 loss: -0.09711337089538574
Batch 50/64 loss: -0.08241903781890869
Batch 51/64 loss: -0.07840502262115479
Batch 52/64 loss: -0.08422523736953735
Batch 53/64 loss: -0.06956732273101807
Batch 54/64 loss: -0.046989619731903076
Batch 55/64 loss: -0.09801948070526123
Batch 56/64 loss: -0.06577503681182861
Batch 57/64 loss: -0.09402436017990112
Batch 58/64 loss: -0.059143662452697754
Batch 59/64 loss: -0.10049784183502197
Batch 60/64 loss: -0.07907688617706299
Batch 61/64 loss: -0.09262454509735107
Batch 62/64 loss: -0.0898979902267456
Batch 63/64 loss: -0.07592630386352539
Batch 64/64 loss: -0.08199894428253174
Epoch 254  Train loss: -0.09017412382013658  Val loss: 0.05806433417133449
Epoch 255
-------------------------------
Batch 1/64 loss: -0.08797502517700195
Batch 2/64 loss: -0.07648110389709473
Batch 3/64 loss: -0.10721302032470703
Batch 4/64 loss: -0.1172376275062561
Batch 5/64 loss: -0.09284090995788574
Batch 6/64 loss: -0.11311537027359009
Batch 7/64 loss: -0.08267021179199219
Batch 8/64 loss: -0.11171197891235352
Batch 9/64 loss: -0.10216134786605835
Batch 10/64 loss: -0.1385256052017212
Batch 11/64 loss: -0.1113654375076294
Batch 12/64 loss: -0.09684085845947266
Batch 13/64 loss: -0.09591597318649292
Batch 14/64 loss: -0.10957592725753784
Batch 15/64 loss: -0.0934249758720398
Batch 16/64 loss: -0.099540114402771
Batch 17/64 loss: -0.09229379892349243
Batch 18/64 loss: -0.08930528163909912
Batch 19/64 loss: -0.10480326414108276
Batch 20/64 loss: -0.09591555595397949
Batch 21/64 loss: -0.0983877182006836
Batch 22/64 loss: -0.05367785692214966
Batch 23/64 loss: -0.07726538181304932
Batch 24/64 loss: -0.07281792163848877
Batch 25/64 loss: -0.04281193017959595
Batch 26/64 loss: -0.09197556972503662
Batch 27/64 loss: -0.07070386409759521
Batch 28/64 loss: -0.07827490568161011
Batch 29/64 loss: -0.04875689744949341
Batch 30/64 loss: -0.08620423078536987
Batch 31/64 loss: -0.11261212825775146
Batch 32/64 loss: -0.08730053901672363
Batch 33/64 loss: -0.08620226383209229
Batch 34/64 loss: -0.08398759365081787
Batch 35/64 loss: -0.09794217348098755
Batch 36/64 loss: -0.10173684358596802
Batch 37/64 loss: -0.07890015840530396
Batch 38/64 loss: -0.09579479694366455
Batch 39/64 loss: -0.10559606552124023
Batch 40/64 loss: -0.09421539306640625
Batch 41/64 loss: -0.0815003514289856
Batch 42/64 loss: -0.06618422269821167
Batch 43/64 loss: -0.10539352893829346
Batch 44/64 loss: -0.055389463901519775
Batch 45/64 loss: -0.08750158548355103
Batch 46/64 loss: -0.08601337671279907
Batch 47/64 loss: -0.09809058904647827
Batch 48/64 loss: -0.039643287658691406
Batch 49/64 loss: -0.07663601636886597
Batch 50/64 loss: -0.10986495018005371
Batch 51/64 loss: -0.07026755809783936
Batch 52/64 loss: -0.09747868776321411
Batch 53/64 loss: -0.10243481397628784
Batch 54/64 loss: -0.11631667613983154
Batch 55/64 loss: -0.0861024260520935
Batch 56/64 loss: -0.09453701972961426
Batch 57/64 loss: -0.06944125890731812
Batch 58/64 loss: -0.0868908166885376
Batch 59/64 loss: -0.08218109607696533
Batch 60/64 loss: -0.07634162902832031
Batch 61/64 loss: -0.10845845937728882
Batch 62/64 loss: -0.11107629537582397
Batch 63/64 loss: -0.09746575355529785
Batch 64/64 loss: -0.11057668924331665
Epoch 255  Train loss: -0.09054462746077893  Val loss: 0.05392943071745515
Epoch 256
-------------------------------
Batch 1/64 loss: -0.05872005224227905
Batch 2/64 loss: -0.10527896881103516
Batch 3/64 loss: -0.05299043655395508
Batch 4/64 loss: -0.11837714910507202
Batch 5/64 loss: -0.07140475511550903
Batch 6/64 loss: -0.09036922454833984
Batch 7/64 loss: -0.08968228101730347
Batch 8/64 loss: -0.07995456457138062
Batch 9/64 loss: -0.10075557231903076
Batch 10/64 loss: -0.114726722240448
Batch 11/64 loss: -0.09662562608718872
Batch 12/64 loss: -0.11556875705718994
Batch 13/64 loss: -0.09187763929367065
Batch 14/64 loss: -0.0763975977897644
Batch 15/64 loss: -0.10876208543777466
Batch 16/64 loss: -0.09376287460327148
Batch 17/64 loss: -0.09683477878570557
Batch 18/64 loss: -0.11198455095291138
Batch 19/64 loss: -0.13079094886779785
Batch 20/64 loss: -0.0694735050201416
Batch 21/64 loss: -0.12094587087631226
Batch 22/64 loss: -0.07415848970413208
Batch 23/64 loss: -0.09559071063995361
Batch 24/64 loss: -0.10997790098190308
Batch 25/64 loss: -0.09582078456878662
Batch 26/64 loss: -0.057693302631378174
Batch 27/64 loss: -0.0759735107421875
Batch 28/64 loss: -0.059149742126464844
Batch 29/64 loss: -0.09780633449554443
Batch 30/64 loss: -0.08033716678619385
Batch 31/64 loss: -0.084586501121521
Batch 32/64 loss: -0.10561704635620117
Batch 33/64 loss: -0.0926026701927185
Batch 34/64 loss: -0.11327826976776123
Batch 35/64 loss: -0.07449227571487427
Batch 36/64 loss: -0.09271222352981567
Batch 37/64 loss: -0.07637828588485718
Batch 38/64 loss: -0.09489178657531738
Batch 39/64 loss: -0.09839600324630737
Batch 40/64 loss: -0.09302204847335815
Batch 41/64 loss: -0.08697950839996338
Batch 42/64 loss: -0.08231312036514282
Batch 43/64 loss: -0.10456651449203491
Batch 44/64 loss: -0.09906017780303955
Batch 45/64 loss: -0.08414483070373535
Batch 46/64 loss: -0.0703842043876648
Batch 47/64 loss: -0.04963481426239014
Batch 48/64 loss: -0.11091333627700806
Batch 49/64 loss: -0.10050904750823975
Batch 50/64 loss: -0.07121467590332031
Batch 51/64 loss: -0.08369994163513184
Batch 52/64 loss: -0.10525780916213989
Batch 53/64 loss: -0.10868287086486816
Batch 54/64 loss: -0.10018384456634521
Batch 55/64 loss: -0.09052109718322754
Batch 56/64 loss: -0.08550786972045898
Batch 57/64 loss: -0.08134865760803223
Batch 58/64 loss: -0.12865716218948364
Batch 59/64 loss: -0.08136725425720215
Batch 60/64 loss: -0.09573429822921753
Batch 61/64 loss: -0.0961652398109436
Batch 62/64 loss: -0.06416487693786621
Batch 63/64 loss: -0.07318329811096191
Batch 64/64 loss: -0.11151325702667236
Epoch 256  Train loss: -0.0910682103213142  Val loss: 0.05242288931948213
Epoch 257
-------------------------------
Batch 1/64 loss: -0.07555127143859863
Batch 2/64 loss: -0.08330196142196655
Batch 3/64 loss: -0.11412972211837769
Batch 4/64 loss: -0.09011781215667725
Batch 5/64 loss: -0.10376328229904175
Batch 6/64 loss: -0.0874597430229187
Batch 7/64 loss: -0.07171732187271118
Batch 8/64 loss: -0.08249908685684204
Batch 9/64 loss: -0.09983199834823608
Batch 10/64 loss: -0.05708873271942139
Batch 11/64 loss: -0.1272616982460022
Batch 12/64 loss: -0.10336685180664062
Batch 13/64 loss: -0.01017087697982788
Batch 14/64 loss: -0.11438095569610596
Batch 15/64 loss: -0.07487314939498901
Batch 16/64 loss: -0.08989202976226807
Batch 17/64 loss: -0.1115800142288208
Batch 18/64 loss: -0.05324244499206543
Batch 19/64 loss: -0.04596215486526489
Batch 20/64 loss: -0.08568435907363892
Batch 21/64 loss: -0.0951586365699768
Batch 22/64 loss: -0.06925064325332642
Batch 23/64 loss: -0.07836860418319702
Batch 24/64 loss: -0.08991116285324097
Batch 25/64 loss: -0.07319176197052002
Batch 26/64 loss: -0.06800752878189087
Batch 27/64 loss: -0.06530803442001343
Batch 28/64 loss: -0.0901641845703125
Batch 29/64 loss: -0.10691219568252563
Batch 30/64 loss: -0.11884737014770508
Batch 31/64 loss: -0.09831470251083374
Batch 32/64 loss: -0.10637694597244263
Batch 33/64 loss: -0.08394885063171387
Batch 34/64 loss: -0.09225636720657349
Batch 35/64 loss: -0.06031358242034912
Batch 36/64 loss: -0.0702369213104248
Batch 37/64 loss: -0.11095595359802246
Batch 38/64 loss: -0.07612264156341553
Batch 39/64 loss: -0.09038031101226807
Batch 40/64 loss: -0.09213244915008545
Batch 41/64 loss: -0.09622740745544434
Batch 42/64 loss: -0.11866390705108643
Batch 43/64 loss: -0.0967869758605957
Batch 44/64 loss: -0.08881926536560059
Batch 45/64 loss: -0.05195796489715576
Batch 46/64 loss: -0.09683650732040405
Batch 47/64 loss: -0.07746946811676025
Batch 48/64 loss: -0.11566978693008423
Batch 49/64 loss: -0.09916913509368896
Batch 50/64 loss: -0.10361969470977783
Batch 51/64 loss: -0.03210270404815674
Batch 52/64 loss: -0.09173667430877686
Batch 53/64 loss: -0.10182154178619385
Batch 54/64 loss: -0.12364232540130615
Batch 55/64 loss: -0.11457324028015137
Batch 56/64 loss: -0.0985877513885498
Batch 57/64 loss: -0.09678924083709717
Batch 58/64 loss: -0.09879505634307861
Batch 59/64 loss: -0.0650988221168518
Batch 60/64 loss: -0.0720708966255188
Batch 61/64 loss: -0.09522724151611328
Batch 62/64 loss: -0.09951388835906982
Batch 63/64 loss: -0.07227271795272827
Batch 64/64 loss: -0.0941612720489502
Epoch 257  Train loss: -0.08778207816329657  Val loss: 0.0692461673746404
Epoch 258
-------------------------------
Batch 1/64 loss: -0.09840130805969238
Batch 2/64 loss: -0.11382055282592773
Batch 3/64 loss: -0.11241024732589722
Batch 4/64 loss: -0.05164015293121338
Batch 5/64 loss: -0.08423227071762085
Batch 6/64 loss: -0.09474372863769531
Batch 7/64 loss: -0.07773065567016602
Batch 8/64 loss: -0.06992876529693604
Batch 9/64 loss: -0.0875086784362793
Batch 10/64 loss: -0.10924547910690308
Batch 11/64 loss: -0.05592465400695801
Batch 12/64 loss: -0.10036790370941162
Batch 13/64 loss: -0.08797121047973633
Batch 14/64 loss: -0.06799054145812988
Batch 15/64 loss: -0.09992760419845581
Batch 16/64 loss: -0.08402031660079956
Batch 17/64 loss: -0.10129433870315552
Batch 18/64 loss: -0.1262667179107666
Batch 19/64 loss: -0.09558236598968506
Batch 20/64 loss: -0.0965874195098877
Batch 21/64 loss: -0.09328001737594604
Batch 22/64 loss: -0.09303939342498779
Batch 23/64 loss: -0.1283453106880188
Batch 24/64 loss: -0.09425699710845947
Batch 25/64 loss: -0.06540501117706299
Batch 26/64 loss: -0.10407662391662598
Batch 27/64 loss: -0.1384863257408142
Batch 28/64 loss: -0.0598408579826355
Batch 29/64 loss: -0.09325498342514038
Batch 30/64 loss: -0.06886720657348633
Batch 31/64 loss: -0.10379499197006226
Batch 32/64 loss: -0.09335094690322876
Batch 33/64 loss: -0.058647990226745605
Batch 34/64 loss: -0.10999941825866699
Batch 35/64 loss: -0.08760726451873779
Batch 36/64 loss: -0.10973536968231201
Batch 37/64 loss: -0.06326800584793091
Batch 38/64 loss: -0.08862149715423584
Batch 39/64 loss: -0.10225212574005127
Batch 40/64 loss: -0.11358273029327393
Batch 41/64 loss: -0.06409966945648193
Batch 42/64 loss: -0.06057929992675781
Batch 43/64 loss: -0.09671545028686523
Batch 44/64 loss: -0.0916869044303894
Batch 45/64 loss: -0.10501086711883545
Batch 46/64 loss: -0.0990591049194336
Batch 47/64 loss: -0.06635832786560059
Batch 48/64 loss: -0.0750008225440979
Batch 49/64 loss: -0.12013566493988037
Batch 50/64 loss: -0.10263592004776001
Batch 51/64 loss: -0.1392989158630371
Batch 52/64 loss: -0.09676694869995117
Batch 53/64 loss: -0.09228593111038208
Batch 54/64 loss: -0.10537868738174438
Batch 55/64 loss: -0.07503515481948853
Batch 56/64 loss: -0.08432126045227051
Batch 57/64 loss: -0.11857163906097412
Batch 58/64 loss: -0.09194135665893555
Batch 59/64 loss: -0.0914463996887207
Batch 60/64 loss: -0.1002652645111084
Batch 61/64 loss: -0.07886183261871338
Batch 62/64 loss: -0.10509705543518066
Batch 63/64 loss: -0.11097663640975952
Batch 64/64 loss: -0.10345351696014404
Epoch 258  Train loss: -0.09308905461255242  Val loss: 0.0650313236459424
Epoch 259
-------------------------------
Batch 1/64 loss: -0.09798914194107056
Batch 2/64 loss: -0.1297163963317871
Batch 3/64 loss: -0.08138000965118408
Batch 4/64 loss: -0.12399625778198242
Batch 5/64 loss: -0.09646487236022949
Batch 6/64 loss: -0.08559650182723999
Batch 7/64 loss: -0.09089750051498413
Batch 8/64 loss: -0.0718233585357666
Batch 9/64 loss: -0.09988117218017578
Batch 10/64 loss: -0.08084535598754883
Batch 11/64 loss: -0.11990344524383545
Batch 12/64 loss: -0.0983431339263916
Batch 13/64 loss: -0.09413301944732666
Batch 14/64 loss: -0.1092802882194519
Batch 15/64 loss: -0.10176140069961548
Batch 16/64 loss: -0.10023844242095947
Batch 17/64 loss: -0.09933078289031982
Batch 18/64 loss: -0.11440473794937134
Batch 19/64 loss: -0.11292839050292969
Batch 20/64 loss: -0.08943301439285278
Batch 21/64 loss: -0.11783707141876221
Batch 22/64 loss: -0.12169474363327026
Batch 23/64 loss: -0.11572039127349854
Batch 24/64 loss: -0.08032727241516113
Batch 25/64 loss: -0.0824347734451294
Batch 26/64 loss: -0.10956096649169922
Batch 27/64 loss: -0.06965088844299316
Batch 28/64 loss: -0.11811065673828125
Batch 29/64 loss: -0.11032736301422119
Batch 30/64 loss: -0.09097278118133545
Batch 31/64 loss: -0.06534695625305176
Batch 32/64 loss: -0.1010470986366272
Batch 33/64 loss: -0.0869104266166687
Batch 34/64 loss: -0.0837666392326355
Batch 35/64 loss: -0.03055441379547119
Batch 36/64 loss: -0.06968265771865845
Batch 37/64 loss: -0.11036652326583862
Batch 38/64 loss: -0.09333628416061401
Batch 39/64 loss: -0.08521610498428345
Batch 40/64 loss: -0.07890433073043823
Batch 41/64 loss: -0.07308679819107056
Batch 42/64 loss: -0.09203904867172241
Batch 43/64 loss: -0.12413668632507324
Batch 44/64 loss: -0.10323989391326904
Batch 45/64 loss: -0.07351875305175781
Batch 46/64 loss: -0.10143232345581055
Batch 47/64 loss: -0.08785104751586914
Batch 48/64 loss: -0.04535520076751709
Batch 49/64 loss: -0.10077399015426636
Batch 50/64 loss: -0.08820414543151855
Batch 51/64 loss: -0.06066864728927612
Batch 52/64 loss: -0.10930556058883667
Batch 53/64 loss: -0.10802650451660156
Batch 54/64 loss: -0.10265189409255981
Batch 55/64 loss: -0.07540357112884521
Batch 56/64 loss: -0.10319459438323975
Batch 57/64 loss: -0.10445845127105713
Batch 58/64 loss: -0.09223300218582153
Batch 59/64 loss: -0.11503434181213379
Batch 60/64 loss: -0.12882739305496216
Batch 61/64 loss: -0.058465778827667236
Batch 62/64 loss: -0.07851874828338623
Batch 63/64 loss: -0.09891527891159058
Batch 64/64 loss: -0.055509746074676514
Epoch 259  Train loss: -0.09391512987660426  Val loss: 0.054018051968407386
Epoch 260
-------------------------------
Batch 1/64 loss: -0.06272566318511963
Batch 2/64 loss: -0.06920212507247925
Batch 3/64 loss: -0.12222898006439209
Batch 4/64 loss: -0.09242868423461914
Batch 5/64 loss: -0.06854456663131714
Batch 6/64 loss: -0.09657609462738037
Batch 7/64 loss: -0.1300903558731079
Batch 8/64 loss: -0.12084197998046875
Batch 9/64 loss: -0.10579025745391846
Batch 10/64 loss: -0.10489404201507568
Batch 11/64 loss: -0.0831180214881897
Batch 12/64 loss: -0.08629924058914185
Batch 13/64 loss: -0.11898648738861084
Batch 14/64 loss: -0.10927355289459229
Batch 15/64 loss: -0.12490874528884888
Batch 16/64 loss: -0.10927224159240723
Batch 17/64 loss: -0.09343576431274414
Batch 18/64 loss: -0.09409689903259277
Batch 19/64 loss: -0.10301053524017334
Batch 20/64 loss: -0.07400012016296387
Batch 21/64 loss: -0.11180102825164795
Batch 22/64 loss: -0.04537653923034668
Batch 23/64 loss: -0.07774949073791504
Batch 24/64 loss: -0.0836954116821289
Batch 25/64 loss: -0.09806334972381592
Batch 26/64 loss: -0.10654306411743164
Batch 27/64 loss: -0.09081339836120605
Batch 28/64 loss: -0.08423727750778198
Batch 29/64 loss: -0.10158175230026245
Batch 30/64 loss: -0.09574627876281738
Batch 31/64 loss: -0.07175683975219727
Batch 32/64 loss: -0.12501120567321777
Batch 33/64 loss: -0.10509312152862549
Batch 34/64 loss: -0.07245457172393799
Batch 35/64 loss: -0.09881937503814697
Batch 36/64 loss: -0.07358944416046143
Batch 37/64 loss: -0.09337806701660156
Batch 38/64 loss: -0.09034907817840576
Batch 39/64 loss: -0.0591389536857605
Batch 40/64 loss: -0.07934367656707764
Batch 41/64 loss: -0.0980454683303833
Batch 42/64 loss: -0.0944448709487915
Batch 43/64 loss: -0.07741880416870117
Batch 44/64 loss: -0.09085428714752197
Batch 45/64 loss: -0.08979886770248413
Batch 46/64 loss: -0.11612206697463989
Batch 47/64 loss: -0.09386402368545532
Batch 48/64 loss: -0.10430985689163208
Batch 49/64 loss: -0.090035080909729
Batch 50/64 loss: -0.10302650928497314
Batch 51/64 loss: -0.08412772417068481
Batch 52/64 loss: -0.11972928047180176
Batch 53/64 loss: -0.059229254722595215
Batch 54/64 loss: -0.09591811895370483
Batch 55/64 loss: -0.08800524473190308
Batch 56/64 loss: -0.07202160358428955
Batch 57/64 loss: -0.09406661987304688
Batch 58/64 loss: -0.12125551700592041
Batch 59/64 loss: -0.11020225286483765
Batch 60/64 loss: -0.10781145095825195
Batch 61/64 loss: -0.0690804123878479
Batch 62/64 loss: -0.10425794124603271
Batch 63/64 loss: -0.10989820957183838
Batch 64/64 loss: -0.09414291381835938
Epoch 260  Train loss: -0.09409250091103946  Val loss: 0.05442591631125748
Epoch 261
-------------------------------
Batch 1/64 loss: -0.09434676170349121
Batch 2/64 loss: -0.11874288320541382
Batch 3/64 loss: -0.10990971326828003
Batch 4/64 loss: -0.1396222710609436
Batch 5/64 loss: -0.1253046989440918
Batch 6/64 loss: -0.11422955989837646
Batch 7/64 loss: -0.05308443307876587
Batch 8/64 loss: -0.12385666370391846
Batch 9/64 loss: -0.09669113159179688
Batch 10/64 loss: -0.08348608016967773
Batch 11/64 loss: -0.08521080017089844
Batch 12/64 loss: -0.0754246711730957
Batch 13/64 loss: -0.0817459225654602
Batch 14/64 loss: -0.0940435528755188
Batch 15/64 loss: -0.10474878549575806
Batch 16/64 loss: -0.06105238199234009
Batch 17/64 loss: -0.10431545972824097
Batch 18/64 loss: -0.07596242427825928
Batch 19/64 loss: -0.08914709091186523
Batch 20/64 loss: -0.09765630960464478
Batch 21/64 loss: -0.11421293020248413
Batch 22/64 loss: -0.08643084764480591
Batch 23/64 loss: -0.08131253719329834
Batch 24/64 loss: -0.09365606307983398
Batch 25/64 loss: -0.09716629981994629
Batch 26/64 loss: -0.08491456508636475
Batch 27/64 loss: -0.037104785442352295
Batch 28/64 loss: -0.10052597522735596
Batch 29/64 loss: -0.0581364631652832
Batch 30/64 loss: -0.11720556020736694
Batch 31/64 loss: -0.08603131771087646
Batch 32/64 loss: -0.09729617834091187
Batch 33/64 loss: -0.0966995358467102
Batch 34/64 loss: -0.0754135251045227
Batch 35/64 loss: -0.11480289697647095
Batch 36/64 loss: -0.0950283408164978
Batch 37/64 loss: -0.10829049348831177
Batch 38/64 loss: -0.10282814502716064
Batch 39/64 loss: -0.09638035297393799
Batch 40/64 loss: -0.051422178745269775
Batch 41/64 loss: -0.0684211254119873
Batch 42/64 loss: -0.10120773315429688
Batch 43/64 loss: -0.08566147089004517
Batch 44/64 loss: -0.1017683744430542
Batch 45/64 loss: -0.10236144065856934
Batch 46/64 loss: -0.08368408679962158
Batch 47/64 loss: -0.11044001579284668
Batch 48/64 loss: -0.07996439933776855
Batch 49/64 loss: -0.10602831840515137
Batch 50/64 loss: -0.08320772647857666
Batch 51/64 loss: -0.0983540415763855
Batch 52/64 loss: -0.05890846252441406
Batch 53/64 loss: -0.1195211410522461
Batch 54/64 loss: -0.113983154296875
Batch 55/64 loss: -0.13027715682983398
Batch 56/64 loss: -0.08719527721405029
Batch 57/64 loss: -0.113791823387146
Batch 58/64 loss: -0.10552990436553955
Batch 59/64 loss: -0.08677875995635986
Batch 60/64 loss: -0.054413437843322754
Batch 61/64 loss: -0.09254980087280273
Batch 62/64 loss: -0.04719662666320801
Batch 63/64 loss: -0.08141207695007324
Batch 64/64 loss: -0.1035088300704956
Epoch 261  Train loss: -0.09276436964670817  Val loss: 0.05741196317771047
Epoch 262
-------------------------------
Batch 1/64 loss: -0.12329882383346558
Batch 2/64 loss: -0.08604991436004639
Batch 3/64 loss: -0.10848045349121094
Batch 4/64 loss: -0.13288623094558716
Batch 5/64 loss: -0.11607056856155396
Batch 6/64 loss: -0.11222368478775024
Batch 7/64 loss: -0.12342000007629395
Batch 8/64 loss: -0.11118459701538086
Batch 9/64 loss: -0.10520130395889282
Batch 10/64 loss: -0.09912830591201782
Batch 11/64 loss: -0.0739138126373291
Batch 12/64 loss: -0.09879505634307861
Batch 13/64 loss: -0.13452541828155518
Batch 14/64 loss: -0.09531134366989136
Batch 15/64 loss: -0.0700216293334961
Batch 16/64 loss: -0.06996864080429077
Batch 17/64 loss: -0.09769845008850098
Batch 18/64 loss: -0.08469843864440918
Batch 19/64 loss: -0.11259084939956665
Batch 20/64 loss: -0.07360345125198364
Batch 21/64 loss: -0.1084592342376709
Batch 22/64 loss: -0.08182287216186523
Batch 23/64 loss: -0.11468547582626343
Batch 24/64 loss: -0.12361180782318115
Batch 25/64 loss: -0.10229510068893433
Batch 26/64 loss: -0.0862392783164978
Batch 27/64 loss: -0.08291590213775635
Batch 28/64 loss: -0.11035233736038208
Batch 29/64 loss: -0.05488443374633789
Batch 30/64 loss: -0.10487586259841919
Batch 31/64 loss: -0.0947619080543518
Batch 32/64 loss: -0.09657561779022217
Batch 33/64 loss: -0.04817551374435425
Batch 34/64 loss: -0.09650564193725586
Batch 35/64 loss: -0.07236331701278687
Batch 36/64 loss: -0.08546239137649536
Batch 37/64 loss: -0.10451877117156982
Batch 38/64 loss: -0.08894652128219604
Batch 39/64 loss: -0.04686892032623291
Batch 40/64 loss: -0.10036289691925049
Batch 41/64 loss: -0.10735797882080078
Batch 42/64 loss: -0.08255916833877563
Batch 43/64 loss: -0.09280312061309814
Batch 44/64 loss: -0.09956032037734985
Batch 45/64 loss: -0.11339610815048218
Batch 46/64 loss: -0.06953656673431396
Batch 47/64 loss: -0.07172775268554688
Batch 48/64 loss: -0.08840042352676392
Batch 49/64 loss: -0.1060030460357666
Batch 50/64 loss: -0.10316747426986694
Batch 51/64 loss: -0.08013296127319336
Batch 52/64 loss: -0.09184730052947998
Batch 53/64 loss: -0.08371132612228394
Batch 54/64 loss: -0.10546326637268066
Batch 55/64 loss: -0.12017619609832764
Batch 56/64 loss: -0.1035735011100769
Batch 57/64 loss: -0.11459362506866455
Batch 58/64 loss: -0.11757886409759521
Batch 59/64 loss: -0.12304049730300903
Batch 60/64 loss: -0.07762414216995239
Batch 61/64 loss: -0.10048937797546387
Batch 62/64 loss: -0.11225223541259766
Batch 63/64 loss: -0.10538339614868164
Batch 64/64 loss: -0.11635220050811768
Epoch 262  Train loss: -0.09711996200037938  Val loss: 0.05445587716971066
Epoch 263
-------------------------------
Batch 1/64 loss: -0.13080310821533203
Batch 2/64 loss: -0.11788642406463623
Batch 3/64 loss: -0.08044523000717163
Batch 4/64 loss: -0.1007775068283081
Batch 5/64 loss: -0.12483769655227661
Batch 6/64 loss: -0.11223691701889038
Batch 7/64 loss: -0.06829482316970825
Batch 8/64 loss: -0.1019824743270874
Batch 9/64 loss: -0.069460928440094
Batch 10/64 loss: -0.05554109811782837
Batch 11/64 loss: -0.1291431188583374
Batch 12/64 loss: -0.12104946374893188
Batch 13/64 loss: -0.10324221849441528
Batch 14/64 loss: -0.08906173706054688
Batch 15/64 loss: -0.06984734535217285
Batch 16/64 loss: -0.10754823684692383
Batch 17/64 loss: -0.07007110118865967
Batch 18/64 loss: -0.10606533288955688
Batch 19/64 loss: -0.12626880407333374
Batch 20/64 loss: -0.08175504207611084
Batch 21/64 loss: -0.030580639839172363
Batch 22/64 loss: -0.11235135793685913
Batch 23/64 loss: -0.12983685731887817
Batch 24/64 loss: -0.09081709384918213
Batch 25/64 loss: -0.10029351711273193
Batch 26/64 loss: -0.10528045892715454
Batch 27/64 loss: -0.0758446455001831
Batch 28/64 loss: -0.11496216058731079
Batch 29/64 loss: -0.10290271043777466
Batch 30/64 loss: -0.08849877119064331
Batch 31/64 loss: -0.13302075862884521
Batch 32/64 loss: -0.08925092220306396
Batch 33/64 loss: -0.07509088516235352
Batch 34/64 loss: -0.08967900276184082
Batch 35/64 loss: -0.07752847671508789
Batch 36/64 loss: -0.12464386224746704
Batch 37/64 loss: -0.07926976680755615
Batch 38/64 loss: -0.09841394424438477
Batch 39/64 loss: -0.08207356929779053
Batch 40/64 loss: -0.062236130237579346
Batch 41/64 loss: -0.10699683427810669
Batch 42/64 loss: -0.09277701377868652
Batch 43/64 loss: -0.09255486726760864
Batch 44/64 loss: -0.10502713918685913
Batch 45/64 loss: -0.08989220857620239
Batch 46/64 loss: -0.07153522968292236
Batch 47/64 loss: -0.08017975091934204
Batch 48/64 loss: -0.11854702234268188
Batch 49/64 loss: -0.08802807331085205
Batch 50/64 loss: -0.11028403043746948
Batch 51/64 loss: -0.10020989179611206
Batch 52/64 loss: -0.10372298955917358
Batch 53/64 loss: -0.02692049741744995
Batch 54/64 loss: -0.05575680732727051
Batch 55/64 loss: -0.07365596294403076
Batch 56/64 loss: -0.08897203207015991
Batch 57/64 loss: -0.09021908044815063
Batch 58/64 loss: -0.07949280738830566
Batch 59/64 loss: -0.09130358695983887
Batch 60/64 loss: -0.11227506399154663
Batch 61/64 loss: -0.12472426891326904
Batch 62/64 loss: -0.10806334018707275
Batch 63/64 loss: -0.08357119560241699
Batch 64/64 loss: -0.11806982755661011
Epoch 263  Train loss: -0.09430833260218302  Val loss: 0.05458477723229792
Epoch 264
-------------------------------
Batch 1/64 loss: -0.08378267288208008
Batch 2/64 loss: -0.11677044630050659
Batch 3/64 loss: -0.10166776180267334
Batch 4/64 loss: -0.10027867555618286
Batch 5/64 loss: -0.10650813579559326
Batch 6/64 loss: -0.12925678491592407
Batch 7/64 loss: -0.09676718711853027
Batch 8/64 loss: -0.1309850811958313
Batch 9/64 loss: -0.10463255643844604
Batch 10/64 loss: -0.06988763809204102
Batch 11/64 loss: -0.08981966972351074
Batch 12/64 loss: -0.08690595626831055
Batch 13/64 loss: -0.11154693365097046
Batch 14/64 loss: -0.11933457851409912
Batch 15/64 loss: -0.10652363300323486
Batch 16/64 loss: -0.1063992977142334
Batch 17/64 loss: -0.08017134666442871
Batch 18/64 loss: -0.09563267230987549
Batch 19/64 loss: -0.10591310262680054
Batch 20/64 loss: -0.10635930299758911
Batch 21/64 loss: -0.09667271375656128
Batch 22/64 loss: -0.11136090755462646
Batch 23/64 loss: -0.10863000154495239
Batch 24/64 loss: -0.07699960470199585
Batch 25/64 loss: -0.11350136995315552
Batch 26/64 loss: -0.07725173234939575
Batch 27/64 loss: -0.11205512285232544
Batch 28/64 loss: -0.08671349287033081
Batch 29/64 loss: -0.087954580783844
Batch 30/64 loss: -0.09268456697463989
Batch 31/64 loss: -0.0693812370300293
Batch 32/64 loss: -0.0621563196182251
Batch 33/64 loss: -0.10618770122528076
Batch 34/64 loss: -0.11069780588150024
Batch 35/64 loss: -0.13371127843856812
Batch 36/64 loss: -0.07857531309127808
Batch 37/64 loss: -0.0959775447845459
Batch 38/64 loss: -0.08643782138824463
Batch 39/64 loss: -0.10265737771987915
Batch 40/64 loss: -0.0670783519744873
Batch 41/64 loss: -0.09723407030105591
Batch 42/64 loss: -0.10990715026855469
Batch 43/64 loss: -0.09438180923461914
Batch 44/64 loss: -0.0919148325920105
Batch 45/64 loss: -0.09357321262359619
Batch 46/64 loss: -0.13750970363616943
Batch 47/64 loss: -0.08724606037139893
Batch 48/64 loss: -0.07781225442886353
Batch 49/64 loss: -0.08143168687820435
Batch 50/64 loss: -0.07931166887283325
Batch 51/64 loss: -0.07797408103942871
Batch 52/64 loss: -0.09232419729232788
Batch 53/64 loss: -0.0997610092163086
Batch 54/64 loss: -0.08493274450302124
Batch 55/64 loss: -0.09307152032852173
Batch 56/64 loss: -0.08253836631774902
Batch 57/64 loss: -0.07144820690155029
Batch 58/64 loss: -0.11992824077606201
Batch 59/64 loss: -0.08600443601608276
Batch 60/64 loss: -0.12550359964370728
Batch 61/64 loss: -0.07882750034332275
Batch 62/64 loss: -0.07723814249038696
Batch 63/64 loss: -0.10359686613082886
Batch 64/64 loss: -0.10879027843475342
Epoch 264  Train loss: -0.09648458583682192  Val loss: 0.05757252784938747
Epoch 265
-------------------------------
Batch 1/64 loss: -0.11882835626602173
Batch 2/64 loss: -0.11447924375534058
Batch 3/64 loss: -0.05195397138595581
Batch 4/64 loss: -0.12894415855407715
Batch 5/64 loss: -0.10249054431915283
Batch 6/64 loss: -0.11500173807144165
Batch 7/64 loss: -0.09873002767562866
Batch 8/64 loss: -0.10209840536117554
Batch 9/64 loss: -0.09935945272445679
Batch 10/64 loss: -0.12169736623764038
Batch 11/64 loss: -0.11298292875289917
Batch 12/64 loss: -0.05882364511489868
Batch 13/64 loss: -0.08039039373397827
Batch 14/64 loss: -0.07267141342163086
Batch 15/64 loss: -0.1305813193321228
Batch 16/64 loss: -0.09669381380081177
Batch 17/64 loss: -0.09361088275909424
Batch 18/64 loss: -0.13309884071350098
Batch 19/64 loss: -0.04309976100921631
Batch 20/64 loss: -0.127221941947937
Batch 21/64 loss: -0.0725511908531189
Batch 22/64 loss: -0.10484391450881958
Batch 23/64 loss: -0.08081555366516113
Batch 24/64 loss: -0.10848486423492432
Batch 25/64 loss: -0.1123775839805603
Batch 26/64 loss: -0.09692788124084473
Batch 27/64 loss: -0.11368238925933838
Batch 28/64 loss: -0.08629697561264038
Batch 29/64 loss: -0.10020983219146729
Batch 30/64 loss: -0.0772213339805603
Batch 31/64 loss: -0.09384113550186157
Batch 32/64 loss: -0.03562849760055542
Batch 33/64 loss: -0.12722784280776978
Batch 34/64 loss: -0.10857248306274414
Batch 35/64 loss: -0.0997471809387207
Batch 36/64 loss: -0.11686205863952637
Batch 37/64 loss: -0.08293110132217407
Batch 38/64 loss: -0.09102284908294678
Batch 39/64 loss: -0.11682415008544922
Batch 40/64 loss: -0.06930094957351685
Batch 41/64 loss: -0.08700168132781982
Batch 42/64 loss: -0.10840088129043579
Batch 43/64 loss: -0.09949171543121338
Batch 44/64 loss: -0.09484660625457764
Batch 45/64 loss: -0.09715759754180908
Batch 46/64 loss: -0.09977459907531738
Batch 47/64 loss: -0.10214036703109741
Batch 48/64 loss: -0.10331535339355469
Batch 49/64 loss: -0.08709275722503662
Batch 50/64 loss: -0.09952908754348755
Batch 51/64 loss: -0.10392069816589355
Batch 52/64 loss: -0.08955436944961548
Batch 53/64 loss: -0.08382648229598999
Batch 54/64 loss: -0.08992987871170044
Batch 55/64 loss: -0.07643961906433105
Batch 56/64 loss: -0.10400807857513428
Batch 57/64 loss: -0.0917433500289917
Batch 58/64 loss: -0.09482109546661377
Batch 59/64 loss: -0.07075881958007812
Batch 60/64 loss: -0.06605744361877441
Batch 61/64 loss: -0.09078270196914673
Batch 62/64 loss: -0.10866713523864746
Batch 63/64 loss: -0.11072385311126709
Batch 64/64 loss: -0.09394252300262451
Epoch 265  Train loss: -0.09613441626230876  Val loss: 0.05885467988109261
Epoch 266
-------------------------------
Batch 1/64 loss: -0.09129422903060913
Batch 2/64 loss: -0.10053092241287231
Batch 3/64 loss: -0.10414373874664307
Batch 4/64 loss: -0.05867564678192139
Batch 5/64 loss: -0.09151583909988403
Batch 6/64 loss: -0.11032408475875854
Batch 7/64 loss: -0.11914938688278198
Batch 8/64 loss: -0.08506035804748535
Batch 9/64 loss: -0.10472983121871948
Batch 10/64 loss: -0.09389996528625488
Batch 11/64 loss: -0.10314559936523438
Batch 12/64 loss: -0.11186999082565308
Batch 13/64 loss: -0.11580407619476318
Batch 14/64 loss: -0.09356266260147095
Batch 15/64 loss: -0.11892896890640259
Batch 16/64 loss: -0.13506019115447998
Batch 17/64 loss: -0.09319686889648438
Batch 18/64 loss: -0.13422638177871704
Batch 19/64 loss: -0.14291810989379883
Batch 20/64 loss: -0.10018044710159302
Batch 21/64 loss: -0.08947378396987915
Batch 22/64 loss: -0.11839044094085693
Batch 23/64 loss: -0.14361572265625
Batch 24/64 loss: -0.09200584888458252
Batch 25/64 loss: -0.07629942893981934
Batch 26/64 loss: -0.11537307500839233
Batch 27/64 loss: -0.11685019731521606
Batch 28/64 loss: -0.10531467199325562
Batch 29/64 loss: -0.07450151443481445
Batch 30/64 loss: -0.08786207437515259
Batch 31/64 loss: -0.08785027265548706
Batch 32/64 loss: -0.09947985410690308
Batch 33/64 loss: -0.12137031555175781
Batch 34/64 loss: -0.08464407920837402
Batch 35/64 loss: -0.10039818286895752
Batch 36/64 loss: -0.10882347822189331
Batch 37/64 loss: -0.11917591094970703
Batch 38/64 loss: -0.11091035604476929
Batch 39/64 loss: -0.0033216476440429688
Batch 40/64 loss: -0.09150612354278564
Batch 41/64 loss: -0.09238523244857788
Batch 42/64 loss: -0.03968018293380737
Batch 43/64 loss: -0.05422389507293701
Batch 44/64 loss: -0.058359503746032715
Batch 45/64 loss: -0.08500665426254272
Batch 46/64 loss: -0.031081795692443848
Batch 47/64 loss: -0.0960540771484375
Batch 48/64 loss: -0.09234285354614258
Batch 49/64 loss: -0.08505678176879883
Batch 50/64 loss: -0.10869097709655762
Batch 51/64 loss: -0.09465324878692627
Batch 52/64 loss: -0.1072075366973877
Batch 53/64 loss: -0.0720900297164917
Batch 54/64 loss: -0.07895416021347046
Batch 55/64 loss: -0.09852021932601929
Batch 56/64 loss: -0.11351555585861206
Batch 57/64 loss: -0.1023871898651123
Batch 58/64 loss: -0.08289623260498047
Batch 59/64 loss: -0.10137414932250977
Batch 60/64 loss: -0.11339461803436279
Batch 61/64 loss: -0.06132739782333374
Batch 62/64 loss: -0.08569115400314331
Batch 63/64 loss: -0.09631705284118652
Batch 64/64 loss: -0.05729210376739502
Epoch 266  Train loss: -0.0948951192930633  Val loss: 0.05802503422773171
Epoch 267
-------------------------------
Batch 1/64 loss: -0.09611433744430542
Batch 2/64 loss: -0.08009040355682373
Batch 3/64 loss: -0.08589392900466919
Batch 4/64 loss: -0.11622679233551025
Batch 5/64 loss: -0.08456391096115112
Batch 6/64 loss: -0.04783588647842407
Batch 7/64 loss: -0.10496145486831665
Batch 8/64 loss: -0.0661514401435852
Batch 9/64 loss: -0.08351749181747437
Batch 10/64 loss: -0.1185007095336914
Batch 11/64 loss: -0.09956449270248413
Batch 12/64 loss: -0.07063889503479004
Batch 13/64 loss: -0.10049211978912354
Batch 14/64 loss: -0.09143632650375366
Batch 15/64 loss: -0.11695235967636108
Batch 16/64 loss: -0.08525925874710083
Batch 17/64 loss: -0.1109420657157898
Batch 18/64 loss: -0.08568388223648071
Batch 19/64 loss: -0.1351165771484375
Batch 20/64 loss: -0.07219511270523071
Batch 21/64 loss: -0.11786830425262451
Batch 22/64 loss: -0.08479934930801392
Batch 23/64 loss: -0.10314768552780151
Batch 24/64 loss: -0.12103450298309326
Batch 25/64 loss: -0.09066933393478394
Batch 26/64 loss: -0.07980650663375854
Batch 27/64 loss: -0.10122138261795044
Batch 28/64 loss: -0.10351645946502686
Batch 29/64 loss: -0.11626207828521729
Batch 30/64 loss: -0.07286006212234497
Batch 31/64 loss: -0.10441756248474121
Batch 32/64 loss: -0.11697971820831299
Batch 33/64 loss: -0.07828980684280396
Batch 34/64 loss: -0.11149740219116211
Batch 35/64 loss: -0.10151070356369019
Batch 36/64 loss: -0.1070936918258667
Batch 37/64 loss: -0.11316555738449097
Batch 38/64 loss: -0.08826065063476562
Batch 39/64 loss: -0.08697766065597534
Batch 40/64 loss: -0.0987619161605835
Batch 41/64 loss: -0.08692246675491333
Batch 42/64 loss: -0.08443820476531982
Batch 43/64 loss: -0.11549824476242065
Batch 44/64 loss: -0.09980058670043945
Batch 45/64 loss: -0.11286115646362305
Batch 46/64 loss: -0.11697089672088623
Batch 47/64 loss: -0.09962427616119385
Batch 48/64 loss: -0.09612596035003662
Batch 49/64 loss: -0.06949347257614136
Batch 50/64 loss: -0.11850237846374512
Batch 51/64 loss: -0.02417623996734619
Batch 52/64 loss: -0.08104389905929565
Batch 53/64 loss: -0.11941611766815186
Batch 54/64 loss: -0.08178329467773438
Batch 55/64 loss: -0.10031187534332275
Batch 56/64 loss: -0.1281675100326538
Batch 57/64 loss: -0.12465083599090576
Batch 58/64 loss: -0.07432055473327637
Batch 59/64 loss: -0.12221837043762207
Batch 60/64 loss: -0.08588731288909912
Batch 61/64 loss: -0.08055400848388672
Batch 62/64 loss: -0.09103953838348389
Batch 63/64 loss: -0.09040993452072144
Batch 64/64 loss: -0.10772472620010376
Epoch 267  Train loss: -0.09623981901243621  Val loss: 0.054737478597057646
Epoch 268
-------------------------------
Batch 1/64 loss: -0.11859315633773804
Batch 2/64 loss: -0.11004352569580078
Batch 3/64 loss: -0.07323569059371948
Batch 4/64 loss: -0.07715678215026855
Batch 5/64 loss: -0.10149645805358887
Batch 6/64 loss: -0.08863645792007446
Batch 7/64 loss: -0.12093687057495117
Batch 8/64 loss: -0.10048520565032959
Batch 9/64 loss: -0.08981382846832275
Batch 10/64 loss: -0.0869971513748169
Batch 11/64 loss: -0.10332512855529785
Batch 12/64 loss: -0.11713302135467529
Batch 13/64 loss: -0.08535414934158325
Batch 14/64 loss: -0.06342297792434692
Batch 15/64 loss: -0.119293212890625
Batch 16/64 loss: -0.10247147083282471
Batch 17/64 loss: -0.09130531549453735
Batch 18/64 loss: -0.09235107898712158
Batch 19/64 loss: -0.09239161014556885
Batch 20/64 loss: -0.07613331079483032
Batch 21/64 loss: -0.0648806095123291
Batch 22/64 loss: -0.07612347602844238
Batch 23/64 loss: -0.11275094747543335
Batch 24/64 loss: -0.12108689546585083
Batch 25/64 loss: -0.12002450227737427
Batch 26/64 loss: -0.11805868148803711
Batch 27/64 loss: -0.07719439268112183
Batch 28/64 loss: -0.09733796119689941
Batch 29/64 loss: -0.11860644817352295
Batch 30/64 loss: -0.12319749593734741
Batch 31/64 loss: -0.10222280025482178
Batch 32/64 loss: -0.11566287279129028
Batch 33/64 loss: -0.09545272588729858
Batch 34/64 loss: -0.06573379039764404
Batch 35/64 loss: -0.10389703512191772
Batch 36/64 loss: -0.09860044717788696
Batch 37/64 loss: -0.08844727277755737
Batch 38/64 loss: -0.1039237380027771
Batch 39/64 loss: -0.1343621015548706
Batch 40/64 loss: -0.12114900350570679
Batch 41/64 loss: -0.10356640815734863
Batch 42/64 loss: -0.10256016254425049
Batch 43/64 loss: -0.08297723531723022
Batch 44/64 loss: -0.09030383825302124
Batch 45/64 loss: -0.11699187755584717
Batch 46/64 loss: -0.09649908542633057
Batch 47/64 loss: -0.06274384260177612
Batch 48/64 loss: -0.09144324064254761
Batch 49/64 loss: -0.12582236528396606
Batch 50/64 loss: -0.11307084560394287
Batch 51/64 loss: -0.06752157211303711
Batch 52/64 loss: -0.1164391040802002
Batch 53/64 loss: -0.06733870506286621
Batch 54/64 loss: -0.06995540857315063
Batch 55/64 loss: -0.10528576374053955
Batch 56/64 loss: -0.07781124114990234
Batch 57/64 loss: -0.13014143705368042
Batch 58/64 loss: -0.09522902965545654
Batch 59/64 loss: -0.10299205780029297
Batch 60/64 loss: -0.1280360221862793
Batch 61/64 loss: -0.10042768716812134
Batch 62/64 loss: -0.09163880348205566
Batch 63/64 loss: -0.09018594026565552
Batch 64/64 loss: -0.11005985736846924
Epoch 268  Train loss: -0.09852260655047847  Val loss: 0.06535905702007595
Epoch 269
-------------------------------
Batch 1/64 loss: -0.11531591415405273
Batch 2/64 loss: -0.07128715515136719
Batch 3/64 loss: -0.12491488456726074
Batch 4/64 loss: -0.1092832088470459
Batch 5/64 loss: -0.14316999912261963
Batch 6/64 loss: -0.09573578834533691
Batch 7/64 loss: -0.1053781509399414
Batch 8/64 loss: -0.0950276255607605
Batch 9/64 loss: -0.1051216721534729
Batch 10/64 loss: -0.1107214093208313
Batch 11/64 loss: -0.09635871648788452
Batch 12/64 loss: -0.08832931518554688
Batch 13/64 loss: -0.0921216607093811
Batch 14/64 loss: -0.11109936237335205
Batch 15/64 loss: -0.1310177445411682
Batch 16/64 loss: -0.10084712505340576
Batch 17/64 loss: -0.0926738977432251
Batch 18/64 loss: -0.0907289981842041
Batch 19/64 loss: -0.0745503306388855
Batch 20/64 loss: -0.08553671836853027
Batch 21/64 loss: -0.1077127456665039
Batch 22/64 loss: -0.11698222160339355
Batch 23/64 loss: -0.11889833211898804
Batch 24/64 loss: -0.09953606128692627
Batch 25/64 loss: -0.10797393321990967
Batch 26/64 loss: -0.0934910774230957
Batch 27/64 loss: -0.09811210632324219
Batch 28/64 loss: -0.10249847173690796
Batch 29/64 loss: -0.05255311727523804
Batch 30/64 loss: -0.09020668268203735
Batch 31/64 loss: -0.07126063108444214
Batch 32/64 loss: -0.08719825744628906
Batch 33/64 loss: -0.10181665420532227
Batch 34/64 loss: -0.09479629993438721
Batch 35/64 loss: -0.09635984897613525
Batch 36/64 loss: -0.07512938976287842
Batch 37/64 loss: -0.0971367359161377
Batch 38/64 loss: -0.060178518295288086
Batch 39/64 loss: -0.0924270749092102
Batch 40/64 loss: -0.09379357099533081
Batch 41/64 loss: -0.0937117338180542
Batch 42/64 loss: -0.12541508674621582
Batch 43/64 loss: -0.0860329270362854
Batch 44/64 loss: -0.10168612003326416
Batch 45/64 loss: -0.06590384244918823
Batch 46/64 loss: -0.09479707479476929
Batch 47/64 loss: -0.11918723583221436
Batch 48/64 loss: -0.05579948425292969
Batch 49/64 loss: -0.11858570575714111
Batch 50/64 loss: -0.12688279151916504
Batch 51/64 loss: -0.09683704376220703
Batch 52/64 loss: -0.07920175790786743
Batch 53/64 loss: -0.10164415836334229
Batch 54/64 loss: -0.10161298513412476
Batch 55/64 loss: -0.11991536617279053
Batch 56/64 loss: -0.14479762315750122
Batch 57/64 loss: -0.07559949159622192
Batch 58/64 loss: -0.11270910501480103
Batch 59/64 loss: -0.1086571216583252
Batch 60/64 loss: -0.04313778877258301
Batch 61/64 loss: -0.11514902114868164
Batch 62/64 loss: -0.0960264801979065
Batch 63/64 loss: -0.10247111320495605
Batch 64/64 loss: -0.09716343879699707
Epoch 269  Train loss: -0.09813200071746228  Val loss: 0.05687588576189021
Epoch 270
-------------------------------
Batch 1/64 loss: -0.12944930791854858
Batch 2/64 loss: -0.08305895328521729
Batch 3/64 loss: -0.09122681617736816
Batch 4/64 loss: -0.07006657123565674
Batch 5/64 loss: -0.09273570775985718
Batch 6/64 loss: -0.11218750476837158
Batch 7/64 loss: -0.07925689220428467
Batch 8/64 loss: -0.09243041276931763
Batch 9/64 loss: -0.09878182411193848
Batch 10/64 loss: -0.07429802417755127
Batch 11/64 loss: -0.07967019081115723
Batch 12/64 loss: -0.09110152721405029
Batch 13/64 loss: -0.0999917984008789
Batch 14/64 loss: -0.11701637506484985
Batch 15/64 loss: -0.08819711208343506
Batch 16/64 loss: -0.10896199941635132
Batch 17/64 loss: -0.11639648675918579
Batch 18/64 loss: -0.09558248519897461
Batch 19/64 loss: -0.14106470346450806
Batch 20/64 loss: -0.09826964139938354
Batch 21/64 loss: -0.08035367727279663
Batch 22/64 loss: -0.10642904043197632
Batch 23/64 loss: -0.10738861560821533
Batch 24/64 loss: -0.10515445470809937
Batch 25/64 loss: -0.10669940710067749
Batch 26/64 loss: -0.12257468700408936
Batch 27/64 loss: -0.11096298694610596
Batch 28/64 loss: -0.10842597484588623
Batch 29/64 loss: -0.11133801937103271
Batch 30/64 loss: -0.09361827373504639
Batch 31/64 loss: -0.09415757656097412
Batch 32/64 loss: -0.11496418714523315
Batch 33/64 loss: -0.11557722091674805
Batch 34/64 loss: -0.11950522661209106
Batch 35/64 loss: -0.1013231873512268
Batch 36/64 loss: -0.07055723667144775
Batch 37/64 loss: -0.13706886768341064
Batch 38/64 loss: -0.10648161172866821
Batch 39/64 loss: -0.11738228797912598
Batch 40/64 loss: -0.09360289573669434
Batch 41/64 loss: -0.12274348735809326
Batch 42/64 loss: -0.07385224103927612
Batch 43/64 loss: -0.09475892782211304
Batch 44/64 loss: -0.11244463920593262
Batch 45/64 loss: -0.1082693338394165
Batch 46/64 loss: -0.10656839609146118
Batch 47/64 loss: -0.06266045570373535
Batch 48/64 loss: -0.11056762933731079
Batch 49/64 loss: -0.11100494861602783
Batch 50/64 loss: -0.09827560186386108
Batch 51/64 loss: -0.08907324075698853
Batch 52/64 loss: -0.09813046455383301
Batch 53/64 loss: -0.08568179607391357
Batch 54/64 loss: -0.08846825361251831
Batch 55/64 loss: -0.11404764652252197
Batch 56/64 loss: -0.0824282169342041
Batch 57/64 loss: -0.10914850234985352
Batch 58/64 loss: -0.10437256097793579
Batch 59/64 loss: -0.11473256349563599
Batch 60/64 loss: -0.12194478511810303
Batch 61/64 loss: -0.1238439679145813
Batch 62/64 loss: -0.07677614688873291
Batch 63/64 loss: -0.08463430404663086
Batch 64/64 loss: -0.06121230125427246
Epoch 270  Train loss: -0.10076309185402066  Val loss: 0.060024313295829745
Epoch 271
-------------------------------
Batch 1/64 loss: -0.1180269718170166
Batch 2/64 loss: -0.1249992847442627
Batch 3/64 loss: -0.08298361301422119
Batch 4/64 loss: -0.0895739197731018
Batch 5/64 loss: -0.12862515449523926
Batch 6/64 loss: -0.0907512903213501
Batch 7/64 loss: -0.11439710855484009
Batch 8/64 loss: -0.12026464939117432
Batch 9/64 loss: -0.13109838962554932
Batch 10/64 loss: -0.10730677843093872
Batch 11/64 loss: -0.08047288656234741
Batch 12/64 loss: -0.08173680305480957
Batch 13/64 loss: -0.11721253395080566
Batch 14/64 loss: -0.09310322999954224
Batch 15/64 loss: -0.10050606727600098
Batch 16/64 loss: -0.07441592216491699
Batch 17/64 loss: -0.12311047315597534
Batch 18/64 loss: -0.12167203426361084
Batch 19/64 loss: -0.10090327262878418
Batch 20/64 loss: -0.08856332302093506
Batch 21/64 loss: -0.06814157962799072
Batch 22/64 loss: -0.09133875370025635
Batch 23/64 loss: -0.054106712341308594
Batch 24/64 loss: -0.06656181812286377
Batch 25/64 loss: -0.10320544242858887
Batch 26/64 loss: -0.0890352725982666
Batch 27/64 loss: -0.101406991481781
Batch 28/64 loss: -0.08877450227737427
Batch 29/64 loss: -0.12093794345855713
Batch 30/64 loss: -0.09542912244796753
Batch 31/64 loss: -0.09220069646835327
Batch 32/64 loss: -0.11868500709533691
Batch 33/64 loss: -0.08927524089813232
Batch 34/64 loss: -0.10245770215988159
Batch 35/64 loss: -0.09106290340423584
Batch 36/64 loss: -0.11286771297454834
Batch 37/64 loss: -0.09023761749267578
Batch 38/64 loss: -0.09037238359451294
Batch 39/64 loss: -0.11708688735961914
Batch 40/64 loss: -0.10297352075576782
Batch 41/64 loss: -0.10677826404571533
Batch 42/64 loss: -0.10525178909301758
Batch 43/64 loss: -0.11540627479553223
Batch 44/64 loss: -0.09526228904724121
Batch 45/64 loss: -0.0760965347290039
Batch 46/64 loss: -0.10177367925643921
Batch 47/64 loss: -0.09734052419662476
Batch 48/64 loss: -0.13458037376403809
Batch 49/64 loss: -0.06466388702392578
Batch 50/64 loss: -0.0702177882194519
Batch 51/64 loss: -0.10696804523468018
Batch 52/64 loss: -0.11369562149047852
Batch 53/64 loss: -0.1069219708442688
Batch 54/64 loss: -0.08674222230911255
Batch 55/64 loss: -0.11676472425460815
Batch 56/64 loss: -0.11155682802200317
Batch 57/64 loss: -0.09639191627502441
Batch 58/64 loss: -0.10463589429855347
Batch 59/64 loss: -0.08377152681350708
Batch 60/64 loss: -0.084564208984375
Batch 61/64 loss: -0.09188985824584961
Batch 62/64 loss: -0.10871601104736328
Batch 63/64 loss: -0.08092224597930908
Batch 64/64 loss: -0.11287277936935425
Epoch 271  Train loss: -0.09915997771655811  Val loss: 0.05845406702703627
Epoch 272
-------------------------------
Batch 1/64 loss: -0.08247321844100952
Batch 2/64 loss: -0.0946921706199646
Batch 3/64 loss: -0.11447936296463013
Batch 4/64 loss: -0.11879050731658936
Batch 5/64 loss: -0.11055666208267212
Batch 6/64 loss: -0.13240909576416016
Batch 7/64 loss: -0.08453088998794556
Batch 8/64 loss: -0.09275692701339722
Batch 9/64 loss: -0.09799641370773315
Batch 10/64 loss: -0.0713760256767273
Batch 11/64 loss: -0.07655227184295654
Batch 12/64 loss: -0.11632168292999268
Batch 13/64 loss: -0.13401484489440918
Batch 14/64 loss: -0.12288612127304077
Batch 15/64 loss: -0.08952075242996216
Batch 16/64 loss: -0.10705035924911499
Batch 17/64 loss: -0.10766971111297607
Batch 18/64 loss: -0.10357522964477539
Batch 19/64 loss: -0.11440712213516235
Batch 20/64 loss: -0.09555113315582275
Batch 21/64 loss: -0.10199463367462158
Batch 22/64 loss: -0.122722327709198
Batch 23/64 loss: -0.10737371444702148
Batch 24/64 loss: -0.06815850734710693
Batch 25/64 loss: -0.08610379695892334
Batch 26/64 loss: -0.07811897993087769
Batch 27/64 loss: -0.11907356977462769
Batch 28/64 loss: -0.1264699101448059
Batch 29/64 loss: -0.12879496812820435
Batch 30/64 loss: -0.08906495571136475
Batch 31/64 loss: -0.11131131649017334
Batch 32/64 loss: -0.10008221864700317
Batch 33/64 loss: -0.0784217119216919
Batch 34/64 loss: -0.08694487810134888
Batch 35/64 loss: -0.1254386305809021
Batch 36/64 loss: -0.08460402488708496
Batch 37/64 loss: -0.12756526470184326
Batch 38/64 loss: -0.09014332294464111
Batch 39/64 loss: -0.12975800037384033
Batch 40/64 loss: -0.10422539710998535
Batch 41/64 loss: -0.11883372068405151
Batch 42/64 loss: -0.09584373235702515
Batch 43/64 loss: -0.11553895473480225
Batch 44/64 loss: -0.12181568145751953
Batch 45/64 loss: -0.10647016763687134
Batch 46/64 loss: -0.09022355079650879
Batch 47/64 loss: -0.0553584098815918
Batch 48/64 loss: -0.09014880657196045
Batch 49/64 loss: -0.08448612689971924
Batch 50/64 loss: -0.13310593366622925
Batch 51/64 loss: -0.1061408519744873
Batch 52/64 loss: -0.10134732723236084
Batch 53/64 loss: -0.10490095615386963
Batch 54/64 loss: -0.11639916896820068
Batch 55/64 loss: -0.11451303958892822
Batch 56/64 loss: -0.09856176376342773
Batch 57/64 loss: -0.05483347177505493
Batch 58/64 loss: -0.11972963809967041
Batch 59/64 loss: -0.0920136570930481
Batch 60/64 loss: -0.13134264945983887
Batch 61/64 loss: -0.11658841371536255
Batch 62/64 loss: -0.058466434478759766
Batch 63/64 loss: -0.08374297618865967
Batch 64/64 loss: -0.07981956005096436
Epoch 272  Train loss: -0.10202746251050164  Val loss: 0.05786559344157321
Epoch 273
-------------------------------
Batch 1/64 loss: -0.09500938653945923
Batch 2/64 loss: -0.088733971118927
Batch 3/64 loss: -0.09935969114303589
Batch 4/64 loss: -0.09937959909439087
Batch 5/64 loss: -0.10831087827682495
Batch 6/64 loss: -0.10087573528289795
Batch 7/64 loss: -0.13641047477722168
Batch 8/64 loss: -0.12426447868347168
Batch 9/64 loss: -0.10513556003570557
Batch 10/64 loss: -0.10061770677566528
Batch 11/64 loss: -0.0573267936706543
Batch 12/64 loss: -0.09412646293640137
Batch 13/64 loss: -0.10060399770736694
Batch 14/64 loss: -0.1217222809791565
Batch 15/64 loss: -0.10846066474914551
Batch 16/64 loss: -0.099071204662323
Batch 17/64 loss: -0.09236764907836914
Batch 18/64 loss: -0.09427809715270996
Batch 19/64 loss: -0.09587496519088745
Batch 20/64 loss: -0.09401977062225342
Batch 21/64 loss: -0.12259232997894287
Batch 22/64 loss: -0.08280980587005615
Batch 23/64 loss: -0.09822022914886475
Batch 24/64 loss: -0.10151559114456177
Batch 25/64 loss: -0.0840001106262207
Batch 26/64 loss: -0.1260772943496704
Batch 27/64 loss: -0.0886991024017334
Batch 28/64 loss: -0.10849320888519287
Batch 29/64 loss: -0.09980744123458862
Batch 30/64 loss: -0.10519659519195557
Batch 31/64 loss: -0.11983132362365723
Batch 32/64 loss: -0.09486746788024902
Batch 33/64 loss: -0.11614632606506348
Batch 34/64 loss: -0.10595160722732544
Batch 35/64 loss: -0.12972253561019897
Batch 36/64 loss: -0.09400665760040283
Batch 37/64 loss: -0.10266774892807007
Batch 38/64 loss: -0.08272898197174072
Batch 39/64 loss: -0.12478750944137573
Batch 40/64 loss: -0.1334552764892578
Batch 41/64 loss: -0.1266230344772339
Batch 42/64 loss: -0.10869300365447998
Batch 43/64 loss: -0.09035539627075195
Batch 44/64 loss: -0.1116722822189331
Batch 45/64 loss: -0.09218531847000122
Batch 46/64 loss: -0.09608209133148193
Batch 47/64 loss: -0.08690667152404785
Batch 48/64 loss: -0.09139066934585571
Batch 49/64 loss: -0.09299618005752563
Batch 50/64 loss: -0.12388330698013306
Batch 51/64 loss: -0.12088227272033691
Batch 52/64 loss: -0.07824259996414185
Batch 53/64 loss: -0.07589221000671387
Batch 54/64 loss: -0.11196327209472656
Batch 55/64 loss: -0.12650656700134277
Batch 56/64 loss: -0.1204674243927002
Batch 57/64 loss: -0.08456182479858398
Batch 58/64 loss: -0.11282414197921753
Batch 59/64 loss: -0.10623341798782349
Batch 60/64 loss: -0.11802995204925537
Batch 61/64 loss: -0.08635872602462769
Batch 62/64 loss: -0.09892737865447998
Batch 63/64 loss: -0.08179992437362671
Batch 64/64 loss: -0.10004317760467529
Epoch 273  Train loss: -0.102839789203569  Val loss: 0.0557483498173481
Epoch 274
-------------------------------
Batch 1/64 loss: -0.07936739921569824
Batch 2/64 loss: -0.09818345308303833
Batch 3/64 loss: -0.08785909414291382
Batch 4/64 loss: -0.09383398294448853
Batch 5/64 loss: -0.12414860725402832
Batch 6/64 loss: -0.11320549249649048
Batch 7/64 loss: -0.11300903558731079
Batch 8/64 loss: -0.07919323444366455
Batch 9/64 loss: -0.1365073323249817
Batch 10/64 loss: -0.08664685487747192
Batch 11/64 loss: -0.07749229669570923
Batch 12/64 loss: -0.07468318939208984
Batch 13/64 loss: -0.14520955085754395
Batch 14/64 loss: -0.0980536937713623
Batch 15/64 loss: -0.08844941854476929
Batch 16/64 loss: -0.0979306697845459
Batch 17/64 loss: -0.1028294563293457
Batch 18/64 loss: -0.1296520233154297
Batch 19/64 loss: -0.11182427406311035
Batch 20/64 loss: -0.10242235660552979
Batch 21/64 loss: -0.08547115325927734
Batch 22/64 loss: -0.09358823299407959
Batch 23/64 loss: -0.06284135580062866
Batch 24/64 loss: -0.09079742431640625
Batch 25/64 loss: -0.12538939714431763
Batch 26/64 loss: -0.09818196296691895
Batch 27/64 loss: -0.11345374584197998
Batch 28/64 loss: -0.1265401840209961
Batch 29/64 loss: -0.09814274311065674
Batch 30/64 loss: -0.08546465635299683
Batch 31/64 loss: -0.1082758903503418
Batch 32/64 loss: -0.10970890522003174
Batch 33/64 loss: -0.10743677616119385
Batch 34/64 loss: -0.09876656532287598
Batch 35/64 loss: -0.11033940315246582
Batch 36/64 loss: -0.11986851692199707
Batch 37/64 loss: -0.040062785148620605
Batch 38/64 loss: -0.11343586444854736
Batch 39/64 loss: -0.09784924983978271
Batch 40/64 loss: -0.12004661560058594
Batch 41/64 loss: -0.11577796936035156
Batch 42/64 loss: -0.07120716571807861
Batch 43/64 loss: -0.1076134443283081
Batch 44/64 loss: -0.12511789798736572
Batch 45/64 loss: -0.11209475994110107
Batch 46/64 loss: -0.08198505640029907
Batch 47/64 loss: -0.1226959228515625
Batch 48/64 loss: -0.11748337745666504
Batch 49/64 loss: -0.08305478096008301
Batch 50/64 loss: -0.09325176477432251
Batch 51/64 loss: -0.10613322257995605
Batch 52/64 loss: -0.09517443180084229
Batch 53/64 loss: -0.09020870923995972
Batch 54/64 loss: -0.1198241114616394
Batch 55/64 loss: -0.11157870292663574
Batch 56/64 loss: -0.0851125717163086
Batch 57/64 loss: -0.12398844957351685
Batch 58/64 loss: -0.11174166202545166
Batch 59/64 loss: -0.04898643493652344
Batch 60/64 loss: -0.10079801082611084
Batch 61/64 loss: -0.08208304643630981
Batch 62/64 loss: -0.10875052213668823
Batch 63/64 loss: -0.0901867151260376
Batch 64/64 loss: -0.11926001310348511
Epoch 274  Train loss: -0.10102676994660321  Val loss: 0.0591671362365644
Epoch 275
-------------------------------
Batch 1/64 loss: -0.13069814443588257
Batch 2/64 loss: -0.11715465784072876
Batch 3/64 loss: -0.07639557123184204
Batch 4/64 loss: -0.06411898136138916
Batch 5/64 loss: -0.11107945442199707
Batch 6/64 loss: -0.10064345598220825
Batch 7/64 loss: -0.10341858863830566
Batch 8/64 loss: -0.09631472826004028
Batch 9/64 loss: -0.09704303741455078
Batch 10/64 loss: -0.08219248056411743
Batch 11/64 loss: -0.07754015922546387
Batch 12/64 loss: -0.13968592882156372
Batch 13/64 loss: -0.1000363826751709
Batch 14/64 loss: -0.09410864114761353
Batch 15/64 loss: -0.12997907400131226
Batch 16/64 loss: -0.07061541080474854
Batch 17/64 loss: -0.09219413995742798
Batch 18/64 loss: -0.09399646520614624
Batch 19/64 loss: -0.11144775152206421
Batch 20/64 loss: -0.0876774787902832
Batch 21/64 loss: -0.1332598328590393
Batch 22/64 loss: -0.12333405017852783
Batch 23/64 loss: -0.10587430000305176
Batch 24/64 loss: -0.11005347967147827
Batch 25/64 loss: -0.11262243986129761
Batch 26/64 loss: -0.10306882858276367
Batch 27/64 loss: -0.07883000373840332
Batch 28/64 loss: -0.09994858503341675
Batch 29/64 loss: -0.11752992868423462
Batch 30/64 loss: -0.12089806795120239
Batch 31/64 loss: -0.11463266611099243
Batch 32/64 loss: -0.11490923166275024
Batch 33/64 loss: -0.1402462124824524
Batch 34/64 loss: -0.09519648551940918
Batch 35/64 loss: -0.12148195505142212
Batch 36/64 loss: -0.12577468156814575
Batch 37/64 loss: -0.08233743906021118
Batch 38/64 loss: -0.07470792531967163
Batch 39/64 loss: -0.11201024055480957
Batch 40/64 loss: -0.10910457372665405
Batch 41/64 loss: -0.1046871542930603
Batch 42/64 loss: -0.08131659030914307
Batch 43/64 loss: -0.09388256072998047
Batch 44/64 loss: -0.12249958515167236
Batch 45/64 loss: -0.095278799533844
Batch 46/64 loss: -0.09557557106018066
Batch 47/64 loss: -0.11334234476089478
Batch 48/64 loss: -0.11469638347625732
Batch 49/64 loss: -0.1002265214920044
Batch 50/64 loss: -0.09965908527374268
Batch 51/64 loss: -0.10708850622177124
Batch 52/64 loss: -0.11482185125350952
Batch 53/64 loss: -0.08827269077301025
Batch 54/64 loss: -0.08267664909362793
Batch 55/64 loss: -0.062355995178222656
Batch 56/64 loss: -0.07281112670898438
Batch 57/64 loss: -0.07795476913452148
Batch 58/64 loss: -0.12027275562286377
Batch 59/64 loss: -0.12444460391998291
Batch 60/64 loss: -0.08816313743591309
Batch 61/64 loss: -0.0964934229850769
Batch 62/64 loss: -0.10980165004730225
Batch 63/64 loss: -0.08629441261291504
Batch 64/64 loss: -0.11330509185791016
Epoch 275  Train loss: -0.10211382660211302  Val loss: 0.05665223315819023
Epoch 276
-------------------------------
Batch 1/64 loss: -0.11633282899856567
Batch 2/64 loss: -0.07935631275177002
Batch 3/64 loss: -0.11120867729187012
Batch 4/64 loss: -0.07683062553405762
Batch 5/64 loss: -0.13159149885177612
Batch 6/64 loss: -0.10176658630371094
Batch 7/64 loss: -0.09821587800979614
Batch 8/64 loss: -0.08731245994567871
Batch 9/64 loss: -0.1266106367111206
Batch 10/64 loss: -0.12939411401748657
Batch 11/64 loss: -0.10486418008804321
Batch 12/64 loss: -0.13405293226242065
Batch 13/64 loss: -0.08940821886062622
Batch 14/64 loss: -0.11782288551330566
Batch 15/64 loss: -0.10457223653793335
Batch 16/64 loss: -0.10429978370666504
Batch 17/64 loss: -0.09599006175994873
Batch 18/64 loss: -0.07499146461486816
Batch 19/64 loss: -0.10944414138793945
Batch 20/64 loss: -0.09920477867126465
Batch 21/64 loss: -0.07332515716552734
Batch 22/64 loss: -0.09022784233093262
Batch 23/64 loss: -0.07317280769348145
Batch 24/64 loss: -0.11175757646560669
Batch 25/64 loss: -0.08525067567825317
Batch 26/64 loss: -0.11380136013031006
Batch 27/64 loss: -0.09887135028839111
Batch 28/64 loss: -0.10152798891067505
Batch 29/64 loss: -0.11810570955276489
Batch 30/64 loss: -0.09459877014160156
Batch 31/64 loss: -0.10049021244049072
Batch 32/64 loss: -0.12515735626220703
Batch 33/64 loss: -0.13032948970794678
Batch 34/64 loss: -0.09176146984100342
Batch 35/64 loss: -0.12150704860687256
Batch 36/64 loss: -0.11221957206726074
Batch 37/64 loss: -0.10344147682189941
Batch 38/64 loss: -0.10098350048065186
Batch 39/64 loss: -0.12368398904800415
Batch 40/64 loss: -0.0758783221244812
Batch 41/64 loss: -0.08479189872741699
Batch 42/64 loss: -0.0987468957901001
Batch 43/64 loss: -0.14122414588928223
Batch 44/64 loss: -0.07161080837249756
Batch 45/64 loss: -0.11037248373031616
Batch 46/64 loss: -0.1204870343208313
Batch 47/64 loss: -0.08725810050964355
Batch 48/64 loss: -0.09649509191513062
Batch 49/64 loss: -0.11325931549072266
Batch 50/64 loss: -0.12450969219207764
Batch 51/64 loss: -0.1095808744430542
Batch 52/64 loss: -0.09821927547454834
Batch 53/64 loss: -0.09419924020767212
Batch 54/64 loss: -0.0871114730834961
Batch 55/64 loss: -0.1334400177001953
Batch 56/64 loss: -0.08652275800704956
Batch 57/64 loss: -0.08473420143127441
Batch 58/64 loss: -0.06951653957366943
Batch 59/64 loss: -0.08291482925415039
Batch 60/64 loss: -0.0551304817199707
Batch 61/64 loss: -0.11182475090026855
Batch 62/64 loss: -0.08692795038223267
Batch 63/64 loss: -0.06698983907699585
Batch 64/64 loss: -0.11151725053787231
Epoch 276  Train loss: -0.10100184492036408  Val loss: 0.061364591326500545
Epoch 277
-------------------------------
Batch 1/64 loss: -0.07719635963439941
Batch 2/64 loss: -0.09361803531646729
Batch 3/64 loss: -0.12698322534561157
Batch 4/64 loss: -0.1080482006072998
Batch 5/64 loss: -0.06849533319473267
Batch 6/64 loss: -0.10442167520523071
Batch 7/64 loss: -0.07985562086105347
Batch 8/64 loss: -0.10194963216781616
Batch 9/64 loss: -0.09235537052154541
Batch 10/64 loss: -0.13295912742614746
Batch 11/64 loss: -0.11068755388259888
Batch 12/64 loss: -0.12646299600601196
Batch 13/64 loss: -0.09457671642303467
Batch 14/64 loss: -0.10388720035552979
Batch 15/64 loss: -0.09509152173995972
Batch 16/64 loss: -0.11355459690093994
Batch 17/64 loss: -0.1294689178466797
Batch 18/64 loss: -0.10811340808868408
Batch 19/64 loss: -0.11020225286483765
Batch 20/64 loss: -0.07922285795211792
Batch 21/64 loss: -0.08534950017929077
Batch 22/64 loss: -0.09175682067871094
Batch 23/64 loss: -0.10417723655700684
Batch 24/64 loss: -0.13203495740890503
Batch 25/64 loss: -0.09444499015808105
Batch 26/64 loss: -0.12528842687606812
Batch 27/64 loss: -0.12076681852340698
Batch 28/64 loss: -0.0869760513305664
Batch 29/64 loss: -0.11457687616348267
Batch 30/64 loss: -0.13517087697982788
Batch 31/64 loss: -0.09402120113372803
Batch 32/64 loss: -0.09108567237854004
Batch 33/64 loss: -0.11419105529785156
Batch 34/64 loss: -0.12691694498062134
Batch 35/64 loss: -0.0960456132888794
Batch 36/64 loss: -0.11200928688049316
Batch 37/64 loss: -0.12830740213394165
Batch 38/64 loss: -0.13218706846237183
Batch 39/64 loss: -0.13370251655578613
Batch 40/64 loss: -0.10551297664642334
Batch 41/64 loss: -0.07808393239974976
Batch 42/64 loss: -0.10238313674926758
Batch 43/64 loss: -0.12229061126708984
Batch 44/64 loss: -0.12374448776245117
Batch 45/64 loss: -0.07833915948867798
Batch 46/64 loss: -0.08387511968612671
Batch 47/64 loss: -0.12578415870666504
Batch 48/64 loss: -0.09898924827575684
Batch 49/64 loss: -0.08200526237487793
Batch 50/64 loss: -0.11546415090560913
Batch 51/64 loss: -0.14111876487731934
Batch 52/64 loss: -0.113719642162323
Batch 53/64 loss: -0.10062247514724731
Batch 54/64 loss: -0.08441776037216187
Batch 55/64 loss: -0.09838533401489258
Batch 56/64 loss: -0.11539566516876221
Batch 57/64 loss: -0.09532105922698975
Batch 58/64 loss: -0.11740249395370483
Batch 59/64 loss: -0.0962526798248291
Batch 60/64 loss: -0.10880124568939209
Batch 61/64 loss: -0.11215382814407349
Batch 62/64 loss: -0.11766666173934937
Batch 63/64 loss: -0.05767327547073364
Batch 64/64 loss: -0.0752837061882019
Epoch 277  Train loss: -0.10522393455692367  Val loss: 0.05731433875781974
Epoch 278
-------------------------------
Batch 1/64 loss: -0.10224831104278564
Batch 2/64 loss: -0.08496761322021484
Batch 3/64 loss: -0.11851125955581665
Batch 4/64 loss: -0.11194711923599243
Batch 5/64 loss: -0.06649631261825562
Batch 6/64 loss: -0.10516184568405151
Batch 7/64 loss: -0.05948936939239502
Batch 8/64 loss: -0.12783366441726685
Batch 9/64 loss: -0.12525802850723267
Batch 10/64 loss: -0.11627030372619629
Batch 11/64 loss: -0.11951792240142822
Batch 12/64 loss: -0.10253655910491943
Batch 13/64 loss: -0.12420821189880371
Batch 14/64 loss: -0.1253567337989807
Batch 15/64 loss: -0.1392863392829895
Batch 16/64 loss: -0.12435084581375122
Batch 17/64 loss: -0.10403674840927124
Batch 18/64 loss: -0.1323297619819641
Batch 19/64 loss: -0.10756617784500122
Batch 20/64 loss: -0.10886293649673462
Batch 21/64 loss: -0.10423564910888672
Batch 22/64 loss: -0.0816698670387268
Batch 23/64 loss: -0.08952867984771729
Batch 24/64 loss: -0.10361915826797485
Batch 25/64 loss: -0.1162460446357727
Batch 26/64 loss: -0.09853255748748779
Batch 27/64 loss: -0.08902180194854736
Batch 28/64 loss: -0.0940021276473999
Batch 29/64 loss: -0.10338830947875977
Batch 30/64 loss: -0.12753397226333618
Batch 31/64 loss: -0.08033400774002075
Batch 32/64 loss: -0.11561059951782227
Batch 33/64 loss: -0.11471247673034668
Batch 34/64 loss: -0.08085501194000244
Batch 35/64 loss: -0.11388373374938965
Batch 36/64 loss: -0.10145902633666992
Batch 37/64 loss: -0.10071444511413574
Batch 38/64 loss: -0.11579316854476929
Batch 39/64 loss: -0.10682737827301025
Batch 40/64 loss: -0.12095987796783447
Batch 41/64 loss: -0.12530499696731567
Batch 42/64 loss: -0.05671960115432739
Batch 43/64 loss: -0.092323899269104
Batch 44/64 loss: -0.09618103504180908
Batch 45/64 loss: -0.09046775102615356
Batch 46/64 loss: -0.12666314840316772
Batch 47/64 loss: -0.08026957511901855
Batch 48/64 loss: -0.09165900945663452
Batch 49/64 loss: -0.14136934280395508
Batch 50/64 loss: -0.07247215509414673
Batch 51/64 loss: -0.11057186126708984
Batch 52/64 loss: -0.10797613859176636
Batch 53/64 loss: -0.11498451232910156
Batch 54/64 loss: -0.06772440671920776
Batch 55/64 loss: -0.10229206085205078
Batch 56/64 loss: -0.11828798055648804
Batch 57/64 loss: -0.10249704122543335
Batch 58/64 loss: -0.11936438083648682
Batch 59/64 loss: -0.09799158573150635
Batch 60/64 loss: -0.08326148986816406
Batch 61/64 loss: -0.11946666240692139
Batch 62/64 loss: -0.11573988199234009
Batch 63/64 loss: -0.08545917272567749
Batch 64/64 loss: -0.09685981273651123
Epoch 278  Train loss: -0.10442127854216332  Val loss: 0.05165992446781434
Epoch 279
-------------------------------
Batch 1/64 loss: -0.05200803279876709
Batch 2/64 loss: -0.10780727863311768
Batch 3/64 loss: -0.10998964309692383
Batch 4/64 loss: -0.13161569833755493
Batch 5/64 loss: -0.1423509120941162
Batch 6/64 loss: -0.10680806636810303
Batch 7/64 loss: -0.08294022083282471
Batch 8/64 loss: -0.10799098014831543
Batch 9/64 loss: -0.09246313571929932
Batch 10/64 loss: -0.12554329633712769
Batch 11/64 loss: -0.10666608810424805
Batch 12/64 loss: -0.11953651905059814
Batch 13/64 loss: -0.10199892520904541
Batch 14/64 loss: -0.10903578996658325
Batch 15/64 loss: -0.13894891738891602
Batch 16/64 loss: -0.08391374349594116
Batch 17/64 loss: -0.12329971790313721
Batch 18/64 loss: -0.10383850336074829
Batch 19/64 loss: -0.08916127681732178
Batch 20/64 loss: -0.09150964021682739
Batch 21/64 loss: -0.11240887641906738
Batch 22/64 loss: -0.13374918699264526
Batch 23/64 loss: -0.13046163320541382
Batch 24/64 loss: -0.0795394778251648
Batch 25/64 loss: -0.14568698406219482
Batch 26/64 loss: -0.09277445077896118
Batch 27/64 loss: -0.13853895664215088
Batch 28/64 loss: -0.1648777723312378
Batch 29/64 loss: -0.08333015441894531
Batch 30/64 loss: -0.11308270692825317
Batch 31/64 loss: -0.088889479637146
Batch 32/64 loss: -0.11216771602630615
Batch 33/64 loss: -0.0760297179222107
Batch 34/64 loss: -0.09874981641769409
Batch 35/64 loss: -0.1028711199760437
Batch 36/64 loss: -0.10785233974456787
Batch 37/64 loss: -0.11325418949127197
Batch 38/64 loss: -0.0947766900062561
Batch 39/64 loss: -0.0756676197052002
Batch 40/64 loss: -0.10486280918121338
Batch 41/64 loss: -0.1332271695137024
Batch 42/64 loss: -0.11665666103363037
Batch 43/64 loss: -0.11949491500854492
Batch 44/64 loss: -0.1131332516670227
Batch 45/64 loss: -0.09601807594299316
Batch 46/64 loss: -0.07780754566192627
Batch 47/64 loss: -0.08811497688293457
Batch 48/64 loss: -0.12146592140197754
Batch 49/64 loss: -0.05866694450378418
Batch 50/64 loss: -0.10433053970336914
Batch 51/64 loss: -0.08751201629638672
Batch 52/64 loss: -0.08063197135925293
Batch 53/64 loss: -0.10644394159317017
Batch 54/64 loss: -0.10180521011352539
Batch 55/64 loss: -0.11822366714477539
Batch 56/64 loss: -0.08949226140975952
Batch 57/64 loss: -0.11746686697006226
Batch 58/64 loss: -0.06502676010131836
Batch 59/64 loss: -0.13346648216247559
Batch 60/64 loss: -0.12067645788192749
Batch 61/64 loss: -0.09204626083374023
Batch 62/64 loss: -0.08828097581863403
Batch 63/64 loss: -0.1039968729019165
Batch 64/64 loss: -0.06397241353988647
Epoch 279  Train loss: -0.10476804924946205  Val loss: 0.056892739333647635
Epoch 280
-------------------------------
Batch 1/64 loss: -0.10596215724945068
Batch 2/64 loss: -0.12018513679504395
Batch 3/64 loss: -0.1317882537841797
Batch 4/64 loss: -0.10991322994232178
Batch 5/64 loss: -0.11104822158813477
Batch 6/64 loss: -0.13734257221221924
Batch 7/64 loss: -0.10704648494720459
Batch 8/64 loss: -0.10176694393157959
Batch 9/64 loss: -0.0902053713798523
Batch 10/64 loss: -0.1340557336807251
Batch 11/64 loss: -0.11385416984558105
Batch 12/64 loss: -0.1010773777961731
Batch 13/64 loss: -0.10837572813034058
Batch 14/64 loss: -0.11145138740539551
Batch 15/64 loss: -0.1034165620803833
Batch 16/64 loss: -0.06789737939834595
Batch 17/64 loss: -0.12118947505950928
Batch 18/64 loss: -0.07837188243865967
Batch 19/64 loss: -0.12325787544250488
Batch 20/64 loss: -0.07591211795806885
Batch 21/64 loss: -0.08144265413284302
Batch 22/64 loss: -0.11525243520736694
Batch 23/64 loss: -0.1183016300201416
Batch 24/64 loss: -0.09659618139266968
Batch 25/64 loss: -0.06473875045776367
Batch 26/64 loss: -0.08679360151290894
Batch 27/64 loss: -0.09538096189498901
Batch 28/64 loss: -0.07164043188095093
Batch 29/64 loss: -0.08922451734542847
Batch 30/64 loss: -0.0742419958114624
Batch 31/64 loss: -0.1353898048400879
Batch 32/64 loss: -0.10233008861541748
Batch 33/64 loss: -0.06384164094924927
Batch 34/64 loss: -0.09789848327636719
Batch 35/64 loss: -0.11153149604797363
Batch 36/64 loss: -0.09650546312332153
Batch 37/64 loss: -0.1248508095741272
Batch 38/64 loss: -0.07854068279266357
Batch 39/64 loss: -0.10354113578796387
Batch 40/64 loss: -0.13473999500274658
Batch 41/64 loss: -0.1158301830291748
Batch 42/64 loss: -0.08790618181228638
Batch 43/64 loss: -0.04117560386657715
Batch 44/64 loss: -0.11961835622787476
Batch 45/64 loss: -0.09741431474685669
Batch 46/64 loss: -0.08432990312576294
Batch 47/64 loss: -0.09881848096847534
Batch 48/64 loss: -0.1078903079032898
Batch 49/64 loss: -0.08711117506027222
Batch 50/64 loss: -0.09221541881561279
Batch 51/64 loss: -0.09219074249267578
Batch 52/64 loss: -0.10511893033981323
Batch 53/64 loss: -0.11274218559265137
Batch 54/64 loss: -0.1260020136833191
Batch 55/64 loss: -0.07927775382995605
Batch 56/64 loss: -0.08596628904342651
Batch 57/64 loss: -0.11488449573516846
Batch 58/64 loss: -0.08825278282165527
Batch 59/64 loss: -0.12063270807266235
Batch 60/64 loss: -0.12385725975036621
Batch 61/64 loss: -0.07224071025848389
Batch 62/64 loss: -0.11243033409118652
Batch 63/64 loss: -0.10608261823654175
Batch 64/64 loss: -0.10524201393127441
Epoch 280  Train loss: -0.10114229146172019  Val loss: 0.06700215716542247
Epoch 281
-------------------------------
Batch 1/64 loss: -0.09972262382507324
Batch 2/64 loss: -0.13458693027496338
Batch 3/64 loss: -0.11646050214767456
Batch 4/64 loss: -0.13000869750976562
Batch 5/64 loss: -0.11392807960510254
Batch 6/64 loss: -0.08497250080108643
Batch 7/64 loss: -0.12616431713104248
Batch 8/64 loss: -0.10357880592346191
Batch 9/64 loss: -0.1187063455581665
Batch 10/64 loss: -0.0903158187866211
Batch 11/64 loss: -0.13533788919448853
Batch 12/64 loss: -0.11561059951782227
Batch 13/64 loss: -0.10750365257263184
Batch 14/64 loss: -0.07692670822143555
Batch 15/64 loss: -0.07967519760131836
Batch 16/64 loss: -0.12464404106140137
Batch 17/64 loss: -0.11325407028198242
Batch 18/64 loss: -0.1331273317337036
Batch 19/64 loss: -0.11484235525131226
Batch 20/64 loss: -0.06822776794433594
Batch 21/64 loss: -0.12949621677398682
Batch 22/64 loss: -0.1030159592628479
Batch 23/64 loss: -0.10700118541717529
Batch 24/64 loss: -0.09120500087738037
Batch 25/64 loss: -0.12060272693634033
Batch 26/64 loss: -0.10681819915771484
Batch 27/64 loss: -0.09670877456665039
Batch 28/64 loss: -0.09858685731887817
Batch 29/64 loss: -0.10431158542633057
Batch 30/64 loss: -0.11051380634307861
Batch 31/64 loss: -0.09128022193908691
Batch 32/64 loss: -0.12008905410766602
Batch 33/64 loss: -0.07170939445495605
Batch 34/64 loss: -0.09562426805496216
Batch 35/64 loss: -0.09461057186126709
Batch 36/64 loss: -0.13340628147125244
Batch 37/64 loss: -0.11294317245483398
Batch 38/64 loss: -0.08495444059371948
Batch 39/64 loss: -0.11194783449172974
Batch 40/64 loss: -0.13125520944595337
Batch 41/64 loss: -0.10834664106369019
Batch 42/64 loss: -0.076515793800354
Batch 43/64 loss: -0.1319698691368103
Batch 44/64 loss: -0.10222131013870239
Batch 45/64 loss: -0.1437361240386963
Batch 46/64 loss: -0.02215445041656494
Batch 47/64 loss: -0.10596007108688354
Batch 48/64 loss: -0.09601867198944092
Batch 49/64 loss: -0.11838763952255249
Batch 50/64 loss: -0.04849898815155029
Batch 51/64 loss: -0.10703474283218384
Batch 52/64 loss: -0.1353505253791809
Batch 53/64 loss: -0.07282030582427979
Batch 54/64 loss: -0.10859382152557373
Batch 55/64 loss: -0.07270652055740356
Batch 56/64 loss: -0.10104984045028687
Batch 57/64 loss: -0.0863654613494873
Batch 58/64 loss: -0.04163002967834473
Batch 59/64 loss: -0.07811450958251953
Batch 60/64 loss: -0.12160027027130127
Batch 61/64 loss: -0.10035812854766846
Batch 62/64 loss: -0.09685587882995605
Batch 63/64 loss: -0.12410575151443481
Batch 64/64 loss: -0.08422482013702393
Epoch 281  Train loss: -0.10301551304611506  Val loss: 0.05939423095729343
Epoch 282
-------------------------------
Batch 1/64 loss: -0.11570119857788086
Batch 2/64 loss: -0.09491932392120361
Batch 3/64 loss: -0.10106682777404785
Batch 4/64 loss: -0.11991703510284424
Batch 5/64 loss: -0.11287105083465576
Batch 6/64 loss: -0.1157497763633728
Batch 7/64 loss: -0.08616101741790771
Batch 8/64 loss: -0.12689626216888428
Batch 9/64 loss: -0.0933678150177002
Batch 10/64 loss: -0.12015259265899658
Batch 11/64 loss: -0.10942113399505615
Batch 12/64 loss: -0.09152388572692871
Batch 13/64 loss: -0.06307637691497803
Batch 14/64 loss: -0.12367182970046997
Batch 15/64 loss: -0.06530022621154785
Batch 16/64 loss: -0.1279180645942688
Batch 17/64 loss: -0.12451457977294922
Batch 18/64 loss: -0.10088557004928589
Batch 19/64 loss: -0.12446033954620361
Batch 20/64 loss: -0.1283717155456543
Batch 21/64 loss: -0.06227743625640869
Batch 22/64 loss: -0.08548533916473389
Batch 23/64 loss: -0.12782371044158936
Batch 24/64 loss: -0.07976341247558594
Batch 25/64 loss: -0.13536441326141357
Batch 26/64 loss: -0.1462687849998474
Batch 27/64 loss: -0.12753915786743164
Batch 28/64 loss: -0.10419958829879761
Batch 29/64 loss: -0.1373687982559204
Batch 30/64 loss: -0.08357548713684082
Batch 31/64 loss: -0.09544700384140015
Batch 32/64 loss: -0.11015307903289795
Batch 33/64 loss: -0.1025094985961914
Batch 34/64 loss: -0.13139992952346802
Batch 35/64 loss: -0.127210795879364
Batch 36/64 loss: -0.12744879722595215
Batch 37/64 loss: -0.12296760082244873
Batch 38/64 loss: -0.07055217027664185
Batch 39/64 loss: -0.13353705406188965
Batch 40/64 loss: -0.1112661361694336
Batch 41/64 loss: -0.13009965419769287
Batch 42/64 loss: -0.10999339818954468
Batch 43/64 loss: -0.13121920824050903
Batch 44/64 loss: -0.12565505504608154
Batch 45/64 loss: -0.12583208084106445
Batch 46/64 loss: -0.08971905708312988
Batch 47/64 loss: -0.06893187761306763
Batch 48/64 loss: -0.11529695987701416
Batch 49/64 loss: -0.0847635269165039
Batch 50/64 loss: -0.10741531848907471
Batch 51/64 loss: -0.10922688245773315
Batch 52/64 loss: -0.11130732297897339
Batch 53/64 loss: -0.09667700529098511
Batch 54/64 loss: -0.09779256582260132
Batch 55/64 loss: -0.087807297706604
Batch 56/64 loss: -0.13841378688812256
Batch 57/64 loss: -0.12077903747558594
Batch 58/64 loss: -0.11061245203018188
Batch 59/64 loss: -0.13800030946731567
Batch 60/64 loss: -0.10666030645370483
Batch 61/64 loss: -0.09460985660552979
Batch 62/64 loss: -0.09544426202774048
Batch 63/64 loss: -0.10516834259033203
Batch 64/64 loss: -0.11179918050765991
Epoch 282  Train loss: -0.1090726237671048  Val loss: 0.055009136494901995
Epoch 283
-------------------------------
Batch 1/64 loss: -0.11569255590438843
Batch 2/64 loss: -0.13711684942245483
Batch 3/64 loss: -0.12414282560348511
Batch 4/64 loss: -0.10088455677032471
Batch 5/64 loss: -0.1247178316116333
Batch 6/64 loss: -0.0976945161819458
Batch 7/64 loss: -0.09998786449432373
Batch 8/64 loss: -0.130343496799469
Batch 9/64 loss: -0.11285203695297241
Batch 10/64 loss: -0.10894006490707397
Batch 11/64 loss: -0.08536452054977417
Batch 12/64 loss: -0.11174023151397705
Batch 13/64 loss: -0.12913882732391357
Batch 14/64 loss: -0.09424662590026855
Batch 15/64 loss: -0.08342963457107544
Batch 16/64 loss: -0.12147104740142822
Batch 17/64 loss: -0.13075262308120728
Batch 18/64 loss: -0.13592660427093506
Batch 19/64 loss: -0.12476480007171631
Batch 20/64 loss: -0.05329340696334839
Batch 21/64 loss: -0.11386477947235107
Batch 22/64 loss: -0.10797673463821411
Batch 23/64 loss: -0.12232261896133423
Batch 24/64 loss: -0.10984182357788086
Batch 25/64 loss: -0.11657577753067017
Batch 26/64 loss: -0.11504679918289185
Batch 27/64 loss: -0.12541651725769043
Batch 28/64 loss: -0.12306559085845947
Batch 29/64 loss: -0.0979468822479248
Batch 30/64 loss: -0.1353670358657837
Batch 31/64 loss: -0.10892277956008911
Batch 32/64 loss: -0.1189347505569458
Batch 33/64 loss: -0.09134137630462646
Batch 34/64 loss: -0.10375165939331055
Batch 35/64 loss: -0.0630272626876831
Batch 36/64 loss: -0.08743458986282349
Batch 37/64 loss: -0.13246935606002808
Batch 38/64 loss: -0.1155129075050354
Batch 39/64 loss: -0.14190208911895752
Batch 40/64 loss: -0.0955701470375061
Batch 41/64 loss: -0.10622608661651611
Batch 42/64 loss: -0.08963710069656372
Batch 43/64 loss: -0.12620657682418823
Batch 44/64 loss: -0.11570161581039429
Batch 45/64 loss: -0.09327411651611328
Batch 46/64 loss: -0.09109252691268921
Batch 47/64 loss: -0.13839399814605713
Batch 48/64 loss: -0.13862591981887817
Batch 49/64 loss: -0.11856019496917725
Batch 50/64 loss: -0.12261146306991577
Batch 51/64 loss: -0.09182322025299072
Batch 52/64 loss: -0.12541639804840088
Batch 53/64 loss: -0.0615193247795105
Batch 54/64 loss: -0.10111755132675171
Batch 55/64 loss: -0.11169278621673584
Batch 56/64 loss: -0.08317071199417114
Batch 57/64 loss: -0.07757741212844849
Batch 58/64 loss: -0.11796444654464722
Batch 59/64 loss: -0.10328423976898193
Batch 60/64 loss: -0.07678711414337158
Batch 61/64 loss: -0.10586297512054443
Batch 62/64 loss: -0.10133582353591919
Batch 63/64 loss: -0.057190537452697754
Batch 64/64 loss: -0.05375725030899048
Epoch 283  Train loss: -0.10735972512002084  Val loss: 0.05957372778469754
Epoch 284
-------------------------------
Batch 1/64 loss: -0.14513355493545532
Batch 2/64 loss: -0.11712157726287842
Batch 3/64 loss: -0.05073809623718262
Batch 4/64 loss: -0.0378383994102478
Batch 5/64 loss: -0.1246914267539978
Batch 6/64 loss: -0.09890139102935791
Batch 7/64 loss: -0.1387474536895752
Batch 8/64 loss: -0.1218796968460083
Batch 9/64 loss: -0.11208415031433105
Batch 10/64 loss: -0.12840771675109863
Batch 11/64 loss: -0.11674147844314575
Batch 12/64 loss: -0.11453109979629517
Batch 13/64 loss: -0.08374202251434326
Batch 14/64 loss: -0.12016409635543823
Batch 15/64 loss: -0.1224743127822876
Batch 16/64 loss: -0.11680078506469727
Batch 17/64 loss: -0.08666890859603882
Batch 18/64 loss: -0.08757448196411133
Batch 19/64 loss: -0.09339386224746704
Batch 20/64 loss: -0.11751353740692139
Batch 21/64 loss: -0.11881154775619507
Batch 22/64 loss: -0.036669909954071045
Batch 23/64 loss: -0.09613573551177979
Batch 24/64 loss: -0.12883055210113525
Batch 25/64 loss: -0.10243040323257446
Batch 26/64 loss: -0.11442971229553223
Batch 27/64 loss: -0.10188066959381104
Batch 28/64 loss: -0.12046587467193604
Batch 29/64 loss: -0.1368323564529419
Batch 30/64 loss: -0.12218689918518066
Batch 31/64 loss: -0.1033315658569336
Batch 32/64 loss: -0.07324516773223877
Batch 33/64 loss: -0.11848866939544678
Batch 34/64 loss: -0.12586474418640137
Batch 35/64 loss: -0.15436053276062012
Batch 36/64 loss: -0.09742701053619385
Batch 37/64 loss: -0.09967130422592163
Batch 38/64 loss: -0.08857494592666626
Batch 39/64 loss: -0.10802805423736572
Batch 40/64 loss: -0.07009589672088623
Batch 41/64 loss: -0.11191076040267944
Batch 42/64 loss: -0.09278404712677002
Batch 43/64 loss: -0.12507951259613037
Batch 44/64 loss: -0.06750082969665527
Batch 45/64 loss: -0.1455485224723816
Batch 46/64 loss: -0.10839468240737915
Batch 47/64 loss: -0.10949498414993286
Batch 48/64 loss: -0.12490111589431763
Batch 49/64 loss: -0.13298308849334717
Batch 50/64 loss: -0.12335419654846191
Batch 51/64 loss: -0.10718095302581787
Batch 52/64 loss: -0.10697269439697266
Batch 53/64 loss: -0.09115064144134521
Batch 54/64 loss: -0.11338591575622559
Batch 55/64 loss: -0.12179982662200928
Batch 56/64 loss: -0.13501763343811035
Batch 57/64 loss: -0.10016453266143799
Batch 58/64 loss: -0.12180626392364502
Batch 59/64 loss: -0.12659072875976562
Batch 60/64 loss: -0.05688166618347168
Batch 61/64 loss: -0.09428954124450684
Batch 62/64 loss: -0.10647237300872803
Batch 63/64 loss: -0.1040046215057373
Batch 64/64 loss: -0.11226147413253784
Epoch 284  Train loss: -0.10768274255827362  Val loss: 0.05391069540043467
Epoch 285
-------------------------------
Batch 1/64 loss: -0.10654479265213013
Batch 2/64 loss: -0.1421724557876587
Batch 3/64 loss: -0.0627368688583374
Batch 4/64 loss: -0.15738952159881592
Batch 5/64 loss: -0.13377171754837036
Batch 6/64 loss: -0.06827878952026367
Batch 7/64 loss: -0.13441401720046997
Batch 8/64 loss: -0.111846923828125
Batch 9/64 loss: -0.11306434869766235
Batch 10/64 loss: -0.12900710105895996
Batch 11/64 loss: -0.12186461687088013
Batch 12/64 loss: -0.10745054483413696
Batch 13/64 loss: -0.10616707801818848
Batch 14/64 loss: -0.12910538911819458
Batch 15/64 loss: -0.12436926364898682
Batch 16/64 loss: -0.10206681489944458
Batch 17/64 loss: -0.11029601097106934
Batch 18/64 loss: -0.07172352075576782
Batch 19/64 loss: -0.11457163095474243
Batch 20/64 loss: -0.0860946774482727
Batch 21/64 loss: -0.12138926982879639
Batch 22/64 loss: -0.12700998783111572
Batch 23/64 loss: -0.12055861949920654
Batch 24/64 loss: -0.1123511791229248
Batch 25/64 loss: -0.14341235160827637
Batch 26/64 loss: -0.07165801525115967
Batch 27/64 loss: -0.12195330858230591
Batch 28/64 loss: -0.1262461543083191
Batch 29/64 loss: -0.11346316337585449
Batch 30/64 loss: -0.10168725252151489
Batch 31/64 loss: -0.06237077713012695
Batch 32/64 loss: -0.09708452224731445
Batch 33/64 loss: -0.12009257078170776
Batch 34/64 loss: -0.12450647354125977
Batch 35/64 loss: -0.10928267240524292
Batch 36/64 loss: -0.12939226627349854
Batch 37/64 loss: -0.12814205884933472
Batch 38/64 loss: -0.13244938850402832
Batch 39/64 loss: -0.12629950046539307
Batch 40/64 loss: -0.10693079233169556
Batch 41/64 loss: -0.11288279294967651
Batch 42/64 loss: -0.10829687118530273
Batch 43/64 loss: -0.09300768375396729
Batch 44/64 loss: -0.12553274631500244
Batch 45/64 loss: -0.11848777532577515
Batch 46/64 loss: -0.05892443656921387
Batch 47/64 loss: -0.10322356224060059
Batch 48/64 loss: -0.11045175790786743
Batch 49/64 loss: -0.10486114025115967
Batch 50/64 loss: -0.05812150239944458
Batch 51/64 loss: -0.13066619634628296
Batch 52/64 loss: -0.10573679208755493
Batch 53/64 loss: -0.12610328197479248
Batch 54/64 loss: -0.08286494016647339
Batch 55/64 loss: -0.10323429107666016
Batch 56/64 loss: -0.0785786509513855
Batch 57/64 loss: -0.09624350070953369
Batch 58/64 loss: -0.07720845937728882
Batch 59/64 loss: -0.0951961874961853
Batch 60/64 loss: -0.09860432147979736
Batch 61/64 loss: -0.09989070892333984
Batch 62/64 loss: -0.08336508274078369
Batch 63/64 loss: -0.0728377103805542
Batch 64/64 loss: -0.10390818119049072
Epoch 285  Train loss: -0.1074740068585265  Val loss: 0.06028761384413414
Epoch 286
-------------------------------
Batch 1/64 loss: -0.10535699129104614
Batch 2/64 loss: -0.11578798294067383
Batch 3/64 loss: -0.11847597360610962
Batch 4/64 loss: -0.13227927684783936
Batch 5/64 loss: -0.10607129335403442
Batch 6/64 loss: -0.08909887075424194
Batch 7/64 loss: -0.13858294486999512
Batch 8/64 loss: -0.09342288970947266
Batch 9/64 loss: -0.12462031841278076
Batch 10/64 loss: -0.11652278900146484
Batch 11/64 loss: -0.09611624479293823
Batch 12/64 loss: -0.11002957820892334
Batch 13/64 loss: -0.07989442348480225
Batch 14/64 loss: -0.12138199806213379
Batch 15/64 loss: -0.13828879594802856
Batch 16/64 loss: -0.12316274642944336
Batch 17/64 loss: -0.11877685785293579
Batch 18/64 loss: -0.12029939889907837
Batch 19/64 loss: -0.07730436325073242
Batch 20/64 loss: -0.119387686252594
Batch 21/64 loss: -0.11209005117416382
Batch 22/64 loss: -0.1265193223953247
Batch 23/64 loss: -0.11021804809570312
Batch 24/64 loss: -0.11757826805114746
Batch 25/64 loss: -0.1236037015914917
Batch 26/64 loss: -0.09346914291381836
Batch 27/64 loss: -0.09208881855010986
Batch 28/64 loss: -0.07802033424377441
Batch 29/64 loss: -0.09363067150115967
Batch 30/64 loss: -0.06676614284515381
Batch 31/64 loss: -0.10288345813751221
Batch 32/64 loss: -0.08030211925506592
Batch 33/64 loss: -0.1008143424987793
Batch 34/64 loss: -0.13032352924346924
Batch 35/64 loss: -0.13292062282562256
Batch 36/64 loss: -0.12610989809036255
Batch 37/64 loss: -0.09386241436004639
Batch 38/64 loss: -0.11560797691345215
Batch 39/64 loss: -0.08032315969467163
Batch 40/64 loss: -0.11762654781341553
Batch 41/64 loss: -0.09676820039749146
Batch 42/64 loss: -0.10280203819274902
Batch 43/64 loss: -0.13124829530715942
Batch 44/64 loss: -0.1257203221321106
Batch 45/64 loss: -0.10052299499511719
Batch 46/64 loss: -0.11960530281066895
Batch 47/64 loss: -0.10917997360229492
Batch 48/64 loss: -0.07815659046173096
Batch 49/64 loss: -0.13507026433944702
Batch 50/64 loss: -0.09988367557525635
Batch 51/64 loss: -0.05771857500076294
Batch 52/64 loss: -0.07825934886932373
Batch 53/64 loss: -0.10649794340133667
Batch 54/64 loss: -0.10482382774353027
Batch 55/64 loss: -0.0985599160194397
Batch 56/64 loss: -0.1313561201095581
Batch 57/64 loss: -0.10973477363586426
Batch 58/64 loss: -0.07245451211929321
Batch 59/64 loss: -0.10989034175872803
Batch 60/64 loss: -0.11001646518707275
Batch 61/64 loss: -0.06948310136795044
Batch 62/64 loss: -0.09070265293121338
Batch 63/64 loss: -0.10353481769561768
Batch 64/64 loss: -0.038116514682769775
Epoch 286  Train loss: -0.10525799896202835  Val loss: 0.05925031646420456
Epoch 287
-------------------------------
Batch 1/64 loss: -0.09732598066329956
Batch 2/64 loss: -0.0984567403793335
Batch 3/64 loss: -0.12740933895111084
Batch 4/64 loss: -0.11807233095169067
Batch 5/64 loss: -0.1224556565284729
Batch 6/64 loss: -0.10803091526031494
Batch 7/64 loss: -0.11786562204360962
Batch 8/64 loss: -0.11517477035522461
Batch 9/64 loss: -0.10889363288879395
Batch 10/64 loss: -0.075600266456604
Batch 11/64 loss: -0.1001577377319336
Batch 12/64 loss: -0.10031485557556152
Batch 13/64 loss: -0.1311497688293457
Batch 14/64 loss: -0.10492897033691406
Batch 15/64 loss: -0.09718579053878784
Batch 16/64 loss: -0.11923873424530029
Batch 17/64 loss: -0.09516048431396484
Batch 18/64 loss: -0.10847198963165283
Batch 19/64 loss: -0.12112712860107422
Batch 20/64 loss: -0.1225883960723877
Batch 21/64 loss: -0.07692790031433105
Batch 22/64 loss: -0.08872509002685547
Batch 23/64 loss: -0.10227060317993164
Batch 24/64 loss: -0.07314115762710571
Batch 25/64 loss: -0.14007896184921265
Batch 26/64 loss: -0.0954134464263916
Batch 27/64 loss: -0.11677777767181396
Batch 28/64 loss: -0.12019246816635132
Batch 29/64 loss: -0.14348632097244263
Batch 30/64 loss: -0.10488688945770264
Batch 31/64 loss: -0.10722434520721436
Batch 32/64 loss: -0.06837433576583862
Batch 33/64 loss: -0.12317019701004028
Batch 34/64 loss: -0.10881078243255615
Batch 35/64 loss: -0.14177221059799194
Batch 36/64 loss: -0.11991482973098755
Batch 37/64 loss: -0.1340116262435913
Batch 38/64 loss: -0.13664096593856812
Batch 39/64 loss: -0.10099899768829346
Batch 40/64 loss: -0.10027742385864258
Batch 41/64 loss: -0.09607517719268799
Batch 42/64 loss: -0.11465787887573242
Batch 43/64 loss: -0.10603678226470947
Batch 44/64 loss: -0.13741636276245117
Batch 45/64 loss: -0.11096745729446411
Batch 46/64 loss: -0.05717265605926514
Batch 47/64 loss: -0.11646604537963867
Batch 48/64 loss: -0.1174764633178711
Batch 49/64 loss: -0.12396544218063354
Batch 50/64 loss: -0.06224030256271362
Batch 51/64 loss: -0.10479986667633057
Batch 52/64 loss: -0.12984460592269897
Batch 53/64 loss: -0.1043393611907959
Batch 54/64 loss: -0.09435570240020752
Batch 55/64 loss: -0.12407916784286499
Batch 56/64 loss: -0.14263606071472168
Batch 57/64 loss: -0.10878044366836548
Batch 58/64 loss: -0.05705559253692627
Batch 59/64 loss: -0.06993567943572998
Batch 60/64 loss: -0.09207016229629517
Batch 61/64 loss: -0.12209045886993408
Batch 62/64 loss: -0.10702037811279297
Batch 63/64 loss: -0.10022240877151489
Batch 64/64 loss: -0.11254262924194336
Epoch 287  Train loss: -0.10787163715736539  Val loss: 0.06408762030585115
Epoch 288
-------------------------------
Batch 1/64 loss: -0.11235129833221436
Batch 2/64 loss: -0.10674816370010376
Batch 3/64 loss: -0.11794638633728027
Batch 4/64 loss: -0.08699655532836914
Batch 5/64 loss: -0.1084212064743042
Batch 6/64 loss: -0.08240628242492676
Batch 7/64 loss: -0.07800531387329102
Batch 8/64 loss: -0.13727867603302002
Batch 9/64 loss: -0.1323665976524353
Batch 10/64 loss: -0.09640538692474365
Batch 11/64 loss: -0.14973342418670654
Batch 12/64 loss: -0.12301599979400635
Batch 13/64 loss: -0.16053390502929688
Batch 14/64 loss: -0.1012958288192749
Batch 15/64 loss: -0.07583588361740112
Batch 16/64 loss: -0.10823142528533936
Batch 17/64 loss: -0.08602863550186157
Batch 18/64 loss: -0.12766540050506592
Batch 19/64 loss: -0.1145126223564148
Batch 20/64 loss: -0.1008443832397461
Batch 21/64 loss: -0.13286656141281128
Batch 22/64 loss: -0.128997802734375
Batch 23/64 loss: -0.11337876319885254
Batch 24/64 loss: -0.11695700883865356
Batch 25/64 loss: -0.09226667881011963
Batch 26/64 loss: -0.10368949174880981
Batch 27/64 loss: -0.0908917784690857
Batch 28/64 loss: -0.13471680879592896
Batch 29/64 loss: -0.13055157661437988
Batch 30/64 loss: -0.08191794157028198
Batch 31/64 loss: -0.10096830129623413
Batch 32/64 loss: -0.13003474473953247
Batch 33/64 loss: -0.11727094650268555
Batch 34/64 loss: -0.08244597911834717
Batch 35/64 loss: -0.09303849935531616
Batch 36/64 loss: -0.0960010290145874
Batch 37/64 loss: -0.10961854457855225
Batch 38/64 loss: -0.0716242790222168
Batch 39/64 loss: -0.10709565877914429
Batch 40/64 loss: -0.1217687726020813
Batch 41/64 loss: -0.1121218204498291
Batch 42/64 loss: -0.10031157732009888
Batch 43/64 loss: -0.10902923345565796
Batch 44/64 loss: -0.1013001799583435
Batch 45/64 loss: -0.12129122018814087
Batch 46/64 loss: -0.12534356117248535
Batch 47/64 loss: -0.06032747030258179
Batch 48/64 loss: -0.11536049842834473
Batch 49/64 loss: -0.06303095817565918
Batch 50/64 loss: -0.11792784929275513
Batch 51/64 loss: -0.05848503112792969
Batch 52/64 loss: -0.12250441312789917
Batch 53/64 loss: -0.07369881868362427
Batch 54/64 loss: -0.11512058973312378
Batch 55/64 loss: -0.1100543737411499
Batch 56/64 loss: -0.08856630325317383
Batch 57/64 loss: -0.0782325267791748
Batch 58/64 loss: -0.07979625463485718
Batch 59/64 loss: -0.13850808143615723
Batch 60/64 loss: -0.06719529628753662
Batch 61/64 loss: -0.12635725736618042
Batch 62/64 loss: -0.13577008247375488
Batch 63/64 loss: -0.11063182353973389
Batch 64/64 loss: -0.1283087134361267
Epoch 288  Train loss: -0.10650856939016604  Val loss: 0.05821314054666106
Epoch 289
-------------------------------
Batch 1/64 loss: -0.12849724292755127
Batch 2/64 loss: -0.10323578119277954
Batch 3/64 loss: -0.09614694118499756
Batch 4/64 loss: -0.10840874910354614
Batch 5/64 loss: -0.11534774303436279
Batch 6/64 loss: -0.10601502656936646
Batch 7/64 loss: -0.09480392932891846
Batch 8/64 loss: -0.08783143758773804
Batch 9/64 loss: -0.08012455701828003
Batch 10/64 loss: -0.12940698862075806
Batch 11/64 loss: -0.10147291421890259
Batch 12/64 loss: -0.114615797996521
Batch 13/64 loss: -0.109474778175354
Batch 14/64 loss: -0.09123682975769043
Batch 15/64 loss: -0.09505897760391235
Batch 16/64 loss: -0.1252591609954834
Batch 17/64 loss: -0.11842715740203857
Batch 18/64 loss: -0.09875231981277466
Batch 19/64 loss: -0.14263802766799927
Batch 20/64 loss: -0.07223743200302124
Batch 21/64 loss: -0.118996262550354
Batch 22/64 loss: -0.11310184001922607
Batch 23/64 loss: -0.1390785574913025
Batch 24/64 loss: -0.1298394799232483
Batch 25/64 loss: -0.12746214866638184
Batch 26/64 loss: -0.10153889656066895
Batch 27/64 loss: -0.10369259119033813
Batch 28/64 loss: -0.12170743942260742
Batch 29/64 loss: -0.12149161100387573
Batch 30/64 loss: -0.0900566577911377
Batch 31/64 loss: -0.12978291511535645
Batch 32/64 loss: -0.11641991138458252
Batch 33/64 loss: -0.11219686269760132
Batch 34/64 loss: -0.11151593923568726
Batch 35/64 loss: -0.11043113470077515
Batch 36/64 loss: -0.09743201732635498
Batch 37/64 loss: -0.08226609230041504
Batch 38/64 loss: -0.07770240306854248
Batch 39/64 loss: -0.13159257173538208
Batch 40/64 loss: -0.08754992485046387
Batch 41/64 loss: -0.13470560312271118
Batch 42/64 loss: -0.10839468240737915
Batch 43/64 loss: -0.10669314861297607
Batch 44/64 loss: -0.07580870389938354
Batch 45/64 loss: -0.07778453826904297
Batch 46/64 loss: -0.13120049238204956
Batch 47/64 loss: -0.13451731204986572
Batch 48/64 loss: -0.12762165069580078
Batch 49/64 loss: -0.10058766603469849
Batch 50/64 loss: -0.11397969722747803
Batch 51/64 loss: -0.07070654630661011
Batch 52/64 loss: -0.11604553461074829
Batch 53/64 loss: -0.09271067380905151
Batch 54/64 loss: -0.1086663007736206
Batch 55/64 loss: -0.09632730484008789
Batch 56/64 loss: -0.11981201171875
Batch 57/64 loss: -0.12061852216720581
Batch 58/64 loss: -0.08067327737808228
Batch 59/64 loss: -0.12492644786834717
Batch 60/64 loss: -0.10507774353027344
Batch 61/64 loss: -0.06799757480621338
Batch 62/64 loss: -0.1258721947669983
Batch 63/64 loss: -0.12299120426177979
Batch 64/64 loss: -0.08962464332580566
Epoch 289  Train loss: -0.10782406844344794  Val loss: 0.06016975415940957
Epoch 290
-------------------------------
Batch 1/64 loss: -0.1591576337814331
Batch 2/64 loss: -0.1040961742401123
Batch 3/64 loss: -0.06243765354156494
Batch 4/64 loss: -0.14167487621307373
Batch 5/64 loss: -0.08525002002716064
Batch 6/64 loss: -0.0983201265335083
Batch 7/64 loss: -0.13995051383972168
Batch 8/64 loss: -0.127855122089386
Batch 9/64 loss: -0.11634266376495361
Batch 10/64 loss: -0.10866224765777588
Batch 11/64 loss: -0.10944485664367676
Batch 12/64 loss: -0.12472265958786011
Batch 13/64 loss: -0.113311767578125
Batch 14/64 loss: -0.06453841924667358
Batch 15/64 loss: -0.11671960353851318
Batch 16/64 loss: -0.041607797145843506
Batch 17/64 loss: -0.11580950021743774
Batch 18/64 loss: -0.1173555850982666
Batch 19/64 loss: -0.09859830141067505
Batch 20/64 loss: -0.11658245325088501
Batch 21/64 loss: -0.12253904342651367
Batch 22/64 loss: -0.1088331937789917
Batch 23/64 loss: -0.12817656993865967
Batch 24/64 loss: -0.0733533501625061
Batch 25/64 loss: -0.10157167911529541
Batch 26/64 loss: -0.07015055418014526
Batch 27/64 loss: -0.13600635528564453
Batch 28/64 loss: -0.08233273029327393
Batch 29/64 loss: -0.13729029893875122
Batch 30/64 loss: -0.1108705997467041
Batch 31/64 loss: -0.09842216968536377
Batch 32/64 loss: -0.11514031887054443
Batch 33/64 loss: -0.07308149337768555
Batch 34/64 loss: -0.1078486442565918
Batch 35/64 loss: -0.11406856775283813
Batch 36/64 loss: -0.12913841009140015
Batch 37/64 loss: -0.13517433404922485
Batch 38/64 loss: -0.12302559614181519
Batch 39/64 loss: -0.10769659280776978
Batch 40/64 loss: -0.13027077913284302
Batch 41/64 loss: -0.1185339093208313
Batch 42/64 loss: -0.09448760747909546
Batch 43/64 loss: -0.11676138639450073
Batch 44/64 loss: -0.09379124641418457
Batch 45/64 loss: -0.09241598844528198
Batch 46/64 loss: -0.12815117835998535
Batch 47/64 loss: -0.12300193309783936
Batch 48/64 loss: -0.10050112009048462
Batch 49/64 loss: -0.09403127431869507
Batch 50/64 loss: -0.09305250644683838
Batch 51/64 loss: -0.09660893678665161
Batch 52/64 loss: -0.12789809703826904
Batch 53/64 loss: -0.1253340244293213
Batch 54/64 loss: -0.10007870197296143
Batch 55/64 loss: -0.12969732284545898
Batch 56/64 loss: -0.1337798833847046
Batch 57/64 loss: -0.07227551937103271
Batch 58/64 loss: -0.10169565677642822
Batch 59/64 loss: -0.08861047029495239
Batch 60/64 loss: -0.10599195957183838
Batch 61/64 loss: -0.1310809850692749
Batch 62/64 loss: -0.14448434114456177
Batch 63/64 loss: -0.11421722173690796
Batch 64/64 loss: -0.12234973907470703
Epoch 290  Train loss: -0.10957918167114258  Val loss: 0.05338417582495516
Epoch 291
-------------------------------
Batch 1/64 loss: -0.11052721738815308
Batch 2/64 loss: -0.1265823245048523
Batch 3/64 loss: -0.13027048110961914
Batch 4/64 loss: -0.12060201168060303
Batch 5/64 loss: -0.13496816158294678
Batch 6/64 loss: -0.12890833616256714
Batch 7/64 loss: -0.13943099975585938
Batch 8/64 loss: -0.11598736047744751
Batch 9/64 loss: -0.14936691522598267
Batch 10/64 loss: -0.09279775619506836
Batch 11/64 loss: -0.08415108919143677
Batch 12/64 loss: -0.10573244094848633
Batch 13/64 loss: -0.12123125791549683
Batch 14/64 loss: -0.13622933626174927
Batch 15/64 loss: -0.11892366409301758
Batch 16/64 loss: -0.12444347143173218
Batch 17/64 loss: -0.10400617122650146
Batch 18/64 loss: -0.11726081371307373
Batch 19/64 loss: -0.106326162815094
Batch 20/64 loss: -0.13430947065353394
Batch 21/64 loss: -0.1309635043144226
Batch 22/64 loss: -0.11403042078018188
Batch 23/64 loss: -0.11280679702758789
Batch 24/64 loss: -0.11601442098617554
Batch 25/64 loss: -0.07584846019744873
Batch 26/64 loss: -0.09915971755981445
Batch 27/64 loss: -0.1107833981513977
Batch 28/64 loss: -0.11361175775527954
Batch 29/64 loss: -0.09597086906433105
Batch 30/64 loss: -0.10178357362747192
Batch 31/64 loss: -0.07766276597976685
Batch 32/64 loss: -0.10106289386749268
Batch 33/64 loss: -0.11912339925765991
Batch 34/64 loss: -0.12491697072982788
Batch 35/64 loss: -0.11222553253173828
Batch 36/64 loss: -0.09322530031204224
Batch 37/64 loss: -0.130160391330719
Batch 38/64 loss: -0.11145055294036865
Batch 39/64 loss: -0.09249985218048096
Batch 40/64 loss: -0.09608536958694458
Batch 41/64 loss: -0.11981630325317383
Batch 42/64 loss: -0.12448680400848389
Batch 43/64 loss: -0.12107646465301514
Batch 44/64 loss: -0.1382744312286377
Batch 45/64 loss: -0.10679453611373901
Batch 46/64 loss: -0.09764134883880615
Batch 47/64 loss: -0.10432237386703491
Batch 48/64 loss: -0.08413434028625488
Batch 49/64 loss: -0.10593873262405396
Batch 50/64 loss: -0.09478640556335449
Batch 51/64 loss: -0.11073297262191772
Batch 52/64 loss: -0.06185507774353027
Batch 53/64 loss: -0.11453992128372192
Batch 54/64 loss: -0.11474478244781494
Batch 55/64 loss: -0.13786959648132324
Batch 56/64 loss: -0.14539730548858643
Batch 57/64 loss: -0.13819921016693115
Batch 58/64 loss: -0.13327348232269287
Batch 59/64 loss: -0.10804092884063721
Batch 60/64 loss: -0.12628471851348877
Batch 61/64 loss: -0.12511718273162842
Batch 62/64 loss: -0.08879226446151733
Batch 63/64 loss: -0.1148330569267273
Batch 64/64 loss: -0.08180439472198486
Epoch 291  Train loss: -0.11309406944349701  Val loss: 0.05735920753675638
Epoch 292
-------------------------------
Batch 1/64 loss: -0.0911707878112793
Batch 2/64 loss: -0.11864948272705078
Batch 3/64 loss: -0.1148877739906311
Batch 4/64 loss: -0.09900647401809692
Batch 5/64 loss: -0.10277467966079712
Batch 6/64 loss: -0.12733972072601318
Batch 7/64 loss: -0.1018756628036499
Batch 8/64 loss: -0.09686434268951416
Batch 9/64 loss: -0.12087857723236084
Batch 10/64 loss: -0.12060081958770752
Batch 11/64 loss: -0.09551453590393066
Batch 12/64 loss: -0.1253126859664917
Batch 13/64 loss: -0.12869220972061157
Batch 14/64 loss: -0.09342050552368164
Batch 15/64 loss: -0.11302655935287476
Batch 16/64 loss: -0.11685687303543091
Batch 17/64 loss: -0.12199831008911133
Batch 18/64 loss: -0.1241876482963562
Batch 19/64 loss: -0.11979275941848755
Batch 20/64 loss: -0.14327359199523926
Batch 21/64 loss: -0.12975013256072998
Batch 22/64 loss: -0.09715813398361206
Batch 23/64 loss: -0.11867082118988037
Batch 24/64 loss: -0.09529507160186768
Batch 25/64 loss: -0.10689389705657959
Batch 26/64 loss: -0.09984934329986572
Batch 27/64 loss: -0.09628671407699585
Batch 28/64 loss: -0.12720799446105957
Batch 29/64 loss: -0.1474449634552002
Batch 30/64 loss: -0.07154327630996704
Batch 31/64 loss: -0.06669217348098755
Batch 32/64 loss: -0.11078870296478271
Batch 33/64 loss: -0.08570331335067749
Batch 34/64 loss: -0.1410488486289978
Batch 35/64 loss: -0.12661361694335938
Batch 36/64 loss: -0.14065921306610107
Batch 37/64 loss: -0.12791931629180908
Batch 38/64 loss: -0.09388160705566406
Batch 39/64 loss: -0.10702717304229736
Batch 40/64 loss: -0.08894121646881104
Batch 41/64 loss: -0.10486602783203125
Batch 42/64 loss: -0.13221603631973267
Batch 43/64 loss: -0.12052476406097412
Batch 44/64 loss: -0.13291478157043457
Batch 45/64 loss: -0.08834117650985718
Batch 46/64 loss: -0.09407353401184082
Batch 47/64 loss: -0.12064772844314575
Batch 48/64 loss: -0.1300773024559021
Batch 49/64 loss: -0.09475231170654297
Batch 50/64 loss: -0.15270888805389404
Batch 51/64 loss: -0.10157930850982666
Batch 52/64 loss: -0.1014324426651001
Batch 53/64 loss: -0.09339654445648193
Batch 54/64 loss: -0.13647007942199707
Batch 55/64 loss: -0.07123768329620361
Batch 56/64 loss: -0.12488770484924316
Batch 57/64 loss: -0.1271432638168335
Batch 58/64 loss: -0.04070383310317993
Batch 59/64 loss: -0.11069881916046143
Batch 60/64 loss: -0.09567344188690186
Batch 61/64 loss: -0.1060439944267273
Batch 62/64 loss: -0.09437322616577148
Batch 63/64 loss: -0.12008470296859741
Batch 64/64 loss: -0.10397124290466309
Epoch 292  Train loss: -0.11024824404249005  Val loss: 0.057496412513182334
Epoch 293
-------------------------------
Batch 1/64 loss: -0.12970447540283203
Batch 2/64 loss: -0.07827973365783691
Batch 3/64 loss: -0.12200057506561279
Batch 4/64 loss: -0.11539340019226074
Batch 5/64 loss: -0.09848898649215698
Batch 6/64 loss: -0.1198994517326355
Batch 7/64 loss: -0.10202020406723022
Batch 8/64 loss: -0.143873393535614
Batch 9/64 loss: -0.11395615339279175
Batch 10/64 loss: -0.08552438020706177
Batch 11/64 loss: -0.1056983470916748
Batch 12/64 loss: -0.12718439102172852
Batch 13/64 loss: -0.10182070732116699
Batch 14/64 loss: -0.08743447065353394
Batch 15/64 loss: -0.14767146110534668
Batch 16/64 loss: -0.12149280309677124
Batch 17/64 loss: -0.11228203773498535
Batch 18/64 loss: -0.11207753419876099
Batch 19/64 loss: -0.0942578911781311
Batch 20/64 loss: -0.11190086603164673
Batch 21/64 loss: -0.09989571571350098
Batch 22/64 loss: -0.10052001476287842
Batch 23/64 loss: -0.11979043483734131
Batch 24/64 loss: -0.10534822940826416
Batch 25/64 loss: -0.11124956607818604
Batch 26/64 loss: -0.11559343338012695
Batch 27/64 loss: -0.08401674032211304
Batch 28/64 loss: -0.11690884828567505
Batch 29/64 loss: -0.10759508609771729
Batch 30/64 loss: -0.11686789989471436
Batch 31/64 loss: -0.1487945318222046
Batch 32/64 loss: -0.1299116015434265
Batch 33/64 loss: -0.10848450660705566
Batch 34/64 loss: -0.13193219900131226
Batch 35/64 loss: -0.09984028339385986
Batch 36/64 loss: -0.11116844415664673
Batch 37/64 loss: -0.14184069633483887
Batch 38/64 loss: -0.12550079822540283
Batch 39/64 loss: -0.10009276866912842
Batch 40/64 loss: -0.11345577239990234
Batch 41/64 loss: -0.11801820993423462
Batch 42/64 loss: -0.11132305860519409
Batch 43/64 loss: -0.13151055574417114
Batch 44/64 loss: -0.1026071310043335
Batch 45/64 loss: -0.10052192211151123
Batch 46/64 loss: -0.13268250226974487
Batch 47/64 loss: -0.1185869574546814
Batch 48/64 loss: -0.11721426248550415
Batch 49/64 loss: -0.08938038349151611
Batch 50/64 loss: -0.14692693948745728
Batch 51/64 loss: -0.07699930667877197
Batch 52/64 loss: -0.06915539503097534
Batch 53/64 loss: -0.13360649347305298
Batch 54/64 loss: -0.11386817693710327
Batch 55/64 loss: -0.08080631494522095
Batch 56/64 loss: -0.10113191604614258
Batch 57/64 loss: -0.1090538501739502
Batch 58/64 loss: -0.14256638288497925
Batch 59/64 loss: -0.09264624118804932
Batch 60/64 loss: -0.1246873140335083
Batch 61/64 loss: -0.09878641366958618
Batch 62/64 loss: -0.09476250410079956
Batch 63/64 loss: -0.10704141855239868
Batch 64/64 loss: -0.0907391905784607
Epoch 293  Train loss: -0.11139932356628718  Val loss: 0.057944425192895214
Epoch 294
-------------------------------
Batch 1/64 loss: -0.11601400375366211
Batch 2/64 loss: -0.13284200429916382
Batch 3/64 loss: -0.1327548623085022
Batch 4/64 loss: -0.0949675440788269
Batch 5/64 loss: -0.08817040920257568
Batch 6/64 loss: -0.12210279703140259
Batch 7/64 loss: -0.09173905849456787
Batch 8/64 loss: -0.09161889553070068
Batch 9/64 loss: -0.0823025107383728
Batch 10/64 loss: -0.13301575183868408
Batch 11/64 loss: -0.1077500581741333
Batch 12/64 loss: -0.11306804418563843
Batch 13/64 loss: -0.14115029573440552
Batch 14/64 loss: -0.08792424201965332
Batch 15/64 loss: -0.15748178958892822
Batch 16/64 loss: -0.12628519535064697
Batch 17/64 loss: -0.12760913372039795
Batch 18/64 loss: -0.11901247501373291
Batch 19/64 loss: -0.15137696266174316
Batch 20/64 loss: -0.10151588916778564
Batch 21/64 loss: -0.07953369617462158
Batch 22/64 loss: -0.1258845329284668
Batch 23/64 loss: -0.10922259092330933
Batch 24/64 loss: -0.11141639947891235
Batch 25/64 loss: -0.1303587555885315
Batch 26/64 loss: -0.10617828369140625
Batch 27/64 loss: -0.06299859285354614
Batch 28/64 loss: -0.06918692588806152
Batch 29/64 loss: -0.14005744457244873
Batch 30/64 loss: -0.12009012699127197
Batch 31/64 loss: -0.11826866865158081
Batch 32/64 loss: -0.09777379035949707
Batch 33/64 loss: -0.13067632913589478
Batch 34/64 loss: -0.1001431941986084
Batch 35/64 loss: -0.1177976131439209
Batch 36/64 loss: -0.12468564510345459
Batch 37/64 loss: -0.10940510034561157
Batch 38/64 loss: -0.12321227788925171
Batch 39/64 loss: -0.1485092043876648
Batch 40/64 loss: -0.08861672878265381
Batch 41/64 loss: -0.12797051668167114
Batch 42/64 loss: -0.11374950408935547
Batch 43/64 loss: -0.08576154708862305
Batch 44/64 loss: -0.13238674402236938
Batch 45/64 loss: -0.10388374328613281
Batch 46/64 loss: -0.09818869829177856
Batch 47/64 loss: -0.09213387966156006
Batch 48/64 loss: -0.10430586338043213
Batch 49/64 loss: -0.10091066360473633
Batch 50/64 loss: -0.11570388078689575
Batch 51/64 loss: -0.11733287572860718
Batch 52/64 loss: -0.14312225580215454
Batch 53/64 loss: -0.07964712381362915
Batch 54/64 loss: -0.10597044229507446
Batch 55/64 loss: -0.10747039318084717
Batch 56/64 loss: -0.12529075145721436
Batch 57/64 loss: -0.13023066520690918
Batch 58/64 loss: -0.09997308254241943
Batch 59/64 loss: -0.11619138717651367
Batch 60/64 loss: -0.1393214464187622
Batch 61/64 loss: -0.1545870304107666
Batch 62/64 loss: -0.12928295135498047
Batch 63/64 loss: -0.10144883394241333
Batch 64/64 loss: -0.09530329704284668
Epoch 294  Train loss: -0.11339701297236424  Val loss: 0.06502904003018775
Epoch 295
-------------------------------
Batch 1/64 loss: -0.11731749773025513
Batch 2/64 loss: -0.14360970258712769
Batch 3/64 loss: -0.09504455327987671
Batch 4/64 loss: -0.12593859434127808
Batch 5/64 loss: -0.12973886728286743
Batch 6/64 loss: -0.13482952117919922
Batch 7/64 loss: -0.12881338596343994
Batch 8/64 loss: -0.11797165870666504
Batch 9/64 loss: -0.14046639204025269
Batch 10/64 loss: -0.13064438104629517
Batch 11/64 loss: -0.096346914768219
Batch 12/64 loss: -0.11946260929107666
Batch 13/64 loss: -0.13570111989974976
Batch 14/64 loss: -0.14487206935882568
Batch 15/64 loss: -0.11403888463973999
Batch 16/64 loss: -0.08738064765930176
Batch 17/64 loss: -0.12963002920150757
Batch 18/64 loss: -0.10552966594696045
Batch 19/64 loss: -0.1168336272239685
Batch 20/64 loss: -0.14974486827850342
Batch 21/64 loss: -0.12217366695404053
Batch 22/64 loss: -0.10487782955169678
Batch 23/64 loss: -0.14124304056167603
Batch 24/64 loss: -0.13745468854904175
Batch 25/64 loss: -0.1007007360458374
Batch 26/64 loss: -0.12166470289230347
Batch 27/64 loss: -0.13675636053085327
Batch 28/64 loss: -0.15407907962799072
Batch 29/64 loss: -0.1259557008743286
Batch 30/64 loss: -0.13235318660736084
Batch 31/64 loss: -0.14129281044006348
Batch 32/64 loss: -0.0739709734916687
Batch 33/64 loss: -0.09207332134246826
Batch 34/64 loss: -0.13495862483978271
Batch 35/64 loss: -0.13390767574310303
Batch 36/64 loss: -0.12044769525527954
Batch 37/64 loss: -0.1191561222076416
Batch 38/64 loss: -0.12088781595230103
Batch 39/64 loss: -0.08878850936889648
Batch 40/64 loss: -0.08439993858337402
Batch 41/64 loss: -0.1219862699508667
Batch 42/64 loss: -0.06409895420074463
Batch 43/64 loss: -0.12690973281860352
Batch 44/64 loss: -0.1281251311302185
Batch 45/64 loss: -0.08872747421264648
Batch 46/64 loss: -0.12421423196792603
Batch 47/64 loss: -0.08896231651306152
Batch 48/64 loss: -0.0988379716873169
Batch 49/64 loss: -0.07964873313903809
Batch 50/64 loss: -0.10234951972961426
Batch 51/64 loss: -0.09260803461074829
Batch 52/64 loss: -0.060283362865448
Batch 53/64 loss: -0.11086905002593994
Batch 54/64 loss: -0.11189639568328857
Batch 55/64 loss: -0.09288263320922852
Batch 56/64 loss: -0.10357242822647095
Batch 57/64 loss: -0.10648280382156372
Batch 58/64 loss: -0.11728900671005249
Batch 59/64 loss: -0.08269011974334717
Batch 60/64 loss: -0.09522438049316406
Batch 61/64 loss: -0.1131824254989624
Batch 62/64 loss: -0.09028434753417969
Batch 63/64 loss: -0.09648841619491577
Batch 64/64 loss: -0.13169467449188232
Epoch 295  Train loss: -0.11368536808911492  Val loss: 0.0600704165668422
Epoch 296
-------------------------------
Batch 1/64 loss: -0.11415213346481323
Batch 2/64 loss: -0.10826152563095093
Batch 3/64 loss: -0.10071766376495361
Batch 4/64 loss: -0.14674144983291626
Batch 5/64 loss: -0.08880782127380371
Batch 6/64 loss: -0.14798378944396973
Batch 7/64 loss: -0.10955333709716797
Batch 8/64 loss: -0.1171155571937561
Batch 9/64 loss: -0.12239861488342285
Batch 10/64 loss: -0.09169071912765503
Batch 11/64 loss: -0.12389814853668213
Batch 12/64 loss: -0.1178896427154541
Batch 13/64 loss: -0.12002432346343994
Batch 14/64 loss: -0.13260233402252197
Batch 15/64 loss: -0.12889838218688965
Batch 16/64 loss: -0.12849658727645874
Batch 17/64 loss: -0.12298351526260376
Batch 18/64 loss: -0.12435042858123779
Batch 19/64 loss: -0.13187915086746216
Batch 20/64 loss: -0.10275942087173462
Batch 21/64 loss: -0.14500701427459717
Batch 22/64 loss: -0.08728587627410889
Batch 23/64 loss: -0.08235996961593628
Batch 24/64 loss: -0.12373435497283936
Batch 25/64 loss: -0.0950387716293335
Batch 26/64 loss: -0.1253596544265747
Batch 27/64 loss: -0.08046352863311768
Batch 28/64 loss: -0.09583288431167603
Batch 29/64 loss: -0.10945510864257812
Batch 30/64 loss: -0.1207687258720398
Batch 31/64 loss: -0.08012056350708008
Batch 32/64 loss: -0.06750082969665527
Batch 33/64 loss: -0.07535380125045776
Batch 34/64 loss: -0.11338841915130615
Batch 35/64 loss: -0.13930809497833252
Batch 36/64 loss: -0.08953040838241577
Batch 37/64 loss: -0.06163257360458374
Batch 38/64 loss: -0.11598873138427734
Batch 39/64 loss: -0.13246428966522217
Batch 40/64 loss: -0.11023187637329102
Batch 41/64 loss: -0.11177736520767212
Batch 42/64 loss: -0.09412479400634766
Batch 43/64 loss: -0.11413365602493286
Batch 44/64 loss: -0.13302481174468994
Batch 45/64 loss: -0.1353520154953003
Batch 46/64 loss: -0.1385430097579956
Batch 47/64 loss: -0.07653594017028809
Batch 48/64 loss: -0.1245574951171875
Batch 49/64 loss: -0.13401198387145996
Batch 50/64 loss: -0.11693352460861206
Batch 51/64 loss: -0.11157441139221191
Batch 52/64 loss: -0.12138265371322632
Batch 53/64 loss: -0.11002951860427856
Batch 54/64 loss: -0.120780348777771
Batch 55/64 loss: -0.10902136564254761
Batch 56/64 loss: -0.10951751470565796
Batch 57/64 loss: -0.10095685720443726
Batch 58/64 loss: -0.10371696949005127
Batch 59/64 loss: -0.08044105768203735
Batch 60/64 loss: -0.10559052228927612
Batch 61/64 loss: -0.12222284078598022
Batch 62/64 loss: -0.12372410297393799
Batch 63/64 loss: -0.09274041652679443
Batch 64/64 loss: -0.11106973886489868
Epoch 296  Train loss: -0.1114670666993833  Val loss: 0.06293230142790018
Epoch 297
-------------------------------
Batch 1/64 loss: -0.14156055450439453
Batch 2/64 loss: -0.09469419717788696
Batch 3/64 loss: -0.10790687799453735
Batch 4/64 loss: -0.10652875900268555
Batch 5/64 loss: -0.13144367933273315
Batch 6/64 loss: -0.10366010665893555
Batch 7/64 loss: -0.11263161897659302
Batch 8/64 loss: -0.13689768314361572
Batch 9/64 loss: -0.11332458257675171
Batch 10/64 loss: -0.14151543378829956
Batch 11/64 loss: -0.13398218154907227
Batch 12/64 loss: -0.10352587699890137
Batch 13/64 loss: -0.09372025728225708
Batch 14/64 loss: -0.15545302629470825
Batch 15/64 loss: -0.09569668769836426
Batch 16/64 loss: -0.08676576614379883
Batch 17/64 loss: -0.12512683868408203
Batch 18/64 loss: -0.10979747772216797
Batch 19/64 loss: -0.12343931198120117
Batch 20/64 loss: -0.09580212831497192
Batch 21/64 loss: -0.12618106603622437
Batch 22/64 loss: -0.09933304786682129
Batch 23/64 loss: -0.10592883825302124
Batch 24/64 loss: -0.09158653020858765
Batch 25/64 loss: -0.09711110591888428
Batch 26/64 loss: -0.1386362910270691
Batch 27/64 loss: -0.12117260694503784
Batch 28/64 loss: -0.11018335819244385
Batch 29/64 loss: -0.11438608169555664
Batch 30/64 loss: -0.06961596012115479
Batch 31/64 loss: -0.09194856882095337
Batch 32/64 loss: -0.10296475887298584
Batch 33/64 loss: -0.10953229665756226
Batch 34/64 loss: -0.1067933440208435
Batch 35/64 loss: -0.13347923755645752
Batch 36/64 loss: -0.11905866861343384
Batch 37/64 loss: -0.08754968643188477
Batch 38/64 loss: -0.1207532286643982
Batch 39/64 loss: -0.09495723247528076
Batch 40/64 loss: -0.10273271799087524
Batch 41/64 loss: -0.11922931671142578
Batch 42/64 loss: -0.13523268699645996
Batch 43/64 loss: -0.08765518665313721
Batch 44/64 loss: -0.07625806331634521
Batch 45/64 loss: -0.08080953359603882
Batch 46/64 loss: -0.11696851253509521
Batch 47/64 loss: -0.10850656032562256
Batch 48/64 loss: -0.12217962741851807
Batch 49/64 loss: -0.12420791387557983
Batch 50/64 loss: -0.11145919561386108
Batch 51/64 loss: -0.09374594688415527
Batch 52/64 loss: -0.12657016515731812
Batch 53/64 loss: -0.12577950954437256
Batch 54/64 loss: -0.1216844916343689
Batch 55/64 loss: -0.12196868658065796
Batch 56/64 loss: -0.13065433502197266
Batch 57/64 loss: -0.10952317714691162
Batch 58/64 loss: -0.1300952434539795
Batch 59/64 loss: -0.13808947801589966
Batch 60/64 loss: -0.12119078636169434
Batch 61/64 loss: -0.08427989482879639
Batch 62/64 loss: -0.1344130039215088
Batch 63/64 loss: -0.1372278332710266
Batch 64/64 loss: -0.14143967628479004
Epoch 297  Train loss: -0.11327351495331409  Val loss: 0.05656232559394181
Epoch 298
-------------------------------
Batch 1/64 loss: -0.10379141569137573
Batch 2/64 loss: -0.12991297245025635
Batch 3/64 loss: -0.11834365129470825
Batch 4/64 loss: -0.10976070165634155
Batch 5/64 loss: -0.11371564865112305
Batch 6/64 loss: -0.10836201906204224
Batch 7/64 loss: -0.0921010971069336
Batch 8/64 loss: -0.11051368713378906
Batch 9/64 loss: -0.08902990818023682
Batch 10/64 loss: -0.10699772834777832
Batch 11/64 loss: -0.14436757564544678
Batch 12/64 loss: -0.14704596996307373
Batch 13/64 loss: -0.1397666335105896
Batch 14/64 loss: -0.11731302738189697
Batch 15/64 loss: -0.14233535528182983
Batch 16/64 loss: -0.10955142974853516
Batch 17/64 loss: -0.11103963851928711
Batch 18/64 loss: -0.14212864637374878
Batch 19/64 loss: -0.11872535943984985
Batch 20/64 loss: -0.11774373054504395
Batch 21/64 loss: -0.09970539808273315
Batch 22/64 loss: -0.13627612590789795
Batch 23/64 loss: -0.13219618797302246
Batch 24/64 loss: -0.13153380155563354
Batch 25/64 loss: -0.13668495416641235
Batch 26/64 loss: -0.10493975877761841
Batch 27/64 loss: -0.13953548669815063
Batch 28/64 loss: -0.11280596256256104
Batch 29/64 loss: -0.12331265211105347
Batch 30/64 loss: -0.13116931915283203
Batch 31/64 loss: -0.10382407903671265
Batch 32/64 loss: -0.11934846639633179
Batch 33/64 loss: -0.08891117572784424
Batch 34/64 loss: -0.1438502073287964
Batch 35/64 loss: -0.11616659164428711
Batch 36/64 loss: -0.13465666770935059
Batch 37/64 loss: -0.08921825885772705
Batch 38/64 loss: -0.09991580247879028
Batch 39/64 loss: -0.09859341382980347
Batch 40/64 loss: -0.10242563486099243
Batch 41/64 loss: -0.13041973114013672
Batch 42/64 loss: -0.09878313541412354
Batch 43/64 loss: -0.11420369148254395
Batch 44/64 loss: -0.11517906188964844
Batch 45/64 loss: -0.129505455493927
Batch 46/64 loss: -0.11410820484161377
Batch 47/64 loss: -0.10980993509292603
Batch 48/64 loss: -0.0981937050819397
Batch 49/64 loss: -0.09261417388916016
Batch 50/64 loss: -0.12732946872711182
Batch 51/64 loss: -0.09296810626983643
Batch 52/64 loss: -0.11089038848876953
Batch 53/64 loss: -0.1317247748374939
Batch 54/64 loss: -0.13408738374710083
Batch 55/64 loss: -0.13271665573120117
Batch 56/64 loss: -0.08891141414642334
Batch 57/64 loss: -0.11569660902023315
Batch 58/64 loss: -0.13109803199768066
Batch 59/64 loss: -0.1056404709815979
Batch 60/64 loss: -0.11721915006637573
Batch 61/64 loss: -0.12397819757461548
Batch 62/64 loss: -0.06909579038619995
Batch 63/64 loss: -0.11318838596343994
Batch 64/64 loss: -0.053951919078826904
Epoch 298  Train loss: -0.11537948239083383  Val loss: 0.05833340182746809
Epoch 299
-------------------------------
Batch 1/64 loss: -0.11651992797851562
Batch 2/64 loss: -0.10824227333068848
Batch 3/64 loss: -0.13241195678710938
Batch 4/64 loss: -0.13855737447738647
Batch 5/64 loss: -0.1438676118850708
Batch 6/64 loss: -0.1406552791595459
Batch 7/64 loss: -0.1276695728302002
Batch 8/64 loss: -0.12266194820404053
Batch 9/64 loss: -0.13587015867233276
Batch 10/64 loss: -0.11088109016418457
Batch 11/64 loss: -0.12533587217330933
Batch 12/64 loss: -0.10885590314865112
Batch 13/64 loss: -0.11250025033950806
Batch 14/64 loss: -0.11423999071121216
Batch 15/64 loss: -0.11987709999084473
Batch 16/64 loss: -0.0751391053199768
Batch 17/64 loss: -0.11990296840667725
Batch 18/64 loss: -0.10258805751800537
Batch 19/64 loss: -0.09271436929702759
Batch 20/64 loss: -0.08537518978118896
Batch 21/64 loss: -0.09096813201904297
Batch 22/64 loss: -0.09259706735610962
Batch 23/64 loss: -0.10615181922912598
Batch 24/64 loss: -0.12595093250274658
Batch 25/64 loss: -0.12006127834320068
Batch 26/64 loss: -0.08262115716934204
Batch 27/64 loss: -0.07559388875961304
Batch 28/64 loss: -0.14870399236679077
Batch 29/64 loss: -0.12282347679138184
Batch 30/64 loss: -0.13839757442474365
Batch 31/64 loss: -0.11754965782165527
Batch 32/64 loss: -0.1167832612991333
Batch 33/64 loss: -0.10871732234954834
Batch 34/64 loss: -0.13332045078277588
Batch 35/64 loss: -0.12041682004928589
Batch 36/64 loss: -0.09526783227920532
Batch 37/64 loss: -0.10620099306106567
Batch 38/64 loss: -0.12114500999450684
Batch 39/64 loss: -0.10535180568695068
Batch 40/64 loss: -0.12875163555145264
Batch 41/64 loss: -0.11806118488311768
Batch 42/64 loss: -0.06786322593688965
Batch 43/64 loss: -0.06729996204376221
Batch 44/64 loss: -0.13066428899765015
Batch 45/64 loss: -0.1420820951461792
Batch 46/64 loss: -0.11622440814971924
Batch 47/64 loss: -0.08113479614257812
Batch 48/64 loss: -0.10979169607162476
Batch 49/64 loss: -0.12730330228805542
Batch 50/64 loss: -0.14190685749053955
Batch 51/64 loss: -0.11628806591033936
Batch 52/64 loss: -0.10508769750595093
Batch 53/64 loss: -0.07927054166793823
Batch 54/64 loss: -0.11705213785171509
Batch 55/64 loss: -0.12855488061904907
Batch 56/64 loss: -0.10413676500320435
Batch 57/64 loss: -0.10866940021514893
Batch 58/64 loss: -0.11566722393035889
Batch 59/64 loss: -0.12976592779159546
Batch 60/64 loss: -0.10831260681152344
Batch 61/64 loss: -0.09328937530517578
Batch 62/64 loss: -0.1450924277305603
Batch 63/64 loss: -0.0977526307106018
Batch 64/64 loss: -0.11318844556808472
Epoch 299  Train loss: -0.11337102651596069  Val loss: 0.058380056492651454
Epoch 300
-------------------------------
Batch 1/64 loss: -0.1262214183807373
Batch 2/64 loss: -0.1333128809928894
Batch 3/64 loss: -0.10486471652984619
Batch 4/64 loss: -0.08191519975662231
Batch 5/64 loss: -0.10331308841705322
Batch 6/64 loss: -0.1163325309753418
Batch 7/64 loss: -0.1331768035888672
Batch 8/64 loss: -0.10786032676696777
Batch 9/64 loss: -0.12447190284729004
Batch 10/64 loss: -0.10908478498458862
Batch 11/64 loss: -0.09918636083602905
Batch 12/64 loss: -0.12176990509033203
Batch 13/64 loss: -0.11527514457702637
Batch 14/64 loss: -0.15180838108062744
Batch 15/64 loss: -0.10639846324920654
Batch 16/64 loss: -0.13211411237716675
Batch 17/64 loss: -0.07936012744903564
Batch 18/64 loss: -0.08986634016036987
Batch 19/64 loss: -0.12768375873565674
Batch 20/64 loss: -0.1197059154510498
Batch 21/64 loss: -0.13531196117401123
Batch 22/64 loss: -0.13580238819122314
Batch 23/64 loss: -0.09856688976287842
Batch 24/64 loss: -0.10465884208679199
Batch 25/64 loss: -0.10177415609359741
Batch 26/64 loss: -0.06865143775939941
Batch 27/64 loss: -0.1409938931465149
Batch 28/64 loss: -0.16201424598693848
Batch 29/64 loss: -0.11601805686950684
Batch 30/64 loss: -0.10756790637969971
Batch 31/64 loss: -0.11533284187316895
Batch 32/64 loss: -0.13426363468170166
Batch 33/64 loss: -0.10962283611297607
Batch 34/64 loss: -0.1168527603149414
Batch 35/64 loss: -0.10836225748062134
Batch 36/64 loss: -0.12898874282836914
Batch 37/64 loss: -0.10062336921691895
Batch 38/64 loss: -0.08019745349884033
Batch 39/64 loss: -0.1348809003829956
Batch 40/64 loss: -0.11220180988311768
Batch 41/64 loss: -0.15297937393188477
Batch 42/64 loss: -0.12856805324554443
Batch 43/64 loss: -0.11869591474533081
Batch 44/64 loss: -0.13019299507141113
Batch 45/64 loss: -0.1096910834312439
Batch 46/64 loss: -0.11024832725524902
Batch 47/64 loss: -0.15382373332977295
Batch 48/64 loss: -0.107868492603302
Batch 49/64 loss: -0.1095736026763916
Batch 50/64 loss: -0.10201960802078247
Batch 51/64 loss: -0.1074783205986023
Batch 52/64 loss: -0.07078540325164795
Batch 53/64 loss: -0.08566361665725708
Batch 54/64 loss: -0.100017249584198
Batch 55/64 loss: -0.09508728981018066
Batch 56/64 loss: -0.1001443862915039
Batch 57/64 loss: -0.11408400535583496
Batch 58/64 loss: -0.09526824951171875
Batch 59/64 loss: -0.0958939790725708
Batch 60/64 loss: -0.1176837682723999
Batch 61/64 loss: -0.10264897346496582
Batch 62/64 loss: -0.08888942003250122
Batch 63/64 loss: -0.06833851337432861
Batch 64/64 loss: -0.08758711814880371
Epoch 300  Train loss: -0.11180773716346891  Val loss: 0.06133695218161619
Epoch 301
-------------------------------
Batch 1/64 loss: -0.12569278478622437
Batch 2/64 loss: -0.11049574613571167
Batch 3/64 loss: -0.10502403974533081
Batch 4/64 loss: -0.08904719352722168
Batch 5/64 loss: -0.10285437107086182
Batch 6/64 loss: -0.08763980865478516
Batch 7/64 loss: -0.13628268241882324
Batch 8/64 loss: -0.11474806070327759
Batch 9/64 loss: -0.10587108135223389
Batch 10/64 loss: -0.1154026985168457
Batch 11/64 loss: -0.08349782228469849
Batch 12/64 loss: -0.12212347984313965
Batch 13/64 loss: -0.13498103618621826
Batch 14/64 loss: -0.11720788478851318
Batch 15/64 loss: -0.08261942863464355
Batch 16/64 loss: -0.08039325475692749
Batch 17/64 loss: -0.12029707431793213
Batch 18/64 loss: -0.11221128702163696
Batch 19/64 loss: -0.12849271297454834
Batch 20/64 loss: -0.11479455232620239
Batch 21/64 loss: -0.09000176191329956
Batch 22/64 loss: -0.12097769975662231
Batch 23/64 loss: -0.13296765089035034
Batch 24/64 loss: -0.1577635407447815
Batch 25/64 loss: -0.10683685541152954
Batch 26/64 loss: -0.12383896112442017
Batch 27/64 loss: -0.1292029619216919
Batch 28/64 loss: -0.10262572765350342
Batch 29/64 loss: -0.0850711464881897
Batch 30/64 loss: -0.083881676197052
Batch 31/64 loss: -0.13665461540222168
Batch 32/64 loss: -0.13329005241394043
Batch 33/64 loss: -0.11279529333114624
Batch 34/64 loss: -0.10286563634872437
Batch 35/64 loss: -0.072243332862854
Batch 36/64 loss: -0.12557172775268555
Batch 37/64 loss: -0.12236946821212769
Batch 38/64 loss: -0.11440026760101318
Batch 39/64 loss: -0.12889361381530762
Batch 40/64 loss: -0.14075833559036255
Batch 41/64 loss: -0.11590701341629028
Batch 42/64 loss: -0.10136079788208008
Batch 43/64 loss: -0.14924830198287964
Batch 44/64 loss: -0.0888485312461853
Batch 45/64 loss: -0.11967587471008301
Batch 46/64 loss: -0.14540106058120728
Batch 47/64 loss: -0.11549168825149536
Batch 48/64 loss: -0.09395545721054077
Batch 49/64 loss: -0.1249469518661499
Batch 50/64 loss: -0.12670159339904785
Batch 51/64 loss: -0.10797464847564697
Batch 52/64 loss: -0.10328489542007446
Batch 53/64 loss: -0.10050421953201294
Batch 54/64 loss: -0.07320481538772583
Batch 55/64 loss: -0.08319014310836792
Batch 56/64 loss: -0.09431838989257812
Batch 57/64 loss: -0.12178671360015869
Batch 58/64 loss: -0.10898196697235107
Batch 59/64 loss: -0.060530245304107666
Batch 60/64 loss: -0.1324588656425476
Batch 61/64 loss: -0.11965775489807129
Batch 62/64 loss: -0.1495426893234253
Batch 63/64 loss: -0.15565645694732666
Batch 64/64 loss: -0.09898453950881958
Epoch 301  Train loss: -0.1126204675319148  Val loss: 0.057423883697011624
Epoch 302
-------------------------------
Batch 1/64 loss: -0.1203923225402832
Batch 2/64 loss: -0.1298609972000122
Batch 3/64 loss: -0.1522396206855774
Batch 4/64 loss: -0.10815232992172241
Batch 5/64 loss: -0.15776848793029785
Batch 6/64 loss: -0.1180601716041565
Batch 7/64 loss: -0.13157951831817627
Batch 8/64 loss: -0.09707129001617432
Batch 9/64 loss: -0.1509404182434082
Batch 10/64 loss: -0.1465057134628296
Batch 11/64 loss: -0.11323881149291992
Batch 12/64 loss: -0.12333405017852783
Batch 13/64 loss: -0.13832062482833862
Batch 14/64 loss: -0.09632378816604614
Batch 15/64 loss: -0.13412946462631226
Batch 16/64 loss: -0.11447370052337646
Batch 17/64 loss: -0.10792380571365356
Batch 18/64 loss: -0.10382264852523804
Batch 19/64 loss: -0.12934339046478271
Batch 20/64 loss: -0.12491869926452637
Batch 21/64 loss: -0.06926590204238892
Batch 22/64 loss: -0.13867032527923584
Batch 23/64 loss: -0.12112206220626831
Batch 24/64 loss: -0.10717856884002686
Batch 25/64 loss: -0.12172502279281616
Batch 26/64 loss: -0.12736189365386963
Batch 27/64 loss: -0.12658274173736572
Batch 28/64 loss: -0.12701600790023804
Batch 29/64 loss: -0.1003795862197876
Batch 30/64 loss: -0.11105608940124512
Batch 31/64 loss: -0.12611019611358643
Batch 32/64 loss: -0.11111855506896973
Batch 33/64 loss: -0.08217859268188477
Batch 34/64 loss: -0.12245786190032959
Batch 35/64 loss: -0.10595989227294922
Batch 36/64 loss: -0.0746915340423584
Batch 37/64 loss: -0.1238861083984375
Batch 38/64 loss: -0.08563649654388428
Batch 39/64 loss: -0.14014863967895508
Batch 40/64 loss: -0.10100632905960083
Batch 41/64 loss: -0.09886783361434937
Batch 42/64 loss: -0.08372032642364502
Batch 43/64 loss: -0.150790274143219
Batch 44/64 loss: -0.11282593011856079
Batch 45/64 loss: -0.08920711278915405
Batch 46/64 loss: -0.11539983749389648
Batch 47/64 loss: -0.08335983753204346
Batch 48/64 loss: -0.12720149755477905
Batch 49/64 loss: -0.07257211208343506
Batch 50/64 loss: -0.09749329090118408
Batch 51/64 loss: -0.10862910747528076
Batch 52/64 loss: -0.10601502656936646
Batch 53/64 loss: -0.1117715835571289
Batch 54/64 loss: -0.1484965682029724
Batch 55/64 loss: -0.10898703336715698
Batch 56/64 loss: -0.11753654479980469
Batch 57/64 loss: -0.1366269588470459
Batch 58/64 loss: -0.08554834127426147
Batch 59/64 loss: -0.10848021507263184
Batch 60/64 loss: -0.059543490409851074
Batch 61/64 loss: -0.1329147219657898
Batch 62/64 loss: -0.1270955204963684
Batch 63/64 loss: -0.07208216190338135
Batch 64/64 loss: -0.10395938158035278
Epoch 302  Train loss: -0.11380528819327261  Val loss: 0.061652962284809126
Epoch 303
-------------------------------
Batch 1/64 loss: -0.13074815273284912
Batch 2/64 loss: -0.08302956819534302
Batch 3/64 loss: -0.10629987716674805
Batch 4/64 loss: -0.1292075514793396
Batch 5/64 loss: -0.11596840620040894
Batch 6/64 loss: -0.08241569995880127
Batch 7/64 loss: -0.10942381620407104
Batch 8/64 loss: -0.11186480522155762
Batch 9/64 loss: -0.10239946842193604
Batch 10/64 loss: -0.13699311017990112
Batch 11/64 loss: -0.11156094074249268
Batch 12/64 loss: -0.13175207376480103
Batch 13/64 loss: -0.11131972074508667
Batch 14/64 loss: -0.1411793828010559
Batch 15/64 loss: -0.11738461256027222
Batch 16/64 loss: -0.12262129783630371
Batch 17/64 loss: -0.1440032720565796
Batch 18/64 loss: -0.09539705514907837
Batch 19/64 loss: -0.11842960119247437
Batch 20/64 loss: -0.14020836353302002
Batch 21/64 loss: -0.1216469407081604
Batch 22/64 loss: -0.13611918687820435
Batch 23/64 loss: -0.12652671337127686
Batch 24/64 loss: -0.12093591690063477
Batch 25/64 loss: -0.09938931465148926
Batch 26/64 loss: -0.12733060121536255
Batch 27/64 loss: -0.07617777585983276
Batch 28/64 loss: -0.11205685138702393
Batch 29/64 loss: -0.1376258134841919
Batch 30/64 loss: -0.11516809463500977
Batch 31/64 loss: -0.08792310953140259
Batch 32/64 loss: -0.11897826194763184
Batch 33/64 loss: -0.12865018844604492
Batch 34/64 loss: -0.08701616525650024
Batch 35/64 loss: -0.05427253246307373
Batch 36/64 loss: -0.1180412769317627
Batch 37/64 loss: -0.116793692111969
Batch 38/64 loss: -0.06067168712615967
Batch 39/64 loss: -0.13013917207717896
Batch 40/64 loss: -0.12856775522232056
Batch 41/64 loss: -0.1256391406059265
Batch 42/64 loss: -0.14171874523162842
Batch 43/64 loss: -0.11807441711425781
Batch 44/64 loss: -0.0992850661277771
Batch 45/64 loss: -0.1454342007637024
Batch 46/64 loss: -0.11323368549346924
Batch 47/64 loss: -0.13523870706558228
Batch 48/64 loss: -0.05869412422180176
Batch 49/64 loss: -0.11048823595046997
Batch 50/64 loss: -0.11490809917449951
Batch 51/64 loss: -0.11541318893432617
Batch 52/64 loss: -0.13535481691360474
Batch 53/64 loss: -0.1262274980545044
Batch 54/64 loss: -0.06898438930511475
Batch 55/64 loss: -0.1467573046684265
Batch 56/64 loss: -0.05768442153930664
Batch 57/64 loss: -0.14496177434921265
Batch 58/64 loss: -0.12056875228881836
Batch 59/64 loss: -0.08998709917068481
Batch 60/64 loss: -0.13615453243255615
Batch 61/64 loss: -0.08585453033447266
Batch 62/64 loss: -0.15120601654052734
Batch 63/64 loss: -0.14156192541122437
Batch 64/64 loss: -0.13984906673431396
Epoch 303  Train loss: -0.11505184781317618  Val loss: 0.059967641773092786
Epoch 304
-------------------------------
Batch 1/64 loss: -0.1376284956932068
Batch 2/64 loss: -0.10746943950653076
Batch 3/64 loss: -0.1477947235107422
Batch 4/64 loss: -0.15006035566329956
Batch 5/64 loss: -0.1309828758239746
Batch 6/64 loss: -0.09876161813735962
Batch 7/64 loss: -0.12027907371520996
Batch 8/64 loss: -0.13973289728164673
Batch 9/64 loss: -0.1366479992866516
Batch 10/64 loss: -0.1509496569633484
Batch 11/64 loss: -0.07655006647109985
Batch 12/64 loss: -0.11355119943618774
Batch 13/64 loss: -0.12218880653381348
Batch 14/64 loss: -0.0999261736869812
Batch 15/64 loss: -0.1283860206604004
Batch 16/64 loss: -0.1456664800643921
Batch 17/64 loss: -0.1377599835395813
Batch 18/64 loss: -0.13257914781570435
Batch 19/64 loss: -0.16511398553848267
Batch 20/64 loss: -0.13693368434906006
Batch 21/64 loss: -0.10344648361206055
Batch 22/64 loss: -0.13519567251205444
Batch 23/64 loss: -0.11826902627944946
Batch 24/64 loss: -0.11193400621414185
Batch 25/64 loss: -0.12186014652252197
Batch 26/64 loss: -0.11758601665496826
Batch 27/64 loss: -0.12599122524261475
Batch 28/64 loss: -0.1211652159690857
Batch 29/64 loss: -0.1318703293800354
Batch 30/64 loss: -0.10397994518280029
Batch 31/64 loss: -0.1319025754928589
Batch 32/64 loss: -0.13983154296875
Batch 33/64 loss: -0.09906888008117676
Batch 34/64 loss: -0.11817705631256104
Batch 35/64 loss: -0.0987476110458374
Batch 36/64 loss: -0.12873005867004395
Batch 37/64 loss: -0.12622803449630737
Batch 38/64 loss: -0.1388474702835083
Batch 39/64 loss: -0.09699898958206177
Batch 40/64 loss: -0.13460302352905273
Batch 41/64 loss: -0.11794805526733398
Batch 42/64 loss: -0.13798624277114868
Batch 43/64 loss: -0.12962937355041504
Batch 44/64 loss: -0.1167258620262146
Batch 45/64 loss: -0.07427912950515747
Batch 46/64 loss: -0.0981827974319458
Batch 47/64 loss: -0.06096756458282471
Batch 48/64 loss: -0.14427244663238525
Batch 49/64 loss: -0.10721755027770996
Batch 50/64 loss: -0.111483633518219
Batch 51/64 loss: -0.10255789756774902
Batch 52/64 loss: -0.0853804349899292
Batch 53/64 loss: -0.11703962087631226
Batch 54/64 loss: -0.1060405969619751
Batch 55/64 loss: -0.09900468587875366
Batch 56/64 loss: -0.060473859310150146
Batch 57/64 loss: -0.11670422554016113
Batch 58/64 loss: -0.13866865634918213
Batch 59/64 loss: -0.1209520697593689
Batch 60/64 loss: -0.13165289163589478
Batch 61/64 loss: -0.09394717216491699
Batch 62/64 loss: -0.10528796911239624
Batch 63/64 loss: -0.1327962875366211
Batch 64/64 loss: -0.12994378805160522
Epoch 304  Train loss: -0.1190596526744319  Val loss: 0.0571988050880301
Epoch 305
-------------------------------
Batch 1/64 loss: -0.12943106889724731
Batch 2/64 loss: -0.1436154842376709
Batch 3/64 loss: -0.11307358741760254
Batch 4/64 loss: -0.12549495697021484
Batch 5/64 loss: -0.13423645496368408
Batch 6/64 loss: -0.13113081455230713
Batch 7/64 loss: -0.11692893505096436
Batch 8/64 loss: -0.1470416784286499
Batch 9/64 loss: -0.10925257205963135
Batch 10/64 loss: -0.15531563758850098
Batch 11/64 loss: -0.11521327495574951
Batch 12/64 loss: -0.10893809795379639
Batch 13/64 loss: -0.15397387742996216
Batch 14/64 loss: -0.12642627954483032
Batch 15/64 loss: -0.1366274356842041
Batch 16/64 loss: -0.13851141929626465
Batch 17/64 loss: -0.1140434741973877
Batch 18/64 loss: -0.09685856103897095
Batch 19/64 loss: -0.08580660820007324
Batch 20/64 loss: -0.09727871417999268
Batch 21/64 loss: -0.1100383996963501
Batch 22/64 loss: -0.10492098331451416
Batch 23/64 loss: -0.10163670778274536
Batch 24/64 loss: -0.10947024822235107
Batch 25/64 loss: -0.14940989017486572
Batch 26/64 loss: -0.14536255598068237
Batch 27/64 loss: -0.10206830501556396
Batch 28/64 loss: -0.09662085771560669
Batch 29/64 loss: -0.12887215614318848
Batch 30/64 loss: -0.11362028121948242
Batch 31/64 loss: -0.10237431526184082
Batch 32/64 loss: -0.104367196559906
Batch 33/64 loss: -0.08935153484344482
Batch 34/64 loss: -0.08669334650039673
Batch 35/64 loss: -0.09843456745147705
Batch 36/64 loss: -0.1228863000869751
Batch 37/64 loss: -0.1217273473739624
Batch 38/64 loss: -0.06387871503829956
Batch 39/64 loss: -0.11923080682754517
Batch 40/64 loss: -0.0972144603729248
Batch 41/64 loss: -0.13611727952957153
Batch 42/64 loss: -0.1518864631652832
Batch 43/64 loss: -0.09236305952072144
Batch 44/64 loss: -0.14721530675888062
Batch 45/64 loss: -0.11308121681213379
Batch 46/64 loss: -0.13910460472106934
Batch 47/64 loss: -0.08928024768829346
Batch 48/64 loss: -0.07896673679351807
Batch 49/64 loss: -0.1422814130783081
Batch 50/64 loss: -0.12933456897735596
Batch 51/64 loss: -0.10726267099380493
Batch 52/64 loss: -0.11182403564453125
Batch 53/64 loss: -0.10747051239013672
Batch 54/64 loss: -0.13021337985992432
Batch 55/64 loss: -0.1221928596496582
Batch 56/64 loss: -0.1151195764541626
Batch 57/64 loss: -0.1464666724205017
Batch 58/64 loss: -0.09155511856079102
Batch 59/64 loss: -0.1264960765838623
Batch 60/64 loss: -0.1272064447402954
Batch 61/64 loss: -0.10233324766159058
Batch 62/64 loss: -0.12776410579681396
Batch 63/64 loss: -0.0878976583480835
Batch 64/64 loss: -0.11006832122802734
Epoch 305  Train loss: -0.11691548871059043  Val loss: 0.0598152298288247
Epoch 306
-------------------------------
Batch 1/64 loss: -0.10123008489608765
Batch 2/64 loss: -0.10370796918869019
Batch 3/64 loss: -0.10425657033920288
Batch 4/64 loss: -0.12392443418502808
Batch 5/64 loss: -0.1605786681175232
Batch 6/64 loss: -0.12818235158920288
Batch 7/64 loss: -0.1190570592880249
Batch 8/64 loss: -0.1003081202507019
Batch 9/64 loss: -0.10773962736129761
Batch 10/64 loss: -0.12288457155227661
Batch 11/64 loss: -0.10103380680084229
Batch 12/64 loss: -0.09353023767471313
Batch 13/64 loss: -0.12561005353927612
Batch 14/64 loss: -0.09380817413330078
Batch 15/64 loss: -0.13560521602630615
Batch 16/64 loss: -0.11370313167572021
Batch 17/64 loss: -0.10899507999420166
Batch 18/64 loss: -0.12499558925628662
Batch 19/64 loss: -0.14126217365264893
Batch 20/64 loss: -0.10596466064453125
Batch 21/64 loss: -0.10549032688140869
Batch 22/64 loss: -0.1324450969696045
Batch 23/64 loss: -0.07271593809127808
Batch 24/64 loss: -0.12887924909591675
Batch 25/64 loss: -0.1575128436088562
Batch 26/64 loss: -0.14303350448608398
Batch 27/64 loss: -0.061269164085388184
Batch 28/64 loss: -0.13218355178833008
Batch 29/64 loss: -0.14436227083206177
Batch 30/64 loss: -0.1524316668510437
Batch 31/64 loss: -0.12653112411499023
Batch 32/64 loss: -0.11331748962402344
Batch 33/64 loss: -0.11082601547241211
Batch 34/64 loss: -0.13874459266662598
Batch 35/64 loss: -0.09344178438186646
Batch 36/64 loss: -0.12535953521728516
Batch 37/64 loss: -0.14195305109024048
Batch 38/64 loss: -0.1181173324584961
Batch 39/64 loss: -0.13557076454162598
Batch 40/64 loss: -0.12460553646087646
Batch 41/64 loss: -0.12420564889907837
Batch 42/64 loss: -0.12029582262039185
Batch 43/64 loss: -0.1277850866317749
Batch 44/64 loss: -0.09202808141708374
Batch 45/64 loss: -0.07806205749511719
Batch 46/64 loss: -0.1049034595489502
Batch 47/64 loss: -0.08783161640167236
Batch 48/64 loss: -0.11206549406051636
Batch 49/64 loss: -0.10131567716598511
Batch 50/64 loss: -0.09938597679138184
Batch 51/64 loss: -0.12472009658813477
Batch 52/64 loss: -0.09859716892242432
Batch 53/64 loss: -0.12641674280166626
Batch 54/64 loss: -0.1314939260482788
Batch 55/64 loss: -0.11880046129226685
Batch 56/64 loss: -0.09650266170501709
Batch 57/64 loss: -0.1102980375289917
Batch 58/64 loss: -0.1163177490234375
Batch 59/64 loss: -0.1293560266494751
Batch 60/64 loss: -0.1409006118774414
Batch 61/64 loss: -0.12255525588989258
Batch 62/64 loss: -0.13627523183822632
Batch 63/64 loss: -0.0947076678276062
Batch 64/64 loss: -0.10086536407470703
Epoch 306  Train loss: -0.11679432158376657  Val loss: 0.06153926837075617
Epoch 307
-------------------------------
Batch 1/64 loss: -0.13759273290634155
Batch 2/64 loss: -0.13389986753463745
Batch 3/64 loss: -0.12900304794311523
Batch 4/64 loss: -0.11774975061416626
Batch 5/64 loss: -0.144911527633667
Batch 6/64 loss: -0.1124843955039978
Batch 7/64 loss: -0.1309431791305542
Batch 8/64 loss: -0.12262451648712158
Batch 9/64 loss: -0.1421828269958496
Batch 10/64 loss: -0.1367969512939453
Batch 11/64 loss: -0.16598153114318848
Batch 12/64 loss: -0.11291545629501343
Batch 13/64 loss: -0.1409904956817627
Batch 14/64 loss: -0.11360585689544678
Batch 15/64 loss: -0.1203697919845581
Batch 16/64 loss: -0.10714542865753174
Batch 17/64 loss: -0.10735487937927246
Batch 18/64 loss: -0.16270285844802856
Batch 19/64 loss: -0.09556812047958374
Batch 20/64 loss: -0.11154913902282715
Batch 21/64 loss: -0.08778524398803711
Batch 22/64 loss: -0.08143919706344604
Batch 23/64 loss: -0.13361549377441406
Batch 24/64 loss: -0.1361238956451416
Batch 25/64 loss: -0.08539777994155884
Batch 26/64 loss: -0.135697603225708
Batch 27/64 loss: -0.125188410282135
Batch 28/64 loss: -0.1478564739227295
Batch 29/64 loss: -0.13532912731170654
Batch 30/64 loss: -0.11294037103652954
Batch 31/64 loss: -0.10843884944915771
Batch 32/64 loss: -0.10458165407180786
Batch 33/64 loss: -0.10683447122573853
Batch 34/64 loss: -0.10136818885803223
Batch 35/64 loss: -0.059565842151641846
Batch 36/64 loss: -0.10163843631744385
Batch 37/64 loss: -0.10636693239212036
Batch 38/64 loss: -0.10536056756973267
Batch 39/64 loss: -0.11498534679412842
Batch 40/64 loss: -0.11751186847686768
Batch 41/64 loss: -0.10839182138442993
Batch 42/64 loss: -0.09516066312789917
Batch 43/64 loss: -0.14629822969436646
Batch 44/64 loss: -0.14237534999847412
Batch 45/64 loss: -0.08041363954544067
Batch 46/64 loss: -0.12902206182479858
Batch 47/64 loss: -0.12333273887634277
Batch 48/64 loss: -0.12066060304641724
Batch 49/64 loss: -0.11342144012451172
Batch 50/64 loss: -0.12696242332458496
Batch 51/64 loss: -0.08106827735900879
Batch 52/64 loss: -0.12285542488098145
Batch 53/64 loss: -0.1249656081199646
Batch 54/64 loss: -0.10387492179870605
Batch 55/64 loss: -0.08868199586868286
Batch 56/64 loss: -0.10363900661468506
Batch 57/64 loss: -0.10693114995956421
Batch 58/64 loss: -0.13650590181350708
Batch 59/64 loss: -0.15262043476104736
Batch 60/64 loss: -0.11941993236541748
Batch 61/64 loss: -0.09673839807510376
Batch 62/64 loss: -0.09105449914932251
Batch 63/64 loss: -0.11368381977081299
Batch 64/64 loss: -0.08090484142303467
Epoch 307  Train loss: -0.11675537380517698  Val loss: 0.05996344896526271
Epoch 308
-------------------------------
Batch 1/64 loss: -0.14374887943267822
Batch 2/64 loss: -0.10031193494796753
Batch 3/64 loss: -0.13736635446548462
Batch 4/64 loss: -0.10732722282409668
Batch 5/64 loss: -0.10743725299835205
Batch 6/64 loss: -0.11793649196624756
Batch 7/64 loss: -0.1155664324760437
Batch 8/64 loss: -0.13016599416732788
Batch 9/64 loss: -0.1397220492362976
Batch 10/64 loss: -0.13504797220230103
Batch 11/64 loss: -0.12661904096603394
Batch 12/64 loss: -0.11068695783615112
Batch 13/64 loss: -0.13052153587341309
Batch 14/64 loss: -0.11098718643188477
Batch 15/64 loss: -0.11527931690216064
Batch 16/64 loss: -0.07460743188858032
Batch 17/64 loss: -0.12208104133605957
Batch 18/64 loss: -0.11412227153778076
Batch 19/64 loss: -0.1408647894859314
Batch 20/64 loss: -0.12213057279586792
Batch 21/64 loss: -0.1370530128479004
Batch 22/64 loss: -0.11057424545288086
Batch 23/64 loss: -0.15871447324752808
Batch 24/64 loss: -0.08687722682952881
Batch 25/64 loss: -0.15490657091140747
Batch 26/64 loss: -0.09422856569290161
Batch 27/64 loss: -0.09552818536758423
Batch 28/64 loss: -0.11262589693069458
Batch 29/64 loss: -0.08204525709152222
Batch 30/64 loss: -0.11069107055664062
Batch 31/64 loss: -0.13676512241363525
Batch 32/64 loss: -0.10100060701370239
Batch 33/64 loss: -0.12756973505020142
Batch 34/64 loss: -0.08546590805053711
Batch 35/64 loss: -0.10452097654342651
Batch 36/64 loss: -0.14184296131134033
Batch 37/64 loss: -0.09985852241516113
Batch 38/64 loss: -0.15806663036346436
Batch 39/64 loss: -0.15169423818588257
Batch 40/64 loss: -0.1139991283416748
Batch 41/64 loss: -0.12992262840270996
Batch 42/64 loss: -0.0538821816444397
Batch 43/64 loss: -0.11583966016769409
Batch 44/64 loss: -0.12199908494949341
Batch 45/64 loss: -0.12825065851211548
Batch 46/64 loss: -0.10218441486358643
Batch 47/64 loss: -0.0895986557006836
Batch 48/64 loss: -0.12404084205627441
Batch 49/64 loss: -0.10964888334274292
Batch 50/64 loss: -0.1165969967842102
Batch 51/64 loss: -0.11928647756576538
Batch 52/64 loss: -0.11717760562896729
Batch 53/64 loss: -0.14244508743286133
Batch 54/64 loss: -0.06786340475082397
Batch 55/64 loss: -0.10677981376647949
Batch 56/64 loss: -0.11331772804260254
Batch 57/64 loss: -0.1000247597694397
Batch 58/64 loss: -0.1339341402053833
Batch 59/64 loss: -0.12482696771621704
Batch 60/64 loss: -0.15040045976638794
Batch 61/64 loss: -0.10031843185424805
Batch 62/64 loss: -0.12962156534194946
Batch 63/64 loss: -0.12118571996688843
Batch 64/64 loss: -0.03294879198074341
Epoch 308  Train loss: -0.11624183257420857  Val loss: 0.06368387523795321
Epoch 309
-------------------------------
Batch 1/64 loss: -0.14418065547943115
Batch 2/64 loss: -0.12072479724884033
Batch 3/64 loss: -0.10084986686706543
Batch 4/64 loss: -0.09506821632385254
Batch 5/64 loss: -0.13182169198989868
Batch 6/64 loss: -0.062385618686676025
Batch 7/64 loss: -0.13599896430969238
Batch 8/64 loss: -0.101498544216156
Batch 9/64 loss: -0.12800490856170654
Batch 10/64 loss: -0.11783492565155029
Batch 11/64 loss: -0.10027730464935303
Batch 12/64 loss: -0.10757380723953247
Batch 13/64 loss: -0.13934540748596191
Batch 14/64 loss: -0.12768685817718506
Batch 15/64 loss: -0.10487359762191772
Batch 16/64 loss: -0.11334002017974854
Batch 17/64 loss: -0.13564985990524292
Batch 18/64 loss: -0.09086471796035767
Batch 19/64 loss: -0.14156222343444824
Batch 20/64 loss: -0.15872961282730103
Batch 21/64 loss: -0.1040845513343811
Batch 22/64 loss: -0.11889207363128662
Batch 23/64 loss: -0.14895260334014893
Batch 24/64 loss: -0.11847060918807983
Batch 25/64 loss: -0.11430054903030396
Batch 26/64 loss: -0.1395612359046936
Batch 27/64 loss: -0.11118721961975098
Batch 28/64 loss: -0.09310662746429443
Batch 29/64 loss: -0.10970467329025269
Batch 30/64 loss: -0.15839970111846924
Batch 31/64 loss: -0.10599285364151001
Batch 32/64 loss: -0.12005102634429932
Batch 33/64 loss: -0.10344052314758301
Batch 34/64 loss: -0.12583881616592407
Batch 35/64 loss: -0.14411723613739014
Batch 36/64 loss: -0.1343519687652588
Batch 37/64 loss: -0.12297999858856201
Batch 38/64 loss: -0.11942976713180542
Batch 39/64 loss: -0.115786612033844
Batch 40/64 loss: -0.11761242151260376
Batch 41/64 loss: -0.13184934854507446
Batch 42/64 loss: -0.07874715328216553
Batch 43/64 loss: -0.12441003322601318
Batch 44/64 loss: -0.15619683265686035
Batch 45/64 loss: -0.10743796825408936
Batch 46/64 loss: -0.1284998655319214
Batch 47/64 loss: -0.12704282999038696
Batch 48/64 loss: -0.12263500690460205
Batch 49/64 loss: -0.11793684959411621
Batch 50/64 loss: -0.14029991626739502
Batch 51/64 loss: -0.10922998189926147
Batch 52/64 loss: -0.1127740740776062
Batch 53/64 loss: -0.10805618762969971
Batch 54/64 loss: -0.14089059829711914
Batch 55/64 loss: -0.11521977186203003
Batch 56/64 loss: -0.1399780511856079
Batch 57/64 loss: -0.10013830661773682
Batch 58/64 loss: -0.09757059812545776
Batch 59/64 loss: -0.05630415678024292
Batch 60/64 loss: -0.07805311679840088
Batch 61/64 loss: -0.11953490972518921
Batch 62/64 loss: -0.11956840753555298
Batch 63/64 loss: -0.14394551515579224
Batch 64/64 loss: -0.07860183715820312
Epoch 309  Train loss: -0.11795770233752681  Val loss: 0.06762054367983054
Epoch 310
-------------------------------
Batch 1/64 loss: -0.1302328109741211
Batch 2/64 loss: -0.09599775075912476
Batch 3/64 loss: -0.09153330326080322
Batch 4/64 loss: -0.13675391674041748
Batch 5/64 loss: -0.12849575281143188
Batch 6/64 loss: -0.12733900547027588
Batch 7/64 loss: -0.09839648008346558
Batch 8/64 loss: -0.1253930926322937
Batch 9/64 loss: -0.12920045852661133
Batch 10/64 loss: -0.1564694046974182
Batch 11/64 loss: -0.1058344841003418
Batch 12/64 loss: -0.12842214107513428
Batch 13/64 loss: -0.11381584405899048
Batch 14/64 loss: -0.12083137035369873
Batch 15/64 loss: -0.0955512523651123
Batch 16/64 loss: -0.1403123140335083
Batch 17/64 loss: -0.1464146375656128
Batch 18/64 loss: -0.11500155925750732
Batch 19/64 loss: -0.09024739265441895
Batch 20/64 loss: -0.11181819438934326
Batch 21/64 loss: -0.12041354179382324
Batch 22/64 loss: -0.12849974632263184
Batch 23/64 loss: -0.13198721408843994
Batch 24/64 loss: -0.10646402835845947
Batch 25/64 loss: -0.14446675777435303
Batch 26/64 loss: -0.14698612689971924
Batch 27/64 loss: -0.05238604545593262
Batch 28/64 loss: -0.1539522409439087
Batch 29/64 loss: -0.13295406103134155
Batch 30/64 loss: -0.15669047832489014
Batch 31/64 loss: -0.14357119798660278
Batch 32/64 loss: -0.09281635284423828
Batch 33/64 loss: -0.12985104322433472
Batch 34/64 loss: -0.09559005498886108
Batch 35/64 loss: -0.11366808414459229
Batch 36/64 loss: -0.1159902811050415
Batch 37/64 loss: -0.11652415990829468
Batch 38/64 loss: -0.11031198501586914
Batch 39/64 loss: -0.10549044609069824
Batch 40/64 loss: -0.10784238576889038
Batch 41/64 loss: -0.14877921342849731
Batch 42/64 loss: -0.12346780300140381
Batch 43/64 loss: -0.161393940448761
Batch 44/64 loss: -0.1406537890434265
Batch 45/64 loss: -0.11741447448730469
Batch 46/64 loss: -0.12618625164031982
Batch 47/64 loss: -0.11952012777328491
Batch 48/64 loss: -0.06515103578567505
Batch 49/64 loss: -0.07588493824005127
Batch 50/64 loss: -0.13781917095184326
Batch 51/64 loss: -0.12591099739074707
Batch 52/64 loss: -0.12475299835205078
Batch 53/64 loss: -0.07955557107925415
Batch 54/64 loss: -0.1286146640777588
Batch 55/64 loss: -0.10711097717285156
Batch 56/64 loss: -0.11027860641479492
Batch 57/64 loss: -0.12137800455093384
Batch 58/64 loss: -0.13341420888900757
Batch 59/64 loss: -0.12323439121246338
Batch 60/64 loss: -0.10121417045593262
Batch 61/64 loss: -0.13393419981002808
Batch 62/64 loss: -0.09508651494979858
Batch 63/64 loss: -0.09691864252090454
Batch 64/64 loss: -0.143873929977417
Epoch 310  Train loss: -0.11921721626730526  Val loss: 0.0708001483346998
Epoch 311
-------------------------------
Batch 1/64 loss: -0.14585167169570923
Batch 2/64 loss: -0.1366068720817566
Batch 3/64 loss: -0.13043713569641113
Batch 4/64 loss: -0.08925658464431763
Batch 5/64 loss: -0.13203740119934082
Batch 6/64 loss: -0.09911859035491943
Batch 7/64 loss: -0.11337554454803467
Batch 8/64 loss: -0.1145775318145752
Batch 9/64 loss: -0.13736355304718018
Batch 10/64 loss: -0.12296676635742188
Batch 11/64 loss: -0.09171020984649658
Batch 12/64 loss: -0.14484578371047974
Batch 13/64 loss: -0.13722610473632812
Batch 14/64 loss: -0.13803178071975708
Batch 15/64 loss: -0.11285555362701416
Batch 16/64 loss: -0.13974368572235107
Batch 17/64 loss: -0.10964035987854004
Batch 18/64 loss: -0.13347160816192627
Batch 19/64 loss: -0.1141672134399414
Batch 20/64 loss: -0.12555044889450073
Batch 21/64 loss: -0.15581750869750977
Batch 22/64 loss: -0.08529913425445557
Batch 23/64 loss: -0.14394444227218628
Batch 24/64 loss: -0.13621878623962402
Batch 25/64 loss: -0.09852170944213867
Batch 26/64 loss: -0.06894445419311523
Batch 27/64 loss: -0.12451303005218506
Batch 28/64 loss: -0.09890049695968628
Batch 29/64 loss: -0.11490011215209961
Batch 30/64 loss: -0.1485007405281067
Batch 31/64 loss: -0.11969596147537231
Batch 32/64 loss: -0.08311444520950317
Batch 33/64 loss: -0.08303570747375488
Batch 34/64 loss: -0.14284205436706543
Batch 35/64 loss: -0.1591227650642395
Batch 36/64 loss: -0.14922666549682617
Batch 37/64 loss: -0.1341894268989563
Batch 38/64 loss: -0.11980009078979492
Batch 39/64 loss: -0.0787016749382019
Batch 40/64 loss: -0.10821378231048584
Batch 41/64 loss: -0.10846734046936035
Batch 42/64 loss: -0.15000134706497192
Batch 43/64 loss: -0.08259546756744385
Batch 44/64 loss: -0.08505499362945557
Batch 45/64 loss: -0.11687803268432617
Batch 46/64 loss: -0.11085188388824463
Batch 47/64 loss: -0.14107775688171387
Batch 48/64 loss: -0.12531977891921997
Batch 49/64 loss: -0.09939014911651611
Batch 50/64 loss: -0.11146295070648193
Batch 51/64 loss: -0.11227631568908691
Batch 52/64 loss: -0.11685383319854736
Batch 53/64 loss: -0.1395493745803833
Batch 54/64 loss: -0.10815894603729248
Batch 55/64 loss: -0.09569233655929565
Batch 56/64 loss: -0.13813233375549316
Batch 57/64 loss: -0.11724990606307983
Batch 58/64 loss: -0.08570903539657593
Batch 59/64 loss: -0.13842248916625977
Batch 60/64 loss: -0.1459183692932129
Batch 61/64 loss: -0.11529010534286499
Batch 62/64 loss: -0.1449223756790161
Batch 63/64 loss: -0.11258995532989502
Batch 64/64 loss: -0.06641483306884766
Epoch 311  Train loss: -0.11880805632647345  Val loss: 0.05727361751995545
Epoch 312
-------------------------------
Batch 1/64 loss: -0.12003737688064575
Batch 2/64 loss: -0.1476413607597351
Batch 3/64 loss: -0.13646721839904785
Batch 4/64 loss: -0.12297677993774414
Batch 5/64 loss: -0.14402633905410767
Batch 6/64 loss: -0.10981947183609009
Batch 7/64 loss: -0.1393311619758606
Batch 8/64 loss: -0.11706268787384033
Batch 9/64 loss: -0.1138342022895813
Batch 10/64 loss: -0.12263703346252441
Batch 11/64 loss: -0.1030583381652832
Batch 12/64 loss: -0.10263317823410034
Batch 13/64 loss: -0.1408754587173462
Batch 14/64 loss: -0.13137954473495483
Batch 15/64 loss: -0.13551342487335205
Batch 16/64 loss: -0.12777245044708252
Batch 17/64 loss: -0.10398024320602417
Batch 18/64 loss: -0.14401310682296753
Batch 19/64 loss: -0.13382554054260254
Batch 20/64 loss: -0.12377661466598511
Batch 21/64 loss: -0.12493622303009033
Batch 22/64 loss: -0.09996861219406128
Batch 23/64 loss: -0.09813487529754639
Batch 24/64 loss: -0.11717402935028076
Batch 25/64 loss: -0.149300217628479
Batch 26/64 loss: -0.11249333620071411
Batch 27/64 loss: -0.11737102270126343
Batch 28/64 loss: -0.10001391172409058
Batch 29/64 loss: -0.13608121871948242
Batch 30/64 loss: -0.12773114442825317
Batch 31/64 loss: -0.11844050884246826
Batch 32/64 loss: -0.09360694885253906
Batch 33/64 loss: -0.1394364833831787
Batch 34/64 loss: -0.11002457141876221
Batch 35/64 loss: -0.13629889488220215
Batch 36/64 loss: -0.09902167320251465
Batch 37/64 loss: -0.12166059017181396
Batch 38/64 loss: -0.13427633047103882
Batch 39/64 loss: -0.1240643858909607
Batch 40/64 loss: -0.13448554277420044
Batch 41/64 loss: -0.1206546425819397
Batch 42/64 loss: -0.1374586820602417
Batch 43/64 loss: -0.1459144949913025
Batch 44/64 loss: -0.15081387758255005
Batch 45/64 loss: -0.11467480659484863
Batch 46/64 loss: -0.11009222269058228
Batch 47/64 loss: -0.11413532495498657
Batch 48/64 loss: -0.056265413761138916
Batch 49/64 loss: -0.11910223960876465
Batch 50/64 loss: -0.12441539764404297
Batch 51/64 loss: -0.09447431564331055
Batch 52/64 loss: -0.14797228574752808
Batch 53/64 loss: -0.14442455768585205
Batch 54/64 loss: -0.12079733610153198
Batch 55/64 loss: -0.09318971633911133
Batch 56/64 loss: -0.08957570791244507
Batch 57/64 loss: -0.13864457607269287
Batch 58/64 loss: -0.11732840538024902
Batch 59/64 loss: -0.08627277612686157
Batch 60/64 loss: -0.13020098209381104
Batch 61/64 loss: -0.1324138641357422
Batch 62/64 loss: -0.14052510261535645
Batch 63/64 loss: -0.08060413599014282
Batch 64/64 loss: -0.09048569202423096
Epoch 312  Train loss: -0.12067446568432977  Val loss: 0.058995202644584105
Epoch 313
-------------------------------
Batch 1/64 loss: -0.11675715446472168
Batch 2/64 loss: -0.11003214120864868
Batch 3/64 loss: -0.06902337074279785
Batch 4/64 loss: -0.13012003898620605
Batch 5/64 loss: -0.1219136118888855
Batch 6/64 loss: -0.12327855825424194
Batch 7/64 loss: -0.12973004579544067
Batch 8/64 loss: -0.09834527969360352
Batch 9/64 loss: -0.16192108392715454
Batch 10/64 loss: -0.1423560380935669
Batch 11/64 loss: -0.13959360122680664
Batch 12/64 loss: -0.17519837617874146
Batch 13/64 loss: -0.09538370370864868
Batch 14/64 loss: -0.13703304529190063
Batch 15/64 loss: -0.1375752091407776
Batch 16/64 loss: -0.11970657110214233
Batch 17/64 loss: -0.12055474519729614
Batch 18/64 loss: -0.12820535898208618
Batch 19/64 loss: -0.11702656745910645
Batch 20/64 loss: -0.08771252632141113
Batch 21/64 loss: -0.13918697834014893
Batch 22/64 loss: -0.13555681705474854
Batch 23/64 loss: -0.09968376159667969
Batch 24/64 loss: -0.12675625085830688
Batch 25/64 loss: -0.1209949254989624
Batch 26/64 loss: -0.118965744972229
Batch 27/64 loss: -0.13383269309997559
Batch 28/64 loss: -0.13309723138809204
Batch 29/64 loss: -0.13988298177719116
Batch 30/64 loss: -0.10924869775772095
Batch 31/64 loss: -0.12025368213653564
Batch 32/64 loss: -0.1258513331413269
Batch 33/64 loss: -0.14967471361160278
Batch 34/64 loss: -0.10761904716491699
Batch 35/64 loss: -0.14790117740631104
Batch 36/64 loss: -0.09969347715377808
Batch 37/64 loss: -0.1220507025718689
Batch 38/64 loss: -0.11131834983825684
Batch 39/64 loss: -0.13541138172149658
Batch 40/64 loss: -0.13629454374313354
Batch 41/64 loss: -0.10674053430557251
Batch 42/64 loss: -0.11581993103027344
Batch 43/64 loss: -0.12509453296661377
Batch 44/64 loss: -0.12524878978729248
Batch 45/64 loss: -0.12133824825286865
Batch 46/64 loss: -0.13309848308563232
Batch 47/64 loss: -0.1564212441444397
Batch 48/64 loss: -0.11688011884689331
Batch 49/64 loss: -0.061402201652526855
Batch 50/64 loss: -0.12521636486053467
Batch 51/64 loss: -0.10887587070465088
Batch 52/64 loss: -0.14758634567260742
Batch 53/64 loss: -0.1023905873298645
Batch 54/64 loss: -0.09305787086486816
Batch 55/64 loss: -0.12257206439971924
Batch 56/64 loss: -0.09993988275527954
Batch 57/64 loss: -0.10873669385910034
Batch 58/64 loss: -0.14548230171203613
Batch 59/64 loss: -0.12610942125320435
Batch 60/64 loss: -0.12700378894805908
Batch 61/64 loss: -0.10740095376968384
Batch 62/64 loss: -0.07729947566986084
Batch 63/64 loss: -0.1153949499130249
Batch 64/64 loss: -0.10008913278579712
Epoch 313  Train loss: -0.12108108272739485  Val loss: 0.06222147056736897
Epoch 314
-------------------------------
Batch 1/64 loss: -0.07938826084136963
Batch 2/64 loss: -0.11453860998153687
Batch 3/64 loss: -0.13725590705871582
Batch 4/64 loss: -0.10101628303527832
Batch 5/64 loss: -0.1388363242149353
Batch 6/64 loss: -0.1306479573249817
Batch 7/64 loss: -0.12394070625305176
Batch 8/64 loss: -0.11106324195861816
Batch 9/64 loss: -0.09609240293502808
Batch 10/64 loss: -0.14191609621047974
Batch 11/64 loss: -0.14443105459213257
Batch 12/64 loss: -0.12855607271194458
Batch 13/64 loss: -0.11686903238296509
Batch 14/64 loss: -0.1180264949798584
Batch 15/64 loss: -0.13836872577667236
Batch 16/64 loss: -0.09595441818237305
Batch 17/64 loss: -0.12770575284957886
Batch 18/64 loss: -0.14044487476348877
Batch 19/64 loss: -0.1253749132156372
Batch 20/64 loss: -0.13287276029586792
Batch 21/64 loss: -0.11292374134063721
Batch 22/64 loss: -0.1370072364807129
Batch 23/64 loss: -0.1345847249031067
Batch 24/64 loss: -0.06016355752944946
Batch 25/64 loss: -0.12077975273132324
Batch 26/64 loss: -0.1222296953201294
Batch 27/64 loss: -0.15190720558166504
Batch 28/64 loss: -0.13920271396636963
Batch 29/64 loss: -0.12807601690292358
Batch 30/64 loss: -0.15776604413986206
Batch 31/64 loss: -0.15796172618865967
Batch 32/64 loss: -0.10683083534240723
Batch 33/64 loss: -0.09496510028839111
Batch 34/64 loss: -0.11814206838607788
Batch 35/64 loss: -0.12197959423065186
Batch 36/64 loss: -0.11595338582992554
Batch 37/64 loss: -0.1236257553100586
Batch 38/64 loss: -0.13244301080703735
Batch 39/64 loss: -0.09421265125274658
Batch 40/64 loss: -0.14480805397033691
Batch 41/64 loss: -0.10656988620758057
Batch 42/64 loss: -0.13243019580841064
Batch 43/64 loss: -0.14510178565979004
Batch 44/64 loss: -0.12314283847808838
Batch 45/64 loss: -0.06851023435592651
Batch 46/64 loss: -0.10945451259613037
Batch 47/64 loss: -0.114593505859375
Batch 48/64 loss: -0.11364555358886719
Batch 49/64 loss: -0.13038212060928345
Batch 50/64 loss: -0.10442030429840088
Batch 51/64 loss: -0.12855160236358643
Batch 52/64 loss: -0.09560763835906982
Batch 53/64 loss: -0.14400553703308105
Batch 54/64 loss: -0.11786520481109619
Batch 55/64 loss: -0.15145117044448853
Batch 56/64 loss: -0.1285828948020935
Batch 57/64 loss: -0.1332971453666687
Batch 58/64 loss: -0.1148979663848877
Batch 59/64 loss: -0.10163432359695435
Batch 60/64 loss: -0.14796966314315796
Batch 61/64 loss: -0.0977405309677124
Batch 62/64 loss: -0.09824550151824951
Batch 63/64 loss: -0.12095624208450317
Batch 64/64 loss: -0.1307225227355957
Epoch 314  Train loss: -0.12150527075225231  Val loss: 0.06051189383280646
Epoch 315
-------------------------------
Batch 1/64 loss: -0.09223896265029907
Batch 2/64 loss: -0.16193819046020508
Batch 3/64 loss: -0.1265661120414734
Batch 4/64 loss: -0.12140095233917236
Batch 5/64 loss: -0.08498251438140869
Batch 6/64 loss: -0.07499277591705322
Batch 7/64 loss: -0.15405267477035522
Batch 8/64 loss: -0.1283535361289978
Batch 9/64 loss: -0.07307219505310059
Batch 10/64 loss: -0.14862060546875
Batch 11/64 loss: -0.12946611642837524
Batch 12/64 loss: -0.12501859664916992
Batch 13/64 loss: -0.10920578241348267
Batch 14/64 loss: -0.09772038459777832
Batch 15/64 loss: -0.12031060457229614
Batch 16/64 loss: -0.1469796895980835
Batch 17/64 loss: -0.12212598323822021
Batch 18/64 loss: -0.12821310758590698
Batch 19/64 loss: -0.13651371002197266
Batch 20/64 loss: -0.10945004224777222
Batch 21/64 loss: -0.1055830717086792
Batch 22/64 loss: -0.12237942218780518
Batch 23/64 loss: -0.10745465755462646
Batch 24/64 loss: -0.10775989294052124
Batch 25/64 loss: -0.14013385772705078
Batch 26/64 loss: -0.12675416469573975
Batch 27/64 loss: -0.12411510944366455
Batch 28/64 loss: -0.15020906925201416
Batch 29/64 loss: -0.11606264114379883
Batch 30/64 loss: -0.14613217115402222
Batch 31/64 loss: -0.08225464820861816
Batch 32/64 loss: -0.15661585330963135
Batch 33/64 loss: -0.11087912321090698
Batch 34/64 loss: -0.12350022792816162
Batch 35/64 loss: -0.09634649753570557
Batch 36/64 loss: -0.12363183498382568
Batch 37/64 loss: -0.14159202575683594
Batch 38/64 loss: -0.1379566788673401
Batch 39/64 loss: -0.11353284120559692
Batch 40/64 loss: -0.10619568824768066
Batch 41/64 loss: -0.09852111339569092
Batch 42/64 loss: -0.14121174812316895
Batch 43/64 loss: -0.10932576656341553
Batch 44/64 loss: -0.11939311027526855
Batch 45/64 loss: -0.13067597150802612
Batch 46/64 loss: -0.10038226842880249
Batch 47/64 loss: -0.14201074838638306
Batch 48/64 loss: -0.10572433471679688
Batch 49/64 loss: -0.12879210710525513
Batch 50/64 loss: -0.12535810470581055
Batch 51/64 loss: -0.12963861227035522
Batch 52/64 loss: -0.10817211866378784
Batch 53/64 loss: -0.13836288452148438
Batch 54/64 loss: -0.12169492244720459
Batch 55/64 loss: -0.1111028790473938
Batch 56/64 loss: -0.11682045459747314
Batch 57/64 loss: -0.15480124950408936
Batch 58/64 loss: -0.11367297172546387
Batch 59/64 loss: -0.1389681100845337
Batch 60/64 loss: -0.10647010803222656
Batch 61/64 loss: -0.1182096004486084
Batch 62/64 loss: -0.12410497665405273
Batch 63/64 loss: -0.12536686658859253
Batch 64/64 loss: -0.15776431560516357
Epoch 315  Train loss: -0.12168496122547225  Val loss: 0.05656572469730967
Epoch 316
-------------------------------
Batch 1/64 loss: -0.1572035551071167
Batch 2/64 loss: -0.1193704605102539
Batch 3/64 loss: -0.12418067455291748
Batch 4/64 loss: -0.16963934898376465
Batch 5/64 loss: -0.13494890928268433
Batch 6/64 loss: -0.1211429238319397
Batch 7/64 loss: -0.12533700466156006
Batch 8/64 loss: -0.12891477346420288
Batch 9/64 loss: -0.12090784311294556
Batch 10/64 loss: -0.12402647733688354
Batch 11/64 loss: -0.11075574159622192
Batch 12/64 loss: -0.1338016390800476
Batch 13/64 loss: -0.09778326749801636
Batch 14/64 loss: -0.13202804327011108
Batch 15/64 loss: -0.135817289352417
Batch 16/64 loss: -0.11426210403442383
Batch 17/64 loss: -0.13860809803009033
Batch 18/64 loss: -0.14952820539474487
Batch 19/64 loss: -0.09079885482788086
Batch 20/64 loss: -0.10848188400268555
Batch 21/64 loss: -0.11929112672805786
Batch 22/64 loss: -0.12877881526947021
Batch 23/64 loss: -0.1014178991317749
Batch 24/64 loss: -0.10419511795043945
Batch 25/64 loss: -0.09961175918579102
Batch 26/64 loss: -0.09051656723022461
Batch 27/64 loss: -0.12581074237823486
Batch 28/64 loss: -0.11563324928283691
Batch 29/64 loss: -0.120278000831604
Batch 30/64 loss: -0.14424645900726318
Batch 31/64 loss: -0.13633406162261963
Batch 32/64 loss: -0.11292213201522827
Batch 33/64 loss: -0.15247434377670288
Batch 34/64 loss: -0.11542767286300659
Batch 35/64 loss: -0.11031240224838257
Batch 36/64 loss: -0.10202699899673462
Batch 37/64 loss: -0.12031567096710205
Batch 38/64 loss: -0.12352412939071655
Batch 39/64 loss: -0.14755582809448242
Batch 40/64 loss: -0.09189116954803467
Batch 41/64 loss: -0.1281512975692749
Batch 42/64 loss: -0.15503275394439697
Batch 43/64 loss: -0.12365394830703735
Batch 44/64 loss: -0.12189716100692749
Batch 45/64 loss: -0.1492266058921814
Batch 46/64 loss: -0.14742588996887207
Batch 47/64 loss: -0.1150122880935669
Batch 48/64 loss: -0.10971683263778687
Batch 49/64 loss: -0.08529460430145264
Batch 50/64 loss: -0.12195330858230591
Batch 51/64 loss: -0.12194538116455078
Batch 52/64 loss: -0.1082260012626648
Batch 53/64 loss: -0.13409721851348877
Batch 54/64 loss: -0.10136616230010986
Batch 55/64 loss: -0.12643814086914062
Batch 56/64 loss: -0.09654521942138672
Batch 57/64 loss: -0.09569138288497925
Batch 58/64 loss: -0.11028635501861572
Batch 59/64 loss: -0.08348739147186279
Batch 60/64 loss: -0.11451184749603271
Batch 61/64 loss: -0.12093263864517212
Batch 62/64 loss: -0.13044977188110352
Batch 63/64 loss: -0.11516118049621582
Batch 64/64 loss: -0.14957964420318604
Epoch 316  Train loss: -0.1212359115189197  Val loss: 0.05719799008156426
Epoch 317
-------------------------------
Batch 1/64 loss: -0.10980582237243652
Batch 2/64 loss: -0.11403119564056396
Batch 3/64 loss: -0.13121426105499268
Batch 4/64 loss: -0.14272594451904297
Batch 5/64 loss: -0.15439587831497192
Batch 6/64 loss: -0.15968871116638184
Batch 7/64 loss: -0.1854029893875122
Batch 8/64 loss: -0.1453806757926941
Batch 9/64 loss: -0.14673852920532227
Batch 10/64 loss: -0.14282864332199097
Batch 11/64 loss: -0.1362423300743103
Batch 12/64 loss: -0.10859471559524536
Batch 13/64 loss: -0.09592747688293457
Batch 14/64 loss: -0.1447398066520691
Batch 15/64 loss: -0.143052875995636
Batch 16/64 loss: -0.132213294506073
Batch 17/64 loss: -0.1398138403892517
Batch 18/64 loss: -0.13498789072036743
Batch 19/64 loss: -0.13906610012054443
Batch 20/64 loss: -0.14902514219284058
Batch 21/64 loss: -0.12583982944488525
Batch 22/64 loss: -0.07964658737182617
Batch 23/64 loss: -0.08137065172195435
Batch 24/64 loss: -0.10907268524169922
Batch 25/64 loss: -0.09167212247848511
Batch 26/64 loss: -0.08046644926071167
Batch 27/64 loss: -0.09474742412567139
Batch 28/64 loss: -0.11198997497558594
Batch 29/64 loss: -0.14375358819961548
Batch 30/64 loss: -0.12236320972442627
Batch 31/64 loss: -0.12758809328079224
Batch 32/64 loss: -0.09783971309661865
Batch 33/64 loss: -0.1305091381072998
Batch 34/64 loss: -0.15095603466033936
Batch 35/64 loss: -0.15084540843963623
Batch 36/64 loss: -0.09183818101882935
Batch 37/64 loss: -0.13117694854736328
Batch 38/64 loss: -0.128709614276886
Batch 39/64 loss: -0.10121685266494751
Batch 40/64 loss: -0.06956106424331665
Batch 41/64 loss: -0.14497101306915283
Batch 42/64 loss: -0.10324114561080933
Batch 43/64 loss: -0.0905846357345581
Batch 44/64 loss: -0.12127047777175903
Batch 45/64 loss: -0.12787610292434692
Batch 46/64 loss: -0.14089173078536987
Batch 47/64 loss: -0.11732655763626099
Batch 48/64 loss: -0.11246907711029053
Batch 49/64 loss: -0.12736552953720093
Batch 50/64 loss: -0.11998462677001953
Batch 51/64 loss: -0.12348073720932007
Batch 52/64 loss: -0.10500401258468628
Batch 53/64 loss: -0.11018437147140503
Batch 54/64 loss: -0.1189950704574585
Batch 55/64 loss: -0.10969418287277222
Batch 56/64 loss: -0.12390267848968506
Batch 57/64 loss: -0.1338205337524414
Batch 58/64 loss: -0.11451447010040283
Batch 59/64 loss: -0.1327940821647644
Batch 60/64 loss: -0.11249440908432007
Batch 61/64 loss: -0.12624013423919678
Batch 62/64 loss: -0.14163875579833984
Batch 63/64 loss: -0.0878913402557373
Batch 64/64 loss: -0.12102127075195312
Epoch 317  Train loss: -0.12257947173773073  Val loss: 0.057575519551935884
Epoch 318
-------------------------------
Batch 1/64 loss: -0.13897556066513062
Batch 2/64 loss: -0.14208167791366577
Batch 3/64 loss: -0.11967551708221436
Batch 4/64 loss: -0.11673200130462646
Batch 5/64 loss: -0.1520531177520752
Batch 6/64 loss: -0.127971351146698
Batch 7/64 loss: -0.15819400548934937
Batch 8/64 loss: -0.1132429838180542
Batch 9/64 loss: -0.13775086402893066
Batch 10/64 loss: -0.1059066653251648
Batch 11/64 loss: -0.11276435852050781
Batch 12/64 loss: -0.15354406833648682
Batch 13/64 loss: -0.12259632349014282
Batch 14/64 loss: -0.11805135011672974
Batch 15/64 loss: -0.12473464012145996
Batch 16/64 loss: -0.1474897861480713
Batch 17/64 loss: -0.10706943273544312
Batch 18/64 loss: -0.16964411735534668
Batch 19/64 loss: -0.10308647155761719
Batch 20/64 loss: -0.13465017080307007
Batch 21/64 loss: -0.11280077695846558
Batch 22/64 loss: -0.10239458084106445
Batch 23/64 loss: -0.12792372703552246
Batch 24/64 loss: -0.13195377588272095
Batch 25/64 loss: -0.1170535683631897
Batch 26/64 loss: -0.12599050998687744
Batch 27/64 loss: -0.1376267671585083
Batch 28/64 loss: -0.13594472408294678
Batch 29/64 loss: -0.12395942211151123
Batch 30/64 loss: -0.1448315978050232
Batch 31/64 loss: -0.13782137632369995
Batch 32/64 loss: -0.10348761081695557
Batch 33/64 loss: -0.12584435939788818
Batch 34/64 loss: -0.14024227857589722
Batch 35/64 loss: -0.13306081295013428
Batch 36/64 loss: -0.10094362497329712
Batch 37/64 loss: -0.123526930809021
Batch 38/64 loss: -0.10572820901870728
Batch 39/64 loss: -0.11235743761062622
Batch 40/64 loss: -0.13609099388122559
Batch 41/64 loss: -0.11037319898605347
Batch 42/64 loss: -0.10336190462112427
Batch 43/64 loss: -0.10662901401519775
Batch 44/64 loss: -0.124073326587677
Batch 45/64 loss: -0.0678168535232544
Batch 46/64 loss: -0.15348535776138306
Batch 47/64 loss: -0.12086856365203857
Batch 48/64 loss: -0.11877542734146118
Batch 49/64 loss: -0.13138294219970703
Batch 50/64 loss: -0.11787652969360352
Batch 51/64 loss: -0.10087603330612183
Batch 52/64 loss: -0.12496471405029297
Batch 53/64 loss: -0.14319539070129395
Batch 54/64 loss: -0.12288624048233032
Batch 55/64 loss: -0.12752825021743774
Batch 56/64 loss: -0.14737415313720703
Batch 57/64 loss: -0.10788750648498535
Batch 58/64 loss: -0.13601559400558472
Batch 59/64 loss: -0.11895328760147095
Batch 60/64 loss: -0.12602853775024414
Batch 61/64 loss: -0.13395828008651733
Batch 62/64 loss: -0.1481112241744995
Batch 63/64 loss: -0.13468486070632935
Batch 64/64 loss: -0.1321696639060974
Epoch 318  Train loss: -0.12567893313426598  Val loss: 0.06017616278527119
Epoch 319
-------------------------------
Batch 1/64 loss: -0.15504240989685059
Batch 2/64 loss: -0.10140889883041382
Batch 3/64 loss: -0.12336993217468262
Batch 4/64 loss: -0.09305059909820557
Batch 5/64 loss: -0.14921784400939941
Batch 6/64 loss: -0.08193552494049072
Batch 7/64 loss: -0.11498421430587769
Batch 8/64 loss: -0.14163005352020264
Batch 9/64 loss: -0.11112856864929199
Batch 10/64 loss: -0.12679481506347656
Batch 11/64 loss: -0.1031343936920166
Batch 12/64 loss: -0.13317662477493286
Batch 13/64 loss: -0.15914052724838257
Batch 14/64 loss: -0.1039743423461914
Batch 15/64 loss: -0.15668481588363647
Batch 16/64 loss: -0.12299484014511108
Batch 17/64 loss: -0.11176347732543945
Batch 18/64 loss: -0.12316519021987915
Batch 19/64 loss: -0.14637666940689087
Batch 20/64 loss: -0.05161786079406738
Batch 21/64 loss: -0.14534538984298706
Batch 22/64 loss: -0.0892641544342041
Batch 23/64 loss: -0.14835870265960693
Batch 24/64 loss: -0.12994110584259033
Batch 25/64 loss: -0.1314542293548584
Batch 26/64 loss: -0.13235855102539062
Batch 27/64 loss: -0.08946549892425537
Batch 28/64 loss: -0.12207931280136108
Batch 29/64 loss: -0.11507308483123779
Batch 30/64 loss: -0.14955079555511475
Batch 31/64 loss: -0.1076475977897644
Batch 32/64 loss: -0.14763939380645752
Batch 33/64 loss: -0.13677674531936646
Batch 34/64 loss: -0.1365903615951538
Batch 35/64 loss: -0.12506794929504395
Batch 36/64 loss: -0.1154794692993164
Batch 37/64 loss: -0.1474820375442505
Batch 38/64 loss: -0.11093932390213013
Batch 39/64 loss: -0.13038474321365356
Batch 40/64 loss: -0.14171189069747925
Batch 41/64 loss: -0.1315932273864746
Batch 42/64 loss: -0.08402258157730103
Batch 43/64 loss: -0.13683217763900757
Batch 44/64 loss: -0.11188369989395142
Batch 45/64 loss: -0.09161186218261719
Batch 46/64 loss: -0.12162035703659058
Batch 47/64 loss: -0.10880941152572632
Batch 48/64 loss: -0.09116286039352417
Batch 49/64 loss: -0.11945104598999023
Batch 50/64 loss: -0.1059449315071106
Batch 51/64 loss: -0.1336715817451477
Batch 52/64 loss: -0.11199241876602173
Batch 53/64 loss: -0.0837026834487915
Batch 54/64 loss: -0.13184863328933716
Batch 55/64 loss: -0.16090631484985352
Batch 56/64 loss: -0.13905376195907593
Batch 57/64 loss: -0.10726237297058105
Batch 58/64 loss: -0.10889291763305664
Batch 59/64 loss: -0.12393814325332642
Batch 60/64 loss: -0.1294715404510498
Batch 61/64 loss: -0.098876953125
Batch 62/64 loss: -0.157007098197937
Batch 63/64 loss: -0.1484575867652893
Batch 64/64 loss: -0.15563398599624634
Epoch 319  Train loss: -0.12263437790029189  Val loss: 0.059288310021469275
Epoch 320
-------------------------------
Batch 1/64 loss: -0.1495787501335144
Batch 2/64 loss: -0.11619538068771362
Batch 3/64 loss: -0.14621156454086304
Batch 4/64 loss: -0.1365429162979126
Batch 5/64 loss: -0.0919644832611084
Batch 6/64 loss: -0.1533968448638916
Batch 7/64 loss: -0.12569671869277954
Batch 8/64 loss: -0.12691622972488403
Batch 9/64 loss: -0.17755931615829468
Batch 10/64 loss: -0.11606144905090332
Batch 11/64 loss: -0.13542407751083374
Batch 12/64 loss: -0.1594935655593872
Batch 13/64 loss: -0.11866271495819092
Batch 14/64 loss: -0.11750757694244385
Batch 15/64 loss: -0.09330856800079346
Batch 16/64 loss: -0.12320321798324585
Batch 17/64 loss: -0.1484038233757019
Batch 18/64 loss: -0.12974488735198975
Batch 19/64 loss: -0.06722313165664673
Batch 20/64 loss: -0.11066919565200806
Batch 21/64 loss: -0.12364792823791504
Batch 22/64 loss: -0.12087899446487427
Batch 23/64 loss: -0.12924140691757202
Batch 24/64 loss: -0.10543990135192871
Batch 25/64 loss: -0.11827903985977173
Batch 26/64 loss: -0.13280773162841797
Batch 27/64 loss: -0.1460900902748108
Batch 28/64 loss: -0.13160592317581177
Batch 29/64 loss: -0.07171595096588135
Batch 30/64 loss: -0.11208927631378174
Batch 31/64 loss: -0.11982637643814087
Batch 32/64 loss: -0.14802277088165283
Batch 33/64 loss: -0.1508200764656067
Batch 34/64 loss: -0.08760541677474976
Batch 35/64 loss: -0.10798412561416626
Batch 36/64 loss: -0.14012295007705688
Batch 37/64 loss: -0.12933099269866943
Batch 38/64 loss: -0.09908068180084229
Batch 39/64 loss: -0.1145944595336914
Batch 40/64 loss: -0.13828188180923462
Batch 41/64 loss: -0.10740715265274048
Batch 42/64 loss: -0.08571398258209229
Batch 43/64 loss: -0.13522619009017944
Batch 44/64 loss: -0.1336767077445984
Batch 45/64 loss: -0.12746059894561768
Batch 46/64 loss: -0.13452351093292236
Batch 47/64 loss: -0.15715831518173218
Batch 48/64 loss: -0.129450261592865
Batch 49/64 loss: -0.1180030107498169
Batch 50/64 loss: -0.08081609010696411
Batch 51/64 loss: -0.1404394507408142
Batch 52/64 loss: -0.0872507095336914
Batch 53/64 loss: -0.10121232271194458
Batch 54/64 loss: -0.14582377672195435
Batch 55/64 loss: -0.12943637371063232
Batch 56/64 loss: -0.07958924770355225
Batch 57/64 loss: -0.12254881858825684
Batch 58/64 loss: -0.09794968366622925
Batch 59/64 loss: -0.12834256887435913
Batch 60/64 loss: -0.11147993803024292
Batch 61/64 loss: -0.10429775714874268
Batch 62/64 loss: -0.09488391876220703
Batch 63/64 loss: -0.13280940055847168
Batch 64/64 loss: -0.13652962446212769
Epoch 320  Train loss: -0.12171180224886127  Val loss: 0.08040203773688614
Epoch 321
-------------------------------
Batch 1/64 loss: -0.1322525143623352
Batch 2/64 loss: -0.17416346073150635
Batch 3/64 loss: -0.1451479196548462
Batch 4/64 loss: -0.1563282608985901
Batch 5/64 loss: -0.13963377475738525
Batch 6/64 loss: -0.134074866771698
Batch 7/64 loss: -0.13616138696670532
Batch 8/64 loss: -0.11879301071166992
Batch 9/64 loss: -0.14538300037384033
Batch 10/64 loss: -0.15350449085235596
Batch 11/64 loss: -0.14476537704467773
Batch 12/64 loss: -0.13989275693893433
Batch 13/64 loss: -0.12309247255325317
Batch 14/64 loss: -0.10267412662506104
Batch 15/64 loss: -0.13241463899612427
Batch 16/64 loss: -0.11718273162841797
Batch 17/64 loss: -0.12818646430969238
Batch 18/64 loss: -0.13219082355499268
Batch 19/64 loss: -0.11706340312957764
Batch 20/64 loss: -0.14160192012786865
Batch 21/64 loss: -0.11604130268096924
Batch 22/64 loss: -0.13608133792877197
Batch 23/64 loss: -0.11969304084777832
Batch 24/64 loss: -0.11588352918624878
Batch 25/64 loss: -0.12265288829803467
Batch 26/64 loss: -0.069649338722229
Batch 27/64 loss: -0.11748123168945312
Batch 28/64 loss: -0.09528142213821411
Batch 29/64 loss: -0.11831319332122803
Batch 30/64 loss: -0.0783536434173584
Batch 31/64 loss: -0.12548673152923584
Batch 32/64 loss: -0.12603777647018433
Batch 33/64 loss: -0.07480812072753906
Batch 34/64 loss: -0.14030981063842773
Batch 35/64 loss: -0.09890180826187134
Batch 36/64 loss: -0.1345476508140564
Batch 37/64 loss: -0.13527196645736694
Batch 38/64 loss: -0.1391887664794922
Batch 39/64 loss: -0.13174426555633545
Batch 40/64 loss: -0.13761919736862183
Batch 41/64 loss: -0.09826630353927612
Batch 42/64 loss: -0.1197504997253418
Batch 43/64 loss: -0.12838369607925415
Batch 44/64 loss: -0.12688928842544556
Batch 45/64 loss: -0.1376054883003235
Batch 46/64 loss: -0.08999490737915039
Batch 47/64 loss: -0.123587965965271
Batch 48/64 loss: -0.10317254066467285
Batch 49/64 loss: -0.14928162097930908
Batch 50/64 loss: -0.10577225685119629
Batch 51/64 loss: -0.12148892879486084
Batch 52/64 loss: -0.07762956619262695
Batch 53/64 loss: -0.1076386570930481
Batch 54/64 loss: -0.10197657346725464
Batch 55/64 loss: -0.11955064535140991
Batch 56/64 loss: -0.11735033988952637
Batch 57/64 loss: -0.09398806095123291
Batch 58/64 loss: -0.09560424089431763
Batch 59/64 loss: -0.11442345380783081
Batch 60/64 loss: -0.14031153917312622
Batch 61/64 loss: -0.12710487842559814
Batch 62/64 loss: -0.12166279554367065
Batch 63/64 loss: -0.11819708347320557
Batch 64/64 loss: -0.12396758794784546
Epoch 321  Train loss: -0.12204645404628679  Val loss: 0.07550733154991648
Epoch 322
-------------------------------
Batch 1/64 loss: -0.11364823579788208
Batch 2/64 loss: -0.1061849594116211
Batch 3/64 loss: -0.15126633644104004
Batch 4/64 loss: -0.12474638223648071
Batch 5/64 loss: -0.13459348678588867
Batch 6/64 loss: -0.11118823289871216
Batch 7/64 loss: -0.10969811677932739
Batch 8/64 loss: -0.11897826194763184
Batch 9/64 loss: -0.15494436025619507
Batch 10/64 loss: -0.06243431568145752
Batch 11/64 loss: -0.12754368782043457
Batch 12/64 loss: -0.11511075496673584
Batch 13/64 loss: -0.13173335790634155
Batch 14/64 loss: -0.10127300024032593
Batch 15/64 loss: -0.17819422483444214
Batch 16/64 loss: -0.1261305809020996
Batch 17/64 loss: -0.1277233362197876
Batch 18/64 loss: -0.1265745759010315
Batch 19/64 loss: -0.11624294519424438
Batch 20/64 loss: -0.12311077117919922
Batch 21/64 loss: -0.1584610939025879
Batch 22/64 loss: -0.09994375705718994
Batch 23/64 loss: -0.13861489295959473
Batch 24/64 loss: -0.12258309125900269
Batch 25/64 loss: -0.16022604703903198
Batch 26/64 loss: -0.13885748386383057
Batch 27/64 loss: -0.1259998083114624
Batch 28/64 loss: -0.12194019556045532
Batch 29/64 loss: -0.12746524810791016
Batch 30/64 loss: -0.17677265405654907
Batch 31/64 loss: -0.09022903442382812
Batch 32/64 loss: -0.08102953433990479
Batch 33/64 loss: -0.12706893682479858
Batch 34/64 loss: -0.11861103773117065
Batch 35/64 loss: -0.14886689186096191
Batch 36/64 loss: -0.11579960584640503
Batch 37/64 loss: -0.1144484281539917
Batch 38/64 loss: -0.14021259546279907
Batch 39/64 loss: -0.13347375392913818
Batch 40/64 loss: -0.14025729894638062
Batch 41/64 loss: -0.08526694774627686
Batch 42/64 loss: -0.14543116092681885
Batch 43/64 loss: -0.14749222993850708
Batch 44/64 loss: -0.15015339851379395
Batch 45/64 loss: -0.09844785928726196
Batch 46/64 loss: -0.14630228281021118
Batch 47/64 loss: -0.09771448373794556
Batch 48/64 loss: -0.09969091415405273
Batch 49/64 loss: -0.10443413257598877
Batch 50/64 loss: -0.14092570543289185
Batch 51/64 loss: -0.10338377952575684
Batch 52/64 loss: -0.13368606567382812
Batch 53/64 loss: -0.08210635185241699
Batch 54/64 loss: -0.12861400842666626
Batch 55/64 loss: -0.11287689208984375
Batch 56/64 loss: -0.11301815509796143
Batch 57/64 loss: -0.08465737104415894
Batch 58/64 loss: -0.141809344291687
Batch 59/64 loss: -0.11085402965545654
Batch 60/64 loss: -0.11332172155380249
Batch 61/64 loss: -0.13499712944030762
Batch 62/64 loss: -0.10880762338638306
Batch 63/64 loss: -0.13671064376831055
Batch 64/64 loss: -0.09804672002792358
Epoch 322  Train loss: -0.12292421308218264  Val loss: 0.05971532548006458
Epoch 323
-------------------------------
Batch 1/64 loss: -0.11362719535827637
Batch 2/64 loss: -0.09204757213592529
Batch 3/64 loss: -0.11425012350082397
Batch 4/64 loss: -0.11263716220855713
Batch 5/64 loss: -0.1301267147064209
Batch 6/64 loss: -0.11999738216400146
Batch 7/64 loss: -0.1468411684036255
Batch 8/64 loss: -0.14398342370986938
Batch 9/64 loss: -0.09776568412780762
Batch 10/64 loss: -0.14069753885269165
Batch 11/64 loss: -0.14809483289718628
Batch 12/64 loss: -0.15215706825256348
Batch 13/64 loss: -0.14936912059783936
Batch 14/64 loss: -0.1375090479850769
Batch 15/64 loss: -0.07866156101226807
Batch 16/64 loss: -0.07419776916503906
Batch 17/64 loss: -0.13094007968902588
Batch 18/64 loss: -0.12617576122283936
Batch 19/64 loss: -0.1471642255783081
Batch 20/64 loss: -0.13405007123947144
Batch 21/64 loss: -0.12472891807556152
Batch 22/64 loss: -0.11126977205276489
Batch 23/64 loss: -0.1244816780090332
Batch 24/64 loss: -0.11692118644714355
Batch 25/64 loss: -0.13626229763031006
Batch 26/64 loss: -0.12518393993377686
Batch 27/64 loss: -0.1346150040626526
Batch 28/64 loss: -0.12290084362030029
Batch 29/64 loss: -0.11000895500183105
Batch 30/64 loss: -0.06518232822418213
Batch 31/64 loss: -0.12109339237213135
Batch 32/64 loss: -0.1344205141067505
Batch 33/64 loss: -0.1354568600654602
Batch 34/64 loss: -0.1280716061592102
Batch 35/64 loss: -0.13428789377212524
Batch 36/64 loss: -0.11545002460479736
Batch 37/64 loss: -0.14712005853652954
Batch 38/64 loss: -0.09462732076644897
Batch 39/64 loss: -0.12770867347717285
Batch 40/64 loss: -0.10772472620010376
Batch 41/64 loss: -0.11367392539978027
Batch 42/64 loss: -0.13817048072814941
Batch 43/64 loss: -0.14245402812957764
Batch 44/64 loss: -0.1533321738243103
Batch 45/64 loss: -0.14492571353912354
Batch 46/64 loss: -0.12273699045181274
Batch 47/64 loss: -0.11837446689605713
Batch 48/64 loss: -0.09429800510406494
Batch 49/64 loss: -0.13226157426834106
Batch 50/64 loss: -0.06655752658843994
Batch 51/64 loss: -0.14129853248596191
Batch 52/64 loss: -0.13485658168792725
Batch 53/64 loss: -0.11752688884735107
Batch 54/64 loss: -0.10173207521438599
Batch 55/64 loss: -0.12860870361328125
Batch 56/64 loss: -0.14020341634750366
Batch 57/64 loss: -0.1686486005783081
Batch 58/64 loss: -0.12857377529144287
Batch 59/64 loss: -0.12149560451507568
Batch 60/64 loss: -0.12225961685180664
Batch 61/64 loss: -0.10341870784759521
Batch 62/64 loss: -0.11449611186981201
Batch 63/64 loss: -0.12836384773254395
Batch 64/64 loss: -0.12072843313217163
Epoch 323  Train loss: -0.123554873232748  Val loss: 0.060990677461591376
Epoch 324
-------------------------------
Batch 1/64 loss: -0.1124352216720581
Batch 2/64 loss: -0.11607050895690918
Batch 3/64 loss: -0.16061615943908691
Batch 4/64 loss: -0.11757540702819824
Batch 5/64 loss: -0.13858038187026978
Batch 6/64 loss: -0.17600500583648682
Batch 7/64 loss: -0.09591299295425415
Batch 8/64 loss: -0.1323743462562561
Batch 9/64 loss: -0.12293320894241333
Batch 10/64 loss: -0.15880191326141357
Batch 11/64 loss: -0.14843976497650146
Batch 12/64 loss: -0.13899177312850952
Batch 13/64 loss: -0.10933858156204224
Batch 14/64 loss: -0.05436420440673828
Batch 15/64 loss: -0.12078869342803955
Batch 16/64 loss: -0.15898782014846802
Batch 17/64 loss: -0.14970993995666504
Batch 18/64 loss: -0.13903605937957764
Batch 19/64 loss: -0.12545883655548096
Batch 20/64 loss: -0.12796807289123535
Batch 21/64 loss: -0.10800683498382568
Batch 22/64 loss: -0.14014166593551636
Batch 23/64 loss: -0.13282155990600586
Batch 24/64 loss: -0.14628762006759644
Batch 25/64 loss: -0.13314813375473022
Batch 26/64 loss: -0.125485360622406
Batch 27/64 loss: -0.12178504467010498
Batch 28/64 loss: -0.09212452173233032
Batch 29/64 loss: -0.10100400447845459
Batch 30/64 loss: -0.13473063707351685
Batch 31/64 loss: -0.0749582052230835
Batch 32/64 loss: -0.050086379051208496
Batch 33/64 loss: -0.1273481249809265
Batch 34/64 loss: -0.10229623317718506
Batch 35/64 loss: -0.13817346096038818
Batch 36/64 loss: -0.13944298028945923
Batch 37/64 loss: -0.10662317276000977
Batch 38/64 loss: -0.12582457065582275
Batch 39/64 loss: -0.14823424816131592
Batch 40/64 loss: -0.11593657732009888
Batch 41/64 loss: -0.1021735668182373
Batch 42/64 loss: -0.13313984870910645
Batch 43/64 loss: -0.1352275013923645
Batch 44/64 loss: -0.13980185985565186
Batch 45/64 loss: -0.1103440523147583
Batch 46/64 loss: -0.10744935274124146
Batch 47/64 loss: -0.12583309412002563
Batch 48/64 loss: -0.06815922260284424
Batch 49/64 loss: -0.12174707651138306
Batch 50/64 loss: -0.1287965178489685
Batch 51/64 loss: -0.11375659704208374
Batch 52/64 loss: -0.13311368227005005
Batch 53/64 loss: -0.11612963676452637
Batch 54/64 loss: -0.12015193700790405
Batch 55/64 loss: -0.13601791858673096
Batch 56/64 loss: -0.12527376413345337
Batch 57/64 loss: -0.1186453104019165
Batch 58/64 loss: -0.13331061601638794
Batch 59/64 loss: -0.13837820291519165
Batch 60/64 loss: -0.13112640380859375
Batch 61/64 loss: -0.15474379062652588
Batch 62/64 loss: -0.11938130855560303
Batch 63/64 loss: -0.1317213773727417
Batch 64/64 loss: -0.1634417176246643
Epoch 324  Train loss: -0.12448395537395103  Val loss: 0.05751083376481361
Epoch 325
-------------------------------
Batch 1/64 loss: -0.12632429599761963
Batch 2/64 loss: -0.15943491458892822
Batch 3/64 loss: -0.13128072023391724
Batch 4/64 loss: -0.13974934816360474
Batch 5/64 loss: -0.12420898675918579
Batch 6/64 loss: -0.13016414642333984
Batch 7/64 loss: -0.12740159034729004
Batch 8/64 loss: -0.10305458307266235
Batch 9/64 loss: -0.11758697032928467
Batch 10/64 loss: -0.13767558336257935
Batch 11/64 loss: -0.13419389724731445
Batch 12/64 loss: -0.14994746446609497
Batch 13/64 loss: -0.11426281929016113
Batch 14/64 loss: -0.1546257734298706
Batch 15/64 loss: -0.12166154384613037
Batch 16/64 loss: -0.12092489004135132
Batch 17/64 loss: -0.113436758518219
Batch 18/64 loss: -0.12304389476776123
Batch 19/64 loss: -0.07464998960494995
Batch 20/64 loss: -0.11355167627334595
Batch 21/64 loss: -0.13812261819839478
Batch 22/64 loss: -0.14572668075561523
Batch 23/64 loss: -0.12911677360534668
Batch 24/64 loss: -0.12049704790115356
Batch 25/64 loss: -0.09704852104187012
Batch 26/64 loss: -0.11311221122741699
Batch 27/64 loss: -0.12370479106903076
Batch 28/64 loss: -0.11168628931045532
Batch 29/64 loss: -0.12283974885940552
Batch 30/64 loss: -0.15128761529922485
Batch 31/64 loss: -0.12469136714935303
Batch 32/64 loss: -0.11689233779907227
Batch 33/64 loss: -0.1014014482498169
Batch 34/64 loss: -0.13515257835388184
Batch 35/64 loss: -0.13022923469543457
Batch 36/64 loss: -0.13904225826263428
Batch 37/64 loss: -0.10729110240936279
Batch 38/64 loss: -0.14533108472824097
Batch 39/64 loss: -0.11454498767852783
Batch 40/64 loss: -0.12508153915405273
Batch 41/64 loss: -0.14177024364471436
Batch 42/64 loss: -0.11921745538711548
Batch 43/64 loss: -0.11770504713058472
Batch 44/64 loss: -0.09969884157180786
Batch 45/64 loss: -0.05128920078277588
Batch 46/64 loss: -0.1493726372718811
Batch 47/64 loss: -0.11048221588134766
Batch 48/64 loss: -0.10759365558624268
Batch 49/64 loss: -0.1455174684524536
Batch 50/64 loss: -0.1436811089515686
Batch 51/64 loss: -0.13545143604278564
Batch 52/64 loss: -0.1372777223587036
Batch 53/64 loss: -0.11012011766433716
Batch 54/64 loss: -0.12674754858016968
Batch 55/64 loss: -0.14261400699615479
Batch 56/64 loss: -0.11576974391937256
Batch 57/64 loss: -0.1174423098564148
Batch 58/64 loss: -0.10369491577148438
Batch 59/64 loss: -0.1185150146484375
Batch 60/64 loss: -0.09616893529891968
Batch 61/64 loss: -0.12986165285110474
Batch 62/64 loss: -0.11956185102462769
Batch 63/64 loss: -0.10782843828201294
Batch 64/64 loss: -0.12017655372619629
Epoch 325  Train loss: -0.1230979461295932  Val loss: 0.059254558430504554
Epoch 326
-------------------------------
Batch 1/64 loss: -0.08801847696304321
Batch 2/64 loss: -0.12958812713623047
Batch 3/64 loss: -0.11093592643737793
Batch 4/64 loss: -0.1242135763168335
Batch 5/64 loss: -0.06997352838516235
Batch 6/64 loss: -0.09055495262145996
Batch 7/64 loss: -0.1156148910522461
Batch 8/64 loss: -0.11272990703582764
Batch 9/64 loss: -0.12339895963668823
Batch 10/64 loss: -0.13984531164169312
Batch 11/64 loss: -0.12427031993865967
Batch 12/64 loss: -0.13978290557861328
Batch 13/64 loss: -0.1532173752784729
Batch 14/64 loss: -0.1190725564956665
Batch 15/64 loss: -0.08746397495269775
Batch 16/64 loss: -0.14639002084732056
Batch 17/64 loss: -0.1516045331954956
Batch 18/64 loss: -0.16113775968551636
Batch 19/64 loss: -0.13278263807296753
Batch 20/64 loss: -0.1445620059967041
Batch 21/64 loss: -0.1318526268005371
Batch 22/64 loss: -0.1806178092956543
Batch 23/64 loss: -0.11738526821136475
Batch 24/64 loss: -0.16004043817520142
Batch 25/64 loss: -0.0932111144065857
Batch 26/64 loss: -0.13699287176132202
Batch 27/64 loss: -0.1518995761871338
Batch 28/64 loss: -0.1173853874206543
Batch 29/64 loss: -0.13715893030166626
Batch 30/64 loss: -0.10774260759353638
Batch 31/64 loss: -0.14930438995361328
Batch 32/64 loss: -0.12666785717010498
Batch 33/64 loss: -0.14131629467010498
Batch 34/64 loss: -0.12653380632400513
Batch 35/64 loss: -0.15280157327651978
Batch 36/64 loss: -0.13935959339141846
Batch 37/64 loss: -0.12965726852416992
Batch 38/64 loss: -0.13918399810791016
Batch 39/64 loss: -0.11141842603683472
Batch 40/64 loss: -0.17004692554473877
Batch 41/64 loss: -0.1256486177444458
Batch 42/64 loss: -0.14406228065490723
Batch 43/64 loss: -0.15122854709625244
Batch 44/64 loss: -0.14609748125076294
Batch 45/64 loss: -0.12099987268447876
Batch 46/64 loss: -0.0948989987373352
Batch 47/64 loss: -0.13252556324005127
Batch 48/64 loss: -0.10307270288467407
Batch 49/64 loss: -0.16186219453811646
Batch 50/64 loss: -0.055612921714782715
Batch 51/64 loss: -0.12833666801452637
Batch 52/64 loss: -0.1818481683731079
Batch 53/64 loss: -0.09935891628265381
Batch 54/64 loss: -0.11802536249160767
Batch 55/64 loss: -0.14905887842178345
Batch 56/64 loss: -0.11600250005722046
Batch 57/64 loss: -0.15934699773788452
Batch 58/64 loss: -0.10198652744293213
Batch 59/64 loss: -0.1224740743637085
Batch 60/64 loss: -0.12720108032226562
Batch 61/64 loss: -0.10760551691055298
Batch 62/64 loss: -0.10400748252868652
Batch 63/64 loss: -0.11166912317276001
Batch 64/64 loss: -0.10862427949905396
Epoch 326  Train loss: -0.1275315011248869  Val loss: 0.05648952353860914
Epoch 327
-------------------------------
Batch 1/64 loss: -0.1276618242263794
Batch 2/64 loss: -0.13368964195251465
Batch 3/64 loss: -0.13070076704025269
Batch 4/64 loss: -0.14726245403289795
Batch 5/64 loss: -0.13065868616104126
Batch 6/64 loss: -0.13843345642089844
Batch 7/64 loss: -0.1065482497215271
Batch 8/64 loss: -0.12413865327835083
Batch 9/64 loss: -0.11963164806365967
Batch 10/64 loss: -0.1153835654258728
Batch 11/64 loss: -0.11688464879989624
Batch 12/64 loss: -0.13027578592300415
Batch 13/64 loss: -0.11052048206329346
Batch 14/64 loss: -0.12736135721206665
Batch 15/64 loss: -0.16051054000854492
Batch 16/64 loss: -0.15275198221206665
Batch 17/64 loss: -0.12862616777420044
Batch 18/64 loss: -0.13167661428451538
Batch 19/64 loss: -0.09289497137069702
Batch 20/64 loss: -0.14633512496948242
Batch 21/64 loss: -0.14302170276641846
Batch 22/64 loss: -0.14440959692001343
Batch 23/64 loss: -0.07379549741744995
Batch 24/64 loss: -0.11520743370056152
Batch 25/64 loss: -0.10839974880218506
Batch 26/64 loss: -0.13831907510757446
Batch 27/64 loss: -0.13198626041412354
Batch 28/64 loss: -0.1382465362548828
Batch 29/64 loss: -0.1503298282623291
Batch 30/64 loss: -0.13581383228302002
Batch 31/64 loss: -0.10613203048706055
Batch 32/64 loss: -0.09248197078704834
Batch 33/64 loss: -0.08713114261627197
Batch 34/64 loss: -0.11974465847015381
Batch 35/64 loss: -0.1374613642692566
Batch 36/64 loss: -0.1043403148651123
Batch 37/64 loss: -0.1286267638206482
Batch 38/64 loss: -0.12193357944488525
Batch 39/64 loss: -0.1255418062210083
Batch 40/64 loss: -0.14240121841430664
Batch 41/64 loss: -0.12744009494781494
Batch 42/64 loss: -0.12969154119491577
Batch 43/64 loss: -0.16040420532226562
Batch 44/64 loss: -0.10803264379501343
Batch 45/64 loss: -0.10327792167663574
Batch 46/64 loss: -0.12017089128494263
Batch 47/64 loss: -0.11818921566009521
Batch 48/64 loss: -0.13298076391220093
Batch 49/64 loss: -0.14411789178848267
Batch 50/64 loss: -0.11705327033996582
Batch 51/64 loss: -0.15464723110198975
Batch 52/64 loss: -0.12330067157745361
Batch 53/64 loss: -0.12983107566833496
Batch 54/64 loss: -0.1504395604133606
Batch 55/64 loss: -0.1325257420539856
Batch 56/64 loss: -0.13434332609176636
Batch 57/64 loss: -0.12311917543411255
Batch 58/64 loss: -0.13448143005371094
Batch 59/64 loss: -0.11681485176086426
Batch 60/64 loss: -0.10562998056411743
Batch 61/64 loss: -0.13105559349060059
Batch 62/64 loss: -0.1493338942527771
Batch 63/64 loss: -0.1686546802520752
Batch 64/64 loss: -0.0789685845375061
Epoch 327  Train loss: -0.1269338521302915  Val loss: 0.05862171666319018
Epoch 328
-------------------------------
Batch 1/64 loss: -0.13877946138381958
Batch 2/64 loss: -0.1540440320968628
Batch 3/64 loss: -0.15060114860534668
Batch 4/64 loss: -0.1175459623336792
Batch 5/64 loss: -0.12890374660491943
Batch 6/64 loss: -0.13417953252792358
Batch 7/64 loss: -0.14611560106277466
Batch 8/64 loss: -0.1316131353378296
Batch 9/64 loss: -0.13472700119018555
Batch 10/64 loss: -0.17251670360565186
Batch 11/64 loss: -0.11956679821014404
Batch 12/64 loss: -0.15201878547668457
Batch 13/64 loss: -0.13498061895370483
Batch 14/64 loss: -0.15514087677001953
Batch 15/64 loss: -0.15890216827392578
Batch 16/64 loss: -0.12232398986816406
Batch 17/64 loss: -0.14737027883529663
Batch 18/64 loss: -0.07357829809188843
Batch 19/64 loss: -0.07170450687408447
Batch 20/64 loss: -0.14597350358963013
Batch 21/64 loss: -0.13889813423156738
Batch 22/64 loss: -0.10363298654556274
Batch 23/64 loss: -0.10832232236862183
Batch 24/64 loss: -0.10254883766174316
Batch 25/64 loss: -0.10636711120605469
Batch 26/64 loss: -0.14088064432144165
Batch 27/64 loss: -0.1041942834854126
Batch 28/64 loss: -0.12653005123138428
Batch 29/64 loss: -0.1438543200492859
Batch 30/64 loss: -0.11699169874191284
Batch 31/64 loss: -0.14747518301010132
Batch 32/64 loss: -0.13157367706298828
Batch 33/64 loss: -0.15226709842681885
Batch 34/64 loss: -0.14596951007843018
Batch 35/64 loss: -0.11209148168563843
Batch 36/64 loss: -0.07518875598907471
Batch 37/64 loss: -0.12376880645751953
Batch 38/64 loss: -0.09198153018951416
Batch 39/64 loss: -0.13091522455215454
Batch 40/64 loss: -0.13548427820205688
Batch 41/64 loss: -0.12404400110244751
Batch 42/64 loss: -0.13914430141448975
Batch 43/64 loss: -0.13381850719451904
Batch 44/64 loss: -0.15362519025802612
Batch 45/64 loss: -0.15966665744781494
Batch 46/64 loss: -0.13128173351287842
Batch 47/64 loss: -0.13745659589767456
Batch 48/64 loss: -0.13103938102722168
Batch 49/64 loss: -0.0966331958770752
Batch 50/64 loss: -0.12976545095443726
Batch 51/64 loss: -0.12614136934280396
Batch 52/64 loss: -0.13012248277664185
Batch 53/64 loss: -0.1239507794380188
Batch 54/64 loss: -0.10857957601547241
Batch 55/64 loss: -0.12176263332366943
Batch 56/64 loss: -0.13144689798355103
Batch 57/64 loss: -0.12556880712509155
Batch 58/64 loss: -0.12803435325622559
Batch 59/64 loss: -0.11241203546524048
Batch 60/64 loss: -0.1599147915840149
Batch 61/64 loss: -0.0935521125793457
Batch 62/64 loss: -0.12965112924575806
Batch 63/64 loss: -0.07510185241699219
Batch 64/64 loss: -0.11835604906082153
Epoch 328  Train loss: -0.12738828168195837  Val loss: 0.06286600979742725
Epoch 329
-------------------------------
Batch 1/64 loss: -0.13718920946121216
Batch 2/64 loss: -0.14706486463546753
Batch 3/64 loss: -0.12623870372772217
Batch 4/64 loss: -0.1435178518295288
Batch 5/64 loss: -0.13650089502334595
Batch 6/64 loss: -0.14829283952713013
Batch 7/64 loss: -0.14952534437179565
Batch 8/64 loss: -0.11332720518112183
Batch 9/64 loss: -0.08463931083679199
Batch 10/64 loss: -0.13530892133712769
Batch 11/64 loss: -0.10460460186004639
Batch 12/64 loss: -0.0950310230255127
Batch 13/64 loss: -0.09591060876846313
Batch 14/64 loss: -0.09058457612991333
Batch 15/64 loss: -0.11119085550308228
Batch 16/64 loss: -0.13275039196014404
Batch 17/64 loss: -0.13248848915100098
Batch 18/64 loss: -0.1113511323928833
Batch 19/64 loss: -0.11411607265472412
Batch 20/64 loss: -0.11267054080963135
Batch 21/64 loss: -0.157464861869812
Batch 22/64 loss: -0.11641746759414673
Batch 23/64 loss: -0.13060015439987183
Batch 24/64 loss: -0.10807746648788452
Batch 25/64 loss: -0.11785584688186646
Batch 26/64 loss: -0.11873447895050049
Batch 27/64 loss: -0.10811817646026611
Batch 28/64 loss: -0.10787302255630493
Batch 29/64 loss: -0.09422767162322998
Batch 30/64 loss: -0.11292338371276855
Batch 31/64 loss: -0.13215786218643188
Batch 32/64 loss: -0.09089308977127075
Batch 33/64 loss: -0.09086817502975464
Batch 34/64 loss: -0.14692288637161255
Batch 35/64 loss: -0.11008292436599731
Batch 36/64 loss: -0.0688098669052124
Batch 37/64 loss: -0.08688473701477051
Batch 38/64 loss: -0.09979081153869629
Batch 39/64 loss: -0.13960671424865723
Batch 40/64 loss: -0.11047494411468506
Batch 41/64 loss: -0.12016761302947998
Batch 42/64 loss: -0.1522918939590454
Batch 43/64 loss: -0.15232717990875244
Batch 44/64 loss: -0.12357252836227417
Batch 45/64 loss: -0.12318360805511475
Batch 46/64 loss: -0.1664140224456787
Batch 47/64 loss: -0.12477362155914307
Batch 48/64 loss: -0.10926234722137451
Batch 49/64 loss: -0.1516486406326294
Batch 50/64 loss: -0.1548159122467041
Batch 51/64 loss: -0.1331270933151245
Batch 52/64 loss: -0.12536698579788208
Batch 53/64 loss: -0.13609397411346436
Batch 54/64 loss: -0.12316566705703735
Batch 55/64 loss: -0.11951178312301636
Batch 56/64 loss: -0.10626786947250366
Batch 57/64 loss: -0.16290134191513062
Batch 58/64 loss: -0.11504894495010376
Batch 59/64 loss: -0.14571189880371094
Batch 60/64 loss: -0.13749176263809204
Batch 61/64 loss: -0.135087251663208
Batch 62/64 loss: -0.13202422857284546
Batch 63/64 loss: -0.13323593139648438
Batch 64/64 loss: -0.11646556854248047
Epoch 329  Train loss: -0.12301065501044778  Val loss: 0.05587261546518385
Epoch 330
-------------------------------
Batch 1/64 loss: -0.13118058443069458
Batch 2/64 loss: -0.14516812562942505
Batch 3/64 loss: -0.11049008369445801
Batch 4/64 loss: -0.125419020652771
Batch 5/64 loss: -0.10403186082839966
Batch 6/64 loss: -0.16038274765014648
Batch 7/64 loss: -0.1449487805366516
Batch 8/64 loss: -0.12800240516662598
Batch 9/64 loss: -0.12819743156433105
Batch 10/64 loss: -0.13187938928604126
Batch 11/64 loss: -0.12966758012771606
Batch 12/64 loss: -0.11178213357925415
Batch 13/64 loss: -0.16332244873046875
Batch 14/64 loss: -0.14015275239944458
Batch 15/64 loss: -0.11118531227111816
Batch 16/64 loss: -0.10782265663146973
Batch 17/64 loss: -0.12263333797454834
Batch 18/64 loss: -0.12305432558059692
Batch 19/64 loss: -0.09970319271087646
Batch 20/64 loss: -0.12702107429504395
Batch 21/64 loss: -0.10771095752716064
Batch 22/64 loss: -0.15976709127426147
Batch 23/64 loss: -0.14039409160614014
Batch 24/64 loss: -0.13408416509628296
Batch 25/64 loss: -0.15321457386016846
Batch 26/64 loss: -0.09127593040466309
Batch 27/64 loss: -0.15371835231781006
Batch 28/64 loss: -0.10856199264526367
Batch 29/64 loss: -0.12012147903442383
Batch 30/64 loss: -0.12993347644805908
Batch 31/64 loss: -0.16570758819580078
Batch 32/64 loss: -0.11727285385131836
Batch 33/64 loss: -0.14624285697937012
Batch 34/64 loss: -0.11455690860748291
Batch 35/64 loss: -0.12259042263031006
Batch 36/64 loss: -0.12349551916122437
Batch 37/64 loss: -0.12784451246261597
Batch 38/64 loss: -0.10882985591888428
Batch 39/64 loss: -0.10997623205184937
Batch 40/64 loss: -0.11571240425109863
Batch 41/64 loss: -0.12535858154296875
Batch 42/64 loss: -0.09250277280807495
Batch 43/64 loss: -0.10962682962417603
Batch 44/64 loss: -0.09765344858169556
Batch 45/64 loss: -0.15598106384277344
Batch 46/64 loss: -0.09852838516235352
Batch 47/64 loss: -0.13487493991851807
Batch 48/64 loss: -0.107169508934021
Batch 49/64 loss: -0.11572349071502686
Batch 50/64 loss: -0.13873672485351562
Batch 51/64 loss: -0.13260382413864136
Batch 52/64 loss: -0.13468605279922485
Batch 53/64 loss: -0.11247861385345459
Batch 54/64 loss: -0.13216328620910645
Batch 55/64 loss: -0.11897051334381104
Batch 56/64 loss: -0.16135162115097046
Batch 57/64 loss: -0.14092683792114258
Batch 58/64 loss: -0.1608215570449829
Batch 59/64 loss: -0.1569817066192627
Batch 60/64 loss: -0.09501767158508301
Batch 61/64 loss: -0.1309680938720703
Batch 62/64 loss: -0.10666817426681519
Batch 63/64 loss: -0.15522700548171997
Batch 64/64 loss: -0.1387205719947815
Epoch 330  Train loss: -0.12731164927576102  Val loss: 0.058926228805096285
Epoch 331
-------------------------------
Batch 1/64 loss: -0.12317007780075073
Batch 2/64 loss: -0.12330126762390137
Batch 3/64 loss: -0.16353774070739746
Batch 4/64 loss: -0.1467795968055725
Batch 5/64 loss: -0.1348632574081421
Batch 6/64 loss: -0.12197059392929077
Batch 7/64 loss: -0.12113016843795776
Batch 8/64 loss: -0.08762067556381226
Batch 9/64 loss: -0.12433600425720215
Batch 10/64 loss: -0.1581045389175415
Batch 11/64 loss: -0.14333516359329224
Batch 12/64 loss: -0.1257677674293518
Batch 13/64 loss: -0.12760818004608154
Batch 14/64 loss: -0.11866956949234009
Batch 15/64 loss: -0.14445024728775024
Batch 16/64 loss: -0.13586413860321045
Batch 17/64 loss: -0.11976516246795654
Batch 18/64 loss: -0.15120166540145874
Batch 19/64 loss: -0.1258564591407776
Batch 20/64 loss: -0.1450575590133667
Batch 21/64 loss: -0.15708696842193604
Batch 22/64 loss: -0.13585728406906128
Batch 23/64 loss: -0.1079666018486023
Batch 24/64 loss: -0.14260220527648926
Batch 25/64 loss: -0.11638045310974121
Batch 26/64 loss: -0.13371062278747559
Batch 27/64 loss: -0.1391926407814026
Batch 28/64 loss: -0.14670675992965698
Batch 29/64 loss: -0.13227081298828125
Batch 30/64 loss: -0.1170952320098877
Batch 31/64 loss: -0.16116583347320557
Batch 32/64 loss: -0.11566305160522461
Batch 33/64 loss: -0.12233024835586548
Batch 34/64 loss: -0.09892189502716064
Batch 35/64 loss: -0.1510237455368042
Batch 36/64 loss: -0.11586344242095947
Batch 37/64 loss: -0.07270616292953491
Batch 38/64 loss: -0.14323651790618896
Batch 39/64 loss: -0.13485479354858398
Batch 40/64 loss: -0.13652193546295166
Batch 41/64 loss: -0.168520987033844
Batch 42/64 loss: -0.10565632581710815
Batch 43/64 loss: -0.1595921516418457
Batch 44/64 loss: -0.13343197107315063
Batch 45/64 loss: -0.10842627286911011
Batch 46/64 loss: -0.153969407081604
Batch 47/64 loss: -0.10743629932403564
Batch 48/64 loss: -0.11497241258621216
Batch 49/64 loss: -0.13397067785263062
Batch 50/64 loss: -0.14027303457260132
Batch 51/64 loss: -0.10561102628707886
Batch 52/64 loss: -0.13587987422943115
Batch 53/64 loss: -0.11612427234649658
Batch 54/64 loss: -0.12384110689163208
Batch 55/64 loss: -0.1462225317955017
Batch 56/64 loss: -0.14556050300598145
Batch 57/64 loss: -0.11481207609176636
Batch 58/64 loss: -0.13919401168823242
Batch 59/64 loss: -0.10623413324356079
Batch 60/64 loss: -0.12853896617889404
Batch 61/64 loss: -0.12600523233413696
Batch 62/64 loss: -0.13034898042678833
Batch 63/64 loss: -0.14017391204833984
Batch 64/64 loss: -0.1312733292579651
Epoch 331  Train loss: -0.1303027169377196  Val loss: 0.0566218122583894
Epoch 332
-------------------------------
Batch 1/64 loss: -0.1728765368461609
Batch 2/64 loss: -0.1582515835762024
Batch 3/64 loss: -0.15510857105255127
Batch 4/64 loss: -0.09898954629898071
Batch 5/64 loss: -0.13856685161590576
Batch 6/64 loss: -0.16785502433776855
Batch 7/64 loss: -0.131872296333313
Batch 8/64 loss: -0.13187021017074585
Batch 9/64 loss: -0.14774668216705322
Batch 10/64 loss: -0.0974276065826416
Batch 11/64 loss: -0.10958105325698853
Batch 12/64 loss: -0.15243220329284668
Batch 13/64 loss: -0.1465352177619934
Batch 14/64 loss: -0.10415440797805786
Batch 15/64 loss: -0.07945990562438965
Batch 16/64 loss: -0.1557345986366272
Batch 17/64 loss: -0.14862823486328125
Batch 18/64 loss: -0.08507221937179565
Batch 19/64 loss: -0.05429762601852417
Batch 20/64 loss: -0.17879152297973633
Batch 21/64 loss: -0.08346426486968994
Batch 22/64 loss: -0.14219993352890015
Batch 23/64 loss: -0.1531519889831543
Batch 24/64 loss: -0.13123548030853271
Batch 25/64 loss: -0.10991603136062622
Batch 26/64 loss: -0.0932316780090332
Batch 27/64 loss: -0.14131247997283936
Batch 28/64 loss: -0.13484543561935425
Batch 29/64 loss: -0.11120635271072388
Batch 30/64 loss: -0.16000759601593018
Batch 31/64 loss: -0.0853162407875061
Batch 32/64 loss: -0.15030795335769653
Batch 33/64 loss: -0.11178028583526611
Batch 34/64 loss: -0.1711217761039734
Batch 35/64 loss: -0.1265532374382019
Batch 36/64 loss: -0.15118777751922607
Batch 37/64 loss: -0.09885448217391968
Batch 38/64 loss: -0.13570290803909302
Batch 39/64 loss: -0.10966479778289795
Batch 40/64 loss: -0.1389608383178711
Batch 41/64 loss: -0.11321884393692017
Batch 42/64 loss: -0.10766726732254028
Batch 43/64 loss: -0.11220657825469971
Batch 44/64 loss: -0.12498259544372559
Batch 45/64 loss: -0.12029588222503662
Batch 46/64 loss: -0.16159433126449585
Batch 47/64 loss: -0.11222386360168457
Batch 48/64 loss: -0.06729018688201904
Batch 49/64 loss: -0.14977961778640747
Batch 50/64 loss: -0.1229906678199768
Batch 51/64 loss: -0.1287781000137329
Batch 52/64 loss: -0.15273404121398926
Batch 53/64 loss: -0.10811829566955566
Batch 54/64 loss: -0.10202383995056152
Batch 55/64 loss: -0.15369641780853271
Batch 56/64 loss: -0.14497750997543335
Batch 57/64 loss: -0.12691640853881836
Batch 58/64 loss: -0.13403284549713135
Batch 59/64 loss: -0.07505697011947632
Batch 60/64 loss: -0.13399845361709595
Batch 61/64 loss: -0.134571373462677
Batch 62/64 loss: -0.134929358959198
Batch 63/64 loss: -0.12135577201843262
Batch 64/64 loss: -0.07513940334320068
Epoch 332  Train loss: -0.12635399547277712  Val loss: 0.059963345527648926
Epoch 333
-------------------------------
Batch 1/64 loss: -0.05732351541519165
Batch 2/64 loss: -0.1380494236946106
Batch 3/64 loss: -0.14196020364761353
Batch 4/64 loss: -0.18304741382598877
Batch 5/64 loss: -0.12894761562347412
Batch 6/64 loss: -0.1570095419883728
Batch 7/64 loss: -0.13048768043518066
Batch 8/64 loss: -0.16478800773620605
Batch 9/64 loss: -0.1360328197479248
Batch 10/64 loss: -0.1218385100364685
Batch 11/64 loss: -0.12566620111465454
Batch 12/64 loss: -0.1641903519630432
Batch 13/64 loss: -0.11375117301940918
Batch 14/64 loss: -0.1207423210144043
Batch 15/64 loss: -0.11142659187316895
Batch 16/64 loss: -0.13763779401779175
Batch 17/64 loss: -0.14157390594482422
Batch 18/64 loss: -0.09176558256149292
Batch 19/64 loss: -0.13060742616653442
Batch 20/64 loss: -0.10602539777755737
Batch 21/64 loss: -0.17301183938980103
Batch 22/64 loss: -0.16245532035827637
Batch 23/64 loss: -0.13039827346801758
Batch 24/64 loss: -0.1256650686264038
Batch 25/64 loss: -0.10193157196044922
Batch 26/64 loss: -0.14826911687850952
Batch 27/64 loss: -0.13014709949493408
Batch 28/64 loss: -0.13794177770614624
Batch 29/64 loss: -0.09416407346725464
Batch 30/64 loss: -0.13116806745529175
Batch 31/64 loss: -0.15654808282852173
Batch 32/64 loss: -0.11631423234939575
Batch 33/64 loss: -0.1230393648147583
Batch 34/64 loss: -0.12858068943023682
Batch 35/64 loss: -0.15679073333740234
Batch 36/64 loss: -0.1400970220565796
Batch 37/64 loss: -0.11596536636352539
Batch 38/64 loss: -0.13957059383392334
Batch 39/64 loss: -0.0694780945777893
Batch 40/64 loss: -0.11640208959579468
Batch 41/64 loss: -0.10375756025314331
Batch 42/64 loss: -0.11519086360931396
Batch 43/64 loss: -0.13502544164657593
Batch 44/64 loss: -0.12271404266357422
Batch 45/64 loss: -0.13700807094573975
Batch 46/64 loss: -0.11314773559570312
Batch 47/64 loss: -0.1249459981918335
Batch 48/64 loss: -0.1028594970703125
Batch 49/64 loss: -0.12249165773391724
Batch 50/64 loss: -0.12673884630203247
Batch 51/64 loss: -0.13216304779052734
Batch 52/64 loss: -0.11254966259002686
Batch 53/64 loss: -0.1345527172088623
Batch 54/64 loss: -0.13337039947509766
Batch 55/64 loss: -0.15262848138809204
Batch 56/64 loss: -0.12414121627807617
Batch 57/64 loss: -0.11727732419967651
Batch 58/64 loss: -0.15229320526123047
Batch 59/64 loss: -0.16211432218551636
Batch 60/64 loss: -0.11485826969146729
Batch 61/64 loss: -0.12669819593429565
Batch 62/64 loss: -0.1260519027709961
Batch 63/64 loss: -0.11643928289413452
Batch 64/64 loss: -0.13041210174560547
Epoch 333  Train loss: -0.12874724350723565  Val loss: 0.0597896168322088
Epoch 334
-------------------------------
Batch 1/64 loss: -0.12223643064498901
Batch 2/64 loss: -0.10512173175811768
Batch 3/64 loss: -0.13258618116378784
Batch 4/64 loss: -0.11264693737030029
Batch 5/64 loss: -0.13403183221817017
Batch 6/64 loss: -0.15558427572250366
Batch 7/64 loss: -0.128773033618927
Batch 8/64 loss: -0.08882749080657959
Batch 9/64 loss: -0.08888411521911621
Batch 10/64 loss: -0.133916974067688
Batch 11/64 loss: -0.12469613552093506
Batch 12/64 loss: -0.13721609115600586
Batch 13/64 loss: -0.10794967412948608
Batch 14/64 loss: -0.13650918006896973
Batch 15/64 loss: -0.11743378639221191
Batch 16/64 loss: -0.12271106243133545
Batch 17/64 loss: -0.09167522192001343
Batch 18/64 loss: -0.12956124544143677
Batch 19/64 loss: -0.1445104479789734
Batch 20/64 loss: -0.139420747756958
Batch 21/64 loss: -0.11223560571670532
Batch 22/64 loss: -0.14302659034729004
Batch 23/64 loss: -0.11956757307052612
Batch 24/64 loss: -0.12617307901382446
Batch 25/64 loss: -0.08877193927764893
Batch 26/64 loss: -0.15040546655654907
Batch 27/64 loss: -0.1153721809387207
Batch 28/64 loss: -0.11881494522094727
Batch 29/64 loss: -0.1378544569015503
Batch 30/64 loss: -0.12318938970565796
Batch 31/64 loss: -0.10907071828842163
Batch 32/64 loss: -0.07022374868392944
Batch 33/64 loss: -0.11246764659881592
Batch 34/64 loss: -0.11497694253921509
Batch 35/64 loss: -0.14988255500793457
Batch 36/64 loss: -0.12748587131500244
Batch 37/64 loss: -0.15394377708435059
Batch 38/64 loss: -0.1273776888847351
Batch 39/64 loss: -0.13220101594924927
Batch 40/64 loss: -0.1270495057106018
Batch 41/64 loss: -0.15996849536895752
Batch 42/64 loss: -0.11019831895828247
Batch 43/64 loss: -0.13327383995056152
Batch 44/64 loss: -0.15554958581924438
Batch 45/64 loss: -0.12485617399215698
Batch 46/64 loss: -0.12946957349777222
Batch 47/64 loss: -0.10881549119949341
Batch 48/64 loss: -0.14920556545257568
Batch 49/64 loss: -0.13487792015075684
Batch 50/64 loss: -0.1328144669532776
Batch 51/64 loss: -0.12771594524383545
Batch 52/64 loss: -0.16988790035247803
Batch 53/64 loss: -0.15171712636947632
Batch 54/64 loss: -0.13197004795074463
Batch 55/64 loss: -0.1491900086402893
Batch 56/64 loss: -0.09747302532196045
Batch 57/64 loss: -0.12576407194137573
Batch 58/64 loss: -0.16553521156311035
Batch 59/64 loss: -0.13574904203414917
Batch 60/64 loss: -0.12655746936798096
Batch 61/64 loss: -0.10568386316299438
Batch 62/64 loss: -0.14657169580459595
Batch 63/64 loss: -0.08921456336975098
Batch 64/64 loss: -0.1234825849533081
Epoch 334  Train loss: -0.12657340601378797  Val loss: 0.05896359067602256
Epoch 335
-------------------------------
Batch 1/64 loss: -0.16401821374893188
Batch 2/64 loss: -0.13330674171447754
Batch 3/64 loss: -0.13303112983703613
Batch 4/64 loss: -0.1135748028755188
Batch 5/64 loss: -0.14448416233062744
Batch 6/64 loss: -0.12392580509185791
Batch 7/64 loss: -0.12040591239929199
Batch 8/64 loss: -0.14123570919036865
Batch 9/64 loss: -0.14363443851470947
Batch 10/64 loss: -0.10173177719116211
Batch 11/64 loss: -0.1314295530319214
Batch 12/64 loss: -0.1347595453262329
Batch 13/64 loss: -0.11230605840682983
Batch 14/64 loss: -0.16221845149993896
Batch 15/64 loss: -0.1155097484588623
Batch 16/64 loss: -0.08356547355651855
Batch 17/64 loss: -0.10283952951431274
Batch 18/64 loss: -0.12240248918533325
Batch 19/64 loss: -0.09792482852935791
Batch 20/64 loss: -0.13065314292907715
Batch 21/64 loss: -0.08825981616973877
Batch 22/64 loss: -0.15521633625030518
Batch 23/64 loss: -0.11472100019454956
Batch 24/64 loss: -0.11715948581695557
Batch 25/64 loss: -0.15218490362167358
Batch 26/64 loss: -0.11355000734329224
Batch 27/64 loss: -0.11997848749160767
Batch 28/64 loss: -0.1462755799293518
Batch 29/64 loss: -0.14017921686172485
Batch 30/64 loss: -0.17903625965118408
Batch 31/64 loss: -0.12244564294815063
Batch 32/64 loss: -0.1368611454963684
Batch 33/64 loss: -0.12242799997329712
Batch 34/64 loss: -0.0874255895614624
Batch 35/64 loss: -0.12408638000488281
Batch 36/64 loss: -0.13892728090286255
Batch 37/64 loss: -0.15182608366012573
Batch 38/64 loss: -0.14909112453460693
Batch 39/64 loss: -0.14418065547943115
Batch 40/64 loss: -0.14005672931671143
Batch 41/64 loss: -0.13851642608642578
Batch 42/64 loss: -0.12834560871124268
Batch 43/64 loss: -0.14271998405456543
Batch 44/64 loss: -0.1236346960067749
Batch 45/64 loss: -0.13757675886154175
Batch 46/64 loss: -0.09335470199584961
Batch 47/64 loss: -0.13387531042099
Batch 48/64 loss: -0.1297510862350464
Batch 49/64 loss: -0.09492254257202148
Batch 50/64 loss: -0.148423433303833
Batch 51/64 loss: -0.13403916358947754
Batch 52/64 loss: -0.13460278511047363
Batch 53/64 loss: -0.13113224506378174
Batch 54/64 loss: -0.11999452114105225
Batch 55/64 loss: -0.16015928983688354
Batch 56/64 loss: -0.14234918355941772
Batch 57/64 loss: -0.11851084232330322
Batch 58/64 loss: -0.15299826860427856
Batch 59/64 loss: -0.15264570713043213
Batch 60/64 loss: -0.1326473355293274
Batch 61/64 loss: -0.1316840648651123
Batch 62/64 loss: -0.1280578374862671
Batch 63/64 loss: -0.1570291519165039
Batch 64/64 loss: -0.11043679714202881
Epoch 335  Train loss: -0.13030032597336114  Val loss: 0.05780609039096898
Epoch 336
-------------------------------
Batch 1/64 loss: -0.12702667713165283
Batch 2/64 loss: -0.12056863307952881
Batch 3/64 loss: -0.1439685821533203
Batch 4/64 loss: -0.18551266193389893
Batch 5/64 loss: -0.15059435367584229
Batch 6/64 loss: -0.12674933671951294
Batch 7/64 loss: -0.11655724048614502
Batch 8/64 loss: -0.1521204113960266
Batch 9/64 loss: -0.12807875871658325
Batch 10/64 loss: -0.14376312494277954
Batch 11/64 loss: -0.10052883625030518
Batch 12/64 loss: -0.14945155382156372
Batch 13/64 loss: -0.15267425775527954
Batch 14/64 loss: -0.14330029487609863
Batch 15/64 loss: -0.1522846221923828
Batch 16/64 loss: -0.11030805110931396
Batch 17/64 loss: -0.14900577068328857
Batch 18/64 loss: -0.12292677164077759
Batch 19/64 loss: -0.1373637318611145
Batch 20/64 loss: -0.11889714002609253
Batch 21/64 loss: -0.13785409927368164
Batch 22/64 loss: -0.13930201530456543
Batch 23/64 loss: -0.09299719333648682
Batch 24/64 loss: -0.14075827598571777
Batch 25/64 loss: -0.1594255566596985
Batch 26/64 loss: -0.13122808933258057
Batch 27/64 loss: -0.11543679237365723
Batch 28/64 loss: -0.15531611442565918
Batch 29/64 loss: -0.11085677146911621
Batch 30/64 loss: -0.10774862766265869
Batch 31/64 loss: -0.10145676136016846
Batch 32/64 loss: -0.13603127002716064
Batch 33/64 loss: -0.10547423362731934
Batch 34/64 loss: -0.1369890570640564
Batch 35/64 loss: -0.10848134756088257
Batch 36/64 loss: -0.15594929456710815
Batch 37/64 loss: -0.11961513757705688
Batch 38/64 loss: -0.1272526979446411
Batch 39/64 loss: -0.11841267347335815
Batch 40/64 loss: -0.11131507158279419
Batch 41/64 loss: -0.1365422010421753
Batch 42/64 loss: -0.11200344562530518
Batch 43/64 loss: -0.1570388674736023
Batch 44/64 loss: -0.11747080087661743
Batch 45/64 loss: -0.11131006479263306
Batch 46/64 loss: -0.10377919673919678
Batch 47/64 loss: -0.1291821002960205
Batch 48/64 loss: -0.14346247911453247
Batch 49/64 loss: -0.14469420909881592
Batch 50/64 loss: -0.16971760988235474
Batch 51/64 loss: -0.15348488092422485
Batch 52/64 loss: -0.13964897394180298
Batch 53/64 loss: -0.12010234594345093
Batch 54/64 loss: -0.11494332551956177
Batch 55/64 loss: -0.1831323504447937
Batch 56/64 loss: -0.14134806394577026
Batch 57/64 loss: -0.12271666526794434
Batch 58/64 loss: -0.1444973349571228
Batch 59/64 loss: -0.11727142333984375
Batch 60/64 loss: -0.12659937143325806
Batch 61/64 loss: -0.15865302085876465
Batch 62/64 loss: -0.12094414234161377
Batch 63/64 loss: -0.149338960647583
Batch 64/64 loss: -0.1419229507446289
Epoch 336  Train loss: -0.13282989707647586  Val loss: 0.05855975896632139
Epoch 337
-------------------------------
Batch 1/64 loss: -0.07449573278427124
Batch 2/64 loss: -0.1503978967666626
Batch 3/64 loss: -0.1576937437057495
Batch 4/64 loss: -0.1569443941116333
Batch 5/64 loss: -0.1467735767364502
Batch 6/64 loss: -0.11561930179595947
Batch 7/64 loss: -0.12864738702774048
Batch 8/64 loss: -0.12017804384231567
Batch 9/64 loss: -0.09947431087493896
Batch 10/64 loss: -0.1378510594367981
Batch 11/64 loss: -0.14960825443267822
Batch 12/64 loss: -0.15082800388336182
Batch 13/64 loss: -0.16007208824157715
Batch 14/64 loss: -0.13512587547302246
Batch 15/64 loss: -0.14620476961135864
Batch 16/64 loss: -0.1499040126800537
Batch 17/64 loss: -0.095531165599823
Batch 18/64 loss: -0.16052156686782837
Batch 19/64 loss: -0.12472707033157349
Batch 20/64 loss: -0.10216951370239258
Batch 21/64 loss: -0.14184093475341797
Batch 22/64 loss: -0.13927286863327026
Batch 23/64 loss: -0.16770970821380615
Batch 24/64 loss: -0.18090754747390747
Batch 25/64 loss: -0.12888872623443604
Batch 26/64 loss: -0.11160421371459961
Batch 27/64 loss: -0.1447751522064209
Batch 28/64 loss: -0.12381339073181152
Batch 29/64 loss: -0.12883985042572021
Batch 30/64 loss: -0.14473378658294678
Batch 31/64 loss: -0.12335169315338135
Batch 32/64 loss: -0.14284056425094604
Batch 33/64 loss: -0.09564012289047241
Batch 34/64 loss: -0.15898746252059937
Batch 35/64 loss: -0.12693870067596436
Batch 36/64 loss: -0.11555612087249756
Batch 37/64 loss: -0.09254205226898193
Batch 38/64 loss: -0.1306055188179016
Batch 39/64 loss: -0.12303853034973145
Batch 40/64 loss: -0.14333248138427734
Batch 41/64 loss: -0.1384134292602539
Batch 42/64 loss: -0.12191992998123169
Batch 43/64 loss: -0.10074806213378906
Batch 44/64 loss: -0.12669771909713745
Batch 45/64 loss: -0.13354450464248657
Batch 46/64 loss: -0.13683819770812988
Batch 47/64 loss: -0.15450114011764526
Batch 48/64 loss: -0.14994502067565918
Batch 49/64 loss: -0.13705819845199585
Batch 50/64 loss: -0.07541865110397339
Batch 51/64 loss: -0.1421186923980713
Batch 52/64 loss: -0.13453459739685059
Batch 53/64 loss: -0.14939415454864502
Batch 54/64 loss: -0.10165125131607056
Batch 55/64 loss: -0.13603878021240234
Batch 56/64 loss: -0.10932713747024536
Batch 57/64 loss: -0.11487579345703125
Batch 58/64 loss: -0.1318293809890747
Batch 59/64 loss: -0.14434093236923218
Batch 60/64 loss: -0.14255571365356445
Batch 61/64 loss: -0.13030892610549927
Batch 62/64 loss: -0.11732155084609985
Batch 63/64 loss: -0.11916065216064453
Batch 64/64 loss: -0.16220849752426147
Epoch 337  Train loss: -0.13173625071843464  Val loss: 0.05975706306929441
Epoch 338
-------------------------------
Batch 1/64 loss: -0.13857001066207886
Batch 2/64 loss: -0.1470131278038025
Batch 3/64 loss: -0.11381149291992188
Batch 4/64 loss: -0.12618714570999146
Batch 5/64 loss: -0.1277759075164795
Batch 6/64 loss: -0.17575854063034058
Batch 7/64 loss: -0.16267895698547363
Batch 8/64 loss: -0.13808321952819824
Batch 9/64 loss: -0.13099193572998047
Batch 10/64 loss: -0.0917324423789978
Batch 11/64 loss: -0.12127965688705444
Batch 12/64 loss: -0.07648026943206787
Batch 13/64 loss: -0.12312877178192139
Batch 14/64 loss: -0.11935865879058838
Batch 15/64 loss: -0.14621740579605103
Batch 16/64 loss: -0.17795509099960327
Batch 17/64 loss: -0.14542323350906372
Batch 18/64 loss: -0.14545273780822754
Batch 19/64 loss: -0.11479353904724121
Batch 20/64 loss: -0.1258169412612915
Batch 21/64 loss: -0.16608178615570068
Batch 22/64 loss: -0.11344826221466064
Batch 23/64 loss: -0.09587067365646362
Batch 24/64 loss: -0.07413756847381592
Batch 25/64 loss: -0.13011693954467773
Batch 26/64 loss: -0.15606248378753662
Batch 27/64 loss: -0.12880736589431763
Batch 28/64 loss: -0.09062111377716064
Batch 29/64 loss: -0.1589164137840271
Batch 30/64 loss: -0.1789168119430542
Batch 31/64 loss: -0.14147794246673584
Batch 32/64 loss: -0.04745745658874512
Batch 33/64 loss: -0.14777672290802002
Batch 34/64 loss: -0.1362515687942505
Batch 35/64 loss: -0.1418110728263855
Batch 36/64 loss: -0.10050749778747559
Batch 37/64 loss: -0.14219790697097778
Batch 38/64 loss: -0.14648115634918213
Batch 39/64 loss: -0.13157904148101807
Batch 40/64 loss: -0.12527698278427124
Batch 41/64 loss: -0.14168864488601685
Batch 42/64 loss: -0.14057683944702148
Batch 43/64 loss: -0.13016730546951294
Batch 44/64 loss: -0.12823379039764404
Batch 45/64 loss: -0.14880961179733276
Batch 46/64 loss: -0.11247217655181885
Batch 47/64 loss: -0.16506391763687134
Batch 48/64 loss: -0.14300942420959473
Batch 49/64 loss: -0.15182560682296753
Batch 50/64 loss: -0.1364993453025818
Batch 51/64 loss: -0.133550226688385
Batch 52/64 loss: -0.1241532564163208
Batch 53/64 loss: -0.15488725900650024
Batch 54/64 loss: -0.15029537677764893
Batch 55/64 loss: -0.12232518196105957
Batch 56/64 loss: -0.13432234525680542
Batch 57/64 loss: -0.1289265751838684
Batch 58/64 loss: -0.14538884162902832
Batch 59/64 loss: -0.1270572543144226
Batch 60/64 loss: -0.13707321882247925
Batch 61/64 loss: -0.0911111831665039
Batch 62/64 loss: -0.1032947301864624
Batch 63/64 loss: -0.09926575422286987
Batch 64/64 loss: -0.1272762417793274
Epoch 338  Train loss: -0.13094526903302062  Val loss: 0.06058822649041402
Epoch 339
-------------------------------
Batch 1/64 loss: -0.13775348663330078
Batch 2/64 loss: -0.1485787034034729
Batch 3/64 loss: -0.10522407293319702
Batch 4/64 loss: -0.1469365358352661
Batch 5/64 loss: -0.13034886121749878
Batch 6/64 loss: -0.13981521129608154
Batch 7/64 loss: -0.13021230697631836
Batch 8/64 loss: -0.1649492383003235
Batch 9/64 loss: -0.15063446760177612
Batch 10/64 loss: -0.12765932083129883
Batch 11/64 loss: -0.08561939001083374
Batch 12/64 loss: -0.1691620945930481
Batch 13/64 loss: -0.15422385931015015
Batch 14/64 loss: -0.14002448320388794
Batch 15/64 loss: -0.15306460857391357
Batch 16/64 loss: -0.1353745460510254
Batch 17/64 loss: -0.14941167831420898
Batch 18/64 loss: -0.0984964370727539
Batch 19/64 loss: -0.15959787368774414
Batch 20/64 loss: -0.1389661431312561
Batch 21/64 loss: -0.14013326168060303
Batch 22/64 loss: -0.11255532503128052
Batch 23/64 loss: -0.10917836427688599
Batch 24/64 loss: -0.1369280219078064
Batch 25/64 loss: -0.0750434398651123
Batch 26/64 loss: -0.15686410665512085
Batch 27/64 loss: -0.14103037118911743
Batch 28/64 loss: -0.12712031602859497
Batch 29/64 loss: -0.1268526315689087
Batch 30/64 loss: -0.1343768835067749
Batch 31/64 loss: -0.10033541917800903
Batch 32/64 loss: -0.1314610242843628
Batch 33/64 loss: -0.13999813795089722
Batch 34/64 loss: -0.13945960998535156
Batch 35/64 loss: -0.1278664469718933
Batch 36/64 loss: -0.12480711936950684
Batch 37/64 loss: -0.10782390832901001
Batch 38/64 loss: -0.1396198272705078
Batch 39/64 loss: -0.07695585489273071
Batch 40/64 loss: -0.14869087934494019
Batch 41/64 loss: -0.10375088453292847
Batch 42/64 loss: -0.13440847396850586
Batch 43/64 loss: -0.1204233169555664
Batch 44/64 loss: -0.1295725703239441
Batch 45/64 loss: -0.12362390756607056
Batch 46/64 loss: -0.12557488679885864
Batch 47/64 loss: -0.14842617511749268
Batch 48/64 loss: -0.11814618110656738
Batch 49/64 loss: -0.15328627824783325
Batch 50/64 loss: -0.15134084224700928
Batch 51/64 loss: -0.14073175191879272
Batch 52/64 loss: -0.11781328916549683
Batch 53/64 loss: -0.10786169767379761
Batch 54/64 loss: -0.1328994631767273
Batch 55/64 loss: -0.15315896272659302
Batch 56/64 loss: -0.10628330707550049
Batch 57/64 loss: -0.10653328895568848
Batch 58/64 loss: -0.14896762371063232
Batch 59/64 loss: -0.13219678401947021
Batch 60/64 loss: -0.12264955043792725
Batch 61/64 loss: -0.12655937671661377
Batch 62/64 loss: -0.10343170166015625
Batch 63/64 loss: -0.11286735534667969
Batch 64/64 loss: -0.1531224250793457
Epoch 339  Train loss: -0.1301726079454609  Val loss: 0.059780365412997215
Epoch 340
-------------------------------
Batch 1/64 loss: -0.07669109106063843
Batch 2/64 loss: -0.1482142210006714
Batch 3/64 loss: -0.1299094557762146
Batch 4/64 loss: -0.13580822944641113
Batch 5/64 loss: -0.1284165382385254
Batch 6/64 loss: -0.12128353118896484
Batch 7/64 loss: -0.13016003370285034
Batch 8/64 loss: -0.15355348587036133
Batch 9/64 loss: -0.15423595905303955
Batch 10/64 loss: -0.12798553705215454
Batch 11/64 loss: -0.15749090909957886
Batch 12/64 loss: -0.10516232252120972
Batch 13/64 loss: -0.1394352912902832
Batch 14/64 loss: -0.09493839740753174
Batch 15/64 loss: -0.1319265365600586
Batch 16/64 loss: -0.11258751153945923
Batch 17/64 loss: -0.10265058279037476
Batch 18/64 loss: -0.15111756324768066
Batch 19/64 loss: -0.15072232484817505
Batch 20/64 loss: -0.15519225597381592
Batch 21/64 loss: -0.15738821029663086
Batch 22/64 loss: -0.12931591272354126
Batch 23/64 loss: -0.11980313062667847
Batch 24/64 loss: -0.17521631717681885
Batch 25/64 loss: -0.16394323110580444
Batch 26/64 loss: -0.10660386085510254
Batch 27/64 loss: -0.1099887490272522
Batch 28/64 loss: -0.08220040798187256
Batch 29/64 loss: -0.14522945880889893
Batch 30/64 loss: -0.08929437398910522
Batch 31/64 loss: -0.15236669778823853
Batch 32/64 loss: -0.12623488903045654
Batch 33/64 loss: -0.1296117901802063
Batch 34/64 loss: -0.13164043426513672
Batch 35/64 loss: -0.13809502124786377
Batch 36/64 loss: -0.10350948572158813
Batch 37/64 loss: -0.16638606786727905
Batch 38/64 loss: -0.13189059495925903
Batch 39/64 loss: -0.15223067998886108
Batch 40/64 loss: -0.1289994716644287
Batch 41/64 loss: -0.10255616903305054
Batch 42/64 loss: -0.15943509340286255
Batch 43/64 loss: -0.1276072859764099
Batch 44/64 loss: -0.1252967119216919
Batch 45/64 loss: -0.15340697765350342
Batch 46/64 loss: -0.078277587890625
Batch 47/64 loss: -0.11211287975311279
Batch 48/64 loss: -0.1375371217727661
Batch 49/64 loss: -0.10432499647140503
Batch 50/64 loss: -0.13111191987991333
Batch 51/64 loss: -0.1533871293067932
Batch 52/64 loss: -0.12500709295272827
Batch 53/64 loss: -0.15780699253082275
Batch 54/64 loss: -0.1090887188911438
Batch 55/64 loss: -0.133944571018219
Batch 56/64 loss: -0.14860332012176514
Batch 57/64 loss: -0.14512860774993896
Batch 58/64 loss: -0.14000970125198364
Batch 59/64 loss: -0.12281382083892822
Batch 60/64 loss: -0.16834700107574463
Batch 61/64 loss: -0.14788901805877686
Batch 62/64 loss: -0.12001955509185791
Batch 63/64 loss: -0.11282467842102051
Batch 64/64 loss: -0.11543899774551392
Epoch 340  Train loss: -0.1309889688211329  Val loss: 0.06213911947925476
Epoch 341
-------------------------------
Batch 1/64 loss: -0.184353768825531
Batch 2/64 loss: -0.12892329692840576
Batch 3/64 loss: -0.11318039894104004
Batch 4/64 loss: -0.12082028388977051
Batch 5/64 loss: -0.14000362157821655
Batch 6/64 loss: -0.11515259742736816
Batch 7/64 loss: -0.12132227420806885
Batch 8/64 loss: -0.17488914728164673
Batch 9/64 loss: -0.12648624181747437
Batch 10/64 loss: -0.16537702083587646
Batch 11/64 loss: -0.15163475275039673
Batch 12/64 loss: -0.1033545732498169
Batch 13/64 loss: -0.1411396861076355
Batch 14/64 loss: -0.10231071710586548
Batch 15/64 loss: -0.10359054803848267
Batch 16/64 loss: -0.09185206890106201
Batch 17/64 loss: -0.142828106880188
Batch 18/64 loss: -0.09690594673156738
Batch 19/64 loss: -0.1252397894859314
Batch 20/64 loss: -0.1248542070388794
Batch 21/64 loss: -0.14239180088043213
Batch 22/64 loss: -0.1647701859474182
Batch 23/64 loss: -0.16897857189178467
Batch 24/64 loss: -0.10313653945922852
Batch 25/64 loss: -0.1315518021583557
Batch 26/64 loss: -0.1328940987586975
Batch 27/64 loss: -0.09609049558639526
Batch 28/64 loss: -0.1432586908340454
Batch 29/64 loss: -0.156264066696167
Batch 30/64 loss: -0.14531803131103516
Batch 31/64 loss: -0.11723721027374268
Batch 32/64 loss: -0.12330508232116699
Batch 33/64 loss: -0.16224145889282227
Batch 34/64 loss: -0.11639648675918579
Batch 35/64 loss: -0.11644452810287476
Batch 36/64 loss: -0.08223545551300049
Batch 37/64 loss: -0.10995310544967651
Batch 38/64 loss: -0.15764182806015015
Batch 39/64 loss: -0.1439584493637085
Batch 40/64 loss: -0.13631612062454224
Batch 41/64 loss: -0.06364184617996216
Batch 42/64 loss: -0.17405831813812256
Batch 43/64 loss: -0.10642683506011963
Batch 44/64 loss: -0.12558239698410034
Batch 45/64 loss: -0.13292181491851807
Batch 46/64 loss: -0.11792159080505371
Batch 47/64 loss: -0.1369112730026245
Batch 48/64 loss: -0.1272519826889038
Batch 49/64 loss: -0.132271409034729
Batch 50/64 loss: -0.12436830997467041
Batch 51/64 loss: -0.1520950198173523
Batch 52/64 loss: -0.15422946214675903
Batch 53/64 loss: -0.1488865613937378
Batch 54/64 loss: -0.10823363065719604
Batch 55/64 loss: -0.14387577772140503
Batch 56/64 loss: -0.12545251846313477
Batch 57/64 loss: -0.1192353367805481
Batch 58/64 loss: -0.13055598735809326
Batch 59/64 loss: -0.15022826194763184
Batch 60/64 loss: -0.13378369808197021
Batch 61/64 loss: -0.0858495831489563
Batch 62/64 loss: -0.1395576000213623
Batch 63/64 loss: -0.11719584465026855
Batch 64/64 loss: -0.12290114164352417
Epoch 341  Train loss: -0.12968335642534143  Val loss: 0.06266934662750087
Epoch 342
-------------------------------
Batch 1/64 loss: -0.1415446400642395
Batch 2/64 loss: -0.13630419969558716
Batch 3/64 loss: -0.14214134216308594
Batch 4/64 loss: -0.12222158908843994
Batch 5/64 loss: -0.14296185970306396
Batch 6/64 loss: -0.13900792598724365
Batch 7/64 loss: -0.08820074796676636
Batch 8/64 loss: -0.1048470139503479
Batch 9/64 loss: -0.11143839359283447
Batch 10/64 loss: -0.14877170324325562
Batch 11/64 loss: -0.1488858461380005
Batch 12/64 loss: -0.11460256576538086
Batch 13/64 loss: -0.142328143119812
Batch 14/64 loss: -0.13101214170455933
Batch 15/64 loss: -0.15820521116256714
Batch 16/64 loss: -0.054015934467315674
Batch 17/64 loss: -0.1331825852394104
Batch 18/64 loss: -0.1238815188407898
Batch 19/64 loss: -0.15314972400665283
Batch 20/64 loss: -0.15204977989196777
Batch 21/64 loss: -0.1716194748878479
Batch 22/64 loss: -0.14671605825424194
Batch 23/64 loss: -0.1036195158958435
Batch 24/64 loss: -0.13708561658859253
Batch 25/64 loss: -0.15494763851165771
Batch 26/64 loss: -0.12729185819625854
Batch 27/64 loss: -0.1662909984588623
Batch 28/64 loss: -0.13810038566589355
Batch 29/64 loss: -0.17367148399353027
Batch 30/64 loss: -0.13672906160354614
Batch 31/64 loss: -0.11864089965820312
Batch 32/64 loss: -0.0968329906463623
Batch 33/64 loss: -0.15760844945907593
Batch 34/64 loss: -0.15093278884887695
Batch 35/64 loss: -0.16218459606170654
Batch 36/64 loss: -0.1334850788116455
Batch 37/64 loss: -0.1056203842163086
Batch 38/64 loss: -0.08259475231170654
Batch 39/64 loss: -0.10060757398605347
Batch 40/64 loss: -0.11406147480010986
Batch 41/64 loss: -0.14742732048034668
Batch 42/64 loss: -0.1386803388595581
Batch 43/64 loss: -0.14799529314041138
Batch 44/64 loss: -0.12007617950439453
Batch 45/64 loss: -0.11453437805175781
Batch 46/64 loss: -0.1129295825958252
Batch 47/64 loss: -0.1154065728187561
Batch 48/64 loss: -0.10450482368469238
Batch 49/64 loss: -0.10141205787658691
Batch 50/64 loss: -0.16174525022506714
Batch 51/64 loss: -0.09680306911468506
Batch 52/64 loss: -0.1510128378868103
Batch 53/64 loss: -0.1281549334526062
Batch 54/64 loss: -0.16245567798614502
Batch 55/64 loss: -0.1325298547744751
Batch 56/64 loss: -0.1646209955215454
Batch 57/64 loss: -0.13919782638549805
Batch 58/64 loss: -0.1089709997177124
Batch 59/64 loss: -0.16941630840301514
Batch 60/64 loss: -0.13394266366958618
Batch 61/64 loss: -0.14558833837509155
Batch 62/64 loss: -0.11879986524581909
Batch 63/64 loss: -0.11213558912277222
Batch 64/64 loss: -0.10199117660522461
Epoch 342  Train loss: -0.13132906801560346  Val loss: 0.06376981817160275
Epoch 343
-------------------------------
Batch 1/64 loss: -0.16570550203323364
Batch 2/64 loss: -0.11633110046386719
Batch 3/64 loss: -0.15961295366287231
Batch 4/64 loss: -0.16996783018112183
Batch 5/64 loss: -0.10977345705032349
Batch 6/64 loss: -0.16357237100601196
Batch 7/64 loss: -0.12419843673706055
Batch 8/64 loss: -0.12526416778564453
Batch 9/64 loss: -0.08026283979415894
Batch 10/64 loss: -0.14274871349334717
Batch 11/64 loss: -0.11944997310638428
Batch 12/64 loss: -0.15541547536849976
Batch 13/64 loss: -0.10453307628631592
Batch 14/64 loss: -0.12833356857299805
Batch 15/64 loss: -0.15991932153701782
Batch 16/64 loss: -0.13536763191223145
Batch 17/64 loss: -0.1451128125190735
Batch 18/64 loss: -0.08929949998855591
Batch 19/64 loss: -0.12693333625793457
Batch 20/64 loss: -0.12529480457305908
Batch 21/64 loss: -0.14054512977600098
Batch 22/64 loss: -0.15272080898284912
Batch 23/64 loss: -0.14955651760101318
Batch 24/64 loss: -0.123421311378479
Batch 25/64 loss: -0.09328794479370117
Batch 26/64 loss: -0.15029919147491455
Batch 27/64 loss: -0.13739049434661865
Batch 28/64 loss: -0.15396970510482788
Batch 29/64 loss: -0.1519331932067871
Batch 30/64 loss: -0.1383466124534607
Batch 31/64 loss: -0.17205369472503662
Batch 32/64 loss: -0.14862895011901855
Batch 33/64 loss: -0.10923755168914795
Batch 34/64 loss: -0.13158059120178223
Batch 35/64 loss: -0.15344607830047607
Batch 36/64 loss: -0.11276382207870483
Batch 37/64 loss: -0.08842915296554565
Batch 38/64 loss: -0.09300464391708374
Batch 39/64 loss: -0.09971225261688232
Batch 40/64 loss: -0.15717792510986328
Batch 41/64 loss: -0.15016913414001465
Batch 42/64 loss: -0.14738595485687256
Batch 43/64 loss: -0.13384640216827393
Batch 44/64 loss: -0.16917270421981812
Batch 45/64 loss: -0.13432610034942627
Batch 46/64 loss: -0.12408101558685303
Batch 47/64 loss: -0.12772595882415771
Batch 48/64 loss: -0.08881169557571411
Batch 49/64 loss: -0.13584548234939575
Batch 50/64 loss: -0.11898225545883179
Batch 51/64 loss: -0.15280842781066895
Batch 52/64 loss: -0.14872032403945923
Batch 53/64 loss: -0.10870116949081421
Batch 54/64 loss: -0.13794803619384766
Batch 55/64 loss: -0.1233866810798645
Batch 56/64 loss: -0.1456502079963684
Batch 57/64 loss: -0.15698760747909546
Batch 58/64 loss: -0.14938002824783325
Batch 59/64 loss: -0.1542937159538269
Batch 60/64 loss: -0.12069672346115112
Batch 61/64 loss: -0.15719306468963623
Batch 62/64 loss: -0.1370059847831726
Batch 63/64 loss: -0.1446790099143982
Batch 64/64 loss: -0.1132516860961914
Epoch 343  Train loss: -0.13423276695550657  Val loss: 0.05905158618061813
Epoch 344
-------------------------------
Batch 1/64 loss: -0.14425331354141235
Batch 2/64 loss: -0.13084018230438232
Batch 3/64 loss: -0.16502541303634644
Batch 4/64 loss: -0.15402323007583618
Batch 5/64 loss: -0.1685640811920166
Batch 6/64 loss: -0.14441055059432983
Batch 7/64 loss: -0.09428191184997559
Batch 8/64 loss: -0.13634800910949707
Batch 9/64 loss: -0.1491760015487671
Batch 10/64 loss: -0.1361333131790161
Batch 11/64 loss: -0.11742180585861206
Batch 12/64 loss: -0.1543847918510437
Batch 13/64 loss: -0.1339195966720581
Batch 14/64 loss: -0.14184296131134033
Batch 15/64 loss: -0.15615946054458618
Batch 16/64 loss: -0.14230340719223022
Batch 17/64 loss: -0.1435565948486328
Batch 18/64 loss: -0.15366995334625244
Batch 19/64 loss: -0.1286659836769104
Batch 20/64 loss: -0.15454506874084473
Batch 21/64 loss: -0.12254774570465088
Batch 22/64 loss: -0.10684806108474731
Batch 23/64 loss: -0.16793477535247803
Batch 24/64 loss: -0.09997987747192383
Batch 25/64 loss: -0.1254584789276123
Batch 26/64 loss: -0.10752588510513306
Batch 27/64 loss: -0.15672636032104492
Batch 28/64 loss: -0.16267859935760498
Batch 29/64 loss: -0.14123547077178955
Batch 30/64 loss: -0.13383877277374268
Batch 31/64 loss: -0.12637925148010254
Batch 32/64 loss: -0.16012001037597656
Batch 33/64 loss: -0.12840509414672852
Batch 34/64 loss: -0.12847942113876343
Batch 35/64 loss: -0.15860021114349365
Batch 36/64 loss: -0.13791799545288086
Batch 37/64 loss: -0.1279388666152954
Batch 38/64 loss: -0.12280833721160889
Batch 39/64 loss: -0.12338459491729736
Batch 40/64 loss: -0.15091347694396973
Batch 41/64 loss: -0.10340434312820435
Batch 42/64 loss: -0.07746756076812744
Batch 43/64 loss: -0.06889843940734863
Batch 44/64 loss: -0.1562677025794983
Batch 45/64 loss: -0.13312369585037231
Batch 46/64 loss: -0.11206704378128052
Batch 47/64 loss: -0.13674259185791016
Batch 48/64 loss: -0.12021654844284058
Batch 49/64 loss: -0.09629130363464355
Batch 50/64 loss: -0.08025991916656494
Batch 51/64 loss: -0.14918214082717896
Batch 52/64 loss: -0.10684287548065186
Batch 53/64 loss: -0.11103618144989014
Batch 54/64 loss: -0.1607191562652588
Batch 55/64 loss: -0.14022833108901978
Batch 56/64 loss: -0.11598515510559082
Batch 57/64 loss: -0.08436077833175659
Batch 58/64 loss: -0.1388222575187683
Batch 59/64 loss: -0.1327943205833435
Batch 60/64 loss: -0.1493927240371704
Batch 61/64 loss: -0.11635977029800415
Batch 62/64 loss: -0.14860880374908447
Batch 63/64 loss: -0.12834185361862183
Batch 64/64 loss: -0.18053889274597168
Epoch 344  Train loss: -0.1324245424831615  Val loss: 0.057845731576283775
Epoch 345
-------------------------------
Batch 1/64 loss: -0.1406937837600708
Batch 2/64 loss: -0.12375122308731079
Batch 3/64 loss: -0.14804071187973022
Batch 4/64 loss: -0.13080579042434692
Batch 5/64 loss: -0.17439210414886475
Batch 6/64 loss: -0.17578887939453125
Batch 7/64 loss: -0.1317136287689209
Batch 8/64 loss: -0.16480916738510132
Batch 9/64 loss: -0.1503443717956543
Batch 10/64 loss: -0.12054222822189331
Batch 11/64 loss: -0.10710632801055908
Batch 12/64 loss: -0.16616606712341309
Batch 13/64 loss: -0.11050492525100708
Batch 14/64 loss: -0.10116368532180786
Batch 15/64 loss: -0.13477575778961182
Batch 16/64 loss: -0.12695229053497314
Batch 17/64 loss: -0.12498438358306885
Batch 18/64 loss: -0.15981650352478027
Batch 19/64 loss: -0.14591848850250244
Batch 20/64 loss: -0.13299649953842163
Batch 21/64 loss: -0.13595640659332275
Batch 22/64 loss: -0.13225817680358887
Batch 23/64 loss: -0.09481072425842285
Batch 24/64 loss: -0.11498868465423584
Batch 25/64 loss: -0.12509757280349731
Batch 26/64 loss: -0.14095020294189453
Batch 27/64 loss: -0.0932278037071228
Batch 28/64 loss: -0.1300603151321411
Batch 29/64 loss: -0.13216018676757812
Batch 30/64 loss: -0.11478632688522339
Batch 31/64 loss: -0.13706088066101074
Batch 32/64 loss: -0.11676585674285889
Batch 33/64 loss: -0.1164015531539917
Batch 34/64 loss: -0.15192914009094238
Batch 35/64 loss: -0.11886352300643921
Batch 36/64 loss: -0.1560215950012207
Batch 37/64 loss: -0.10728621482849121
Batch 38/64 loss: -0.12844997644424438
Batch 39/64 loss: -0.13192200660705566
Batch 40/64 loss: -0.1361446976661682
Batch 41/64 loss: -0.1669527292251587
Batch 42/64 loss: -0.13341659307479858
Batch 43/64 loss: -0.11823749542236328
Batch 44/64 loss: -0.10950052738189697
Batch 45/64 loss: -0.13545680046081543
Batch 46/64 loss: -0.14219951629638672
Batch 47/64 loss: -0.14362114667892456
Batch 48/64 loss: -0.13563328981399536
Batch 49/64 loss: -0.12406253814697266
Batch 50/64 loss: -0.10007268190383911
Batch 51/64 loss: -0.14732646942138672
Batch 52/64 loss: -0.12182796001434326
Batch 53/64 loss: -0.12124806642532349
Batch 54/64 loss: -0.1361147165298462
Batch 55/64 loss: -0.08487164974212646
Batch 56/64 loss: -0.116199791431427
Batch 57/64 loss: -0.13191092014312744
Batch 58/64 loss: -0.14087063074111938
Batch 59/64 loss: -0.13444042205810547
Batch 60/64 loss: -0.1564900279045105
Batch 61/64 loss: -0.16601741313934326
Batch 62/64 loss: -0.10279464721679688
Batch 63/64 loss: -0.14075791835784912
Batch 64/64 loss: -0.09265446662902832
Epoch 345  Train loss: -0.13123017198899212  Val loss: 0.06337780026635763
Epoch 346
-------------------------------
Batch 1/64 loss: -0.1619260311126709
Batch 2/64 loss: -0.13953113555908203
Batch 3/64 loss: -0.13139981031417847
Batch 4/64 loss: -0.13593411445617676
Batch 5/64 loss: -0.15446114540100098
Batch 6/64 loss: -0.1510835886001587
Batch 7/64 loss: -0.14201349020004272
Batch 8/64 loss: -0.1268969178199768
Batch 9/64 loss: -0.15922629833221436
Batch 10/64 loss: -0.12610149383544922
Batch 11/64 loss: -0.13566124439239502
Batch 12/64 loss: -0.1677510142326355
Batch 13/64 loss: -0.17501533031463623
Batch 14/64 loss: -0.09987080097198486
Batch 15/64 loss: -0.13351494073867798
Batch 16/64 loss: -0.11249536275863647
Batch 17/64 loss: -0.1666269302368164
Batch 18/64 loss: -0.1351063847541809
Batch 19/64 loss: -0.12948471307754517
Batch 20/64 loss: -0.11330801248550415
Batch 21/64 loss: -0.1452673077583313
Batch 22/64 loss: -0.14775782823562622
Batch 23/64 loss: -0.14458554983139038
Batch 24/64 loss: -0.16594046354293823
Batch 25/64 loss: -0.1397775411605835
Batch 26/64 loss: -0.11871141195297241
Batch 27/64 loss: -0.1194644570350647
Batch 28/64 loss: -0.12127161026000977
Batch 29/64 loss: -0.11988121271133423
Batch 30/64 loss: -0.099709153175354
Batch 31/64 loss: -0.1437232494354248
Batch 32/64 loss: -0.133009135723114
Batch 33/64 loss: -0.10063016414642334
Batch 34/64 loss: -0.14410293102264404
Batch 35/64 loss: -0.1332680583000183
Batch 36/64 loss: -0.15766245126724243
Batch 37/64 loss: -0.14067202806472778
Batch 38/64 loss: -0.1127440333366394
Batch 39/64 loss: -0.0807376503944397
Batch 40/64 loss: -0.1396387815475464
Batch 41/64 loss: -0.16779011487960815
Batch 42/64 loss: -0.12163031101226807
Batch 43/64 loss: -0.12157940864562988
Batch 44/64 loss: -0.1428910493850708
Batch 45/64 loss: -0.10901713371276855
Batch 46/64 loss: -0.11593568325042725
Batch 47/64 loss: -0.1460566520690918
Batch 48/64 loss: -0.12080442905426025
Batch 49/64 loss: -0.15285956859588623
Batch 50/64 loss: -0.11612546443939209
Batch 51/64 loss: -0.10277014970779419
Batch 52/64 loss: -0.13929122686386108
Batch 53/64 loss: -0.12365370988845825
Batch 54/64 loss: -0.13768428564071655
Batch 55/64 loss: -0.16274654865264893
Batch 56/64 loss: -0.16987836360931396
Batch 57/64 loss: -0.14996302127838135
Batch 58/64 loss: -0.16697126626968384
Batch 59/64 loss: -0.10840773582458496
Batch 60/64 loss: -0.14841234683990479
Batch 61/64 loss: -0.14928829669952393
Batch 62/64 loss: -0.10400629043579102
Batch 63/64 loss: -0.14771080017089844
Batch 64/64 loss: -0.09240967035293579
Epoch 346  Train loss: -0.13491364530488556  Val loss: 0.06069208174636684
Epoch 347
-------------------------------
Batch 1/64 loss: -0.15364998579025269
Batch 2/64 loss: -0.13811790943145752
Batch 3/64 loss: -0.15143918991088867
Batch 4/64 loss: -0.12399685382843018
Batch 5/64 loss: -0.13014978170394897
Batch 6/64 loss: -0.13828110694885254
Batch 7/64 loss: -0.10889196395874023
Batch 8/64 loss: -0.1488560438156128
Batch 9/64 loss: -0.11361634731292725
Batch 10/64 loss: -0.13498318195343018
Batch 11/64 loss: -0.1601102352142334
Batch 12/64 loss: -0.13562613725662231
Batch 13/64 loss: -0.14368444681167603
Batch 14/64 loss: -0.12610578536987305
Batch 15/64 loss: -0.13403505086898804
Batch 16/64 loss: -0.1485539674758911
Batch 17/64 loss: -0.15112602710723877
Batch 18/64 loss: -0.15496641397476196
Batch 19/64 loss: -0.17171138525009155
Batch 20/64 loss: -0.12379682064056396
Batch 21/64 loss: -0.11603409051895142
Batch 22/64 loss: -0.15084141492843628
Batch 23/64 loss: -0.1292276382446289
Batch 24/64 loss: -0.12447607517242432
Batch 25/64 loss: -0.11644041538238525
Batch 26/64 loss: -0.14014118909835815
Batch 27/64 loss: -0.110240638256073
Batch 28/64 loss: -0.14951306581497192
Batch 29/64 loss: -0.1358351707458496
Batch 30/64 loss: -0.08316349983215332
Batch 31/64 loss: -0.17749547958374023
Batch 32/64 loss: -0.14945387840270996
Batch 33/64 loss: -0.12332022190093994
Batch 34/64 loss: -0.14662998914718628
Batch 35/64 loss: -0.11907696723937988
Batch 36/64 loss: -0.14652186632156372
Batch 37/64 loss: -0.15375608205795288
Batch 38/64 loss: -0.1726611852645874
Batch 39/64 loss: -0.1416454315185547
Batch 40/64 loss: -0.11510300636291504
Batch 41/64 loss: -0.1342344880104065
Batch 42/64 loss: -0.1256352663040161
Batch 43/64 loss: -0.10584568977355957
Batch 44/64 loss: -0.12905269861221313
Batch 45/64 loss: -0.14301759004592896
Batch 46/64 loss: -0.11615622043609619
Batch 47/64 loss: -0.13275468349456787
Batch 48/64 loss: -0.12130236625671387
Batch 49/64 loss: -0.14177590608596802
Batch 50/64 loss: -0.1296539306640625
Batch 51/64 loss: -0.13725346326828003
Batch 52/64 loss: -0.14072328805923462
Batch 53/64 loss: -0.11275619268417358
Batch 54/64 loss: -0.1304524540901184
Batch 55/64 loss: -0.11899840831756592
Batch 56/64 loss: -0.1200452446937561
Batch 57/64 loss: -0.13206052780151367
Batch 58/64 loss: -0.14219695329666138
Batch 59/64 loss: -0.1217237114906311
Batch 60/64 loss: -0.14081156253814697
Batch 61/64 loss: -0.15169906616210938
Batch 62/64 loss: -0.14026981592178345
Batch 63/64 loss: -0.12549829483032227
Batch 64/64 loss: -0.10771709680557251
Epoch 347  Train loss: -0.13439971115074906  Val loss: 0.06424880335011433
Epoch 348
-------------------------------
Batch 1/64 loss: -0.15288090705871582
Batch 2/64 loss: -0.14735931158065796
Batch 3/64 loss: -0.15739256143569946
Batch 4/64 loss: -0.14961791038513184
Batch 5/64 loss: -0.13195770978927612
Batch 6/64 loss: -0.10911476612091064
Batch 7/64 loss: -0.12250113487243652
Batch 8/64 loss: -0.16627120971679688
Batch 9/64 loss: -0.12929660081863403
Batch 10/64 loss: -0.13812053203582764
Batch 11/64 loss: -0.16784793138504028
Batch 12/64 loss: -0.1372855305671692
Batch 13/64 loss: -0.11492300033569336
Batch 14/64 loss: -0.14061999320983887
Batch 15/64 loss: -0.13094061613082886
Batch 16/64 loss: -0.15545368194580078
Batch 17/64 loss: -0.12197160720825195
Batch 18/64 loss: -0.1411985158920288
Batch 19/64 loss: -0.1107020378112793
Batch 20/64 loss: -0.1247982382774353
Batch 21/64 loss: -0.13889718055725098
Batch 22/64 loss: -0.09706217050552368
Batch 23/64 loss: -0.16239303350448608
Batch 24/64 loss: -0.111117422580719
Batch 25/64 loss: -0.15056532621383667
Batch 26/64 loss: -0.1043844223022461
Batch 27/64 loss: -0.14828068017959595
Batch 28/64 loss: -0.13231760263442993
Batch 29/64 loss: -0.1555575728416443
Batch 30/64 loss: -0.13078343868255615
Batch 31/64 loss: -0.1571105718612671
Batch 32/64 loss: -0.14179974794387817
Batch 33/64 loss: -0.13912004232406616
Batch 34/64 loss: -0.14697325229644775
Batch 35/64 loss: -0.08584141731262207
Batch 36/64 loss: -0.13995110988616943
Batch 37/64 loss: -0.1316511034965515
Batch 38/64 loss: -0.15975350141525269
Batch 39/64 loss: -0.11129230260848999
Batch 40/64 loss: -0.11322450637817383
Batch 41/64 loss: -0.13692188262939453
Batch 42/64 loss: -0.14580345153808594
Batch 43/64 loss: -0.13854074478149414
Batch 44/64 loss: -0.13498353958129883
Batch 45/64 loss: -0.1360454559326172
Batch 46/64 loss: -0.1332465410232544
Batch 47/64 loss: -0.09997427463531494
Batch 48/64 loss: -0.08293062448501587
Batch 49/64 loss: -0.11007678508758545
Batch 50/64 loss: -0.14220964908599854
Batch 51/64 loss: -0.12464016675949097
Batch 52/64 loss: -0.13621234893798828
Batch 53/64 loss: -0.11221718788146973
Batch 54/64 loss: -0.13967645168304443
Batch 55/64 loss: -0.1322963833808899
Batch 56/64 loss: -0.15821069478988647
Batch 57/64 loss: -0.1336294412612915
Batch 58/64 loss: -0.12849152088165283
Batch 59/64 loss: -0.12632101774215698
Batch 60/64 loss: -0.16861295700073242
Batch 61/64 loss: -0.14535367488861084
Batch 62/64 loss: -0.11324751377105713
Batch 63/64 loss: -0.10904484987258911
Batch 64/64 loss: -0.1468106508255005
Epoch 348  Train loss: -0.13344510349572875  Val loss: 0.06185027987686629
Epoch 349
-------------------------------
Batch 1/64 loss: -0.15807116031646729
Batch 2/64 loss: -0.164983332157135
Batch 3/64 loss: -0.1659223437309265
Batch 4/64 loss: -0.16269689798355103
Batch 5/64 loss: -0.1551108956336975
Batch 6/64 loss: -0.1207994818687439
Batch 7/64 loss: -0.20111194252967834
Batch 8/64 loss: -0.1042824387550354
Batch 9/64 loss: -0.10228413343429565
Batch 10/64 loss: -0.15848374366760254
Batch 11/64 loss: -0.14910686016082764
Batch 12/64 loss: -0.15591192245483398
Batch 13/64 loss: -0.16453081369400024
Batch 14/64 loss: -0.15576881170272827
Batch 15/64 loss: -0.11204934120178223
Batch 16/64 loss: -0.12198162078857422
Batch 17/64 loss: -0.1295466423034668
Batch 18/64 loss: -0.1641542911529541
Batch 19/64 loss: -0.13752412796020508
Batch 20/64 loss: -0.14080309867858887
Batch 21/64 loss: -0.1443192958831787
Batch 22/64 loss: -0.11757957935333252
Batch 23/64 loss: -0.1083381175994873
Batch 24/64 loss: -0.14448261260986328
Batch 25/64 loss: -0.13749092817306519
Batch 26/64 loss: -0.10660874843597412
Batch 27/64 loss: -0.12087273597717285
Batch 28/64 loss: -0.14330077171325684
Batch 29/64 loss: -0.14765119552612305
Batch 30/64 loss: -0.12919992208480835
Batch 31/64 loss: -0.13227450847625732
Batch 32/64 loss: -0.1439269781112671
Batch 33/64 loss: -0.12585556507110596
Batch 34/64 loss: -0.15830278396606445
Batch 35/64 loss: -0.12107598781585693
Batch 36/64 loss: -0.1317933201789856
Batch 37/64 loss: -0.16477340459823608
Batch 38/64 loss: -0.1299474835395813
Batch 39/64 loss: -0.13163530826568604
Batch 40/64 loss: -0.1257961392402649
Batch 41/64 loss: -0.15665417909622192
Batch 42/64 loss: -0.16111081838607788
Batch 43/64 loss: -0.13318032026290894
Batch 44/64 loss: -0.13071870803833008
Batch 45/64 loss: -0.10268616676330566
Batch 46/64 loss: -0.1375269889831543
Batch 47/64 loss: -0.08844947814941406
Batch 48/64 loss: -0.10286378860473633
Batch 49/64 loss: -0.1239660382270813
Batch 50/64 loss: -0.07217657566070557
Batch 51/64 loss: -0.12088507413864136
Batch 52/64 loss: -0.12314122915267944
Batch 53/64 loss: -0.1357082724571228
Batch 54/64 loss: -0.1290554404258728
Batch 55/64 loss: -0.1258259415626526
Batch 56/64 loss: -0.1229810118675232
Batch 57/64 loss: -0.14595085382461548
Batch 58/64 loss: -0.13289088010787964
Batch 59/64 loss: -0.11326378583908081
Batch 60/64 loss: -0.14478331804275513
Batch 61/64 loss: -0.07795780897140503
Batch 62/64 loss: -0.1347743272781372
Batch 63/64 loss: -0.13603967428207397
Batch 64/64 loss: -0.09511995315551758
Epoch 349  Train loss: -0.13355733226327335  Val loss: 0.06927768225522385
Epoch 350
-------------------------------
Batch 1/64 loss: -0.11502647399902344
Batch 2/64 loss: -0.14274883270263672
Batch 3/64 loss: -0.12204837799072266
Batch 4/64 loss: -0.1405853033065796
Batch 5/64 loss: -0.1183626651763916
Batch 6/64 loss: -0.12483000755310059
Batch 7/64 loss: -0.12859177589416504
Batch 8/64 loss: -0.11606848239898682
Batch 9/64 loss: -0.1203923225402832
Batch 10/64 loss: -0.12248009443283081
Batch 11/64 loss: -0.131098210811615
Batch 12/64 loss: -0.1361117959022522
Batch 13/64 loss: -0.13612645864486694
Batch 14/64 loss: -0.0994265079498291
Batch 15/64 loss: -0.13133758306503296
Batch 16/64 loss: -0.11294788122177124
Batch 17/64 loss: -0.15972495079040527
Batch 18/64 loss: -0.1503559947013855
Batch 19/64 loss: -0.14763522148132324
Batch 20/64 loss: -0.11969846487045288
Batch 21/64 loss: -0.14132338762283325
Batch 22/64 loss: -0.13866251707077026
Batch 23/64 loss: -0.1532939076423645
Batch 24/64 loss: -0.13767915964126587
Batch 25/64 loss: -0.13316380977630615
Batch 26/64 loss: -0.1760324239730835
Batch 27/64 loss: -0.17946463823318481
Batch 28/64 loss: -0.12424886226654053
Batch 29/64 loss: -0.14970171451568604
Batch 30/64 loss: -0.10521209239959717
Batch 31/64 loss: -0.15866243839263916
Batch 32/64 loss: -0.13773703575134277
Batch 33/64 loss: -0.14724773168563843
Batch 34/64 loss: -0.15935742855072021
Batch 35/64 loss: -0.11897599697113037
Batch 36/64 loss: -0.12492573261260986
Batch 37/64 loss: -0.11788582801818848
Batch 38/64 loss: -0.11893868446350098
Batch 39/64 loss: -0.142930269241333
Batch 40/64 loss: -0.162672221660614
Batch 41/64 loss: -0.16066211462020874
Batch 42/64 loss: -0.15065604448318481
Batch 43/64 loss: -0.09185540676116943
Batch 44/64 loss: -0.11830073595046997
Batch 45/64 loss: -0.09822845458984375
Batch 46/64 loss: -0.1321811079978943
Batch 47/64 loss: -0.13190877437591553
Batch 48/64 loss: -0.12903088331222534
Batch 49/64 loss: -0.16031414270401
Batch 50/64 loss: -0.13469702005386353
Batch 51/64 loss: -0.12786006927490234
Batch 52/64 loss: -0.1748485565185547
Batch 53/64 loss: -0.13932102918624878
Batch 54/64 loss: -0.15015900135040283
Batch 55/64 loss: -0.17253315448760986
Batch 56/64 loss: -0.09974503517150879
Batch 57/64 loss: -0.11480283737182617
Batch 58/64 loss: -0.15047603845596313
Batch 59/64 loss: -0.07124018669128418
Batch 60/64 loss: -0.13537061214447021
Batch 61/64 loss: -0.12192714214324951
Batch 62/64 loss: -0.1497056484222412
Batch 63/64 loss: -0.13343769311904907
Batch 64/64 loss: -0.13352656364440918
Epoch 350  Train loss: -0.13416658663282208  Val loss: 0.07061773784381827
Epoch 351
-------------------------------
Batch 1/64 loss: -0.1765698790550232
Batch 2/64 loss: -0.11783146858215332
Batch 3/64 loss: -0.17623013257980347
Batch 4/64 loss: -0.07587987184524536
Batch 5/64 loss: -0.1574336290359497
Batch 6/64 loss: -0.13428056240081787
Batch 7/64 loss: -0.12938261032104492
Batch 8/64 loss: -0.14261245727539062
Batch 9/64 loss: -0.1497441530227661
Batch 10/64 loss: -0.13464152812957764
Batch 11/64 loss: -0.14172202348709106
Batch 12/64 loss: -0.12026894092559814
Batch 13/64 loss: -0.09135723114013672
Batch 14/64 loss: -0.13960790634155273
Batch 15/64 loss: -0.13245755434036255
Batch 16/64 loss: -0.14020013809204102
Batch 17/64 loss: -0.15763044357299805
Batch 18/64 loss: -0.13788115978240967
Batch 19/64 loss: -0.1275908350944519
Batch 20/64 loss: -0.13893228769302368
Batch 21/64 loss: -0.1404709815979004
Batch 22/64 loss: -0.10226655006408691
Batch 23/64 loss: -0.13278859853744507
Batch 24/64 loss: -0.1419772505760193
Batch 25/64 loss: -0.10168039798736572
Batch 26/64 loss: -0.1351243257522583
Batch 27/64 loss: -0.15327692031860352
Batch 28/64 loss: -0.09512042999267578
Batch 29/64 loss: -0.1537233591079712
Batch 30/64 loss: -0.14618432521820068
Batch 31/64 loss: -0.14355218410491943
Batch 32/64 loss: -0.14740246534347534
Batch 33/64 loss: -0.14759165048599243
Batch 34/64 loss: -0.15633219480514526
Batch 35/64 loss: -0.12347412109375
Batch 36/64 loss: -0.1593138575553894
Batch 37/64 loss: -0.15340816974639893
Batch 38/64 loss: -0.1424815058708191
Batch 39/64 loss: -0.14948642253875732
Batch 40/64 loss: -0.07003414630889893
Batch 41/64 loss: -0.12602251768112183
Batch 42/64 loss: -0.1313570737838745
Batch 43/64 loss: -0.1249692440032959
Batch 44/64 loss: -0.12211239337921143
Batch 45/64 loss: -0.12339937686920166
Batch 46/64 loss: -0.1557551622390747
Batch 47/64 loss: -0.14007019996643066
Batch 48/64 loss: -0.13976281881332397
Batch 49/64 loss: -0.12362426519393921
Batch 50/64 loss: -0.1298508644104004
Batch 51/64 loss: -0.13763773441314697
Batch 52/64 loss: -0.16075646877288818
Batch 53/64 loss: -0.1358070969581604
Batch 54/64 loss: -0.13586628437042236
Batch 55/64 loss: -0.10537105798721313
Batch 56/64 loss: -0.14235883951187134
Batch 57/64 loss: -0.12348127365112305
Batch 58/64 loss: -0.14542680978775024
Batch 59/64 loss: -0.08810675144195557
Batch 60/64 loss: -0.12484729290008545
Batch 61/64 loss: -0.1373719573020935
Batch 62/64 loss: -0.13645517826080322
Batch 63/64 loss: -0.14914071559906006
Batch 64/64 loss: -0.17574149370193481
Epoch 351  Train loss: -0.13473415164386524  Val loss: 0.06077119492992912
Epoch 352
-------------------------------
Batch 1/64 loss: -0.18024611473083496
Batch 2/64 loss: -0.1284361481666565
Batch 3/64 loss: -0.15407264232635498
Batch 4/64 loss: -0.1528632640838623
Batch 5/64 loss: -0.16570138931274414
Batch 6/64 loss: -0.1243407130241394
Batch 7/64 loss: -0.0978015661239624
Batch 8/64 loss: -0.16275262832641602
Batch 9/64 loss: -0.1494004726409912
Batch 10/64 loss: -0.1283128261566162
Batch 11/64 loss: -0.14013665914535522
Batch 12/64 loss: -0.13001251220703125
Batch 13/64 loss: -0.13014185428619385
Batch 14/64 loss: -0.15579086542129517
Batch 15/64 loss: -0.10935860872268677
Batch 16/64 loss: -0.11017990112304688
Batch 17/64 loss: -0.13338232040405273
Batch 18/64 loss: -0.16870981454849243
Batch 19/64 loss: -0.1385096311569214
Batch 20/64 loss: -0.13369929790496826
Batch 21/64 loss: -0.1009376049041748
Batch 22/64 loss: -0.14228087663650513
Batch 23/64 loss: -0.17854249477386475
Batch 24/64 loss: -0.15178930759429932
Batch 25/64 loss: -0.130989670753479
Batch 26/64 loss: -0.11891859769821167
Batch 27/64 loss: -0.1501997709274292
Batch 28/64 loss: -0.12534117698669434
Batch 29/64 loss: -0.12303882837295532
Batch 30/64 loss: -0.14762413501739502
Batch 31/64 loss: -0.18567323684692383
Batch 32/64 loss: -0.11626654863357544
Batch 33/64 loss: -0.1372564435005188
Batch 34/64 loss: -0.13704335689544678
Batch 35/64 loss: -0.12531518936157227
Batch 36/64 loss: -0.09831029176712036
Batch 37/64 loss: -0.16936945915222168
Batch 38/64 loss: -0.17149513959884644
Batch 39/64 loss: -0.07763099670410156
Batch 40/64 loss: -0.16815906763076782
Batch 41/64 loss: -0.14301669597625732
Batch 42/64 loss: -0.12430357933044434
Batch 43/64 loss: -0.10611462593078613
Batch 44/64 loss: -0.18022221326828003
Batch 45/64 loss: -0.17429643869400024
Batch 46/64 loss: -0.14786279201507568
Batch 47/64 loss: -0.14484643936157227
Batch 48/64 loss: -0.15550214052200317
Batch 49/64 loss: -0.0942426323890686
Batch 50/64 loss: -0.12355101108551025
Batch 51/64 loss: -0.14433926343917847
Batch 52/64 loss: -0.12492990493774414
Batch 53/64 loss: -0.1104961633682251
Batch 54/64 loss: -0.14104682207107544
Batch 55/64 loss: -0.11610519886016846
Batch 56/64 loss: -0.09499537944793701
Batch 57/64 loss: -0.13411331176757812
Batch 58/64 loss: -0.10787039995193481
Batch 59/64 loss: -0.11919891834259033
Batch 60/64 loss: -0.13816940784454346
Batch 61/64 loss: -0.12927812337875366
Batch 62/64 loss: -0.15401041507720947
Batch 63/64 loss: -0.13791751861572266
Batch 64/64 loss: -0.09512758255004883
Epoch 352  Train loss: -0.1359655922534419  Val loss: 0.06452274732163682
Epoch 353
-------------------------------
Batch 1/64 loss: -0.1803344488143921
Batch 2/64 loss: -0.13211029767990112
Batch 3/64 loss: -0.14893871545791626
Batch 4/64 loss: -0.16315215826034546
Batch 5/64 loss: -0.16198468208312988
Batch 6/64 loss: -0.11197549104690552
Batch 7/64 loss: -0.1451238989830017
Batch 8/64 loss: -0.15106821060180664
Batch 9/64 loss: -0.1354825496673584
Batch 10/64 loss: -0.12119358777999878
Batch 11/64 loss: -0.14301049709320068
Batch 12/64 loss: -0.1168559193611145
Batch 13/64 loss: -0.10427707433700562
Batch 14/64 loss: -0.12339985370635986
Batch 15/64 loss: -0.14822614192962646
Batch 16/64 loss: -0.15107059478759766
Batch 17/64 loss: -0.11007648706436157
Batch 18/64 loss: -0.1684126853942871
Batch 19/64 loss: -0.13181793689727783
Batch 20/64 loss: -0.11366647481918335
Batch 21/64 loss: -0.10057592391967773
Batch 22/64 loss: -0.13723742961883545
Batch 23/64 loss: -0.13535380363464355
Batch 24/64 loss: -0.11628884077072144
Batch 25/64 loss: -0.13330042362213135
Batch 26/64 loss: -0.11549943685531616
Batch 27/64 loss: -0.11446636915206909
Batch 28/64 loss: -0.1092684268951416
Batch 29/64 loss: -0.11716234683990479
Batch 30/64 loss: -0.12730634212493896
Batch 31/64 loss: -0.14691919088363647
Batch 32/64 loss: -0.1267939805984497
Batch 33/64 loss: -0.16094934940338135
Batch 34/64 loss: -0.13687145709991455
Batch 35/64 loss: -0.09811854362487793
Batch 36/64 loss: -0.13213062286376953
Batch 37/64 loss: -0.1281207799911499
Batch 38/64 loss: -0.11708098649978638
Batch 39/64 loss: -0.15563911199569702
Batch 40/64 loss: -0.1131356954574585
Batch 41/64 loss: -0.08673202991485596
Batch 42/64 loss: -0.16410601139068604
Batch 43/64 loss: -0.12712961435317993
Batch 44/64 loss: -0.11041378974914551
Batch 45/64 loss: -0.1661750078201294
Batch 46/64 loss: -0.14820337295532227
Batch 47/64 loss: -0.14683467149734497
Batch 48/64 loss: -0.15997982025146484
Batch 49/64 loss: -0.1383785605430603
Batch 50/64 loss: -0.16747689247131348
Batch 51/64 loss: -0.1572967767715454
Batch 52/64 loss: -0.11709856986999512
Batch 53/64 loss: -0.14763355255126953
Batch 54/64 loss: -0.13813531398773193
Batch 55/64 loss: -0.17181670665740967
Batch 56/64 loss: -0.13809418678283691
Batch 57/64 loss: -0.1443922519683838
Batch 58/64 loss: -0.10344016551971436
Batch 59/64 loss: -0.13609039783477783
Batch 60/64 loss: -0.12412464618682861
Batch 61/64 loss: -0.126539409160614
Batch 62/64 loss: -0.1331639289855957
Batch 63/64 loss: -0.15044832229614258
Batch 64/64 loss: -0.06627649068832397
Epoch 353  Train loss: -0.13392640212002924  Val loss: 0.060499327494106754
Epoch 354
-------------------------------
Batch 1/64 loss: -0.1840531826019287
Batch 2/64 loss: -0.13904958963394165
Batch 3/64 loss: -0.14896386861801147
Batch 4/64 loss: -0.10178160667419434
Batch 5/64 loss: -0.14250779151916504
Batch 6/64 loss: -0.14925193786621094
Batch 7/64 loss: -0.14612317085266113
Batch 8/64 loss: -0.13547474145889282
Batch 9/64 loss: -0.1141999363899231
Batch 10/64 loss: -0.15125060081481934
Batch 11/64 loss: -0.13638609647750854
Batch 12/64 loss: -0.16625851392745972
Batch 13/64 loss: -0.13090217113494873
Batch 14/64 loss: -0.16530132293701172
Batch 15/64 loss: -0.1049681305885315
Batch 16/64 loss: -0.12335771322250366
Batch 17/64 loss: -0.15088945627212524
Batch 18/64 loss: -0.12417536973953247
Batch 19/64 loss: -0.12531626224517822
Batch 20/64 loss: -0.16488206386566162
Batch 21/64 loss: -0.11806666851043701
Batch 22/64 loss: -0.16131985187530518
Batch 23/64 loss: -0.1569666862487793
Batch 24/64 loss: -0.17875242233276367
Batch 25/64 loss: -0.14966440200805664
Batch 26/64 loss: -0.12390673160552979
Batch 27/64 loss: -0.1714584231376648
Batch 28/64 loss: -0.17081910371780396
Batch 29/64 loss: -0.1490035057067871
Batch 30/64 loss: -0.15563958883285522
Batch 31/64 loss: -0.14213770627975464
Batch 32/64 loss: -0.1508161425590515
Batch 33/64 loss: -0.11385679244995117
Batch 34/64 loss: -0.12966018915176392
Batch 35/64 loss: -0.11836600303649902
Batch 36/64 loss: -0.12405496835708618
Batch 37/64 loss: -0.15779662132263184
Batch 38/64 loss: -0.11265230178833008
Batch 39/64 loss: -0.10931646823883057
Batch 40/64 loss: -0.1330541968345642
Batch 41/64 loss: -0.10524433851242065
Batch 42/64 loss: -0.13917863368988037
Batch 43/64 loss: -0.14473849534988403
Batch 44/64 loss: -0.1188628077507019
Batch 45/64 loss: -0.08797472715377808
Batch 46/64 loss: -0.1192522644996643
Batch 47/64 loss: -0.14101338386535645
Batch 48/64 loss: -0.14773398637771606
Batch 49/64 loss: -0.13150489330291748
Batch 50/64 loss: -0.14429283142089844
Batch 51/64 loss: -0.13174808025360107
Batch 52/64 loss: -0.1175035834312439
Batch 53/64 loss: -0.12099778652191162
Batch 54/64 loss: -0.08562374114990234
Batch 55/64 loss: -0.15114718675613403
Batch 56/64 loss: -0.1254928708076477
Batch 57/64 loss: -0.1450592279434204
Batch 58/64 loss: -0.13340413570404053
Batch 59/64 loss: -0.15828442573547363
Batch 60/64 loss: -0.14313310384750366
Batch 61/64 loss: -0.16734665632247925
Batch 62/64 loss: -0.12092185020446777
Batch 63/64 loss: -0.09760284423828125
Batch 64/64 loss: -0.10863345861434937
Epoch 354  Train loss: -0.1363441450923097  Val loss: 0.06219258341182958
Epoch 355
-------------------------------
Batch 1/64 loss: -0.17626476287841797
Batch 2/64 loss: -0.12564963102340698
Batch 3/64 loss: -0.15340662002563477
Batch 4/64 loss: -0.137839674949646
Batch 5/64 loss: -0.15351951122283936
Batch 6/64 loss: -0.14246928691864014
Batch 7/64 loss: -0.1662919521331787
Batch 8/64 loss: -0.1589902639389038
Batch 9/64 loss: -0.14960938692092896
Batch 10/64 loss: -0.16341853141784668
Batch 11/64 loss: -0.1535908579826355
Batch 12/64 loss: -0.14369982481002808
Batch 13/64 loss: -0.14421337842941284
Batch 14/64 loss: -0.13494277000427246
Batch 15/64 loss: -0.15294396877288818
Batch 16/64 loss: -0.1274755597114563
Batch 17/64 loss: -0.10436993837356567
Batch 18/64 loss: -0.08646965026855469
Batch 19/64 loss: -0.1535835862159729
Batch 20/64 loss: -0.1377663016319275
Batch 21/64 loss: -0.15043485164642334
Batch 22/64 loss: -0.11387205123901367
Batch 23/64 loss: -0.16128116846084595
Batch 24/64 loss: -0.13089048862457275
Batch 25/64 loss: -0.1396622657775879
Batch 26/64 loss: -0.14064359664916992
Batch 27/64 loss: -0.15602022409439087
Batch 28/64 loss: -0.13775593042373657
Batch 29/64 loss: -0.11208999156951904
Batch 30/64 loss: -0.14288371801376343
Batch 31/64 loss: -0.12956571578979492
Batch 32/64 loss: -0.15502679347991943
Batch 33/64 loss: -0.13967347145080566
Batch 34/64 loss: -0.1468113660812378
Batch 35/64 loss: -0.13793092966079712
Batch 36/64 loss: -0.14590483903884888
Batch 37/64 loss: -0.09968984127044678
Batch 38/64 loss: -0.10394352674484253
Batch 39/64 loss: -0.11972618103027344
Batch 40/64 loss: -0.11688035726547241
Batch 41/64 loss: -0.14707207679748535
Batch 42/64 loss: -0.12458932399749756
Batch 43/64 loss: -0.134971022605896
Batch 44/64 loss: -0.14089888334274292
Batch 45/64 loss: -0.13164961338043213
Batch 46/64 loss: -0.14327383041381836
Batch 47/64 loss: -0.1444830298423767
Batch 48/64 loss: -0.17218971252441406
Batch 49/64 loss: -0.15093469619750977
Batch 50/64 loss: -0.11098802089691162
Batch 51/64 loss: -0.13914459943771362
Batch 52/64 loss: -0.0659182071685791
Batch 53/64 loss: -0.1263689398765564
Batch 54/64 loss: -0.13723331689834595
Batch 55/64 loss: -0.14237290620803833
Batch 56/64 loss: -0.1315699815750122
Batch 57/64 loss: -0.15546369552612305
Batch 58/64 loss: -0.15353238582611084
Batch 59/64 loss: -0.14842331409454346
Batch 60/64 loss: -0.13669657707214355
Batch 61/64 loss: -0.1501883864402771
Batch 62/64 loss: -0.12180393934249878
Batch 63/64 loss: -0.165211021900177
Batch 64/64 loss: -0.14639288187026978
Epoch 355  Train loss: -0.13854078288171806  Val loss: 0.05997050156708026
Epoch 356
-------------------------------
Batch 1/64 loss: -0.14598023891448975
Batch 2/64 loss: -0.15659117698669434
Batch 3/64 loss: -0.1804978847503662
Batch 4/64 loss: -0.10727155208587646
Batch 5/64 loss: -0.06281650066375732
Batch 6/64 loss: -0.11364924907684326
Batch 7/64 loss: -0.15284985303878784
Batch 8/64 loss: -0.16232389211654663
Batch 9/64 loss: -0.1708589792251587
Batch 10/64 loss: -0.13232135772705078
Batch 11/64 loss: -0.15975093841552734
Batch 12/64 loss: -0.14162510633468628
Batch 13/64 loss: -0.1624307632446289
Batch 14/64 loss: -0.15081804990768433
Batch 15/64 loss: -0.11962777376174927
Batch 16/64 loss: -0.1335044503211975
Batch 17/64 loss: -0.1615009903907776
Batch 18/64 loss: -0.12359094619750977
Batch 19/64 loss: -0.13242411613464355
Batch 20/64 loss: -0.16551744937896729
Batch 21/64 loss: -0.15733808279037476
Batch 22/64 loss: -0.1449052095413208
Batch 23/64 loss: -0.13088130950927734
Batch 24/64 loss: -0.12312787771224976
Batch 25/64 loss: -0.10070198774337769
Batch 26/64 loss: -0.15108734369277954
Batch 27/64 loss: -0.11134916543960571
Batch 28/64 loss: -0.12710320949554443
Batch 29/64 loss: -0.1458929181098938
Batch 30/64 loss: -0.13561469316482544
Batch 31/64 loss: -0.1207054853439331
Batch 32/64 loss: -0.1127856969833374
Batch 33/64 loss: -0.12004423141479492
Batch 34/64 loss: -0.14212560653686523
Batch 35/64 loss: -0.15154165029525757
Batch 36/64 loss: -0.11361634731292725
Batch 37/64 loss: -0.11381077766418457
Batch 38/64 loss: -0.15049535036087036
Batch 39/64 loss: -0.13640457391738892
Batch 40/64 loss: -0.15417903661727905
Batch 41/64 loss: -0.16427242755889893
Batch 42/64 loss: -0.12211030721664429
Batch 43/64 loss: -0.1683383584022522
Batch 44/64 loss: -0.13721638917922974
Batch 45/64 loss: -0.1393129825592041
Batch 46/64 loss: -0.17285513877868652
Batch 47/64 loss: -0.1401161551475525
Batch 48/64 loss: -0.14709532260894775
Batch 49/64 loss: -0.1618620753288269
Batch 50/64 loss: -0.05241525173187256
Batch 51/64 loss: -0.14389842748641968
Batch 52/64 loss: -0.10979145765304565
Batch 53/64 loss: -0.15787577629089355
Batch 54/64 loss: -0.15982913970947266
Batch 55/64 loss: -0.139828622341156
Batch 56/64 loss: -0.13809734582901
Batch 57/64 loss: -0.14133399724960327
Batch 58/64 loss: -0.1463192105293274
Batch 59/64 loss: -0.14987576007843018
Batch 60/64 loss: -0.12213778495788574
Batch 61/64 loss: -0.1588343381881714
Batch 62/64 loss: -0.13777077198028564
Batch 63/64 loss: -0.1500290036201477
Batch 64/64 loss: -0.13665366172790527
Epoch 356  Train loss: -0.13871949981240667  Val loss: 0.061098407224281545
Epoch 357
-------------------------------
Batch 1/64 loss: -0.1673734188079834
Batch 2/64 loss: -0.15765339136123657
Batch 3/64 loss: -0.14059579372406006
Batch 4/64 loss: -0.13430321216583252
Batch 5/64 loss: -0.1105889081954956
Batch 6/64 loss: -0.14459222555160522
Batch 7/64 loss: -0.11972159147262573
Batch 8/64 loss: -0.11558634042739868
Batch 9/64 loss: -0.15438181161880493
Batch 10/64 loss: -0.12699204683303833
Batch 11/64 loss: -0.16944628953933716
Batch 12/64 loss: -0.14642006158828735
Batch 13/64 loss: -0.13960349559783936
Batch 14/64 loss: -0.14752811193466187
Batch 15/64 loss: -0.10599398612976074
Batch 16/64 loss: -0.16031193733215332
Batch 17/64 loss: -0.1057283878326416
Batch 18/64 loss: -0.06520307064056396
Batch 19/64 loss: -0.12704241275787354
Batch 20/64 loss: -0.147841215133667
Batch 21/64 loss: -0.13888001441955566
Batch 22/64 loss: -0.1522170901298523
Batch 23/64 loss: -0.15695291757583618
Batch 24/64 loss: -0.16767865419387817
Batch 25/64 loss: -0.12738174200057983
Batch 26/64 loss: -0.13205605745315552
Batch 27/64 loss: -0.14155519008636475
Batch 28/64 loss: -0.15439903736114502
Batch 29/64 loss: -0.13527804613113403
Batch 30/64 loss: -0.15487879514694214
Batch 31/64 loss: -0.15443730354309082
Batch 32/64 loss: -0.10944312810897827
Batch 33/64 loss: -0.11301761865615845
Batch 34/64 loss: -0.11795443296432495
Batch 35/64 loss: -0.1414540410041809
Batch 36/64 loss: -0.13457965850830078
Batch 37/64 loss: -0.14268982410430908
Batch 38/64 loss: -0.1309678554534912
Batch 39/64 loss: -0.1542237401008606
Batch 40/64 loss: -0.13432377576828003
Batch 41/64 loss: -0.1384919285774231
Batch 42/64 loss: -0.10046225786209106
Batch 43/64 loss: -0.1708117127418518
Batch 44/64 loss: -0.10797739028930664
Batch 45/64 loss: -0.16803359985351562
Batch 46/64 loss: -0.14864861965179443
Batch 47/64 loss: -0.16844391822814941
Batch 48/64 loss: -0.14272922277450562
Batch 49/64 loss: -0.13916432857513428
Batch 50/64 loss: -0.15829014778137207
Batch 51/64 loss: -0.1275424361228943
Batch 52/64 loss: -0.13674604892730713
Batch 53/64 loss: -0.16599053144454956
Batch 54/64 loss: -0.17705082893371582
Batch 55/64 loss: -0.14039552211761475
Batch 56/64 loss: -0.10614967346191406
Batch 57/64 loss: -0.14481723308563232
Batch 58/64 loss: -0.13085675239562988
Batch 59/64 loss: -0.1446777582168579
Batch 60/64 loss: -0.1529552936553955
Batch 61/64 loss: -0.15312016010284424
Batch 62/64 loss: -0.1193118691444397
Batch 63/64 loss: -0.12011849880218506
Batch 64/64 loss: -0.13793063163757324
Epoch 357  Train loss: -0.138784475887523  Val loss: 0.06249382753962094
Epoch 358
-------------------------------
Batch 1/64 loss: -0.11897128820419312
Batch 2/64 loss: -0.05986148118972778
Batch 3/64 loss: -0.13329404592514038
Batch 4/64 loss: -0.15686333179473877
Batch 5/64 loss: -0.15534734725952148
Batch 6/64 loss: -0.14672517776489258
Batch 7/64 loss: -0.09305703639984131
Batch 8/64 loss: -0.1766190528869629
Batch 9/64 loss: -0.16736871004104614
Batch 10/64 loss: -0.13472771644592285
Batch 11/64 loss: -0.15034914016723633
Batch 12/64 loss: -0.13466143608093262
Batch 13/64 loss: -0.14672285318374634
Batch 14/64 loss: -0.18136537075042725
Batch 15/64 loss: -0.1601359248161316
Batch 16/64 loss: -0.1526849865913391
Batch 17/64 loss: -0.10822409391403198
Batch 18/64 loss: -0.11632037162780762
Batch 19/64 loss: -0.15283167362213135
Batch 20/64 loss: -0.11538797616958618
Batch 21/64 loss: -0.1447533369064331
Batch 22/64 loss: -0.09195524454116821
Batch 23/64 loss: -0.15959417819976807
Batch 24/64 loss: -0.12179702520370483
Batch 25/64 loss: -0.14344894886016846
Batch 26/64 loss: -0.11629533767700195
Batch 27/64 loss: -0.11661547422409058
Batch 28/64 loss: -0.12192302942276001
Batch 29/64 loss: -0.1511601209640503
Batch 30/64 loss: -0.156005859375
Batch 31/64 loss: -0.08284509181976318
Batch 32/64 loss: -0.10577470064163208
Batch 33/64 loss: -0.11077648401260376
Batch 34/64 loss: -0.16604357957839966
Batch 35/64 loss: -0.12320679426193237
Batch 36/64 loss: -0.12484759092330933
Batch 37/64 loss: -0.1250632405281067
Batch 38/64 loss: -0.12201541662216187
Batch 39/64 loss: -0.127763569355011
Batch 40/64 loss: -0.12810266017913818
Batch 41/64 loss: -0.15886372327804565
Batch 42/64 loss: -0.11829417943954468
Batch 43/64 loss: -0.11571991443634033
Batch 44/64 loss: -0.16163241863250732
Batch 45/64 loss: -0.14240401983261108
Batch 46/64 loss: -0.15828728675842285
Batch 47/64 loss: -0.16153329610824585
Batch 48/64 loss: -0.150995135307312
Batch 49/64 loss: -0.1417069435119629
Batch 50/64 loss: -0.14384829998016357
Batch 51/64 loss: -0.14303505420684814
Batch 52/64 loss: -0.07092893123626709
Batch 53/64 loss: -0.13901537656784058
Batch 54/64 loss: -0.12699878215789795
Batch 55/64 loss: -0.11818897724151611
Batch 56/64 loss: -0.15287446975708008
Batch 57/64 loss: -0.11130189895629883
Batch 58/64 loss: -0.14827823638916016
Batch 59/64 loss: -0.16709166765213013
Batch 60/64 loss: -0.1659122109413147
Batch 61/64 loss: -0.14303815364837646
Batch 62/64 loss: -0.16495996713638306
Batch 63/64 loss: -0.12521463632583618
Batch 64/64 loss: -0.12200874090194702
Epoch 358  Train loss: -0.13526489150290397  Val loss: 0.05866514776170868
Epoch 359
-------------------------------
Batch 1/64 loss: -0.16531133651733398
Batch 2/64 loss: -0.14524990320205688
Batch 3/64 loss: -0.16413289308547974
Batch 4/64 loss: -0.14295315742492676
Batch 5/64 loss: -0.16245174407958984
Batch 6/64 loss: -0.16392070055007935
Batch 7/64 loss: -0.13931727409362793
Batch 8/64 loss: -0.19614917039871216
Batch 9/64 loss: -0.18225467205047607
Batch 10/64 loss: -0.13662594556808472
Batch 11/64 loss: -0.14113622903823853
Batch 12/64 loss: -0.1589985489845276
Batch 13/64 loss: -0.13567626476287842
Batch 14/64 loss: -0.13989859819412231
Batch 15/64 loss: -0.11803609132766724
Batch 16/64 loss: -0.16129666566848755
Batch 17/64 loss: -0.11217927932739258
Batch 18/64 loss: -0.16043436527252197
Batch 19/64 loss: -0.12448084354400635
Batch 20/64 loss: -0.16598069667816162
Batch 21/64 loss: -0.0991550087928772
Batch 22/64 loss: -0.10614556074142456
Batch 23/64 loss: -0.1112823486328125
Batch 24/64 loss: -0.16239088773727417
Batch 25/64 loss: -0.1401241421699524
Batch 26/64 loss: -0.17650312185287476
Batch 27/64 loss: -0.10299980640411377
Batch 28/64 loss: -0.10515934228897095
Batch 29/64 loss: -0.10746407508850098
Batch 30/64 loss: -0.1407793164253235
Batch 31/64 loss: -0.15555471181869507
Batch 32/64 loss: -0.10433143377304077
Batch 33/64 loss: -0.11878269910812378
Batch 34/64 loss: -0.15326356887817383
Batch 35/64 loss: -0.12855201959609985
Batch 36/64 loss: -0.13013678789138794
Batch 37/64 loss: -0.1499977707862854
Batch 38/64 loss: -0.10170799493789673
Batch 39/64 loss: -0.12265115976333618
Batch 40/64 loss: -0.13975465297698975
Batch 41/64 loss: -0.08781129121780396
Batch 42/64 loss: -0.14526808261871338
Batch 43/64 loss: -0.14810383319854736
Batch 44/64 loss: -0.15879851579666138
Batch 45/64 loss: -0.10575759410858154
Batch 46/64 loss: -0.1405237913131714
Batch 47/64 loss: -0.15811657905578613
Batch 48/64 loss: -0.11648505926132202
Batch 49/64 loss: -0.10723066329956055
Batch 50/64 loss: -0.11985641717910767
Batch 51/64 loss: -0.11028587818145752
Batch 52/64 loss: -0.08908736705780029
Batch 53/64 loss: -0.14671361446380615
Batch 54/64 loss: -0.1691211462020874
Batch 55/64 loss: -0.1634061336517334
Batch 56/64 loss: -0.1427585482597351
Batch 57/64 loss: -0.17108815908432007
Batch 58/64 loss: -0.12537741661071777
Batch 59/64 loss: -0.14422321319580078
Batch 60/64 loss: -0.11991667747497559
Batch 61/64 loss: -0.13640129566192627
Batch 62/64 loss: -0.1365351676940918
Batch 63/64 loss: -0.13298296928405762
Batch 64/64 loss: -0.17585277557373047
Epoch 359  Train loss: -0.13774054564681706  Val loss: 0.06276724100932222
Epoch 360
-------------------------------
Batch 1/64 loss: -0.1677926778793335
Batch 2/64 loss: -0.14611589908599854
Batch 3/64 loss: -0.13629072904586792
Batch 4/64 loss: -0.15518105030059814
Batch 5/64 loss: -0.15783119201660156
Batch 6/64 loss: -0.16478031873703003
Batch 7/64 loss: -0.14901429414749146
Batch 8/64 loss: -0.1466018557548523
Batch 9/64 loss: -0.15907305479049683
Batch 10/64 loss: -0.15777134895324707
Batch 11/64 loss: -0.10616171360015869
Batch 12/64 loss: -0.17127227783203125
Batch 13/64 loss: -0.15929770469665527
Batch 14/64 loss: -0.15349829196929932
Batch 15/64 loss: -0.15474861860275269
Batch 16/64 loss: -0.12734729051589966
Batch 17/64 loss: -0.15764951705932617
Batch 18/64 loss: -0.0918460488319397
Batch 19/64 loss: -0.11658674478530884
Batch 20/64 loss: -0.18042320013046265
Batch 21/64 loss: -0.1073218584060669
Batch 22/64 loss: -0.07188886404037476
Batch 23/64 loss: -0.13682466745376587
Batch 24/64 loss: -0.13936489820480347
Batch 25/64 loss: -0.15604037046432495
Batch 26/64 loss: -0.12896764278411865
Batch 27/64 loss: -0.11016726493835449
Batch 28/64 loss: -0.13569796085357666
Batch 29/64 loss: -0.15696483850479126
Batch 30/64 loss: -0.09358859062194824
Batch 31/64 loss: -0.12492024898529053
Batch 32/64 loss: -0.16284018754959106
Batch 33/64 loss: -0.12919366359710693
Batch 34/64 loss: -0.1676342487335205
Batch 35/64 loss: -0.13183462619781494
Batch 36/64 loss: -0.15336859226226807
Batch 37/64 loss: -0.10635840892791748
Batch 38/64 loss: -0.1708369255065918
Batch 39/64 loss: -0.1414337158203125
Batch 40/64 loss: -0.14290165901184082
Batch 41/64 loss: -0.10699081420898438
Batch 42/64 loss: -0.14831942319869995
Batch 43/64 loss: -0.10916757583618164
Batch 44/64 loss: -0.16123080253601074
Batch 45/64 loss: -0.08632868528366089
Batch 46/64 loss: -0.14401668310165405
Batch 47/64 loss: -0.1490776538848877
Batch 48/64 loss: -0.14193224906921387
Batch 49/64 loss: -0.12331372499465942
Batch 50/64 loss: -0.13035345077514648
Batch 51/64 loss: -0.132828950881958
Batch 52/64 loss: -0.12964117527008057
Batch 53/64 loss: -0.11539626121520996
Batch 54/64 loss: -0.14432412385940552
Batch 55/64 loss: -0.18268531560897827
Batch 56/64 loss: -0.14669835567474365
Batch 57/64 loss: -0.16805166006088257
Batch 58/64 loss: -0.16290676593780518
Batch 59/64 loss: -0.14418143033981323
Batch 60/64 loss: -0.150995135307312
Batch 61/64 loss: -0.0748131275177002
Batch 62/64 loss: -0.1611512303352356
Batch 63/64 loss: -0.11773395538330078
Batch 64/64 loss: -0.1260027289390564
Epoch 360  Train loss: -0.13888742993859685  Val loss: 0.06718396526022055
Epoch 361
-------------------------------
Batch 1/64 loss: -0.12824678421020508
Batch 2/64 loss: -0.15093004703521729
Batch 3/64 loss: -0.14437884092330933
Batch 4/64 loss: -0.17374873161315918
Batch 5/64 loss: -0.1625942587852478
Batch 6/64 loss: -0.12653732299804688
Batch 7/64 loss: -0.14493131637573242
Batch 8/64 loss: -0.14875853061676025
Batch 9/64 loss: -0.1449705958366394
Batch 10/64 loss: -0.11934047937393188
Batch 11/64 loss: -0.12462908029556274
Batch 12/64 loss: -0.15925204753875732
Batch 13/64 loss: -0.10760939121246338
Batch 14/64 loss: -0.14085078239440918
Batch 15/64 loss: -0.14908123016357422
Batch 16/64 loss: -0.13767552375793457
Batch 17/64 loss: -0.1342846155166626
Batch 18/64 loss: -0.1015586256980896
Batch 19/64 loss: -0.16117644309997559
Batch 20/64 loss: -0.15907829999923706
Batch 21/64 loss: -0.13795995712280273
Batch 22/64 loss: -0.15012401342391968
Batch 23/64 loss: -0.126836895942688
Batch 24/64 loss: -0.13537979125976562
Batch 25/64 loss: -0.12289267778396606
Batch 26/64 loss: -0.14738184213638306
Batch 27/64 loss: -0.15940284729003906
Batch 28/64 loss: -0.10047322511672974
Batch 29/64 loss: -0.1465771198272705
Batch 30/64 loss: -0.1355806589126587
Batch 31/64 loss: -0.15274900197982788
Batch 32/64 loss: -0.14128756523132324
Batch 33/64 loss: -0.17368173599243164
Batch 34/64 loss: -0.15726590156555176
Batch 35/64 loss: -0.18391507863998413
Batch 36/64 loss: -0.14099067449569702
Batch 37/64 loss: -0.16257095336914062
Batch 38/64 loss: -0.1528899073600769
Batch 39/64 loss: -0.145255446434021
Batch 40/64 loss: -0.12831008434295654
Batch 41/64 loss: -0.14688682556152344
Batch 42/64 loss: -0.11527144908905029
Batch 43/64 loss: -0.1430448293685913
Batch 44/64 loss: -0.13501769304275513
Batch 45/64 loss: -0.14318746328353882
Batch 46/64 loss: -0.1298142671585083
Batch 47/64 loss: -0.12872159481048584
Batch 48/64 loss: -0.12581324577331543
Batch 49/64 loss: -0.08810091018676758
Batch 50/64 loss: -0.11690527200698853
Batch 51/64 loss: -0.1376485824584961
Batch 52/64 loss: -0.1429038643836975
Batch 53/64 loss: -0.1471220850944519
Batch 54/64 loss: -0.13538968563079834
Batch 55/64 loss: -0.11662191152572632
Batch 56/64 loss: -0.17140543460845947
Batch 57/64 loss: -0.12961363792419434
Batch 58/64 loss: -0.12012302875518799
Batch 59/64 loss: -0.1042776107788086
Batch 60/64 loss: -0.15012621879577637
Batch 61/64 loss: -0.13905644416809082
Batch 62/64 loss: -0.14969885349273682
Batch 63/64 loss: -0.11532682180404663
Batch 64/64 loss: -0.12154465913772583
Epoch 361  Train loss: -0.13873560078003827  Val loss: 0.0686133417886557
Epoch 362
-------------------------------
Batch 1/64 loss: -0.14089637994766235
Batch 2/64 loss: -0.09013998508453369
Batch 3/64 loss: -0.16974318027496338
Batch 4/64 loss: -0.14199453592300415
Batch 5/64 loss: -0.1612674593925476
Batch 6/64 loss: -0.11142969131469727
Batch 7/64 loss: -0.15131878852844238
Batch 8/64 loss: -0.1614300012588501
Batch 9/64 loss: -0.12084895372390747
Batch 10/64 loss: -0.17615628242492676
Batch 11/64 loss: -0.14362168312072754
Batch 12/64 loss: -0.09345769882202148
Batch 13/64 loss: -0.14389896392822266
Batch 14/64 loss: -0.18308031558990479
Batch 15/64 loss: -0.13755661249160767
Batch 16/64 loss: -0.16982638835906982
Batch 17/64 loss: -0.13971924781799316
Batch 18/64 loss: -0.1451103687286377
Batch 19/64 loss: -0.1293271780014038
Batch 20/64 loss: -0.173653244972229
Batch 21/64 loss: -0.19029390811920166
Batch 22/64 loss: -0.17526447772979736
Batch 23/64 loss: -0.13709652423858643
Batch 24/64 loss: -0.11231327056884766
Batch 25/64 loss: -0.1434546709060669
Batch 26/64 loss: -0.14393872022628784
Batch 27/64 loss: -0.143876850605011
Batch 28/64 loss: -0.14978796243667603
Batch 29/64 loss: -0.16062963008880615
Batch 30/64 loss: -0.1353064775466919
Batch 31/64 loss: -0.15388619899749756
Batch 32/64 loss: -0.13682502508163452
Batch 33/64 loss: -0.16120469570159912
Batch 34/64 loss: -0.16769134998321533
Batch 35/64 loss: -0.121346116065979
Batch 36/64 loss: -0.16196870803833008
Batch 37/64 loss: -0.11642396450042725
Batch 38/64 loss: -0.12301278114318848
Batch 39/64 loss: -0.12269425392150879
Batch 40/64 loss: -0.11933869123458862
Batch 41/64 loss: -0.15883636474609375
Batch 42/64 loss: -0.1601313352584839
Batch 43/64 loss: -0.14366477727890015
Batch 44/64 loss: -0.10935842990875244
Batch 45/64 loss: -0.1359378695487976
Batch 46/64 loss: -0.12601619958877563
Batch 47/64 loss: -0.1470540165901184
Batch 48/64 loss: -0.1272890567779541
Batch 49/64 loss: -0.13935917615890503
Batch 50/64 loss: -0.11444616317749023
Batch 51/64 loss: -0.11357665061950684
Batch 52/64 loss: -0.16458189487457275
Batch 53/64 loss: -0.15094411373138428
Batch 54/64 loss: -0.14711761474609375
Batch 55/64 loss: -0.14150232076644897
Batch 56/64 loss: -0.11383438110351562
Batch 57/64 loss: -0.1353757381439209
Batch 58/64 loss: -0.11056673526763916
Batch 59/64 loss: -0.1385260820388794
Batch 60/64 loss: -0.10602390766143799
Batch 61/64 loss: -0.14446818828582764
Batch 62/64 loss: -0.14768368005752563
Batch 63/64 loss: -0.16965258121490479
Batch 64/64 loss: -0.10971242189407349
Epoch 362  Train loss: -0.14100490714989455  Val loss: 0.07543594239094004
Epoch 363
-------------------------------
Batch 1/64 loss: -0.14294934272766113
Batch 2/64 loss: -0.13290846347808838
Batch 3/64 loss: -0.12533080577850342
Batch 4/64 loss: -0.15027141571044922
Batch 5/64 loss: -0.14458274841308594
Batch 6/64 loss: -0.17145293951034546
Batch 7/64 loss: -0.11731398105621338
Batch 8/64 loss: -0.1280270218849182
Batch 9/64 loss: -0.13984394073486328
Batch 10/64 loss: -0.16350609064102173
Batch 11/64 loss: -0.09683340787887573
Batch 12/64 loss: -0.12255215644836426
Batch 13/64 loss: -0.16854238510131836
Batch 14/64 loss: -0.12421858310699463
Batch 15/64 loss: -0.15032154321670532
Batch 16/64 loss: -0.11820626258850098
Batch 17/64 loss: -0.16693508625030518
Batch 18/64 loss: -0.16874456405639648
Batch 19/64 loss: -0.10649281740188599
Batch 20/64 loss: -0.13535934686660767
Batch 21/64 loss: -0.15231961011886597
Batch 22/64 loss: -0.15733277797698975
Batch 23/64 loss: -0.12926596403121948
Batch 24/64 loss: -0.15765845775604248
Batch 25/64 loss: -0.13897597789764404
Batch 26/64 loss: -0.1723077893257141
Batch 27/64 loss: -0.12450051307678223
Batch 28/64 loss: -0.1079719066619873
Batch 29/64 loss: -0.14774829149246216
Batch 30/64 loss: -0.13732647895812988
Batch 31/64 loss: -0.10747319459915161
Batch 32/64 loss: -0.16534388065338135
Batch 33/64 loss: -0.12195706367492676
Batch 34/64 loss: -0.13727611303329468
Batch 35/64 loss: -0.12709850072860718
Batch 36/64 loss: -0.12473815679550171
Batch 37/64 loss: -0.11132436990737915
Batch 38/64 loss: -0.1756104826927185
Batch 39/64 loss: -0.1663285493850708
Batch 40/64 loss: -0.14187967777252197
Batch 41/64 loss: -0.1509581208229065
Batch 42/64 loss: -0.10386776924133301
Batch 43/64 loss: -0.13576394319534302
Batch 44/64 loss: -0.13427239656448364
Batch 45/64 loss: -0.12509191036224365
Batch 46/64 loss: -0.11560606956481934
Batch 47/64 loss: -0.1262006163597107
Batch 48/64 loss: -0.12585753202438354
Batch 49/64 loss: -0.1311536431312561
Batch 50/64 loss: -0.13528817892074585
Batch 51/64 loss: -0.16325783729553223
Batch 52/64 loss: -0.13700318336486816
Batch 53/64 loss: -0.13186824321746826
Batch 54/64 loss: -0.14329510927200317
Batch 55/64 loss: -0.11471062898635864
Batch 56/64 loss: -0.15042740106582642
Batch 57/64 loss: -0.12307119369506836
Batch 58/64 loss: -0.14270806312561035
Batch 59/64 loss: -0.12353658676147461
Batch 60/64 loss: -0.13341659307479858
Batch 61/64 loss: -0.11720055341720581
Batch 62/64 loss: -0.13071942329406738
Batch 63/64 loss: -0.14487826824188232
Batch 64/64 loss: -0.12356841564178467
Epoch 363  Train loss: -0.1366534938999251  Val loss: 0.059747487818662244
Epoch 364
-------------------------------
Batch 1/64 loss: -0.16248726844787598
Batch 2/64 loss: -0.16075891256332397
Batch 3/64 loss: -0.1440415382385254
Batch 4/64 loss: -0.13882964849472046
Batch 5/64 loss: -0.11347222328186035
Batch 6/64 loss: -0.17548269033432007
Batch 7/64 loss: -0.16893064975738525
Batch 8/64 loss: -0.09120452404022217
Batch 9/64 loss: -0.1580258011817932
Batch 10/64 loss: -0.1591200828552246
Batch 11/64 loss: -0.13377594947814941
Batch 12/64 loss: -0.16096365451812744
Batch 13/64 loss: -0.13693451881408691
Batch 14/64 loss: -0.16559761762619019
Batch 15/64 loss: -0.16727620363235474
Batch 16/64 loss: -0.17116910219192505
Batch 17/64 loss: -0.1635187864303589
Batch 18/64 loss: -0.1388249397277832
Batch 19/64 loss: -0.14991378784179688
Batch 20/64 loss: -0.1550573706626892
Batch 21/64 loss: -0.1664748191833496
Batch 22/64 loss: -0.15418756008148193
Batch 23/64 loss: -0.14260834455490112
Batch 24/64 loss: -0.1783352494239807
Batch 25/64 loss: -0.14290213584899902
Batch 26/64 loss: -0.1417778730392456
Batch 27/64 loss: -0.11241006851196289
Batch 28/64 loss: -0.16743987798690796
Batch 29/64 loss: -0.14003479480743408
Batch 30/64 loss: -0.11393022537231445
Batch 31/64 loss: -0.08762437105178833
Batch 32/64 loss: -0.1357877254486084
Batch 33/64 loss: -0.1581944227218628
Batch 34/64 loss: -0.13593131303787231
Batch 35/64 loss: -0.12275499105453491
Batch 36/64 loss: -0.15899282693862915
Batch 37/64 loss: -0.11652088165283203
Batch 38/64 loss: -0.1631150245666504
Batch 39/64 loss: -0.12466716766357422
Batch 40/64 loss: -0.13335108757019043
Batch 41/64 loss: -0.14599573612213135
Batch 42/64 loss: -0.14869844913482666
Batch 43/64 loss: -0.0963020920753479
Batch 44/64 loss: -0.169594407081604
Batch 45/64 loss: -0.11440473794937134
Batch 46/64 loss: -0.10755658149719238
Batch 47/64 loss: -0.13533145189285278
Batch 48/64 loss: -0.1287858486175537
Batch 49/64 loss: -0.14366888999938965
Batch 50/64 loss: -0.14664942026138306
Batch 51/64 loss: -0.12683826684951782
Batch 52/64 loss: -0.14348363876342773
Batch 53/64 loss: -0.1457228660583496
Batch 54/64 loss: -0.16425013542175293
Batch 55/64 loss: -0.12280815839767456
Batch 56/64 loss: -0.17608368396759033
Batch 57/64 loss: -0.10082006454467773
Batch 58/64 loss: -0.15406560897827148
Batch 59/64 loss: -0.08057618141174316
Batch 60/64 loss: -0.11682730913162231
Batch 61/64 loss: -0.10135817527770996
Batch 62/64 loss: -0.14548325538635254
Batch 63/64 loss: -0.12913107872009277
Batch 64/64 loss: -0.09150314331054688
Epoch 364  Train loss: -0.14000767726524205  Val loss: 0.08052794466313627
Epoch 365
-------------------------------
Batch 1/64 loss: -0.16302525997161865
Batch 2/64 loss: -0.14882677793502808
Batch 3/64 loss: -0.1261652708053589
Batch 4/64 loss: -0.10925912857055664
Batch 5/64 loss: -0.10478848218917847
Batch 6/64 loss: -0.14693468809127808
Batch 7/64 loss: -0.12566477060317993
Batch 8/64 loss: -0.12961316108703613
Batch 9/64 loss: -0.10155165195465088
Batch 10/64 loss: -0.16929346323013306
Batch 11/64 loss: -0.13957178592681885
Batch 12/64 loss: -0.09589189291000366
Batch 13/64 loss: -0.16892749071121216
Batch 14/64 loss: -0.12026429176330566
Batch 15/64 loss: -0.14097213745117188
Batch 16/64 loss: -0.1332644820213318
Batch 17/64 loss: -0.17705971002578735
Batch 18/64 loss: -0.15652358531951904
Batch 19/64 loss: -0.14311039447784424
Batch 20/64 loss: -0.13139814138412476
Batch 21/64 loss: -0.1328263282775879
Batch 22/64 loss: -0.13880521059036255
Batch 23/64 loss: -0.15223771333694458
Batch 24/64 loss: -0.16023492813110352
Batch 25/64 loss: -0.1471361517906189
Batch 26/64 loss: -0.15850847959518433
Batch 27/64 loss: -0.11735635995864868
Batch 28/64 loss: -0.13244032859802246
Batch 29/64 loss: -0.1604996919631958
Batch 30/64 loss: -0.13729053735733032
Batch 31/64 loss: -0.13436651229858398
Batch 32/64 loss: -0.09847044944763184
Batch 33/64 loss: -0.17843693494796753
Batch 34/64 loss: -0.15294325351715088
Batch 35/64 loss: -0.14339065551757812
Batch 36/64 loss: -0.12608706951141357
Batch 37/64 loss: -0.1063571572303772
Batch 38/64 loss: -0.13111364841461182
Batch 39/64 loss: -0.1686391830444336
Batch 40/64 loss: -0.1487257480621338
Batch 41/64 loss: -0.18863439559936523
Batch 42/64 loss: -0.14105820655822754
Batch 43/64 loss: -0.12760460376739502
Batch 44/64 loss: -0.1434260606765747
Batch 45/64 loss: -0.1554419994354248
Batch 46/64 loss: -0.1434425711631775
Batch 47/64 loss: -0.07812505960464478
Batch 48/64 loss: -0.13962405920028687
Batch 49/64 loss: -0.12895965576171875
Batch 50/64 loss: -0.16626214981079102
Batch 51/64 loss: -0.15888649225234985
Batch 52/64 loss: -0.13463670015335083
Batch 53/64 loss: -0.1342182755470276
Batch 54/64 loss: -0.1740134358406067
Batch 55/64 loss: -0.14075517654418945
Batch 56/64 loss: -0.13451462984085083
Batch 57/64 loss: -0.12415146827697754
Batch 58/64 loss: -0.13766539096832275
Batch 59/64 loss: -0.1304423213005066
Batch 60/64 loss: -0.16904252767562866
Batch 61/64 loss: -0.15079790353775024
Batch 62/64 loss: -0.15949589014053345
Batch 63/64 loss: -0.14537262916564941
Batch 64/64 loss: -0.12026035785675049
Epoch 365  Train loss: -0.14046650634092442  Val loss: 0.06158790432710418
Epoch 366
-------------------------------
Batch 1/64 loss: -0.17017138004302979
Batch 2/64 loss: -0.1420193910598755
Batch 3/64 loss: -0.18120121955871582
Batch 4/64 loss: -0.09917265176773071
Batch 5/64 loss: -0.15402406454086304
Batch 6/64 loss: -0.12630945444107056
Batch 7/64 loss: -0.11939895153045654
Batch 8/64 loss: -0.1346326470375061
Batch 9/64 loss: -0.14022618532180786
Batch 10/64 loss: -0.13400989770889282
Batch 11/64 loss: -0.1201941967010498
Batch 12/64 loss: -0.16489624977111816
Batch 13/64 loss: -0.1292399764060974
Batch 14/64 loss: -0.11301147937774658
Batch 15/64 loss: -0.14609938859939575
Batch 16/64 loss: -0.15291380882263184
Batch 17/64 loss: -0.1380823850631714
Batch 18/64 loss: -0.15812617540359497
Batch 19/64 loss: -0.14538931846618652
Batch 20/64 loss: -0.15515810251235962
Batch 21/64 loss: -0.15142583847045898
Batch 22/64 loss: -0.1347578763961792
Batch 23/64 loss: -0.13077008724212646
Batch 24/64 loss: -0.13664859533309937
Batch 25/64 loss: -0.1471848487854004
Batch 26/64 loss: -0.17955416440963745
Batch 27/64 loss: -0.12298351526260376
Batch 28/64 loss: -0.15490204095840454
Batch 29/64 loss: -0.1477898359298706
Batch 30/64 loss: -0.12371814250946045
Batch 31/64 loss: -0.16935384273529053
Batch 32/64 loss: -0.15959572792053223
Batch 33/64 loss: -0.10898488759994507
Batch 34/64 loss: -0.1531381607055664
Batch 35/64 loss: -0.1634739637374878
Batch 36/64 loss: -0.14357489347457886
Batch 37/64 loss: -0.15997076034545898
Batch 38/64 loss: -0.11959576606750488
Batch 39/64 loss: -0.14213621616363525
Batch 40/64 loss: -0.1422356367111206
Batch 41/64 loss: -0.12833338975906372
Batch 42/64 loss: -0.17357295751571655
Batch 43/64 loss: -0.17583435773849487
Batch 44/64 loss: -0.12304043769836426
Batch 45/64 loss: -0.13685071468353271
Batch 46/64 loss: -0.14798879623413086
Batch 47/64 loss: -0.1340211033821106
Batch 48/64 loss: -0.15907377004623413
Batch 49/64 loss: -0.1400299072265625
Batch 50/64 loss: -0.13877320289611816
Batch 51/64 loss: -0.16987311840057373
Batch 52/64 loss: -0.14537376165390015
Batch 53/64 loss: -0.14292097091674805
Batch 54/64 loss: -0.14164835214614868
Batch 55/64 loss: -0.1447688341140747
Batch 56/64 loss: -0.11043143272399902
Batch 57/64 loss: -0.1353163719177246
Batch 58/64 loss: -0.10201394557952881
Batch 59/64 loss: -0.14839982986450195
Batch 60/64 loss: -0.11949455738067627
Batch 61/64 loss: -0.1285640001296997
Batch 62/64 loss: -0.09387838840484619
Batch 63/64 loss: -0.13569051027297974
Batch 64/64 loss: -0.08410918712615967
Epoch 366  Train loss: -0.14047131491642373  Val loss: 0.0643854090028612
Epoch 367
-------------------------------
Batch 1/64 loss: -0.17770040035247803
Batch 2/64 loss: -0.1436328887939453
Batch 3/64 loss: -0.14573991298675537
Batch 4/64 loss: -0.1486825942993164
Batch 5/64 loss: -0.12614595890045166
Batch 6/64 loss: -0.16696631908416748
Batch 7/64 loss: -0.1552600860595703
Batch 8/64 loss: -0.11432349681854248
Batch 9/64 loss: -0.14604008197784424
Batch 10/64 loss: -0.1385214924812317
Batch 11/64 loss: -0.19023656845092773
Batch 12/64 loss: -0.13397061824798584
Batch 13/64 loss: -0.1697710156440735
Batch 14/64 loss: -0.15548866987228394
Batch 15/64 loss: -0.14567530155181885
Batch 16/64 loss: -0.12213242053985596
Batch 17/64 loss: -0.16225779056549072
Batch 18/64 loss: -0.1056969165802002
Batch 19/64 loss: -0.1457802653312683
Batch 20/64 loss: -0.1548159122467041
Batch 21/64 loss: -0.1186714768409729
Batch 22/64 loss: -0.12123167514801025
Batch 23/64 loss: -0.137750506401062
Batch 24/64 loss: -0.16074711084365845
Batch 25/64 loss: -0.13000857830047607
Batch 26/64 loss: -0.14798367023468018
Batch 27/64 loss: -0.11410444974899292
Batch 28/64 loss: -0.16342759132385254
Batch 29/64 loss: -0.1225813627243042
Batch 30/64 loss: -0.15265721082687378
Batch 31/64 loss: -0.16515696048736572
Batch 32/64 loss: -0.13481277227401733
Batch 33/64 loss: -0.14876466989517212
Batch 34/64 loss: -0.1581687331199646
Batch 35/64 loss: -0.1482049822807312
Batch 36/64 loss: -0.1146993637084961
Batch 37/64 loss: -0.17095130681991577
Batch 38/64 loss: -0.11116087436676025
Batch 39/64 loss: -0.1528032422065735
Batch 40/64 loss: -0.18200284242630005
Batch 41/64 loss: -0.14804822206497192
Batch 42/64 loss: -0.14607447385787964
Batch 43/64 loss: -0.15946638584136963
Batch 44/64 loss: -0.13351428508758545
Batch 45/64 loss: -0.17255133390426636
Batch 46/64 loss: -0.10687768459320068
Batch 47/64 loss: -0.15774011611938477
Batch 48/64 loss: -0.16274213790893555
Batch 49/64 loss: -0.14191746711730957
Batch 50/64 loss: -0.10549712181091309
Batch 51/64 loss: -0.14592230319976807
Batch 52/64 loss: -0.1208539605140686
Batch 53/64 loss: -0.16153281927108765
Batch 54/64 loss: -0.14478814601898193
Batch 55/64 loss: -0.11281847953796387
Batch 56/64 loss: -0.15943574905395508
Batch 57/64 loss: -0.13970381021499634
Batch 58/64 loss: -0.1328580379486084
Batch 59/64 loss: -0.13965433835983276
Batch 60/64 loss: -0.13721871376037598
Batch 61/64 loss: -0.15139025449752808
Batch 62/64 loss: -0.10748797655105591
Batch 63/64 loss: -0.08980679512023926
Batch 64/64 loss: -0.15390580892562866
Epoch 367  Train loss: -0.1427146362323387  Val loss: 0.06116846992387805
Epoch 368
-------------------------------
Batch 1/64 loss: -0.1402987837791443
Batch 2/64 loss: -0.15862661600112915
Batch 3/64 loss: -0.1486244797706604
Batch 4/64 loss: -0.12799525260925293
Batch 5/64 loss: -0.14369207620620728
Batch 6/64 loss: -0.17112398147583008
Batch 7/64 loss: -0.1510075330734253
Batch 8/64 loss: -0.1467057466506958
Batch 9/64 loss: -0.1451612114906311
Batch 10/64 loss: -0.18584376573562622
Batch 11/64 loss: -0.13563579320907593
Batch 12/64 loss: -0.1507854461669922
Batch 13/64 loss: -0.11606699228286743
Batch 14/64 loss: -0.13911104202270508
Batch 15/64 loss: -0.10661596059799194
Batch 16/64 loss: -0.1724715232849121
Batch 17/64 loss: -0.15467727184295654
Batch 18/64 loss: -0.0919000506401062
Batch 19/64 loss: -0.15872585773468018
Batch 20/64 loss: -0.12780678272247314
Batch 21/64 loss: -0.16548389196395874
Batch 22/64 loss: -0.13356810808181763
Batch 23/64 loss: -0.13524490594863892
Batch 24/64 loss: -0.115306556224823
Batch 25/64 loss: -0.14002275466918945
Batch 26/64 loss: -0.15922045707702637
Batch 27/64 loss: -0.13572967052459717
Batch 28/64 loss: -0.15778231620788574
Batch 29/64 loss: -0.1406450867652893
Batch 30/64 loss: -0.16532689332962036
Batch 31/64 loss: -0.14946424961090088
Batch 32/64 loss: -0.13429272174835205
Batch 33/64 loss: -0.14670085906982422
Batch 34/64 loss: -0.144750714302063
Batch 35/64 loss: -0.16341257095336914
Batch 36/64 loss: -0.12102758884429932
Batch 37/64 loss: -0.14332127571105957
Batch 38/64 loss: -0.18384432792663574
Batch 39/64 loss: -0.13036233186721802
Batch 40/64 loss: -0.15974527597427368
Batch 41/64 loss: -0.12692177295684814
Batch 42/64 loss: -0.09145784378051758
Batch 43/64 loss: -0.11252635717391968
Batch 44/64 loss: -0.13266468048095703
Batch 45/64 loss: -0.1506638526916504
Batch 46/64 loss: -0.08144652843475342
Batch 47/64 loss: -0.10924422740936279
Batch 48/64 loss: -0.16109752655029297
Batch 49/64 loss: -0.153450608253479
Batch 50/64 loss: -0.12926751375198364
Batch 51/64 loss: -0.13255733251571655
Batch 52/64 loss: -0.138749361038208
Batch 53/64 loss: -0.09777575731277466
Batch 54/64 loss: -0.13949555158615112
Batch 55/64 loss: -0.1712084412574768
Batch 56/64 loss: -0.17313027381896973
Batch 57/64 loss: -0.13337254524230957
Batch 58/64 loss: -0.09461104869842529
Batch 59/64 loss: -0.1498369574546814
Batch 60/64 loss: -0.1599574089050293
Batch 61/64 loss: -0.13815838098526
Batch 62/64 loss: -0.10872739553451538
Batch 63/64 loss: -0.13644003868103027
Batch 64/64 loss: -0.08171677589416504
Epoch 368  Train loss: -0.13932827781228457  Val loss: 0.06248260568507349
Epoch 369
-------------------------------
Batch 1/64 loss: -0.12996011972427368
Batch 2/64 loss: -0.17826741933822632
Batch 3/64 loss: -0.15783727169036865
Batch 4/64 loss: -0.1633838415145874
Batch 5/64 loss: -0.1332165002822876
Batch 6/64 loss: -0.15922290086746216
Batch 7/64 loss: -0.1231468915939331
Batch 8/64 loss: -0.15070849657058716
Batch 9/64 loss: -0.13781046867370605
Batch 10/64 loss: -0.14829373359680176
Batch 11/64 loss: -0.1345692276954651
Batch 12/64 loss: -0.14475691318511963
Batch 13/64 loss: -0.1444162130355835
Batch 14/64 loss: -0.12645208835601807
Batch 15/64 loss: -0.13642674684524536
Batch 16/64 loss: -0.14152050018310547
Batch 17/64 loss: -0.15032732486724854
Batch 18/64 loss: -0.15113306045532227
Batch 19/64 loss: -0.13262534141540527
Batch 20/64 loss: -0.15834063291549683
Batch 21/64 loss: -0.1353391408920288
Batch 22/64 loss: -0.13056254386901855
Batch 23/64 loss: -0.1463753581047058
Batch 24/64 loss: -0.17040199041366577
Batch 25/64 loss: -0.1584072709083557
Batch 26/64 loss: -0.17075932025909424
Batch 27/64 loss: -0.14961642026901245
Batch 28/64 loss: -0.1075751781463623
Batch 29/64 loss: -0.1273813247680664
Batch 30/64 loss: -0.13620072603225708
Batch 31/64 loss: -0.14845186471939087
Batch 32/64 loss: -0.15379559993743896
Batch 33/64 loss: -0.14860761165618896
Batch 34/64 loss: -0.14600443840026855
Batch 35/64 loss: -0.13499969244003296
Batch 36/64 loss: -0.11773413419723511
Batch 37/64 loss: -0.14986270666122437
Batch 38/64 loss: -0.15151822566986084
Batch 39/64 loss: -0.1688903570175171
Batch 40/64 loss: -0.17711102962493896
Batch 41/64 loss: -0.11909681558609009
Batch 42/64 loss: -0.1273368000984192
Batch 43/64 loss: -0.10338646173477173
Batch 44/64 loss: -0.1878584623336792
Batch 45/64 loss: -0.17561423778533936
Batch 46/64 loss: -0.18386751413345337
Batch 47/64 loss: -0.1235501766204834
Batch 48/64 loss: -0.1186671257019043
Batch 49/64 loss: -0.13897645473480225
Batch 50/64 loss: -0.14963334798812866
Batch 51/64 loss: -0.1885286569595337
Batch 52/64 loss: -0.12058788537979126
Batch 53/64 loss: -0.128340482711792
Batch 54/64 loss: -0.15701895952224731
Batch 55/64 loss: -0.07746881246566772
Batch 56/64 loss: -0.14028877019882202
Batch 57/64 loss: -0.10490751266479492
Batch 58/64 loss: -0.14004719257354736
Batch 59/64 loss: -0.14542704820632935
Batch 60/64 loss: -0.11598539352416992
Batch 61/64 loss: -0.1397228240966797
Batch 62/64 loss: -0.16131848096847534
Batch 63/64 loss: -0.10388702154159546
Batch 64/64 loss: -0.09093993902206421
Epoch 369  Train loss: -0.1419879532327839  Val loss: 0.06079585117982425
Epoch 370
-------------------------------
Batch 1/64 loss: -0.15396010875701904
Batch 2/64 loss: -0.17531359195709229
Batch 3/64 loss: -0.16093581914901733
Batch 4/64 loss: -0.15889465808868408
Batch 5/64 loss: -0.12423473596572876
Batch 6/64 loss: -0.14510726928710938
Batch 7/64 loss: -0.13733607530593872
Batch 8/64 loss: -0.1219940185546875
Batch 9/64 loss: -0.1371622085571289
Batch 10/64 loss: -0.18619835376739502
Batch 11/64 loss: -0.13878214359283447
Batch 12/64 loss: -0.1056707501411438
Batch 13/64 loss: -0.14191317558288574
Batch 14/64 loss: -0.1782030463218689
Batch 15/64 loss: -0.18053120374679565
Batch 16/64 loss: -0.14439499378204346
Batch 17/64 loss: -0.13850843906402588
Batch 18/64 loss: -0.1587182879447937
Batch 19/64 loss: -0.1442597508430481
Batch 20/64 loss: -0.1377834677696228
Batch 21/64 loss: -0.08263379335403442
Batch 22/64 loss: -0.17275142669677734
Batch 23/64 loss: -0.14473956823349
Batch 24/64 loss: -0.17601019144058228
Batch 25/64 loss: -0.15933996438980103
Batch 26/64 loss: -0.1775369644165039
Batch 27/64 loss: -0.1506851315498352
Batch 28/64 loss: -0.16504621505737305
Batch 29/64 loss: -0.1420760154724121
Batch 30/64 loss: -0.1568129062652588
Batch 31/64 loss: -0.14313477277755737
Batch 32/64 loss: -0.16243338584899902
Batch 33/64 loss: -0.12269049882888794
Batch 34/64 loss: -0.10222667455673218
Batch 35/64 loss: -0.058438658714294434
Batch 36/64 loss: -0.17430776357650757
Batch 37/64 loss: -0.14723217487335205
Batch 38/64 loss: -0.12167882919311523
Batch 39/64 loss: -0.14739996194839478
Batch 40/64 loss: -0.17162013053894043
Batch 41/64 loss: -0.1509953737258911
Batch 42/64 loss: -0.14482402801513672
Batch 43/64 loss: -0.12006795406341553
Batch 44/64 loss: -0.1309642791748047
Batch 45/64 loss: -0.16280168294906616
Batch 46/64 loss: -0.1556107997894287
Batch 47/64 loss: -0.11227422952651978
Batch 48/64 loss: -0.12009066343307495
Batch 49/64 loss: -0.13239574432373047
Batch 50/64 loss: -0.16003400087356567
Batch 51/64 loss: -0.13801932334899902
Batch 52/64 loss: -0.1495688557624817
Batch 53/64 loss: -0.12673419713974
Batch 54/64 loss: -0.1448756456375122
Batch 55/64 loss: -0.15692877769470215
Batch 56/64 loss: -0.11834073066711426
Batch 57/64 loss: -0.15909695625305176
Batch 58/64 loss: -0.1029396653175354
Batch 59/64 loss: -0.1334223747253418
Batch 60/64 loss: -0.1292545199394226
Batch 61/64 loss: -0.14937782287597656
Batch 62/64 loss: -0.16958725452423096
Batch 63/64 loss: -0.16264426708221436
Batch 64/64 loss: -0.1322532296180725
Epoch 370  Train loss: -0.1439801756073447  Val loss: 0.06279954058198176
Epoch 371
-------------------------------
Batch 1/64 loss: -0.12454545497894287
Batch 2/64 loss: -0.13710403442382812
Batch 3/64 loss: -0.1318609118461609
Batch 4/64 loss: -0.1260974407196045
Batch 5/64 loss: -0.13707399368286133
Batch 6/64 loss: -0.13583874702453613
Batch 7/64 loss: -0.15324777364730835
Batch 8/64 loss: -0.11665421724319458
Batch 9/64 loss: -0.11747407913208008
Batch 10/64 loss: -0.1466946005821228
Batch 11/64 loss: -0.15187448263168335
Batch 12/64 loss: -0.17009037733078003
Batch 13/64 loss: -0.14817345142364502
Batch 14/64 loss: -0.12082207202911377
Batch 15/64 loss: -0.16157907247543335
Batch 16/64 loss: -0.16253042221069336
Batch 17/64 loss: -0.15055155754089355
Batch 18/64 loss: -0.1422586441040039
Batch 19/64 loss: -0.10116326808929443
Batch 20/64 loss: -0.14598631858825684
Batch 21/64 loss: -0.128013014793396
Batch 22/64 loss: -0.13935935497283936
Batch 23/64 loss: -0.13279807567596436
Batch 24/64 loss: -0.13941210508346558
Batch 25/64 loss: -0.16159015893936157
Batch 26/64 loss: -0.14828157424926758
Batch 27/64 loss: -0.15887421369552612
Batch 28/64 loss: -0.16918349266052246
Batch 29/64 loss: -0.17058861255645752
Batch 30/64 loss: -0.14234691858291626
Batch 31/64 loss: -0.11051046848297119
Batch 32/64 loss: -0.14365345239639282
Batch 33/64 loss: -0.16144013404846191
Batch 34/64 loss: -0.15223169326782227
Batch 35/64 loss: -0.09978926181793213
Batch 36/64 loss: -0.09502005577087402
Batch 37/64 loss: -0.15459424257278442
Batch 38/64 loss: -0.16081500053405762
Batch 39/64 loss: -0.16677504777908325
Batch 40/64 loss: -0.16782879829406738
Batch 41/64 loss: -0.15800070762634277
Batch 42/64 loss: -0.14226478338241577
Batch 43/64 loss: -0.1550731062889099
Batch 44/64 loss: -0.11635613441467285
Batch 45/64 loss: -0.13640683889389038
Batch 46/64 loss: -0.1462344527244568
Batch 47/64 loss: -0.1684364676475525
Batch 48/64 loss: -0.16603142023086548
Batch 49/64 loss: -0.1729416847229004
Batch 50/64 loss: -0.19275641441345215
Batch 51/64 loss: -0.11955171823501587
Batch 52/64 loss: -0.14074867963790894
Batch 53/64 loss: -0.13436460494995117
Batch 54/64 loss: -0.13643652200698853
Batch 55/64 loss: -0.126434326171875
Batch 56/64 loss: -0.13932907581329346
Batch 57/64 loss: -0.12878799438476562
Batch 58/64 loss: -0.10799413919448853
Batch 59/64 loss: -0.13311129808425903
Batch 60/64 loss: -0.1820143461227417
Batch 61/64 loss: -0.1582288146018982
Batch 62/64 loss: -0.17077910900115967
Batch 63/64 loss: -0.12672674655914307
Batch 64/64 loss: -0.10501772165298462
Epoch 371  Train loss: -0.14309802779964373  Val loss: 0.059992320758780256
Epoch 372
-------------------------------
Batch 1/64 loss: -0.13048195838928223
Batch 2/64 loss: -0.1984615921974182
Batch 3/64 loss: -0.15896672010421753
Batch 4/64 loss: -0.15262120962142944
Batch 5/64 loss: -0.13773351907730103
Batch 6/64 loss: -0.1267995834350586
Batch 7/64 loss: -0.12122529745101929
Batch 8/64 loss: -0.14483368396759033
Batch 9/64 loss: -0.10793113708496094
Batch 10/64 loss: -0.18408972024917603
Batch 11/64 loss: -0.09864270687103271
Batch 12/64 loss: -0.1415776014328003
Batch 13/64 loss: -0.16195863485336304
Batch 14/64 loss: -0.15793710947036743
Batch 15/64 loss: -0.17685115337371826
Batch 16/64 loss: -0.14642935991287231
Batch 17/64 loss: -0.12768620252609253
Batch 18/64 loss: -0.16975271701812744
Batch 19/64 loss: -0.1620197892189026
Batch 20/64 loss: -0.1638854742050171
Batch 21/64 loss: -0.15913724899291992
Batch 22/64 loss: -0.17865580320358276
Batch 23/64 loss: -0.15227818489074707
Batch 24/64 loss: -0.16938209533691406
Batch 25/64 loss: -0.14487552642822266
Batch 26/64 loss: -0.12310916185379028
Batch 27/64 loss: -0.14604979753494263
Batch 28/64 loss: -0.1789398193359375
Batch 29/64 loss: -0.14783120155334473
Batch 30/64 loss: -0.16767668724060059
Batch 31/64 loss: -0.1619715690612793
Batch 32/64 loss: -0.15528905391693115
Batch 33/64 loss: -0.15540218353271484
Batch 34/64 loss: -0.15544670820236206
Batch 35/64 loss: -0.09219503402709961
Batch 36/64 loss: -0.12073928117752075
Batch 37/64 loss: -0.1683627963066101
Batch 38/64 loss: -0.13749337196350098
Batch 39/64 loss: -0.1701979637145996
Batch 40/64 loss: -0.1620660424232483
Batch 41/64 loss: -0.11634773015975952
Batch 42/64 loss: -0.0934566855430603
Batch 43/64 loss: -0.11514711380004883
Batch 44/64 loss: -0.14429235458374023
Batch 45/64 loss: -0.13875210285186768
Batch 46/64 loss: -0.15652215480804443
Batch 47/64 loss: -0.10265052318572998
Batch 48/64 loss: -0.1489836573600769
Batch 49/64 loss: -0.10064595937728882
Batch 50/64 loss: -0.13477975130081177
Batch 51/64 loss: -0.11092466115951538
Batch 52/64 loss: -0.1358487606048584
Batch 53/64 loss: -0.1498948335647583
Batch 54/64 loss: -0.14265763759613037
Batch 55/64 loss: -0.15795999765396118
Batch 56/64 loss: -0.14647263288497925
Batch 57/64 loss: -0.1585741639137268
Batch 58/64 loss: -0.13735932111740112
Batch 59/64 loss: -0.15010333061218262
Batch 60/64 loss: -0.14603722095489502
Batch 61/64 loss: -0.15502393245697021
Batch 62/64 loss: -0.10589814186096191
Batch 63/64 loss: -0.16193056106567383
Batch 64/64 loss: -0.08913296461105347
Epoch 372  Train loss: -0.1442211396553937  Val loss: 0.061340037080430496
Epoch 373
-------------------------------
Batch 1/64 loss: -0.16484159231185913
Batch 2/64 loss: -0.09644538164138794
Batch 3/64 loss: -0.12418431043624878
Batch 4/64 loss: -0.15762799978256226
Batch 5/64 loss: -0.1609959602355957
Batch 6/64 loss: -0.15634536743164062
Batch 7/64 loss: -0.14405733346939087
Batch 8/64 loss: -0.16156107187271118
Batch 9/64 loss: -0.19367527961730957
Batch 10/64 loss: -0.12703698873519897
Batch 11/64 loss: -0.16854417324066162
Batch 12/64 loss: -0.1673048734664917
Batch 13/64 loss: -0.09018254280090332
Batch 14/64 loss: -0.15208178758621216
Batch 15/64 loss: -0.121376633644104
Batch 16/64 loss: -0.16488027572631836
Batch 17/64 loss: -0.11479508876800537
Batch 18/64 loss: -0.1463375687599182
Batch 19/64 loss: -0.1206408143043518
Batch 20/64 loss: -0.1448560357093811
Batch 21/64 loss: -0.15691858530044556
Batch 22/64 loss: -0.15412330627441406
Batch 23/64 loss: -0.13674241304397583
Batch 24/64 loss: -0.15403258800506592
Batch 25/64 loss: -0.12513870000839233
Batch 26/64 loss: -0.15592777729034424
Batch 27/64 loss: -0.12196588516235352
Batch 28/64 loss: -0.14282971620559692
Batch 29/64 loss: -0.14617681503295898
Batch 30/64 loss: -0.16791224479675293
Batch 31/64 loss: -0.13999944925308228
Batch 32/64 loss: -0.14792311191558838
Batch 33/64 loss: -0.13885003328323364
Batch 34/64 loss: -0.14019745588302612
Batch 35/64 loss: -0.16250967979431152
Batch 36/64 loss: -0.14692747592926025
Batch 37/64 loss: -0.17181402444839478
Batch 38/64 loss: -0.17373335361480713
Batch 39/64 loss: -0.16872435808181763
Batch 40/64 loss: -0.14266037940979004
Batch 41/64 loss: -0.12169778347015381
Batch 42/64 loss: -0.1620234251022339
Batch 43/64 loss: -0.1598801612854004
Batch 44/64 loss: -0.12537097930908203
Batch 45/64 loss: -0.15226495265960693
Batch 46/64 loss: -0.147022545337677
Batch 47/64 loss: -0.15798211097717285
Batch 48/64 loss: -0.14986169338226318
Batch 49/64 loss: -0.13947898149490356
Batch 50/64 loss: -0.17148053646087646
Batch 51/64 loss: -0.1390552520751953
Batch 52/64 loss: -0.13576644659042358
Batch 53/64 loss: -0.17114436626434326
Batch 54/64 loss: -0.13743162155151367
Batch 55/64 loss: -0.1779891848564148
Batch 56/64 loss: -0.1369430422782898
Batch 57/64 loss: -0.16999685764312744
Batch 58/64 loss: -0.14942532777786255
Batch 59/64 loss: -0.12697672843933105
Batch 60/64 loss: -0.12697875499725342
Batch 61/64 loss: -0.11685764789581299
Batch 62/64 loss: -0.13359802961349487
Batch 63/64 loss: -0.1680126190185547
Batch 64/64 loss: -0.10161155462265015
Epoch 373  Train loss: -0.14629531210544064  Val loss: 0.07079503032350049
Epoch 374
-------------------------------
Batch 1/64 loss: -0.13847237825393677
Batch 2/64 loss: -0.1268519163131714
Batch 3/64 loss: -0.15255141258239746
Batch 4/64 loss: -0.17932641506195068
Batch 5/64 loss: -0.0825759768486023
Batch 6/64 loss: -0.1408482789993286
Batch 7/64 loss: -0.155975341796875
Batch 8/64 loss: -0.14138400554656982
Batch 9/64 loss: -0.14164018630981445
Batch 10/64 loss: -0.09166610240936279
Batch 11/64 loss: -0.1320675015449524
Batch 12/64 loss: -0.15438878536224365
Batch 13/64 loss: -0.18212240934371948
Batch 14/64 loss: -0.1557130217552185
Batch 15/64 loss: -0.12555420398712158
Batch 16/64 loss: -0.15187793970108032
Batch 17/64 loss: -0.11845135688781738
Batch 18/64 loss: -0.15775883197784424
Batch 19/64 loss: -0.18188577890396118
Batch 20/64 loss: -0.1399875283241272
Batch 21/64 loss: -0.09365111589431763
Batch 22/64 loss: -0.1447499394416809
Batch 23/64 loss: -0.15101230144500732
Batch 24/64 loss: -0.13749146461486816
Batch 25/64 loss: -0.15654700994491577
Batch 26/64 loss: -0.13265615701675415
Batch 27/64 loss: -0.15569376945495605
Batch 28/64 loss: -0.13908857107162476
Batch 29/64 loss: -0.12659108638763428
Batch 30/64 loss: -0.11790663003921509
Batch 31/64 loss: -0.14262759685516357
Batch 32/64 loss: -0.17204660177230835
Batch 33/64 loss: -0.15156865119934082
Batch 34/64 loss: -0.17892146110534668
Batch 35/64 loss: -0.13453954458236694
Batch 36/64 loss: -0.17866027355194092
Batch 37/64 loss: -0.16660857200622559
Batch 38/64 loss: -0.15629315376281738
Batch 39/64 loss: -0.17012399435043335
Batch 40/64 loss: -0.13088607788085938
Batch 41/64 loss: -0.12399953603744507
Batch 42/64 loss: -0.14502280950546265
Batch 43/64 loss: -0.1913745403289795
Batch 44/64 loss: -0.11594510078430176
Batch 45/64 loss: -0.15031945705413818
Batch 46/64 loss: -0.1892482042312622
Batch 47/64 loss: -0.11530125141143799
Batch 48/64 loss: -0.13393855094909668
Batch 49/64 loss: -0.1325097680091858
Batch 50/64 loss: -0.12998980283737183
Batch 51/64 loss: -0.13175570964813232
Batch 52/64 loss: -0.1834275722503662
Batch 53/64 loss: -0.15114200115203857
Batch 54/64 loss: -0.15103739500045776
Batch 55/64 loss: -0.1735391616821289
Batch 56/64 loss: -0.13191604614257812
Batch 57/64 loss: -0.15838348865509033
Batch 58/64 loss: -0.12296271324157715
Batch 59/64 loss: -0.1480836272239685
Batch 60/64 loss: -0.12891364097595215
Batch 61/64 loss: -0.09743142127990723
Batch 62/64 loss: -0.12000423669815063
Batch 63/64 loss: -0.12904632091522217
Batch 64/64 loss: -0.10055816173553467
Epoch 374  Train loss: -0.1430505775937847  Val loss: 0.06315229111111041
Epoch 375
-------------------------------
Batch 1/64 loss: -0.13385820388793945
Batch 2/64 loss: -0.13349509239196777
Batch 3/64 loss: -0.1291978359222412
Batch 4/64 loss: -0.15554547309875488
Batch 5/64 loss: -0.15569043159484863
Batch 6/64 loss: -0.15387779474258423
Batch 7/64 loss: -0.11926281452178955
Batch 8/64 loss: -0.15588194131851196
Batch 9/64 loss: -0.15605944395065308
Batch 10/64 loss: -0.11385881900787354
Batch 11/64 loss: -0.13376903533935547
Batch 12/64 loss: -0.1068793535232544
Batch 13/64 loss: -0.14036822319030762
Batch 14/64 loss: -0.13253360986709595
Batch 15/64 loss: -0.17251825332641602
Batch 16/64 loss: -0.16052424907684326
Batch 17/64 loss: -0.17893922328948975
Batch 18/64 loss: -0.1449946165084839
Batch 19/64 loss: -0.15956753492355347
Batch 20/64 loss: -0.10793530941009521
Batch 21/64 loss: -0.1422804594039917
Batch 22/64 loss: -0.17467796802520752
Batch 23/64 loss: -0.15478384494781494
Batch 24/64 loss: -0.1471179723739624
Batch 25/64 loss: -0.16329419612884521
Batch 26/64 loss: -0.14839744567871094
Batch 27/64 loss: -0.11198312044143677
Batch 28/64 loss: -0.09204697608947754
Batch 29/64 loss: -0.13034558296203613
Batch 30/64 loss: -0.1598331332206726
Batch 31/64 loss: -0.15101778507232666
Batch 32/64 loss: -0.14231061935424805
Batch 33/64 loss: -0.13473403453826904
Batch 34/64 loss: -0.15702581405639648
Batch 35/64 loss: -0.1542975902557373
Batch 36/64 loss: -0.17729461193084717
Batch 37/64 loss: -0.15394634008407593
Batch 38/64 loss: -0.1387895941734314
Batch 39/64 loss: -0.12035882472991943
Batch 40/64 loss: -0.10803031921386719
Batch 41/64 loss: -0.12279200553894043
Batch 42/64 loss: -0.14375197887420654
Batch 43/64 loss: -0.1410086750984192
Batch 44/64 loss: -0.14303171634674072
Batch 45/64 loss: -0.15149569511413574
Batch 46/64 loss: -0.12973839044570923
Batch 47/64 loss: -0.17089873552322388
Batch 48/64 loss: -0.13078534603118896
Batch 49/64 loss: -0.18347734212875366
Batch 50/64 loss: -0.1545066237449646
Batch 51/64 loss: -0.15086185932159424
Batch 52/64 loss: -0.13533109426498413
Batch 53/64 loss: -0.1580049991607666
Batch 54/64 loss: -0.10605621337890625
Batch 55/64 loss: -0.16369885206222534
Batch 56/64 loss: -0.15268754959106445
Batch 57/64 loss: -0.17391395568847656
Batch 58/64 loss: -0.10544824600219727
Batch 59/64 loss: -0.17463469505310059
Batch 60/64 loss: -0.15975171327590942
Batch 61/64 loss: -0.11868488788604736
Batch 62/64 loss: -0.16479778289794922
Batch 63/64 loss: -0.12596094608306885
Batch 64/64 loss: -0.1397051215171814
Epoch 375  Train loss: -0.14389680997998106  Val loss: 0.06826372777473476
Epoch 376
-------------------------------
Batch 1/64 loss: -0.1295909881591797
Batch 2/64 loss: -0.16127026081085205
Batch 3/64 loss: -0.1288934350013733
Batch 4/64 loss: -0.17168986797332764
Batch 5/64 loss: -0.16418129205703735
Batch 6/64 loss: -0.13304179906845093
Batch 7/64 loss: -0.15679115056991577
Batch 8/64 loss: -0.14019709825515747
Batch 9/64 loss: -0.1675902009010315
Batch 10/64 loss: -0.15998947620391846
Batch 11/64 loss: -0.19207513332366943
Batch 12/64 loss: -0.13056457042694092
Batch 13/64 loss: -0.1690068244934082
Batch 14/64 loss: -0.14007949829101562
Batch 15/64 loss: -0.15769487619400024
Batch 16/64 loss: -0.1299659013748169
Batch 17/64 loss: -0.14574581384658813
Batch 18/64 loss: -0.1343899369239807
Batch 19/64 loss: -0.1207500696182251
Batch 20/64 loss: -0.1586606502532959
Batch 21/64 loss: -0.14817094802856445
Batch 22/64 loss: -0.11066234111785889
Batch 23/64 loss: -0.1306089162826538
Batch 24/64 loss: -0.16517329216003418
Batch 25/64 loss: -0.17349714040756226
Batch 26/64 loss: -0.1241006851196289
Batch 27/64 loss: -0.1195915937423706
Batch 28/64 loss: -0.16794973611831665
Batch 29/64 loss: -0.1613483428955078
Batch 30/64 loss: -0.15400725603103638
Batch 31/64 loss: -0.11273759603500366
Batch 32/64 loss: -0.12433099746704102
Batch 33/64 loss: -0.1433587670326233
Batch 34/64 loss: -0.1523306965827942
Batch 35/64 loss: -0.12865960597991943
Batch 36/64 loss: -0.1527855396270752
Batch 37/64 loss: -0.1414843201637268
Batch 38/64 loss: -0.09775811433792114
Batch 39/64 loss: -0.1588394045829773
Batch 40/64 loss: -0.16380220651626587
Batch 41/64 loss: -0.1472642421722412
Batch 42/64 loss: -0.12154889106750488
Batch 43/64 loss: -0.14822334051132202
Batch 44/64 loss: -0.17165476083755493
Batch 45/64 loss: -0.15062814950942993
Batch 46/64 loss: -0.1316651701927185
Batch 47/64 loss: -0.11054122447967529
Batch 48/64 loss: -0.14050400257110596
Batch 49/64 loss: -0.15574055910110474
Batch 50/64 loss: -0.11567741632461548
Batch 51/64 loss: -0.14619660377502441
Batch 52/64 loss: -0.1174691915512085
Batch 53/64 loss: -0.15677767992019653
Batch 54/64 loss: -0.13281184434890747
Batch 55/64 loss: -0.14054495096206665
Batch 56/64 loss: -0.1285918951034546
Batch 57/64 loss: -0.14862990379333496
Batch 58/64 loss: -0.16585850715637207
Batch 59/64 loss: -0.14084523916244507
Batch 60/64 loss: -0.15882378816604614
Batch 61/64 loss: -0.16901761293411255
Batch 62/64 loss: -0.12974333763122559
Batch 63/64 loss: -0.14931738376617432
Batch 64/64 loss: -0.09209239482879639
Epoch 376  Train loss: -0.14385115819818833  Val loss: 0.05960586021855934
Epoch 377
-------------------------------
Batch 1/64 loss: -0.13592267036437988
Batch 2/64 loss: -0.1660746932029724
Batch 3/64 loss: -0.14534616470336914
Batch 4/64 loss: -0.15153318643569946
Batch 5/64 loss: -0.1515701413154602
Batch 6/64 loss: -0.16473305225372314
Batch 7/64 loss: -0.13937455415725708
Batch 8/64 loss: -0.1563500165939331
Batch 9/64 loss: -0.18194079399108887
Batch 10/64 loss: -0.17396050691604614
Batch 11/64 loss: -0.137751042842865
Batch 12/64 loss: -0.1701764464378357
Batch 13/64 loss: -0.10851067304611206
Batch 14/64 loss: -0.13584357500076294
Batch 15/64 loss: -0.17567706108093262
Batch 16/64 loss: -0.11860692501068115
Batch 17/64 loss: -0.10330623388290405
Batch 18/64 loss: -0.1113138198852539
Batch 19/64 loss: -0.10376554727554321
Batch 20/64 loss: -0.14527195692062378
Batch 21/64 loss: -0.13972824811935425
Batch 22/64 loss: -0.20260784029960632
Batch 23/64 loss: -0.16106528043746948
Batch 24/64 loss: -0.13678932189941406
Batch 25/64 loss: -0.1259450912475586
Batch 26/64 loss: -0.10676908493041992
Batch 27/64 loss: -0.1563434600830078
Batch 28/64 loss: -0.18703097105026245
Batch 29/64 loss: -0.16454917192459106
Batch 30/64 loss: -0.17055261135101318
Batch 31/64 loss: -0.15882468223571777
Batch 32/64 loss: -0.14228075742721558
Batch 33/64 loss: -0.1316235065460205
Batch 34/64 loss: -0.18069523572921753
Batch 35/64 loss: -0.13770508766174316
Batch 36/64 loss: -0.1712033748626709
Batch 37/64 loss: -0.15599781274795532
Batch 38/64 loss: -0.13663309812545776
Batch 39/64 loss: -0.14510369300842285
Batch 40/64 loss: -0.16154402494430542
Batch 41/64 loss: -0.1427484154701233
Batch 42/64 loss: -0.15498244762420654
Batch 43/64 loss: -0.17480337619781494
Batch 44/64 loss: -0.09186118841171265
Batch 45/64 loss: -0.14550966024398804
Batch 46/64 loss: -0.16653972864151
Batch 47/64 loss: -0.1353777050971985
Batch 48/64 loss: -0.16834968328475952
Batch 49/64 loss: -0.12060385942459106
Batch 50/64 loss: -0.12836527824401855
Batch 51/64 loss: -0.13608288764953613
Batch 52/64 loss: -0.18105489015579224
Batch 53/64 loss: -0.1599004864692688
Batch 54/64 loss: -0.16046428680419922
Batch 55/64 loss: -0.12407320737838745
Batch 56/64 loss: -0.15690875053405762
Batch 57/64 loss: -0.17630672454833984
Batch 58/64 loss: -0.13355648517608643
Batch 59/64 loss: -0.14255744218826294
Batch 60/64 loss: -0.12922120094299316
Batch 61/64 loss: -0.10489696264266968
Batch 62/64 loss: -0.18218940496444702
Batch 63/64 loss: -0.10277420282363892
Batch 64/64 loss: -0.10273998975753784
Epoch 377  Train loss: -0.146607132285249  Val loss: 0.061491691779434886
Epoch 378
-------------------------------
Batch 1/64 loss: -0.15272396802902222
Batch 2/64 loss: -0.1429234743118286
Batch 3/64 loss: -0.1791478991508484
Batch 4/64 loss: -0.16413164138793945
Batch 5/64 loss: -0.16610318422317505
Batch 6/64 loss: -0.13219082355499268
Batch 7/64 loss: -0.15848767757415771
Batch 8/64 loss: -0.13841307163238525
Batch 9/64 loss: -0.13470453023910522
Batch 10/64 loss: -0.14520615339279175
Batch 11/64 loss: -0.16919291019439697
Batch 12/64 loss: -0.151436448097229
Batch 13/64 loss: -0.13761979341506958
Batch 14/64 loss: -0.15693765878677368
Batch 15/64 loss: -0.15375924110412598
Batch 16/64 loss: -0.15698188543319702
Batch 17/64 loss: -0.15835994482040405
Batch 18/64 loss: -0.1317906379699707
Batch 19/64 loss: -0.16998028755187988
Batch 20/64 loss: -0.15066009759902954
Batch 21/64 loss: -0.16438400745391846
Batch 22/64 loss: -0.11244368553161621
Batch 23/64 loss: -0.1474589705467224
Batch 24/64 loss: -0.13771790266036987
Batch 25/64 loss: -0.16947025060653687
Batch 26/64 loss: -0.13455504179000854
Batch 27/64 loss: -0.12361431121826172
Batch 28/64 loss: -0.15142762660980225
Batch 29/64 loss: -0.13127166032791138
Batch 30/64 loss: -0.1421322226524353
Batch 31/64 loss: -0.15007507801055908
Batch 32/64 loss: -0.12845313549041748
Batch 33/64 loss: -0.17641186714172363
Batch 34/64 loss: -0.13896912336349487
Batch 35/64 loss: -0.1487845778465271
Batch 36/64 loss: -0.13815897703170776
Batch 37/64 loss: -0.13962429761886597
Batch 38/64 loss: -0.14109033346176147
Batch 39/64 loss: -0.17701971530914307
Batch 40/64 loss: -0.1364138126373291
Batch 41/64 loss: -0.16695910692214966
Batch 42/64 loss: -0.08718550205230713
Batch 43/64 loss: -0.1573694944381714
Batch 44/64 loss: -0.16273534297943115
Batch 45/64 loss: -0.14080184698104858
Batch 46/64 loss: -0.15275073051452637
Batch 47/64 loss: -0.16572582721710205
Batch 48/64 loss: -0.13215208053588867
Batch 49/64 loss: -0.13222026824951172
Batch 50/64 loss: -0.13954639434814453
Batch 51/64 loss: -0.14206862449645996
Batch 52/64 loss: -0.17571085691452026
Batch 53/64 loss: -0.1733223795890808
Batch 54/64 loss: -0.14246845245361328
Batch 55/64 loss: -0.1023411750793457
Batch 56/64 loss: -0.07277786731719971
Batch 57/64 loss: -0.16759884357452393
Batch 58/64 loss: -0.09686493873596191
Batch 59/64 loss: -0.14294254779815674
Batch 60/64 loss: -0.1153104305267334
Batch 61/64 loss: -0.15566480159759521
Batch 62/64 loss: -0.12721890211105347
Batch 63/64 loss: -0.14800679683685303
Batch 64/64 loss: -0.13665771484375
Epoch 378  Train loss: -0.14497983408909218  Val loss: 0.06811352202163119
Epoch 379
-------------------------------
Batch 1/64 loss: -0.14180642366409302
Batch 2/64 loss: -0.15360695123672485
Batch 3/64 loss: -0.1634533405303955
Batch 4/64 loss: -0.17230379581451416
Batch 5/64 loss: -0.16485100984573364
Batch 6/64 loss: -0.17599153518676758
Batch 7/64 loss: -0.1329614520072937
Batch 8/64 loss: -0.16194212436676025
Batch 9/64 loss: -0.10491013526916504
Batch 10/64 loss: -0.13194698095321655
Batch 11/64 loss: -0.14150333404541016
Batch 12/64 loss: -0.14722222089767456
Batch 13/64 loss: -0.15633141994476318
Batch 14/64 loss: -0.10282987356185913
Batch 15/64 loss: -0.18491750955581665
Batch 16/64 loss: -0.11689364910125732
Batch 17/64 loss: -0.15090632438659668
Batch 18/64 loss: -0.15150493383407593
Batch 19/64 loss: -0.14676356315612793
Batch 20/64 loss: -0.15909981727600098
Batch 21/64 loss: -0.17745637893676758
Batch 22/64 loss: -0.17589879035949707
Batch 23/64 loss: -0.16195476055145264
Batch 24/64 loss: -0.11962145566940308
Batch 25/64 loss: -0.1666344404220581
Batch 26/64 loss: -0.12605172395706177
Batch 27/64 loss: -0.15178287029266357
Batch 28/64 loss: -0.14569377899169922
Batch 29/64 loss: -0.1664886474609375
Batch 30/64 loss: -0.15059947967529297
Batch 31/64 loss: -0.17690324783325195
Batch 32/64 loss: -0.130202054977417
Batch 33/64 loss: -0.13749730587005615
Batch 34/64 loss: -0.10698431730270386
Batch 35/64 loss: -0.12567222118377686
Batch 36/64 loss: -0.14279717206954956
Batch 37/64 loss: -0.16172200441360474
Batch 38/64 loss: -0.1293720006942749
Batch 39/64 loss: -0.11925917863845825
Batch 40/64 loss: -0.16385400295257568
Batch 41/64 loss: -0.14392340183258057
Batch 42/64 loss: -0.13018369674682617
Batch 43/64 loss: -0.13113701343536377
Batch 44/64 loss: -0.1354617476463318
Batch 45/64 loss: -0.1614188551902771
Batch 46/64 loss: -0.13812339305877686
Batch 47/64 loss: -0.15946614742279053
Batch 48/64 loss: -0.15959793329238892
Batch 49/64 loss: -0.10740649700164795
Batch 50/64 loss: -0.17102313041687012
Batch 51/64 loss: -0.09767138957977295
Batch 52/64 loss: -0.12844055891036987
Batch 53/64 loss: -0.1420987844467163
Batch 54/64 loss: -0.125932514667511
Batch 55/64 loss: -0.16089624166488647
Batch 56/64 loss: -0.10859572887420654
Batch 57/64 loss: -0.16467726230621338
Batch 58/64 loss: -0.1590709686279297
Batch 59/64 loss: -0.1300450563430786
Batch 60/64 loss: -0.14533734321594238
Batch 61/64 loss: -0.1136629581451416
Batch 62/64 loss: -0.15053540468215942
Batch 63/64 loss: -0.18364769220352173
Batch 64/64 loss: -0.16263455152511597
Epoch 379  Train loss: -0.14538857819987278  Val loss: 0.059485999374455194
Epoch 380
-------------------------------
Batch 1/64 loss: -0.13170921802520752
Batch 2/64 loss: -0.15955448150634766
Batch 3/64 loss: -0.14504706859588623
Batch 4/64 loss: -0.16478168964385986
Batch 5/64 loss: -0.1546633243560791
Batch 6/64 loss: -0.1619962453842163
Batch 7/64 loss: -0.1403590440750122
Batch 8/64 loss: -0.14437639713287354
Batch 9/64 loss: -0.10463029146194458
Batch 10/64 loss: -0.16681361198425293
Batch 11/64 loss: -0.1732388138771057
Batch 12/64 loss: -0.16275042295455933
Batch 13/64 loss: -0.18043959140777588
Batch 14/64 loss: -0.16714608669281006
Batch 15/64 loss: -0.15024429559707642
Batch 16/64 loss: -0.13954675197601318
Batch 17/64 loss: -0.14815795421600342
Batch 18/64 loss: -0.16708910465240479
Batch 19/64 loss: -0.15004843473434448
Batch 20/64 loss: -0.15452539920806885
Batch 21/64 loss: -0.10850334167480469
Batch 22/64 loss: -0.1481539011001587
Batch 23/64 loss: -0.16590183973312378
Batch 24/64 loss: -0.16396713256835938
Batch 25/64 loss: -0.10575336217880249
Batch 26/64 loss: -0.1661805510520935
Batch 27/64 loss: -0.14341378211975098
Batch 28/64 loss: -0.12875407934188843
Batch 29/64 loss: -0.14115500450134277
Batch 30/64 loss: -0.1488969922065735
Batch 31/64 loss: -0.11507296562194824
Batch 32/64 loss: -0.13603293895721436
Batch 33/64 loss: -0.09544605016708374
Batch 34/64 loss: -0.15567725896835327
Batch 35/64 loss: -0.13978761434555054
Batch 36/64 loss: -0.11216229200363159
Batch 37/64 loss: -0.15561515092849731
Batch 38/64 loss: -0.1207737922668457
Batch 39/64 loss: -0.1514359712600708
Batch 40/64 loss: -0.14850854873657227
Batch 41/64 loss: -0.19042891263961792
Batch 42/64 loss: -0.14521384239196777
Batch 43/64 loss: -0.1774560809135437
Batch 44/64 loss: -0.14635348320007324
Batch 45/64 loss: -0.11684346199035645
Batch 46/64 loss: -0.1577625274658203
Batch 47/64 loss: -0.13598692417144775
Batch 48/64 loss: -0.1736273169517517
Batch 49/64 loss: -0.11919552087783813
Batch 50/64 loss: -0.14180535078048706
Batch 51/64 loss: -0.15224575996398926
Batch 52/64 loss: -0.1432178020477295
Batch 53/64 loss: -0.15989470481872559
Batch 54/64 loss: -0.14618033170700073
Batch 55/64 loss: -0.1248931884765625
Batch 56/64 loss: -0.14350974559783936
Batch 57/64 loss: -0.1811918020248413
Batch 58/64 loss: -0.1313287615776062
Batch 59/64 loss: -0.16089099645614624
Batch 60/64 loss: -0.14604586362838745
Batch 61/64 loss: -0.1315222978591919
Batch 62/64 loss: -0.13713008165359497
Batch 63/64 loss: -0.14026832580566406
Batch 64/64 loss: -0.13617247343063354
Epoch 380  Train loss: -0.14624993310255163  Val loss: 0.06193199415796811
Epoch 381
-------------------------------
Batch 1/64 loss: -0.1205022931098938
Batch 2/64 loss: -0.17717784643173218
Batch 3/64 loss: -0.1906457543373108
Batch 4/64 loss: -0.15768098831176758
Batch 5/64 loss: -0.11916524171829224
Batch 6/64 loss: -0.15531814098358154
Batch 7/64 loss: -0.10814648866653442
Batch 8/64 loss: -0.1265760064125061
Batch 9/64 loss: -0.1550493836402893
Batch 10/64 loss: -0.1580694317817688
Batch 11/64 loss: -0.14286494255065918
Batch 12/64 loss: -0.17680764198303223
Batch 13/64 loss: -0.12707781791687012
Batch 14/64 loss: -0.19861814379692078
Batch 15/64 loss: -0.10752582550048828
Batch 16/64 loss: -0.13363265991210938
Batch 17/64 loss: -0.12782645225524902
Batch 18/64 loss: -0.16672861576080322
Batch 19/64 loss: -0.14998263120651245
Batch 20/64 loss: -0.13908284902572632
Batch 21/64 loss: -0.14017939567565918
Batch 22/64 loss: -0.19135212898254395
Batch 23/64 loss: -0.16476476192474365
Batch 24/64 loss: -0.14030098915100098
Batch 25/64 loss: -0.1615581512451172
Batch 26/64 loss: -0.14741748571395874
Batch 27/64 loss: -0.13712060451507568
Batch 28/64 loss: -0.17619502544403076
Batch 29/64 loss: -0.18557524681091309
Batch 30/64 loss: -0.1355959177017212
Batch 31/64 loss: -0.12197995185852051
Batch 32/64 loss: -0.09709835052490234
Batch 33/64 loss: -0.13151562213897705
Batch 34/64 loss: -0.1663876175880432
Batch 35/64 loss: -0.11432206630706787
Batch 36/64 loss: -0.1459881067276001
Batch 37/64 loss: -0.12314450740814209
Batch 38/64 loss: -0.16952502727508545
Batch 39/64 loss: -0.17645734548568726
Batch 40/64 loss: -0.1435888409614563
Batch 41/64 loss: -0.1681962013244629
Batch 42/64 loss: -0.17868036031723022
Batch 43/64 loss: -0.12031000852584839
Batch 44/64 loss: -0.1286909580230713
Batch 45/64 loss: -0.12384665012359619
Batch 46/64 loss: -0.14077436923980713
Batch 47/64 loss: -0.17911136150360107
Batch 48/64 loss: -0.12952250242233276
Batch 49/64 loss: -0.14878952503204346
Batch 50/64 loss: -0.1467251181602478
Batch 51/64 loss: -0.1844266653060913
Batch 52/64 loss: -0.08489960432052612
Batch 53/64 loss: -0.14716720581054688
Batch 54/64 loss: -0.1616777777671814
Batch 55/64 loss: -0.10773003101348877
Batch 56/64 loss: -0.10814988613128662
Batch 57/64 loss: -0.11417394876480103
Batch 58/64 loss: -0.1558701992034912
Batch 59/64 loss: -0.13811028003692627
Batch 60/64 loss: -0.14046210050582886
Batch 61/64 loss: -0.15849202871322632
Batch 62/64 loss: -0.18461114168167114
Batch 63/64 loss: -0.17522460222244263
Batch 64/64 loss: -0.14249545335769653
Epoch 381  Train loss: -0.14652643788094613  Val loss: 0.0572859233187646
Epoch 382
-------------------------------
Batch 1/64 loss: -0.17701828479766846
Batch 2/64 loss: -0.18406057357788086
Batch 3/64 loss: -0.09999936819076538
Batch 4/64 loss: -0.1557525396347046
Batch 5/64 loss: -0.16018974781036377
Batch 6/64 loss: -0.15397363901138306
Batch 7/64 loss: -0.12056183815002441
Batch 8/64 loss: -0.16968375444412231
Batch 9/64 loss: -0.10936236381530762
Batch 10/64 loss: -0.16542643308639526
Batch 11/64 loss: -0.1495736837387085
Batch 12/64 loss: -0.14922606945037842
Batch 13/64 loss: -0.15526556968688965
Batch 14/64 loss: -0.15212589502334595
Batch 15/64 loss: -0.1471102237701416
Batch 16/64 loss: -0.15583741664886475
Batch 17/64 loss: -0.09825378656387329
Batch 18/64 loss: -0.1628422737121582
Batch 19/64 loss: -0.15913605690002441
Batch 20/64 loss: -0.17204785346984863
Batch 21/64 loss: -0.14540070295333862
Batch 22/64 loss: -0.14878439903259277
Batch 23/64 loss: -0.16109955310821533
Batch 24/64 loss: -0.15459918975830078
Batch 25/64 loss: -0.1557164192199707
Batch 26/64 loss: -0.13942325115203857
Batch 27/64 loss: -0.10076701641082764
Batch 28/64 loss: -0.16828864812850952
Batch 29/64 loss: -0.1865992546081543
Batch 30/64 loss: -0.11869877576828003
Batch 31/64 loss: -0.10761076211929321
Batch 32/64 loss: -0.16158175468444824
Batch 33/64 loss: -0.12863105535507202
Batch 34/64 loss: -0.1394892930984497
Batch 35/64 loss: -0.15405809879302979
Batch 36/64 loss: -0.16687148809432983
Batch 37/64 loss: -0.15583407878875732
Batch 38/64 loss: -0.14862209558486938
Batch 39/64 loss: -0.15895718336105347
Batch 40/64 loss: -0.15697115659713745
Batch 41/64 loss: -0.10774922370910645
Batch 42/64 loss: -0.1549593210220337
Batch 43/64 loss: -0.13603025674819946
Batch 44/64 loss: -0.14029622077941895
Batch 45/64 loss: -0.16288620233535767
Batch 46/64 loss: -0.1742669939994812
Batch 47/64 loss: -0.15855926275253296
Batch 48/64 loss: -0.17710000276565552
Batch 49/64 loss: -0.15779799222946167
Batch 50/64 loss: -0.15116119384765625
Batch 51/64 loss: -0.14983117580413818
Batch 52/64 loss: -0.10500407218933105
Batch 53/64 loss: -0.13978219032287598
Batch 54/64 loss: -0.15046203136444092
Batch 55/64 loss: -0.12012171745300293
Batch 56/64 loss: -0.15305978059768677
Batch 57/64 loss: -0.12109029293060303
Batch 58/64 loss: -0.09336841106414795
Batch 59/64 loss: -0.17265170812606812
Batch 60/64 loss: -0.09871923923492432
Batch 61/64 loss: -0.15002256631851196
Batch 62/64 loss: -0.1433507204055786
Batch 63/64 loss: -0.11550045013427734
Batch 64/64 loss: -0.16067886352539062
Epoch 382  Train loss: -0.14603500740200864  Val loss: 0.06353602642865525
Epoch 383
-------------------------------
Batch 1/64 loss: -0.15462839603424072
Batch 2/64 loss: -0.1586124300956726
Batch 3/64 loss: -0.1340043544769287
Batch 4/64 loss: -0.18032652139663696
Batch 5/64 loss: -0.15888911485671997
Batch 6/64 loss: -0.16798746585845947
Batch 7/64 loss: -0.12913143634796143
Batch 8/64 loss: -0.13025259971618652
Batch 9/64 loss: -0.15470337867736816
Batch 10/64 loss: -0.12221205234527588
Batch 11/64 loss: -0.1438518762588501
Batch 12/64 loss: -0.12306326627731323
Batch 13/64 loss: -0.11123490333557129
Batch 14/64 loss: -0.1368556022644043
Batch 15/64 loss: -0.17760992050170898
Batch 16/64 loss: -0.1614246368408203
Batch 17/64 loss: -0.1469787359237671
Batch 18/64 loss: -0.12738651037216187
Batch 19/64 loss: -0.18657249212265015
Batch 20/64 loss: -0.18379801511764526
Batch 21/64 loss: -0.14203625917434692
Batch 22/64 loss: -0.1455707550048828
Batch 23/64 loss: -0.12272375822067261
Batch 24/64 loss: -0.1624690294265747
Batch 25/64 loss: -0.20010745525360107
Batch 26/64 loss: -0.13562637567520142
Batch 27/64 loss: -0.15280777215957642
Batch 28/64 loss: -0.11414945125579834
Batch 29/64 loss: -0.13877713680267334
Batch 30/64 loss: -0.1748952865600586
Batch 31/64 loss: -0.11832070350646973
Batch 32/64 loss: -0.15330612659454346
Batch 33/64 loss: -0.12839418649673462
Batch 34/64 loss: -0.1790751814842224
Batch 35/64 loss: -0.1626361608505249
Batch 36/64 loss: -0.15845388174057007
Batch 37/64 loss: -0.1436784267425537
Batch 38/64 loss: -0.12117069959640503
Batch 39/64 loss: -0.1627124547958374
Batch 40/64 loss: -0.09036296606063843
Batch 41/64 loss: -0.15195882320404053
Batch 42/64 loss: -0.17574739456176758
Batch 43/64 loss: -0.11023110151290894
Batch 44/64 loss: -0.1809874176979065
Batch 45/64 loss: -0.15892881155014038
Batch 46/64 loss: -0.1532212495803833
Batch 47/64 loss: -0.16447114944458008
Batch 48/64 loss: -0.14227783679962158
Batch 49/64 loss: -0.13084208965301514
Batch 50/64 loss: -0.09124588966369629
Batch 51/64 loss: -0.1138385534286499
Batch 52/64 loss: -0.17891275882720947
Batch 53/64 loss: -0.1377137303352356
Batch 54/64 loss: -0.14515942335128784
Batch 55/64 loss: -0.13444507122039795
Batch 56/64 loss: -0.11934584379196167
Batch 57/64 loss: -0.16633057594299316
Batch 58/64 loss: -0.12330663204193115
Batch 59/64 loss: -0.13552862405776978
Batch 60/64 loss: -0.16260427236557007
Batch 61/64 loss: -0.134843647480011
Batch 62/64 loss: -0.15942537784576416
Batch 63/64 loss: -0.14360755681991577
Batch 64/64 loss: -0.14209753274917603
Epoch 383  Train loss: -0.1461701138346803  Val loss: 0.06665787807444937
Epoch 384
-------------------------------
Batch 1/64 loss: -0.10754585266113281
Batch 2/64 loss: -0.1490759253501892
Batch 3/64 loss: -0.15135687589645386
Batch 4/64 loss: -0.18802005052566528
Batch 5/64 loss: -0.13567602634429932
Batch 6/64 loss: -0.1412264108657837
Batch 7/64 loss: -0.13936161994934082
Batch 8/64 loss: -0.1500464677810669
Batch 9/64 loss: -0.18950587511062622
Batch 10/64 loss: -0.15403127670288086
Batch 11/64 loss: -0.16553157567977905
Batch 12/64 loss: -0.15558582544326782
Batch 13/64 loss: -0.15956151485443115
Batch 14/64 loss: -0.13736331462860107
Batch 15/64 loss: -0.14403235912322998
Batch 16/64 loss: -0.16013497114181519
Batch 17/64 loss: -0.14405667781829834
Batch 18/64 loss: -0.15241193771362305
Batch 19/64 loss: -0.14073419570922852
Batch 20/64 loss: -0.13296091556549072
Batch 21/64 loss: -0.16927438974380493
Batch 22/64 loss: -0.14653921127319336
Batch 23/64 loss: -0.15185528993606567
Batch 24/64 loss: -0.15907281637191772
Batch 25/64 loss: -0.12818628549575806
Batch 26/64 loss: -0.14538651704788208
Batch 27/64 loss: -0.1594589352607727
Batch 28/64 loss: -0.16462117433547974
Batch 29/64 loss: -0.13230925798416138
Batch 30/64 loss: -0.14992141723632812
Batch 31/64 loss: -0.12637269496917725
Batch 32/64 loss: -0.16269463300704956
Batch 33/64 loss: -0.14986038208007812
Batch 34/64 loss: -0.14878511428833008
Batch 35/64 loss: -0.12925118207931519
Batch 36/64 loss: -0.14263606071472168
Batch 37/64 loss: -0.15365523099899292
Batch 38/64 loss: -0.12172591686248779
Batch 39/64 loss: -0.14813679456710815
Batch 40/64 loss: -0.1369391679763794
Batch 41/64 loss: -0.13931912183761597
Batch 42/64 loss: -0.14650768041610718
Batch 43/64 loss: -0.17265677452087402
Batch 44/64 loss: -0.1219061017036438
Batch 45/64 loss: -0.1626875400543213
Batch 46/64 loss: -0.13119637966156006
Batch 47/64 loss: -0.16078025102615356
Batch 48/64 loss: -0.16654372215270996
Batch 49/64 loss: -0.16632258892059326
Batch 50/64 loss: -0.18518412113189697
Batch 51/64 loss: -0.09306585788726807
Batch 52/64 loss: -0.1325051188468933
Batch 53/64 loss: -0.1552596092224121
Batch 54/64 loss: -0.1312330961227417
Batch 55/64 loss: -0.17519521713256836
Batch 56/64 loss: -0.1441023349761963
Batch 57/64 loss: -0.1594940423965454
Batch 58/64 loss: -0.16134345531463623
Batch 59/64 loss: -0.1733858585357666
Batch 60/64 loss: -0.1574920415878296
Batch 61/64 loss: -0.13764935731887817
Batch 62/64 loss: -0.15049153566360474
Batch 63/64 loss: -0.14570868015289307
Batch 64/64 loss: -0.11577826738357544
Epoch 384  Train loss: -0.14873358946220547  Val loss: 0.0638177050757654
Epoch 385
-------------------------------
Batch 1/64 loss: -0.16555029153823853
Batch 2/64 loss: -0.18482965230941772
Batch 3/64 loss: -0.13887923955917358
Batch 4/64 loss: -0.17367446422576904
Batch 5/64 loss: -0.18025392293930054
Batch 6/64 loss: -0.15739387273788452
Batch 7/64 loss: -0.17913144826889038
Batch 8/64 loss: -0.15809673070907593
Batch 9/64 loss: -0.17521768808364868
Batch 10/64 loss: -0.19130468368530273
Batch 11/64 loss: -0.13818204402923584
Batch 12/64 loss: -0.11850374937057495
Batch 13/64 loss: -0.1879710555076599
Batch 14/64 loss: -0.16325461864471436
Batch 15/64 loss: -0.07448971271514893
Batch 16/64 loss: -0.15216082334518433
Batch 17/64 loss: -0.1811848282814026
Batch 18/64 loss: -0.13628315925598145
Batch 19/64 loss: -0.17149633169174194
Batch 20/64 loss: -0.16974854469299316
Batch 21/64 loss: -0.13440179824829102
Batch 22/64 loss: -0.12623268365859985
Batch 23/64 loss: -0.1576104760169983
Batch 24/64 loss: -0.13704311847686768
Batch 25/64 loss: -0.1759830117225647
Batch 26/64 loss: -0.1955690085887909
Batch 27/64 loss: -0.10611486434936523
Batch 28/64 loss: -0.15710240602493286
Batch 29/64 loss: -0.1665259599685669
Batch 30/64 loss: -0.12713980674743652
Batch 31/64 loss: -0.12465798854827881
Batch 32/64 loss: -0.14358949661254883
Batch 33/64 loss: -0.10150784254074097
Batch 34/64 loss: -0.13364464044570923
Batch 35/64 loss: -0.12144559621810913
Batch 36/64 loss: -0.14576315879821777
Batch 37/64 loss: -0.11782026290893555
Batch 38/64 loss: -0.17794030904769897
Batch 39/64 loss: -0.1760343313217163
Batch 40/64 loss: -0.15983927249908447
Batch 41/64 loss: -0.1371469497680664
Batch 42/64 loss: -0.14179319143295288
Batch 43/64 loss: -0.18923938274383545
Batch 44/64 loss: -0.17941784858703613
Batch 45/64 loss: -0.16832411289215088
Batch 46/64 loss: -0.18450045585632324
Batch 47/64 loss: -0.14807558059692383
Batch 48/64 loss: -0.160541832447052
Batch 49/64 loss: -0.11911511421203613
Batch 50/64 loss: -0.16403436660766602
Batch 51/64 loss: -0.17467892169952393
Batch 52/64 loss: -0.09797430038452148
Batch 53/64 loss: -0.10980844497680664
Batch 54/64 loss: -0.14760994911193848
Batch 55/64 loss: -0.17817062139511108
Batch 56/64 loss: -0.18161362409591675
Batch 57/64 loss: -0.13870739936828613
Batch 58/64 loss: -0.16767573356628418
Batch 59/64 loss: -0.11739027500152588
Batch 60/64 loss: -0.1719765067100525
Batch 61/64 loss: -0.14978688955307007
Batch 62/64 loss: -0.07311272621154785
Batch 63/64 loss: -0.09925884008407593
Batch 64/64 loss: -0.1060103178024292
Epoch 385  Train loss: -0.15000837176453832  Val loss: 0.06433079885862947
Epoch 386
-------------------------------
Batch 1/64 loss: -0.13272911310195923
Batch 2/64 loss: -0.17805147171020508
Batch 3/64 loss: -0.1541752815246582
Batch 4/64 loss: -0.15655285120010376
Batch 5/64 loss: -0.13817065954208374
Batch 6/64 loss: -0.14131534099578857
Batch 7/64 loss: -0.14929473400115967
Batch 8/64 loss: -0.14471173286437988
Batch 9/64 loss: -0.16668176651000977
Batch 10/64 loss: -0.11350607872009277
Batch 11/64 loss: -0.17838048934936523
Batch 12/64 loss: -0.12891870737075806
Batch 13/64 loss: -0.1668696403503418
Batch 14/64 loss: -0.18179243803024292
Batch 15/64 loss: -0.14294582605361938
Batch 16/64 loss: -0.15573549270629883
Batch 17/64 loss: -0.12874174118041992
Batch 18/64 loss: -0.09674829244613647
Batch 19/64 loss: -0.16076600551605225
Batch 20/64 loss: -0.17820674180984497
Batch 21/64 loss: -0.1510765552520752
Batch 22/64 loss: -0.0661393404006958
Batch 23/64 loss: -0.12017452716827393
Batch 24/64 loss: -0.13029062747955322
Batch 25/64 loss: -0.1672811508178711
Batch 26/64 loss: -0.15048205852508545
Batch 27/64 loss: -0.1640106439590454
Batch 28/64 loss: -0.174247145652771
Batch 29/64 loss: -0.18276101350784302
Batch 30/64 loss: -0.14035654067993164
Batch 31/64 loss: -0.17158901691436768
Batch 32/64 loss: -0.1310594081878662
Batch 33/64 loss: -0.1528053879737854
Batch 34/64 loss: -0.1386939287185669
Batch 35/64 loss: -0.15673726797103882
Batch 36/64 loss: -0.17986220121383667
Batch 37/64 loss: -0.12679678201675415
Batch 38/64 loss: -0.15280228853225708
Batch 39/64 loss: -0.10818493366241455
Batch 40/64 loss: -0.14421194791793823
Batch 41/64 loss: -0.12348675727844238
Batch 42/64 loss: -0.13170063495635986
Batch 43/64 loss: -0.14608168601989746
Batch 44/64 loss: -0.12553924322128296
Batch 45/64 loss: -0.12808316946029663
Batch 46/64 loss: -0.15774953365325928
Batch 47/64 loss: -0.16130417585372925
Batch 48/64 loss: -0.1266041398048401
Batch 49/64 loss: -0.12346160411834717
Batch 50/64 loss: -0.15240752696990967
Batch 51/64 loss: -0.15119314193725586
Batch 52/64 loss: -0.17208409309387207
Batch 53/64 loss: -0.15645122528076172
Batch 54/64 loss: -0.14926564693450928
Batch 55/64 loss: -0.14389413595199585
Batch 56/64 loss: -0.15660786628723145
Batch 57/64 loss: -0.15606504678726196
Batch 58/64 loss: -0.13622885942459106
Batch 59/64 loss: -0.18444430828094482
Batch 60/64 loss: -0.1378328800201416
Batch 61/64 loss: -0.10064542293548584
Batch 62/64 loss: -0.18117088079452515
Batch 63/64 loss: -0.14554840326309204
Batch 64/64 loss: -0.14669793844223022
Epoch 386  Train loss: -0.14685068247365016  Val loss: 0.06675983007830852
Epoch 387
-------------------------------
Batch 1/64 loss: -0.16824030876159668
Batch 2/64 loss: -0.13099819421768188
Batch 3/64 loss: -0.1697845458984375
Batch 4/64 loss: -0.16818273067474365
Batch 5/64 loss: -0.09978848695755005
Batch 6/64 loss: -0.15712440013885498
Batch 7/64 loss: -0.15408951044082642
Batch 8/64 loss: -0.16585248708724976
Batch 9/64 loss: -0.15634536743164062
Batch 10/64 loss: -0.1790771484375
Batch 11/64 loss: -0.17439699172973633
Batch 12/64 loss: -0.1678558588027954
Batch 13/64 loss: -0.13456231355667114
Batch 14/64 loss: -0.13123399019241333
Batch 15/64 loss: -0.1711024045944214
Batch 16/64 loss: -0.141515851020813
Batch 17/64 loss: -0.155400812625885
Batch 18/64 loss: -0.16495847702026367
Batch 19/64 loss: -0.16031593084335327
Batch 20/64 loss: -0.14032703638076782
Batch 21/64 loss: -0.13594692945480347
Batch 22/64 loss: -0.1420843005180359
Batch 23/64 loss: -0.16089093685150146
Batch 24/64 loss: -0.14307618141174316
Batch 25/64 loss: -0.1644601821899414
Batch 26/64 loss: -0.13880330324172974
Batch 27/64 loss: -0.15393108129501343
Batch 28/64 loss: -0.16305118799209595
Batch 29/64 loss: -0.18115234375
Batch 30/64 loss: -0.10680758953094482
Batch 31/64 loss: -0.16154301166534424
Batch 32/64 loss: -0.14667099714279175
Batch 33/64 loss: -0.08203208446502686
Batch 34/64 loss: -0.16846126317977905
Batch 35/64 loss: -0.16296422481536865
Batch 36/64 loss: -0.1722838282585144
Batch 37/64 loss: -0.13773006200790405
Batch 38/64 loss: -0.1616595983505249
Batch 39/64 loss: -0.1077190637588501
Batch 40/64 loss: -0.11138182878494263
Batch 41/64 loss: -0.14022672176361084
Batch 42/64 loss: -0.13809823989868164
Batch 43/64 loss: -0.1317911148071289
Batch 44/64 loss: -0.14826923608779907
Batch 45/64 loss: -0.1490759253501892
Batch 46/64 loss: -0.18246549367904663
Batch 47/64 loss: -0.16200989484786987
Batch 48/64 loss: -0.18607807159423828
Batch 49/64 loss: -0.1371537446975708
Batch 50/64 loss: -0.14383941888809204
Batch 51/64 loss: -0.15423941612243652
Batch 52/64 loss: -0.12423580884933472
Batch 53/64 loss: -0.151350736618042
Batch 54/64 loss: -0.1299331784248352
Batch 55/64 loss: -0.12780147790908813
Batch 56/64 loss: -0.12331712245941162
Batch 57/64 loss: -0.16408872604370117
Batch 58/64 loss: -0.16868436336517334
Batch 59/64 loss: -0.10192382335662842
Batch 60/64 loss: -0.1616315245628357
Batch 61/64 loss: -0.16572421789169312
Batch 62/64 loss: -0.14127123355865479
Batch 63/64 loss: -0.16023504734039307
Batch 64/64 loss: -0.12116366624832153
Epoch 387  Train loss: -0.14867639424754123  Val loss: 0.06713021620852021
Epoch 388
-------------------------------
Batch 1/64 loss: -0.1760857105255127
Batch 2/64 loss: -0.17464745044708252
Batch 3/64 loss: -0.1590067744255066
Batch 4/64 loss: -0.1625424027442932
Batch 5/64 loss: -0.1524680256843567
Batch 6/64 loss: -0.18799042701721191
Batch 7/64 loss: -0.16080981492996216
Batch 8/64 loss: -0.11071795225143433
Batch 9/64 loss: -0.1592719554901123
Batch 10/64 loss: -0.17793989181518555
Batch 11/64 loss: -0.13654804229736328
Batch 12/64 loss: -0.16977500915527344
Batch 13/64 loss: -0.14554792642593384
Batch 14/64 loss: -0.14201408624649048
Batch 15/64 loss: -0.1549476981163025
Batch 16/64 loss: -0.14845657348632812
Batch 17/64 loss: -0.1712738275527954
Batch 18/64 loss: -0.15031015872955322
Batch 19/64 loss: -0.14618009328842163
Batch 20/64 loss: -0.12040907144546509
Batch 21/64 loss: -0.14979541301727295
Batch 22/64 loss: -0.1589510440826416
Batch 23/64 loss: -0.13604241609573364
Batch 24/64 loss: -0.13378554582595825
Batch 25/64 loss: -0.13469111919403076
Batch 26/64 loss: -0.1689090132713318
Batch 27/64 loss: -0.11967277526855469
Batch 28/64 loss: -0.1139182448387146
Batch 29/64 loss: -0.17234158515930176
Batch 30/64 loss: -0.13531571626663208
Batch 31/64 loss: -0.10858273506164551
Batch 32/64 loss: -0.14762842655181885
Batch 33/64 loss: -0.13975852727890015
Batch 34/64 loss: -0.15778130292892456
Batch 35/64 loss: -0.15519684553146362
Batch 36/64 loss: -0.14126700162887573
Batch 37/64 loss: -0.1715867519378662
Batch 38/64 loss: -0.15251320600509644
Batch 39/64 loss: -0.1436018943786621
Batch 40/64 loss: -0.18171507120132446
Batch 41/64 loss: -0.15726381540298462
Batch 42/64 loss: -0.1816713809967041
Batch 43/64 loss: -0.13641023635864258
Batch 44/64 loss: -0.16266965866088867
Batch 45/64 loss: -0.1774318814277649
Batch 46/64 loss: -0.14386194944381714
Batch 47/64 loss: -0.1266964077949524
Batch 48/64 loss: -0.14987993240356445
Batch 49/64 loss: -0.14473789930343628
Batch 50/64 loss: -0.13114088773727417
Batch 51/64 loss: -0.12240326404571533
Batch 52/64 loss: -0.12241339683532715
Batch 53/64 loss: -0.14912110567092896
Batch 54/64 loss: -0.155417799949646
Batch 55/64 loss: -0.12599587440490723
Batch 56/64 loss: -0.1760350465774536
Batch 57/64 loss: -0.07305014133453369
Batch 58/64 loss: -0.17575013637542725
Batch 59/64 loss: -0.11321866512298584
Batch 60/64 loss: -0.17243897914886475
Batch 61/64 loss: -0.16321879625320435
Batch 62/64 loss: -0.1461774706840515
Batch 63/64 loss: -0.15725171566009521
Batch 64/64 loss: -0.14695924520492554
Epoch 388  Train loss: -0.14908977887209723  Val loss: 0.06269886751764829
Epoch 389
-------------------------------
Batch 1/64 loss: -0.14103710651397705
Batch 2/64 loss: -0.13513857126235962
Batch 3/64 loss: -0.15239381790161133
Batch 4/64 loss: -0.17515277862548828
Batch 5/64 loss: -0.15059930086135864
Batch 6/64 loss: -0.16715455055236816
Batch 7/64 loss: -0.129696786403656
Batch 8/64 loss: -0.17010492086410522
Batch 9/64 loss: -0.17904138565063477
Batch 10/64 loss: -0.16790646314620972
Batch 11/64 loss: -0.09881961345672607
Batch 12/64 loss: -0.15428149700164795
Batch 13/64 loss: -0.12926602363586426
Batch 14/64 loss: -0.15693360567092896
Batch 15/64 loss: -0.13755583763122559
Batch 16/64 loss: -0.17664045095443726
Batch 17/64 loss: -0.16753917932510376
Batch 18/64 loss: -0.15256112813949585
Batch 19/64 loss: -0.16923648118972778
Batch 20/64 loss: -0.13928544521331787
Batch 21/64 loss: -0.11961567401885986
Batch 22/64 loss: -0.14772295951843262
Batch 23/64 loss: -0.14503967761993408
Batch 24/64 loss: -0.15224981307983398
Batch 25/64 loss: -0.15172940492630005
Batch 26/64 loss: -0.1649278998374939
Batch 27/64 loss: -0.17575544118881226
Batch 28/64 loss: -0.14090675115585327
Batch 29/64 loss: -0.16415172815322876
Batch 30/64 loss: -0.16212177276611328
Batch 31/64 loss: -0.1765938401222229
Batch 32/64 loss: -0.17099034786224365
Batch 33/64 loss: -0.19506418704986572
Batch 34/64 loss: -0.13169610500335693
Batch 35/64 loss: -0.16679596900939941
Batch 36/64 loss: -0.125715434551239
Batch 37/64 loss: -0.16970402002334595
Batch 38/64 loss: -0.15544748306274414
Batch 39/64 loss: -0.10931295156478882
Batch 40/64 loss: -0.17012161016464233
Batch 41/64 loss: -0.1561538577079773
Batch 42/64 loss: -0.1585533618927002
Batch 43/64 loss: -0.13035166263580322
Batch 44/64 loss: -0.16142523288726807
Batch 45/64 loss: -0.1383044719696045
Batch 46/64 loss: -0.11708581447601318
Batch 47/64 loss: -0.17207551002502441
Batch 48/64 loss: -0.18047881126403809
Batch 49/64 loss: -0.14301449060440063
Batch 50/64 loss: -0.1491936445236206
Batch 51/64 loss: -0.15100973844528198
Batch 52/64 loss: -0.1522340178489685
Batch 53/64 loss: -0.13880980014801025
Batch 54/64 loss: -0.12801223993301392
Batch 55/64 loss: -0.1539667248725891
Batch 56/64 loss: -0.13647222518920898
Batch 57/64 loss: -0.11687880754470825
Batch 58/64 loss: -0.10980558395385742
Batch 59/64 loss: -0.1700221300125122
Batch 60/64 loss: -0.14953410625457764
Batch 61/64 loss: -0.1850529909133911
Batch 62/64 loss: -0.15222525596618652
Batch 63/64 loss: -0.14542198181152344
Batch 64/64 loss: -0.17152267694473267
Epoch 389  Train loss: -0.15169776444341623  Val loss: 0.06404386721935469
Epoch 390
-------------------------------
Batch 1/64 loss: -0.12416082620620728
Batch 2/64 loss: -0.14103776216506958
Batch 3/64 loss: -0.1658487319946289
Batch 4/64 loss: -0.15079182386398315
Batch 5/64 loss: -0.16587090492248535
Batch 6/64 loss: -0.15888822078704834
Batch 7/64 loss: -0.16966354846954346
Batch 8/64 loss: -0.1680455207824707
Batch 9/64 loss: -0.1579018235206604
Batch 10/64 loss: -0.12315410375595093
Batch 11/64 loss: -0.16163212060928345
Batch 12/64 loss: -0.16778242588043213
Batch 13/64 loss: -0.1684449315071106
Batch 14/64 loss: -0.12939250469207764
Batch 15/64 loss: -0.13487344980239868
Batch 16/64 loss: -0.16653090715408325
Batch 17/64 loss: -0.11418259143829346
Batch 18/64 loss: -0.12453770637512207
Batch 19/64 loss: -0.1522533893585205
Batch 20/64 loss: -0.1278611421585083
Batch 21/64 loss: -0.14884072542190552
Batch 22/64 loss: -0.1505565047264099
Batch 23/64 loss: -0.18069565296173096
Batch 24/64 loss: -0.16399836540222168
Batch 25/64 loss: -0.1694437861442566
Batch 26/64 loss: -0.15474152565002441
Batch 27/64 loss: -0.15308541059494019
Batch 28/64 loss: -0.16987234354019165
Batch 29/64 loss: -0.14338725805282593
Batch 30/64 loss: -0.16171163320541382
Batch 31/64 loss: -0.15125322341918945
Batch 32/64 loss: -0.15565651655197144
Batch 33/64 loss: -0.1867794394493103
Batch 34/64 loss: -0.14538943767547607
Batch 35/64 loss: -0.12915313243865967
Batch 36/64 loss: -0.17516469955444336
Batch 37/64 loss: -0.10406595468521118
Batch 38/64 loss: -0.1743878722190857
Batch 39/64 loss: -0.1586112380027771
Batch 40/64 loss: -0.1517370343208313
Batch 41/64 loss: -0.12399005889892578
Batch 42/64 loss: -0.16986948251724243
Batch 43/64 loss: -0.1602962613105774
Batch 44/64 loss: -0.11443823575973511
Batch 45/64 loss: -0.18948769569396973
Batch 46/64 loss: -0.1321004033088684
Batch 47/64 loss: -0.170063316822052
Batch 48/64 loss: -0.16979694366455078
Batch 49/64 loss: -0.13007766008377075
Batch 50/64 loss: -0.18890535831451416
Batch 51/64 loss: -0.11713564395904541
Batch 52/64 loss: -0.12994647026062012
Batch 53/64 loss: -0.10705077648162842
Batch 54/64 loss: -0.12305307388305664
Batch 55/64 loss: -0.1664438247680664
Batch 56/64 loss: -0.1618158221244812
Batch 57/64 loss: -0.17486155033111572
Batch 58/64 loss: -0.11807972192764282
Batch 59/64 loss: -0.1321626901626587
Batch 60/64 loss: -0.1407070755958557
Batch 61/64 loss: -0.13663864135742188
Batch 62/64 loss: -0.15583699941635132
Batch 63/64 loss: -0.10631567239761353
Batch 64/64 loss: -0.11117905378341675
Epoch 390  Train loss: -0.14907990366804832  Val loss: 0.06446593160072144
Epoch 391
-------------------------------
Batch 1/64 loss: -0.13340169191360474
Batch 2/64 loss: -0.17769700288772583
Batch 3/64 loss: -0.14488005638122559
Batch 4/64 loss: -0.18522405624389648
Batch 5/64 loss: -0.1795099973678589
Batch 6/64 loss: -0.17479944229125977
Batch 7/64 loss: -0.10311037302017212
Batch 8/64 loss: -0.16110122203826904
Batch 9/64 loss: -0.15716594457626343
Batch 10/64 loss: -0.1605314016342163
Batch 11/64 loss: -0.19094407558441162
Batch 12/64 loss: -0.15496611595153809
Batch 13/64 loss: -0.16318124532699585
Batch 14/64 loss: -0.1774529218673706
Batch 15/64 loss: -0.12378603219985962
Batch 16/64 loss: -0.10977756977081299
Batch 17/64 loss: -0.09974497556686401
Batch 18/64 loss: -0.15356796979904175
Batch 19/64 loss: -0.18417561054229736
Batch 20/64 loss: -0.152959942817688
Batch 21/64 loss: -0.1606951355934143
Batch 22/64 loss: -0.10336291790008545
Batch 23/64 loss: -0.15079164505004883
Batch 24/64 loss: -0.15928900241851807
Batch 25/64 loss: -0.12850308418273926
Batch 26/64 loss: -0.19517487287521362
Batch 27/64 loss: -0.12199962139129639
Batch 28/64 loss: -0.1616584062576294
Batch 29/64 loss: -0.14839094877243042
Batch 30/64 loss: -0.1557942032814026
Batch 31/64 loss: -0.16536492109298706
Batch 32/64 loss: -0.1578291654586792
Batch 33/64 loss: -0.1300121545791626
Batch 34/64 loss: -0.1606186032295227
Batch 35/64 loss: -0.10333561897277832
Batch 36/64 loss: -0.16847509145736694
Batch 37/64 loss: -0.15734463930130005
Batch 38/64 loss: -0.11987102031707764
Batch 39/64 loss: -0.1502869725227356
Batch 40/64 loss: -0.10365206003189087
Batch 41/64 loss: -0.1264781355857849
Batch 42/64 loss: -0.15370041131973267
Batch 43/64 loss: -0.13598567247390747
Batch 44/64 loss: -0.1675383448600769
Batch 45/64 loss: -0.1799837350845337
Batch 46/64 loss: -0.15484619140625
Batch 47/64 loss: -0.14489340782165527
Batch 48/64 loss: -0.09438109397888184
Batch 49/64 loss: -0.18713414669036865
Batch 50/64 loss: -0.14977681636810303
Batch 51/64 loss: -0.16266340017318726
Batch 52/64 loss: -0.14944827556610107
Batch 53/64 loss: -0.15959292650222778
Batch 54/64 loss: -0.14422935247421265
Batch 55/64 loss: -0.14057421684265137
Batch 56/64 loss: -0.1246253252029419
Batch 57/64 loss: -0.13898420333862305
Batch 58/64 loss: -0.1227908730506897
Batch 59/64 loss: -0.12879407405853271
Batch 60/64 loss: -0.13002508878707886
Batch 61/64 loss: -0.17997205257415771
Batch 62/64 loss: -0.17490041255950928
Batch 63/64 loss: -0.1310340166091919
Batch 64/64 loss: -0.1587834358215332
Epoch 391  Train loss: -0.1488920389437208  Val loss: 0.07245160173304711
Epoch 392
-------------------------------
Batch 1/64 loss: -0.1851135492324829
Batch 2/64 loss: -0.165646493434906
Batch 3/64 loss: -0.1268366575241089
Batch 4/64 loss: -0.19307005405426025
Batch 5/64 loss: -0.15565872192382812
Batch 6/64 loss: -0.1394026279449463
Batch 7/64 loss: -0.18203389644622803
Batch 8/64 loss: -0.17042070627212524
Batch 9/64 loss: -0.16735637187957764
Batch 10/64 loss: -0.19331493973731995
Batch 11/64 loss: -0.18580353260040283
Batch 12/64 loss: -0.1561295986175537
Batch 13/64 loss: -0.10913598537445068
Batch 14/64 loss: -0.12958037853240967
Batch 15/64 loss: -0.1725170612335205
Batch 16/64 loss: -0.17499852180480957
Batch 17/64 loss: -0.13833487033843994
Batch 18/64 loss: -0.16515129804611206
Batch 19/64 loss: -0.14420557022094727
Batch 20/64 loss: -0.18226087093353271
Batch 21/64 loss: -0.16224563121795654
Batch 22/64 loss: -0.16677117347717285
Batch 23/64 loss: -0.13701891899108887
Batch 24/64 loss: -0.14236056804656982
Batch 25/64 loss: -0.16376656293869019
Batch 26/64 loss: -0.1408182978630066
Batch 27/64 loss: -0.13241714239120483
Batch 28/64 loss: -0.18982505798339844
Batch 29/64 loss: -0.14749830961227417
Batch 30/64 loss: -0.10620927810668945
Batch 31/64 loss: -0.10611569881439209
Batch 32/64 loss: -0.17065280675888062
Batch 33/64 loss: -0.14616996049880981
Batch 34/64 loss: -0.1901119351387024
Batch 35/64 loss: -0.17749303579330444
Batch 36/64 loss: -0.17754924297332764
Batch 37/64 loss: -0.1510874629020691
Batch 38/64 loss: -0.15742570161819458
Batch 39/64 loss: -0.12252020835876465
Batch 40/64 loss: -0.1251988410949707
Batch 41/64 loss: -0.1638864278793335
Batch 42/64 loss: -0.16811519861221313
Batch 43/64 loss: -0.12562566995620728
Batch 44/64 loss: -0.17488807439804077
Batch 45/64 loss: -0.14658385515213013
Batch 46/64 loss: -0.14898312091827393
Batch 47/64 loss: -0.16543591022491455
Batch 48/64 loss: -0.11657077074050903
Batch 49/64 loss: -0.16104984283447266
Batch 50/64 loss: -0.15706908702850342
Batch 51/64 loss: -0.15345007181167603
Batch 52/64 loss: -0.11193954944610596
Batch 53/64 loss: -0.14726001024246216
Batch 54/64 loss: -0.130945086479187
Batch 55/64 loss: -0.12350493669509888
Batch 56/64 loss: -0.16419482231140137
Batch 57/64 loss: -0.16620004177093506
Batch 58/64 loss: -0.14396166801452637
Batch 59/64 loss: -0.159204363822937
Batch 60/64 loss: -0.13866949081420898
Batch 61/64 loss: -0.11832934617996216
Batch 62/64 loss: -0.15323090553283691
Batch 63/64 loss: -0.10220742225646973
Batch 64/64 loss: -0.1476731300354004
Epoch 392  Train loss: -0.15219275390400605  Val loss: 0.07210621555236607
Epoch 393
-------------------------------
Batch 1/64 loss: -0.15841901302337646
Batch 2/64 loss: -0.17608219385147095
Batch 3/64 loss: -0.12914663553237915
Batch 4/64 loss: -0.13452857732772827
Batch 5/64 loss: -0.14481395483016968
Batch 6/64 loss: -0.12374246120452881
Batch 7/64 loss: -0.14724713563919067
Batch 8/64 loss: -0.16268187761306763
Batch 9/64 loss: -0.16293120384216309
Batch 10/64 loss: -0.18419194221496582
Batch 11/64 loss: -0.16986441612243652
Batch 12/64 loss: -0.1421971321105957
Batch 13/64 loss: -0.12110865116119385
Batch 14/64 loss: -0.18552875518798828
Batch 15/64 loss: -0.1833103895187378
Batch 16/64 loss: -0.13269579410552979
Batch 17/64 loss: -0.18636977672576904
Batch 18/64 loss: -0.14004695415496826
Batch 19/64 loss: -0.17916715145111084
Batch 20/64 loss: -0.14594441652297974
Batch 21/64 loss: -0.1543462872505188
Batch 22/64 loss: -0.102958083152771
Batch 23/64 loss: -0.15154743194580078
Batch 24/64 loss: -0.15076971054077148
Batch 25/64 loss: -0.1588612198829651
Batch 26/64 loss: -0.1452031135559082
Batch 27/64 loss: -0.17731398344039917
Batch 28/64 loss: -0.1637219786643982
Batch 29/64 loss: -0.16266006231307983
Batch 30/64 loss: -0.1300317645072937
Batch 31/64 loss: -0.15960782766342163
Batch 32/64 loss: -0.13622212409973145
Batch 33/64 loss: -0.18128281831741333
Batch 34/64 loss: -0.16115611791610718
Batch 35/64 loss: -0.1536380648612976
Batch 36/64 loss: -0.14307153224945068
Batch 37/64 loss: -0.12072688341140747
Batch 38/64 loss: -0.16343212127685547
Batch 39/64 loss: -0.12045848369598389
Batch 40/64 loss: -0.17614120244979858
Batch 41/64 loss: -0.13699746131896973
Batch 42/64 loss: -0.13557815551757812
Batch 43/64 loss: -0.13694143295288086
Batch 44/64 loss: -0.14367496967315674
Batch 45/64 loss: -0.16641080379486084
Batch 46/64 loss: -0.14734303951263428
Batch 47/64 loss: -0.13793349266052246
Batch 48/64 loss: -0.1647687554359436
Batch 49/64 loss: -0.12504231929779053
Batch 50/64 loss: -0.18344277143478394
Batch 51/64 loss: -0.16346192359924316
Batch 52/64 loss: -0.16855698823928833
Batch 53/64 loss: -0.15284836292266846
Batch 54/64 loss: -0.07585263252258301
Batch 55/64 loss: -0.10411208868026733
Batch 56/64 loss: -0.13543641567230225
Batch 57/64 loss: -0.146956205368042
Batch 58/64 loss: -0.1503511667251587
Batch 59/64 loss: -0.14635062217712402
Batch 60/64 loss: -0.1682787537574768
Batch 61/64 loss: -0.1540856957435608
Batch 62/64 loss: -0.17741191387176514
Batch 63/64 loss: -0.1330355405807495
Batch 64/64 loss: -0.1442984938621521
Epoch 393  Train loss: -0.15037277843437943  Val loss: 0.06517634666252792
Epoch 394
-------------------------------
Batch 1/64 loss: -0.18346357345581055
Batch 2/64 loss: -0.15744894742965698
Batch 3/64 loss: -0.16524815559387207
Batch 4/64 loss: -0.15435433387756348
Batch 5/64 loss: -0.18048220872879028
Batch 6/64 loss: -0.18159383535385132
Batch 7/64 loss: -0.11587750911712646
Batch 8/64 loss: -0.18207788467407227
Batch 9/64 loss: -0.1439296007156372
Batch 10/64 loss: -0.17054975032806396
Batch 11/64 loss: -0.15193355083465576
Batch 12/64 loss: -0.12078619003295898
Batch 13/64 loss: -0.1511598825454712
Batch 14/64 loss: -0.1607850193977356
Batch 15/64 loss: -0.1582840085029602
Batch 16/64 loss: -0.18863099813461304
Batch 17/64 loss: -0.12567025423049927
Batch 18/64 loss: -0.164290189743042
Batch 19/64 loss: -0.15953510999679565
Batch 20/64 loss: -0.16009342670440674
Batch 21/64 loss: -0.13636434078216553
Batch 22/64 loss: -0.11846381425857544
Batch 23/64 loss: -0.14817965030670166
Batch 24/64 loss: -0.16415631771087646
Batch 25/64 loss: -0.1649932861328125
Batch 26/64 loss: -0.15587562322616577
Batch 27/64 loss: -0.13555097579956055
Batch 28/64 loss: -0.1684684157371521
Batch 29/64 loss: -0.1803431510925293
Batch 30/64 loss: -0.15580904483795166
Batch 31/64 loss: -0.15078204870224
Batch 32/64 loss: -0.1590602993965149
Batch 33/64 loss: -0.15661704540252686
Batch 34/64 loss: -0.1469811201095581
Batch 35/64 loss: -0.14773917198181152
Batch 36/64 loss: -0.13939785957336426
Batch 37/64 loss: -0.15998512506484985
Batch 38/64 loss: -0.11042171716690063
Batch 39/64 loss: -0.11604839563369751
Batch 40/64 loss: -0.12546777725219727
Batch 41/64 loss: -0.10904830694198608
Batch 42/64 loss: -0.180561363697052
Batch 43/64 loss: -0.1334863305091858
Batch 44/64 loss: -0.16591566801071167
Batch 45/64 loss: -0.16641002893447876
Batch 46/64 loss: -0.14804816246032715
Batch 47/64 loss: -0.16627061367034912
Batch 48/64 loss: -0.1633533239364624
Batch 49/64 loss: -0.18993616104125977
Batch 50/64 loss: -0.14849168062210083
Batch 51/64 loss: -0.10023552179336548
Batch 52/64 loss: -0.12457013130187988
Batch 53/64 loss: -0.13796263933181763
Batch 54/64 loss: -0.09780502319335938
Batch 55/64 loss: -0.14641845226287842
Batch 56/64 loss: -0.06304824352264404
Batch 57/64 loss: -0.14777827262878418
Batch 58/64 loss: -0.13536405563354492
Batch 59/64 loss: -0.15600740909576416
Batch 60/64 loss: -0.14097654819488525
Batch 61/64 loss: -0.16759192943572998
Batch 62/64 loss: -0.11876529455184937
Batch 63/64 loss: -0.18891525268554688
Batch 64/64 loss: -0.14557665586471558
Epoch 394  Train loss: -0.14938105905757232  Val loss: 0.061508129142813665
Epoch 395
-------------------------------
Batch 1/64 loss: -0.1687309741973877
Batch 2/64 loss: -0.1440134048461914
Batch 3/64 loss: -0.14967429637908936
Batch 4/64 loss: -0.19433236122131348
Batch 5/64 loss: -0.13956469297409058
Batch 6/64 loss: -0.17247170209884644
Batch 7/64 loss: -0.11796176433563232
Batch 8/64 loss: -0.12057030200958252
Batch 9/64 loss: -0.12303990125656128
Batch 10/64 loss: -0.16807633638381958
Batch 11/64 loss: -0.1674429178237915
Batch 12/64 loss: -0.153084397315979
Batch 13/64 loss: -0.15096265077590942
Batch 14/64 loss: -0.12482887506484985
Batch 15/64 loss: -0.1537024974822998
Batch 16/64 loss: -0.14360278844833374
Batch 17/64 loss: -0.15867769718170166
Batch 18/64 loss: -0.1695263385772705
Batch 19/64 loss: -0.12799829244613647
Batch 20/64 loss: -0.15266883373260498
Batch 21/64 loss: -0.16471493244171143
Batch 22/64 loss: -0.15867877006530762
Batch 23/64 loss: -0.17056667804718018
Batch 24/64 loss: -0.15497612953186035
Batch 25/64 loss: -0.13203424215316772
Batch 26/64 loss: -0.1533464789390564
Batch 27/64 loss: -0.13314706087112427
Batch 28/64 loss: -0.17269867658615112
Batch 29/64 loss: -0.10142552852630615
Batch 30/64 loss: -0.17429578304290771
Batch 31/64 loss: -0.16308683156967163
Batch 32/64 loss: -0.14396452903747559
Batch 33/64 loss: -0.18325495719909668
Batch 34/64 loss: -0.16581416130065918
Batch 35/64 loss: -0.13785237073898315
Batch 36/64 loss: -0.15606528520584106
Batch 37/64 loss: -0.15178483724594116
Batch 38/64 loss: -0.12932872772216797
Batch 39/64 loss: -0.15058082342147827
Batch 40/64 loss: -0.13959157466888428
Batch 41/64 loss: -0.16339385509490967
Batch 42/64 loss: -0.18655294179916382
Batch 43/64 loss: -0.15635168552398682
Batch 44/64 loss: -0.15719085931777954
Batch 45/64 loss: -0.10230386257171631
Batch 46/64 loss: -0.1459563970565796
Batch 47/64 loss: -0.15476065874099731
Batch 48/64 loss: -0.14747905731201172
Batch 49/64 loss: -0.15394431352615356
Batch 50/64 loss: -0.16916519403457642
Batch 51/64 loss: -0.15608566999435425
Batch 52/64 loss: -0.14896363019943237
Batch 53/64 loss: -0.139665424823761
Batch 54/64 loss: -0.16719579696655273
Batch 55/64 loss: -0.15696090459823608
Batch 56/64 loss: -0.1401609182357788
Batch 57/64 loss: -0.17593342065811157
Batch 58/64 loss: -0.15365707874298096
Batch 59/64 loss: -0.13505011796951294
Batch 60/64 loss: -0.16208600997924805
Batch 61/64 loss: -0.13245302438735962
Batch 62/64 loss: -0.12553393840789795
Batch 63/64 loss: -0.1545814871788025
Batch 64/64 loss: -0.11301863193511963
Epoch 395  Train loss: -0.1507188960617664  Val loss: 0.06894894625313093
Epoch 396
-------------------------------
Batch 1/64 loss: -0.18800264596939087
Batch 2/64 loss: -0.13556432723999023
Batch 3/64 loss: -0.15043509006500244
Batch 4/64 loss: -0.12735098600387573
Batch 5/64 loss: -0.14846855401992798
Batch 6/64 loss: -0.14681792259216309
Batch 7/64 loss: -0.1690254807472229
Batch 8/64 loss: -0.1466449499130249
Batch 9/64 loss: -0.15351426601409912
Batch 10/64 loss: -0.06174063682556152
Batch 11/64 loss: -0.18219906091690063
Batch 12/64 loss: -0.16689592599868774
Batch 13/64 loss: -0.17702746391296387
Batch 14/64 loss: -0.1730360984802246
Batch 15/64 loss: -0.17121446132659912
Batch 16/64 loss: -0.15713775157928467
Batch 17/64 loss: -0.141490638256073
Batch 18/64 loss: -0.18657314777374268
Batch 19/64 loss: -0.15985798835754395
Batch 20/64 loss: -0.17404139041900635
Batch 21/64 loss: -0.16405725479125977
Batch 22/64 loss: -0.13389837741851807
Batch 23/64 loss: -0.17098075151443481
Batch 24/64 loss: -0.13997477293014526
Batch 25/64 loss: -0.17351901531219482
Batch 26/64 loss: -0.18708372116088867
Batch 27/64 loss: -0.15537166595458984
Batch 28/64 loss: -0.1610751748085022
Batch 29/64 loss: -0.15890520811080933
Batch 30/64 loss: -0.14024025201797485
Batch 31/64 loss: -0.18584048748016357
Batch 32/64 loss: -0.11889135837554932
Batch 33/64 loss: -0.18250977993011475
Batch 34/64 loss: -0.154491126537323
Batch 35/64 loss: -0.13891541957855225
Batch 36/64 loss: -0.14734435081481934
Batch 37/64 loss: -0.1273822784423828
Batch 38/64 loss: -0.11145520210266113
Batch 39/64 loss: -0.12298727035522461
Batch 40/64 loss: -0.1653352975845337
Batch 41/64 loss: -0.1289244294166565
Batch 42/64 loss: -0.1632724404335022
Batch 43/64 loss: -0.15573835372924805
Batch 44/64 loss: -0.13323110342025757
Batch 45/64 loss: -0.14785230159759521
Batch 46/64 loss: -0.08207929134368896
Batch 47/64 loss: -0.14946609735488892
Batch 48/64 loss: -0.14759403467178345
Batch 49/64 loss: -0.1841328740119934
Batch 50/64 loss: -0.1251010298728943
Batch 51/64 loss: -0.13195496797561646
Batch 52/64 loss: -0.14979678392410278
Batch 53/64 loss: -0.13508856296539307
Batch 54/64 loss: -0.17208492755889893
Batch 55/64 loss: -0.15823256969451904
Batch 56/64 loss: -0.15860742330551147
Batch 57/64 loss: -0.12596678733825684
Batch 58/64 loss: -0.14874136447906494
Batch 59/64 loss: -0.15125322341918945
Batch 60/64 loss: -0.16826534271240234
Batch 61/64 loss: -0.1633930802345276
Batch 62/64 loss: -0.16859006881713867
Batch 63/64 loss: -0.11316752433776855
Batch 64/64 loss: -0.12382662296295166
Epoch 396  Train loss: -0.1507874839446124  Val loss: 0.06398300391292244
Epoch 397
-------------------------------
Batch 1/64 loss: -0.18914449214935303
Batch 2/64 loss: -0.13253772258758545
Batch 3/64 loss: -0.17684072256088257
Batch 4/64 loss: -0.15994596481323242
Batch 5/64 loss: -0.1552034616470337
Batch 6/64 loss: -0.14693504571914673
Batch 7/64 loss: -0.10523056983947754
Batch 8/64 loss: -0.1558430790901184
Batch 9/64 loss: -0.13713198900222778
Batch 10/64 loss: -0.14136898517608643
Batch 11/64 loss: -0.15864837169647217
Batch 12/64 loss: -0.1952257752418518
Batch 13/64 loss: -0.18587982654571533
Batch 14/64 loss: -0.14138245582580566
Batch 15/64 loss: -0.12893134355545044
Batch 16/64 loss: -0.18370115756988525
Batch 17/64 loss: -0.1636584997177124
Batch 18/64 loss: -0.19157928228378296
Batch 19/64 loss: -0.1462634801864624
Batch 20/64 loss: -0.163421630859375
Batch 21/64 loss: -0.14108335971832275
Batch 22/64 loss: -0.178117036819458
Batch 23/64 loss: -0.14230400323867798
Batch 24/64 loss: -0.12876850366592407
Batch 25/64 loss: -0.10942530632019043
Batch 26/64 loss: -0.16883301734924316
Batch 27/64 loss: -0.1589261293411255
Batch 28/64 loss: -0.15755873918533325
Batch 29/64 loss: -0.13954651355743408
Batch 30/64 loss: -0.14784592390060425
Batch 31/64 loss: -0.1452675461769104
Batch 32/64 loss: -0.14257901906967163
Batch 33/64 loss: -0.09321862459182739
Batch 34/64 loss: -0.16408038139343262
Batch 35/64 loss: -0.13823354244232178
Batch 36/64 loss: -0.15971875190734863
Batch 37/64 loss: -0.180039644241333
Batch 38/64 loss: -0.14729803800582886
Batch 39/64 loss: -0.11770898103713989
Batch 40/64 loss: -0.1352778673171997
Batch 41/64 loss: -0.13411033153533936
Batch 42/64 loss: -0.18259257078170776
Batch 43/64 loss: -0.15596234798431396
Batch 44/64 loss: -0.13451653718948364
Batch 45/64 loss: -0.14717376232147217
Batch 46/64 loss: -0.17108029127120972
Batch 47/64 loss: -0.1463310718536377
Batch 48/64 loss: -0.17618393898010254
Batch 49/64 loss: -0.13949745893478394
Batch 50/64 loss: -0.1609390377998352
Batch 51/64 loss: -0.131480872631073
Batch 52/64 loss: -0.11510366201400757
Batch 53/64 loss: -0.1406266689300537
Batch 54/64 loss: -0.1576477289199829
Batch 55/64 loss: -0.1610512137413025
Batch 56/64 loss: -0.13131988048553467
Batch 57/64 loss: -0.174191415309906
Batch 58/64 loss: -0.16494524478912354
Batch 59/64 loss: -0.17773795127868652
Batch 60/64 loss: -0.11362040042877197
Batch 61/64 loss: -0.12637048959732056
Batch 62/64 loss: -0.13767367601394653
Batch 63/64 loss: -0.1492316722869873
Batch 64/64 loss: -0.15117883682250977
Epoch 397  Train loss: -0.15054866098890118  Val loss: 0.07404000427305084
Epoch 398
-------------------------------
Batch 1/64 loss: -0.16678285598754883
Batch 2/64 loss: -0.09547996520996094
Batch 3/64 loss: -0.1864141821861267
Batch 4/64 loss: -0.16623032093048096
Batch 5/64 loss: -0.17253386974334717
Batch 6/64 loss: -0.1981717348098755
Batch 7/64 loss: -0.16292321681976318
Batch 8/64 loss: -0.11094439029693604
Batch 9/64 loss: -0.16671693325042725
Batch 10/64 loss: -0.17719638347625732
Batch 11/64 loss: -0.15975266695022583
Batch 12/64 loss: -0.17730939388275146
Batch 13/64 loss: -0.1873961091041565
Batch 14/64 loss: -0.17714321613311768
Batch 15/64 loss: -0.16036760807037354
Batch 16/64 loss: -0.18534010648727417
Batch 17/64 loss: -0.18649935722351074
Batch 18/64 loss: -0.15239816904067993
Batch 19/64 loss: -0.16764265298843384
Batch 20/64 loss: -0.1662880778312683
Batch 21/64 loss: -0.14352262020111084
Batch 22/64 loss: -0.150160551071167
Batch 23/64 loss: -0.14681404829025269
Batch 24/64 loss: -0.09711134433746338
Batch 25/64 loss: -0.12191838026046753
Batch 26/64 loss: -0.13608360290527344
Batch 27/64 loss: -0.1464659571647644
Batch 28/64 loss: -0.1678074598312378
Batch 29/64 loss: -0.16714727878570557
Batch 30/64 loss: -0.1441410779953003
Batch 31/64 loss: -0.17581439018249512
Batch 32/64 loss: -0.1515621542930603
Batch 33/64 loss: -0.19570308923721313
Batch 34/64 loss: -0.16632920503616333
Batch 35/64 loss: -0.12910157442092896
Batch 36/64 loss: -0.09893947839736938
Batch 37/64 loss: -0.14702486991882324
Batch 38/64 loss: -0.11145365238189697
Batch 39/64 loss: -0.13840168714523315
Batch 40/64 loss: -0.18890291452407837
Batch 41/64 loss: -0.17920279502868652
Batch 42/64 loss: -0.1714092493057251
Batch 43/64 loss: -0.17965638637542725
Batch 44/64 loss: -0.1724686622619629
Batch 45/64 loss: -0.145283043384552
Batch 46/64 loss: -0.15546971559524536
Batch 47/64 loss: -0.1371854543685913
Batch 48/64 loss: -0.1225203275680542
Batch 49/64 loss: -0.18970376253128052
Batch 50/64 loss: -0.13378143310546875
Batch 51/64 loss: -0.14860624074935913
Batch 52/64 loss: -0.18498766422271729
Batch 53/64 loss: -0.13957732915878296
Batch 54/64 loss: -0.15625429153442383
Batch 55/64 loss: -0.1411077380180359
Batch 56/64 loss: -0.14909452199935913
Batch 57/64 loss: -0.16258102655410767
Batch 58/64 loss: -0.10915440320968628
Batch 59/64 loss: -0.14291280508041382
Batch 60/64 loss: -0.1365959644317627
Batch 61/64 loss: -0.16682004928588867
Batch 62/64 loss: -0.15808284282684326
Batch 63/64 loss: -0.17088252305984497
Batch 64/64 loss: -0.1102069616317749
Epoch 398  Train loss: -0.15504203917933446  Val loss: 0.06364940224644244
Epoch 399
-------------------------------
Batch 1/64 loss: -0.1659156084060669
Batch 2/64 loss: -0.1886240839958191
Batch 3/64 loss: -0.12706118822097778
Batch 4/64 loss: -0.14686822891235352
Batch 5/64 loss: -0.15073156356811523
Batch 6/64 loss: -0.16962283849716187
Batch 7/64 loss: -0.16099047660827637
Batch 8/64 loss: -0.14749503135681152
Batch 9/64 loss: -0.16091430187225342
Batch 10/64 loss: -0.14469754695892334
Batch 11/64 loss: -0.1559637188911438
Batch 12/64 loss: -0.1681133508682251
Batch 13/64 loss: -0.14447861909866333
Batch 14/64 loss: -0.1290084719657898
Batch 15/64 loss: -0.17281723022460938
Batch 16/64 loss: -0.18428128957748413
Batch 17/64 loss: -0.1201372742652893
Batch 18/64 loss: -0.1817036271095276
Batch 19/64 loss: -0.14572173357009888
Batch 20/64 loss: -0.17545580863952637
Batch 21/64 loss: -0.17258614301681519
Batch 22/64 loss: -0.15710455179214478
Batch 23/64 loss: -0.14279860258102417
Batch 24/64 loss: -0.18381452560424805
Batch 25/64 loss: -0.17299580574035645
Batch 26/64 loss: -0.12979960441589355
Batch 27/64 loss: -0.14984411001205444
Batch 28/64 loss: -0.18326473236083984
Batch 29/64 loss: -0.1725674271583557
Batch 30/64 loss: -0.1349925398826599
Batch 31/64 loss: -0.16615891456604004
Batch 32/64 loss: -0.1815822720527649
Batch 33/64 loss: -0.16326230764389038
Batch 34/64 loss: -0.1739875078201294
Batch 35/64 loss: -0.15634840726852417
Batch 36/64 loss: -0.17340779304504395
Batch 37/64 loss: -0.1426791548728943
Batch 38/64 loss: -0.08872461318969727
Batch 39/64 loss: -0.09990942478179932
Batch 40/64 loss: -0.14522242546081543
Batch 41/64 loss: -0.11298346519470215
Batch 42/64 loss: -0.15152448415756226
Batch 43/64 loss: -0.12462306022644043
Batch 44/64 loss: -0.15046119689941406
Batch 45/64 loss: -0.15895015001296997
Batch 46/64 loss: -0.15369504690170288
Batch 47/64 loss: -0.16863811016082764
Batch 48/64 loss: -0.14791369438171387
Batch 49/64 loss: -0.13211005926132202
Batch 50/64 loss: -0.1523524522781372
Batch 51/64 loss: -0.17142587900161743
Batch 52/64 loss: -0.14145195484161377
Batch 53/64 loss: -0.15719354152679443
Batch 54/64 loss: -0.1395363211631775
Batch 55/64 loss: -0.1739756464958191
Batch 56/64 loss: -0.13304829597473145
Batch 57/64 loss: -0.15592581033706665
Batch 58/64 loss: -0.1582576036453247
Batch 59/64 loss: -0.14215481281280518
Batch 60/64 loss: -0.15532296895980835
Batch 61/64 loss: -0.10630732774734497
Batch 62/64 loss: -0.1549297571182251
Batch 63/64 loss: -0.1295771598815918
Batch 64/64 loss: -0.15752410888671875
Epoch 399  Train loss: -0.15250438802382527  Val loss: 0.0713029628766771
Epoch 400
-------------------------------
Batch 1/64 loss: -0.19092953205108643
Batch 2/64 loss: -0.16529220342636108
Batch 3/64 loss: -0.0988951325416565
Batch 4/64 loss: -0.15600228309631348
Batch 5/64 loss: -0.14602696895599365
Batch 6/64 loss: -0.15125048160552979
Batch 7/64 loss: -0.13947248458862305
Batch 8/64 loss: -0.17840033769607544
Batch 9/64 loss: -0.15092289447784424
Batch 10/64 loss: -0.1310473084449768
Batch 11/64 loss: -0.17049026489257812
Batch 12/64 loss: -0.1698564887046814
Batch 13/64 loss: -0.10348910093307495
Batch 14/64 loss: -0.18419772386550903
Batch 15/64 loss: -0.15444058179855347
Batch 16/64 loss: -0.1503923535346985
Batch 17/64 loss: -0.15732723474502563
Batch 18/64 loss: -0.1490863561630249
Batch 19/64 loss: -0.12211352586746216
Batch 20/64 loss: -0.13338929414749146
Batch 21/64 loss: -0.1708509922027588
Batch 22/64 loss: -0.18578988313674927
Batch 23/64 loss: -0.15506529808044434
Batch 24/64 loss: -0.1529017686843872
Batch 25/64 loss: -0.14222705364227295
Batch 26/64 loss: -0.14660221338272095
Batch 27/64 loss: -0.1577671766281128
Batch 28/64 loss: -0.15524601936340332
Batch 29/64 loss: -0.1413344144821167
Batch 30/64 loss: -0.1703280210494995
Batch 31/64 loss: -0.14253270626068115
Batch 32/64 loss: -0.1616402268409729
Batch 33/64 loss: -0.1804485321044922
Batch 34/64 loss: -0.12008202075958252
Batch 35/64 loss: -0.16579031944274902
Batch 36/64 loss: -0.15006059408187866
Batch 37/64 loss: -0.17966234683990479
Batch 38/64 loss: -0.1502397656440735
Batch 39/64 loss: -0.13662570714950562
Batch 40/64 loss: -0.11675584316253662
Batch 41/64 loss: -0.1492915153503418
Batch 42/64 loss: -0.16566145420074463
Batch 43/64 loss: -0.16035079956054688
Batch 44/64 loss: -0.167239248752594
Batch 45/64 loss: -0.12718158960342407
Batch 46/64 loss: -0.151262104511261
Batch 47/64 loss: -0.12912803888320923
Batch 48/64 loss: -0.1650087833404541
Batch 49/64 loss: -0.16640520095825195
Batch 50/64 loss: -0.08637797832489014
Batch 51/64 loss: -0.10577160120010376
Batch 52/64 loss: -0.1284746527671814
Batch 53/64 loss: -0.13409292697906494
Batch 54/64 loss: -0.1588653326034546
Batch 55/64 loss: -0.13839387893676758
Batch 56/64 loss: -0.13364523649215698
Batch 57/64 loss: -0.16002815961837769
Batch 58/64 loss: -0.11130231618881226
Batch 59/64 loss: -0.17411547899246216
Batch 60/64 loss: -0.1820961833000183
Batch 61/64 loss: -0.12797772884368896
Batch 62/64 loss: -0.16909760236740112
Batch 63/64 loss: -0.12851905822753906
Batch 64/64 loss: -0.17457139492034912
Epoch 400  Train loss: -0.14958727640264174  Val loss: 0.07346591732346315
Epoch 401
-------------------------------
Batch 1/64 loss: -0.13529402017593384
Batch 2/64 loss: -0.13881999254226685
Batch 3/64 loss: -0.1449558138847351
Batch 4/64 loss: -0.2087472677230835
Batch 5/64 loss: -0.1861438751220703
Batch 6/64 loss: -0.15252971649169922
Batch 7/64 loss: -0.17184466123580933
Batch 8/64 loss: -0.15131211280822754
Batch 9/64 loss: -0.1798616647720337
Batch 10/64 loss: -0.14380717277526855
Batch 11/64 loss: -0.15982073545455933
Batch 12/64 loss: -0.2095402181148529
Batch 13/64 loss: -0.14655888080596924
Batch 14/64 loss: -0.1653253436088562
Batch 15/64 loss: -0.1758856177330017
Batch 16/64 loss: -0.16277897357940674
Batch 17/64 loss: -0.20536136627197266
Batch 18/64 loss: -0.11512058973312378
Batch 19/64 loss: -0.17891085147857666
Batch 20/64 loss: -0.13394641876220703
Batch 21/64 loss: -0.17082393169403076
Batch 22/64 loss: -0.17700564861297607
Batch 23/64 loss: -0.16391557455062866
Batch 24/64 loss: -0.14417290687561035
Batch 25/64 loss: -0.18656259775161743
Batch 26/64 loss: -0.15689575672149658
Batch 27/64 loss: -0.1632670760154724
Batch 28/64 loss: -0.14298510551452637
Batch 29/64 loss: -0.1870395541191101
Batch 30/64 loss: -0.17306894063949585
Batch 31/64 loss: -0.16505616903305054
Batch 32/64 loss: -0.13628274202346802
Batch 33/64 loss: -0.17834794521331787
Batch 34/64 loss: -0.16672897338867188
Batch 35/64 loss: -0.16722744703292847
Batch 36/64 loss: -0.12279373407363892
Batch 37/64 loss: -0.15300261974334717
Batch 38/64 loss: -0.1766977310180664
Batch 39/64 loss: -0.14489126205444336
Batch 40/64 loss: -0.16652345657348633
Batch 41/64 loss: -0.15044844150543213
Batch 42/64 loss: -0.16123676300048828
Batch 43/64 loss: -0.15050631761550903
Batch 44/64 loss: -0.11018329858779907
Batch 45/64 loss: -0.1001858115196228
Batch 46/64 loss: -0.14415979385375977
Batch 47/64 loss: -0.14092212915420532
Batch 48/64 loss: -0.12208831310272217
Batch 49/64 loss: -0.14073246717453003
Batch 50/64 loss: -0.15521299839019775
Batch 51/64 loss: -0.14743679761886597
Batch 52/64 loss: -0.18106836080551147
Batch 53/64 loss: -0.13544988632202148
Batch 54/64 loss: -0.10969555377960205
Batch 55/64 loss: -0.15429174900054932
Batch 56/64 loss: -0.1256704330444336
Batch 57/64 loss: -0.1376258134841919
Batch 58/64 loss: -0.1845327615737915
Batch 59/64 loss: -0.10334336757659912
Batch 60/64 loss: -0.1330932378768921
Batch 61/64 loss: -0.13226425647735596
Batch 62/64 loss: -0.15199917554855347
Batch 63/64 loss: -0.14106285572052002
Batch 64/64 loss: -0.11883032321929932
Epoch 401  Train loss: -0.1539166712293438  Val loss: 0.06562621978550023
Epoch 402
-------------------------------
Batch 1/64 loss: -0.155467689037323
Batch 2/64 loss: -0.17084646224975586
Batch 3/64 loss: -0.14031225442886353
Batch 4/64 loss: -0.1882028579711914
Batch 5/64 loss: -0.15575826168060303
Batch 6/64 loss: -0.17530488967895508
Batch 7/64 loss: -0.12278705835342407
Batch 8/64 loss: -0.09740906953811646
Batch 9/64 loss: -0.13654875755310059
Batch 10/64 loss: -0.15917980670928955
Batch 11/64 loss: -0.18067103624343872
Batch 12/64 loss: -0.14819025993347168
Batch 13/64 loss: -0.13754701614379883
Batch 14/64 loss: -0.1421782374382019
Batch 15/64 loss: -0.1683875322341919
Batch 16/64 loss: -0.1380367875099182
Batch 17/64 loss: -0.1194993257522583
Batch 18/64 loss: -0.17549484968185425
Batch 19/64 loss: -0.12578076124191284
Batch 20/64 loss: -0.16507315635681152
Batch 21/64 loss: -0.17491191625595093
Batch 22/64 loss: -0.1272323727607727
Batch 23/64 loss: -0.15042060613632202
Batch 24/64 loss: -0.16797828674316406
Batch 25/64 loss: -0.17949050664901733
Batch 26/64 loss: -0.1569734811782837
Batch 27/64 loss: -0.1462225317955017
Batch 28/64 loss: -0.15406489372253418
Batch 29/64 loss: -0.12064135074615479
Batch 30/64 loss: -0.2027326226234436
Batch 31/64 loss: -0.1728808879852295
Batch 32/64 loss: -0.15591824054718018
Batch 33/64 loss: -0.13835793733596802
Batch 34/64 loss: -0.14827865362167358
Batch 35/64 loss: -0.18194079399108887
Batch 36/64 loss: -0.14234763383865356
Batch 37/64 loss: -0.15375864505767822
Batch 38/64 loss: -0.16524887084960938
Batch 39/64 loss: -0.19066768884658813
Batch 40/64 loss: -0.14873427152633667
Batch 41/64 loss: -0.1390417218208313
Batch 42/64 loss: -0.12492692470550537
Batch 43/64 loss: -0.16477590799331665
Batch 44/64 loss: -0.16198694705963135
Batch 45/64 loss: -0.13036996126174927
Batch 46/64 loss: -0.1405361294746399
Batch 47/64 loss: -0.1299392580986023
Batch 48/64 loss: -0.1580982804298401
Batch 49/64 loss: -0.1666538119316101
Batch 50/64 loss: -0.11188626289367676
Batch 51/64 loss: -0.10478740930557251
Batch 52/64 loss: -0.13961344957351685
Batch 53/64 loss: -0.1656011939048767
Batch 54/64 loss: -0.16190624237060547
Batch 55/64 loss: -0.17518377304077148
Batch 56/64 loss: -0.16534030437469482
Batch 57/64 loss: -0.149976909160614
Batch 58/64 loss: -0.12888532876968384
Batch 59/64 loss: -0.17235291004180908
Batch 60/64 loss: -0.17164134979248047
Batch 61/64 loss: -0.1773659586906433
Batch 62/64 loss: -0.16584992408752441
Batch 63/64 loss: -0.21179503202438354
Batch 64/64 loss: -0.09576112031936646
Epoch 402  Train loss: -0.1532833583214704  Val loss: 0.06804535438105003
Epoch 403
-------------------------------
Batch 1/64 loss: -0.15511423349380493
Batch 2/64 loss: -0.17863476276397705
Batch 3/64 loss: -0.19199371337890625
Batch 4/64 loss: -0.17459547519683838
Batch 5/64 loss: -0.16670101881027222
Batch 6/64 loss: -0.17348915338516235
Batch 7/64 loss: -0.18694889545440674
Batch 8/64 loss: -0.15105664730072021
Batch 9/64 loss: -0.21917027235031128
Batch 10/64 loss: -0.17501848936080933
Batch 11/64 loss: -0.13288557529449463
Batch 12/64 loss: -0.16216695308685303
Batch 13/64 loss: -0.1740856170654297
Batch 14/64 loss: -0.18847477436065674
Batch 15/64 loss: -0.19559824466705322
Batch 16/64 loss: -0.15023291110992432
Batch 17/64 loss: -0.15440165996551514
Batch 18/64 loss: -0.1776767373085022
Batch 19/64 loss: -0.15836071968078613
Batch 20/64 loss: -0.17210733890533447
Batch 21/64 loss: -0.12872588634490967
Batch 22/64 loss: -0.14492708444595337
Batch 23/64 loss: -0.10112249851226807
Batch 24/64 loss: -0.16100382804870605
Batch 25/64 loss: -0.17878103256225586
Batch 26/64 loss: -0.1560826301574707
Batch 27/64 loss: -0.13730591535568237
Batch 28/64 loss: -0.10746818780899048
Batch 29/64 loss: -0.16426849365234375
Batch 30/64 loss: -0.11145895719528198
Batch 31/64 loss: -0.1467166543006897
Batch 32/64 loss: -0.15754860639572144
Batch 33/64 loss: -0.17051976919174194
Batch 34/64 loss: -0.18505483865737915
Batch 35/64 loss: -0.1418914794921875
Batch 36/64 loss: -0.12661820650100708
Batch 37/64 loss: -0.16192090511322021
Batch 38/64 loss: -0.15368205308914185
Batch 39/64 loss: -0.12076056003570557
Batch 40/64 loss: -0.13026654720306396
Batch 41/64 loss: -0.16630518436431885
Batch 42/64 loss: -0.11872565746307373
Batch 43/64 loss: -0.15472453832626343
Batch 44/64 loss: -0.14444053173065186
Batch 45/64 loss: -0.1746664047241211
Batch 46/64 loss: -0.12992966175079346
Batch 47/64 loss: -0.16131150722503662
Batch 48/64 loss: -0.10312789678573608
Batch 49/64 loss: -0.11385786533355713
Batch 50/64 loss: -0.16412174701690674
Batch 51/64 loss: -0.15739226341247559
Batch 52/64 loss: -0.17368686199188232
Batch 53/64 loss: -0.158044695854187
Batch 54/64 loss: -0.17604416608810425
Batch 55/64 loss: -0.1465362310409546
Batch 56/64 loss: -0.14062786102294922
Batch 57/64 loss: -0.16237932443618774
Batch 58/64 loss: -0.1428135633468628
Batch 59/64 loss: -0.1481035351753235
Batch 60/64 loss: -0.1886172890663147
Batch 61/64 loss: -0.13871616125106812
Batch 62/64 loss: -0.13788414001464844
Batch 63/64 loss: -0.16296136379241943
Batch 64/64 loss: -0.19435608386993408
Epoch 403  Train loss: -0.1558529073116826  Val loss: 0.06563201556910354
Epoch 404
-------------------------------
Batch 1/64 loss: -0.13374221324920654
Batch 2/64 loss: -0.18200409412384033
Batch 3/64 loss: -0.15442055463790894
Batch 4/64 loss: -0.10475397109985352
Batch 5/64 loss: -0.17581582069396973
Batch 6/64 loss: -0.17541730403900146
Batch 7/64 loss: -0.17373931407928467
Batch 8/64 loss: -0.19715142250061035
Batch 9/64 loss: -0.14593201875686646
Batch 10/64 loss: -0.13051241636276245
Batch 11/64 loss: -0.1516190767288208
Batch 12/64 loss: -0.1628049612045288
Batch 13/64 loss: -0.16416096687316895
Batch 14/64 loss: -0.11037081480026245
Batch 15/64 loss: -0.1412096619606018
Batch 16/64 loss: -0.17720770835876465
Batch 17/64 loss: -0.18852871656417847
Batch 18/64 loss: -0.14994490146636963
Batch 19/64 loss: -0.1748003363609314
Batch 20/64 loss: -0.1661708950996399
Batch 21/64 loss: -0.1377219557762146
Batch 22/64 loss: -0.1504436731338501
Batch 23/64 loss: -0.12290900945663452
Batch 24/64 loss: -0.1601462960243225
Batch 25/64 loss: -0.16477876901626587
Batch 26/64 loss: -0.1355365514755249
Batch 27/64 loss: -0.14509224891662598
Batch 28/64 loss: -0.1550033688545227
Batch 29/64 loss: -0.10648322105407715
Batch 30/64 loss: -0.14505016803741455
Batch 31/64 loss: -0.1775866150856018
Batch 32/64 loss: -0.15292006731033325
Batch 33/64 loss: -0.17348319292068481
Batch 34/64 loss: -0.1643366813659668
Batch 35/64 loss: -0.11963379383087158
Batch 36/64 loss: -0.15051496028900146
Batch 37/64 loss: -0.15662914514541626
Batch 38/64 loss: -0.1717650294303894
Batch 39/64 loss: -0.15230506658554077
Batch 40/64 loss: -0.18160903453826904
Batch 41/64 loss: -0.1303759217262268
Batch 42/64 loss: -0.17382770776748657
Batch 43/64 loss: -0.15050190687179565
Batch 44/64 loss: -0.14186835289001465
Batch 45/64 loss: -0.16465526819229126
Batch 46/64 loss: -0.1367093324661255
Batch 47/64 loss: -0.16674083471298218
Batch 48/64 loss: -0.18395113945007324
Batch 49/64 loss: -0.17086994647979736
Batch 50/64 loss: -0.15835803747177124
Batch 51/64 loss: -0.16614675521850586
Batch 52/64 loss: -0.11942744255065918
Batch 53/64 loss: -0.12208616733551025
Batch 54/64 loss: -0.14175885915756226
Batch 55/64 loss: -0.1634434461593628
Batch 56/64 loss: -0.14539670944213867
Batch 57/64 loss: -0.16204118728637695
Batch 58/64 loss: -0.16021764278411865
Batch 59/64 loss: -0.15723437070846558
Batch 60/64 loss: -0.1854168176651001
Batch 61/64 loss: -0.15680021047592163
Batch 62/64 loss: -0.15804851055145264
Batch 63/64 loss: -0.12153416872024536
Batch 64/64 loss: -0.13931477069854736
Epoch 404  Train loss: -0.1541357306873097  Val loss: 0.06558290031767383
Epoch 405
-------------------------------
Batch 1/64 loss: -0.17058491706848145
Batch 2/64 loss: -0.1820446252822876
Batch 3/64 loss: -0.15621060132980347
Batch 4/64 loss: -0.15471452474594116
Batch 5/64 loss: -0.1738530397415161
Batch 6/64 loss: -0.17629438638687134
Batch 7/64 loss: -0.1297764778137207
Batch 8/64 loss: -0.15284645557403564
Batch 9/64 loss: -0.18394315242767334
Batch 10/64 loss: -0.14927172660827637
Batch 11/64 loss: -0.19864526391029358
Batch 12/64 loss: -0.12009924650192261
Batch 13/64 loss: -0.14322364330291748
Batch 14/64 loss: -0.1449609398841858
Batch 15/64 loss: -0.1706746220588684
Batch 16/64 loss: -0.15489566326141357
Batch 17/64 loss: -0.14317536354064941
Batch 18/64 loss: -0.18693983554840088
Batch 19/64 loss: -0.13624411821365356
Batch 20/64 loss: -0.16055822372436523
Batch 21/64 loss: -0.16107934713363647
Batch 22/64 loss: -0.14005041122436523
Batch 23/64 loss: -0.1141824722290039
Batch 24/64 loss: -0.11240655183792114
Batch 25/64 loss: -0.18680167198181152
Batch 26/64 loss: -0.12963581085205078
Batch 27/64 loss: -0.14532911777496338
Batch 28/64 loss: -0.20584553480148315
Batch 29/64 loss: -0.1508883833885193
Batch 30/64 loss: -0.12868350744247437
Batch 31/64 loss: -0.18776750564575195
Batch 32/64 loss: -0.11765903234481812
Batch 33/64 loss: -0.10958749055862427
Batch 34/64 loss: -0.1318545937538147
Batch 35/64 loss: -0.17079198360443115
Batch 36/64 loss: -0.14675432443618774
Batch 37/64 loss: -0.15243971347808838
Batch 38/64 loss: -0.1451748013496399
Batch 39/64 loss: -0.15179800987243652
Batch 40/64 loss: -0.16306650638580322
Batch 41/64 loss: -0.16761082410812378
Batch 42/64 loss: -0.1526750922203064
Batch 43/64 loss: -0.18021541833877563
Batch 44/64 loss: -0.13889431953430176
Batch 45/64 loss: -0.1473628282546997
Batch 46/64 loss: -0.17435353994369507
Batch 47/64 loss: -0.15426981449127197
Batch 48/64 loss: -0.14692366123199463
Batch 49/64 loss: -0.12072128057479858
Batch 50/64 loss: -0.10642504692077637
Batch 51/64 loss: -0.18685221672058105
Batch 52/64 loss: -0.16015666723251343
Batch 53/64 loss: -0.12841832637786865
Batch 54/64 loss: -0.19595226645469666
Batch 55/64 loss: -0.139801025390625
Batch 56/64 loss: -0.17776834964752197
Batch 57/64 loss: -0.16086000204086304
Batch 58/64 loss: -0.15048843622207642
Batch 59/64 loss: -0.15558791160583496
Batch 60/64 loss: -0.1855846643447876
Batch 61/64 loss: -0.14052730798721313
Batch 62/64 loss: -0.11123740673065186
Batch 63/64 loss: -0.13851600885391235
Batch 64/64 loss: -0.13712799549102783
Epoch 405  Train loss: -0.15317336484497668  Val loss: 0.06658954493368614
Epoch 406
-------------------------------
Batch 1/64 loss: -0.19592615962028503
Batch 2/64 loss: -0.15702301263809204
Batch 3/64 loss: -0.14832919836044312
Batch 4/64 loss: -0.14234161376953125
Batch 5/64 loss: -0.08584439754486084
Batch 6/64 loss: -0.15280288457870483
Batch 7/64 loss: -0.15977948904037476
Batch 8/64 loss: -0.1839916706085205
Batch 9/64 loss: -0.14982187747955322
Batch 10/64 loss: -0.14816713333129883
Batch 11/64 loss: -0.17557966709136963
Batch 12/64 loss: -0.1790081262588501
Batch 13/64 loss: -0.2015007734298706
Batch 14/64 loss: -0.1432468295097351
Batch 15/64 loss: -0.15789926052093506
Batch 16/64 loss: -0.11186009645462036
Batch 17/64 loss: -0.15530771017074585
Batch 18/64 loss: -0.13097703456878662
Batch 19/64 loss: -0.18255555629730225
Batch 20/64 loss: -0.16232234239578247
Batch 21/64 loss: -0.1576169729232788
Batch 22/64 loss: -0.1859360933303833
Batch 23/64 loss: -0.18175727128982544
Batch 24/64 loss: -0.12149745225906372
Batch 25/64 loss: -0.11729705333709717
Batch 26/64 loss: -0.1566116213798523
Batch 27/64 loss: -0.17577368021011353
Batch 28/64 loss: -0.16526544094085693
Batch 29/64 loss: -0.18122166395187378
Batch 30/64 loss: -0.15604275465011597
Batch 31/64 loss: -0.19299429655075073
Batch 32/64 loss: -0.11958163976669312
Batch 33/64 loss: -0.18522322177886963
Batch 34/64 loss: -0.1201629638671875
Batch 35/64 loss: -0.17125046253204346
Batch 36/64 loss: -0.17880189418792725
Batch 37/64 loss: -0.19141364097595215
Batch 38/64 loss: -0.1591792106628418
Batch 39/64 loss: -0.16100823879241943
Batch 40/64 loss: -0.12697511911392212
Batch 41/64 loss: -0.1883072853088379
Batch 42/64 loss: -0.16844820976257324
Batch 43/64 loss: -0.16868138313293457
Batch 44/64 loss: -0.1287798285484314
Batch 45/64 loss: -0.15712279081344604
Batch 46/64 loss: -0.1460421085357666
Batch 47/64 loss: -0.11151707172393799
Batch 48/64 loss: -0.15777617692947388
Batch 49/64 loss: -0.15782690048217773
Batch 50/64 loss: -0.16121506690979004
Batch 51/64 loss: -0.15859144926071167
Batch 52/64 loss: -0.158807635307312
Batch 53/64 loss: -0.17160165309906006
Batch 54/64 loss: -0.14276504516601562
Batch 55/64 loss: -0.15625041723251343
Batch 56/64 loss: -0.16704261302947998
Batch 57/64 loss: -0.17120856046676636
Batch 58/64 loss: -0.15062737464904785
Batch 59/64 loss: -0.16686290502548218
Batch 60/64 loss: -0.12002182006835938
Batch 61/64 loss: -0.1705251932144165
Batch 62/64 loss: -0.13429933786392212
Batch 63/64 loss: -0.1482861042022705
Batch 64/64 loss: -0.08457714319229126
Epoch 406  Train loss: -0.15617153434192432  Val loss: 0.07716179283214189
Epoch 407
-------------------------------
Batch 1/64 loss: -0.11930227279663086
Batch 2/64 loss: -0.17931479215621948
Batch 3/64 loss: -0.17562592029571533
Batch 4/64 loss: -0.1811264157295227
Batch 5/64 loss: -0.1451129913330078
Batch 6/64 loss: -0.1756378412246704
Batch 7/64 loss: -0.1457117795944214
Batch 8/64 loss: -0.16946947574615479
Batch 9/64 loss: -0.11878842115402222
Batch 10/64 loss: -0.1690535545349121
Batch 11/64 loss: -0.10799950361251831
Batch 12/64 loss: -0.16927462816238403
Batch 13/64 loss: -0.15122604370117188
Batch 14/64 loss: -0.16422927379608154
Batch 15/64 loss: -0.16417312622070312
Batch 16/64 loss: -0.16034573316574097
Batch 17/64 loss: -0.1614220142364502
Batch 18/64 loss: -0.16983747482299805
Batch 19/64 loss: -0.17442458868026733
Batch 20/64 loss: -0.16422003507614136
Batch 21/64 loss: -0.14448827505111694
Batch 22/64 loss: -0.15716737508773804
Batch 23/64 loss: -0.12240827083587646
Batch 24/64 loss: -0.11300206184387207
Batch 25/64 loss: -0.15672814846038818
Batch 26/64 loss: -0.17788583040237427
Batch 27/64 loss: -0.13083124160766602
Batch 28/64 loss: -0.15864145755767822
Batch 29/64 loss: -0.16187173128128052
Batch 30/64 loss: -0.14303570985794067
Batch 31/64 loss: -0.16542422771453857
Batch 32/64 loss: -0.15085291862487793
Batch 33/64 loss: -0.17038220167160034
Batch 34/64 loss: -0.09017038345336914
Batch 35/64 loss: -0.17135441303253174
Batch 36/64 loss: -0.14012491703033447
Batch 37/64 loss: -0.17295658588409424
Batch 38/64 loss: -0.11285078525543213
Batch 39/64 loss: -0.14937299489974976
Batch 40/64 loss: -0.16830778121948242
Batch 41/64 loss: -0.15108543634414673
Batch 42/64 loss: -0.15684866905212402
Batch 43/64 loss: -0.13673913478851318
Batch 44/64 loss: -0.13235139846801758
Batch 45/64 loss: -0.16117358207702637
Batch 46/64 loss: -0.16362649202346802
Batch 47/64 loss: -0.16793233156204224
Batch 48/64 loss: -0.16719073057174683
Batch 49/64 loss: -0.14832735061645508
Batch 50/64 loss: -0.16897475719451904
Batch 51/64 loss: -0.13040506839752197
Batch 52/64 loss: -0.15700161457061768
Batch 53/64 loss: -0.16941523551940918
Batch 54/64 loss: -0.13426756858825684
Batch 55/64 loss: -0.1430341601371765
Batch 56/64 loss: -0.13401758670806885
Batch 57/64 loss: -0.14773720502853394
Batch 58/64 loss: -0.16243427991867065
Batch 59/64 loss: -0.19281595945358276
Batch 60/64 loss: -0.1591464877128601
Batch 61/64 loss: -0.1934429407119751
Batch 62/64 loss: -0.15848952531814575
Batch 63/64 loss: -0.09257745742797852
Batch 64/64 loss: -0.14300256967544556
Epoch 407  Train loss: -0.15310494268641753  Val loss: 0.06958735214475914
Epoch 408
-------------------------------
Batch 1/64 loss: -0.16529130935668945
Batch 2/64 loss: -0.1520390510559082
Batch 3/64 loss: -0.1671208143234253
Batch 4/64 loss: -0.16536098718643188
Batch 5/64 loss: -0.16160917282104492
Batch 6/64 loss: -0.16944414377212524
Batch 7/64 loss: -0.1463574767112732
Batch 8/64 loss: -0.18078356981277466
Batch 9/64 loss: -0.17097020149230957
Batch 10/64 loss: -0.18740952014923096
Batch 11/64 loss: -0.12824690341949463
Batch 12/64 loss: -0.16848713159561157
Batch 13/64 loss: -0.17963802814483643
Batch 14/64 loss: -0.16908693313598633
Batch 15/64 loss: -0.17893850803375244
Batch 16/64 loss: -0.14817994832992554
Batch 17/64 loss: -0.1654025912284851
Batch 18/64 loss: -0.15048134326934814
Batch 19/64 loss: -0.15269255638122559
Batch 20/64 loss: -0.1580389142036438
Batch 21/64 loss: -0.11726069450378418
Batch 22/64 loss: -0.14801627397537231
Batch 23/64 loss: -0.14830869436264038
Batch 24/64 loss: -0.13627415895462036
Batch 25/64 loss: -0.14978283643722534
Batch 26/64 loss: -0.1210484504699707
Batch 27/64 loss: -0.16766750812530518
Batch 28/64 loss: -0.15802526473999023
Batch 29/64 loss: -0.176555335521698
Batch 30/64 loss: -0.11197137832641602
Batch 31/64 loss: -0.17184829711914062
Batch 32/64 loss: -0.13698810338974
Batch 33/64 loss: -0.1900273561477661
Batch 34/64 loss: -0.17360901832580566
Batch 35/64 loss: -0.17128562927246094
Batch 36/64 loss: -0.14401406049728394
Batch 37/64 loss: -0.12351304292678833
Batch 38/64 loss: -0.1576421856880188
Batch 39/64 loss: -0.16533058881759644
Batch 40/64 loss: -0.14232242107391357
Batch 41/64 loss: -0.1733158826828003
Batch 42/64 loss: -0.17511200904846191
Batch 43/64 loss: -0.1505996584892273
Batch 44/64 loss: -0.14984387159347534
Batch 45/64 loss: -0.11445820331573486
Batch 46/64 loss: -0.10630464553833008
Batch 47/64 loss: -0.15273380279541016
Batch 48/64 loss: -0.1310253143310547
Batch 49/64 loss: -0.1536208987236023
Batch 50/64 loss: -0.1605163812637329
Batch 51/64 loss: -0.16186463832855225
Batch 52/64 loss: -0.1546870470046997
Batch 53/64 loss: -0.13538604974746704
Batch 54/64 loss: -0.14953309297561646
Batch 55/64 loss: -0.11336958408355713
Batch 56/64 loss: -0.1414918303489685
Batch 57/64 loss: -0.14580947160720825
Batch 58/64 loss: -0.16711485385894775
Batch 59/64 loss: -0.17958879470825195
Batch 60/64 loss: -0.19569480419158936
Batch 61/64 loss: -0.08665049076080322
Batch 62/64 loss: -0.15666526556015015
Batch 63/64 loss: -0.18732982873916626
Batch 64/64 loss: -0.1172095537185669
Epoch 408  Train loss: -0.15384618020525165  Val loss: 0.0740545854945363
Epoch 409
-------------------------------
Batch 1/64 loss: -0.16949105262756348
Batch 2/64 loss: -0.15764117240905762
Batch 3/64 loss: -0.1708693504333496
Batch 4/64 loss: -0.15955686569213867
Batch 5/64 loss: -0.17445659637451172
Batch 6/64 loss: -0.1457807421684265
Batch 7/64 loss: -0.1975420117378235
Batch 8/64 loss: -0.1904878318309784
Batch 9/64 loss: -0.17240005731582642
Batch 10/64 loss: -0.15944910049438477
Batch 11/64 loss: -0.16245895624160767
Batch 12/64 loss: -0.16835469007492065
Batch 13/64 loss: -0.12670016288757324
Batch 14/64 loss: -0.15517473220825195
Batch 15/64 loss: -0.16536343097686768
Batch 16/64 loss: -0.13405704498291016
Batch 17/64 loss: -0.21432310342788696
Batch 18/64 loss: -0.17118597030639648
Batch 19/64 loss: -0.17101317644119263
Batch 20/64 loss: -0.14126431941986084
Batch 21/64 loss: -0.08874630928039551
Batch 22/64 loss: -0.18778157234191895
Batch 23/64 loss: -0.17336905002593994
Batch 24/64 loss: -0.14817357063293457
Batch 25/64 loss: -0.16871577501296997
Batch 26/64 loss: -0.173720121383667
Batch 27/64 loss: -0.1618589162826538
Batch 28/64 loss: -0.09599047899246216
Batch 29/64 loss: -0.14797675609588623
Batch 30/64 loss: -0.16286331415176392
Batch 31/64 loss: -0.12440675497055054
Batch 32/64 loss: -0.14345842599868774
Batch 33/64 loss: -0.1819974184036255
Batch 34/64 loss: -0.09186899662017822
Batch 35/64 loss: -0.16041123867034912
Batch 36/64 loss: -0.15328335762023926
Batch 37/64 loss: -0.14566963911056519
Batch 38/64 loss: -0.15964162349700928
Batch 39/64 loss: -0.1778736710548401
Batch 40/64 loss: -0.15851879119873047
Batch 41/64 loss: -0.1458834409713745
Batch 42/64 loss: -0.16648262739181519
Batch 43/64 loss: -0.10715246200561523
Batch 44/64 loss: -0.14886963367462158
Batch 45/64 loss: -0.17564111948013306
Batch 46/64 loss: -0.12778019905090332
Batch 47/64 loss: -0.1409403681755066
Batch 48/64 loss: -0.1608942151069641
Batch 49/64 loss: -0.20174309611320496
Batch 50/64 loss: -0.14267122745513916
Batch 51/64 loss: -0.11298435926437378
Batch 52/64 loss: -0.15011656284332275
Batch 53/64 loss: -0.1580548882484436
Batch 54/64 loss: -0.10942929983139038
Batch 55/64 loss: -0.163618803024292
Batch 56/64 loss: -0.1661773920059204
Batch 57/64 loss: -0.11857295036315918
Batch 58/64 loss: -0.15398895740509033
Batch 59/64 loss: -0.1489788293838501
Batch 60/64 loss: -0.14102625846862793
Batch 61/64 loss: -0.18601828813552856
Batch 62/64 loss: -0.13574236631393433
Batch 63/64 loss: -0.11408126354217529
Batch 64/64 loss: -0.1774311661720276
Epoch 409  Train loss: -0.1540991079573538  Val loss: 0.06785312612441807
Epoch 410
-------------------------------
Batch 1/64 loss: -0.16543138027191162
Batch 2/64 loss: -0.19381126761436462
Batch 3/64 loss: -0.13483619689941406
Batch 4/64 loss: -0.1478404402732849
Batch 5/64 loss: -0.15557026863098145
Batch 6/64 loss: -0.14108234643936157
Batch 7/64 loss: -0.1653367280960083
Batch 8/64 loss: -0.1567394733428955
Batch 9/64 loss: -0.19721996784210205
Batch 10/64 loss: -0.15822815895080566
Batch 11/64 loss: -0.15463536977767944
Batch 12/64 loss: -0.1494523286819458
Batch 13/64 loss: -0.16940003633499146
Batch 14/64 loss: -0.15682673454284668
Batch 15/64 loss: -0.16592711210250854
Batch 16/64 loss: -0.13059234619140625
Batch 17/64 loss: -0.16753637790679932
Batch 18/64 loss: -0.17858600616455078
Batch 19/64 loss: -0.16777092218399048
Batch 20/64 loss: -0.12661325931549072
Batch 21/64 loss: -0.16285890340805054
Batch 22/64 loss: -0.12279915809631348
Batch 23/64 loss: -0.1807146668434143
Batch 24/64 loss: -0.18784171342849731
Batch 25/64 loss: -0.15920543670654297
Batch 26/64 loss: -0.12984931468963623
Batch 27/64 loss: -0.17968404293060303
Batch 28/64 loss: -0.13535135984420776
Batch 29/64 loss: -0.19517138600349426
Batch 30/64 loss: -0.18921637535095215
Batch 31/64 loss: -0.12425076961517334
Batch 32/64 loss: -0.1645829677581787
Batch 33/64 loss: -0.14840131998062134
Batch 34/64 loss: -0.13724088668823242
Batch 35/64 loss: -0.1460743546485901
Batch 36/64 loss: -0.18786025047302246
Batch 37/64 loss: -0.1665329933166504
Batch 38/64 loss: -0.1602233648300171
Batch 39/64 loss: -0.1896572709083557
Batch 40/64 loss: -0.17738747596740723
Batch 41/64 loss: -0.15752649307250977
Batch 42/64 loss: -0.12420862913131714
Batch 43/64 loss: -0.1303180456161499
Batch 44/64 loss: -0.1524013876914978
Batch 45/64 loss: -0.14091598987579346
Batch 46/64 loss: -0.14279711246490479
Batch 47/64 loss: -0.1850430965423584
Batch 48/64 loss: -0.13357502222061157
Batch 49/64 loss: -0.1549091339111328
Batch 50/64 loss: -0.16732347011566162
Batch 51/64 loss: -0.16537892818450928
Batch 52/64 loss: -0.18347227573394775
Batch 53/64 loss: -0.17103636264801025
Batch 54/64 loss: -0.14486682415008545
Batch 55/64 loss: -0.15739184617996216
Batch 56/64 loss: -0.17356497049331665
Batch 57/64 loss: -0.13741493225097656
Batch 58/64 loss: -0.162395179271698
Batch 59/64 loss: -0.1487734317779541
Batch 60/64 loss: -0.15932291746139526
Batch 61/64 loss: -0.13732445240020752
Batch 62/64 loss: -0.1216202974319458
Batch 63/64 loss: -0.13945060968399048
Batch 64/64 loss: -0.18842512369155884
Epoch 410  Train loss: -0.15781476053537108  Val loss: 0.06455320611442487
Epoch 411
-------------------------------
Batch 1/64 loss: -0.13591182231903076
Batch 2/64 loss: -0.15474450588226318
Batch 3/64 loss: -0.15902745723724365
Batch 4/64 loss: -0.194252610206604
Batch 5/64 loss: -0.12676215171813965
Batch 6/64 loss: -0.17277902364730835
Batch 7/64 loss: -0.12155503034591675
Batch 8/64 loss: -0.15714812278747559
Batch 9/64 loss: -0.12736374139785767
Batch 10/64 loss: -0.16394191980361938
Batch 11/64 loss: -0.17325294017791748
Batch 12/64 loss: -0.15300887823104858
Batch 13/64 loss: -0.15205079317092896
Batch 14/64 loss: -0.1775193214416504
Batch 15/64 loss: -0.141917884349823
Batch 16/64 loss: -0.1785510778427124
Batch 17/64 loss: -0.1151081919670105
Batch 18/64 loss: -0.15229904651641846
Batch 19/64 loss: -0.176294207572937
Batch 20/64 loss: -0.18557381629943848
Batch 21/64 loss: -0.17487198114395142
Batch 22/64 loss: -0.16212892532348633
Batch 23/64 loss: -0.17593258619308472
Batch 24/64 loss: -0.16032296419143677
Batch 25/64 loss: -0.15311658382415771
Batch 26/64 loss: -0.16000759601593018
Batch 27/64 loss: -0.14471346139907837
Batch 28/64 loss: -0.12881863117218018
Batch 29/64 loss: -0.15698814392089844
Batch 30/64 loss: -0.16310209035873413
Batch 31/64 loss: -0.1403599977493286
Batch 32/64 loss: -0.16173183917999268
Batch 33/64 loss: -0.16389113664627075
Batch 34/64 loss: -0.14151078462600708
Batch 35/64 loss: -0.17740511894226074
Batch 36/64 loss: -0.17209982872009277
Batch 37/64 loss: -0.14573395252227783
Batch 38/64 loss: -0.15273666381835938
Batch 39/64 loss: -0.1430143117904663
Batch 40/64 loss: -0.11446696519851685
Batch 41/64 loss: -0.1315562129020691
Batch 42/64 loss: -0.13956856727600098
Batch 43/64 loss: -0.1649877429008484
Batch 44/64 loss: -0.15004777908325195
Batch 45/64 loss: -0.18302053213119507
Batch 46/64 loss: -0.16141730546951294
Batch 47/64 loss: -0.1610216498374939
Batch 48/64 loss: -0.165338397026062
Batch 49/64 loss: -0.15787619352340698
Batch 50/64 loss: -0.11572688817977905
Batch 51/64 loss: -0.15603351593017578
Batch 52/64 loss: -0.14816468954086304
Batch 53/64 loss: -0.1306295394897461
Batch 54/64 loss: -0.16446781158447266
Batch 55/64 loss: -0.17225080728530884
Batch 56/64 loss: -0.15659230947494507
Batch 57/64 loss: -0.16134780645370483
Batch 58/64 loss: -0.1676998734474182
Batch 59/64 loss: -0.13378679752349854
Batch 60/64 loss: -0.1624828577041626
Batch 61/64 loss: -0.1642284393310547
Batch 62/64 loss: -0.15554958581924438
Batch 63/64 loss: -0.15924781560897827
Batch 64/64 loss: -0.15726089477539062
Epoch 411  Train loss: -0.15518439049814262  Val loss: 0.06288347846453952
Epoch 412
-------------------------------
Batch 1/64 loss: -0.1678144335746765
Batch 2/64 loss: -0.1782107949256897
Batch 3/64 loss: -0.16220992803573608
Batch 4/64 loss: -0.19832000136375427
Batch 5/64 loss: -0.1444147825241089
Batch 6/64 loss: -0.1475173830986023
Batch 7/64 loss: -0.18100714683532715
Batch 8/64 loss: -0.16913318634033203
Batch 9/64 loss: -0.1597164273262024
Batch 10/64 loss: -0.15665656328201294
Batch 11/64 loss: -0.16731703281402588
Batch 12/64 loss: -0.14784759283065796
Batch 13/64 loss: -0.1369273066520691
Batch 14/64 loss: -0.13632619380950928
Batch 15/64 loss: -0.17917793989181519
Batch 16/64 loss: -0.1261419653892517
Batch 17/64 loss: -0.11045956611633301
Batch 18/64 loss: -0.1468064785003662
Batch 19/64 loss: -0.16971629858016968
Batch 20/64 loss: -0.18303698301315308
Batch 21/64 loss: -0.160286545753479
Batch 22/64 loss: -0.1822960376739502
Batch 23/64 loss: -0.16243994235992432
Batch 24/64 loss: -0.171409010887146
Batch 25/64 loss: -0.15635806322097778
Batch 26/64 loss: -0.15804153680801392
Batch 27/64 loss: -0.11469054222106934
Batch 28/64 loss: -0.1301824450492859
Batch 29/64 loss: -0.1776382327079773
Batch 30/64 loss: -0.20090162754058838
Batch 31/64 loss: -0.15465694665908813
Batch 32/64 loss: -0.15639132261276245
Batch 33/64 loss: -0.1463204026222229
Batch 34/64 loss: -0.16656547784805298
Batch 35/64 loss: -0.1463755965232849
Batch 36/64 loss: -0.15515130758285522
Batch 37/64 loss: -0.1666586995124817
Batch 38/64 loss: -0.16053783893585205
Batch 39/64 loss: -0.17808985710144043
Batch 40/64 loss: -0.18088525533676147
Batch 41/64 loss: -0.1521570086479187
Batch 42/64 loss: -0.14891475439071655
Batch 43/64 loss: -0.18804943561553955
Batch 44/64 loss: -0.16646742820739746
Batch 45/64 loss: -0.13487321138381958
Batch 46/64 loss: -0.17598021030426025
Batch 47/64 loss: -0.16324502229690552
Batch 48/64 loss: -0.15264886617660522
Batch 49/64 loss: -0.18302661180496216
Batch 50/64 loss: -0.1423027515411377
Batch 51/64 loss: -0.13848590850830078
Batch 52/64 loss: -0.16552025079727173
Batch 53/64 loss: -0.1347435712814331
Batch 54/64 loss: -0.16501063108444214
Batch 55/64 loss: -0.1761426329612732
Batch 56/64 loss: -0.15799659490585327
Batch 57/64 loss: -0.15270495414733887
Batch 58/64 loss: -0.1698511242866516
Batch 59/64 loss: -0.14146316051483154
Batch 60/64 loss: -0.12639814615249634
Batch 61/64 loss: -0.19429686665534973
Batch 62/64 loss: -0.13894063234329224
Batch 63/64 loss: -0.15705782175064087
Batch 64/64 loss: -0.1794666051864624
Epoch 412  Train loss: -0.15930215283936144  Val loss: 0.06690958357348885
Epoch 413
-------------------------------
Batch 1/64 loss: -0.1785106658935547
Batch 2/64 loss: -0.1707499623298645
Batch 3/64 loss: -0.1582074761390686
Batch 4/64 loss: -0.11567938327789307
Batch 5/64 loss: -0.17683321237564087
Batch 6/64 loss: -0.1629701852798462
Batch 7/64 loss: -0.18670779466629028
Batch 8/64 loss: -0.14465993642807007
Batch 9/64 loss: -0.14642488956451416
Batch 10/64 loss: -0.16642743349075317
Batch 11/64 loss: -0.17701804637908936
Batch 12/64 loss: -0.19120091199874878
Batch 13/64 loss: -0.1733492612838745
Batch 14/64 loss: -0.16500884294509888
Batch 15/64 loss: -0.1408785581588745
Batch 16/64 loss: -0.17736297845840454
Batch 17/64 loss: -0.17334014177322388
Batch 18/64 loss: -0.1765846610069275
Batch 19/64 loss: -0.18287688493728638
Batch 20/64 loss: -0.15380501747131348
Batch 21/64 loss: -0.1550600528717041
Batch 22/64 loss: -0.1418919563293457
Batch 23/64 loss: -0.15033310651779175
Batch 24/64 loss: -0.14403218030929565
Batch 25/64 loss: -0.17511165142059326
Batch 26/64 loss: -0.17501187324523926
Batch 27/64 loss: -0.1749887466430664
Batch 28/64 loss: -0.16101598739624023
Batch 29/64 loss: -0.18772637844085693
Batch 30/64 loss: -0.1568506956100464
Batch 31/64 loss: -0.15178537368774414
Batch 32/64 loss: -0.17156058549880981
Batch 33/64 loss: -0.14352726936340332
Batch 34/64 loss: -0.1682450771331787
Batch 35/64 loss: -0.1544535756111145
Batch 36/64 loss: -0.1892108917236328
Batch 37/64 loss: -0.1801038384437561
Batch 38/64 loss: -0.15348589420318604
Batch 39/64 loss: -0.1452624797821045
Batch 40/64 loss: -0.1315169334411621
Batch 41/64 loss: -0.14396584033966064
Batch 42/64 loss: -0.18577152490615845
Batch 43/64 loss: -0.15871161222457886
Batch 44/64 loss: -0.10384398698806763
Batch 45/64 loss: -0.1684386134147644
Batch 46/64 loss: -0.14191627502441406
Batch 47/64 loss: -0.1260567307472229
Batch 48/64 loss: -0.16903537511825562
Batch 49/64 loss: -0.15954458713531494
Batch 50/64 loss: -0.12193351984024048
Batch 51/64 loss: -0.1357836127281189
Batch 52/64 loss: -0.1650751829147339
Batch 53/64 loss: -0.15863579511642456
Batch 54/64 loss: -0.16981148719787598
Batch 55/64 loss: -0.13140296936035156
Batch 56/64 loss: -0.13431429862976074
Batch 57/64 loss: -0.10565763711929321
Batch 58/64 loss: -0.15877503156661987
Batch 59/64 loss: -0.14848393201828003
Batch 60/64 loss: -0.1676616668701172
Batch 61/64 loss: -0.11624264717102051
Batch 62/64 loss: -0.16220754384994507
Batch 63/64 loss: -0.1570751667022705
Batch 64/64 loss: -0.17659282684326172
Epoch 413  Train loss: -0.15768722272386737  Val loss: 0.06833760967778996
Epoch 414
-------------------------------
Batch 1/64 loss: -0.18414002656936646
Batch 2/64 loss: -0.1714521050453186
Batch 3/64 loss: -0.13055568933486938
Batch 4/64 loss: -0.183424174785614
Batch 5/64 loss: -0.13869529962539673
Batch 6/64 loss: -0.1862773895263672
Batch 7/64 loss: -0.17949867248535156
Batch 8/64 loss: -0.15970194339752197
Batch 9/64 loss: -0.13377434015274048
Batch 10/64 loss: -0.11261820793151855
Batch 11/64 loss: -0.1675870418548584
Batch 12/64 loss: -0.15350598096847534
Batch 13/64 loss: -0.1776452660560608
Batch 14/64 loss: -0.15235662460327148
Batch 15/64 loss: -0.14454197883605957
Batch 16/64 loss: -0.17077869176864624
Batch 17/64 loss: -0.1529039740562439
Batch 18/64 loss: -0.11680948734283447
Batch 19/64 loss: -0.11006104946136475
Batch 20/64 loss: -0.16820478439331055
Batch 21/64 loss: -0.1501767635345459
Batch 22/64 loss: -0.10255813598632812
Batch 23/64 loss: -0.15120071172714233
Batch 24/64 loss: -0.17135530710220337
Batch 25/64 loss: -0.152288019657135
Batch 26/64 loss: -0.13213080167770386
Batch 27/64 loss: -0.15019166469573975
Batch 28/64 loss: -0.1476738452911377
Batch 29/64 loss: -0.1652809977531433
Batch 30/64 loss: -0.19068819284439087
Batch 31/64 loss: -0.1383945345878601
Batch 32/64 loss: -0.14684730768203735
Batch 33/64 loss: -0.15016400814056396
Batch 34/64 loss: -0.11885976791381836
Batch 35/64 loss: -0.12730062007904053
Batch 36/64 loss: -0.16243410110473633
Batch 37/64 loss: -0.13727664947509766
Batch 38/64 loss: -0.14928215742111206
Batch 39/64 loss: -0.14221614599227905
Batch 40/64 loss: -0.18258506059646606
Batch 41/64 loss: -0.17364805936813354
Batch 42/64 loss: -0.12086665630340576
Batch 43/64 loss: -0.16472631692886353
Batch 44/64 loss: -0.1413622498512268
Batch 45/64 loss: -0.12716341018676758
Batch 46/64 loss: -0.177392840385437
Batch 47/64 loss: -0.1809612512588501
Batch 48/64 loss: -0.119098961353302
Batch 49/64 loss: -0.1432291865348816
Batch 50/64 loss: -0.19153237342834473
Batch 51/64 loss: -0.10931378602981567
Batch 52/64 loss: -0.1731792688369751
Batch 53/64 loss: -0.12934726476669312
Batch 54/64 loss: -0.17196506261825562
Batch 55/64 loss: -0.18114453554153442
Batch 56/64 loss: -0.13744819164276123
Batch 57/64 loss: -0.18181860446929932
Batch 58/64 loss: -0.1413792371749878
Batch 59/64 loss: -0.15345138311386108
Batch 60/64 loss: -0.19068557024002075
Batch 61/64 loss: -0.16306638717651367
Batch 62/64 loss: -0.15468567609786987
Batch 63/64 loss: -0.16907578706741333
Batch 64/64 loss: -0.12341153621673584
Epoch 414  Train loss: -0.1529809919058108  Val loss: 0.06244541566396497
Epoch 415
-------------------------------
Batch 1/64 loss: -0.1866816282272339
Batch 2/64 loss: -0.14894509315490723
Batch 3/64 loss: -0.18104243278503418
Batch 4/64 loss: -0.11779546737670898
Batch 5/64 loss: -0.1847010850906372
Batch 6/64 loss: -0.159823477268219
Batch 7/64 loss: -0.1578386425971985
Batch 8/64 loss: -0.14141160249710083
Batch 9/64 loss: -0.16392487287521362
Batch 10/64 loss: -0.11672872304916382
Batch 11/64 loss: -0.15485131740570068
Batch 12/64 loss: -0.1665627360343933
Batch 13/64 loss: -0.1634429693222046
Batch 14/64 loss: -0.19639495015144348
Batch 15/64 loss: -0.14597707986831665
Batch 16/64 loss: -0.13586068153381348
Batch 17/64 loss: -0.19168591499328613
Batch 18/64 loss: -0.14563286304473877
Batch 19/64 loss: -0.20211654901504517
Batch 20/64 loss: -0.14392369985580444
Batch 21/64 loss: -0.1414662003517151
Batch 22/64 loss: -0.16838061809539795
Batch 23/64 loss: -0.14107739925384521
Batch 24/64 loss: -0.16416078805923462
Batch 25/64 loss: -0.15839701890945435
Batch 26/64 loss: -0.16348302364349365
Batch 27/64 loss: -0.16492438316345215
Batch 28/64 loss: -0.17088133096694946
Batch 29/64 loss: -0.15138542652130127
Batch 30/64 loss: -0.16997820138931274
Batch 31/64 loss: -0.17197000980377197
Batch 32/64 loss: -0.16005593538284302
Batch 33/64 loss: -0.1503639817237854
Batch 34/64 loss: -0.11559730768203735
Batch 35/64 loss: -0.13777858018875122
Batch 36/64 loss: -0.18632644414901733
Batch 37/64 loss: -0.1743927001953125
Batch 38/64 loss: -0.14416950941085815
Batch 39/64 loss: -0.1486417055130005
Batch 40/64 loss: -0.12102395296096802
Batch 41/64 loss: -0.17803609371185303
Batch 42/64 loss: -0.1562061905860901
Batch 43/64 loss: -0.15297818183898926
Batch 44/64 loss: -0.169300377368927
Batch 45/64 loss: -0.12147015333175659
Batch 46/64 loss: -0.14124810695648193
Batch 47/64 loss: -0.1835176944732666
Batch 48/64 loss: -0.161260724067688
Batch 49/64 loss: -0.162847638130188
Batch 50/64 loss: -0.10100531578063965
Batch 51/64 loss: -0.17603003978729248
Batch 52/64 loss: -0.18611574172973633
Batch 53/64 loss: -0.1642543077468872
Batch 54/64 loss: -0.20713704824447632
Batch 55/64 loss: -0.1638549566268921
Batch 56/64 loss: -0.15102791786193848
Batch 57/64 loss: -0.1955774426460266
Batch 58/64 loss: -0.13057202100753784
Batch 59/64 loss: -0.1462113857269287
Batch 60/64 loss: -0.1689935326576233
Batch 61/64 loss: -0.17836987972259521
Batch 62/64 loss: -0.1310344934463501
Batch 63/64 loss: -0.14394599199295044
Batch 64/64 loss: -0.1775519847869873
Epoch 415  Train loss: -0.15865028278500426  Val loss: 0.06486189160560005
Epoch 416
-------------------------------
Batch 1/64 loss: -0.18344807624816895
Batch 2/64 loss: -0.16418659687042236
Batch 3/64 loss: -0.1871887445449829
Batch 4/64 loss: -0.15967100858688354
Batch 5/64 loss: -0.15385282039642334
Batch 6/64 loss: -0.18159866333007812
Batch 7/64 loss: -0.2005005180835724
Batch 8/64 loss: -0.1800675392150879
Batch 9/64 loss: -0.18000411987304688
Batch 10/64 loss: -0.1516937017440796
Batch 11/64 loss: -0.1505448818206787
Batch 12/64 loss: -0.14494651556015015
Batch 13/64 loss: -0.17481452226638794
Batch 14/64 loss: -0.15864503383636475
Batch 15/64 loss: -0.18012535572052002
Batch 16/64 loss: -0.1906580924987793
Batch 17/64 loss: -0.16525429487228394
Batch 18/64 loss: -0.17685377597808838
Batch 19/64 loss: -0.1514742374420166
Batch 20/64 loss: -0.13694137334823608
Batch 21/64 loss: -0.1322656273841858
Batch 22/64 loss: -0.1343393325805664
Batch 23/64 loss: -0.18372607231140137
Batch 24/64 loss: -0.11745864152908325
Batch 25/64 loss: -0.15096920728683472
Batch 26/64 loss: -0.1557024121284485
Batch 27/64 loss: -0.19452005624771118
Batch 28/64 loss: -0.1361403465270996
Batch 29/64 loss: -0.12691456079483032
Batch 30/64 loss: -0.16381311416625977
Batch 31/64 loss: -0.15510880947113037
Batch 32/64 loss: -0.15023750066757202
Batch 33/64 loss: -0.16529637575149536
Batch 34/64 loss: -0.1483231782913208
Batch 35/64 loss: -0.17138826847076416
Batch 36/64 loss: -0.1713079810142517
Batch 37/64 loss: -0.17221033573150635
Batch 38/64 loss: -0.1450871229171753
Batch 39/64 loss: -0.15433627367019653
Batch 40/64 loss: -0.14219236373901367
Batch 41/64 loss: -0.14252638816833496
Batch 42/64 loss: -0.09944027662277222
Batch 43/64 loss: -0.17613756656646729
Batch 44/64 loss: -0.14045345783233643
Batch 45/64 loss: -0.12492036819458008
Batch 46/64 loss: -0.1658111810684204
Batch 47/64 loss: -0.1530793309211731
Batch 48/64 loss: -0.16539889574050903
Batch 49/64 loss: -0.164686918258667
Batch 50/64 loss: -0.17260241508483887
Batch 51/64 loss: -0.16601097583770752
Batch 52/64 loss: -0.14410406351089478
Batch 53/64 loss: -0.19514456391334534
Batch 54/64 loss: -0.1365097165107727
Batch 55/64 loss: -0.13924258947372437
Batch 56/64 loss: -0.18690061569213867
Batch 57/64 loss: -0.15139955282211304
Batch 58/64 loss: -0.148143470287323
Batch 59/64 loss: -0.19329488277435303
Batch 60/64 loss: -0.12617361545562744
Batch 61/64 loss: -0.1197618842124939
Batch 62/64 loss: -0.15310657024383545
Batch 63/64 loss: -0.1754385232925415
Batch 64/64 loss: -0.16078519821166992
Epoch 416  Train loss: -0.15850484978918936  Val loss: 0.0630568480983223
Epoch 417
-------------------------------
Batch 1/64 loss: -0.16184085607528687
Batch 2/64 loss: -0.1511598825454712
Batch 3/64 loss: -0.15365934371948242
Batch 4/64 loss: -0.14908474683761597
Batch 5/64 loss: -0.11498081684112549
Batch 6/64 loss: -0.16983377933502197
Batch 7/64 loss: -0.1259198784828186
Batch 8/64 loss: -0.13848495483398438
Batch 9/64 loss: -0.18713849782943726
Batch 10/64 loss: -0.14649921655654907
Batch 11/64 loss: -0.1389089822769165
Batch 12/64 loss: -0.11675393581390381
Batch 13/64 loss: -0.17781037092208862
Batch 14/64 loss: -0.16112852096557617
Batch 15/64 loss: -0.14961588382720947
Batch 16/64 loss: -0.13280999660491943
Batch 17/64 loss: -0.1526252031326294
Batch 18/64 loss: -0.1646488904953003
Batch 19/64 loss: -0.1660286784172058
Batch 20/64 loss: -0.17468208074569702
Batch 21/64 loss: -0.13673996925354004
Batch 22/64 loss: -0.1570979356765747
Batch 23/64 loss: -0.16024959087371826
Batch 24/64 loss: -0.19915342330932617
Batch 25/64 loss: -0.12990248203277588
Batch 26/64 loss: -0.19184041023254395
Batch 27/64 loss: -0.1336507797241211
Batch 28/64 loss: -0.15603667497634888
Batch 29/64 loss: -0.17511314153671265
Batch 30/64 loss: -0.16466158628463745
Batch 31/64 loss: -0.17596572637557983
Batch 32/64 loss: -0.1566556692123413
Batch 33/64 loss: -0.15841007232666016
Batch 34/64 loss: -0.14108353853225708
Batch 35/64 loss: -0.17438644170761108
Batch 36/64 loss: -0.1754089593887329
Batch 37/64 loss: -0.15601897239685059
Batch 38/64 loss: -0.14663243293762207
Batch 39/64 loss: -0.18651533126831055
Batch 40/64 loss: -0.15785038471221924
Batch 41/64 loss: -0.1802229881286621
Batch 42/64 loss: -0.15076696872711182
Batch 43/64 loss: -0.14985406398773193
Batch 44/64 loss: -0.16486942768096924
Batch 45/64 loss: -0.14890170097351074
Batch 46/64 loss: -0.14383161067962646
Batch 47/64 loss: -0.1460425853729248
Batch 48/64 loss: -0.12960082292556763
Batch 49/64 loss: -0.14777660369873047
Batch 50/64 loss: -0.1722944974899292
Batch 51/64 loss: -0.16891908645629883
Batch 52/64 loss: -0.1593700647354126
Batch 53/64 loss: -0.16933763027191162
Batch 54/64 loss: -0.15568244457244873
Batch 55/64 loss: -0.16126489639282227
Batch 56/64 loss: -0.18565326929092407
Batch 57/64 loss: -0.16055816411972046
Batch 58/64 loss: -0.1750556230545044
Batch 59/64 loss: -0.1386004090309143
Batch 60/64 loss: -0.13884806632995605
Batch 61/64 loss: -0.15676510334014893
Batch 62/64 loss: -0.15262019634246826
Batch 63/64 loss: -0.1757814884185791
Batch 64/64 loss: -0.1355181336402893
Epoch 417  Train loss: -0.15688226433361277  Val loss: 0.06451268335388288
Epoch 418
-------------------------------
Batch 1/64 loss: -0.15776866674423218
Batch 2/64 loss: -0.17802637815475464
Batch 3/64 loss: -0.1754286289215088
Batch 4/64 loss: -0.1228799819946289
Batch 5/64 loss: -0.15201836824417114
Batch 6/64 loss: -0.1338537335395813
Batch 7/64 loss: -0.1822061538696289
Batch 8/64 loss: -0.17955565452575684
Batch 9/64 loss: -0.14244896173477173
Batch 10/64 loss: -0.1895846426486969
Batch 11/64 loss: -0.17115700244903564
Batch 12/64 loss: -0.128104567527771
Batch 13/64 loss: -0.1593363881111145
Batch 14/64 loss: -0.1518762707710266
Batch 15/64 loss: -0.1792057752609253
Batch 16/64 loss: -0.13626646995544434
Batch 17/64 loss: -0.11679869890213013
Batch 18/64 loss: -0.12102854251861572
Batch 19/64 loss: -0.17696291208267212
Batch 20/64 loss: -0.1534317135810852
Batch 21/64 loss: -0.12625980377197266
Batch 22/64 loss: -0.1510838270187378
Batch 23/64 loss: -0.19274389743804932
Batch 24/64 loss: -0.18516021966934204
Batch 25/64 loss: -0.1638292670249939
Batch 26/64 loss: -0.1539069414138794
Batch 27/64 loss: -0.16886180639266968
Batch 28/64 loss: -0.15665525197982788
Batch 29/64 loss: -0.17427527904510498
Batch 30/64 loss: -0.1463795304298401
Batch 31/64 loss: -0.17185264825820923
Batch 32/64 loss: -0.1579616665840149
Batch 33/64 loss: -0.14164215326309204
Batch 34/64 loss: -0.1657959222793579
Batch 35/64 loss: -0.12453603744506836
Batch 36/64 loss: -0.1578095555305481
Batch 37/64 loss: -0.2077656090259552
Batch 38/64 loss: -0.1624964475631714
Batch 39/64 loss: -0.17395418882369995
Batch 40/64 loss: -0.13388997316360474
Batch 41/64 loss: -0.19390830397605896
Batch 42/64 loss: -0.18764805793762207
Batch 43/64 loss: -0.18649059534072876
Batch 44/64 loss: -0.16730183362960815
Batch 45/64 loss: -0.12236791849136353
Batch 46/64 loss: -0.1704896092414856
Batch 47/64 loss: -0.13138151168823242
Batch 48/64 loss: -0.18813276290893555
Batch 49/64 loss: -0.15944528579711914
Batch 50/64 loss: -0.15805590152740479
Batch 51/64 loss: -0.1665779948234558
Batch 52/64 loss: -0.1325175166130066
Batch 53/64 loss: -0.17433691024780273
Batch 54/64 loss: -0.16247373819351196
Batch 55/64 loss: -0.1457972526550293
Batch 56/64 loss: -0.16972899436950684
Batch 57/64 loss: -0.17618149518966675
Batch 58/64 loss: -0.10977476835250854
Batch 59/64 loss: -0.20132887363433838
Batch 60/64 loss: -0.16167014837265015
Batch 61/64 loss: -0.160000741481781
Batch 62/64 loss: -0.14771735668182373
Batch 63/64 loss: -0.18114185333251953
Batch 64/64 loss: -0.08764445781707764
Epoch 418  Train loss: -0.1591372910667868  Val loss: 0.07099560097730447
Epoch 419
-------------------------------
Batch 1/64 loss: -0.14708614349365234
Batch 2/64 loss: -0.17307066917419434
Batch 3/64 loss: -0.13830745220184326
Batch 4/64 loss: -0.162176251411438
Batch 5/64 loss: -0.12478262186050415
Batch 6/64 loss: -0.18467581272125244
Batch 7/64 loss: -0.14480262994766235
Batch 8/64 loss: -0.19266259670257568
Batch 9/64 loss: -0.20814388990402222
Batch 10/64 loss: -0.15656161308288574
Batch 11/64 loss: -0.16246700286865234
Batch 12/64 loss: -0.15398114919662476
Batch 13/64 loss: -0.19594013690948486
Batch 14/64 loss: -0.17035365104675293
Batch 15/64 loss: -0.19187533855438232
Batch 16/64 loss: -0.1602236032485962
Batch 17/64 loss: -0.14181888103485107
Batch 18/64 loss: -0.16952764987945557
Batch 19/64 loss: -0.171758770942688
Batch 20/64 loss: -0.17169344425201416
Batch 21/64 loss: -0.1662752628326416
Batch 22/64 loss: -0.17108023166656494
Batch 23/64 loss: -0.15188932418823242
Batch 24/64 loss: -0.16685032844543457
Batch 25/64 loss: -0.12049096822738647
Batch 26/64 loss: -0.16894745826721191
Batch 27/64 loss: -0.17983782291412354
Batch 28/64 loss: -0.16719788312911987
Batch 29/64 loss: -0.1751558780670166
Batch 30/64 loss: -0.16635161638259888
Batch 31/64 loss: -0.19427847862243652
Batch 32/64 loss: -0.16857564449310303
Batch 33/64 loss: -0.1975685954093933
Batch 34/64 loss: -0.17942774295806885
Batch 35/64 loss: -0.15870416164398193
Batch 36/64 loss: -0.18474739789962769
Batch 37/64 loss: -0.1730005145072937
Batch 38/64 loss: -0.13536834716796875
Batch 39/64 loss: -0.12257373332977295
Batch 40/64 loss: -0.14745396375656128
Batch 41/64 loss: -0.1563132405281067
Batch 42/64 loss: -0.16539549827575684
Batch 43/64 loss: -0.16301530599594116
Batch 44/64 loss: -0.09184330701828003
Batch 45/64 loss: -0.13925158977508545
Batch 46/64 loss: -0.16953003406524658
Batch 47/64 loss: -0.15860527753829956
Batch 48/64 loss: -0.16998159885406494
Batch 49/64 loss: -0.1460055112838745
Batch 50/64 loss: -0.13883399963378906
Batch 51/64 loss: -0.18391156196594238
Batch 52/64 loss: -0.147058367729187
Batch 53/64 loss: -0.1723266839981079
Batch 54/64 loss: -0.1513357162475586
Batch 55/64 loss: -0.17565083503723145
Batch 56/64 loss: -0.17349326610565186
Batch 57/64 loss: -0.15868639945983887
Batch 58/64 loss: -0.15324640274047852
Batch 59/64 loss: -0.16435492038726807
Batch 60/64 loss: -0.16881388425827026
Batch 61/64 loss: -0.14875489473342896
Batch 62/64 loss: -0.151624858379364
Batch 63/64 loss: -0.1454017162322998
Batch 64/64 loss: -0.1513535976409912
Epoch 419  Train loss: -0.16195505460103352  Val loss: 0.06458632192251199
Epoch 420
-------------------------------
Batch 1/64 loss: -0.17416232824325562
Batch 2/64 loss: -0.16300374269485474
Batch 3/64 loss: -0.15911519527435303
Batch 4/64 loss: -0.16095376014709473
Batch 5/64 loss: -0.15319812297821045
Batch 6/64 loss: -0.17268896102905273
Batch 7/64 loss: -0.16651612520217896
Batch 8/64 loss: -0.18722963333129883
Batch 9/64 loss: -0.18900293111801147
Batch 10/64 loss: -0.16873234510421753
Batch 11/64 loss: -0.13847887516021729
Batch 12/64 loss: -0.15358710289001465
Batch 13/64 loss: -0.12099981307983398
Batch 14/64 loss: -0.16034376621246338
Batch 15/64 loss: -0.16232246160507202
Batch 16/64 loss: -0.18088102340698242
Batch 17/64 loss: -0.1920625865459442
Batch 18/64 loss: -0.16426479816436768
Batch 19/64 loss: -0.16123396158218384
Batch 20/64 loss: -0.13042747974395752
Batch 21/64 loss: -0.14397746324539185
Batch 22/64 loss: -0.1659194827079773
Batch 23/64 loss: -0.1510545015335083
Batch 24/64 loss: -0.18249791860580444
Batch 25/64 loss: -0.15980911254882812
Batch 26/64 loss: -0.13395172357559204
Batch 27/64 loss: -0.16055387258529663
Batch 28/64 loss: -0.1204911470413208
Batch 29/64 loss: -0.1727699637413025
Batch 30/64 loss: -0.18708425760269165
Batch 31/64 loss: -0.11745262145996094
Batch 32/64 loss: -0.14005225896835327
Batch 33/64 loss: -0.1668548583984375
Batch 34/64 loss: -0.1339969038963318
Batch 35/64 loss: -0.16366714239120483
Batch 36/64 loss: -0.1461867094039917
Batch 37/64 loss: -0.1576656699180603
Batch 38/64 loss: -0.1735903024673462
Batch 39/64 loss: -0.15250307321548462
Batch 40/64 loss: -0.1361672282218933
Batch 41/64 loss: -0.16999804973602295
Batch 42/64 loss: -0.16085726022720337
Batch 43/64 loss: -0.1553100347518921
Batch 44/64 loss: -0.185744047164917
Batch 45/64 loss: -0.17826306819915771
Batch 46/64 loss: -0.16880089044570923
Batch 47/64 loss: -0.1550518274307251
Batch 48/64 loss: -0.17509770393371582
Batch 49/64 loss: -0.15631431341171265
Batch 50/64 loss: -0.1883336305618286
Batch 51/64 loss: -0.16969478130340576
Batch 52/64 loss: -0.16598820686340332
Batch 53/64 loss: -0.168121337890625
Batch 54/64 loss: -0.130048930644989
Batch 55/64 loss: -0.12185877561569214
Batch 56/64 loss: -0.17741280794143677
Batch 57/64 loss: -0.15378820896148682
Batch 58/64 loss: -0.18177646398544312
Batch 59/64 loss: -0.1158900260925293
Batch 60/64 loss: -0.17390531301498413
Batch 61/64 loss: -0.17046481370925903
Batch 62/64 loss: -0.18422538042068481
Batch 63/64 loss: -0.15431052446365356
Batch 64/64 loss: -0.1461675763130188
Epoch 420  Train loss: -0.15994248320074642  Val loss: 0.06510450012495428
Epoch 421
-------------------------------
Batch 1/64 loss: -0.1356460452079773
Batch 2/64 loss: -0.11908918619155884
Batch 3/64 loss: -0.18872874975204468
Batch 4/64 loss: -0.18784773349761963
Batch 5/64 loss: -0.13532918691635132
Batch 6/64 loss: -0.15686368942260742
Batch 7/64 loss: -0.1645907163619995
Batch 8/64 loss: -0.1368567943572998
Batch 9/64 loss: -0.17015796899795532
Batch 10/64 loss: -0.16901439428329468
Batch 11/64 loss: -0.17611396312713623
Batch 12/64 loss: -0.15357190370559692
Batch 13/64 loss: -0.13436567783355713
Batch 14/64 loss: -0.20069339871406555
Batch 15/64 loss: -0.1684795618057251
Batch 16/64 loss: -0.139950692653656
Batch 17/64 loss: -0.1574636697769165
Batch 18/64 loss: -0.0951005220413208
Batch 19/64 loss: -0.16182100772857666
Batch 20/64 loss: -0.13592427968978882
Batch 21/64 loss: -0.1748175024986267
Batch 22/64 loss: -0.11601829528808594
Batch 23/64 loss: -0.17778903245925903
Batch 24/64 loss: -0.17308545112609863
Batch 25/64 loss: -0.1697748303413391
Batch 26/64 loss: -0.19379651546478271
Batch 27/64 loss: -0.19210755825042725
Batch 28/64 loss: -0.18754863739013672
Batch 29/64 loss: -0.12712740898132324
Batch 30/64 loss: -0.14956152439117432
Batch 31/64 loss: -0.15486878156661987
Batch 32/64 loss: -0.17419344186782837
Batch 33/64 loss: -0.1745234727859497
Batch 34/64 loss: -0.1852799654006958
Batch 35/64 loss: -0.1500212550163269
Batch 36/64 loss: -0.16078895330429077
Batch 37/64 loss: -0.19879069924354553
Batch 38/64 loss: -0.2065119743347168
Batch 39/64 loss: -0.20236867666244507
Batch 40/64 loss: -0.16070961952209473
Batch 41/64 loss: -0.17814677953720093
Batch 42/64 loss: -0.1769726276397705
Batch 43/64 loss: -0.1730857491493225
Batch 44/64 loss: -0.1533241868019104
Batch 45/64 loss: -0.14908868074417114
Batch 46/64 loss: -0.17171460390090942
Batch 47/64 loss: -0.14002889394760132
Batch 48/64 loss: -0.171647846698761
Batch 49/64 loss: -0.13836407661437988
Batch 50/64 loss: -0.1801358461380005
Batch 51/64 loss: -0.18765142560005188
Batch 52/64 loss: -0.14862191677093506
Batch 53/64 loss: -0.1117066740989685
Batch 54/64 loss: -0.1543872356414795
Batch 55/64 loss: -0.1260460615158081
Batch 56/64 loss: -0.17036259174346924
Batch 57/64 loss: -0.1918545961380005
Batch 58/64 loss: -0.15974730253219604
Batch 59/64 loss: -0.15419846773147583
Batch 60/64 loss: -0.09662652015686035
Batch 61/64 loss: -0.19232016801834106
Batch 62/64 loss: -0.16100478172302246
Batch 63/64 loss: -0.15485703945159912
Batch 64/64 loss: -0.18849903345108032
Epoch 421  Train loss: -0.16157742878969977  Val loss: 0.06777499078475323
Epoch 422
-------------------------------
Batch 1/64 loss: -0.1604558825492859
Batch 2/64 loss: -0.19862672686576843
Batch 3/64 loss: -0.12144839763641357
Batch 4/64 loss: -0.1857026219367981
Batch 5/64 loss: -0.17135536670684814
Batch 6/64 loss: -0.17396962642669678
Batch 7/64 loss: -0.17302757501602173
Batch 8/64 loss: -0.13863885402679443
Batch 9/64 loss: -0.17599642276763916
Batch 10/64 loss: -0.196305513381958
Batch 11/64 loss: -0.1314794421195984
Batch 12/64 loss: -0.20045822858810425
Batch 13/64 loss: -0.1719379425048828
Batch 14/64 loss: -0.1361445188522339
Batch 15/64 loss: -0.1769629716873169
Batch 16/64 loss: -0.18779194355010986
Batch 17/64 loss: -0.17337363958358765
Batch 18/64 loss: -0.20630842447280884
Batch 19/64 loss: -0.1805146336555481
Batch 20/64 loss: -0.20736175775527954
Batch 21/64 loss: -0.17238253355026245
Batch 22/64 loss: -0.14990699291229248
Batch 23/64 loss: -0.16463720798492432
Batch 24/64 loss: -0.16943180561065674
Batch 25/64 loss: -0.1651543378829956
Batch 26/64 loss: -0.19947117567062378
Batch 27/64 loss: -0.1590442657470703
Batch 28/64 loss: -0.1903918981552124
Batch 29/64 loss: -0.1442357301712036
Batch 30/64 loss: -0.12474268674850464
Batch 31/64 loss: -0.12389326095581055
Batch 32/64 loss: -0.15090006589889526
Batch 33/64 loss: -0.15998810529708862
Batch 34/64 loss: -0.11783385276794434
Batch 35/64 loss: -0.1385183334350586
Batch 36/64 loss: -0.12069672346115112
Batch 37/64 loss: -0.14540427923202515
Batch 38/64 loss: -0.18224698305130005
Batch 39/64 loss: -0.16799306869506836
Batch 40/64 loss: -0.1610812544822693
Batch 41/64 loss: -0.16823840141296387
Batch 42/64 loss: -0.11752992868423462
Batch 43/64 loss: -0.14863848686218262
Batch 44/64 loss: -0.14597171545028687
Batch 45/64 loss: -0.17510664463043213
Batch 46/64 loss: -0.17078375816345215
Batch 47/64 loss: -0.15714102983474731
Batch 48/64 loss: -0.1340428590774536
Batch 49/64 loss: -0.1380438208580017
Batch 50/64 loss: -0.11827301979064941
Batch 51/64 loss: -0.15026170015335083
Batch 52/64 loss: -0.14674890041351318
Batch 53/64 loss: -0.17360460758209229
Batch 54/64 loss: -0.1806066632270813
Batch 55/64 loss: -0.16561579704284668
Batch 56/64 loss: -0.1387178897857666
Batch 57/64 loss: -0.13815367221832275
Batch 58/64 loss: -0.10955709218978882
Batch 59/64 loss: -0.14032459259033203
Batch 60/64 loss: -0.17641431093215942
Batch 61/64 loss: -0.1492406129837036
Batch 62/64 loss: -0.1438046097755432
Batch 63/64 loss: -0.16377973556518555
Batch 64/64 loss: -0.1312178373336792
Epoch 422  Train loss: -0.15882083574930828  Val loss: 0.06202913600554581
Epoch 423
-------------------------------
Batch 1/64 loss: -0.1447598934173584
Batch 2/64 loss: -0.18575996160507202
Batch 3/64 loss: -0.16849994659423828
Batch 4/64 loss: -0.17358475923538208
Batch 5/64 loss: -0.12858349084854126
Batch 6/64 loss: -0.17365777492523193
Batch 7/64 loss: -0.21159163117408752
Batch 8/64 loss: -0.17944014072418213
Batch 9/64 loss: -0.15318822860717773
Batch 10/64 loss: -0.16890853643417358
Batch 11/64 loss: -0.12113910913467407
Batch 12/64 loss: -0.12384140491485596
Batch 13/64 loss: -0.17677181959152222
Batch 14/64 loss: -0.17067992687225342
Batch 15/64 loss: -0.1575971245765686
Batch 16/64 loss: -0.18891513347625732
Batch 17/64 loss: -0.1658085584640503
Batch 18/64 loss: -0.12443310022354126
Batch 19/64 loss: -0.0976337194442749
Batch 20/64 loss: -0.1597035527229309
Batch 21/64 loss: -0.14832907915115356
Batch 22/64 loss: -0.1558983325958252
Batch 23/64 loss: -0.1599370837211609
Batch 24/64 loss: -0.17879647016525269
Batch 25/64 loss: -0.17624080181121826
Batch 26/64 loss: -0.14780056476593018
Batch 27/64 loss: -0.14372611045837402
Batch 28/64 loss: -0.1755315661430359
Batch 29/64 loss: -0.1730310320854187
Batch 30/64 loss: -0.15977412462234497
Batch 31/64 loss: -0.14706063270568848
Batch 32/64 loss: -0.16029995679855347
Batch 33/64 loss: -0.1748051643371582
Batch 34/64 loss: -0.15881311893463135
Batch 35/64 loss: -0.13292235136032104
Batch 36/64 loss: -0.13514232635498047
Batch 37/64 loss: -0.14049506187438965
Batch 38/64 loss: -0.11625546216964722
Batch 39/64 loss: -0.16605746746063232
Batch 40/64 loss: -0.18202996253967285
Batch 41/64 loss: -0.16333913803100586
Batch 42/64 loss: -0.15742266178131104
Batch 43/64 loss: -0.13807755708694458
Batch 44/64 loss: -0.17521452903747559
Batch 45/64 loss: -0.18645435571670532
Batch 46/64 loss: -0.14716970920562744
Batch 47/64 loss: -0.1707172989845276
Batch 48/64 loss: -0.13982528448104858
Batch 49/64 loss: -0.12809205055236816
Batch 50/64 loss: -0.16635465621948242
Batch 51/64 loss: -0.19221431016921997
Batch 52/64 loss: -0.18394553661346436
Batch 53/64 loss: -0.19188040494918823
Batch 54/64 loss: -0.15221953392028809
Batch 55/64 loss: -0.13672125339508057
Batch 56/64 loss: -0.15250033140182495
Batch 57/64 loss: -0.16414374113082886
Batch 58/64 loss: -0.14881032705307007
Batch 59/64 loss: -0.20052790641784668
Batch 60/64 loss: -0.14305275678634644
Batch 61/64 loss: -0.13610297441482544
Batch 62/64 loss: -0.15156173706054688
Batch 63/64 loss: -0.11022913455963135
Batch 64/64 loss: -0.18663352727890015
Epoch 423  Train loss: -0.1581803422348172  Val loss: 0.06892633561006527
Epoch 424
-------------------------------
Batch 1/64 loss: -0.17491281032562256
Batch 2/64 loss: -0.18919390439987183
Batch 3/64 loss: -0.17782318592071533
Batch 4/64 loss: -0.16165930032730103
Batch 5/64 loss: -0.09093821048736572
Batch 6/64 loss: -0.13445031642913818
Batch 7/64 loss: -0.17700296640396118
Batch 8/64 loss: -0.17506420612335205
Batch 9/64 loss: -0.1807156801223755
Batch 10/64 loss: -0.19925838708877563
Batch 11/64 loss: -0.18888914585113525
Batch 12/64 loss: -0.1633690595626831
Batch 13/64 loss: -0.19110631942749023
Batch 14/64 loss: -0.16567230224609375
Batch 15/64 loss: -0.19454854726791382
Batch 16/64 loss: -0.15045815706253052
Batch 17/64 loss: -0.11968684196472168
Batch 18/64 loss: -0.15692031383514404
Batch 19/64 loss: -0.1728203296661377
Batch 20/64 loss: -0.1625877022743225
Batch 21/64 loss: -0.15338492393493652
Batch 22/64 loss: -0.2060207724571228
Batch 23/64 loss: -0.16661930084228516
Batch 24/64 loss: -0.18520718812942505
Batch 25/64 loss: -0.15528100728988647
Batch 26/64 loss: -0.15631341934204102
Batch 27/64 loss: -0.11928856372833252
Batch 28/64 loss: -0.15188133716583252
Batch 29/64 loss: -0.1606312394142151
Batch 30/64 loss: -0.18983960151672363
Batch 31/64 loss: -0.18295538425445557
Batch 32/64 loss: -0.1716490387916565
Batch 33/64 loss: -0.18646621704101562
Batch 34/64 loss: -0.17457282543182373
Batch 35/64 loss: -0.18942487239837646
Batch 36/64 loss: -0.13556021451950073
Batch 37/64 loss: -0.1566544771194458
Batch 38/64 loss: -0.17649412155151367
Batch 39/64 loss: -0.1622387170791626
Batch 40/64 loss: -0.16093045473098755
Batch 41/64 loss: -0.1297823190689087
Batch 42/64 loss: -0.16685348749160767
Batch 43/64 loss: -0.1460360288619995
Batch 44/64 loss: -0.17700207233428955
Batch 45/64 loss: -0.16008472442626953
Batch 46/64 loss: -0.15835785865783691
Batch 47/64 loss: -0.1189650297164917
Batch 48/64 loss: -0.1587693691253662
Batch 49/64 loss: -0.17347848415374756
Batch 50/64 loss: -0.1903885304927826
Batch 51/64 loss: -0.1329188346862793
Batch 52/64 loss: -0.1885783076286316
Batch 53/64 loss: -0.1749134659767151
Batch 54/64 loss: -0.12985217571258545
Batch 55/64 loss: -0.13897264003753662
Batch 56/64 loss: -0.11531108617782593
Batch 57/64 loss: -0.172110915184021
Batch 58/64 loss: -0.18001145124435425
Batch 59/64 loss: -0.1802930235862732
Batch 60/64 loss: -0.149530827999115
Batch 61/64 loss: -0.1613098382949829
Batch 62/64 loss: -0.11993187665939331
Batch 63/64 loss: -0.13779771327972412
Batch 64/64 loss: -0.12996482849121094
Epoch 424  Train loss: -0.161995530128479  Val loss: 0.07155022506451689
Epoch 425
-------------------------------
Batch 1/64 loss: -0.13826435804367065
Batch 2/64 loss: -0.17104101181030273
Batch 3/64 loss: -0.15789282321929932
Batch 4/64 loss: -0.2075415849685669
Batch 5/64 loss: -0.1341610550880432
Batch 6/64 loss: -0.19079720973968506
Batch 7/64 loss: -0.18089652061462402
Batch 8/64 loss: -0.20442846417427063
Batch 9/64 loss: -0.1645938754081726
Batch 10/64 loss: -0.18372559547424316
Batch 11/64 loss: -0.17828583717346191
Batch 12/64 loss: -0.16334474086761475
Batch 13/64 loss: -0.18718940019607544
Batch 14/64 loss: -0.14103859663009644
Batch 15/64 loss: -0.19284012913703918
Batch 16/64 loss: -0.18934273719787598
Batch 17/64 loss: -0.16322588920593262
Batch 18/64 loss: -0.11157816648483276
Batch 19/64 loss: -0.17424726486206055
Batch 20/64 loss: -0.16093569993972778
Batch 21/64 loss: -0.12566924095153809
Batch 22/64 loss: -0.17578721046447754
Batch 23/64 loss: -0.1985667645931244
Batch 24/64 loss: -0.1727430820465088
Batch 25/64 loss: -0.15727394819259644
Batch 26/64 loss: -0.18228298425674438
Batch 27/64 loss: -0.1773967146873474
Batch 28/64 loss: -0.12766242027282715
Batch 29/64 loss: -0.18012166023254395
Batch 30/64 loss: -0.08583939075469971
Batch 31/64 loss: -0.1655486822128296
Batch 32/64 loss: -0.14650654792785645
Batch 33/64 loss: -0.16208285093307495
Batch 34/64 loss: -0.13706082105636597
Batch 35/64 loss: -0.16182160377502441
Batch 36/64 loss: -0.1463334560394287
Batch 37/64 loss: -0.16359949111938477
Batch 38/64 loss: -0.15872138738632202
Batch 39/64 loss: -0.16739946603775024
Batch 40/64 loss: -0.1155710220336914
Batch 41/64 loss: -0.17149269580841064
Batch 42/64 loss: -0.16330760717391968
Batch 43/64 loss: -0.17653971910476685
Batch 44/64 loss: -0.15952181816101074
Batch 45/64 loss: -0.13078850507736206
Batch 46/64 loss: -0.16750895977020264
Batch 47/64 loss: -0.18922656774520874
Batch 48/64 loss: -0.1321852207183838
Batch 49/64 loss: -0.1391204595565796
Batch 50/64 loss: -0.18281912803649902
Batch 51/64 loss: -0.18760383129119873
Batch 52/64 loss: -0.1277254819869995
Batch 53/64 loss: -0.152410626411438
Batch 54/64 loss: -0.10857516527175903
Batch 55/64 loss: -0.13689559698104858
Batch 56/64 loss: -0.16481029987335205
Batch 57/64 loss: -0.17702049016952515
Batch 58/64 loss: -0.1790488362312317
Batch 59/64 loss: -0.1807849407196045
Batch 60/64 loss: -0.14558768272399902
Batch 61/64 loss: -0.19037029147148132
Batch 62/64 loss: -0.11744827032089233
Batch 63/64 loss: -0.14757007360458374
Batch 64/64 loss: -0.1294945478439331
Epoch 425  Train loss: -0.16045243740081788  Val loss: 0.06417480940671311
Epoch 426
-------------------------------
Batch 1/64 loss: -0.19063860177993774
Batch 2/64 loss: -0.15645474195480347
Batch 3/64 loss: -0.16076141595840454
Batch 4/64 loss: -0.1340692639350891
Batch 5/64 loss: -0.14966166019439697
Batch 6/64 loss: -0.19414272904396057
Batch 7/64 loss: -0.1818305253982544
Batch 8/64 loss: -0.1410885453224182
Batch 9/64 loss: -0.17368817329406738
Batch 10/64 loss: -0.19489502906799316
Batch 11/64 loss: -0.1412264108657837
Batch 12/64 loss: -0.16632509231567383
Batch 13/64 loss: -0.13082420825958252
Batch 14/64 loss: -0.19189119338989258
Batch 15/64 loss: -0.21210598945617676
Batch 16/64 loss: -0.17836546897888184
Batch 17/64 loss: -0.1495724320411682
Batch 18/64 loss: -0.16314905881881714
Batch 19/64 loss: -0.1887521743774414
Batch 20/64 loss: -0.1612306833267212
Batch 21/64 loss: -0.1831551194190979
Batch 22/64 loss: -0.1670430302619934
Batch 23/64 loss: -0.17766964435577393
Batch 24/64 loss: -0.16997480392456055
Batch 25/64 loss: -0.15036046504974365
Batch 26/64 loss: -0.11175417900085449
Batch 27/64 loss: -0.14477622509002686
Batch 28/64 loss: -0.1484697461128235
Batch 29/64 loss: -0.16404736042022705
Batch 30/64 loss: -0.14058345556259155
Batch 31/64 loss: -0.1869751214981079
Batch 32/64 loss: -0.16943812370300293
Batch 33/64 loss: -0.15664464235305786
Batch 34/64 loss: -0.16305005550384521
Batch 35/64 loss: -0.10699361562728882
Batch 36/64 loss: -0.1436777114868164
Batch 37/64 loss: -0.14481669664382935
Batch 38/64 loss: -0.16597038507461548
Batch 39/64 loss: -0.1363600492477417
Batch 40/64 loss: -0.14812487363815308
Batch 41/64 loss: -0.1995016634464264
Batch 42/64 loss: -0.1597219705581665
Batch 43/64 loss: -0.16046345233917236
Batch 44/64 loss: -0.1863890290260315
Batch 45/64 loss: -0.18198245763778687
Batch 46/64 loss: -0.13871890306472778
Batch 47/64 loss: -0.16368597745895386
Batch 48/64 loss: -0.1572667360305786
Batch 49/64 loss: -0.13583999872207642
Batch 50/64 loss: -0.1833796501159668
Batch 51/64 loss: -0.14000296592712402
Batch 52/64 loss: -0.16489660739898682
Batch 53/64 loss: -0.18211686611175537
Batch 54/64 loss: -0.18788659572601318
Batch 55/64 loss: -0.15127742290496826
Batch 56/64 loss: -0.1454756259918213
Batch 57/64 loss: -0.17611843347549438
Batch 58/64 loss: -0.14745497703552246
Batch 59/64 loss: -0.18470293283462524
Batch 60/64 loss: -0.18344783782958984
Batch 61/64 loss: -0.16415727138519287
Batch 62/64 loss: -0.1733981966972351
Batch 63/64 loss: -0.13373208045959473
Batch 64/64 loss: -0.1617862582206726
Epoch 426  Train loss: -0.16256495760936363  Val loss: 0.06188381742365991
Epoch 427
-------------------------------
Batch 1/64 loss: -0.18563729524612427
Batch 2/64 loss: -0.1349426507949829
Batch 3/64 loss: -0.1076851487159729
Batch 4/64 loss: -0.19195890426635742
Batch 5/64 loss: -0.1418529748916626
Batch 6/64 loss: -0.1745871901512146
Batch 7/64 loss: -0.13990634679794312
Batch 8/64 loss: -0.1696842908859253
Batch 9/64 loss: -0.19873422384262085
Batch 10/64 loss: -0.17802894115447998
Batch 11/64 loss: -0.19799259305000305
Batch 12/64 loss: -0.17945432662963867
Batch 13/64 loss: -0.18379920721054077
Batch 14/64 loss: -0.15465718507766724
Batch 15/64 loss: -0.18670737743377686
Batch 16/64 loss: -0.1919703483581543
Batch 17/64 loss: -0.1630878448486328
Batch 18/64 loss: -0.16456401348114014
Batch 19/64 loss: -0.17425674200057983
Batch 20/64 loss: -0.15828180313110352
Batch 21/64 loss: -0.17133736610412598
Batch 22/64 loss: -0.14144039154052734
Batch 23/64 loss: -0.15898799896240234
Batch 24/64 loss: -0.16805988550186157
Batch 25/64 loss: -0.1678086519241333
Batch 26/64 loss: -0.18616032600402832
Batch 27/64 loss: -0.19077998399734497
Batch 28/64 loss: -0.17927294969558716
Batch 29/64 loss: -0.14894574880599976
Batch 30/64 loss: -0.15387600660324097
Batch 31/64 loss: -0.12894034385681152
Batch 32/64 loss: -0.17189955711364746
Batch 33/64 loss: -0.16378355026245117
Batch 34/64 loss: -0.16815173625946045
Batch 35/64 loss: -0.12302732467651367
Batch 36/64 loss: -0.19052305817604065
Batch 37/64 loss: -0.17773091793060303
Batch 38/64 loss: -0.1889156699180603
Batch 39/64 loss: -0.13022512197494507
Batch 40/64 loss: -0.14474523067474365
Batch 41/64 loss: -0.19207987189292908
Batch 42/64 loss: -0.15311431884765625
Batch 43/64 loss: -0.15491724014282227
Batch 44/64 loss: -0.1700097918510437
Batch 45/64 loss: -0.1773328185081482
Batch 46/64 loss: -0.16477489471435547
Batch 47/64 loss: -0.1478680968284607
Batch 48/64 loss: -0.15191113948822021
Batch 49/64 loss: -0.13878607749938965
Batch 50/64 loss: -0.16381728649139404
Batch 51/64 loss: -0.14631283283233643
Batch 52/64 loss: -0.14043140411376953
Batch 53/64 loss: -0.10111117362976074
Batch 54/64 loss: -0.18440985679626465
Batch 55/64 loss: -0.10828948020935059
Batch 56/64 loss: -0.16624236106872559
Batch 57/64 loss: -0.16862964630126953
Batch 58/64 loss: -0.16091036796569824
Batch 59/64 loss: -0.18627643585205078
Batch 60/64 loss: -0.15695005655288696
Batch 61/64 loss: -0.1751890778541565
Batch 62/64 loss: -0.19873860478401184
Batch 63/64 loss: -0.1571272611618042
Batch 64/64 loss: -0.08062314987182617
Epoch 427  Train loss: -0.1624799793841792  Val loss: 0.07075833291122594
Epoch 428
-------------------------------
Batch 1/64 loss: -0.13964444398880005
Batch 2/64 loss: -0.19026494026184082
Batch 3/64 loss: -0.14151710271835327
Batch 4/64 loss: -0.1876189112663269
Batch 5/64 loss: -0.21087780594825745
Batch 6/64 loss: -0.1392425298690796
Batch 7/64 loss: -0.19718152284622192
Batch 8/64 loss: -0.1722733974456787
Batch 9/64 loss: -0.15177738666534424
Batch 10/64 loss: -0.17335820198059082
Batch 11/64 loss: -0.1482677459716797
Batch 12/64 loss: -0.169272780418396
Batch 13/64 loss: -0.15311026573181152
Batch 14/64 loss: -0.14675569534301758
Batch 15/64 loss: -0.17744606733322144
Batch 16/64 loss: -0.12882280349731445
Batch 17/64 loss: -0.18153107166290283
Batch 18/64 loss: -0.17010807991027832
Batch 19/64 loss: -0.21013417840003967
Batch 20/64 loss: -0.15331435203552246
Batch 21/64 loss: -0.13805794715881348
Batch 22/64 loss: -0.160713791847229
Batch 23/64 loss: -0.15547943115234375
Batch 24/64 loss: -0.11302977800369263
Batch 25/64 loss: -0.14813876152038574
Batch 26/64 loss: -0.1611739993095398
Batch 27/64 loss: -0.12765705585479736
Batch 28/64 loss: -0.178472638130188
Batch 29/64 loss: -0.15001213550567627
Batch 30/64 loss: -0.14453113079071045
Batch 31/64 loss: -0.17940664291381836
Batch 32/64 loss: -0.14253544807434082
Batch 33/64 loss: -0.17807036638259888
Batch 34/64 loss: -0.16713762283325195
Batch 35/64 loss: -0.18318027257919312
Batch 36/64 loss: -0.10027235746383667
Batch 37/64 loss: -0.1268436312675476
Batch 38/64 loss: -0.15088337659835815
Batch 39/64 loss: -0.15824264287948608
Batch 40/64 loss: -0.10800588130950928
Batch 41/64 loss: -0.14696156978607178
Batch 42/64 loss: -0.13436049222946167
Batch 43/64 loss: -0.1934802532196045
Batch 44/64 loss: -0.16313952207565308
Batch 45/64 loss: -0.19193634390830994
Batch 46/64 loss: -0.1644507646560669
Batch 47/64 loss: -0.15676730871200562
Batch 48/64 loss: -0.1776106357574463
Batch 49/64 loss: -0.14696913957595825
Batch 50/64 loss: -0.14758628606796265
Batch 51/64 loss: -0.17060041427612305
Batch 52/64 loss: -0.17042207717895508
Batch 53/64 loss: -0.1694021224975586
Batch 54/64 loss: -0.18230777978897095
Batch 55/64 loss: -0.1743190884590149
Batch 56/64 loss: -0.17079681158065796
Batch 57/64 loss: -0.18152856826782227
Batch 58/64 loss: -0.16274422407150269
Batch 59/64 loss: -0.15042060613632202
Batch 60/64 loss: -0.17245030403137207
Batch 61/64 loss: -0.1310989260673523
Batch 62/64 loss: -0.17815732955932617
Batch 63/64 loss: -0.16273951530456543
Batch 64/64 loss: -0.11264771223068237
Epoch 428  Train loss: -0.15998588332942887  Val loss: 0.06600335045778465
Epoch 429
-------------------------------
Batch 1/64 loss: -0.1716288924217224
Batch 2/64 loss: -0.15196770429611206
Batch 3/64 loss: -0.14355552196502686
Batch 4/64 loss: -0.17829537391662598
Batch 5/64 loss: -0.1485115885734558
Batch 6/64 loss: -0.1212228536605835
Batch 7/64 loss: -0.19535309076309204
Batch 8/64 loss: -0.12656444311141968
Batch 9/64 loss: -0.18612951040267944
Batch 10/64 loss: -0.1548992395401001
Batch 11/64 loss: -0.14979946613311768
Batch 12/64 loss: -0.19693315029144287
Batch 13/64 loss: -0.17681944370269775
Batch 14/64 loss: -0.14944684505462646
Batch 15/64 loss: -0.1588810682296753
Batch 16/64 loss: -0.16661548614501953
Batch 17/64 loss: -0.2040005624294281
Batch 18/64 loss: -0.17300903797149658
Batch 19/64 loss: -0.15101075172424316
Batch 20/64 loss: -0.1359134316444397
Batch 21/64 loss: -0.16816341876983643
Batch 22/64 loss: -0.1420201063156128
Batch 23/64 loss: -0.15254926681518555
Batch 24/64 loss: -0.16444700956344604
Batch 25/64 loss: -0.2027754783630371
Batch 26/64 loss: -0.1397807002067566
Batch 27/64 loss: -0.19055628776550293
Batch 28/64 loss: -0.1612657904624939
Batch 29/64 loss: -0.179759681224823
Batch 30/64 loss: -0.14570587873458862
Batch 31/64 loss: -0.1737438440322876
Batch 32/64 loss: -0.1520167589187622
Batch 33/64 loss: -0.16433662176132202
Batch 34/64 loss: -0.1291365623474121
Batch 35/64 loss: -0.133114755153656
Batch 36/64 loss: -0.16786658763885498
Batch 37/64 loss: -0.13534146547317505
Batch 38/64 loss: -0.17136871814727783
Batch 39/64 loss: -0.122958242893219
Batch 40/64 loss: -0.14995795488357544
Batch 41/64 loss: -0.11943709850311279
Batch 42/64 loss: -0.15824902057647705
Batch 43/64 loss: -0.1776323914527893
Batch 44/64 loss: -0.16114532947540283
Batch 45/64 loss: -0.14578759670257568
Batch 46/64 loss: -0.12274152040481567
Batch 47/64 loss: -0.15008002519607544
Batch 48/64 loss: -0.16325539350509644
Batch 49/64 loss: -0.1829051971435547
Batch 50/64 loss: -0.1340160369873047
Batch 51/64 loss: -0.16686832904815674
Batch 52/64 loss: -0.169053316116333
Batch 53/64 loss: -0.18828260898590088
Batch 54/64 loss: -0.20087748765945435
Batch 55/64 loss: -0.15078920125961304
Batch 56/64 loss: -0.18440008163452148
Batch 57/64 loss: -0.16410541534423828
Batch 58/64 loss: -0.2127688229084015
Batch 59/64 loss: -0.17105811834335327
Batch 60/64 loss: -0.1540827751159668
Batch 61/64 loss: -0.19481027126312256
Batch 62/64 loss: -0.15923553705215454
Batch 63/64 loss: -0.1915583610534668
Batch 64/64 loss: -0.15558785200119019
Epoch 429  Train loss: -0.1619961320185194  Val loss: 0.06346861316576037
Epoch 430
-------------------------------
Batch 1/64 loss: -0.17342686653137207
Batch 2/64 loss: -0.16957712173461914
Batch 3/64 loss: -0.1605122685432434
Batch 4/64 loss: -0.17077910900115967
Batch 5/64 loss: -0.2057989239692688
Batch 6/64 loss: -0.207253098487854
Batch 7/64 loss: -0.16844624280929565
Batch 8/64 loss: -0.14878100156784058
Batch 9/64 loss: -0.1591441035270691
Batch 10/64 loss: -0.18738412857055664
Batch 11/64 loss: -0.14975899457931519
Batch 12/64 loss: -0.16405147314071655
Batch 13/64 loss: -0.13508164882659912
Batch 14/64 loss: -0.14258939027786255
Batch 15/64 loss: -0.1490599513053894
Batch 16/64 loss: -0.12254714965820312
Batch 17/64 loss: -0.11621350049972534
Batch 18/64 loss: -0.12375414371490479
Batch 19/64 loss: -0.13835370540618896
Batch 20/64 loss: -0.16886872053146362
Batch 21/64 loss: -0.15456509590148926
Batch 22/64 loss: -0.12217110395431519
Batch 23/64 loss: -0.15285521745681763
Batch 24/64 loss: -0.1720966100692749
Batch 25/64 loss: -0.18181097507476807
Batch 26/64 loss: -0.15083730220794678
Batch 27/64 loss: -0.20602089166641235
Batch 28/64 loss: -0.10260903835296631
Batch 29/64 loss: -0.1578945517539978
Batch 30/64 loss: -0.1456700563430786
Batch 31/64 loss: -0.17251628637313843
Batch 32/64 loss: -0.19428175687789917
Batch 33/64 loss: -0.14631080627441406
Batch 34/64 loss: -0.15952688455581665
Batch 35/64 loss: -0.17702430486679077
Batch 36/64 loss: -0.17279410362243652
Batch 37/64 loss: -0.14294958114624023
Batch 38/64 loss: -0.1692686676979065
Batch 39/64 loss: -0.14807385206222534
Batch 40/64 loss: -0.19572225213050842
Batch 41/64 loss: -0.16448140144348145
Batch 42/64 loss: -0.14570355415344238
Batch 43/64 loss: -0.16874581575393677
Batch 44/64 loss: -0.16161417961120605
Batch 45/64 loss: -0.17484557628631592
Batch 46/64 loss: -0.14913439750671387
Batch 47/64 loss: -0.178408682346344
Batch 48/64 loss: -0.1461857557296753
Batch 49/64 loss: -0.1750713586807251
Batch 50/64 loss: -0.14984816312789917
Batch 51/64 loss: -0.21069598197937012
Batch 52/64 loss: -0.11492019891738892
Batch 53/64 loss: -0.16607075929641724
Batch 54/64 loss: -0.17107808589935303
Batch 55/64 loss: -0.14041846990585327
Batch 56/64 loss: -0.16944175958633423
Batch 57/64 loss: -0.17064553499221802
Batch 58/64 loss: -0.18148398399353027
Batch 59/64 loss: -0.16817128658294678
Batch 60/64 loss: -0.16894567012786865
Batch 61/64 loss: -0.19015604257583618
Batch 62/64 loss: -0.18571960926055908
Batch 63/64 loss: -0.15679168701171875
Batch 64/64 loss: -0.13891124725341797
Epoch 430  Train loss: -0.16155517288282806  Val loss: 0.0668856894847044
Epoch 431
-------------------------------
Batch 1/64 loss: -0.18448948860168457
Batch 2/64 loss: -0.18981313705444336
Batch 3/64 loss: -0.20953291654586792
Batch 4/64 loss: -0.17881882190704346
Batch 5/64 loss: -0.14924484491348267
Batch 6/64 loss: -0.20172256231307983
Batch 7/64 loss: -0.15221655368804932
Batch 8/64 loss: -0.1911681890487671
Batch 9/64 loss: -0.12984603643417358
Batch 10/64 loss: -0.18655884265899658
Batch 11/64 loss: -0.18515092134475708
Batch 12/64 loss: -0.15353208780288696
Batch 13/64 loss: -0.10289651155471802
Batch 14/64 loss: -0.15845441818237305
Batch 15/64 loss: -0.2018490433692932
Batch 16/64 loss: -0.1693057417869568
Batch 17/64 loss: -0.16735714673995972
Batch 18/64 loss: -0.08034610748291016
Batch 19/64 loss: -0.14871609210968018
Batch 20/64 loss: -0.17394864559173584
Batch 21/64 loss: -0.18515944480895996
Batch 22/64 loss: -0.1778576374053955
Batch 23/64 loss: -0.15733587741851807
Batch 24/64 loss: -0.18918883800506592
Batch 25/64 loss: -0.14742666482925415
Batch 26/64 loss: -0.14644235372543335
Batch 27/64 loss: -0.17857342958450317
Batch 28/64 loss: -0.13419276475906372
Batch 29/64 loss: -0.1801544427871704
Batch 30/64 loss: -0.15457862615585327
Batch 31/64 loss: -0.12422299385070801
Batch 32/64 loss: -0.15859991312026978
Batch 33/64 loss: -0.19584280252456665
Batch 34/64 loss: -0.16028571128845215
Batch 35/64 loss: -0.16451454162597656
Batch 36/64 loss: -0.17977041006088257
Batch 37/64 loss: -0.16067951917648315
Batch 38/64 loss: -0.10199344158172607
Batch 39/64 loss: -0.16607487201690674
Batch 40/64 loss: -0.148002028465271
Batch 41/64 loss: -0.1822371482849121
Batch 42/64 loss: -0.1792261004447937
Batch 43/64 loss: -0.1628512740135193
Batch 44/64 loss: -0.1973389983177185
Batch 45/64 loss: -0.163129985332489
Batch 46/64 loss: -0.17128533124923706
Batch 47/64 loss: -0.1378980278968811
Batch 48/64 loss: -0.1364871859550476
Batch 49/64 loss: -0.1965254247188568
Batch 50/64 loss: -0.16683989763259888
Batch 51/64 loss: -0.11299562454223633
Batch 52/64 loss: -0.16063082218170166
Batch 53/64 loss: -0.1972343921661377
Batch 54/64 loss: -0.17473632097244263
Batch 55/64 loss: -0.18075740337371826
Batch 56/64 loss: -0.1729283332824707
Batch 57/64 loss: -0.1530851125717163
Batch 58/64 loss: -0.18359404802322388
Batch 59/64 loss: -0.1892538070678711
Batch 60/64 loss: -0.16440296173095703
Batch 61/64 loss: -0.15988141298294067
Batch 62/64 loss: -0.142039954662323
Batch 63/64 loss: -0.13274800777435303
Batch 64/64 loss: -0.12460088729858398
Epoch 431  Train loss: -0.16372427706624948  Val loss: 0.07040944910541024
Epoch 432
-------------------------------
Batch 1/64 loss: -0.2007935643196106
Batch 2/64 loss: -0.16661012172698975
Batch 3/64 loss: -0.16946780681610107
Batch 4/64 loss: -0.17274785041809082
Batch 5/64 loss: -0.15190541744232178
Batch 6/64 loss: -0.20443767309188843
Batch 7/64 loss: -0.13765394687652588
Batch 8/64 loss: -0.16425484418869019
Batch 9/64 loss: -0.15384632349014282
Batch 10/64 loss: -0.18660730123519897
Batch 11/64 loss: -0.13758552074432373
Batch 12/64 loss: -0.1650506854057312
Batch 13/64 loss: -0.12463116645812988
Batch 14/64 loss: -0.09618186950683594
Batch 15/64 loss: -0.12864357233047485
Batch 16/64 loss: -0.1951110064983368
Batch 17/64 loss: -0.17292290925979614
Batch 18/64 loss: -0.15655463933944702
Batch 19/64 loss: -0.19563260674476624
Batch 20/64 loss: -0.16828233003616333
Batch 21/64 loss: -0.13776826858520508
Batch 22/64 loss: -0.175592303276062
Batch 23/64 loss: -0.15147411823272705
Batch 24/64 loss: -0.14844363927841187
Batch 25/64 loss: -0.148881196975708
Batch 26/64 loss: -0.17459863424301147
Batch 27/64 loss: -0.13343697786331177
Batch 28/64 loss: -0.17764407396316528
Batch 29/64 loss: -0.1514573097229004
Batch 30/64 loss: -0.1568819284439087
Batch 31/64 loss: -0.1489850878715515
Batch 32/64 loss: -0.17510342597961426
Batch 33/64 loss: -0.1820540428161621
Batch 34/64 loss: -0.17267155647277832
Batch 35/64 loss: -0.1288384199142456
Batch 36/64 loss: -0.11964231729507446
Batch 37/64 loss: -0.177751362323761
Batch 38/64 loss: -0.13411599397659302
Batch 39/64 loss: -0.19812512397766113
Batch 40/64 loss: -0.18895041942596436
Batch 41/64 loss: -0.18958252668380737
Batch 42/64 loss: -0.1775296926498413
Batch 43/64 loss: -0.15990924835205078
Batch 44/64 loss: -0.1668751835823059
Batch 45/64 loss: -0.1918794810771942
Batch 46/64 loss: -0.14220857620239258
Batch 47/64 loss: -0.15973663330078125
Batch 48/64 loss: -0.14873671531677246
Batch 49/64 loss: -0.18318063020706177
Batch 50/64 loss: -0.17274755239486694
Batch 51/64 loss: -0.1829940676689148
Batch 52/64 loss: -0.202507883310318
Batch 53/64 loss: -0.147333562374115
Batch 54/64 loss: -0.17087948322296143
Batch 55/64 loss: -0.1692061424255371
Batch 56/64 loss: -0.14818644523620605
Batch 57/64 loss: -0.18113774061203003
Batch 58/64 loss: -0.1583748459815979
Batch 59/64 loss: -0.18690156936645508
Batch 60/64 loss: -0.1868739128112793
Batch 61/64 loss: -0.16365230083465576
Batch 62/64 loss: -0.1624126434326172
Batch 63/64 loss: -0.13998663425445557
Batch 64/64 loss: -0.13827675580978394
Epoch 432  Train loss: -0.16357456305447746  Val loss: 0.06816418801795986
Epoch 433
-------------------------------
Batch 1/64 loss: -0.18171638250350952
Batch 2/64 loss: -0.15602433681488037
Batch 3/64 loss: -0.17104119062423706
Batch 4/64 loss: -0.1469196081161499
Batch 5/64 loss: -0.14769434928894043
Batch 6/64 loss: -0.14030981063842773
Batch 7/64 loss: -0.17288267612457275
Batch 8/64 loss: -0.13183587789535522
Batch 9/64 loss: -0.18833082914352417
Batch 10/64 loss: -0.14959150552749634
Batch 11/64 loss: -0.1810394525527954
Batch 12/64 loss: -0.16816717386245728
Batch 13/64 loss: -0.1713969111442566
Batch 14/64 loss: -0.17853152751922607
Batch 15/64 loss: -0.14337414503097534
Batch 16/64 loss: -0.18593019247055054
Batch 17/64 loss: -0.1880066990852356
Batch 18/64 loss: -0.19430670142173767
Batch 19/64 loss: -0.15771055221557617
Batch 20/64 loss: -0.07816588878631592
Batch 21/64 loss: -0.10384809970855713
Batch 22/64 loss: -0.16262072324752808
Batch 23/64 loss: -0.16856348514556885
Batch 24/64 loss: -0.16011488437652588
Batch 25/64 loss: -0.16451507806777954
Batch 26/64 loss: -0.22751468420028687
Batch 27/64 loss: -0.14778071641921997
Batch 28/64 loss: -0.19328927993774414
Batch 29/64 loss: -0.1872008740901947
Batch 30/64 loss: -0.07517147064208984
Batch 31/64 loss: -0.14797645807266235
Batch 32/64 loss: -0.1368809938430786
Batch 33/64 loss: -0.17312109470367432
Batch 34/64 loss: -0.17724823951721191
Batch 35/64 loss: -0.10755407810211182
Batch 36/64 loss: -0.18282479047775269
Batch 37/64 loss: -0.17056667804718018
Batch 38/64 loss: -0.15048009157180786
Batch 39/64 loss: -0.18380695581436157
Batch 40/64 loss: -0.15024220943450928
Batch 41/64 loss: -0.2147427499294281
Batch 42/64 loss: -0.1670655608177185
Batch 43/64 loss: -0.14536237716674805
Batch 44/64 loss: -0.2020229697227478
Batch 45/64 loss: -0.193587988615036
Batch 46/64 loss: -0.19004899263381958
Batch 47/64 loss: -0.13620436191558838
Batch 48/64 loss: -0.13732850551605225
Batch 49/64 loss: -0.17540717124938965
Batch 50/64 loss: -0.17649298906326294
Batch 51/64 loss: -0.1322338581085205
Batch 52/64 loss: -0.2119758129119873
Batch 53/64 loss: -0.15608137845993042
Batch 54/64 loss: -0.17491978406906128
Batch 55/64 loss: -0.12889355421066284
Batch 56/64 loss: -0.1697782278060913
Batch 57/64 loss: -0.18423974514007568
Batch 58/64 loss: -0.14477550983428955
Batch 59/64 loss: -0.1511061191558838
Batch 60/64 loss: -0.17652076482772827
Batch 61/64 loss: -0.16074758768081665
Batch 62/64 loss: -0.17349278926849365
Batch 63/64 loss: -0.16697508096694946
Batch 64/64 loss: -0.1553681492805481
Epoch 433  Train loss: -0.16299335979947857  Val loss: 0.07881473306937725
Epoch 434
-------------------------------
Batch 1/64 loss: -0.17033249139785767
Batch 2/64 loss: -0.15476131439208984
Batch 3/64 loss: -0.12999862432479858
Batch 4/64 loss: -0.17336797714233398
Batch 5/64 loss: -0.16473257541656494
Batch 6/64 loss: -0.1713305115699768
Batch 7/64 loss: -0.1853356957435608
Batch 8/64 loss: -0.13342928886413574
Batch 9/64 loss: -0.13548243045806885
Batch 10/64 loss: -0.15884417295455933
Batch 11/64 loss: -0.1531084179878235
Batch 12/64 loss: -0.14853566884994507
Batch 13/64 loss: -0.15945178270339966
Batch 14/64 loss: -0.18637937307357788
Batch 15/64 loss: -0.19411110877990723
Batch 16/64 loss: -0.15754276514053345
Batch 17/64 loss: -0.1804976463317871
Batch 18/64 loss: -0.18820041418075562
Batch 19/64 loss: -0.17980068922042847
Batch 20/64 loss: -0.1607401967048645
Batch 21/64 loss: -0.17909234762191772
Batch 22/64 loss: -0.13035368919372559
Batch 23/64 loss: -0.13522791862487793
Batch 24/64 loss: -0.2084750235080719
Batch 25/64 loss: -0.17061543464660645
Batch 26/64 loss: -0.19310498237609863
Batch 27/64 loss: -0.14564967155456543
Batch 28/64 loss: -0.19009298086166382
Batch 29/64 loss: -0.15062636137008667
Batch 30/64 loss: -0.15657538175582886
Batch 31/64 loss: -0.18084019422531128
Batch 32/64 loss: -0.13119101524353027
Batch 33/64 loss: -0.16083520650863647
Batch 34/64 loss: -0.19706887006759644
Batch 35/64 loss: -0.1631428599357605
Batch 36/64 loss: -0.1835864782333374
Batch 37/64 loss: -0.16712737083435059
Batch 38/64 loss: -0.17730867862701416
Batch 39/64 loss: -0.19226041436195374
Batch 40/64 loss: -0.19028723239898682
Batch 41/64 loss: -0.14179813861846924
Batch 42/64 loss: -0.17463940382003784
Batch 43/64 loss: -0.16745328903198242
Batch 44/64 loss: -0.16785788536071777
Batch 45/64 loss: -0.14600664377212524
Batch 46/64 loss: -0.1773112416267395
Batch 47/64 loss: -0.19058001041412354
Batch 48/64 loss: -0.164231538772583
Batch 49/64 loss: -0.1240532398223877
Batch 50/64 loss: -0.18607443571090698
Batch 51/64 loss: -0.17387855052947998
Batch 52/64 loss: -0.17600178718566895
Batch 53/64 loss: -0.18506109714508057
Batch 54/64 loss: -0.15858465433120728
Batch 55/64 loss: -0.15632086992263794
Batch 56/64 loss: -0.14700186252593994
Batch 57/64 loss: -0.1678527593612671
Batch 58/64 loss: -0.16637730598449707
Batch 59/64 loss: -0.1858176589012146
Batch 60/64 loss: -0.11544132232666016
Batch 61/64 loss: -0.18264257907867432
Batch 62/64 loss: -0.11878609657287598
Batch 63/64 loss: -0.15084809064865112
Batch 64/64 loss: -0.1376250982284546
Epoch 434  Train loss: -0.16497701242858287  Val loss: 0.06659818555890899
Epoch 435
-------------------------------
Batch 1/64 loss: -0.17096376419067383
Batch 2/64 loss: -0.18198060989379883
Batch 3/64 loss: -0.19851893186569214
Batch 4/64 loss: -0.1823917031288147
Batch 5/64 loss: -0.17163974046707153
Batch 6/64 loss: -0.17608225345611572
Batch 7/64 loss: -0.1744701862335205
Batch 8/64 loss: -0.1707688570022583
Batch 9/64 loss: -0.16663646697998047
Batch 10/64 loss: -0.11252105236053467
Batch 11/64 loss: -0.1488599181175232
Batch 12/64 loss: -0.18390518426895142
Batch 13/64 loss: -0.15359991788864136
Batch 14/64 loss: -0.13935625553131104
Batch 15/64 loss: -0.17816317081451416
Batch 16/64 loss: -0.1933509111404419
Batch 17/64 loss: -0.18653583526611328
Batch 18/64 loss: -0.18836534023284912
Batch 19/64 loss: -0.1802874207496643
Batch 20/64 loss: -0.1280808448791504
Batch 21/64 loss: -0.1763688325881958
Batch 22/64 loss: -0.18870264291763306
Batch 23/64 loss: -0.15888643264770508
Batch 24/64 loss: -0.17999035120010376
Batch 25/64 loss: -0.17843133211135864
Batch 26/64 loss: -0.17851370573043823
Batch 27/64 loss: -0.15387433767318726
Batch 28/64 loss: -0.16590720415115356
Batch 29/64 loss: -0.16263717412948608
Batch 30/64 loss: -0.21160206198692322
Batch 31/64 loss: -0.16977500915527344
Batch 32/64 loss: -0.19800537824630737
Batch 33/64 loss: -0.17736196517944336
Batch 34/64 loss: -0.1891445517539978
Batch 35/64 loss: -0.17233961820602417
Batch 36/64 loss: -0.18099671602249146
Batch 37/64 loss: -0.19516146183013916
Batch 38/64 loss: -0.16405534744262695
Batch 39/64 loss: -0.18958640098571777
Batch 40/64 loss: -0.15095782279968262
Batch 41/64 loss: -0.1207539439201355
Batch 42/64 loss: -0.17185091972351074
Batch 43/64 loss: -0.16947346925735474
Batch 44/64 loss: -0.18429630994796753
Batch 45/64 loss: -0.15075147151947021
Batch 46/64 loss: -0.16731196641921997
Batch 47/64 loss: -0.13159531354904175
Batch 48/64 loss: -0.17188620567321777
Batch 49/64 loss: -0.12734127044677734
Batch 50/64 loss: -0.1762433648109436
Batch 51/64 loss: -0.1638306975364685
Batch 52/64 loss: -0.17383074760437012
Batch 53/64 loss: -0.15196311473846436
Batch 54/64 loss: -0.13699126243591309
Batch 55/64 loss: -0.12811028957366943
Batch 56/64 loss: -0.1562901735305786
Batch 57/64 loss: -0.15703421831130981
Batch 58/64 loss: -0.1661663055419922
Batch 59/64 loss: -0.15191715955734253
Batch 60/64 loss: -0.1697949767112732
Batch 61/64 loss: -0.17111819982528687
Batch 62/64 loss: -0.1500786542892456
Batch 63/64 loss: -0.17214107513427734
Batch 64/64 loss: -0.07122915983200073
Epoch 435  Train loss: -0.16632109319462496  Val loss: 0.06555423621869169
Epoch 436
-------------------------------
Batch 1/64 loss: -0.11545354127883911
Batch 2/64 loss: -0.1293087601661682
Batch 3/64 loss: -0.1575108766555786
Batch 4/64 loss: -0.15996074676513672
Batch 5/64 loss: -0.16582375764846802
Batch 6/64 loss: -0.17089980840682983
Batch 7/64 loss: -0.17206424474716187
Batch 8/64 loss: -0.1942387819290161
Batch 9/64 loss: -0.13355088233947754
Batch 10/64 loss: -0.1445804238319397
Batch 11/64 loss: -0.17492669820785522
Batch 12/64 loss: -0.16315913200378418
Batch 13/64 loss: -0.11470937728881836
Batch 14/64 loss: -0.169685959815979
Batch 15/64 loss: -0.13971221446990967
Batch 16/64 loss: -0.11517447233200073
Batch 17/64 loss: -0.16974323987960815
Batch 18/64 loss: -0.173467218875885
Batch 19/64 loss: -0.20451107621192932
Batch 20/64 loss: -0.1709255576133728
Batch 21/64 loss: -0.17584466934204102
Batch 22/64 loss: -0.1657731533050537
Batch 23/64 loss: -0.18428820371627808
Batch 24/64 loss: -0.17579025030136108
Batch 25/64 loss: -0.146124005317688
Batch 26/64 loss: -0.15878450870513916
Batch 27/64 loss: -0.20652401447296143
Batch 28/64 loss: -0.19804084300994873
Batch 29/64 loss: -0.16392874717712402
Batch 30/64 loss: -0.16203904151916504
Batch 31/64 loss: -0.1458061933517456
Batch 32/64 loss: -0.17479991912841797
Batch 33/64 loss: -0.18234705924987793
Batch 34/64 loss: -0.17113089561462402
Batch 35/64 loss: -0.17697471380233765
Batch 36/64 loss: -0.17458117008209229
Batch 37/64 loss: -0.15960943698883057
Batch 38/64 loss: -0.14099431037902832
Batch 39/64 loss: -0.13966721296310425
Batch 40/64 loss: -0.12936627864837646
Batch 41/64 loss: -0.17949950695037842
Batch 42/64 loss: -0.16217249631881714
Batch 43/64 loss: -0.19513851404190063
Batch 44/64 loss: -0.17584645748138428
Batch 45/64 loss: -0.16689610481262207
Batch 46/64 loss: -0.15175974369049072
Batch 47/64 loss: -0.15476179122924805
Batch 48/64 loss: -0.1366875171661377
Batch 49/64 loss: -0.17390936613082886
Batch 50/64 loss: -0.1818140149116516
Batch 51/64 loss: -0.19527971744537354
Batch 52/64 loss: -0.16830867528915405
Batch 53/64 loss: -0.17656677961349487
Batch 54/64 loss: -0.13799142837524414
Batch 55/64 loss: -0.17807573080062866
Batch 56/64 loss: -0.1663522720336914
Batch 57/64 loss: -0.16147327423095703
Batch 58/64 loss: -0.10357439517974854
Batch 59/64 loss: -0.17689120769500732
Batch 60/64 loss: -0.16033315658569336
Batch 61/64 loss: -0.14503967761993408
Batch 62/64 loss: -0.13310998678207397
Batch 63/64 loss: -0.166762113571167
Batch 64/64 loss: -0.17378997802734375
Epoch 436  Train loss: -0.1623593381806916  Val loss: 0.06947999660092122
Epoch 437
-------------------------------
Batch 1/64 loss: -0.2033519148826599
Batch 2/64 loss: -0.17118656635284424
Batch 3/64 loss: -0.17885524034500122
Batch 4/64 loss: -0.1520538330078125
Batch 5/64 loss: -0.14814245700836182
Batch 6/64 loss: -0.1999741792678833
Batch 7/64 loss: -0.16859382390975952
Batch 8/64 loss: -0.19391083717346191
Batch 9/64 loss: -0.11184477806091309
Batch 10/64 loss: -0.15657252073287964
Batch 11/64 loss: -0.15226399898529053
Batch 12/64 loss: -0.12282860279083252
Batch 13/64 loss: -0.17315620183944702
Batch 14/64 loss: -0.1466941237449646
Batch 15/64 loss: -0.15857958793640137
Batch 16/64 loss: -0.19194906949996948
Batch 17/64 loss: -0.18772590160369873
Batch 18/64 loss: -0.1698017716407776
Batch 19/64 loss: -0.10442519187927246
Batch 20/64 loss: -0.18935894966125488
Batch 21/64 loss: -0.1591050624847412
Batch 22/64 loss: -0.18801775574684143
Batch 23/64 loss: -0.1781402826309204
Batch 24/64 loss: -0.16607773303985596
Batch 25/64 loss: -0.15953612327575684
Batch 26/64 loss: -0.1762067675590515
Batch 27/64 loss: -0.19166940450668335
Batch 28/64 loss: -0.19313013553619385
Batch 29/64 loss: -0.20117589831352234
Batch 30/64 loss: -0.2098231315612793
Batch 31/64 loss: -0.17529505491256714
Batch 32/64 loss: -0.1719667911529541
Batch 33/64 loss: -0.1306515336036682
Batch 34/64 loss: -0.18976879119873047
Batch 35/64 loss: -0.1714310646057129
Batch 36/64 loss: -0.14694100618362427
Batch 37/64 loss: -0.17792749404907227
Batch 38/64 loss: -0.17377477884292603
Batch 39/64 loss: -0.1828097701072693
Batch 40/64 loss: -0.1337040662765503
Batch 41/64 loss: -0.16740894317626953
Batch 42/64 loss: -0.15813297033309937
Batch 43/64 loss: -0.16393029689788818
Batch 44/64 loss: -0.13882941007614136
Batch 45/64 loss: -0.15761899948120117
Batch 46/64 loss: -0.13682687282562256
Batch 47/64 loss: -0.16338098049163818
Batch 48/64 loss: -0.16421616077423096
Batch 49/64 loss: -0.19583463668823242
Batch 50/64 loss: -0.12826871871948242
Batch 51/64 loss: -0.1833266019821167
Batch 52/64 loss: -0.13015031814575195
Batch 53/64 loss: -0.17916971445083618
Batch 54/64 loss: -0.16521573066711426
Batch 55/64 loss: -0.1780405044555664
Batch 56/64 loss: -0.14543819427490234
Batch 57/64 loss: -0.1692022681236267
Batch 58/64 loss: -0.12064528465270996
Batch 59/64 loss: -0.1447453498840332
Batch 60/64 loss: -0.18138277530670166
Batch 61/64 loss: -0.15850120782852173
Batch 62/64 loss: -0.15522503852844238
Batch 63/64 loss: -0.16109031438827515
Batch 64/64 loss: -0.1704038381576538
Epoch 437  Train loss: -0.16522049202638514  Val loss: 0.06305559619595505
Epoch 438
-------------------------------
Batch 1/64 loss: -0.12398290634155273
Batch 2/64 loss: -0.17068713903427124
Batch 3/64 loss: -0.16655349731445312
Batch 4/64 loss: -0.19897326827049255
Batch 5/64 loss: -0.142181396484375
Batch 6/64 loss: -0.19749972224235535
Batch 7/64 loss: -0.15988004207611084
Batch 8/64 loss: -0.13846343755722046
Batch 9/64 loss: -0.15304666757583618
Batch 10/64 loss: -0.18705439567565918
Batch 11/64 loss: -0.17105036973953247
Batch 12/64 loss: -0.16757291555404663
Batch 13/64 loss: -0.15313774347305298
Batch 14/64 loss: -0.14618456363677979
Batch 15/64 loss: -0.1834803819656372
Batch 16/64 loss: -0.1911245882511139
Batch 17/64 loss: -0.17736250162124634
Batch 18/64 loss: -0.13917803764343262
Batch 19/64 loss: -0.19238996505737305
Batch 20/64 loss: -0.1307554841041565
Batch 21/64 loss: -0.16453325748443604
Batch 22/64 loss: -0.206363707780838
Batch 23/64 loss: -0.12786448001861572
Batch 24/64 loss: -0.16436463594436646
Batch 25/64 loss: -0.17536252737045288
Batch 26/64 loss: -0.19753223657608032
Batch 27/64 loss: -0.07883477210998535
Batch 28/64 loss: -0.15717440843582153
Batch 29/64 loss: -0.20123422145843506
Batch 30/64 loss: -0.17724817991256714
Batch 31/64 loss: -0.19455009698867798
Batch 32/64 loss: -0.17325139045715332
Batch 33/64 loss: -0.10226565599441528
Batch 34/64 loss: -0.1783747673034668
Batch 35/64 loss: -0.13831037282943726
Batch 36/64 loss: -0.20384949445724487
Batch 37/64 loss: -0.13802874088287354
Batch 38/64 loss: -0.17109739780426025
Batch 39/64 loss: -0.14368557929992676
Batch 40/64 loss: -0.14118266105651855
Batch 41/64 loss: -0.13718783855438232
Batch 42/64 loss: -0.1997155249118805
Batch 43/64 loss: -0.17370343208312988
Batch 44/64 loss: -0.15515977144241333
Batch 45/64 loss: -0.15142220258712769
Batch 46/64 loss: -0.18041181564331055
Batch 47/64 loss: -0.17523646354675293
Batch 48/64 loss: -0.16621720790863037
Batch 49/64 loss: -0.1368274688720703
Batch 50/64 loss: -0.17055654525756836
Batch 51/64 loss: -0.13158941268920898
Batch 52/64 loss: -0.1473325490951538
Batch 53/64 loss: -0.16405105590820312
Batch 54/64 loss: -0.1606447696685791
Batch 55/64 loss: -0.16562294960021973
Batch 56/64 loss: -0.16135519742965698
Batch 57/64 loss: -0.15387427806854248
Batch 58/64 loss: -0.18930277228355408
Batch 59/64 loss: -0.1901029348373413
Batch 60/64 loss: -0.15348392724990845
Batch 61/64 loss: -0.1640700101852417
Batch 62/64 loss: -0.18178433179855347
Batch 63/64 loss: -0.1945585012435913
Batch 64/64 loss: -0.133672833442688
Epoch 438  Train loss: -0.1636099324506872  Val loss: 0.06652581937534292
Epoch 439
-------------------------------
Batch 1/64 loss: -0.17185205221176147
Batch 2/64 loss: -0.16939353942871094
Batch 3/64 loss: -0.15643268823623657
Batch 4/64 loss: -0.2052987813949585
Batch 5/64 loss: -0.17463326454162598
Batch 6/64 loss: -0.20429706573486328
Batch 7/64 loss: -0.18037772178649902
Batch 8/64 loss: -0.18005132675170898
Batch 9/64 loss: -0.1651894450187683
Batch 10/64 loss: -0.14210736751556396
Batch 11/64 loss: -0.16762667894363403
Batch 12/64 loss: -0.17825287580490112
Batch 13/64 loss: -0.21812653541564941
Batch 14/64 loss: -0.16096925735473633
Batch 15/64 loss: -0.15024304389953613
Batch 16/64 loss: -0.15372884273529053
Batch 17/64 loss: -0.21341854333877563
Batch 18/64 loss: -0.10083287954330444
Batch 19/64 loss: -0.18975597620010376
Batch 20/64 loss: -0.16064417362213135
Batch 21/64 loss: -0.1623130440711975
Batch 22/64 loss: -0.11846756935119629
Batch 23/64 loss: -0.15929651260375977
Batch 24/64 loss: -0.1491173505783081
Batch 25/64 loss: -0.16017621755599976
Batch 26/64 loss: -0.16251665353775024
Batch 27/64 loss: -0.19414755702018738
Batch 28/64 loss: -0.12788403034210205
Batch 29/64 loss: -0.15966945886611938
Batch 30/64 loss: -0.17376172542572021
Batch 31/64 loss: -0.1657077670097351
Batch 32/64 loss: -0.16848993301391602
Batch 33/64 loss: -0.17058974504470825
Batch 34/64 loss: -0.16741561889648438
Batch 35/64 loss: -0.15808731317520142
Batch 36/64 loss: -0.15770316123962402
Batch 37/64 loss: -0.17994743585586548
Batch 38/64 loss: -0.21253830194473267
Batch 39/64 loss: -0.14613062143325806
Batch 40/64 loss: -0.17756390571594238
Batch 41/64 loss: -0.19294363260269165
Batch 42/64 loss: -0.16990137100219727
Batch 43/64 loss: -0.16483592987060547
Batch 44/64 loss: -0.1399286985397339
Batch 45/64 loss: -0.16807794570922852
Batch 46/64 loss: -0.19054967164993286
Batch 47/64 loss: -0.19064393639564514
Batch 48/64 loss: -0.16807407140731812
Batch 49/64 loss: -0.1537386178970337
Batch 50/64 loss: -0.14391297101974487
Batch 51/64 loss: -0.1788274645805359
Batch 52/64 loss: -0.15680086612701416
Batch 53/64 loss: -0.17700064182281494
Batch 54/64 loss: -0.14396995306015015
Batch 55/64 loss: -0.15707927942276
Batch 56/64 loss: -0.18276327848434448
Batch 57/64 loss: -0.1459869146347046
Batch 58/64 loss: -0.1521119475364685
Batch 59/64 loss: -0.18337208032608032
Batch 60/64 loss: -0.15108603239059448
Batch 61/64 loss: -0.165838360786438
Batch 62/64 loss: -0.12031280994415283
Batch 63/64 loss: -0.11982113122940063
Batch 64/64 loss: -0.14470189809799194
Epoch 439  Train loss: -0.16534682372037102  Val loss: 0.06676103609943718
Epoch 440
-------------------------------
Batch 1/64 loss: -0.163865327835083
Batch 2/64 loss: -0.1251739263534546
Batch 3/64 loss: -0.1588628888130188
Batch 4/64 loss: -0.17168289422988892
Batch 5/64 loss: -0.16544747352600098
Batch 6/64 loss: -0.15282964706420898
Batch 7/64 loss: -0.13883841037750244
Batch 8/64 loss: -0.16642063856124878
Batch 9/64 loss: -0.18354225158691406
Batch 10/64 loss: -0.1817682981491089
Batch 11/64 loss: -0.1270970106124878
Batch 12/64 loss: -0.13372880220413208
Batch 13/64 loss: -0.11343002319335938
Batch 14/64 loss: -0.1692270040512085
Batch 15/64 loss: -0.1253846287727356
Batch 16/64 loss: -0.2028820812702179
Batch 17/64 loss: -0.11614936590194702
Batch 18/64 loss: -0.19094234704971313
Batch 19/64 loss: -0.1804894208908081
Batch 20/64 loss: -0.11586147546768188
Batch 21/64 loss: -0.16462039947509766
Batch 22/64 loss: -0.1634405255317688
Batch 23/64 loss: -0.187513530254364
Batch 24/64 loss: -0.1682865023612976
Batch 25/64 loss: -0.11939066648483276
Batch 26/64 loss: -0.16949772834777832
Batch 27/64 loss: -0.18193918466567993
Batch 28/64 loss: -0.15890294313430786
Batch 29/64 loss: -0.16702383756637573
Batch 30/64 loss: -0.19916695356369019
Batch 31/64 loss: -0.15905070304870605
Batch 32/64 loss: -0.1886969804763794
Batch 33/64 loss: -0.17451447248458862
Batch 34/64 loss: -0.16863399744033813
Batch 35/64 loss: -0.14167261123657227
Batch 36/64 loss: -0.1596219539642334
Batch 37/64 loss: -0.14539241790771484
Batch 38/64 loss: -0.14528250694274902
Batch 39/64 loss: -0.13889622688293457
Batch 40/64 loss: -0.16662615537643433
Batch 41/64 loss: -0.16845107078552246
Batch 42/64 loss: -0.15484309196472168
Batch 43/64 loss: -0.14855122566223145
Batch 44/64 loss: -0.16919410228729248
Batch 45/64 loss: -0.14909100532531738
Batch 46/64 loss: -0.21095317602157593
Batch 47/64 loss: -0.20549455285072327
Batch 48/64 loss: -0.15961730480194092
Batch 49/64 loss: -0.14094984531402588
Batch 50/64 loss: -0.20036298036575317
Batch 51/64 loss: -0.12573868036270142
Batch 52/64 loss: -0.18600958585739136
Batch 53/64 loss: -0.18526947498321533
Batch 54/64 loss: -0.15867912769317627
Batch 55/64 loss: -0.15930116176605225
Batch 56/64 loss: -0.17048096656799316
Batch 57/64 loss: -0.1799374222755432
Batch 58/64 loss: -0.16867345571517944
Batch 59/64 loss: -0.14003807306289673
Batch 60/64 loss: -0.1429070234298706
Batch 61/64 loss: -0.13899099826812744
Batch 62/64 loss: -0.16940510272979736
Batch 63/64 loss: -0.1397438645362854
Batch 64/64 loss: -0.18146467208862305
Epoch 440  Train loss: -0.1609502119176528  Val loss: 0.06902877694552707
Epoch 441
-------------------------------
Batch 1/64 loss: -0.17657095193862915
Batch 2/64 loss: -0.13179415464401245
Batch 3/64 loss: -0.13672447204589844
Batch 4/64 loss: -0.15351253747940063
Batch 5/64 loss: -0.15104436874389648
Batch 6/64 loss: -0.18280619382858276
Batch 7/64 loss: -0.1784617304801941
Batch 8/64 loss: -0.1748984456062317
Batch 9/64 loss: -0.12368935346603394
Batch 10/64 loss: -0.15021240711212158
Batch 11/64 loss: -0.1697484850883484
Batch 12/64 loss: -0.17072391510009766
Batch 13/64 loss: -0.17760300636291504
Batch 14/64 loss: -0.15802693367004395
Batch 15/64 loss: -0.15888243913650513
Batch 16/64 loss: -0.19495603442192078
Batch 17/64 loss: -0.1923511028289795
Batch 18/64 loss: -0.16444766521453857
Batch 19/64 loss: -0.1590195894241333
Batch 20/64 loss: -0.18438726663589478
Batch 21/64 loss: -0.1843738555908203
Batch 22/64 loss: -0.1571478247642517
Batch 23/64 loss: -0.13638544082641602
Batch 24/64 loss: -0.20468670129776
Batch 25/64 loss: -0.17809778451919556
Batch 26/64 loss: -0.19201058149337769
Batch 27/64 loss: -0.1857566237449646
Batch 28/64 loss: -0.17253649234771729
Batch 29/64 loss: -0.16685211658477783
Batch 30/64 loss: -0.18598395586013794
Batch 31/64 loss: -0.16607016324996948
Batch 32/64 loss: -0.1855049729347229
Batch 33/64 loss: -0.16843163967132568
Batch 34/64 loss: -0.2073100209236145
Batch 35/64 loss: -0.13484108448028564
Batch 36/64 loss: -0.18760251998901367
Batch 37/64 loss: -0.1605079174041748
Batch 38/64 loss: -0.15237915515899658
Batch 39/64 loss: -0.15873032808303833
Batch 40/64 loss: -0.13322782516479492
Batch 41/64 loss: -0.1582040786743164
Batch 42/64 loss: -0.1417229175567627
Batch 43/64 loss: -0.19132599234580994
Batch 44/64 loss: -0.1747000813484192
Batch 45/64 loss: -0.150465726852417
Batch 46/64 loss: -0.1703578233718872
Batch 47/64 loss: -0.16507017612457275
Batch 48/64 loss: -0.15190523862838745
Batch 49/64 loss: -0.1645846962928772
Batch 50/64 loss: -0.15832972526550293
Batch 51/64 loss: -0.13738363981246948
Batch 52/64 loss: -0.16564619541168213
Batch 53/64 loss: -0.15623366832733154
Batch 54/64 loss: -0.1858232617378235
Batch 55/64 loss: -0.15158653259277344
Batch 56/64 loss: -0.14883840084075928
Batch 57/64 loss: -0.15033119916915894
Batch 58/64 loss: -0.16326630115509033
Batch 59/64 loss: -0.17966628074645996
Batch 60/64 loss: -0.16272234916687012
Batch 61/64 loss: -0.14394629001617432
Batch 62/64 loss: -0.17061471939086914
Batch 63/64 loss: -0.1617315411567688
Batch 64/64 loss: -0.17865270376205444
Epoch 441  Train loss: -0.1654391281745013  Val loss: 0.0726178165563603
Epoch 442
-------------------------------
Batch 1/64 loss: -0.1995769739151001
Batch 2/64 loss: -0.1728980541229248
Batch 3/64 loss: -0.18564140796661377
Batch 4/64 loss: -0.1562090516090393
Batch 5/64 loss: -0.16845619678497314
Batch 6/64 loss: -0.1449902057647705
Batch 7/64 loss: -0.146026611328125
Batch 8/64 loss: -0.1813298463821411
Batch 9/64 loss: -0.17558592557907104
Batch 10/64 loss: -0.21128585934638977
Batch 11/64 loss: -0.16580241918563843
Batch 12/64 loss: -0.20383432507514954
Batch 13/64 loss: -0.1561141014099121
Batch 14/64 loss: -0.1882879137992859
Batch 15/64 loss: -0.18788903951644897
Batch 16/64 loss: -0.19682246446609497
Batch 17/64 loss: -0.14519894123077393
Batch 18/64 loss: -0.1692415475845337
Batch 19/64 loss: -0.18721246719360352
Batch 20/64 loss: -0.13499701023101807
Batch 21/64 loss: -0.2051655650138855
Batch 22/64 loss: -0.17353510856628418
Batch 23/64 loss: -0.1678510308265686
Batch 24/64 loss: -0.18370044231414795
Batch 25/64 loss: -0.15632736682891846
Batch 26/64 loss: -0.13586384057998657
Batch 27/64 loss: -0.1423928141593933
Batch 28/64 loss: -0.19183212518692017
Batch 29/64 loss: -0.12971711158752441
Batch 30/64 loss: -0.20021644234657288
Batch 31/64 loss: -0.17903101444244385
Batch 32/64 loss: -0.14115679264068604
Batch 33/64 loss: -0.15108895301818848
Batch 34/64 loss: -0.14717859029769897
Batch 35/64 loss: -0.1479032039642334
Batch 36/64 loss: -0.1665855050086975
Batch 37/64 loss: -0.18642449378967285
Batch 38/64 loss: -0.13585561513900757
Batch 39/64 loss: -0.18968534469604492
Batch 40/64 loss: -0.1808745265007019
Batch 41/64 loss: -0.15990853309631348
Batch 42/64 loss: -0.17125862836837769
Batch 43/64 loss: -0.15501827001571655
Batch 44/64 loss: -0.11430656909942627
Batch 45/64 loss: -0.10813862085342407
Batch 46/64 loss: -0.16374605894088745
Batch 47/64 loss: -0.1835169792175293
Batch 48/64 loss: -0.18156659603118896
Batch 49/64 loss: -0.19308936595916748
Batch 50/64 loss: -0.13315176963806152
Batch 51/64 loss: -0.1662474274635315
Batch 52/64 loss: -0.1892140507698059
Batch 53/64 loss: -0.12255531549453735
Batch 54/64 loss: -0.1917957067489624
Batch 55/64 loss: -0.14944106340408325
Batch 56/64 loss: -0.13647782802581787
Batch 57/64 loss: -0.17122066020965576
Batch 58/64 loss: -0.19521886110305786
Batch 59/64 loss: -0.1962071657180786
Batch 60/64 loss: -0.18330764770507812
Batch 61/64 loss: -0.17756032943725586
Batch 62/64 loss: -0.15087616443634033
Batch 63/64 loss: -0.183385968208313
Batch 64/64 loss: -0.16212743520736694
Epoch 442  Train loss: -0.16766417891371485  Val loss: 0.06560384960928324
Epoch 443
-------------------------------
Batch 1/64 loss: -0.1831609010696411
Batch 2/64 loss: -0.17932510375976562
Batch 3/64 loss: -0.18425273895263672
Batch 4/64 loss: -0.1620883345603943
Batch 5/64 loss: -0.1327161192893982
Batch 6/64 loss: -0.1968863606452942
Batch 7/64 loss: -0.13082963228225708
Batch 8/64 loss: -0.16828888654708862
Batch 9/64 loss: -0.1862282156944275
Batch 10/64 loss: -0.18646103143692017
Batch 11/64 loss: -0.19404315948486328
Batch 12/64 loss: -0.16166061162948608
Batch 13/64 loss: -0.1960117518901825
Batch 14/64 loss: -0.1441037654876709
Batch 15/64 loss: -0.19036751985549927
Batch 16/64 loss: -0.15119892358779907
Batch 17/64 loss: -0.16581732034683228
Batch 18/64 loss: -0.19665300846099854
Batch 19/64 loss: -0.21597987413406372
Batch 20/64 loss: -0.14896559715270996
Batch 21/64 loss: -0.19914525747299194
Batch 22/64 loss: -0.14710402488708496
Batch 23/64 loss: -0.15127980709075928
Batch 24/64 loss: -0.2020362913608551
Batch 25/64 loss: -0.18163102865219116
Batch 26/64 loss: -0.17502951622009277
Batch 27/64 loss: -0.19551384449005127
Batch 28/64 loss: -0.16224539279937744
Batch 29/64 loss: -0.12193983793258667
Batch 30/64 loss: -0.18576675653457642
Batch 31/64 loss: -0.1858786940574646
Batch 32/64 loss: -0.14940643310546875
Batch 33/64 loss: -0.1471349000930786
Batch 34/64 loss: -0.1795704960823059
Batch 35/64 loss: -0.16702347993850708
Batch 36/64 loss: -0.1447904109954834
Batch 37/64 loss: -0.14167547225952148
Batch 38/64 loss: -0.19966435432434082
Batch 39/64 loss: -0.1819065809249878
Batch 40/64 loss: -0.1663016676902771
Batch 41/64 loss: -0.19479036331176758
Batch 42/64 loss: -0.18533217906951904
Batch 43/64 loss: -0.1605074405670166
Batch 44/64 loss: -0.16297364234924316
Batch 45/64 loss: -0.10701560974121094
Batch 46/64 loss: -0.11882948875427246
Batch 47/64 loss: -0.10860466957092285
Batch 48/64 loss: -0.18413043022155762
Batch 49/64 loss: -0.15349936485290527
Batch 50/64 loss: -0.1747225522994995
Batch 51/64 loss: -0.16088926792144775
Batch 52/64 loss: -0.1728287935256958
Batch 53/64 loss: -0.13414716720581055
Batch 54/64 loss: -0.1532185673713684
Batch 55/64 loss: -0.1259879469871521
Batch 56/64 loss: -0.18743902444839478
Batch 57/64 loss: -0.17406219244003296
Batch 58/64 loss: -0.15024560689926147
Batch 59/64 loss: -0.1421998143196106
Batch 60/64 loss: -0.17993128299713135
Batch 61/64 loss: -0.12147772312164307
Batch 62/64 loss: -0.17328518629074097
Batch 63/64 loss: -0.19505751132965088
Batch 64/64 loss: -0.13946491479873657
Epoch 443  Train loss: -0.16605266846862493  Val loss: 0.06930557395174741
Epoch 444
-------------------------------
Batch 1/64 loss: -0.1615445613861084
Batch 2/64 loss: -0.17555606365203857
Batch 3/64 loss: -0.13482379913330078
Batch 4/64 loss: -0.17542892694473267
Batch 5/64 loss: -0.1391361951828003
Batch 6/64 loss: -0.17919045686721802
Batch 7/64 loss: -0.18903779983520508
Batch 8/64 loss: -0.14619088172912598
Batch 9/64 loss: -0.1616426706314087
Batch 10/64 loss: -0.14988797903060913
Batch 11/64 loss: -0.1652669906616211
Batch 12/64 loss: -0.18422579765319824
Batch 13/64 loss: -0.16639173030853271
Batch 14/64 loss: -0.20889580249786377
Batch 15/64 loss: -0.18129050731658936
Batch 16/64 loss: -0.1666153073310852
Batch 17/64 loss: -0.14667648077011108
Batch 18/64 loss: -0.1265108585357666
Batch 19/64 loss: -0.17443698644638062
Batch 20/64 loss: -0.18871378898620605
Batch 21/64 loss: -0.17562198638916016
Batch 22/64 loss: -0.16935253143310547
Batch 23/64 loss: -0.151439368724823
Batch 24/64 loss: -0.15901046991348267
Batch 25/64 loss: -0.1917172074317932
Batch 26/64 loss: -0.18693113327026367
Batch 27/64 loss: -0.1720868945121765
Batch 28/64 loss: -0.16361397504806519
Batch 29/64 loss: -0.18223220109939575
Batch 30/64 loss: -0.1691150665283203
Batch 31/64 loss: -0.1705912947654724
Batch 32/64 loss: -0.1270250678062439
Batch 33/64 loss: -0.1653972864151001
Batch 34/64 loss: -0.17734116315841675
Batch 35/64 loss: -0.18201011419296265
Batch 36/64 loss: -0.17711138725280762
Batch 37/64 loss: -0.1630004644393921
Batch 38/64 loss: -0.1669532060623169
Batch 39/64 loss: -0.17465931177139282
Batch 40/64 loss: -0.1676139235496521
Batch 41/64 loss: -0.16083747148513794
Batch 42/64 loss: -0.15559262037277222
Batch 43/64 loss: -0.15814435482025146
Batch 44/64 loss: -0.13376080989837646
Batch 45/64 loss: -0.13433223962783813
Batch 46/64 loss: -0.1574949026107788
Batch 47/64 loss: -0.1380518674850464
Batch 48/64 loss: -0.19785767793655396
Batch 49/64 loss: -0.16867393255233765
Batch 50/64 loss: -0.15954715013504028
Batch 51/64 loss: -0.18160641193389893
Batch 52/64 loss: -0.14201867580413818
Batch 53/64 loss: -0.13376665115356445
Batch 54/64 loss: -0.1655890941619873
Batch 55/64 loss: -0.16196823120117188
Batch 56/64 loss: -0.16515368223190308
Batch 57/64 loss: -0.1653306484222412
Batch 58/64 loss: -0.14679652452468872
Batch 59/64 loss: -0.19437959790229797
Batch 60/64 loss: -0.20075157284736633
Batch 61/64 loss: -0.16747111082077026
Batch 62/64 loss: -0.19481533765792847
Batch 63/64 loss: -0.07836675643920898
Batch 64/64 loss: -0.1719382405281067
Epoch 444  Train loss: -0.16479291986016667  Val loss: 0.06559628719316725
Epoch 445
-------------------------------
Batch 1/64 loss: -0.17328697443008423
Batch 2/64 loss: -0.1639426350593567
Batch 3/64 loss: -0.181817889213562
Batch 4/64 loss: -0.18238943815231323
Batch 5/64 loss: -0.1745760440826416
Batch 6/64 loss: -0.1681051254272461
Batch 7/64 loss: -0.15106266736984253
Batch 8/64 loss: -0.15762990713119507
Batch 9/64 loss: -0.18323349952697754
Batch 10/64 loss: -0.168709397315979
Batch 11/64 loss: -0.17681121826171875
Batch 12/64 loss: -0.16608649492263794
Batch 13/64 loss: -0.10937947034835815
Batch 14/64 loss: -0.17804980278015137
Batch 15/64 loss: -0.16203278303146362
Batch 16/64 loss: -0.15028798580169678
Batch 17/64 loss: -0.09005606174468994
Batch 18/64 loss: -0.16520804166793823
Batch 19/64 loss: -0.11897706985473633
Batch 20/64 loss: -0.16470682621002197
Batch 21/64 loss: -0.13788962364196777
Batch 22/64 loss: -0.16989028453826904
Batch 23/64 loss: -0.17512917518615723
Batch 24/64 loss: -0.16589224338531494
Batch 25/64 loss: -0.16068553924560547
Batch 26/64 loss: -0.18874827027320862
Batch 27/64 loss: -0.1283198595046997
Batch 28/64 loss: -0.17615985870361328
Batch 29/64 loss: -0.19207018613815308
Batch 30/64 loss: -0.16034597158432007
Batch 31/64 loss: -0.15946125984191895
Batch 32/64 loss: -0.17062515020370483
Batch 33/64 loss: -0.11134004592895508
Batch 34/64 loss: -0.18468618392944336
Batch 35/64 loss: -0.17622947692871094
Batch 36/64 loss: -0.17363417148590088
Batch 37/64 loss: -0.18451574444770813
Batch 38/64 loss: -0.18967056274414062
Batch 39/64 loss: -0.15997189283370972
Batch 40/64 loss: -0.17203450202941895
Batch 41/64 loss: -0.1715632677078247
Batch 42/64 loss: -0.17265987396240234
Batch 43/64 loss: -0.17475301027297974
Batch 44/64 loss: -0.17940962314605713
Batch 45/64 loss: -0.14862054586410522
Batch 46/64 loss: -0.17557352781295776
Batch 47/64 loss: -0.18281948566436768
Batch 48/64 loss: -0.2159045934677124
Batch 49/64 loss: -0.15155041217803955
Batch 50/64 loss: -0.15236449241638184
Batch 51/64 loss: -0.20321208238601685
Batch 52/64 loss: -0.14204227924346924
Batch 53/64 loss: -0.18269240856170654
Batch 54/64 loss: -0.1850295066833496
Batch 55/64 loss: -0.16246014833450317
Batch 56/64 loss: -0.17989420890808105
Batch 57/64 loss: -0.16286003589630127
Batch 58/64 loss: -0.187920480966568
Batch 59/64 loss: -0.12392747402191162
Batch 60/64 loss: -0.17806386947631836
Batch 61/64 loss: -0.06628215312957764
Batch 62/64 loss: -0.13840365409851074
Batch 63/64 loss: -0.1718490719795227
Batch 64/64 loss: -0.19724014401435852
Epoch 445  Train loss: -0.16441467684857985  Val loss: 0.06631979663757115
Epoch 446
-------------------------------
Batch 1/64 loss: -0.17635458707809448
Batch 2/64 loss: -0.2007964849472046
Batch 3/64 loss: -0.16695797443389893
Batch 4/64 loss: -0.18842840194702148
Batch 5/64 loss: -0.18168359994888306
Batch 6/64 loss: -0.14471882581710815
Batch 7/64 loss: -0.15859603881835938
Batch 8/64 loss: -0.1895214319229126
Batch 9/64 loss: -0.16486728191375732
Batch 10/64 loss: -0.19721722602844238
Batch 11/64 loss: -0.1936430037021637
Batch 12/64 loss: -0.20136329531669617
Batch 13/64 loss: -0.16489571332931519
Batch 14/64 loss: -0.15029728412628174
Batch 15/64 loss: -0.17232877016067505
Batch 16/64 loss: -0.10784244537353516
Batch 17/64 loss: -0.2061903476715088
Batch 18/64 loss: -0.13761764764785767
Batch 19/64 loss: -0.1291431188583374
Batch 20/64 loss: -0.07847857475280762
Batch 21/64 loss: -0.1847262978553772
Batch 22/64 loss: -0.19619706273078918
Batch 23/64 loss: -0.17955172061920166
Batch 24/64 loss: -0.11118292808532715
Batch 25/64 loss: -0.21132218837738037
Batch 26/64 loss: -0.10664373636245728
Batch 27/64 loss: -0.17459619045257568
Batch 28/64 loss: -0.16624879837036133
Batch 29/64 loss: -0.139734148979187
Batch 30/64 loss: -0.125247061252594
Batch 31/64 loss: -0.17509323358535767
Batch 32/64 loss: -0.1460222601890564
Batch 33/64 loss: -0.11285364627838135
Batch 34/64 loss: -0.17068946361541748
Batch 35/64 loss: -0.14426076412200928
Batch 36/64 loss: -0.16491705179214478
Batch 37/64 loss: -0.17668026685714722
Batch 38/64 loss: -0.1579720377922058
Batch 39/64 loss: -0.14268451929092407
Batch 40/64 loss: -0.1887621283531189
Batch 41/64 loss: -0.144189715385437
Batch 42/64 loss: -0.17570430040359497
Batch 43/64 loss: -0.1763429045677185
Batch 44/64 loss: -0.18046146631240845
Batch 45/64 loss: -0.21280354261398315
Batch 46/64 loss: -0.1271291971206665
Batch 47/64 loss: -0.195695161819458
Batch 48/64 loss: -0.14881932735443115
Batch 49/64 loss: -0.17193609476089478
Batch 50/64 loss: -0.18179237842559814
Batch 51/64 loss: -0.17295092344284058
Batch 52/64 loss: -0.1391708254814148
Batch 53/64 loss: -0.1817185878753662
Batch 54/64 loss: -0.15469181537628174
Batch 55/64 loss: -0.1538764238357544
Batch 56/64 loss: -0.2056012749671936
Batch 57/64 loss: -0.19588959217071533
Batch 58/64 loss: -0.1654191017150879
Batch 59/64 loss: -0.17211109399795532
Batch 60/64 loss: -0.14406746625900269
Batch 61/64 loss: -0.17547136545181274
Batch 62/64 loss: -0.1448010802268982
Batch 63/64 loss: -0.08027046918869019
Batch 64/64 loss: -0.14649617671966553
Epoch 446  Train loss: -0.16309198117723653  Val loss: 0.06627824134433392
Epoch 447
-------------------------------
Batch 1/64 loss: -0.1936848759651184
Batch 2/64 loss: -0.14263367652893066
Batch 3/64 loss: -0.13878369331359863
Batch 4/64 loss: -0.13363075256347656
Batch 5/64 loss: -0.11766040325164795
Batch 6/64 loss: -0.18772423267364502
Batch 7/64 loss: -0.17382681369781494
Batch 8/64 loss: -0.16584062576293945
Batch 9/64 loss: -0.1446940302848816
Batch 10/64 loss: -0.19466078281402588
Batch 11/64 loss: -0.16531407833099365
Batch 12/64 loss: -0.18356859683990479
Batch 13/64 loss: -0.16218388080596924
Batch 14/64 loss: -0.1851550042629242
Batch 15/64 loss: -0.1871814727783203
Batch 16/64 loss: -0.14385521411895752
Batch 17/64 loss: -0.1541142463684082
Batch 18/64 loss: -0.14996987581253052
Batch 19/64 loss: -0.1681261658668518
Batch 20/64 loss: -0.1800607442855835
Batch 21/64 loss: -0.13183724880218506
Batch 22/64 loss: -0.19207653403282166
Batch 23/64 loss: -0.1844574213027954
Batch 24/64 loss: -0.18023079633712769
Batch 25/64 loss: -0.20063716173171997
Batch 26/64 loss: -0.18617886304855347
Batch 27/64 loss: -0.1316375732421875
Batch 28/64 loss: -0.15082579851150513
Batch 29/64 loss: -0.15820527076721191
Batch 30/64 loss: -0.16416287422180176
Batch 31/64 loss: -0.16511929035186768
Batch 32/64 loss: -0.14325594902038574
Batch 33/64 loss: -0.18379807472229004
Batch 34/64 loss: -0.1764310598373413
Batch 35/64 loss: -0.1285839080810547
Batch 36/64 loss: -0.14629220962524414
Batch 37/64 loss: -0.19469624757766724
Batch 38/64 loss: -0.15503865480422974
Batch 39/64 loss: -0.19578802585601807
Batch 40/64 loss: -0.13194060325622559
Batch 41/64 loss: -0.12621164321899414
Batch 42/64 loss: -0.14973926544189453
Batch 43/64 loss: -0.1439637541770935
Batch 44/64 loss: -0.16784095764160156
Batch 45/64 loss: -0.15697264671325684
Batch 46/64 loss: -0.16783642768859863
Batch 47/64 loss: -0.09752559661865234
Batch 48/64 loss: -0.17985254526138306
Batch 49/64 loss: -0.16834604740142822
Batch 50/64 loss: -0.14527440071105957
Batch 51/64 loss: -0.17561489343643188
Batch 52/64 loss: -0.15338504314422607
Batch 53/64 loss: -0.17829269170761108
Batch 54/64 loss: -0.18987882137298584
Batch 55/64 loss: -0.1941407322883606
Batch 56/64 loss: -0.18302488327026367
Batch 57/64 loss: -0.194771409034729
Batch 58/64 loss: -0.19598686695098877
Batch 59/64 loss: -0.16925835609436035
Batch 60/64 loss: -0.200269877910614
Batch 61/64 loss: -0.179540753364563
Batch 62/64 loss: -0.1757512092590332
Batch 63/64 loss: -0.1815478801727295
Batch 64/64 loss: -0.17950201034545898
Epoch 447  Train loss: -0.16601593634661507  Val loss: 0.06756867924097068
Epoch 448
-------------------------------
Batch 1/64 loss: -0.224232017993927
Batch 2/64 loss: -0.18558645248413086
Batch 3/64 loss: -0.14620864391326904
Batch 4/64 loss: -0.20716887712478638
Batch 5/64 loss: -0.12531501054763794
Batch 6/64 loss: -0.1980801224708557
Batch 7/64 loss: -0.10813480615615845
Batch 8/64 loss: -0.19704052805900574
Batch 9/64 loss: -0.15899032354354858
Batch 10/64 loss: -0.11452126502990723
Batch 11/64 loss: -0.17938679456710815
Batch 12/64 loss: -0.17904990911483765
Batch 13/64 loss: -0.1578582525253296
Batch 14/64 loss: -0.14459764957427979
Batch 15/64 loss: -0.1970466673374176
Batch 16/64 loss: -0.17937028408050537
Batch 17/64 loss: -0.17702728509902954
Batch 18/64 loss: -0.19309669733047485
Batch 19/64 loss: -0.17688769102096558
Batch 20/64 loss: -0.1703941822052002
Batch 21/64 loss: -0.1649484634399414
Batch 22/64 loss: -0.144198477268219
Batch 23/64 loss: -0.1683344841003418
Batch 24/64 loss: -0.17730402946472168
Batch 25/64 loss: -0.18984311819076538
Batch 26/64 loss: -0.21475368738174438
Batch 27/64 loss: -0.17324745655059814
Batch 28/64 loss: -0.138358473777771
Batch 29/64 loss: -0.15498757362365723
Batch 30/64 loss: -0.17495548725128174
Batch 31/64 loss: -0.16699856519699097
Batch 32/64 loss: -0.17009258270263672
Batch 33/64 loss: -0.16254973411560059
Batch 34/64 loss: -0.15761739015579224
Batch 35/64 loss: -0.13882547616958618
Batch 36/64 loss: -0.14415979385375977
Batch 37/64 loss: -0.16140931844711304
Batch 38/64 loss: -0.14224553108215332
Batch 39/64 loss: -0.17700821161270142
Batch 40/64 loss: -0.1890648603439331
Batch 41/64 loss: -0.12674254179000854
Batch 42/64 loss: -0.15025103092193604
Batch 43/64 loss: -0.19302093982696533
Batch 44/64 loss: -0.15929269790649414
Batch 45/64 loss: -0.15929186344146729
Batch 46/64 loss: -0.1292281150817871
Batch 47/64 loss: -0.19498774409294128
Batch 48/64 loss: -0.1385170817375183
Batch 49/64 loss: -0.1399613618850708
Batch 50/64 loss: -0.1795756220817566
Batch 51/64 loss: -0.15101248025894165
Batch 52/64 loss: -0.17434632778167725
Batch 53/64 loss: -0.17362022399902344
Batch 54/64 loss: -0.17632406949996948
Batch 55/64 loss: -0.14462292194366455
Batch 56/64 loss: -0.18041127920150757
Batch 57/64 loss: -0.20685678720474243
Batch 58/64 loss: -0.16071170568466187
Batch 59/64 loss: -0.15594863891601562
Batch 60/64 loss: -0.10809028148651123
Batch 61/64 loss: -0.13691401481628418
Batch 62/64 loss: -0.18753135204315186
Batch 63/64 loss: -0.17670315504074097
Batch 64/64 loss: -0.0984073281288147
Epoch 448  Train loss: -0.1648417867866217  Val loss: 0.06915187753762576
Epoch 449
-------------------------------
Batch 1/64 loss: -0.19785013794898987
Batch 2/64 loss: -0.19071239233016968
Batch 3/64 loss: -0.12461268901824951
Batch 4/64 loss: -0.14003127813339233
Batch 5/64 loss: -0.17786240577697754
Batch 6/64 loss: -0.15947294235229492
Batch 7/64 loss: -0.16532212495803833
Batch 8/64 loss: -0.20963376760482788
Batch 9/64 loss: -0.14620310068130493
Batch 10/64 loss: -0.17660939693450928
Batch 11/64 loss: -0.1913336217403412
Batch 12/64 loss: -0.17579048871994019
Batch 13/64 loss: -0.1654093861579895
Batch 14/64 loss: -0.20174092054367065
Batch 15/64 loss: -0.17553001642227173
Batch 16/64 loss: -0.13859981298446655
Batch 17/64 loss: -0.1290530562400818
Batch 18/64 loss: -0.19222691655158997
Batch 19/64 loss: -0.18989035487174988
Batch 20/64 loss: -0.18072134256362915
Batch 21/64 loss: -0.1346513032913208
Batch 22/64 loss: -0.18237805366516113
Batch 23/64 loss: -0.17367279529571533
Batch 24/64 loss: -0.1183927059173584
Batch 25/64 loss: -0.2029029130935669
Batch 26/64 loss: -0.19052255153656006
Batch 27/64 loss: -0.15616381168365479
Batch 28/64 loss: -0.19481447339057922
Batch 29/64 loss: -0.1876271367073059
Batch 30/64 loss: -0.15183693170547485
Batch 31/64 loss: -0.1763128638267517
Batch 32/64 loss: -0.18774941563606262
Batch 33/64 loss: -0.14668679237365723
Batch 34/64 loss: -0.18161755800247192
Batch 35/64 loss: -0.16737401485443115
Batch 36/64 loss: -0.16821056604385376
Batch 37/64 loss: -0.09877324104309082
Batch 38/64 loss: -0.1770586371421814
Batch 39/64 loss: -0.14851224422454834
Batch 40/64 loss: -0.19376632571220398
Batch 41/64 loss: -0.1359175443649292
Batch 42/64 loss: -0.1471748948097229
Batch 43/64 loss: -0.18131470680236816
Batch 44/64 loss: -0.17748284339904785
Batch 45/64 loss: -0.16546833515167236
Batch 46/64 loss: -0.20212167501449585
Batch 47/64 loss: -0.1683695912361145
Batch 48/64 loss: -0.20014983415603638
Batch 49/64 loss: -0.15331387519836426
Batch 50/64 loss: -0.15641599893569946
Batch 51/64 loss: -0.18515849113464355
Batch 52/64 loss: -0.1714409589767456
Batch 53/64 loss: -0.19680184125900269
Batch 54/64 loss: -0.09962159395217896
Batch 55/64 loss: -0.17938441038131714
Batch 56/64 loss: -0.13622140884399414
Batch 57/64 loss: -0.1583636999130249
Batch 58/64 loss: -0.17770099639892578
Batch 59/64 loss: -0.18725281953811646
Batch 60/64 loss: -0.15537118911743164
Batch 61/64 loss: -0.16272974014282227
Batch 62/64 loss: -0.16866719722747803
Batch 63/64 loss: -0.1378050446510315
Batch 64/64 loss: -0.09661179780960083
Epoch 449  Train loss: -0.1669700082610635  Val loss: 0.07567444474426742
Epoch 450
-------------------------------
Batch 1/64 loss: -0.20317107439041138
Batch 2/64 loss: -0.14300435781478882
Batch 3/64 loss: -0.17810499668121338
Batch 4/64 loss: -0.183560311794281
Batch 5/64 loss: -0.1787436604499817
Batch 6/64 loss: -0.15853071212768555
Batch 7/64 loss: -0.18535500764846802
Batch 8/64 loss: -0.15998566150665283
Batch 9/64 loss: -0.16034364700317383
Batch 10/64 loss: -0.18633759021759033
Batch 11/64 loss: -0.13450056314468384
Batch 12/64 loss: -0.17828369140625
Batch 13/64 loss: -0.15007126331329346
Batch 14/64 loss: -0.19939672946929932
Batch 15/64 loss: -0.1729169487953186
Batch 16/64 loss: -0.23014378547668457
Batch 17/64 loss: -0.16416823863983154
Batch 18/64 loss: -0.20012608170509338
Batch 19/64 loss: -0.16054695844650269
Batch 20/64 loss: -0.14776664972305298
Batch 21/64 loss: -0.19392240047454834
Batch 22/64 loss: -0.175961434841156
Batch 23/64 loss: -0.17724823951721191
Batch 24/64 loss: -0.1443861722946167
Batch 25/64 loss: -0.1797560453414917
Batch 26/64 loss: -0.17923253774642944
Batch 27/64 loss: -0.1354769468307495
Batch 28/64 loss: -0.1703694462776184
Batch 29/64 loss: -0.1188974380493164
Batch 30/64 loss: -0.18800044059753418
Batch 31/64 loss: -0.1471812129020691
Batch 32/64 loss: -0.18433749675750732
Batch 33/64 loss: -0.1867644190788269
Batch 34/64 loss: -0.17005079984664917
Batch 35/64 loss: -0.19589674472808838
Batch 36/64 loss: -0.11885559558868408
Batch 37/64 loss: -0.15796923637390137
Batch 38/64 loss: -0.14701223373413086
Batch 39/64 loss: -0.17671340703964233
Batch 40/64 loss: -0.16425621509552002
Batch 41/64 loss: -0.1567886471748352
Batch 42/64 loss: -0.18110549449920654
Batch 43/64 loss: -0.13885611295700073
Batch 44/64 loss: -0.13627034425735474
Batch 45/64 loss: -0.16691076755523682
Batch 46/64 loss: -0.17262208461761475
Batch 47/64 loss: -0.18110406398773193
Batch 48/64 loss: -0.14242076873779297
Batch 49/64 loss: -0.17733639478683472
Batch 50/64 loss: -0.15319609642028809
Batch 51/64 loss: -0.171902596950531
Batch 52/64 loss: -0.15794312953948975
Batch 53/64 loss: -0.17651081085205078
Batch 54/64 loss: -0.183063805103302
Batch 55/64 loss: -0.15310049057006836
Batch 56/64 loss: -0.20684540271759033
Batch 57/64 loss: -0.20266935229301453
Batch 58/64 loss: -0.17517751455307007
Batch 59/64 loss: -0.11376667022705078
Batch 60/64 loss: -0.14734899997711182
Batch 61/64 loss: -0.17203277349472046
Batch 62/64 loss: -0.1568162441253662
Batch 63/64 loss: -0.17226022481918335
Batch 64/64 loss: -0.2011759877204895
Epoch 450  Train loss: -0.16838081841375313  Val loss: 0.06757504854005636
Epoch 451
-------------------------------
Batch 1/64 loss: -0.15506219863891602
Batch 2/64 loss: -0.15183168649673462
Batch 3/64 loss: -0.17751550674438477
Batch 4/64 loss: -0.17564421892166138
Batch 5/64 loss: -0.1971246600151062
Batch 6/64 loss: -0.13650894165039062
Batch 7/64 loss: -0.11541670560836792
Batch 8/64 loss: -0.20259150862693787
Batch 9/64 loss: -0.20550024509429932
Batch 10/64 loss: -0.21060457825660706
Batch 11/64 loss: -0.13177573680877686
Batch 12/64 loss: -0.16283869743347168
Batch 13/64 loss: -0.1751827597618103
Batch 14/64 loss: -0.2070610523223877
Batch 15/64 loss: -0.13894623517990112
Batch 16/64 loss: -0.18416672945022583
Batch 17/64 loss: -0.19547271728515625
Batch 18/64 loss: -0.19312596321105957
Batch 19/64 loss: -0.19659817218780518
Batch 20/64 loss: -0.1832241415977478
Batch 21/64 loss: -0.18188166618347168
Batch 22/64 loss: -0.19960790872573853
Batch 23/64 loss: -0.1596444845199585
Batch 24/64 loss: -0.1914570927619934
Batch 25/64 loss: -0.1836479902267456
Batch 26/64 loss: -0.17915451526641846
Batch 27/64 loss: -0.14360415935516357
Batch 28/64 loss: -0.15360027551651
Batch 29/64 loss: -0.16908103227615356
Batch 30/64 loss: -0.17137056589126587
Batch 31/64 loss: -0.1628206968307495
Batch 32/64 loss: -0.16237908601760864
Batch 33/64 loss: -0.1735350489616394
Batch 34/64 loss: -0.13674819469451904
Batch 35/64 loss: -0.183538019657135
Batch 36/64 loss: -0.18833434581756592
Batch 37/64 loss: -0.18510040640830994
Batch 38/64 loss: -0.1374492645263672
Batch 39/64 loss: -0.16640150547027588
Batch 40/64 loss: -0.18097782135009766
Batch 41/64 loss: -0.159373939037323
Batch 42/64 loss: -0.16999053955078125
Batch 43/64 loss: -0.1545710563659668
Batch 44/64 loss: -0.18557250499725342
Batch 45/64 loss: -0.16016435623168945
Batch 46/64 loss: -0.13852417469024658
Batch 47/64 loss: -0.20028778910636902
Batch 48/64 loss: -0.16204524040222168
Batch 49/64 loss: -0.1272345781326294
Batch 50/64 loss: -0.12483805418014526
Batch 51/64 loss: -0.17193031311035156
Batch 52/64 loss: -0.14739221334457397
Batch 53/64 loss: -0.16885900497436523
Batch 54/64 loss: -0.18336105346679688
Batch 55/64 loss: -0.19330456852912903
Batch 56/64 loss: -0.19931060075759888
Batch 57/64 loss: -0.16138839721679688
Batch 58/64 loss: -0.16787934303283691
Batch 59/64 loss: -0.17031657695770264
Batch 60/64 loss: -0.16798889636993408
Batch 61/64 loss: -0.15269696712493896
Batch 62/64 loss: -0.18783831596374512
Batch 63/64 loss: -0.18092137575149536
Batch 64/64 loss: -0.18095284700393677
Epoch 451  Train loss: -0.1706357808674083  Val loss: 0.07328001699087136
Epoch 452
-------------------------------
Batch 1/64 loss: -0.1620137095451355
Batch 2/64 loss: -0.18612176179885864
Batch 3/64 loss: -0.18157172203063965
Batch 4/64 loss: -0.16840696334838867
Batch 5/64 loss: -0.16675353050231934
Batch 6/64 loss: -0.18656975030899048
Batch 7/64 loss: -0.20934444665908813
Batch 8/64 loss: -0.12100625038146973
Batch 9/64 loss: -0.20275551080703735
Batch 10/64 loss: -0.2143620252609253
Batch 11/64 loss: -0.1502476930618286
Batch 12/64 loss: -0.18043309450149536
Batch 13/64 loss: -0.20034217834472656
Batch 14/64 loss: -0.15021777153015137
Batch 15/64 loss: -0.20206129550933838
Batch 16/64 loss: -0.18409520387649536
Batch 17/64 loss: -0.18190723657608032
Batch 18/64 loss: -0.14714550971984863
Batch 19/64 loss: -0.1268482208251953
Batch 20/64 loss: -0.17074906826019287
Batch 21/64 loss: -0.153317391872406
Batch 22/64 loss: -0.17050707340240479
Batch 23/64 loss: -0.14625310897827148
Batch 24/64 loss: -0.1456865668296814
Batch 25/64 loss: -0.175750732421875
Batch 26/64 loss: -0.1735418438911438
Batch 27/64 loss: -0.2034378945827484
Batch 28/64 loss: -0.14679455757141113
Batch 29/64 loss: -0.18645939230918884
Batch 30/64 loss: -0.19397452473640442
Batch 31/64 loss: -0.12686800956726074
Batch 32/64 loss: -0.142750084400177
Batch 33/64 loss: -0.17275726795196533
Batch 34/64 loss: -0.11834609508514404
Batch 35/64 loss: -0.1522362232208252
Batch 36/64 loss: -0.17925900220870972
Batch 37/64 loss: -0.13924002647399902
Batch 38/64 loss: -0.17523235082626343
Batch 39/64 loss: -0.15477335453033447
Batch 40/64 loss: -0.12057209014892578
Batch 41/64 loss: -0.17268896102905273
Batch 42/64 loss: -0.17657244205474854
Batch 43/64 loss: -0.1752622127532959
Batch 44/64 loss: -0.1731927990913391
Batch 45/64 loss: -0.1615966558456421
Batch 46/64 loss: -0.18091827630996704
Batch 47/64 loss: -0.16448932886123657
Batch 48/64 loss: -0.19932818412780762
Batch 49/64 loss: -0.21485400199890137
Batch 50/64 loss: -0.19880908727645874
Batch 51/64 loss: -0.172804057598114
Batch 52/64 loss: -0.1486847996711731
Batch 53/64 loss: -0.12066972255706787
Batch 54/64 loss: -0.1645488142967224
Batch 55/64 loss: -0.15385174751281738
Batch 56/64 loss: -0.1328970193862915
Batch 57/64 loss: -0.17977911233901978
Batch 58/64 loss: -0.17111510038375854
Batch 59/64 loss: -0.18098294734954834
Batch 60/64 loss: -0.11683851480484009
Batch 61/64 loss: -0.16157108545303345
Batch 62/64 loss: -0.19216972589492798
Batch 63/64 loss: -0.13500303030014038
Batch 64/64 loss: -0.19252946972846985
Epoch 452  Train loss: -0.1672742786360722  Val loss: 0.06575519952577413
Epoch 453
-------------------------------
Batch 1/64 loss: -0.17971068620681763
Batch 2/64 loss: -0.19085121154785156
Batch 3/64 loss: -0.20773518085479736
Batch 4/64 loss: -0.15560072660446167
Batch 5/64 loss: -0.20481550693511963
Batch 6/64 loss: -0.19493988156318665
Batch 7/64 loss: -0.19063609838485718
Batch 8/64 loss: -0.14745402336120605
Batch 9/64 loss: -0.1357749104499817
Batch 10/64 loss: -0.1622105836868286
Batch 11/64 loss: -0.1980433464050293
Batch 12/64 loss: -0.13649439811706543
Batch 13/64 loss: -0.1810186505317688
Batch 14/64 loss: -0.1503446102142334
Batch 15/64 loss: -0.1711098551750183
Batch 16/64 loss: -0.2003244161605835
Batch 17/64 loss: -0.1984967589378357
Batch 18/64 loss: -0.13810616731643677
Batch 19/64 loss: -0.19424229860305786
Batch 20/64 loss: -0.15183603763580322
Batch 21/64 loss: -0.2005218267440796
Batch 22/64 loss: -0.16027337312698364
Batch 23/64 loss: -0.15610671043395996
Batch 24/64 loss: -0.09375065565109253
Batch 25/64 loss: -0.14919209480285645
Batch 26/64 loss: -0.13782763481140137
Batch 27/64 loss: -0.19372543692588806
Batch 28/64 loss: -0.0980939269065857
Batch 29/64 loss: -0.14901018142700195
Batch 30/64 loss: -0.1565701961517334
Batch 31/64 loss: -0.1640494465827942
Batch 32/64 loss: -0.17199105024337769
Batch 33/64 loss: -0.143876850605011
Batch 34/64 loss: -0.16474199295043945
Batch 35/64 loss: -0.18204158544540405
Batch 36/64 loss: -0.18141162395477295
Batch 37/64 loss: -0.15936338901519775
Batch 38/64 loss: -0.14793860912322998
Batch 39/64 loss: -0.16887545585632324
Batch 40/64 loss: -0.1645352840423584
Batch 41/64 loss: -0.20113492012023926
Batch 42/64 loss: -0.1778165102005005
Batch 43/64 loss: -0.1592997908592224
Batch 44/64 loss: -0.17922133207321167
Batch 45/64 loss: -0.19630390405654907
Batch 46/64 loss: -0.17363804578781128
Batch 47/64 loss: -0.21657699346542358
Batch 48/64 loss: -0.16180294752120972
Batch 49/64 loss: -0.17266035079956055
Batch 50/64 loss: -0.13841533660888672
Batch 51/64 loss: -0.17038702964782715
Batch 52/64 loss: -0.16618311405181885
Batch 53/64 loss: -0.19895261526107788
Batch 54/64 loss: -0.14373993873596191
Batch 55/64 loss: -0.17644411325454712
Batch 56/64 loss: -0.213165283203125
Batch 57/64 loss: -0.19889169931411743
Batch 58/64 loss: -0.18692922592163086
Batch 59/64 loss: -0.14325296878814697
Batch 60/64 loss: -0.14468860626220703
Batch 61/64 loss: -0.21560066938400269
Batch 62/64 loss: -0.1713862419128418
Batch 63/64 loss: -0.11552834510803223
Batch 64/64 loss: -0.13880383968353271
Epoch 453  Train loss: -0.16878063585243974  Val loss: 0.0663410967977596
Epoch 454
-------------------------------
Batch 1/64 loss: -0.1859307885169983
Batch 2/64 loss: -0.1849232316017151
Batch 3/64 loss: -0.17973631620407104
Batch 4/64 loss: -0.12311184406280518
Batch 5/64 loss: -0.15122133493423462
Batch 6/64 loss: -0.15368342399597168
Batch 7/64 loss: -0.20142418146133423
Batch 8/64 loss: -0.09418672323226929
Batch 9/64 loss: -0.17111003398895264
Batch 10/64 loss: -0.19082200527191162
Batch 11/64 loss: -0.14288222789764404
Batch 12/64 loss: -0.16222703456878662
Batch 13/64 loss: -0.1563834547996521
Batch 14/64 loss: -0.16624587774276733
Batch 15/64 loss: -0.17725300788879395
Batch 16/64 loss: -0.21159958839416504
Batch 17/64 loss: -0.19511032104492188
Batch 18/64 loss: -0.17788583040237427
Batch 19/64 loss: -0.19048842787742615
Batch 20/64 loss: -0.15548259019851685
Batch 21/64 loss: -0.15174341201782227
Batch 22/64 loss: -0.16553246974945068
Batch 23/64 loss: -0.20561477541923523
Batch 24/64 loss: -0.15360629558563232
Batch 25/64 loss: -0.18346798419952393
Batch 26/64 loss: -0.16029256582260132
Batch 27/64 loss: -0.09089928865432739
Batch 28/64 loss: -0.11247223615646362
Batch 29/64 loss: -0.16009920835494995
Batch 30/64 loss: -0.19633561372756958
Batch 31/64 loss: -0.08967667818069458
Batch 32/64 loss: -0.17132824659347534
Batch 33/64 loss: -0.1497170329093933
Batch 34/64 loss: -0.17141258716583252
Batch 35/64 loss: -0.1409737467765808
Batch 36/64 loss: -0.2147449254989624
Batch 37/64 loss: -0.1801714301109314
Batch 38/64 loss: -0.18618303537368774
Batch 39/64 loss: -0.21220287680625916
Batch 40/64 loss: -0.15908312797546387
Batch 41/64 loss: -0.15690326690673828
Batch 42/64 loss: -0.18691259622573853
Batch 43/64 loss: -0.21683913469314575
Batch 44/64 loss: -0.14305472373962402
Batch 45/64 loss: -0.20900407433509827
Batch 46/64 loss: -0.10983788967132568
Batch 47/64 loss: -0.18627172708511353
Batch 48/64 loss: -0.19057482481002808
Batch 49/64 loss: -0.20098233222961426
Batch 50/64 loss: -0.1678498387336731
Batch 51/64 loss: -0.20468920469284058
Batch 52/64 loss: -0.1466660499572754
Batch 53/64 loss: -0.17713993787765503
Batch 54/64 loss: -0.1740661859512329
Batch 55/64 loss: -0.1693902611732483
Batch 56/64 loss: -0.1701439619064331
Batch 57/64 loss: -0.18504035472869873
Batch 58/64 loss: -0.14412719011306763
Batch 59/64 loss: -0.19573521614074707
Batch 60/64 loss: -0.15207701921463013
Batch 61/64 loss: -0.1616053581237793
Batch 62/64 loss: -0.20201271772384644
Batch 63/64 loss: -0.1790105700492859
Batch 64/64 loss: -0.17643362283706665
Epoch 454  Train loss: -0.16924745779411465  Val loss: 0.0662602034631054
Epoch 455
-------------------------------
Batch 1/64 loss: -0.14269423484802246
Batch 2/64 loss: -0.1364835500717163
Batch 3/64 loss: -0.1919596791267395
Batch 4/64 loss: -0.21446922421455383
Batch 5/64 loss: -0.15645313262939453
Batch 6/64 loss: -0.19780313968658447
Batch 7/64 loss: -0.16409766674041748
Batch 8/64 loss: -0.1348496675491333
Batch 9/64 loss: -0.16858774423599243
Batch 10/64 loss: -0.20174077153205872
Batch 11/64 loss: -0.1838362216949463
Batch 12/64 loss: -0.17526894807815552
Batch 13/64 loss: -0.13965392112731934
Batch 14/64 loss: -0.1774635910987854
Batch 15/64 loss: -0.19386345148086548
Batch 16/64 loss: -0.18043416738510132
Batch 17/64 loss: -0.18093812465667725
Batch 18/64 loss: -0.13261198997497559
Batch 19/64 loss: -0.19177472591400146
Batch 20/64 loss: -0.19994813203811646
Batch 21/64 loss: -0.19278991222381592
Batch 22/64 loss: -0.17921221256256104
Batch 23/64 loss: -0.19057610630989075
Batch 24/64 loss: -0.15253114700317383
Batch 25/64 loss: -0.14783120155334473
Batch 26/64 loss: -0.1839662790298462
Batch 27/64 loss: -0.18376246094703674
Batch 28/64 loss: -0.14612704515457153
Batch 29/64 loss: -0.12787258625030518
Batch 30/64 loss: -0.1963600516319275
Batch 31/64 loss: -0.1885601282119751
Batch 32/64 loss: -0.17197245359420776
Batch 33/64 loss: -0.20815116167068481
Batch 34/64 loss: -0.1985263228416443
Batch 35/64 loss: -0.17711210250854492
Batch 36/64 loss: -0.16623681783676147
Batch 37/64 loss: -0.18617737293243408
Batch 38/64 loss: -0.13076049089431763
Batch 39/64 loss: -0.1866334080696106
Batch 40/64 loss: -0.12458086013793945
Batch 41/64 loss: -0.20172154903411865
Batch 42/64 loss: -0.17112839221954346
Batch 43/64 loss: -0.15759485960006714
Batch 44/64 loss: -0.12673521041870117
Batch 45/64 loss: -0.20108044147491455
Batch 46/64 loss: -0.2050461769104004
Batch 47/64 loss: -0.17759239673614502
Batch 48/64 loss: -0.1272384524345398
Batch 49/64 loss: -0.15775370597839355
Batch 50/64 loss: -0.15235841274261475
Batch 51/64 loss: -0.17763859033584595
Batch 52/64 loss: -0.14118605852127075
Batch 53/64 loss: -0.1631062626838684
Batch 54/64 loss: -0.16751933097839355
Batch 55/64 loss: -0.16197079420089722
Batch 56/64 loss: -0.17629164457321167
Batch 57/64 loss: -0.20208516716957092
Batch 58/64 loss: -0.16294193267822266
Batch 59/64 loss: -0.18556302785873413
Batch 60/64 loss: -0.14453750848770142
Batch 61/64 loss: -0.17614060640335083
Batch 62/64 loss: -0.19134390354156494
Batch 63/64 loss: -0.12681818008422852
Batch 64/64 loss: -0.17319965362548828
Epoch 455  Train loss: -0.17082297334484026  Val loss: 0.06883645160091702
Epoch 456
-------------------------------
Batch 1/64 loss: -0.14972108602523804
Batch 2/64 loss: -0.18650519847869873
Batch 3/64 loss: -0.1756652593612671
Batch 4/64 loss: -0.17165100574493408
Batch 5/64 loss: -0.1734147071838379
Batch 6/64 loss: -0.19995790719985962
Batch 7/64 loss: -0.190964937210083
Batch 8/64 loss: -0.18309366703033447
Batch 9/64 loss: -0.16787272691726685
Batch 10/64 loss: -0.17463970184326172
Batch 11/64 loss: -0.10610854625701904
Batch 12/64 loss: -0.1508495807647705
Batch 13/64 loss: -0.19817835092544556
Batch 14/64 loss: -0.181840181350708
Batch 15/64 loss: -0.16023623943328857
Batch 16/64 loss: -0.18908724188804626
Batch 17/64 loss: -0.15348678827285767
Batch 18/64 loss: -0.1911657452583313
Batch 19/64 loss: -0.1592753529548645
Batch 20/64 loss: -0.1662992238998413
Batch 21/64 loss: -0.17559003829956055
Batch 22/64 loss: -0.16490411758422852
Batch 23/64 loss: -0.20231550931930542
Batch 24/64 loss: -0.12671047449111938
Batch 25/64 loss: -0.16412878036499023
Batch 26/64 loss: -0.1809406280517578
Batch 27/64 loss: -0.1306135058403015
Batch 28/64 loss: -0.17606043815612793
Batch 29/64 loss: -0.16801106929779053
Batch 30/64 loss: -0.1669173240661621
Batch 31/64 loss: -0.16662538051605225
Batch 32/64 loss: -0.06996738910675049
Batch 33/64 loss: -0.17018866539001465
Batch 34/64 loss: -0.1662890911102295
Batch 35/64 loss: -0.13550400733947754
Batch 36/64 loss: -0.20157569646835327
Batch 37/64 loss: -0.19319164752960205
Batch 38/64 loss: -0.15493106842041016
Batch 39/64 loss: -0.21003937721252441
Batch 40/64 loss: -0.14353013038635254
Batch 41/64 loss: -0.14397883415222168
Batch 42/64 loss: -0.15828663110733032
Batch 43/64 loss: -0.1890028715133667
Batch 44/64 loss: -0.21111249923706055
Batch 45/64 loss: -0.13787561655044556
Batch 46/64 loss: -0.18691915273666382
Batch 47/64 loss: -0.17551052570343018
Batch 48/64 loss: -0.13673311471939087
Batch 49/64 loss: -0.17113816738128662
Batch 50/64 loss: -0.17149245738983154
Batch 51/64 loss: -0.16363418102264404
Batch 52/64 loss: -0.2066689133644104
Batch 53/64 loss: -0.13226652145385742
Batch 54/64 loss: -0.19119781255722046
Batch 55/64 loss: -0.19696828722953796
Batch 56/64 loss: -0.1598498821258545
Batch 57/64 loss: -0.1771336793899536
Batch 58/64 loss: -0.17923402786254883
Batch 59/64 loss: -0.17915505170822144
Batch 60/64 loss: -0.15454113483428955
Batch 61/64 loss: -0.16003739833831787
Batch 62/64 loss: -0.1685476303100586
Batch 63/64 loss: -0.20082074403762817
Batch 64/64 loss: -0.14587408304214478
Epoch 456  Train loss: -0.16877738798365874  Val loss: 0.0687136899974338
Epoch 457
-------------------------------
Batch 1/64 loss: -0.12112915515899658
Batch 2/64 loss: -0.20520246028900146
Batch 3/64 loss: -0.16429084539413452
Batch 4/64 loss: -0.1992836594581604
Batch 5/64 loss: -0.2047024965286255
Batch 6/64 loss: -0.1519961953163147
Batch 7/64 loss: -0.17392206192016602
Batch 8/64 loss: -0.18248367309570312
Batch 9/64 loss: -0.14723938703536987
Batch 10/64 loss: -0.18841469287872314
Batch 11/64 loss: -0.1941608190536499
Batch 12/64 loss: -0.148281991481781
Batch 13/64 loss: -0.15847033262252808
Batch 14/64 loss: -0.1742159128189087
Batch 15/64 loss: -0.19313818216323853
Batch 16/64 loss: -0.13256299495697021
Batch 17/64 loss: -0.15545392036437988
Batch 18/64 loss: -0.19965732097625732
Batch 19/64 loss: -0.16119647026062012
Batch 20/64 loss: -0.20260733366012573
Batch 21/64 loss: -0.19389820098876953
Batch 22/64 loss: -0.15730679035186768
Batch 23/64 loss: -0.18721014261245728
Batch 24/64 loss: -0.1489579677581787
Batch 25/64 loss: -0.17297834157943726
Batch 26/64 loss: -0.1451585292816162
Batch 27/64 loss: -0.15121030807495117
Batch 28/64 loss: -0.20642554759979248
Batch 29/64 loss: -0.1560513973236084
Batch 30/64 loss: -0.17751741409301758
Batch 31/64 loss: -0.18516838550567627
Batch 32/64 loss: -0.13713085651397705
Batch 33/64 loss: -0.1749042272567749
Batch 34/64 loss: -0.14665812253952026
Batch 35/64 loss: -0.14988327026367188
Batch 36/64 loss: -0.18232035636901855
Batch 37/64 loss: -0.18199586868286133
Batch 38/64 loss: -0.16305696964263916
Batch 39/64 loss: -0.16014301776885986
Batch 40/64 loss: -0.1695830225944519
Batch 41/64 loss: -0.16085094213485718
Batch 42/64 loss: -0.1457388997077942
Batch 43/64 loss: -0.18914127349853516
Batch 44/64 loss: -0.1948527693748474
Batch 45/64 loss: -0.14311736822128296
Batch 46/64 loss: -0.18266308307647705
Batch 47/64 loss: -0.20558708906173706
Batch 48/64 loss: -0.17183023691177368
Batch 49/64 loss: -0.13599157333374023
Batch 50/64 loss: -0.10798829793930054
Batch 51/64 loss: -0.18927153944969177
Batch 52/64 loss: -0.1635533571243286
Batch 53/64 loss: -0.17543017864227295
Batch 54/64 loss: -0.1763858199119568
Batch 55/64 loss: -0.18575513362884521
Batch 56/64 loss: -0.14929437637329102
Batch 57/64 loss: -0.16098850965499878
Batch 58/64 loss: -0.1656610369682312
Batch 59/64 loss: -0.1980905830860138
Batch 60/64 loss: -0.14261776208877563
Batch 61/64 loss: -0.21644598245620728
Batch 62/64 loss: -0.18454402685165405
Batch 63/64 loss: -0.18592911958694458
Batch 64/64 loss: -0.14080345630645752
Epoch 457  Train loss: -0.1700909834282071  Val loss: 0.06610490059115223
Epoch 458
-------------------------------
Batch 1/64 loss: -0.15549272298812866
Batch 2/64 loss: -0.19055169820785522
Batch 3/64 loss: -0.1694185733795166
Batch 4/64 loss: -0.17440158128738403
Batch 5/64 loss: -0.16798275709152222
Batch 6/64 loss: -0.1733914613723755
Batch 7/64 loss: -0.15875506401062012
Batch 8/64 loss: -0.16047042608261108
Batch 9/64 loss: -0.15187078714370728
Batch 10/64 loss: -0.16346436738967896
Batch 11/64 loss: -0.20866915583610535
Batch 12/64 loss: -0.17566823959350586
Batch 13/64 loss: -0.17072898149490356
Batch 14/64 loss: -0.1539478302001953
Batch 15/64 loss: -0.17573833465576172
Batch 16/64 loss: -0.20337209105491638
Batch 17/64 loss: -0.21067282557487488
Batch 18/64 loss: -0.164259135723114
Batch 19/64 loss: -0.14456135034561157
Batch 20/64 loss: -0.14005684852600098
Batch 21/64 loss: -0.14823412895202637
Batch 22/64 loss: -0.2042517364025116
Batch 23/64 loss: -0.17989760637283325
Batch 24/64 loss: -0.12671637535095215
Batch 25/64 loss: -0.18995335698127747
Batch 26/64 loss: -0.17700237035751343
Batch 27/64 loss: -0.14249879121780396
Batch 28/64 loss: -0.1804523468017578
Batch 29/64 loss: -0.1407659649848938
Batch 30/64 loss: -0.18019503355026245
Batch 31/64 loss: -0.14977020025253296
Batch 32/64 loss: -0.16360199451446533
Batch 33/64 loss: -0.16875267028808594
Batch 34/64 loss: -0.1744145154953003
Batch 35/64 loss: -0.17218762636184692
Batch 36/64 loss: -0.162905752658844
Batch 37/64 loss: -0.15877854824066162
Batch 38/64 loss: -0.16691046953201294
Batch 39/64 loss: -0.21264150738716125
Batch 40/64 loss: -0.1698731780052185
Batch 41/64 loss: -0.1837608814239502
Batch 42/64 loss: -0.16220825910568237
Batch 43/64 loss: -0.20071333646774292
Batch 44/64 loss: -0.1764671802520752
Batch 45/64 loss: -0.15504807233810425
Batch 46/64 loss: -0.18402647972106934
Batch 47/64 loss: -0.20259222388267517
Batch 48/64 loss: -0.16886651515960693
Batch 49/64 loss: -0.17318272590637207
Batch 50/64 loss: -0.1831064224243164
Batch 51/64 loss: -0.19564300775527954
Batch 52/64 loss: -0.16843640804290771
Batch 53/64 loss: -0.12795394659042358
Batch 54/64 loss: -0.1601899266242981
Batch 55/64 loss: -0.1609574556350708
Batch 56/64 loss: -0.18139463663101196
Batch 57/64 loss: -0.1499335765838623
Batch 58/64 loss: -0.19913876056671143
Batch 59/64 loss: -0.18348091840744019
Batch 60/64 loss: -0.1749436855316162
Batch 61/64 loss: -0.16194599866867065
Batch 62/64 loss: -0.2065001130104065
Batch 63/64 loss: -0.15922915935516357
Batch 64/64 loss: -0.1638590693473816
Epoch 458  Train loss: -0.17138654741586423  Val loss: 0.0809670959141656
Epoch 459
-------------------------------
Batch 1/64 loss: -0.2051813304424286
Batch 2/64 loss: -0.1815413236618042
Batch 3/64 loss: -0.20582979917526245
Batch 4/64 loss: -0.19495883584022522
Batch 5/64 loss: -0.1969049572944641
Batch 6/64 loss: -0.15227991342544556
Batch 7/64 loss: -0.1982102394104004
Batch 8/64 loss: -0.20193088054656982
Batch 9/64 loss: -0.1757420301437378
Batch 10/64 loss: -0.1969926357269287
Batch 11/64 loss: -0.17066043615341187
Batch 12/64 loss: -0.14097994565963745
Batch 13/64 loss: -0.1850954294204712
Batch 14/64 loss: -0.2073458433151245
Batch 15/64 loss: -0.15789884328842163
Batch 16/64 loss: -0.15497314929962158
Batch 17/64 loss: -0.1485217809677124
Batch 18/64 loss: -0.1961185336112976
Batch 19/64 loss: -0.13538587093353271
Batch 20/64 loss: -0.1748703122138977
Batch 21/64 loss: -0.19796347618103027
Batch 22/64 loss: -0.15369075536727905
Batch 23/64 loss: -0.2027294933795929
Batch 24/64 loss: -0.16188186407089233
Batch 25/64 loss: -0.20468753576278687
Batch 26/64 loss: -0.16381913423538208
Batch 27/64 loss: -0.13308149576187134
Batch 28/64 loss: -0.1776764988899231
Batch 29/64 loss: -0.19627398252487183
Batch 30/64 loss: -0.20436957478523254
Batch 31/64 loss: -0.18005043268203735
Batch 32/64 loss: -0.1775112748146057
Batch 33/64 loss: -0.18557405471801758
Batch 34/64 loss: -0.17913377285003662
Batch 35/64 loss: -0.17435181140899658
Batch 36/64 loss: -0.1372188925743103
Batch 37/64 loss: -0.19829553365707397
Batch 38/64 loss: -0.12235140800476074
Batch 39/64 loss: -0.174066424369812
Batch 40/64 loss: -0.13613003492355347
Batch 41/64 loss: -0.1794987916946411
Batch 42/64 loss: -0.10104334354400635
Batch 43/64 loss: -0.17713433504104614
Batch 44/64 loss: -0.13623100519180298
Batch 45/64 loss: -0.1814197301864624
Batch 46/64 loss: -0.1563895344734192
Batch 47/64 loss: -0.1511496901512146
Batch 48/64 loss: -0.16723763942718506
Batch 49/64 loss: -0.13323938846588135
Batch 50/64 loss: -0.18165111541748047
Batch 51/64 loss: -0.14419394731521606
Batch 52/64 loss: -0.17086750268936157
Batch 53/64 loss: -0.17913007736206055
Batch 54/64 loss: -0.160361647605896
Batch 55/64 loss: -0.1679210066795349
Batch 56/64 loss: -0.17101842164993286
Batch 57/64 loss: -0.20155972242355347
Batch 58/64 loss: -0.1771487593650818
Batch 59/64 loss: -0.1879110038280487
Batch 60/64 loss: -0.16879653930664062
Batch 61/64 loss: -0.14296364784240723
Batch 62/64 loss: -0.16732168197631836
Batch 63/64 loss: -0.1725051999092102
Batch 64/64 loss: -0.14963537454605103
Epoch 459  Train loss: -0.17146980084624944  Val loss: 0.06644735446910269
Epoch 460
-------------------------------
Batch 1/64 loss: -0.17585474252700806
Batch 2/64 loss: -0.17849135398864746
Batch 3/64 loss: -0.19311130046844482
Batch 4/64 loss: -0.13921940326690674
Batch 5/64 loss: -0.17329823970794678
Batch 6/64 loss: -0.20860037207603455
Batch 7/64 loss: -0.1883230209350586
Batch 8/64 loss: -0.17580968141555786
Batch 9/64 loss: -0.12590956687927246
Batch 10/64 loss: -0.18123376369476318
Batch 11/64 loss: -0.1466379165649414
Batch 12/64 loss: -0.21846717596054077
Batch 13/64 loss: -0.1844170093536377
Batch 14/64 loss: -0.14983290433883667
Batch 15/64 loss: -0.16821503639221191
Batch 16/64 loss: -0.1544138789176941
Batch 17/64 loss: -0.18526104092597961
Batch 18/64 loss: -0.18795853853225708
Batch 19/64 loss: -0.1288316249847412
Batch 20/64 loss: -0.17269456386566162
Batch 21/64 loss: -0.1976025402545929
Batch 22/64 loss: -0.17322832345962524
Batch 23/64 loss: -0.15635931491851807
Batch 24/64 loss: -0.19525033235549927
Batch 25/64 loss: -0.1902320384979248
Batch 26/64 loss: -0.1343705654144287
Batch 27/64 loss: -0.12495696544647217
Batch 28/64 loss: -0.19696617126464844
Batch 29/64 loss: -0.20278692245483398
Batch 30/64 loss: -0.13376379013061523
Batch 31/64 loss: -0.1655023694038391
Batch 32/64 loss: -0.19606047868728638
Batch 33/64 loss: -0.20455211400985718
Batch 34/64 loss: -0.16406089067459106
Batch 35/64 loss: -0.18366724252700806
Batch 36/64 loss: -0.15790283679962158
Batch 37/64 loss: -0.12912476062774658
Batch 38/64 loss: -0.16221904754638672
Batch 39/64 loss: -0.19590681791305542
Batch 40/64 loss: -0.17843222618103027
Batch 41/64 loss: -0.1744539737701416
Batch 42/64 loss: -0.17581665515899658
Batch 43/64 loss: -0.14909416437149048
Batch 44/64 loss: -0.15497976541519165
Batch 45/64 loss: -0.18613308668136597
Batch 46/64 loss: -0.16229933500289917
Batch 47/64 loss: -0.19053685665130615
Batch 48/64 loss: -0.16681182384490967
Batch 49/64 loss: -0.18371087312698364
Batch 50/64 loss: -0.18150323629379272
Batch 51/64 loss: -0.1864912509918213
Batch 52/64 loss: -0.18218934535980225
Batch 53/64 loss: -0.11731463670730591
Batch 54/64 loss: -0.18140435218811035
Batch 55/64 loss: -0.19122329354286194
Batch 56/64 loss: -0.1781654953956604
Batch 57/64 loss: -0.19826826453208923
Batch 58/64 loss: -0.12312972545623779
Batch 59/64 loss: -0.15930277109146118
Batch 60/64 loss: -0.13116943836212158
Batch 61/64 loss: -0.1673080325126648
Batch 62/64 loss: -0.19339430332183838
Batch 63/64 loss: -0.18095183372497559
Batch 64/64 loss: -0.13365280628204346
Epoch 460  Train loss: -0.17090853335810643  Val loss: 0.06526821011939819
Epoch 461
-------------------------------
Batch 1/64 loss: -0.20683777332305908
Batch 2/64 loss: -0.14505016803741455
Batch 3/64 loss: -0.18995508551597595
Batch 4/64 loss: -0.14314168691635132
Batch 5/64 loss: -0.18328779935836792
Batch 6/64 loss: -0.13957738876342773
Batch 7/64 loss: -0.18628454208374023
Batch 8/64 loss: -0.19965419173240662
Batch 9/64 loss: -0.19698980450630188
Batch 10/64 loss: -0.1720207929611206
Batch 11/64 loss: -0.20036351680755615
Batch 12/64 loss: -0.1465798020362854
Batch 13/64 loss: -0.22610312700271606
Batch 14/64 loss: -0.16670626401901245
Batch 15/64 loss: -0.16080641746520996
Batch 16/64 loss: -0.15352535247802734
Batch 17/64 loss: -0.19496077299118042
Batch 18/64 loss: -0.18392401933670044
Batch 19/64 loss: -0.19590187072753906
Batch 20/64 loss: -0.1595696210861206
Batch 21/64 loss: -0.15585952997207642
Batch 22/64 loss: -0.17757874727249146
Batch 23/64 loss: -0.16212058067321777
Batch 24/64 loss: -0.15567821264266968
Batch 25/64 loss: -0.1409595012664795
Batch 26/64 loss: -0.16375726461410522
Batch 27/64 loss: -0.1492910385131836
Batch 28/64 loss: -0.15071213245391846
Batch 29/64 loss: -0.1659981608390808
Batch 30/64 loss: -0.17678821086883545
Batch 31/64 loss: -0.1590479016304016
Batch 32/64 loss: -0.1629672646522522
Batch 33/64 loss: -0.1738073229789734
Batch 34/64 loss: -0.1461978554725647
Batch 35/64 loss: -0.1668129563331604
Batch 36/64 loss: -0.1981973648071289
Batch 37/64 loss: -0.1801702380180359
Batch 38/64 loss: -0.1999254822731018
Batch 39/64 loss: -0.1366909146308899
Batch 40/64 loss: -0.155614972114563
Batch 41/64 loss: -0.17963051795959473
Batch 42/64 loss: -0.20626196265220642
Batch 43/64 loss: -0.1940649449825287
Batch 44/64 loss: -0.16594207286834717
Batch 45/64 loss: -0.11788898706436157
Batch 46/64 loss: -0.15652060508728027
Batch 47/64 loss: -0.16715407371520996
Batch 48/64 loss: -0.2084997594356537
Batch 49/64 loss: -0.1489853858947754
Batch 50/64 loss: -0.12944573163986206
Batch 51/64 loss: -0.16651201248168945
Batch 52/64 loss: -0.19791331887245178
Batch 53/64 loss: -0.17263567447662354
Batch 54/64 loss: -0.12823724746704102
Batch 55/64 loss: -0.17195641994476318
Batch 56/64 loss: -0.1967909336090088
Batch 57/64 loss: -0.2137548327445984
Batch 58/64 loss: -0.16095834970474243
Batch 59/64 loss: -0.15346693992614746
Batch 60/64 loss: -0.16743898391723633
Batch 61/64 loss: -0.17878246307373047
Batch 62/64 loss: -0.17247456312179565
Batch 63/64 loss: -0.16883838176727295
Batch 64/64 loss: -0.20950138568878174
Epoch 461  Train loss: -0.17114817020939846  Val loss: 0.06628068062857664
Epoch 462
-------------------------------
Batch 1/64 loss: -0.1814543604850769
Batch 2/64 loss: -0.1369084119796753
Batch 3/64 loss: -0.17637485265731812
Batch 4/64 loss: -0.19071471691131592
Batch 5/64 loss: -0.20990043878555298
Batch 6/64 loss: -0.13319998979568481
Batch 7/64 loss: -0.12712907791137695
Batch 8/64 loss: -0.16440421342849731
Batch 9/64 loss: -0.19820266962051392
Batch 10/64 loss: -0.2304222583770752
Batch 11/64 loss: -0.17929702997207642
Batch 12/64 loss: -0.189508318901062
Batch 13/64 loss: -0.18238943815231323
Batch 14/64 loss: -0.17585092782974243
Batch 15/64 loss: -0.19182944297790527
Batch 16/64 loss: -0.19701603055000305
Batch 17/64 loss: -0.19562694430351257
Batch 18/64 loss: -0.1812695860862732
Batch 19/64 loss: -0.11848962306976318
Batch 20/64 loss: -0.1608978509902954
Batch 21/64 loss: -0.1595766544342041
Batch 22/64 loss: -0.20663899183273315
Batch 23/64 loss: -0.19704434275627136
Batch 24/64 loss: -0.1631702184677124
Batch 25/64 loss: -0.19626399874687195
Batch 26/64 loss: -0.169505774974823
Batch 27/64 loss: -0.12888389825820923
Batch 28/64 loss: -0.12099415063858032
Batch 29/64 loss: -0.1983247697353363
Batch 30/64 loss: -0.13816195726394653
Batch 31/64 loss: -0.14916741847991943
Batch 32/64 loss: -0.19648194313049316
Batch 33/64 loss: -0.170310378074646
Batch 34/64 loss: -0.150340735912323
Batch 35/64 loss: -0.20000308752059937
Batch 36/64 loss: -0.16619586944580078
Batch 37/64 loss: -0.1724327802658081
Batch 38/64 loss: -0.17195528745651245
Batch 39/64 loss: -0.16816473007202148
Batch 40/64 loss: -0.15361493825912476
Batch 41/64 loss: -0.17832201719284058
Batch 42/64 loss: -0.12499034404754639
Batch 43/64 loss: -0.1325017213821411
Batch 44/64 loss: -0.18253391981124878
Batch 45/64 loss: -0.11534053087234497
Batch 46/64 loss: -0.17161822319030762
Batch 47/64 loss: -0.15867286920547485
Batch 48/64 loss: -0.18357181549072266
Batch 49/64 loss: -0.16846972703933716
Batch 50/64 loss: -0.1723644733428955
Batch 51/64 loss: -0.17441362142562866
Batch 52/64 loss: -0.1573885679244995
Batch 53/64 loss: -0.16413307189941406
Batch 54/64 loss: -0.20907914638519287
Batch 55/64 loss: -0.17704957723617554
Batch 56/64 loss: -0.16977602243423462
Batch 57/64 loss: -0.16592705249786377
Batch 58/64 loss: -0.20675572752952576
Batch 59/64 loss: -0.18839523196220398
Batch 60/64 loss: -0.14271223545074463
Batch 61/64 loss: -0.17569750547409058
Batch 62/64 loss: -0.18347853422164917
Batch 63/64 loss: -0.14349472522735596
Batch 64/64 loss: -0.15028101205825806
Epoch 462  Train loss: -0.17031396907918595  Val loss: 0.0669796245614278
Epoch 463
-------------------------------
Batch 1/64 loss: -0.16520488262176514
Batch 2/64 loss: -0.12543070316314697
Batch 3/64 loss: -0.19524183869361877
Batch 4/64 loss: -0.19300588965415955
Batch 5/64 loss: -0.12685775756835938
Batch 6/64 loss: -0.19812971353530884
Batch 7/64 loss: -0.11270195245742798
Batch 8/64 loss: -0.17469018697738647
Batch 9/64 loss: -0.2195315957069397
Batch 10/64 loss: -0.1553105115890503
Batch 11/64 loss: -0.18501198291778564
Batch 12/64 loss: -0.19924747943878174
Batch 13/64 loss: -0.19167247414588928
Batch 14/64 loss: -0.11876970529556274
Batch 15/64 loss: -0.19733017683029175
Batch 16/64 loss: -0.16522014141082764
Batch 17/64 loss: -0.1820349097251892
Batch 18/64 loss: -0.14861398935317993
Batch 19/64 loss: -0.2166513204574585
Batch 20/64 loss: -0.16080182790756226
Batch 21/64 loss: -0.17095524072647095
Batch 22/64 loss: -0.16797852516174316
Batch 23/64 loss: -0.21413826942443848
Batch 24/64 loss: -0.20025241374969482
Batch 25/64 loss: -0.15150225162506104
Batch 26/64 loss: -0.16753393411636353
Batch 27/64 loss: -0.1636887788772583
Batch 28/64 loss: -0.19520515203475952
Batch 29/64 loss: -0.1400189995765686
Batch 30/64 loss: -0.1597602367401123
Batch 31/64 loss: -0.18129026889801025
Batch 32/64 loss: -0.19365784525871277
Batch 33/64 loss: -0.1822720766067505
Batch 34/64 loss: -0.19819465279579163
Batch 35/64 loss: -0.16308873891830444
Batch 36/64 loss: -0.18974149227142334
Batch 37/64 loss: -0.1588495373725891
Batch 38/64 loss: -0.1896260678768158
Batch 39/64 loss: -0.18546819686889648
Batch 40/64 loss: -0.17067891359329224
Batch 41/64 loss: -0.16744846105575562
Batch 42/64 loss: -0.17131996154785156
Batch 43/64 loss: -0.16628456115722656
Batch 44/64 loss: -0.21040412783622742
Batch 45/64 loss: -0.14125430583953857
Batch 46/64 loss: -0.16637581586837769
Batch 47/64 loss: -0.1775006651878357
Batch 48/64 loss: -0.20202183723449707
Batch 49/64 loss: -0.18636268377304077
Batch 50/64 loss: -0.17686909437179565
Batch 51/64 loss: -0.12343841791152954
Batch 52/64 loss: -0.17900300025939941
Batch 53/64 loss: -0.1798480749130249
Batch 54/64 loss: -0.1177518367767334
Batch 55/64 loss: -0.17077219486236572
Batch 56/64 loss: -0.1732274293899536
Batch 57/64 loss: -0.1765519380569458
Batch 58/64 loss: -0.1721838116645813
Batch 59/64 loss: -0.16590309143066406
Batch 60/64 loss: -0.1836531162261963
Batch 61/64 loss: -0.1527332067489624
Batch 62/64 loss: -0.1922435164451599
Batch 63/64 loss: -0.16483503580093384
Batch 64/64 loss: -0.15165311098098755
Epoch 463  Train loss: -0.17266018273783665  Val loss: 0.06890452543075141
Epoch 464
-------------------------------
Batch 1/64 loss: -0.16973388195037842
Batch 2/64 loss: -0.16467630863189697
Batch 3/64 loss: -0.20054256916046143
Batch 4/64 loss: -0.14560621976852417
Batch 5/64 loss: -0.1690794825553894
Batch 6/64 loss: -0.19145727157592773
Batch 7/64 loss: -0.1969369649887085
Batch 8/64 loss: -0.1849871277809143
Batch 9/64 loss: -0.1795610785484314
Batch 10/64 loss: -0.1875070333480835
Batch 11/64 loss: -0.13586872816085815
Batch 12/64 loss: -0.1639084815979004
Batch 13/64 loss: -0.18392986059188843
Batch 14/64 loss: -0.1591782569885254
Batch 15/64 loss: -0.18242919445037842
Batch 16/64 loss: -0.1942010521888733
Batch 17/64 loss: -0.184906005859375
Batch 18/64 loss: -0.19053581357002258
Batch 19/64 loss: -0.17427992820739746
Batch 20/64 loss: -0.2229485809803009
Batch 21/64 loss: -0.1852375566959381
Batch 22/64 loss: -0.15203863382339478
Batch 23/64 loss: -0.16946345567703247
Batch 24/64 loss: -0.1600334644317627
Batch 25/64 loss: -0.17269891500473022
Batch 26/64 loss: -0.1563941240310669
Batch 27/64 loss: -0.14972060918807983
Batch 28/64 loss: -0.21075689792633057
Batch 29/64 loss: -0.1718084216117859
Batch 30/64 loss: -0.19622331857681274
Batch 31/64 loss: -0.19408714771270752
Batch 32/64 loss: -0.19243764877319336
Batch 33/64 loss: -0.17045080661773682
Batch 34/64 loss: -0.18413501977920532
Batch 35/64 loss: -0.1410003900527954
Batch 36/64 loss: -0.18151134252548218
Batch 37/64 loss: -0.17984634637832642
Batch 38/64 loss: -0.18951666355133057
Batch 39/64 loss: -0.16983872652053833
Batch 40/64 loss: -0.15829086303710938
Batch 41/64 loss: -0.1647966504096985
Batch 42/64 loss: -0.13920271396636963
Batch 43/64 loss: -0.14047032594680786
Batch 44/64 loss: -0.17879819869995117
Batch 45/64 loss: -0.20698314905166626
Batch 46/64 loss: -0.15783345699310303
Batch 47/64 loss: -0.1890471875667572
Batch 48/64 loss: -0.1650303602218628
Batch 49/64 loss: -0.19961243867874146
Batch 50/64 loss: -0.1917717456817627
Batch 51/64 loss: -0.19477853178977966
Batch 52/64 loss: -0.18323427438735962
Batch 53/64 loss: -0.08379358053207397
Batch 54/64 loss: -0.1765669584274292
Batch 55/64 loss: -0.1815657615661621
Batch 56/64 loss: -0.18774569034576416
Batch 57/64 loss: -0.17400431632995605
Batch 58/64 loss: -0.1713329553604126
Batch 59/64 loss: -0.13385581970214844
Batch 60/64 loss: -0.19277167320251465
Batch 61/64 loss: -0.1919741928577423
Batch 62/64 loss: -0.17862451076507568
Batch 63/64 loss: -0.16893160343170166
Batch 64/64 loss: -0.10629862546920776
Epoch 464  Train loss: -0.17412100796606025  Val loss: 0.06707049849926401
Epoch 465
-------------------------------
Batch 1/64 loss: -0.1746789813041687
Batch 2/64 loss: -0.19617381691932678
Batch 3/64 loss: -0.18422484397888184
Batch 4/64 loss: -0.17390960454940796
Batch 5/64 loss: -0.12543994188308716
Batch 6/64 loss: -0.16372132301330566
Batch 7/64 loss: -0.18021827936172485
Batch 8/64 loss: -0.14994937181472778
Batch 9/64 loss: -0.17526686191558838
Batch 10/64 loss: -0.2144213318824768
Batch 11/64 loss: -0.18295234441757202
Batch 12/64 loss: -0.19533586502075195
Batch 13/64 loss: -0.17174899578094482
Batch 14/64 loss: -0.1746043562889099
Batch 15/64 loss: -0.18411338329315186
Batch 16/64 loss: -0.18270277976989746
Batch 17/64 loss: -0.1721864938735962
Batch 18/64 loss: -0.16172456741333008
Batch 19/64 loss: -0.17829257249832153
Batch 20/64 loss: -0.2117374837398529
Batch 21/64 loss: -0.2126242220401764
Batch 22/64 loss: -0.19597139954566956
Batch 23/64 loss: -0.18152540922164917
Batch 24/64 loss: -0.18941742181777954
Batch 25/64 loss: -0.1911579966545105
Batch 26/64 loss: -0.14246636629104614
Batch 27/64 loss: -0.18457400798797607
Batch 28/64 loss: -0.22042784094810486
Batch 29/64 loss: -0.19664514064788818
Batch 30/64 loss: -0.18697547912597656
Batch 31/64 loss: -0.1672016978263855
Batch 32/64 loss: -0.19297271966934204
Batch 33/64 loss: -0.14996683597564697
Batch 34/64 loss: -0.1611916422843933
Batch 35/64 loss: -0.12196695804595947
Batch 36/64 loss: -0.1681910753250122
Batch 37/64 loss: -0.19935649633407593
Batch 38/64 loss: -0.1491013765335083
Batch 39/64 loss: -0.17136472463607788
Batch 40/64 loss: -0.17676997184753418
Batch 41/64 loss: -0.14447182416915894
Batch 42/64 loss: -0.16493463516235352
Batch 43/64 loss: -0.16592377424240112
Batch 44/64 loss: -0.19080066680908203
Batch 45/64 loss: -0.1628948450088501
Batch 46/64 loss: -0.17287355661392212
Batch 47/64 loss: -0.19149553775787354
Batch 48/64 loss: -0.1534406542778015
Batch 49/64 loss: -0.17257475852966309
Batch 50/64 loss: -0.17846471071243286
Batch 51/64 loss: -0.16322743892669678
Batch 52/64 loss: -0.19989928603172302
Batch 53/64 loss: -0.17080152034759521
Batch 54/64 loss: -0.15951764583587646
Batch 55/64 loss: -0.17610639333724976
Batch 56/64 loss: -0.1702830195426941
Batch 57/64 loss: -0.18374663591384888
Batch 58/64 loss: -0.17085719108581543
Batch 59/64 loss: -0.15670335292816162
Batch 60/64 loss: -0.15508848428726196
Batch 61/64 loss: -0.19831368327140808
Batch 62/64 loss: -0.17158758640289307
Batch 63/64 loss: -0.15891379117965698
Batch 64/64 loss: -0.14384585618972778
Epoch 465  Train loss: -0.17496591946657966  Val loss: 0.06559261034444436
Epoch 466
-------------------------------
Batch 1/64 loss: -0.200553297996521
Batch 2/64 loss: -0.20785093307495117
Batch 3/64 loss: -0.23672926425933838
Batch 4/64 loss: -0.1730252504348755
Batch 5/64 loss: -0.11768877506256104
Batch 6/64 loss: -0.13436496257781982
Batch 7/64 loss: -0.18958306312561035
Batch 8/64 loss: -0.1787865161895752
Batch 9/64 loss: -0.14394235610961914
Batch 10/64 loss: -0.12425094842910767
Batch 11/64 loss: -0.16212248802185059
Batch 12/64 loss: -0.17638754844665527
Batch 13/64 loss: -0.1341409683227539
Batch 14/64 loss: -0.18011730909347534
Batch 15/64 loss: -0.19347822666168213
Batch 16/64 loss: -0.13323956727981567
Batch 17/64 loss: -0.1658494472503662
Batch 18/64 loss: -0.17537105083465576
Batch 19/64 loss: -0.16613543033599854
Batch 20/64 loss: -0.1362716555595398
Batch 21/64 loss: -0.1626095175743103
Batch 22/64 loss: -0.13875490427017212
Batch 23/64 loss: -0.16405975818634033
Batch 24/64 loss: -0.20299768447875977
Batch 25/64 loss: -0.15326499938964844
Batch 26/64 loss: -0.20836690068244934
Batch 27/64 loss: -0.19644016027450562
Batch 28/64 loss: -0.1695946455001831
Batch 29/64 loss: -0.13311731815338135
Batch 30/64 loss: -0.2144942581653595
Batch 31/64 loss: -0.2097143530845642
Batch 32/64 loss: -0.1996690332889557
Batch 33/64 loss: -0.17863595485687256
Batch 34/64 loss: -0.1354648470878601
Batch 35/64 loss: -0.14565199613571167
Batch 36/64 loss: -0.137889564037323
Batch 37/64 loss: -0.21112197637557983
Batch 38/64 loss: -0.1543492078781128
Batch 39/64 loss: -0.1655207872390747
Batch 40/64 loss: -0.1902276873588562
Batch 41/64 loss: -0.18295037746429443
Batch 42/64 loss: -0.15796589851379395
Batch 43/64 loss: -0.16299885511398315
Batch 44/64 loss: -0.15482735633850098
Batch 45/64 loss: -0.17694169282913208
Batch 46/64 loss: -0.1557081937789917
Batch 47/64 loss: -0.176891028881073
Batch 48/64 loss: -0.1380903720855713
Batch 49/64 loss: -0.17768102884292603
Batch 50/64 loss: -0.18077689409255981
Batch 51/64 loss: -0.17250412702560425
Batch 52/64 loss: -0.18665248155593872
Batch 53/64 loss: -0.20181483030319214
Batch 54/64 loss: -0.17510110139846802
Batch 55/64 loss: -0.13649016618728638
Batch 56/64 loss: -0.14754092693328857
Batch 57/64 loss: -0.19155260920524597
Batch 58/64 loss: -0.19473248720169067
Batch 59/64 loss: -0.2070799469947815
Batch 60/64 loss: -0.18384945392608643
Batch 61/64 loss: -0.1431741714477539
Batch 62/64 loss: -0.19084346294403076
Batch 63/64 loss: -0.17657631635665894
Batch 64/64 loss: -0.1654173731803894
Epoch 466  Train loss: -0.17095908113554412  Val loss: 0.06613756946681701
Epoch 467
-------------------------------
Batch 1/64 loss: -0.16722726821899414
Batch 2/64 loss: -0.15662610530853271
Batch 3/64 loss: -0.1576516032218933
Batch 4/64 loss: -0.16175544261932373
Batch 5/64 loss: -0.1380971074104309
Batch 6/64 loss: -0.21202737092971802
Batch 7/64 loss: -0.17491865158081055
Batch 8/64 loss: -0.17169737815856934
Batch 9/64 loss: -0.1875290870666504
Batch 10/64 loss: -0.18241310119628906
Batch 11/64 loss: -0.1468195915222168
Batch 12/64 loss: -0.16558837890625
Batch 13/64 loss: -0.1602792739868164
Batch 14/64 loss: -0.19598203897476196
Batch 15/64 loss: -0.14655625820159912
Batch 16/64 loss: -0.18701446056365967
Batch 17/64 loss: -0.18489551544189453
Batch 18/64 loss: -0.13250476121902466
Batch 19/64 loss: -0.18104791641235352
Batch 20/64 loss: -0.1810135841369629
Batch 21/64 loss: -0.1958245038986206
Batch 22/64 loss: -0.17134785652160645
Batch 23/64 loss: -0.18046563863754272
Batch 24/64 loss: -0.17465287446975708
Batch 25/64 loss: -0.2265579104423523
Batch 26/64 loss: -0.16854113340377808
Batch 27/64 loss: -0.1824445128440857
Batch 28/64 loss: -0.13593870401382446
Batch 29/64 loss: -0.15122520923614502
Batch 30/64 loss: -0.13745564222335815
Batch 31/64 loss: -0.21575570106506348
Batch 32/64 loss: -0.19064122438430786
Batch 33/64 loss: -0.18301934003829956
Batch 34/64 loss: -0.1701822280883789
Batch 35/64 loss: -0.17067337036132812
Batch 36/64 loss: -0.159551739692688
Batch 37/64 loss: -0.19753992557525635
Batch 38/64 loss: -0.1782260537147522
Batch 39/64 loss: -0.1572200059890747
Batch 40/64 loss: -0.19357001781463623
Batch 41/64 loss: -0.1949450969696045
Batch 42/64 loss: -0.15187525749206543
Batch 43/64 loss: -0.19581764936447144
Batch 44/64 loss: -0.17136842012405396
Batch 45/64 loss: -0.18595489859580994
Batch 46/64 loss: -0.18439018726348877
Batch 47/64 loss: -0.14489328861236572
Batch 48/64 loss: -0.12378841638565063
Batch 49/64 loss: -0.18685781955718994
Batch 50/64 loss: -0.10329580307006836
Batch 51/64 loss: -0.18076670169830322
Batch 52/64 loss: -0.13335192203521729
Batch 53/64 loss: -0.21009597182273865
Batch 54/64 loss: -0.18092983961105347
Batch 55/64 loss: -0.1636866331100464
Batch 56/64 loss: -0.16188687086105347
Batch 57/64 loss: -0.17501115798950195
Batch 58/64 loss: -0.16982674598693848
Batch 59/64 loss: -0.15969449281692505
Batch 60/64 loss: -0.15526235103607178
Batch 61/64 loss: -0.17346686124801636
Batch 62/64 loss: -0.19300836324691772
Batch 63/64 loss: -0.17426609992980957
Batch 64/64 loss: -0.1892005205154419
Epoch 467  Train loss: -0.1717461917914596  Val loss: 0.06770018552177141
Epoch 468
-------------------------------
Batch 1/64 loss: -0.15441817045211792
Batch 2/64 loss: -0.12311935424804688
Batch 3/64 loss: -0.18835538625717163
Batch 4/64 loss: -0.13394957780838013
Batch 5/64 loss: -0.1680516004562378
Batch 6/64 loss: -0.1543598771095276
Batch 7/64 loss: -0.17248815298080444
Batch 8/64 loss: -0.1960839331150055
Batch 9/64 loss: -0.16197943687438965
Batch 10/64 loss: -0.1872425675392151
Batch 11/64 loss: -0.1826467514038086
Batch 12/64 loss: -0.18794161081314087
Batch 13/64 loss: -0.19727712869644165
Batch 14/64 loss: -0.16949015855789185
Batch 15/64 loss: -0.17455923557281494
Batch 16/64 loss: -0.20690381526947021
Batch 17/64 loss: -0.13699382543563843
Batch 18/64 loss: -0.19674259424209595
Batch 19/64 loss: -0.1989167332649231
Batch 20/64 loss: -0.21943652629852295
Batch 21/64 loss: -0.18349343538284302
Batch 22/64 loss: -0.19925731420516968
Batch 23/64 loss: -0.16315174102783203
Batch 24/64 loss: -0.17010825872421265
Batch 25/64 loss: -0.19904208183288574
Batch 26/64 loss: -0.1784343719482422
Batch 27/64 loss: -0.20476627349853516
Batch 28/64 loss: -0.1300487518310547
Batch 29/64 loss: -0.12898015975952148
Batch 30/64 loss: -0.2147946059703827
Batch 31/64 loss: -0.17810481786727905
Batch 32/64 loss: -0.15534871816635132
Batch 33/64 loss: -0.16768300533294678
Batch 34/64 loss: -0.15856444835662842
Batch 35/64 loss: -0.1539333462715149
Batch 36/64 loss: -0.16753405332565308
Batch 37/64 loss: -0.20594584941864014
Batch 38/64 loss: -0.13887405395507812
Batch 39/64 loss: -0.18891441822052002
Batch 40/64 loss: -0.2066960334777832
Batch 41/64 loss: -0.19982558488845825
Batch 42/64 loss: -0.16074788570404053
Batch 43/64 loss: -0.17860490083694458
Batch 44/64 loss: -0.1634962558746338
Batch 45/64 loss: -0.19819992780685425
Batch 46/64 loss: -0.17267662286758423
Batch 47/64 loss: -0.18236470222473145
Batch 48/64 loss: -0.17409229278564453
Batch 49/64 loss: -0.18699273467063904
Batch 50/64 loss: -0.1671229600906372
Batch 51/64 loss: -0.19024616479873657
Batch 52/64 loss: -0.14269065856933594
Batch 53/64 loss: -0.18759149312973022
Batch 54/64 loss: -0.19483599066734314
Batch 55/64 loss: -0.14471131563186646
Batch 56/64 loss: -0.1608986258506775
Batch 57/64 loss: -0.22290641069412231
Batch 58/64 loss: -0.16946053504943848
Batch 59/64 loss: -0.20933297276496887
Batch 60/64 loss: -0.1497301459312439
Batch 61/64 loss: -0.1532006859779358
Batch 62/64 loss: -0.14739423990249634
Batch 63/64 loss: -0.16185176372528076
Batch 64/64 loss: -0.18640947341918945
Epoch 468  Train loss: -0.17511237883100322  Val loss: 0.07072452745077126
Epoch 469
-------------------------------
Batch 1/64 loss: -0.21101629734039307
Batch 2/64 loss: -0.17474007606506348
Batch 3/64 loss: -0.19832736253738403
Batch 4/64 loss: -0.1694619059562683
Batch 5/64 loss: -0.21456855535507202
Batch 6/64 loss: -0.20092672109603882
Batch 7/64 loss: -0.17187988758087158
Batch 8/64 loss: -0.1739886999130249
Batch 9/64 loss: -0.1755589246749878
Batch 10/64 loss: -0.20079994201660156
Batch 11/64 loss: -0.20794185996055603
Batch 12/64 loss: -0.19121915102005005
Batch 13/64 loss: -0.17937558889389038
Batch 14/64 loss: -0.11400800943374634
Batch 15/64 loss: -0.15121948719024658
Batch 16/64 loss: -0.14681291580200195
Batch 17/64 loss: -0.17889738082885742
Batch 18/64 loss: -0.18498840928077698
Batch 19/64 loss: -0.21023905277252197
Batch 20/64 loss: -0.16363108158111572
Batch 21/64 loss: -0.19167786836624146
Batch 22/64 loss: -0.17894142866134644
Batch 23/64 loss: -0.19615867733955383
Batch 24/64 loss: -0.18926575779914856
Batch 25/64 loss: -0.16179627180099487
Batch 26/64 loss: -0.20953071117401123
Batch 27/64 loss: -0.16948258876800537
Batch 28/64 loss: -0.18244636058807373
Batch 29/64 loss: -0.1468767523765564
Batch 30/64 loss: -0.1693858504295349
Batch 31/64 loss: -0.17597532272338867
Batch 32/64 loss: -0.18262732028961182
Batch 33/64 loss: -0.10345673561096191
Batch 34/64 loss: -0.17875182628631592
Batch 35/64 loss: -0.16760492324829102
Batch 36/64 loss: -0.1721368432044983
Batch 37/64 loss: -0.16335374116897583
Batch 38/64 loss: -0.17700403928756714
Batch 39/64 loss: -0.18175673484802246
Batch 40/64 loss: -0.2043459415435791
Batch 41/64 loss: -0.1498284935951233
Batch 42/64 loss: -0.15530776977539062
Batch 43/64 loss: -0.1633697748184204
Batch 44/64 loss: -0.21804893016815186
Batch 45/64 loss: -0.19021469354629517
Batch 46/64 loss: -0.18230336904525757
Batch 47/64 loss: -0.15615242719650269
Batch 48/64 loss: -0.15797466039657593
Batch 49/64 loss: -0.2098293900489807
Batch 50/64 loss: -0.16933149099349976
Batch 51/64 loss: -0.1162118911743164
Batch 52/64 loss: -0.1670793890953064
Batch 53/64 loss: -0.1478281021118164
Batch 54/64 loss: -0.1800571084022522
Batch 55/64 loss: -0.15526974201202393
Batch 56/64 loss: -0.15726548433303833
Batch 57/64 loss: -0.18859106302261353
Batch 58/64 loss: -0.10939961671829224
Batch 59/64 loss: -0.12503021955490112
Batch 60/64 loss: -0.16792529821395874
Batch 61/64 loss: -0.11544877290725708
Batch 62/64 loss: -0.19937366247177124
Batch 63/64 loss: -0.1666196584701538
Batch 64/64 loss: -0.19415968656539917
Epoch 469  Train loss: -0.17280404357349172  Val loss: 0.07408470058768886
Epoch 470
-------------------------------
Batch 1/64 loss: -0.16122817993164062
Batch 2/64 loss: -0.14257347583770752
Batch 3/64 loss: -0.17136216163635254
Batch 4/64 loss: -0.19867724180221558
Batch 5/64 loss: -0.14589333534240723
Batch 6/64 loss: -0.1999267339706421
Batch 7/64 loss: -0.15612196922302246
Batch 8/64 loss: -0.15570807456970215
Batch 9/64 loss: -0.20340844988822937
Batch 10/64 loss: -0.18969017267227173
Batch 11/64 loss: -0.19034352898597717
Batch 12/64 loss: -0.17093336582183838
Batch 13/64 loss: -0.17343610525131226
Batch 14/64 loss: -0.17105990648269653
Batch 15/64 loss: -0.16416269540786743
Batch 16/64 loss: -0.16908293962478638
Batch 17/64 loss: -0.1476050615310669
Batch 18/64 loss: -0.16126608848571777
Batch 19/64 loss: -0.1779780387878418
Batch 20/64 loss: -0.12691324949264526
Batch 21/64 loss: -0.1387087106704712
Batch 22/64 loss: -0.1920926570892334
Batch 23/64 loss: -0.19991415739059448
Batch 24/64 loss: -0.17677205801010132
Batch 25/64 loss: -0.17357289791107178
Batch 26/64 loss: -0.1504526138305664
Batch 27/64 loss: -0.17441630363464355
Batch 28/64 loss: -0.219431072473526
Batch 29/64 loss: -0.1635833978652954
Batch 30/64 loss: -0.12441670894622803
Batch 31/64 loss: -0.20878535509109497
Batch 32/64 loss: -0.18023782968521118
Batch 33/64 loss: -0.18604320287704468
Batch 34/64 loss: -0.15895074605941772
Batch 35/64 loss: -0.19098082184791565
Batch 36/64 loss: -0.19209259748458862
Batch 37/64 loss: -0.18532776832580566
Batch 38/64 loss: -0.14131814241409302
Batch 39/64 loss: -0.17484694719314575
Batch 40/64 loss: -0.21156728267669678
Batch 41/64 loss: -0.1751335859298706
Batch 42/64 loss: -0.16694706678390503
Batch 43/64 loss: -0.17800694704055786
Batch 44/64 loss: -0.15924417972564697
Batch 45/64 loss: -0.17385345697402954
Batch 46/64 loss: -0.17554301023483276
Batch 47/64 loss: -0.19004130363464355
Batch 48/64 loss: -0.15013056993484497
Batch 49/64 loss: -0.13208800554275513
Batch 50/64 loss: -0.15700674057006836
Batch 51/64 loss: -0.15687698125839233
Batch 52/64 loss: -0.1728925108909607
Batch 53/64 loss: -0.16264969110488892
Batch 54/64 loss: -0.1768132448196411
Batch 55/64 loss: -0.15318292379379272
Batch 56/64 loss: -0.21189704537391663
Batch 57/64 loss: -0.16685742139816284
Batch 58/64 loss: -0.1104920506477356
Batch 59/64 loss: -0.18411827087402344
Batch 60/64 loss: -0.1870509386062622
Batch 61/64 loss: -0.1965767741203308
Batch 62/64 loss: -0.14177978038787842
Batch 63/64 loss: -0.2003576159477234
Batch 64/64 loss: -0.1411045789718628
Epoch 470  Train loss: -0.17107844072229722  Val loss: 0.07209421924709045
Epoch 471
-------------------------------
Batch 1/64 loss: -0.15204644203186035
Batch 2/64 loss: -0.16819661855697632
Batch 3/64 loss: -0.21223312616348267
Batch 4/64 loss: -0.1782969832420349
Batch 5/64 loss: -0.2106233537197113
Batch 6/64 loss: -0.1385897397994995
Batch 7/64 loss: -0.18673193454742432
Batch 8/64 loss: -0.19048839807510376
Batch 9/64 loss: -0.19433650374412537
Batch 10/64 loss: -0.15543389320373535
Batch 11/64 loss: -0.15992045402526855
Batch 12/64 loss: -0.19843900203704834
Batch 13/64 loss: -0.20747941732406616
Batch 14/64 loss: -0.17407077550888062
Batch 15/64 loss: -0.17512226104736328
Batch 16/64 loss: -0.20658999681472778
Batch 17/64 loss: -0.14849698543548584
Batch 18/64 loss: -0.17370271682739258
Batch 19/64 loss: -0.13828647136688232
Batch 20/64 loss: -0.18954938650131226
Batch 21/64 loss: -0.17664164304733276
Batch 22/64 loss: -0.17095035314559937
Batch 23/64 loss: -0.16698789596557617
Batch 24/64 loss: -0.15686845779418945
Batch 25/64 loss: -0.18243420124053955
Batch 26/64 loss: -0.16075807809829712
Batch 27/64 loss: -0.1544891595840454
Batch 28/64 loss: -0.17518281936645508
Batch 29/64 loss: -0.18629521131515503
Batch 30/64 loss: -0.1849115490913391
Batch 31/64 loss: -0.1728183627128601
Batch 32/64 loss: -0.2135743498802185
Batch 33/64 loss: -0.18528282642364502
Batch 34/64 loss: -0.1809455156326294
Batch 35/64 loss: -0.1970704197883606
Batch 36/64 loss: -0.14990729093551636
Batch 37/64 loss: -0.2012239396572113
Batch 38/64 loss: -0.19201457500457764
Batch 39/64 loss: -0.13627827167510986
Batch 40/64 loss: -0.1977558434009552
Batch 41/64 loss: -0.1953810453414917
Batch 42/64 loss: -0.17931336164474487
Batch 43/64 loss: -0.1867803931236267
Batch 44/64 loss: -0.18951669335365295
Batch 45/64 loss: -0.17367178201675415
Batch 46/64 loss: -0.1623639464378357
Batch 47/64 loss: -0.2020876407623291
Batch 48/64 loss: -0.19653183221817017
Batch 49/64 loss: -0.18688049912452698
Batch 50/64 loss: -0.15856915712356567
Batch 51/64 loss: -0.17935705184936523
Batch 52/64 loss: -0.2006239891052246
Batch 53/64 loss: -0.14730483293533325
Batch 54/64 loss: -0.16192495822906494
Batch 55/64 loss: -0.20231172442436218
Batch 56/64 loss: -0.1236872673034668
Batch 57/64 loss: -0.18905696272850037
Batch 58/64 loss: -0.16481995582580566
Batch 59/64 loss: -0.1441289186477661
Batch 60/64 loss: -0.19396832585334778
Batch 61/64 loss: -0.13312792778015137
Batch 62/64 loss: -0.13151365518569946
Batch 63/64 loss: -0.17503565549850464
Batch 64/64 loss: -0.178777277469635
Epoch 471  Train loss: -0.17589122758192174  Val loss: 0.06902115902130547
Epoch 472
-------------------------------
Batch 1/64 loss: -0.15156638622283936
Batch 2/64 loss: -0.15110594034194946
Batch 3/64 loss: -0.1457064151763916
Batch 4/64 loss: -0.16977053880691528
Batch 5/64 loss: -0.17835813760757446
Batch 6/64 loss: -0.20808130502700806
Batch 7/64 loss: -0.18522894382476807
Batch 8/64 loss: -0.18775084614753723
Batch 9/64 loss: -0.1728540062904358
Batch 10/64 loss: -0.18367093801498413
Batch 11/64 loss: -0.17506003379821777
Batch 12/64 loss: -0.2110503613948822
Batch 13/64 loss: -0.12907785177230835
Batch 14/64 loss: -0.19226160645484924
Batch 15/64 loss: -0.21284598112106323
Batch 16/64 loss: -0.1731419563293457
Batch 17/64 loss: -0.1642094850540161
Batch 18/64 loss: -0.16072535514831543
Batch 19/64 loss: -0.1205143928527832
Batch 20/64 loss: -0.18834224343299866
Batch 21/64 loss: -0.17723286151885986
Batch 22/64 loss: -0.1803315281867981
Batch 23/64 loss: -0.18568575382232666
Batch 24/64 loss: -0.18903785943984985
Batch 25/64 loss: -0.16479933261871338
Batch 26/64 loss: -0.17559736967086792
Batch 27/64 loss: -0.14684182405471802
Batch 28/64 loss: -0.19006216526031494
Batch 29/64 loss: -0.18838810920715332
Batch 30/64 loss: -0.17922377586364746
Batch 31/64 loss: -0.19680672883987427
Batch 32/64 loss: -0.17626214027404785
Batch 33/64 loss: -0.18393203616142273
Batch 34/64 loss: -0.20194631814956665
Batch 35/64 loss: -0.17897003889083862
Batch 36/64 loss: -0.15819042921066284
Batch 37/64 loss: -0.2193235158920288
Batch 38/64 loss: -0.15406066179275513
Batch 39/64 loss: -0.19197267293930054
Batch 40/64 loss: -0.1827811598777771
Batch 41/64 loss: -0.16711002588272095
Batch 42/64 loss: -0.182187020778656
Batch 43/64 loss: -0.1824476718902588
Batch 44/64 loss: -0.15457677841186523
Batch 45/64 loss: -0.19003376364707947
Batch 46/64 loss: -0.18671914935112
Batch 47/64 loss: -0.2047748565673828
Batch 48/64 loss: -0.21375900506973267
Batch 49/64 loss: -0.15866434574127197
Batch 50/64 loss: -0.21223783493041992
Batch 51/64 loss: -0.17319750785827637
Batch 52/64 loss: -0.20507007837295532
Batch 53/64 loss: -0.1571328043937683
Batch 54/64 loss: -0.15188902616500854
Batch 55/64 loss: -0.16393399238586426
Batch 56/64 loss: -0.16171622276306152
Batch 57/64 loss: -0.1953737735748291
Batch 58/64 loss: -0.14709198474884033
Batch 59/64 loss: -0.17561006546020508
Batch 60/64 loss: -0.19139236211776733
Batch 61/64 loss: -0.1518462896347046
Batch 62/64 loss: -0.1674785017967224
Batch 63/64 loss: -0.1885087490081787
Batch 64/64 loss: -0.09632682800292969
Epoch 472  Train loss: -0.176278681381076  Val loss: 0.0678714028338796
Epoch 473
-------------------------------
Batch 1/64 loss: -0.14883673191070557
Batch 2/64 loss: -0.18671399354934692
Batch 3/64 loss: -0.23043984174728394
Batch 4/64 loss: -0.2144564390182495
Batch 5/64 loss: -0.1608598828315735
Batch 6/64 loss: -0.19640663266181946
Batch 7/64 loss: -0.1984463930130005
Batch 8/64 loss: -0.16739493608474731
Batch 9/64 loss: -0.11333954334259033
Batch 10/64 loss: -0.18813002109527588
Batch 11/64 loss: -0.21768677234649658
Batch 12/64 loss: -0.15196174383163452
Batch 13/64 loss: -0.18690532445907593
Batch 14/64 loss: -0.18636402487754822
Batch 15/64 loss: -0.14604824781417847
Batch 16/64 loss: -0.19761180877685547
Batch 17/64 loss: -0.1762043833732605
Batch 18/64 loss: -0.15478092432022095
Batch 19/64 loss: -0.17736965417861938
Batch 20/64 loss: -0.14590716361999512
Batch 21/64 loss: -0.18764111399650574
Batch 22/64 loss: -0.17735439538955688
Batch 23/64 loss: -0.1399635672569275
Batch 24/64 loss: -0.19743448495864868
Batch 25/64 loss: -0.15858805179595947
Batch 26/64 loss: -0.15837013721466064
Batch 27/64 loss: -0.16480332612991333
Batch 28/64 loss: -0.15687942504882812
Batch 29/64 loss: -0.17612582445144653
Batch 30/64 loss: -0.17529988288879395
Batch 31/64 loss: -0.183513343334198
Batch 32/64 loss: -0.1868996024131775
Batch 33/64 loss: -0.14195704460144043
Batch 34/64 loss: -0.12076079845428467
Batch 35/64 loss: -0.17372888326644897
Batch 36/64 loss: -0.13829994201660156
Batch 37/64 loss: -0.16387319564819336
Batch 38/64 loss: -0.2218172550201416
Batch 39/64 loss: -0.18015170097351074
Batch 40/64 loss: -0.16240477561950684
Batch 41/64 loss: -0.17930907011032104
Batch 42/64 loss: -0.19169080257415771
Batch 43/64 loss: -0.14057600498199463
Batch 44/64 loss: -0.13640743494033813
Batch 45/64 loss: -0.2040344476699829
Batch 46/64 loss: -0.1921517252922058
Batch 47/64 loss: -0.1746392846107483
Batch 48/64 loss: -0.18513792753219604
Batch 49/64 loss: -0.19183894991874695
Batch 50/64 loss: -0.15136873722076416
Batch 51/64 loss: -0.15287548303604126
Batch 52/64 loss: -0.13586318492889404
Batch 53/64 loss: -0.20466020703315735
Batch 54/64 loss: -0.18993735313415527
Batch 55/64 loss: -0.1717401146888733
Batch 56/64 loss: -0.20501559972763062
Batch 57/64 loss: -0.20278173685073853
Batch 58/64 loss: -0.1724071502685547
Batch 59/64 loss: -0.1544591188430786
Batch 60/64 loss: -0.15314757823944092
Batch 61/64 loss: -0.16544586420059204
Batch 62/64 loss: -0.16636592149734497
Batch 63/64 loss: -0.1915365755558014
Batch 64/64 loss: -0.17142879962921143
Epoch 473  Train loss: -0.17339126409268846  Val loss: 0.070884137833651
Epoch 474
-------------------------------
Batch 1/64 loss: -0.1400967836380005
Batch 2/64 loss: -0.17265993356704712
Batch 3/64 loss: -0.15658509731292725
Batch 4/64 loss: -0.12622356414794922
Batch 5/64 loss: -0.18119335174560547
Batch 6/64 loss: -0.19002720713615417
Batch 7/64 loss: -0.2007940709590912
Batch 8/64 loss: -0.15239393711090088
Batch 9/64 loss: -0.1801474690437317
Batch 10/64 loss: -0.18630218505859375
Batch 11/64 loss: -0.19696670770645142
Batch 12/64 loss: -0.17398619651794434
Batch 13/64 loss: -0.16655337810516357
Batch 14/64 loss: -0.20808643102645874
Batch 15/64 loss: -0.17347979545593262
Batch 16/64 loss: -0.15171527862548828
Batch 17/64 loss: -0.1558520793914795
Batch 18/64 loss: -0.175442636013031
Batch 19/64 loss: -0.16187602281570435
Batch 20/64 loss: -0.17090535163879395
Batch 21/64 loss: -0.20823761820793152
Batch 22/64 loss: -0.17758458852767944
Batch 23/64 loss: -0.1957736611366272
Batch 24/64 loss: -0.17355000972747803
Batch 25/64 loss: -0.15145546197891235
Batch 26/64 loss: -0.16879045963287354
Batch 27/64 loss: -0.15334534645080566
Batch 28/64 loss: -0.18541008234024048
Batch 29/64 loss: -0.18023723363876343
Batch 30/64 loss: -0.19858905673027039
Batch 31/64 loss: -0.10105228424072266
Batch 32/64 loss: -0.16610997915267944
Batch 33/64 loss: -0.17490017414093018
Batch 34/64 loss: -0.18024635314941406
Batch 35/64 loss: -0.18542411923408508
Batch 36/64 loss: -0.2116687297821045
Batch 37/64 loss: -0.1313561201095581
Batch 38/64 loss: -0.14551663398742676
Batch 39/64 loss: -0.18384185433387756
Batch 40/64 loss: -0.15737152099609375
Batch 41/64 loss: -0.17217838764190674
Batch 42/64 loss: -0.16367346048355103
Batch 43/64 loss: -0.1926867961883545
Batch 44/64 loss: -0.20352810621261597
Batch 45/64 loss: -0.18703603744506836
Batch 46/64 loss: -0.16339772939682007
Batch 47/64 loss: -0.16868609189987183
Batch 48/64 loss: -0.16151058673858643
Batch 49/64 loss: -0.20076525211334229
Batch 50/64 loss: -0.2250298261642456
Batch 51/64 loss: -0.1684085726737976
Batch 52/64 loss: -0.1533530354499817
Batch 53/64 loss: -0.1981651484966278
Batch 54/64 loss: -0.1830301284790039
Batch 55/64 loss: -0.1210889220237732
Batch 56/64 loss: -0.1977073848247528
Batch 57/64 loss: -0.18583106994628906
Batch 58/64 loss: -0.19879621267318726
Batch 59/64 loss: -0.16213864088058472
Batch 60/64 loss: -0.22332045435905457
Batch 61/64 loss: -0.1825096607208252
Batch 62/64 loss: -0.17433303594589233
Batch 63/64 loss: -0.1650460958480835
Batch 64/64 loss: -0.18318045139312744
Epoch 474  Train loss: -0.17476634885750564  Val loss: 0.07211534264161415
Epoch 475
-------------------------------
Batch 1/64 loss: -0.1836417317390442
Batch 2/64 loss: -0.20990395545959473
Batch 3/64 loss: -0.20274391770362854
Batch 4/64 loss: -0.1759873628616333
Batch 5/64 loss: -0.17762064933776855
Batch 6/64 loss: -0.1963379979133606
Batch 7/64 loss: -0.1918202042579651
Batch 8/64 loss: -0.16923582553863525
Batch 9/64 loss: -0.17887437343597412
Batch 10/64 loss: -0.2042829990386963
Batch 11/64 loss: -0.1816508173942566
Batch 12/64 loss: -0.19440022110939026
Batch 13/64 loss: -0.11141586303710938
Batch 14/64 loss: -0.18251550197601318
Batch 15/64 loss: -0.1754704713821411
Batch 16/64 loss: -0.21651461720466614
Batch 17/64 loss: -0.1816604733467102
Batch 18/64 loss: -0.21035778522491455
Batch 19/64 loss: -0.1707969307899475
Batch 20/64 loss: -0.17084771394729614
Batch 21/64 loss: -0.19312238693237305
Batch 22/64 loss: -0.1404043436050415
Batch 23/64 loss: -0.10942012071609497
Batch 24/64 loss: -0.1710662841796875
Batch 25/64 loss: -0.17664140462875366
Batch 26/64 loss: -0.18449661135673523
Batch 27/64 loss: -0.1884807050228119
Batch 28/64 loss: -0.17893266677856445
Batch 29/64 loss: -0.20076411962509155
Batch 30/64 loss: -0.12874305248260498
Batch 31/64 loss: -0.18336889147758484
Batch 32/64 loss: -0.1732214093208313
Batch 33/64 loss: -0.17377853393554688
Batch 34/64 loss: -0.1547679901123047
Batch 35/64 loss: -0.17302566766738892
Batch 36/64 loss: -0.12362432479858398
Batch 37/64 loss: -0.16560840606689453
Batch 38/64 loss: -0.1693408489227295
Batch 39/64 loss: -0.16904789209365845
Batch 40/64 loss: -0.1391843557357788
Batch 41/64 loss: -0.16898053884506226
Batch 42/64 loss: -0.18201959133148193
Batch 43/64 loss: -0.18756306171417236
Batch 44/64 loss: -0.13930630683898926
Batch 45/64 loss: -0.19341522455215454
Batch 46/64 loss: -0.20636308193206787
Batch 47/64 loss: -0.16113930940628052
Batch 48/64 loss: -0.18902847170829773
Batch 49/64 loss: -0.17309463024139404
Batch 50/64 loss: -0.17951279878616333
Batch 51/64 loss: -0.17672353982925415
Batch 52/64 loss: -0.17269575595855713
Batch 53/64 loss: -0.16808903217315674
Batch 54/64 loss: -0.16497212648391724
Batch 55/64 loss: -0.15289974212646484
Batch 56/64 loss: -0.17100369930267334
Batch 57/64 loss: -0.1850593090057373
Batch 58/64 loss: -0.18086397647857666
Batch 59/64 loss: -0.15705031156539917
Batch 60/64 loss: -0.18714988231658936
Batch 61/64 loss: -0.11157369613647461
Batch 62/64 loss: -0.10899406671524048
Batch 63/64 loss: -0.1407334804534912
Batch 64/64 loss: -0.19034233689308167
Epoch 475  Train loss: -0.17229973042712493  Val loss: 0.06873043050471041
Epoch 476
-------------------------------
Batch 1/64 loss: -0.21371039748191833
Batch 2/64 loss: -0.20259177684783936
Batch 3/64 loss: -0.1776861548423767
Batch 4/64 loss: -0.1573960781097412
Batch 5/64 loss: -0.12658262252807617
Batch 6/64 loss: -0.12999480962753296
Batch 7/64 loss: -0.19524377584457397
Batch 8/64 loss: -0.20621442794799805
Batch 9/64 loss: -0.1740451455116272
Batch 10/64 loss: -0.18032217025756836
Batch 11/64 loss: -0.1470683217048645
Batch 12/64 loss: -0.15712440013885498
Batch 13/64 loss: -0.18768960237503052
Batch 14/64 loss: -0.20299071073532104
Batch 15/64 loss: -0.16523122787475586
Batch 16/64 loss: -0.12466675043106079
Batch 17/64 loss: -0.15678584575653076
Batch 18/64 loss: -0.2010500729084015
Batch 19/64 loss: -0.14760416746139526
Batch 20/64 loss: -0.1784524917602539
Batch 21/64 loss: -0.08983314037322998
Batch 22/64 loss: -0.2037307620048523
Batch 23/64 loss: -0.20326000452041626
Batch 24/64 loss: -0.18532544374465942
Batch 25/64 loss: -0.13320815563201904
Batch 26/64 loss: -0.174929678440094
Batch 27/64 loss: -0.18466556072235107
Batch 28/64 loss: -0.19975125789642334
Batch 29/64 loss: -0.17992812395095825
Batch 30/64 loss: -0.1645384430885315
Batch 31/64 loss: -0.2005930244922638
Batch 32/64 loss: -0.12798607349395752
Batch 33/64 loss: -0.17018377780914307
Batch 34/64 loss: -0.21012461185455322
Batch 35/64 loss: -0.19754213094711304
Batch 36/64 loss: -0.18307042121887207
Batch 37/64 loss: -0.1859668493270874
Batch 38/64 loss: -0.1729947328567505
Batch 39/64 loss: -0.12735581398010254
Batch 40/64 loss: -0.1945902705192566
Batch 41/64 loss: -0.169031023979187
Batch 42/64 loss: -0.16375881433486938
Batch 43/64 loss: -0.15549397468566895
Batch 44/64 loss: -0.07511848211288452
Batch 45/64 loss: -0.1840723156929016
Batch 46/64 loss: -0.17569762468338013
Batch 47/64 loss: -0.15492111444473267
Batch 48/64 loss: -0.17816376686096191
Batch 49/64 loss: -0.18791693449020386
Batch 50/64 loss: -0.16159558296203613
Batch 51/64 loss: -0.207858145236969
Batch 52/64 loss: -0.16703259944915771
Batch 53/64 loss: -0.20335906744003296
Batch 54/64 loss: -0.13886576890945435
Batch 55/64 loss: -0.19817402958869934
Batch 56/64 loss: -0.1807662844657898
Batch 57/64 loss: -0.160713791847229
Batch 58/64 loss: -0.1133069396018982
Batch 59/64 loss: -0.16874223947525024
Batch 60/64 loss: -0.17552155256271362
Batch 61/64 loss: -0.2037188708782196
Batch 62/64 loss: -0.1786644458770752
Batch 63/64 loss: -0.20945319533348083
Batch 64/64 loss: -0.15657800436019897
Epoch 476  Train loss: -0.17178681247374591  Val loss: 0.06641224508023344
Epoch 477
-------------------------------
Batch 1/64 loss: -0.16263693571090698
Batch 2/64 loss: -0.10394054651260376
Batch 3/64 loss: -0.17345774173736572
Batch 4/64 loss: -0.14991235733032227
Batch 5/64 loss: -0.18157368898391724
Batch 6/64 loss: -0.2106267511844635
Batch 7/64 loss: -0.20961296558380127
Batch 8/64 loss: -0.16597461700439453
Batch 9/64 loss: -0.202989399433136
Batch 10/64 loss: -0.20164847373962402
Batch 11/64 loss: -0.14384615421295166
Batch 12/64 loss: -0.1656639575958252
Batch 13/64 loss: -0.19458842277526855
Batch 14/64 loss: -0.1867552101612091
Batch 15/64 loss: -0.1670147180557251
Batch 16/64 loss: -0.1686660647392273
Batch 17/64 loss: -0.17145395278930664
Batch 18/64 loss: -0.17491096258163452
Batch 19/64 loss: -0.17521387338638306
Batch 20/64 loss: -0.19680249691009521
Batch 21/64 loss: -0.18519359827041626
Batch 22/64 loss: -0.20528453588485718
Batch 23/64 loss: -0.14046812057495117
Batch 24/64 loss: -0.17716574668884277
Batch 25/64 loss: -0.17500895261764526
Batch 26/64 loss: -0.1781545877456665
Batch 27/64 loss: -0.163516104221344
Batch 28/64 loss: -0.1541576385498047
Batch 29/64 loss: -0.18087905645370483
Batch 30/64 loss: -0.12647759914398193
Batch 31/64 loss: -0.17876797914505005
Batch 32/64 loss: -0.22574183344841003
Batch 33/64 loss: -0.18020612001419067
Batch 34/64 loss: -0.1921011209487915
Batch 35/64 loss: -0.17577606439590454
Batch 36/64 loss: -0.14192301034927368
Batch 37/64 loss: -0.14628857374191284
Batch 38/64 loss: -0.176594078540802
Batch 39/64 loss: -0.1886449158191681
Batch 40/64 loss: -0.19662830233573914
Batch 41/64 loss: -0.1441706418991089
Batch 42/64 loss: -0.18226343393325806
Batch 43/64 loss: -0.19639462232589722
Batch 44/64 loss: -0.17270678281784058
Batch 45/64 loss: -0.19006499648094177
Batch 46/64 loss: -0.19269594550132751
Batch 47/64 loss: -0.1403082013130188
Batch 48/64 loss: -0.2007613182067871
Batch 49/64 loss: -0.18708807229995728
Batch 50/64 loss: -0.21123933792114258
Batch 51/64 loss: -0.17100799083709717
Batch 52/64 loss: -0.14347487688064575
Batch 53/64 loss: -0.17956900596618652
Batch 54/64 loss: -0.17799514532089233
Batch 55/64 loss: -0.14810359477996826
Batch 56/64 loss: -0.2105448842048645
Batch 57/64 loss: -0.18030154705047607
Batch 58/64 loss: -0.18848735094070435
Batch 59/64 loss: -0.19235187768936157
Batch 60/64 loss: -0.20443996787071228
Batch 61/64 loss: -0.17646878957748413
Batch 62/64 loss: -0.1883164644241333
Batch 63/64 loss: -0.14538323879241943
Batch 64/64 loss: -0.1335688829421997
Epoch 477  Train loss: -0.17600912907544305  Val loss: 0.06943043441706916
Epoch 478
-------------------------------
Batch 1/64 loss: -0.14049065113067627
Batch 2/64 loss: -0.19155335426330566
Batch 3/64 loss: -0.1827777624130249
Batch 4/64 loss: -0.16955721378326416
Batch 5/64 loss: -0.1702030897140503
Batch 6/64 loss: -0.18956086039543152
Batch 7/64 loss: -0.18449664115905762
Batch 8/64 loss: -0.21206477284431458
Batch 9/64 loss: -0.2138221263885498
Batch 10/64 loss: -0.19071829319000244
Batch 11/64 loss: -0.15536457300186157
Batch 12/64 loss: -0.19093537330627441
Batch 13/64 loss: -0.16844892501831055
Batch 14/64 loss: -0.1891002655029297
Batch 15/64 loss: -0.20409509539604187
Batch 16/64 loss: -0.16927742958068848
Batch 17/64 loss: -0.21942049264907837
Batch 18/64 loss: -0.15810400247573853
Batch 19/64 loss: -0.1449604630470276
Batch 20/64 loss: -0.17242449522018433
Batch 21/64 loss: -0.14823490381240845
Batch 22/64 loss: -0.19460654258728027
Batch 23/64 loss: -0.1899057924747467
Batch 24/64 loss: -0.1795995831489563
Batch 25/64 loss: -0.1838657259941101
Batch 26/64 loss: -0.19346922636032104
Batch 27/64 loss: -0.1770535707473755
Batch 28/64 loss: -0.1749526858329773
Batch 29/64 loss: -0.16885435581207275
Batch 30/64 loss: -0.15191638469696045
Batch 31/64 loss: -0.13474661111831665
Batch 32/64 loss: -0.18150222301483154
Batch 33/64 loss: -0.12306195497512817
Batch 34/64 loss: -0.17613965272903442
Batch 35/64 loss: -0.18164485692977905
Batch 36/64 loss: -0.16779762506484985
Batch 37/64 loss: -0.17476248741149902
Batch 38/64 loss: -0.14138025045394897
Batch 39/64 loss: -0.16707569360733032
Batch 40/64 loss: -0.20988160371780396
Batch 41/64 loss: -0.16827791929244995
Batch 42/64 loss: -0.1539875864982605
Batch 43/64 loss: -0.21045958995819092
Batch 44/64 loss: -0.14012980461120605
Batch 45/64 loss: -0.1607927680015564
Batch 46/64 loss: -0.13381510972976685
Batch 47/64 loss: -0.12701666355133057
Batch 48/64 loss: -0.19212737679481506
Batch 49/64 loss: -0.16456663608551025
Batch 50/64 loss: -0.18372386693954468
Batch 51/64 loss: -0.1862533688545227
Batch 52/64 loss: -0.2025693655014038
Batch 53/64 loss: -0.1354474425315857
Batch 54/64 loss: -0.2196885645389557
Batch 55/64 loss: -0.15907597541809082
Batch 56/64 loss: -0.1593623161315918
Batch 57/64 loss: -0.1959584653377533
Batch 58/64 loss: -0.15391933917999268
Batch 59/64 loss: -0.17523342370986938
Batch 60/64 loss: -0.2023044228553772
Batch 61/64 loss: -0.17725753784179688
Batch 62/64 loss: -0.18942797183990479
Batch 63/64 loss: -0.18568682670593262
Batch 64/64 loss: -0.181257963180542
Epoch 478  Train loss: -0.17491534784728405  Val loss: 0.06684992526405047
Epoch 479
-------------------------------
Batch 1/64 loss: -0.20677423477172852
Batch 2/64 loss: -0.18899759650230408
Batch 3/64 loss: -0.1561177372932434
Batch 4/64 loss: -0.19142162799835205
Batch 5/64 loss: -0.1945427656173706
Batch 6/64 loss: -0.20897310972213745
Batch 7/64 loss: -0.17607033252716064
Batch 8/64 loss: -0.22495800256729126
Batch 9/64 loss: -0.21567505598068237
Batch 10/64 loss: -0.19104385375976562
Batch 11/64 loss: -0.20978271961212158
Batch 12/64 loss: -0.1770263910293579
Batch 13/64 loss: -0.18920236825942993
Batch 14/64 loss: -0.1855618953704834
Batch 15/64 loss: -0.20440256595611572
Batch 16/64 loss: -0.16200244426727295
Batch 17/64 loss: -0.1962433159351349
Batch 18/64 loss: -0.17625969648361206
Batch 19/64 loss: -0.15759289264678955
Batch 20/64 loss: -0.19946995377540588
Batch 21/64 loss: -0.2024851143360138
Batch 22/64 loss: -0.16716361045837402
Batch 23/64 loss: -0.21012839674949646
Batch 24/64 loss: -0.18026107549667358
Batch 25/64 loss: -0.19716596603393555
Batch 26/64 loss: -0.1916281282901764
Batch 27/64 loss: -0.16544806957244873
Batch 28/64 loss: -0.09119439125061035
Batch 29/64 loss: -0.18417412042617798
Batch 30/64 loss: -0.19168853759765625
Batch 31/64 loss: -0.17976796627044678
Batch 32/64 loss: -0.1793382167816162
Batch 33/64 loss: -0.17338204383850098
Batch 34/64 loss: -0.18698525428771973
Batch 35/64 loss: -0.15887236595153809
Batch 36/64 loss: -0.18848955631256104
Batch 37/64 loss: -0.16756653785705566
Batch 38/64 loss: -0.20205259323120117
Batch 39/64 loss: -0.12725096940994263
Batch 40/64 loss: -0.20558536052703857
Batch 41/64 loss: -0.1771497130393982
Batch 42/64 loss: -0.2029021978378296
Batch 43/64 loss: -0.192297101020813
Batch 44/64 loss: -0.18634164333343506
Batch 45/64 loss: -0.16890370845794678
Batch 46/64 loss: -0.20782500505447388
Batch 47/64 loss: -0.1785053014755249
Batch 48/64 loss: -0.18743112683296204
Batch 49/64 loss: -0.15605783462524414
Batch 50/64 loss: -0.17153644561767578
Batch 51/64 loss: -0.1496213674545288
Batch 52/64 loss: -0.1943557858467102
Batch 53/64 loss: -0.1753479242324829
Batch 54/64 loss: -0.13199448585510254
Batch 55/64 loss: -0.1446581482887268
Batch 56/64 loss: -0.2010955810546875
Batch 57/64 loss: -0.17053169012069702
Batch 58/64 loss: -0.15729892253875732
Batch 59/64 loss: -0.19482636451721191
Batch 60/64 loss: -0.12050729990005493
Batch 61/64 loss: -0.11733722686767578
Batch 62/64 loss: -0.1748301386833191
Batch 63/64 loss: -0.18138879537582397
Batch 64/64 loss: -0.19558385014533997
Epoch 479  Train loss: -0.1796420161630593  Val loss: 0.06927240365968947
Epoch 480
-------------------------------
Batch 1/64 loss: -0.18430614471435547
Batch 2/64 loss: -0.1822529435157776
Batch 3/64 loss: -0.14184820652008057
Batch 4/64 loss: -0.15653705596923828
Batch 5/64 loss: -0.13828837871551514
Batch 6/64 loss: -0.195247620344162
Batch 7/64 loss: -0.15562069416046143
Batch 8/64 loss: -0.1907985806465149
Batch 9/64 loss: -0.1941489279270172
Batch 10/64 loss: -0.19947940111160278
Batch 11/64 loss: -0.1984483003616333
Batch 12/64 loss: -0.14510095119476318
Batch 13/64 loss: -0.1890447735786438
Batch 14/64 loss: -0.18448764085769653
Batch 15/64 loss: -0.19005340337753296
Batch 16/64 loss: -0.13308310508728027
Batch 17/64 loss: -0.16013944149017334
Batch 18/64 loss: -0.18918654322624207
Batch 19/64 loss: -0.18745946884155273
Batch 20/64 loss: -0.19575855135917664
Batch 21/64 loss: -0.18628960847854614
Batch 22/64 loss: -0.1721402406692505
Batch 23/64 loss: -0.21752208471298218
Batch 24/64 loss: -0.19312167167663574
Batch 25/64 loss: -0.1838112473487854
Batch 26/64 loss: -0.1451016664505005
Batch 27/64 loss: -0.169103741645813
Batch 28/64 loss: -0.19504916667938232
Batch 29/64 loss: -0.17117542028427124
Batch 30/64 loss: -0.19746416807174683
Batch 31/64 loss: -0.21534013748168945
Batch 32/64 loss: -0.17276978492736816
Batch 33/64 loss: -0.20908880233764648
Batch 34/64 loss: -0.16007912158966064
Batch 35/64 loss: -0.15115869045257568
Batch 36/64 loss: -0.16658127307891846
Batch 37/64 loss: -0.18465471267700195
Batch 38/64 loss: -0.15703505277633667
Batch 39/64 loss: -0.16469275951385498
Batch 40/64 loss: -0.15786904096603394
Batch 41/64 loss: -0.14930665493011475
Batch 42/64 loss: -0.13553065061569214
Batch 43/64 loss: -0.1653769612312317
Batch 44/64 loss: -0.1725940704345703
Batch 45/64 loss: -0.173456609249115
Batch 46/64 loss: -0.2143930196762085
Batch 47/64 loss: -0.18234604597091675
Batch 48/64 loss: -0.14989686012268066
Batch 49/64 loss: -0.1698538064956665
Batch 50/64 loss: -0.21410778164863586
Batch 51/64 loss: -0.1772557497024536
Batch 52/64 loss: -0.15501487255096436
Batch 53/64 loss: -0.15223932266235352
Batch 54/64 loss: -0.16021966934204102
Batch 55/64 loss: -0.16931003332138062
Batch 56/64 loss: -0.1845569610595703
Batch 57/64 loss: -0.16784071922302246
Batch 58/64 loss: -0.20622685551643372
Batch 59/64 loss: -0.16046500205993652
Batch 60/64 loss: -0.19084572792053223
Batch 61/64 loss: -0.18245667219161987
Batch 62/64 loss: -0.15890169143676758
Batch 63/64 loss: -0.13794803619384766
Batch 64/64 loss: -0.21524375677108765
Epoch 480  Train loss: -0.1752614135835685  Val loss: 0.06883066134764157
Epoch 481
-------------------------------
Batch 1/64 loss: -0.20391631126403809
Batch 2/64 loss: -0.2152698040008545
Batch 3/64 loss: -0.17010289430618286
Batch 4/64 loss: -0.21397948265075684
Batch 5/64 loss: -0.19876456260681152
Batch 6/64 loss: -0.18856191635131836
Batch 7/64 loss: -0.19847238063812256
Batch 8/64 loss: -0.1475873589515686
Batch 9/64 loss: -0.14016121625900269
Batch 10/64 loss: -0.17497587203979492
Batch 11/64 loss: -0.16487592458724976
Batch 12/64 loss: -0.14016985893249512
Batch 13/64 loss: -0.18200504779815674
Batch 14/64 loss: -0.16599202156066895
Batch 15/64 loss: -0.18325811624526978
Batch 16/64 loss: -0.16977465152740479
Batch 17/64 loss: -0.16688674688339233
Batch 18/64 loss: -0.17140072584152222
Batch 19/64 loss: -0.15596574544906616
Batch 20/64 loss: -0.11286187171936035
Batch 21/64 loss: -0.1409093737602234
Batch 22/64 loss: -0.1308910846710205
Batch 23/64 loss: -0.1883152425289154
Batch 24/64 loss: -0.1858469843864441
Batch 25/64 loss: -0.17352116107940674
Batch 26/64 loss: -0.17743438482284546
Batch 27/64 loss: -0.19300737977027893
Batch 28/64 loss: -0.20118889212608337
Batch 29/64 loss: -0.19025731086730957
Batch 30/64 loss: -0.19102787971496582
Batch 31/64 loss: -0.17823368310928345
Batch 32/64 loss: -0.15616315603256226
Batch 33/64 loss: -0.18659096956253052
Batch 34/64 loss: -0.16351348161697388
Batch 35/64 loss: -0.19789156317710876
Batch 36/64 loss: -0.19500339031219482
Batch 37/64 loss: -0.1569836139678955
Batch 38/64 loss: -0.17614954710006714
Batch 39/64 loss: -0.15935397148132324
Batch 40/64 loss: -0.1843840479850769
Batch 41/64 loss: -0.18754056096076965
Batch 42/64 loss: -0.16083931922912598
Batch 43/64 loss: -0.1694902777671814
Batch 44/64 loss: -0.2165055274963379
Batch 45/64 loss: -0.16193091869354248
Batch 46/64 loss: -0.18715828657150269
Batch 47/64 loss: -0.21125534176826477
Batch 48/64 loss: -0.18109667301177979
Batch 49/64 loss: -0.211978018283844
Batch 50/64 loss: -0.19254690408706665
Batch 51/64 loss: -0.13696646690368652
Batch 52/64 loss: -0.15766793489456177
Batch 53/64 loss: -0.1881614327430725
Batch 54/64 loss: -0.1512211561203003
Batch 55/64 loss: -0.15188068151474
Batch 56/64 loss: -0.19930148124694824
Batch 57/64 loss: -0.15498852729797363
Batch 58/64 loss: -0.19664716720581055
Batch 59/64 loss: -0.21404936909675598
Batch 60/64 loss: -0.19852447509765625
Batch 61/64 loss: -0.18308693170547485
Batch 62/64 loss: -0.18184894323349
Batch 63/64 loss: -0.16052371263504028
Batch 64/64 loss: -0.15572619438171387
Epoch 481  Train loss: -0.17668477460449816  Val loss: 0.07009403586797289
Epoch 482
-------------------------------
Batch 1/64 loss: -0.18298029899597168
Batch 2/64 loss: -0.19246906042099
Batch 3/64 loss: -0.17882031202316284
Batch 4/64 loss: -0.20636925101280212
Batch 5/64 loss: -0.19797152280807495
Batch 6/64 loss: -0.1722956895828247
Batch 7/64 loss: -0.16692876815795898
Batch 8/64 loss: -0.17917406558990479
Batch 9/64 loss: -0.1340770721435547
Batch 10/64 loss: -0.1963309645652771
Batch 11/64 loss: -0.17281460762023926
Batch 12/64 loss: -0.16605299711227417
Batch 13/64 loss: -0.19471272826194763
Batch 14/64 loss: -0.21692651510238647
Batch 15/64 loss: -0.204181969165802
Batch 16/64 loss: -0.18502312898635864
Batch 17/64 loss: -0.17079174518585205
Batch 18/64 loss: -0.11267942190170288
Batch 19/64 loss: -0.2108212113380432
Batch 20/64 loss: -0.2028675377368927
Batch 21/64 loss: -0.19601520895957947
Batch 22/64 loss: -0.18284058570861816
Batch 23/64 loss: -0.17141729593276978
Batch 24/64 loss: -0.1711145043373108
Batch 25/64 loss: -0.1884484589099884
Batch 26/64 loss: -0.1902235448360443
Batch 27/64 loss: -0.18321597576141357
Batch 28/64 loss: -0.21292993426322937
Batch 29/64 loss: -0.14902400970458984
Batch 30/64 loss: -0.16962164640426636
Batch 31/64 loss: -0.1958010494709015
Batch 32/64 loss: -0.18603962659835815
Batch 33/64 loss: -0.11279362440109253
Batch 34/64 loss: -0.1913507580757141
Batch 35/64 loss: -0.17147332429885864
Batch 36/64 loss: -0.19309967756271362
Batch 37/64 loss: -0.16528403759002686
Batch 38/64 loss: -0.17110389471054077
Batch 39/64 loss: -0.17829149961471558
Batch 40/64 loss: -0.16320621967315674
Batch 41/64 loss: -0.19209998846054077
Batch 42/64 loss: -0.17688727378845215
Batch 43/64 loss: -0.19566380977630615
Batch 44/64 loss: -0.17475074529647827
Batch 45/64 loss: -0.13665544986724854
Batch 46/64 loss: -0.22462022304534912
Batch 47/64 loss: -0.1501328945159912
Batch 48/64 loss: -0.1496673822402954
Batch 49/64 loss: -0.15814614295959473
Batch 50/64 loss: -0.14044219255447388
Batch 51/64 loss: -0.1357002854347229
Batch 52/64 loss: -0.1677393913269043
Batch 53/64 loss: -0.17446434497833252
Batch 54/64 loss: -0.16596907377243042
Batch 55/64 loss: -0.1500605344772339
Batch 56/64 loss: -0.2093055248260498
Batch 57/64 loss: -0.1915036141872406
Batch 58/64 loss: -0.17397648096084595
Batch 59/64 loss: -0.18469679355621338
Batch 60/64 loss: -0.15593087673187256
Batch 61/64 loss: -0.19757413864135742
Batch 62/64 loss: -0.11350321769714355
Batch 63/64 loss: -0.18585222959518433
Batch 64/64 loss: -0.1783023476600647
Epoch 482  Train loss: -0.1761043625719407  Val loss: 0.07523829932884663
Epoch 483
-------------------------------
Batch 1/64 loss: -0.17073196172714233
Batch 2/64 loss: -0.10998731851577759
Batch 3/64 loss: -0.20181089639663696
Batch 4/64 loss: -0.19070351123809814
Batch 5/64 loss: -0.15902841091156006
Batch 6/64 loss: -0.19597405195236206
Batch 7/64 loss: -0.17537838220596313
Batch 8/64 loss: -0.1898256242275238
Batch 9/64 loss: -0.2011239230632782
Batch 10/64 loss: -0.19793176651000977
Batch 11/64 loss: -0.192783385515213
Batch 12/64 loss: -0.15765255689620972
Batch 13/64 loss: -0.14411675930023193
Batch 14/64 loss: -0.19900208711624146
Batch 15/64 loss: -0.20242854952812195
Batch 16/64 loss: -0.20518600940704346
Batch 17/64 loss: -0.13321900367736816
Batch 18/64 loss: -0.19144898653030396
Batch 19/64 loss: -0.11450666189193726
Batch 20/64 loss: -0.19580423831939697
Batch 21/64 loss: -0.18112313747406006
Batch 22/64 loss: -0.19119608402252197
Batch 23/64 loss: -0.15063542127609253
Batch 24/64 loss: -0.16813963651657104
Batch 25/64 loss: -0.19351336359977722
Batch 26/64 loss: -0.19329726696014404
Batch 27/64 loss: -0.19537630677223206
Batch 28/64 loss: -0.1373884677886963
Batch 29/64 loss: -0.2070731520652771
Batch 30/64 loss: -0.1538662314414978
Batch 31/64 loss: -0.19806814193725586
Batch 32/64 loss: -0.18598675727844238
Batch 33/64 loss: -0.19772011041641235
Batch 34/64 loss: -0.17850685119628906
Batch 35/64 loss: -0.21116042137145996
Batch 36/64 loss: -0.1822614073753357
Batch 37/64 loss: -0.15527808666229248
Batch 38/64 loss: -0.1880367398262024
Batch 39/64 loss: -0.16308599710464478
Batch 40/64 loss: -0.1809828281402588
Batch 41/64 loss: -0.20769596099853516
Batch 42/64 loss: -0.1782064437866211
Batch 43/64 loss: -0.14103305339813232
Batch 44/64 loss: -0.16773945093154907
Batch 45/64 loss: -0.14549237489700317
Batch 46/64 loss: -0.143610417842865
Batch 47/64 loss: -0.18111813068389893
Batch 48/64 loss: -0.1791982650756836
Batch 49/64 loss: -0.1910359263420105
Batch 50/64 loss: -0.16020923852920532
Batch 51/64 loss: -0.1790626049041748
Batch 52/64 loss: -0.1866186261177063
Batch 53/64 loss: -0.17439574003219604
Batch 54/64 loss: -0.1405673623085022
Batch 55/64 loss: -0.10820549726486206
Batch 56/64 loss: -0.1105383038520813
Batch 57/64 loss: -0.17168831825256348
Batch 58/64 loss: -0.18473964929580688
Batch 59/64 loss: -0.20022717118263245
Batch 60/64 loss: -0.1547147035598755
Batch 61/64 loss: -0.19555872678756714
Batch 62/64 loss: -0.1718774437904358
Batch 63/64 loss: -0.1692625880241394
Batch 64/64 loss: -0.16491931676864624
Epoch 483  Train loss: -0.17424040752298692  Val loss: 0.06826188883830592
Epoch 484
-------------------------------
Batch 1/64 loss: -0.18963652849197388
Batch 2/64 loss: -0.14931422472000122
Batch 3/64 loss: -0.1918727457523346
Batch 4/64 loss: -0.21661710739135742
Batch 5/64 loss: -0.20654511451721191
Batch 6/64 loss: -0.17638814449310303
Batch 7/64 loss: -0.11724913120269775
Batch 8/64 loss: -0.14169782400131226
Batch 9/64 loss: -0.19875335693359375
Batch 10/64 loss: -0.184533953666687
Batch 11/64 loss: -0.1850176453590393
Batch 12/64 loss: -0.1933819055557251
Batch 13/64 loss: -0.17921191453933716
Batch 14/64 loss: -0.18578988313674927
Batch 15/64 loss: -0.15597259998321533
Batch 16/64 loss: -0.21544569730758667
Batch 17/64 loss: -0.14334964752197266
Batch 18/64 loss: -0.18333321809768677
Batch 19/64 loss: -0.12582576274871826
Batch 20/64 loss: -0.1994619369506836
Batch 21/64 loss: -0.18834659457206726
Batch 22/64 loss: -0.19973617792129517
Batch 23/64 loss: -0.18794703483581543
Batch 24/64 loss: -0.16837066411972046
Batch 25/64 loss: -0.1883488893508911
Batch 26/64 loss: -0.19737055897712708
Batch 27/64 loss: -0.16531240940093994
Batch 28/64 loss: -0.1751360297203064
Batch 29/64 loss: -0.1273629069328308
Batch 30/64 loss: -0.19848036766052246
Batch 31/64 loss: -0.17690187692642212
Batch 32/64 loss: -0.1777830719947815
Batch 33/64 loss: -0.17479878664016724
Batch 34/64 loss: -0.16396892070770264
Batch 35/64 loss: -0.12305164337158203
Batch 36/64 loss: -0.09216701984405518
Batch 37/64 loss: -0.17485690116882324
Batch 38/64 loss: -0.12175875902175903
Batch 39/64 loss: -0.18043112754821777
Batch 40/64 loss: -0.17508679628372192
Batch 41/64 loss: -0.13863682746887207
Batch 42/64 loss: -0.19289010763168335
Batch 43/64 loss: -0.20855557918548584
Batch 44/64 loss: -0.18541914224624634
Batch 45/64 loss: -0.19708573818206787
Batch 46/64 loss: -0.17092907428741455
Batch 47/64 loss: -0.19187507033348083
Batch 48/64 loss: -0.16383355855941772
Batch 49/64 loss: -0.17851072549819946
Batch 50/64 loss: -0.19240286946296692
Batch 51/64 loss: -0.14672476053237915
Batch 52/64 loss: -0.15117216110229492
Batch 53/64 loss: -0.1761072874069214
Batch 54/64 loss: -0.160103440284729
Batch 55/64 loss: -0.19725465774536133
Batch 56/64 loss: -0.18227636814117432
Batch 57/64 loss: -0.16781526803970337
Batch 58/64 loss: -0.192244291305542
Batch 59/64 loss: -0.18994316458702087
Batch 60/64 loss: -0.17154383659362793
Batch 61/64 loss: -0.16699665784835815
Batch 62/64 loss: -0.17032599449157715
Batch 63/64 loss: -0.171270489692688
Batch 64/64 loss: -0.16553378105163574
Epoch 484  Train loss: -0.1738773691887949  Val loss: 0.06671452071658525
Epoch 485
-------------------------------
Batch 1/64 loss: -0.19944292306900024
Batch 2/64 loss: -0.21385818719863892
Batch 3/64 loss: -0.19166746735572815
Batch 4/64 loss: -0.15678322315216064
Batch 5/64 loss: -0.17766624689102173
Batch 6/64 loss: -0.15668541193008423
Batch 7/64 loss: -0.1734330654144287
Batch 8/64 loss: -0.17312127351760864
Batch 9/64 loss: -0.19371381402015686
Batch 10/64 loss: -0.17624551057815552
Batch 11/64 loss: -0.17665469646453857
Batch 12/64 loss: -0.19287097454071045
Batch 13/64 loss: -0.14935767650604248
Batch 14/64 loss: -0.17827093601226807
Batch 15/64 loss: -0.15537524223327637
Batch 16/64 loss: -0.18869462609291077
Batch 17/64 loss: -0.21919730305671692
Batch 18/64 loss: -0.16064536571502686
Batch 19/64 loss: -0.18393218517303467
Batch 20/64 loss: -0.1636103391647339
Batch 21/64 loss: -0.17372667789459229
Batch 22/64 loss: -0.13666307926177979
Batch 23/64 loss: -0.21725285053253174
Batch 24/64 loss: -0.1931207776069641
Batch 25/64 loss: -0.1435147523880005
Batch 26/64 loss: -0.18615597486495972
Batch 27/64 loss: -0.14665675163269043
Batch 28/64 loss: -0.17784404754638672
Batch 29/64 loss: -0.15611803531646729
Batch 30/64 loss: -0.16915768384933472
Batch 31/64 loss: -0.22474238276481628
Batch 32/64 loss: -0.17841309309005737
Batch 33/64 loss: -0.201299250125885
Batch 34/64 loss: -0.19970369338989258
Batch 35/64 loss: -0.18447262048721313
Batch 36/64 loss: -0.13198834657669067
Batch 37/64 loss: -0.2133089303970337
Batch 38/64 loss: -0.13361406326293945
Batch 39/64 loss: -0.18029648065567017
Batch 40/64 loss: -0.13961058855056763
Batch 41/64 loss: -0.17497241497039795
Batch 42/64 loss: -0.17865324020385742
Batch 43/64 loss: -0.15396636724472046
Batch 44/64 loss: -0.2064771056175232
Batch 45/64 loss: -0.15590667724609375
Batch 46/64 loss: -0.1744290590286255
Batch 47/64 loss: -0.13658493757247925
Batch 48/64 loss: -0.14051681756973267
Batch 49/64 loss: -0.18440499901771545
Batch 50/64 loss: -0.1636177897453308
Batch 51/64 loss: -0.16562235355377197
Batch 52/64 loss: -0.16939133405685425
Batch 53/64 loss: -0.16345763206481934
Batch 54/64 loss: -0.17350506782531738
Batch 55/64 loss: -0.17610031366348267
Batch 56/64 loss: -0.1811983585357666
Batch 57/64 loss: -0.20635557174682617
Batch 58/64 loss: -0.1608051061630249
Batch 59/64 loss: -0.18679577112197876
Batch 60/64 loss: -0.18821454048156738
Batch 61/64 loss: -0.20843178033828735
Batch 62/64 loss: -0.1653105616569519
Batch 63/64 loss: -0.1778644323348999
Batch 64/64 loss: -0.17960524559020996
Epoch 485  Train loss: -0.17562623865464155  Val loss: 0.06807259431819326
Epoch 486
-------------------------------
Batch 1/64 loss: -0.1628323793411255
Batch 2/64 loss: -0.14969980716705322
Batch 3/64 loss: -0.1511668562889099
Batch 4/64 loss: -0.20148402452468872
Batch 5/64 loss: -0.18740612268447876
Batch 6/64 loss: -0.17527318000793457
Batch 7/64 loss: -0.1936437487602234
Batch 8/64 loss: -0.1754854917526245
Batch 9/64 loss: -0.16602814197540283
Batch 10/64 loss: -0.14755666255950928
Batch 11/64 loss: -0.19887441396713257
Batch 12/64 loss: -0.2057057023048401
Batch 13/64 loss: -0.14204925298690796
Batch 14/64 loss: -0.18899056315422058
Batch 15/64 loss: -0.14515483379364014
Batch 16/64 loss: -0.1549275517463684
Batch 17/64 loss: -0.15588796138763428
Batch 18/64 loss: -0.17012512683868408
Batch 19/64 loss: -0.17864596843719482
Batch 20/64 loss: -0.19945362210273743
Batch 21/64 loss: -0.17332088947296143
Batch 22/64 loss: -0.18041211366653442
Batch 23/64 loss: -0.1718977689743042
Batch 24/64 loss: -0.16587460041046143
Batch 25/64 loss: -0.1788102388381958
Batch 26/64 loss: -0.17654848098754883
Batch 27/64 loss: -0.1921030879020691
Batch 28/64 loss: -0.1363738775253296
Batch 29/64 loss: -0.21989285945892334
Batch 30/64 loss: -0.1828378438949585
Batch 31/64 loss: -0.148565411567688
Batch 32/64 loss: -0.12924528121948242
Batch 33/64 loss: -0.20607656240463257
Batch 34/64 loss: -0.16356760263442993
Batch 35/64 loss: -0.21022498607635498
Batch 36/64 loss: -0.18547779321670532
Batch 37/64 loss: -0.2027425765991211
Batch 38/64 loss: -0.12221264839172363
Batch 39/64 loss: -0.17631065845489502
Batch 40/64 loss: -0.17274099588394165
Batch 41/64 loss: -0.1419517993927002
Batch 42/64 loss: -0.19132551550865173
Batch 43/64 loss: -0.15853846073150635
Batch 44/64 loss: -0.17980331182479858
Batch 45/64 loss: -0.18120455741882324
Batch 46/64 loss: -0.16064059734344482
Batch 47/64 loss: -0.17310667037963867
Batch 48/64 loss: -0.22280597686767578
Batch 49/64 loss: -0.13703316450119019
Batch 50/64 loss: -0.17672795057296753
Batch 51/64 loss: -0.17888712882995605
Batch 52/64 loss: -0.16112470626831055
Batch 53/64 loss: -0.18789565563201904
Batch 54/64 loss: -0.196340411901474
Batch 55/64 loss: -0.19477790594100952
Batch 56/64 loss: -0.1950860619544983
Batch 57/64 loss: -0.17913079261779785
Batch 58/64 loss: -0.20840826630592346
Batch 59/64 loss: -0.20765358209609985
Batch 60/64 loss: -0.19363367557525635
Batch 61/64 loss: -0.18499356508255005
Batch 62/64 loss: -0.18273401260375977
Batch 63/64 loss: -0.17951416969299316
Batch 64/64 loss: -0.20791244506835938
Epoch 486  Train loss: -0.1768608621522492  Val loss: 0.06699494397926986
Epoch 487
-------------------------------
Batch 1/64 loss: -0.15774834156036377
Batch 2/64 loss: -0.1928994059562683
Batch 3/64 loss: -0.174119770526886
Batch 4/64 loss: -0.18467986583709717
Batch 5/64 loss: -0.19472265243530273
Batch 6/64 loss: -0.21128225326538086
Batch 7/64 loss: -0.2029469609260559
Batch 8/64 loss: -0.14346152544021606
Batch 9/64 loss: -0.2248246669769287
Batch 10/64 loss: -0.23448088765144348
Batch 11/64 loss: -0.22105512022972107
Batch 12/64 loss: -0.15982842445373535
Batch 13/64 loss: -0.16794991493225098
Batch 14/64 loss: -0.19773155450820923
Batch 15/64 loss: -0.2108713984489441
Batch 16/64 loss: -0.15155959129333496
Batch 17/64 loss: -0.12242400646209717
Batch 18/64 loss: -0.12463241815567017
Batch 19/64 loss: -0.20004227757453918
Batch 20/64 loss: -0.20202210545539856
Batch 21/64 loss: -0.2002958357334137
Batch 22/64 loss: -0.223158597946167
Batch 23/64 loss: -0.1838485300540924
Batch 24/64 loss: -0.1801602840423584
Batch 25/64 loss: -0.20538705587387085
Batch 26/64 loss: -0.19308924674987793
Batch 27/64 loss: -0.1942943036556244
Batch 28/64 loss: -0.19251304864883423
Batch 29/64 loss: -0.19268590211868286
Batch 30/64 loss: -0.16753607988357544
Batch 31/64 loss: -0.1946437656879425
Batch 32/64 loss: -0.14184421300888062
Batch 33/64 loss: -0.19546842575073242
Batch 34/64 loss: -0.15844035148620605
Batch 35/64 loss: -0.17767363786697388
Batch 36/64 loss: -0.17205756902694702
Batch 37/64 loss: -0.15852296352386475
Batch 38/64 loss: -0.15092289447784424
Batch 39/64 loss: -0.18347805738449097
Batch 40/64 loss: -0.16792362928390503
Batch 41/64 loss: -0.18548327684402466
Batch 42/64 loss: -0.14979428052902222
Batch 43/64 loss: -0.11582690477371216
Batch 44/64 loss: -0.1968936324119568
Batch 45/64 loss: -0.1381935477256775
Batch 46/64 loss: -0.18096184730529785
Batch 47/64 loss: -0.19284069538116455
Batch 48/64 loss: -0.17631542682647705
Batch 49/64 loss: -0.1479206085205078
Batch 50/64 loss: -0.14949429035186768
Batch 51/64 loss: -0.12185072898864746
Batch 52/64 loss: -0.18864768743515015
Batch 53/64 loss: -0.19883081316947937
Batch 54/64 loss: -0.20269817113876343
Batch 55/64 loss: -0.19209903478622437
Batch 56/64 loss: -0.16967928409576416
Batch 57/64 loss: -0.12680286169052124
Batch 58/64 loss: -0.17695969343185425
Batch 59/64 loss: -0.16506540775299072
Batch 60/64 loss: -0.18907785415649414
Batch 61/64 loss: -0.18548619747161865
Batch 62/64 loss: -0.15292668342590332
Batch 63/64 loss: -0.2015603482723236
Batch 64/64 loss: -0.16636735200881958
Epoch 487  Train loss: -0.1779358796044892  Val loss: 0.06807995497975562
Epoch 488
-------------------------------
Batch 1/64 loss: -0.18546855449676514
Batch 2/64 loss: -0.20629644393920898
Batch 3/64 loss: -0.17301225662231445
Batch 4/64 loss: -0.18282848596572876
Batch 5/64 loss: -0.105837881565094
Batch 6/64 loss: -0.192116379737854
Batch 7/64 loss: -0.21930217742919922
Batch 8/64 loss: -0.19396406412124634
Batch 9/64 loss: -0.17487531900405884
Batch 10/64 loss: -0.20971477031707764
Batch 11/64 loss: -0.15636098384857178
Batch 12/64 loss: -0.19127726554870605
Batch 13/64 loss: -0.14618879556655884
Batch 14/64 loss: -0.20473366975784302
Batch 15/64 loss: -0.17418897151947021
Batch 16/64 loss: -0.14990919828414917
Batch 17/64 loss: -0.2224779725074768
Batch 18/64 loss: -0.1679295301437378
Batch 19/64 loss: -0.18060296773910522
Batch 20/64 loss: -0.1915862262248993
Batch 21/64 loss: -0.17383170127868652
Batch 22/64 loss: -0.18304240703582764
Batch 23/64 loss: -0.2131468951702118
Batch 24/64 loss: -0.174973726272583
Batch 25/64 loss: -0.2022291123867035
Batch 26/64 loss: -0.16653603315353394
Batch 27/64 loss: -0.19346600770950317
Batch 28/64 loss: -0.15224993228912354
Batch 29/64 loss: -0.1925220489501953
Batch 30/64 loss: -0.18195438385009766
Batch 31/64 loss: -0.19654834270477295
Batch 32/64 loss: -0.15246325731277466
Batch 33/64 loss: -0.1737515926361084
Batch 34/64 loss: -0.20524486899375916
Batch 35/64 loss: -0.16259634494781494
Batch 36/64 loss: -0.1569129228591919
Batch 37/64 loss: -0.1956167221069336
Batch 38/64 loss: -0.2071284055709839
Batch 39/64 loss: -0.2089339792728424
Batch 40/64 loss: -0.16172349452972412
Batch 41/64 loss: -0.1706485152244568
Batch 42/64 loss: -0.18822264671325684
Batch 43/64 loss: -0.20127710700035095
Batch 44/64 loss: -0.19351336359977722
Batch 45/64 loss: -0.12024831771850586
Batch 46/64 loss: -0.16456615924835205
Batch 47/64 loss: -0.17437833547592163
Batch 48/64 loss: -0.14204776287078857
Batch 49/64 loss: -0.19822365045547485
Batch 50/64 loss: -0.15097254514694214
Batch 51/64 loss: -0.18110227584838867
Batch 52/64 loss: -0.2071188986301422
Batch 53/64 loss: -0.19232231378555298
Batch 54/64 loss: -0.19279032945632935
Batch 55/64 loss: -0.1749434471130371
Batch 56/64 loss: -0.17264270782470703
Batch 57/64 loss: -0.18485921621322632
Batch 58/64 loss: -0.18268102407455444
Batch 59/64 loss: -0.17171204090118408
Batch 60/64 loss: -0.1690956950187683
Batch 61/64 loss: -0.2104218602180481
Batch 62/64 loss: -0.1773889660835266
Batch 63/64 loss: -0.2016305923461914
Batch 64/64 loss: -0.16406738758087158
Epoch 488  Train loss: -0.18085337105919333  Val loss: 0.06655353881239481
Epoch 489
-------------------------------
Batch 1/64 loss: -0.20496177673339844
Batch 2/64 loss: -0.1657559871673584
Batch 3/64 loss: -0.1980072259902954
Batch 4/64 loss: -0.23474296927452087
Batch 5/64 loss: -0.179290771484375
Batch 6/64 loss: -0.18148738145828247
Batch 7/64 loss: -0.18252146244049072
Batch 8/64 loss: -0.1769874095916748
Batch 9/64 loss: -0.188581645488739
Batch 10/64 loss: -0.2153225541114807
Batch 11/64 loss: -0.18009400367736816
Batch 12/64 loss: -0.17309033870697021
Batch 13/64 loss: -0.198250412940979
Batch 14/64 loss: -0.2041155993938446
Batch 15/64 loss: -0.1288570761680603
Batch 16/64 loss: -0.16473180055618286
Batch 17/64 loss: -0.2053069770336151
Batch 18/64 loss: -0.18863099813461304
Batch 19/64 loss: -0.15045768022537231
Batch 20/64 loss: -0.23440581560134888
Batch 21/64 loss: -0.18534362316131592
Batch 22/64 loss: -0.1884300410747528
Batch 23/64 loss: -0.14955216646194458
Batch 24/64 loss: -0.14757966995239258
Batch 25/64 loss: -0.19330039620399475
Batch 26/64 loss: -0.15854966640472412
Batch 27/64 loss: -0.10523331165313721
Batch 28/64 loss: -0.1870204508304596
Batch 29/64 loss: -0.17096573114395142
Batch 30/64 loss: -0.19903579354286194
Batch 31/64 loss: -0.13255351781845093
Batch 32/64 loss: -0.20116278529167175
Batch 33/64 loss: -0.20908623933792114
Batch 34/64 loss: -0.1912664771080017
Batch 35/64 loss: -0.18369323015213013
Batch 36/64 loss: -0.18700867891311646
Batch 37/64 loss: -0.19221088290214539
Batch 38/64 loss: -0.1586816906929016
Batch 39/64 loss: -0.17270570993423462
Batch 40/64 loss: -0.14853835105895996
Batch 41/64 loss: -0.15532898902893066
Batch 42/64 loss: -0.1590508222579956
Batch 43/64 loss: -0.1364383101463318
Batch 44/64 loss: -0.14758354425430298
Batch 45/64 loss: -0.15891790390014648
Batch 46/64 loss: -0.17727315425872803
Batch 47/64 loss: -0.20195549726486206
Batch 48/64 loss: -0.20206928253173828
Batch 49/64 loss: -0.13348573446273804
Batch 50/64 loss: -0.1572062373161316
Batch 51/64 loss: -0.14610451459884644
Batch 52/64 loss: -0.16894090175628662
Batch 53/64 loss: -0.22094765305519104
Batch 54/64 loss: -0.14316004514694214
Batch 55/64 loss: -0.18109309673309326
Batch 56/64 loss: -0.22758248448371887
Batch 57/64 loss: -0.18141275644302368
Batch 58/64 loss: -0.21142876148223877
Batch 59/64 loss: -0.16914314031600952
Batch 60/64 loss: -0.20221325755119324
Batch 61/64 loss: -0.22923225164413452
Batch 62/64 loss: -0.16208523511886597
Batch 63/64 loss: -0.16962748765945435
Batch 64/64 loss: -0.14286082983016968
Epoch 489  Train loss: -0.17830501935061285  Val loss: 0.0707256150819182
Epoch 490
-------------------------------
Batch 1/64 loss: -0.22040390968322754
Batch 2/64 loss: -0.15330731868743896
Batch 3/64 loss: -0.2020777463912964
Batch 4/64 loss: -0.19249117374420166
Batch 5/64 loss: -0.1576501727104187
Batch 6/64 loss: -0.17386740446090698
Batch 7/64 loss: -0.14403581619262695
Batch 8/64 loss: -0.17697012424468994
Batch 9/64 loss: -0.14808285236358643
Batch 10/64 loss: -0.17927104234695435
Batch 11/64 loss: -0.16776639223098755
Batch 12/64 loss: -0.1378345489501953
Batch 13/64 loss: -0.2069665789604187
Batch 14/64 loss: -0.18113493919372559
Batch 15/64 loss: -0.18273413181304932
Batch 16/64 loss: -0.18441033363342285
Batch 17/64 loss: -0.14717447757720947
Batch 18/64 loss: -0.21697384119033813
Batch 19/64 loss: -0.20217326283454895
Batch 20/64 loss: -0.16589033603668213
Batch 21/64 loss: -0.18052446842193604
Batch 22/64 loss: -0.20545518398284912
Batch 23/64 loss: -0.20364665985107422
Batch 24/64 loss: -0.2008969485759735
Batch 25/64 loss: -0.20765751600265503
Batch 26/64 loss: -0.20120665431022644
Batch 27/64 loss: -0.16130799055099487
Batch 28/64 loss: -0.17417091131210327
Batch 29/64 loss: -0.1767684817314148
Batch 30/64 loss: -0.13005989789962769
Batch 31/64 loss: -0.19067060947418213
Batch 32/64 loss: -0.15129804611206055
Batch 33/64 loss: -0.1940346360206604
Batch 34/64 loss: -0.18857216835021973
Batch 35/64 loss: -0.19957560300827026
Batch 36/64 loss: -0.15384477376937866
Batch 37/64 loss: -0.17183846235275269
Batch 38/64 loss: -0.16928702592849731
Batch 39/64 loss: -0.14420974254608154
Batch 40/64 loss: -0.17344576120376587
Batch 41/64 loss: -0.19868972897529602
Batch 42/64 loss: -0.13500607013702393
Batch 43/64 loss: -0.1298162341117859
Batch 44/64 loss: -0.1902022361755371
Batch 45/64 loss: -0.18839693069458008
Batch 46/64 loss: -0.1269378662109375
Batch 47/64 loss: -0.1653701663017273
Batch 48/64 loss: -0.1887122094631195
Batch 49/64 loss: -0.2107413113117218
Batch 50/64 loss: -0.09728503227233887
Batch 51/64 loss: -0.17849409580230713
Batch 52/64 loss: -0.1931554675102234
Batch 53/64 loss: -0.1417548656463623
Batch 54/64 loss: -0.17909491062164307
Batch 55/64 loss: -0.17333650588989258
Batch 56/64 loss: -0.18559110164642334
Batch 57/64 loss: -0.1928623914718628
Batch 58/64 loss: -0.12523913383483887
Batch 59/64 loss: -0.180090993642807
Batch 60/64 loss: -0.20720499753952026
Batch 61/64 loss: -0.20369237661361694
Batch 62/64 loss: -0.20091697573661804
Batch 63/64 loss: -0.1662115454673767
Batch 64/64 loss: -0.16268235445022583
Epoch 490  Train loss: -0.17569416247162165  Val loss: 0.07126030356613632
Epoch 491
-------------------------------
Batch 1/64 loss: -0.14901971817016602
Batch 2/64 loss: -0.14172399044036865
Batch 3/64 loss: -0.14865028858184814
Batch 4/64 loss: -0.18489855527877808
Batch 5/64 loss: -0.13247114419937134
Batch 6/64 loss: -0.1892438530921936
Batch 7/64 loss: -0.1587514877319336
Batch 8/64 loss: -0.20194414258003235
Batch 9/64 loss: -0.16227799654006958
Batch 10/64 loss: -0.2078258991241455
Batch 11/64 loss: -0.19425278902053833
Batch 12/64 loss: -0.11846339702606201
Batch 13/64 loss: -0.1762526035308838
Batch 14/64 loss: -0.14886122941970825
Batch 15/64 loss: -0.16594624519348145
Batch 16/64 loss: -0.19173014163970947
Batch 17/64 loss: -0.2047407329082489
Batch 18/64 loss: -0.16941434144973755
Batch 19/64 loss: -0.1630839705467224
Batch 20/64 loss: -0.19081515073776245
Batch 21/64 loss: -0.21418964862823486
Batch 22/64 loss: -0.1477346420288086
Batch 23/64 loss: -0.16503888368606567
Batch 24/64 loss: -0.15581506490707397
Batch 25/64 loss: -0.11442244052886963
Batch 26/64 loss: -0.15626782178878784
Batch 27/64 loss: -0.16797542572021484
Batch 28/64 loss: -0.18718433380126953
Batch 29/64 loss: -0.1609787940979004
Batch 30/64 loss: -0.17132723331451416
Batch 31/64 loss: -0.19562411308288574
Batch 32/64 loss: -0.15989643335342407
Batch 33/64 loss: -0.16917699575424194
Batch 34/64 loss: -0.15222805738449097
Batch 35/64 loss: -0.19429248571395874
Batch 36/64 loss: -0.17228913307189941
Batch 37/64 loss: -0.19404897093772888
Batch 38/64 loss: -0.170851469039917
Batch 39/64 loss: -0.202786386013031
Batch 40/64 loss: -0.18030273914337158
Batch 41/64 loss: -0.19296160340309143
Batch 42/64 loss: -0.1982734203338623
Batch 43/64 loss: -0.22600331902503967
Batch 44/64 loss: -0.17542719841003418
Batch 45/64 loss: -0.18227219581604004
Batch 46/64 loss: -0.19548505544662476
Batch 47/64 loss: -0.15588146448135376
Batch 48/64 loss: -0.17120420932769775
Batch 49/64 loss: -0.16404902935028076
Batch 50/64 loss: -0.19674274325370789
Batch 51/64 loss: -0.18537992238998413
Batch 52/64 loss: -0.16055333614349365
Batch 53/64 loss: -0.16553360223770142
Batch 54/64 loss: -0.14376533031463623
Batch 55/64 loss: -0.17176878452301025
Batch 56/64 loss: -0.22601622343063354
Batch 57/64 loss: -0.18628638982772827
Batch 58/64 loss: -0.20268642902374268
Batch 59/64 loss: -0.17018890380859375
Batch 60/64 loss: -0.15231245756149292
Batch 61/64 loss: -0.19164320826530457
Batch 62/64 loss: -0.21933525800704956
Batch 63/64 loss: -0.18693703413009644
Batch 64/64 loss: -0.15140283107757568
Epoch 491  Train loss: -0.17516953711416208  Val loss: 0.06474974700265734
Epoch 492
-------------------------------
Batch 1/64 loss: -0.1678851842880249
Batch 2/64 loss: -0.17446649074554443
Batch 3/64 loss: -0.10730433464050293
Batch 4/64 loss: -0.16046905517578125
Batch 5/64 loss: -0.17652755975723267
Batch 6/64 loss: -0.17323046922683716
Batch 7/64 loss: -0.17206329107284546
Batch 8/64 loss: -0.1572527289390564
Batch 9/64 loss: -0.18867921829223633
Batch 10/64 loss: -0.1858290433883667
Batch 11/64 loss: -0.17185449600219727
Batch 12/64 loss: -0.2149096131324768
Batch 13/64 loss: -0.17073893547058105
Batch 14/64 loss: -0.16748636960983276
Batch 15/64 loss: -0.20385169982910156
Batch 16/64 loss: -0.22607600688934326
Batch 17/64 loss: -0.20502665638923645
Batch 18/64 loss: -0.2022935152053833
Batch 19/64 loss: -0.20460432767868042
Batch 20/64 loss: -0.18969202041625977
Batch 21/64 loss: -0.17730331420898438
Batch 22/64 loss: -0.20960837602615356
Batch 23/64 loss: -0.15643560886383057
Batch 24/64 loss: -0.16972601413726807
Batch 25/64 loss: -0.17639130353927612
Batch 26/64 loss: -0.21390044689178467
Batch 27/64 loss: -0.15688133239746094
Batch 28/64 loss: -0.16951793432235718
Batch 29/64 loss: -0.1502683162689209
Batch 30/64 loss: -0.1871299147605896
Batch 31/64 loss: -0.15108323097229004
Batch 32/64 loss: -0.1915493607521057
Batch 33/64 loss: -0.1970927119255066
Batch 34/64 loss: -0.1842132806777954
Batch 35/64 loss: -0.16117894649505615
Batch 36/64 loss: -0.18722856044769287
Batch 37/64 loss: -0.20902001857757568
Batch 38/64 loss: -0.2039031684398651
Batch 39/64 loss: -0.2157137095928192
Batch 40/64 loss: -0.21613067388534546
Batch 41/64 loss: -0.19744518399238586
Batch 42/64 loss: -0.15680164098739624
Batch 43/64 loss: -0.17020434141159058
Batch 44/64 loss: -0.181208074092865
Batch 45/64 loss: -0.22182229161262512
Batch 46/64 loss: -0.17912545800209045
Batch 47/64 loss: -0.1480804681777954
Batch 48/64 loss: -0.16518449783325195
Batch 49/64 loss: -0.19781336188316345
Batch 50/64 loss: -0.18112558126449585
Batch 51/64 loss: -0.17573821544647217
Batch 52/64 loss: -0.1603175401687622
Batch 53/64 loss: -0.14530348777770996
Batch 54/64 loss: -0.19264477491378784
Batch 55/64 loss: -0.20536592602729797
Batch 56/64 loss: -0.20588752627372742
Batch 57/64 loss: -0.21745026111602783
Batch 58/64 loss: -0.19604778289794922
Batch 59/64 loss: -0.14641797542572021
Batch 60/64 loss: -0.13140416145324707
Batch 61/64 loss: -0.15070921182632446
Batch 62/64 loss: -0.1584787368774414
Batch 63/64 loss: -0.16002464294433594
Batch 64/64 loss: -0.08877754211425781
Epoch 492  Train loss: -0.17906982805214677  Val loss: 0.07023807858273745
Epoch 493
-------------------------------
Batch 1/64 loss: -0.15228712558746338
Batch 2/64 loss: -0.13760906457901
Batch 3/64 loss: -0.1766461730003357
Batch 4/64 loss: -0.17680728435516357
Batch 5/64 loss: -0.1611972451210022
Batch 6/64 loss: -0.15426641702651978
Batch 7/64 loss: -0.19737720489501953
Batch 8/64 loss: -0.16858285665512085
Batch 9/64 loss: -0.20298999547958374
Batch 10/64 loss: -0.21672558784484863
Batch 11/64 loss: -0.17713743448257446
Batch 12/64 loss: -0.20386600494384766
Batch 13/64 loss: -0.21333923935890198
Batch 14/64 loss: -0.16961461305618286
Batch 15/64 loss: -0.17956799268722534
Batch 16/64 loss: -0.16442787647247314
Batch 17/64 loss: -0.20230644941329956
Batch 18/64 loss: -0.15087354183197021
Batch 19/64 loss: -0.1988486349582672
Batch 20/64 loss: -0.14902561902999878
Batch 21/64 loss: -0.19760993123054504
Batch 22/64 loss: -0.18807202577590942
Batch 23/64 loss: -0.15314018726348877
Batch 24/64 loss: -0.16305536031723022
Batch 25/64 loss: -0.18152105808258057
Batch 26/64 loss: -0.1943417191505432
Batch 27/64 loss: -0.194899320602417
Batch 28/64 loss: -0.13860690593719482
Batch 29/64 loss: -0.14709967374801636
Batch 30/64 loss: -0.1653713583946228
Batch 31/64 loss: -0.20470523834228516
Batch 32/64 loss: -0.2018551528453827
Batch 33/64 loss: -0.1331356167793274
Batch 34/64 loss: -0.17145538330078125
Batch 35/64 loss: -0.19620555639266968
Batch 36/64 loss: -0.15502238273620605
Batch 37/64 loss: -0.1985662579536438
Batch 38/64 loss: -0.11135780811309814
Batch 39/64 loss: -0.16531580686569214
Batch 40/64 loss: -0.1989177167415619
Batch 41/64 loss: -0.1345043182373047
Batch 42/64 loss: -0.21111878752708435
Batch 43/64 loss: -0.20218157768249512
Batch 44/64 loss: -0.1398468017578125
Batch 45/64 loss: -0.21841976046562195
Batch 46/64 loss: -0.1822088062763214
Batch 47/64 loss: -0.17446839809417725
Batch 48/64 loss: -0.18847692012786865
Batch 49/64 loss: -0.18984222412109375
Batch 50/64 loss: -0.18934744596481323
Batch 51/64 loss: -0.1787797212600708
Batch 52/64 loss: -0.15787136554718018
Batch 53/64 loss: -0.12537086009979248
Batch 54/64 loss: -0.1298334002494812
Batch 55/64 loss: -0.147893488407135
Batch 56/64 loss: -0.21009624004364014
Batch 57/64 loss: -0.19234657287597656
Batch 58/64 loss: -0.19207966327667236
Batch 59/64 loss: -0.19398847222328186
Batch 60/64 loss: -0.17853349447250366
Batch 61/64 loss: -0.19648092985153198
Batch 62/64 loss: -0.14535468816757202
Batch 63/64 loss: -0.1749609112739563
Batch 64/64 loss: -0.1583845615386963
Epoch 493  Train loss: -0.17547520141975553  Val loss: 0.07047462279034644
Epoch 494
-------------------------------
Batch 1/64 loss: -0.16547518968582153
Batch 2/64 loss: -0.18457737565040588
Batch 3/64 loss: -0.20933979749679565
Batch 4/64 loss: -0.1793978214263916
Batch 5/64 loss: -0.1689121127128601
Batch 6/64 loss: -0.1813046634197235
Batch 7/64 loss: -0.18319225311279297
Batch 8/64 loss: -0.18526536226272583
Batch 9/64 loss: -0.17076730728149414
Batch 10/64 loss: -0.15776193141937256
Batch 11/64 loss: -0.19058546423912048
Batch 12/64 loss: -0.15741604566574097
Batch 13/64 loss: -0.17543458938598633
Batch 14/64 loss: -0.18139863014221191
Batch 15/64 loss: -0.1851579248905182
Batch 16/64 loss: -0.2113538682460785
Batch 17/64 loss: -0.18686071038246155
Batch 18/64 loss: -0.17791080474853516
Batch 19/64 loss: -0.16467833518981934
Batch 20/64 loss: -0.19015362858772278
Batch 21/64 loss: -0.14191412925720215
Batch 22/64 loss: -0.16403412818908691
Batch 23/64 loss: -0.17942148447036743
Batch 24/64 loss: -0.18233400583267212
Batch 25/64 loss: -0.2235182523727417
Batch 26/64 loss: -0.20955753326416016
Batch 27/64 loss: -0.1610623002052307
Batch 28/64 loss: -0.16202890872955322
Batch 29/64 loss: -0.18767520785331726
Batch 30/64 loss: -0.18374210596084595
Batch 31/64 loss: -0.1561146378517151
Batch 32/64 loss: -0.21546560525894165
Batch 33/64 loss: -0.19056212902069092
Batch 34/64 loss: -0.17331302165985107
Batch 35/64 loss: -0.15054452419281006
Batch 36/64 loss: -0.20600539445877075
Batch 37/64 loss: -0.19478440284729004
Batch 38/64 loss: -0.19188231229782104
Batch 39/64 loss: -0.19943103194236755
Batch 40/64 loss: -0.17511355876922607
Batch 41/64 loss: -0.191380113363266
Batch 42/64 loss: -0.1551671028137207
Batch 43/64 loss: -0.19816625118255615
Batch 44/64 loss: -0.18239033222198486
Batch 45/64 loss: -0.19850075244903564
Batch 46/64 loss: -0.17511004209518433
Batch 47/64 loss: -0.19249388575553894
Batch 48/64 loss: -0.16900646686553955
Batch 49/64 loss: -0.1437552571296692
Batch 50/64 loss: -0.1467040777206421
Batch 51/64 loss: -0.1414816975593567
Batch 52/64 loss: -0.18816065788269043
Batch 53/64 loss: -0.18008404970169067
Batch 54/64 loss: -0.22024086117744446
Batch 55/64 loss: -0.20745983719825745
Batch 56/64 loss: -0.19634544849395752
Batch 57/64 loss: -0.1479054093360901
Batch 58/64 loss: -0.1611645221710205
Batch 59/64 loss: -0.19674214720726013
Batch 60/64 loss: -0.20042404532432556
Batch 61/64 loss: -0.16636842489242554
Batch 62/64 loss: -0.1959351897239685
Batch 63/64 loss: -0.1910574734210968
Batch 64/64 loss: -0.18000739812850952
Epoch 494  Train loss: -0.18096467583787207  Val loss: 0.0691431428968292
Epoch 495
-------------------------------
Batch 1/64 loss: -0.20114237070083618
Batch 2/64 loss: -0.15913784503936768
Batch 3/64 loss: -0.1924736499786377
Batch 4/64 loss: -0.18066149950027466
Batch 5/64 loss: -0.14829957485198975
Batch 6/64 loss: -0.20285961031913757
Batch 7/64 loss: -0.21276980638504028
Batch 8/64 loss: -0.10522621870040894
Batch 9/64 loss: -0.18731921911239624
Batch 10/64 loss: -0.2015940546989441
Batch 11/64 loss: -0.17497378587722778
Batch 12/64 loss: -0.20486590266227722
Batch 13/64 loss: -0.18235552310943604
Batch 14/64 loss: -0.16992676258087158
Batch 15/64 loss: -0.21317344903945923
Batch 16/64 loss: -0.16844618320465088
Batch 17/64 loss: -0.22523409128189087
Batch 18/64 loss: -0.17215758562088013
Batch 19/64 loss: -0.18331077694892883
Batch 20/64 loss: -0.2177918255329132
Batch 21/64 loss: -0.19933944940567017
Batch 22/64 loss: -0.165729820728302
Batch 23/64 loss: -0.18715688586235046
Batch 24/64 loss: -0.1767646074295044
Batch 25/64 loss: -0.19219651818275452
Batch 26/64 loss: -0.20281368494033813
Batch 27/64 loss: -0.178092360496521
Batch 28/64 loss: -0.20070526003837585
Batch 29/64 loss: -0.1989421844482422
Batch 30/64 loss: -0.2023303508758545
Batch 31/64 loss: -0.18147408962249756
Batch 32/64 loss: -0.17684155702590942
Batch 33/64 loss: -0.18926674127578735
Batch 34/64 loss: -0.1902899146080017
Batch 35/64 loss: -0.19342631101608276
Batch 36/64 loss: -0.18913915753364563
Batch 37/64 loss: -0.16809910535812378
Batch 38/64 loss: -0.18234783411026
Batch 39/64 loss: -0.1949135661125183
Batch 40/64 loss: -0.211503803730011
Batch 41/64 loss: -0.13497543334960938
Batch 42/64 loss: -0.1570149064064026
Batch 43/64 loss: -0.19997143745422363
Batch 44/64 loss: -0.18874859809875488
Batch 45/64 loss: -0.1668742299079895
Batch 46/64 loss: -0.16921871900558472
Batch 47/64 loss: -0.1704012155532837
Batch 48/64 loss: -0.2004070281982422
Batch 49/64 loss: -0.17082363367080688
Batch 50/64 loss: -0.19696402549743652
Batch 51/64 loss: -0.18579673767089844
Batch 52/64 loss: -0.11946976184844971
Batch 53/64 loss: -0.14570122957229614
Batch 54/64 loss: -0.18209290504455566
Batch 55/64 loss: -0.19535315036773682
Batch 56/64 loss: -0.19069373607635498
Batch 57/64 loss: -0.1755048632621765
Batch 58/64 loss: -0.14100396633148193
Batch 59/64 loss: -0.21097257733345032
Batch 60/64 loss: -0.1823936402797699
Batch 61/64 loss: -0.16975408792495728
Batch 62/64 loss: -0.1736927032470703
Batch 63/64 loss: -0.1471995711326599
Batch 64/64 loss: -0.18316972255706787
Epoch 495  Train loss: -0.18192154332703236  Val loss: 0.07024795943519094
Epoch 496
-------------------------------
Batch 1/64 loss: -0.19302579760551453
Batch 2/64 loss: -0.22054585814476013
Batch 3/64 loss: -0.2186737060546875
Batch 4/64 loss: -0.19418251514434814
Batch 5/64 loss: -0.18418222665786743
Batch 6/64 loss: -0.19599702954292297
Batch 7/64 loss: -0.2053333818912506
Batch 8/64 loss: -0.17105066776275635
Batch 9/64 loss: -0.194710373878479
Batch 10/64 loss: -0.1837884783744812
Batch 11/64 loss: -0.17280668020248413
Batch 12/64 loss: -0.15557515621185303
Batch 13/64 loss: -0.20198577642440796
Batch 14/64 loss: -0.2016647756099701
Batch 15/64 loss: -0.15846681594848633
Batch 16/64 loss: -0.20594245195388794
Batch 17/64 loss: -0.1620153784751892
Batch 18/64 loss: -0.18422436714172363
Batch 19/64 loss: -0.16044890880584717
Batch 20/64 loss: -0.19164171814918518
Batch 21/64 loss: -0.20056748390197754
Batch 22/64 loss: -0.18137800693511963
Batch 23/64 loss: -0.14324456453323364
Batch 24/64 loss: -0.16660261154174805
Batch 25/64 loss: -0.18159425258636475
Batch 26/64 loss: -0.1858711838722229
Batch 27/64 loss: -0.1783561110496521
Batch 28/64 loss: -0.1843215525150299
Batch 29/64 loss: -0.15189868211746216
Batch 30/64 loss: -0.2129785120487213
Batch 31/64 loss: -0.18309110403060913
Batch 32/64 loss: -0.21063017845153809
Batch 33/64 loss: -0.17330873012542725
Batch 34/64 loss: -0.12111532688140869
Batch 35/64 loss: -0.1353626251220703
Batch 36/64 loss: -0.19628176093101501
Batch 37/64 loss: -0.20374906063079834
Batch 38/64 loss: -0.18572944402694702
Batch 39/64 loss: -0.1712334156036377
Batch 40/64 loss: -0.1611236333847046
Batch 41/64 loss: -0.17666518688201904
Batch 42/64 loss: -0.1364099383354187
Batch 43/64 loss: -0.18543854355812073
Batch 44/64 loss: -0.15146279335021973
Batch 45/64 loss: -0.17740005254745483
Batch 46/64 loss: -0.17804086208343506
Batch 47/64 loss: -0.16483432054519653
Batch 48/64 loss: -0.17848968505859375
Batch 49/64 loss: -0.17588001489639282
Batch 50/64 loss: -0.1709069013595581
Batch 51/64 loss: -0.19161644577980042
Batch 52/64 loss: -0.19510617852210999
Batch 53/64 loss: -0.18195289373397827
Batch 54/64 loss: -0.18181085586547852
Batch 55/64 loss: -0.171758770942688
Batch 56/64 loss: -0.1885060966014862
Batch 57/64 loss: -0.17427808046340942
Batch 58/64 loss: -0.18677860498428345
Batch 59/64 loss: -0.20725005865097046
Batch 60/64 loss: -0.19838953018188477
Batch 61/64 loss: -0.14504122734069824
Batch 62/64 loss: -0.1627274751663208
Batch 63/64 loss: -0.1651865839958191
Batch 64/64 loss: -0.19507703185081482
Epoch 496  Train loss: -0.18003041849416845  Val loss: 0.06775189019560404
Epoch 497
-------------------------------
Batch 1/64 loss: -0.1674102544784546
Batch 2/64 loss: -0.16348683834075928
Batch 3/64 loss: -0.2134804129600525
Batch 4/64 loss: -0.22047197818756104
Batch 5/64 loss: -0.21596607565879822
Batch 6/64 loss: -0.1990499496459961
Batch 7/64 loss: -0.17686867713928223
Batch 8/64 loss: -0.0866767168045044
Batch 9/64 loss: -0.20692431926727295
Batch 10/64 loss: -0.20906776189804077
Batch 11/64 loss: -0.14923447370529175
Batch 12/64 loss: -0.16113132238388062
Batch 13/64 loss: -0.1563776731491089
Batch 14/64 loss: -0.11090540885925293
Batch 15/64 loss: -0.20684531331062317
Batch 16/64 loss: -0.1714295744895935
Batch 17/64 loss: -0.1236182451248169
Batch 18/64 loss: -0.20934608578681946
Batch 19/64 loss: -0.20433607697486877
Batch 20/64 loss: -0.20184409618377686
Batch 21/64 loss: -0.15740609169006348
Batch 22/64 loss: -0.22464096546173096
Batch 23/64 loss: -0.19123989343643188
Batch 24/64 loss: -0.19306844472885132
Batch 25/64 loss: -0.16420888900756836
Batch 26/64 loss: -0.18647515773773193
Batch 27/64 loss: -0.19336485862731934
Batch 28/64 loss: -0.1666610836982727
Batch 29/64 loss: -0.21342778205871582
Batch 30/64 loss: -0.18738627433776855
Batch 31/64 loss: -0.21129930019378662
Batch 32/64 loss: -0.21356689929962158
Batch 33/64 loss: -0.15401345491409302
Batch 34/64 loss: -0.14799171686172485
Batch 35/64 loss: -0.20157498121261597
Batch 36/64 loss: -0.18157005310058594
Batch 37/64 loss: -0.12078940868377686
Batch 38/64 loss: -0.12889695167541504
Batch 39/64 loss: -0.18172508478164673
Batch 40/64 loss: -0.19043123722076416
Batch 41/64 loss: -0.16852957010269165
Batch 42/64 loss: -0.1495511531829834
Batch 43/64 loss: -0.14100587368011475
Batch 44/64 loss: -0.17182397842407227
Batch 45/64 loss: -0.1934795379638672
Batch 46/64 loss: -0.19627362489700317
Batch 47/64 loss: -0.16194218397140503
Batch 48/64 loss: -0.20287054777145386
Batch 49/64 loss: -0.18669873476028442
Batch 50/64 loss: -0.15715688467025757
Batch 51/64 loss: -0.1709613800048828
Batch 52/64 loss: -0.159906268119812
Batch 53/64 loss: -0.20290517807006836
Batch 54/64 loss: -0.20471912622451782
Batch 55/64 loss: -0.18190854787826538
Batch 56/64 loss: -0.16291660070419312
Batch 57/64 loss: -0.1913556158542633
Batch 58/64 loss: -0.2041688859462738
Batch 59/64 loss: -0.18972867727279663
Batch 60/64 loss: -0.15406179428100586
Batch 61/64 loss: -0.17720520496368408
Batch 62/64 loss: -0.15340906381607056
Batch 63/64 loss: -0.2130994200706482
Batch 64/64 loss: -0.18942981958389282
Epoch 497  Train loss: -0.17885427451601216  Val loss: 0.07018502860544473
Epoch 498
-------------------------------
Batch 1/64 loss: -0.1771918535232544
Batch 2/64 loss: -0.19150876998901367
Batch 3/64 loss: -0.08048480749130249
Batch 4/64 loss: -0.17067992687225342
Batch 5/64 loss: -0.1784624457359314
Batch 6/64 loss: -0.1686258316040039
Batch 7/64 loss: -0.19965001940727234
Batch 8/64 loss: -0.17369890213012695
Batch 9/64 loss: -0.1931764781475067
Batch 10/64 loss: -0.1367490291595459
Batch 11/64 loss: -0.14598077535629272
Batch 12/64 loss: -0.18544277548789978
Batch 13/64 loss: -0.1556645631790161
Batch 14/64 loss: -0.17407578229904175
Batch 15/64 loss: -0.20877575874328613
Batch 16/64 loss: -0.15248608589172363
Batch 17/64 loss: -0.2112889289855957
Batch 18/64 loss: -0.15613305568695068
Batch 19/64 loss: -0.18236970901489258
Batch 20/64 loss: -0.19012534618377686
Batch 21/64 loss: -0.17668390274047852
Batch 22/64 loss: -0.18043625354766846
Batch 23/64 loss: -0.1849518120288849
Batch 24/64 loss: -0.19078576564788818
Batch 25/64 loss: -0.15210294723510742
Batch 26/64 loss: -0.17643767595291138
Batch 27/64 loss: -0.17683666944503784
Batch 28/64 loss: -0.13233691453933716
Batch 29/64 loss: -0.10583001375198364
Batch 30/64 loss: -0.1721763014793396
Batch 31/64 loss: -0.14796346426010132
Batch 32/64 loss: -0.17680740356445312
Batch 33/64 loss: -0.2013043761253357
Batch 34/64 loss: -0.18888422846794128
Batch 35/64 loss: -0.1726664900779724
Batch 36/64 loss: -0.1997089982032776
Batch 37/64 loss: -0.17280519008636475
Batch 38/64 loss: -0.1996699571609497
Batch 39/64 loss: -0.17922157049179077
Batch 40/64 loss: -0.20333486795425415
Batch 41/64 loss: -0.20464563369750977
Batch 42/64 loss: -0.19568923115730286
Batch 43/64 loss: -0.18810465931892395
Batch 44/64 loss: -0.17790687084197998
Batch 45/64 loss: -0.19550642371177673
Batch 46/64 loss: -0.2013404369354248
Batch 47/64 loss: -0.21028226613998413
Batch 48/64 loss: -0.16998225450515747
Batch 49/64 loss: -0.20094913244247437
Batch 50/64 loss: -0.18991312384605408
Batch 51/64 loss: -0.15983808040618896
Batch 52/64 loss: -0.1799589991569519
Batch 53/64 loss: -0.12514150142669678
Batch 54/64 loss: -0.1640494465827942
Batch 55/64 loss: -0.15291571617126465
Batch 56/64 loss: -0.1727425456047058
Batch 57/64 loss: -0.19413048028945923
Batch 58/64 loss: -0.2101641595363617
Batch 59/64 loss: -0.17198455333709717
Batch 60/64 loss: -0.16237974166870117
Batch 61/64 loss: -0.1871321201324463
Batch 62/64 loss: -0.18368536233901978
Batch 63/64 loss: -0.19251367449760437
Batch 64/64 loss: -0.14899325370788574
Epoch 498  Train loss: -0.17609752159492642  Val loss: 0.07420913972395801
Epoch 499
-------------------------------
Batch 1/64 loss: -0.2010226845741272
Batch 2/64 loss: -0.1882806122303009
Batch 3/64 loss: -0.21911579370498657
Batch 4/64 loss: -0.1986810863018036
Batch 5/64 loss: -0.14814192056655884
Batch 6/64 loss: -0.1816316843032837
Batch 7/64 loss: -0.1456143856048584
Batch 8/64 loss: -0.18233174085617065
Batch 9/64 loss: -0.21100622415542603
Batch 10/64 loss: -0.20627307891845703
Batch 11/64 loss: -0.21246638894081116
Batch 12/64 loss: -0.1567167043685913
Batch 13/64 loss: -0.20382213592529297
Batch 14/64 loss: -0.1568312644958496
Batch 15/64 loss: -0.14367365837097168
Batch 16/64 loss: -0.14881974458694458
Batch 17/64 loss: -0.20288929343223572
Batch 18/64 loss: -0.14572250843048096
Batch 19/64 loss: -0.17613738775253296
Batch 20/64 loss: -0.20038825273513794
Batch 21/64 loss: -0.18473941087722778
Batch 22/64 loss: -0.17283791303634644
Batch 23/64 loss: -0.19418734312057495
Batch 24/64 loss: -0.20570504665374756
Batch 25/64 loss: -0.1954994797706604
Batch 26/64 loss: -0.17990559339523315
Batch 27/64 loss: -0.17081677913665771
Batch 28/64 loss: -0.17277860641479492
Batch 29/64 loss: -0.15655720233917236
Batch 30/64 loss: -0.18800565600395203
Batch 31/64 loss: -0.16603457927703857
Batch 32/64 loss: -0.1779915690422058
Batch 33/64 loss: -0.16513162851333618
Batch 34/64 loss: -0.20296400785446167
Batch 35/64 loss: -0.1297510266304016
Batch 36/64 loss: -0.22003453969955444
Batch 37/64 loss: -0.2150384783744812
Batch 38/64 loss: -0.1819610595703125
Batch 39/64 loss: -0.21054935455322266
Batch 40/64 loss: -0.20741963386535645
Batch 41/64 loss: -0.19770443439483643
Batch 42/64 loss: -0.1527308225631714
Batch 43/64 loss: -0.11321699619293213
Batch 44/64 loss: -0.17306870222091675
Batch 45/64 loss: -0.18092560768127441
Batch 46/64 loss: -0.16782712936401367
Batch 47/64 loss: -0.19423723220825195
Batch 48/64 loss: -0.16322952508926392
Batch 49/64 loss: -0.18452519178390503
Batch 50/64 loss: -0.16772443056106567
Batch 51/64 loss: -0.15394830703735352
Batch 52/64 loss: -0.15599340200424194
Batch 53/64 loss: -0.17239594459533691
Batch 54/64 loss: -0.18516778945922852
Batch 55/64 loss: -0.1719985008239746
Batch 56/64 loss: -0.1605607271194458
Batch 57/64 loss: -0.1397746205329895
Batch 58/64 loss: -0.18821299076080322
Batch 59/64 loss: -0.19469135999679565
Batch 60/64 loss: -0.18652695417404175
Batch 61/64 loss: -0.1754869818687439
Batch 62/64 loss: -0.1661078929901123
Batch 63/64 loss: -0.19667130708694458
Batch 64/64 loss: -0.18049147725105286
Epoch 499  Train loss: -0.1789109163424548  Val loss: 0.0691385099158664
Epoch 500
-------------------------------
Batch 1/64 loss: -0.16609692573547363
Batch 2/64 loss: -0.1859082281589508
Batch 3/64 loss: -0.16209477186203003
Batch 4/64 loss: -0.13603121042251587
Batch 5/64 loss: -0.1755281686782837
Batch 6/64 loss: -0.19887405633926392
Batch 7/64 loss: -0.21764487028121948
Batch 8/64 loss: -0.16343295574188232
Batch 9/64 loss: -0.1957937777042389
Batch 10/64 loss: -0.20197296142578125
Batch 11/64 loss: -0.19266098737716675
Batch 12/64 loss: -0.19193610548973083
Batch 13/64 loss: -0.1965368390083313
Batch 14/64 loss: -0.19919037818908691
Batch 15/64 loss: -0.12079548835754395
Batch 16/64 loss: -0.16231626272201538
Batch 17/64 loss: -0.16832774877548218
Batch 18/64 loss: -0.19066697359085083
Batch 19/64 loss: -0.1872345209121704
Batch 20/64 loss: -0.17570757865905762
Batch 21/64 loss: -0.14690923690795898
Batch 22/64 loss: -0.1312379240989685
Batch 23/64 loss: -0.15553444623947144
Batch 24/64 loss: -0.19934263825416565
Batch 25/64 loss: -0.21397143602371216
Batch 26/64 loss: -0.18970048427581787
Batch 27/64 loss: -0.1627233624458313
Batch 28/64 loss: -0.21378028392791748
Batch 29/64 loss: -0.1676519513130188
Batch 30/64 loss: -0.18039554357528687
Batch 31/64 loss: -0.1926576793193817
Batch 32/64 loss: -0.17839515209197998
Batch 33/64 loss: -0.18227732181549072
Batch 34/64 loss: -0.20479053258895874
Batch 35/64 loss: -0.16438394784927368
Batch 36/64 loss: -0.1864144206047058
Batch 37/64 loss: -0.1777958869934082
Batch 38/64 loss: -0.17110604047775269
Batch 39/64 loss: -0.1507313847541809
Batch 40/64 loss: -0.1811389923095703
Batch 41/64 loss: -0.1710793375968933
Batch 42/64 loss: -0.19055703282356262
Batch 43/64 loss: -0.21166479587554932
Batch 44/64 loss: -0.15596222877502441
Batch 45/64 loss: -0.19228601455688477
Batch 46/64 loss: -0.20393040776252747
Batch 47/64 loss: -0.19040611386299133
Batch 48/64 loss: -0.1786484718322754
Batch 49/64 loss: -0.183671772480011
Batch 50/64 loss: -0.1724514365196228
Batch 51/64 loss: -0.17696607112884521
Batch 52/64 loss: -0.18056070804595947
Batch 53/64 loss: -0.1925506591796875
Batch 54/64 loss: -0.14796984195709229
Batch 55/64 loss: -0.1852366328239441
Batch 56/64 loss: -0.20335015654563904
Batch 57/64 loss: -0.1600511074066162
Batch 58/64 loss: -0.21559980511665344
Batch 59/64 loss: -0.19766449928283691
Batch 60/64 loss: -0.1974410116672516
Batch 61/64 loss: -0.15414315462112427
Batch 62/64 loss: -0.1438794732093811
Batch 63/64 loss: -0.1733739972114563
Batch 64/64 loss: -0.2008894681930542
Epoch 500  Train loss: -0.17991845187018898  Val loss: 0.07113731460472972
SLIC undersegmentation error: 0.12412920962199316
SLIC inter-cluster variation: 0.13904419774313004
SLIC number of superpixels: 21483
SLIC superpixels per image: 73.82474226804123
Model loaded
Test metrics:
0.042652021978319306 0.31395876288659796 28.97107445573888 tensor(0.2652, dtype=torch.float64) 0.8411469000186185 3.750214112828151 26855
Inference time: 0.0024002704423727447 seconds
Relabeled undersegmentation error: 0.09533470790378007
Relabeled inter-cluster variation: 0.052087818883798
Relabeled mean superpixels count: 346.0893470790378
Original mean superpixels count: 92.2852233676976
Done!
Job id: 420575
Job id: 422909
