Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 0.41658854484558105
Batch 2/64 loss: 0.35150033235549927
Batch 3/64 loss: 0.3336864709854126
Batch 4/64 loss: 0.32607460021972656
Batch 5/64 loss: 0.32463908195495605
Batch 6/64 loss: 0.3213188648223877
Batch 7/64 loss: 0.3183854818344116
Batch 8/64 loss: 0.31365716457366943
Batch 9/64 loss: 0.31408601999282837
Batch 10/64 loss: 0.30944472551345825
Batch 11/64 loss: 0.3045691251754761
Batch 12/64 loss: 0.3150925636291504
Batch 13/64 loss: 0.30753135681152344
Batch 14/64 loss: 0.3010828495025635
Batch 15/64 loss: 0.3114013671875
Batch 16/64 loss: 0.30254578590393066
Batch 17/64 loss: 0.3041573762893677
Batch 18/64 loss: 0.30450451374053955
Batch 19/64 loss: 0.2979309558868408
Batch 20/64 loss: 0.29828596115112305
Batch 21/64 loss: 0.2915278673171997
Batch 22/64 loss: 0.29778677225112915
Batch 23/64 loss: 0.29901623725891113
Batch 24/64 loss: 0.2941819429397583
Batch 25/64 loss: 0.29000741243362427
Batch 26/64 loss: 0.29016590118408203
Batch 27/64 loss: 0.2834055423736572
Batch 28/64 loss: 0.2851066589355469
Batch 29/64 loss: 0.28937840461730957
Batch 30/64 loss: 0.2888481020927429
Batch 31/64 loss: 0.28567421436309814
Batch 32/64 loss: 0.2922250032424927
Batch 33/64 loss: 0.2876631021499634
Batch 34/64 loss: 0.2864665985107422
Batch 35/64 loss: 0.28900134563446045
Batch 36/64 loss: 0.2824503183364868
Batch 37/64 loss: 0.2843945026397705
Batch 38/64 loss: 0.28443944454193115
Batch 39/64 loss: 0.27740341424942017
Batch 40/64 loss: 0.28348881006240845
Batch 41/64 loss: 0.2785531282424927
Batch 42/64 loss: 0.2793464660644531
Batch 43/64 loss: 0.27855443954467773
Batch 44/64 loss: 0.27657628059387207
Batch 45/64 loss: 0.2737947106361389
Batch 46/64 loss: 0.2727128863334656
Batch 47/64 loss: 0.2757183909416199
Batch 48/64 loss: 0.27275729179382324
Batch 49/64 loss: 0.27640974521636963
Batch 50/64 loss: 0.2658255100250244
Batch 51/64 loss: 0.2775542140007019
Batch 52/64 loss: 0.277302622795105
Batch 53/64 loss: 0.2745199203491211
Batch 54/64 loss: 0.26860612630844116
Batch 55/64 loss: 0.2769681215286255
Batch 56/64 loss: 0.26739680767059326
Batch 57/64 loss: 0.27157139778137207
Batch 58/64 loss: 0.2714410424232483
Batch 59/64 loss: 0.26673054695129395
Batch 60/64 loss: 0.26715272665023804
Batch 61/64 loss: 0.2714001536369324
Batch 62/64 loss: 0.2761945128440857
Batch 63/64 loss: 0.26815539598464966
Batch 64/64 loss: 0.2742350101470947
Epoch 1  Train loss: 0.2922358185637231  Val loss: 0.28024952333817366
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 0.2692226767539978
Batch 2/64 loss: 0.26012033224105835
Batch 3/64 loss: 0.2655590772628784
Batch 4/64 loss: 0.26197725534439087
Batch 5/64 loss: 0.2544494867324829
Batch 6/64 loss: 0.259006142616272
Batch 7/64 loss: 0.26493704319000244
Batch 8/64 loss: 0.2609063982963562
Batch 9/64 loss: 0.26669245958328247
Batch 10/64 loss: 0.25103461742401123
Batch 11/64 loss: 0.252518892288208
Batch 12/64 loss: 0.25763726234436035
Batch 13/64 loss: 0.2583049535751343
Batch 14/64 loss: 0.25679725408554077
Batch 15/64 loss: 0.26074618101119995
Batch 16/64 loss: 0.26816844940185547
Batch 17/64 loss: 0.26473939418792725
Batch 18/64 loss: 0.2592679262161255
Batch 19/64 loss: 0.2613125443458557
Batch 20/64 loss: 0.24991321563720703
Batch 21/64 loss: 0.2600886821746826
Batch 22/64 loss: 0.25704413652420044
Batch 23/64 loss: 0.25417858362197876
Batch 24/64 loss: 0.2641737461090088
Batch 25/64 loss: 0.26656579971313477
Batch 26/64 loss: 0.24894404411315918
Batch 27/64 loss: 0.25764548778533936
Batch 28/64 loss: 0.24092382192611694
Batch 29/64 loss: 0.2544071078300476
Batch 30/64 loss: 0.24976855516433716
Batch 31/64 loss: 0.25217652320861816
Batch 32/64 loss: 0.2360159158706665
Batch 33/64 loss: 0.24894022941589355
Batch 34/64 loss: 0.248482346534729
Batch 35/64 loss: 0.2513773441314697
Batch 36/64 loss: 0.25040459632873535
Batch 37/64 loss: 0.2464807629585266
Batch 38/64 loss: 0.2501380443572998
Batch 39/64 loss: 0.23394018411636353
Batch 40/64 loss: 0.2525111436843872
Batch 41/64 loss: 0.25070029497146606
Batch 42/64 loss: 0.2451568841934204
Batch 43/64 loss: 0.24830812215805054
Batch 44/64 loss: 0.24735915660858154
Batch 45/64 loss: 0.2355489730834961
Batch 46/64 loss: 0.2543560266494751
Batch 47/64 loss: 0.2453504204750061
Batch 48/64 loss: 0.24309611320495605
Batch 49/64 loss: 0.25109589099884033
Batch 50/64 loss: 0.24104273319244385
Batch 51/64 loss: 0.2440640926361084
Batch 52/64 loss: 0.2406238317489624
Batch 53/64 loss: 0.24945330619812012
Batch 54/64 loss: 0.24061214923858643
Batch 55/64 loss: 0.24484038352966309
Batch 56/64 loss: 0.24300801753997803
Batch 57/64 loss: 0.24382346868515015
Batch 58/64 loss: 0.2355647087097168
Batch 59/64 loss: 0.25314825773239136
Batch 60/64 loss: 0.24676716327667236
Batch 61/64 loss: 0.23677849769592285
Batch 62/64 loss: 0.2297939658164978
Batch 63/64 loss: 0.22586870193481445
Batch 64/64 loss: 0.24081957340240479
Epoch 2  Train loss: 0.2510508934656779  Val loss: 0.24795323856098136
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 0.23506200313568115
Batch 2/64 loss: 0.2363561987876892
Batch 3/64 loss: 0.24270951747894287
Batch 4/64 loss: 0.23185330629348755
Batch 5/64 loss: 0.23156821727752686
Batch 6/64 loss: 0.2352762222290039
Batch 7/64 loss: 0.24601352214813232
Batch 8/64 loss: 0.2301691174507141
Batch 9/64 loss: 0.23088407516479492
Batch 10/64 loss: 0.2259838581085205
Batch 11/64 loss: 0.24232113361358643
Batch 12/64 loss: 0.22964227199554443
Batch 13/64 loss: 0.23504042625427246
Batch 14/64 loss: 0.23861032724380493
Batch 15/64 loss: 0.23314762115478516
Batch 16/64 loss: 0.2255328893661499
Batch 17/64 loss: 0.24231958389282227
Batch 18/64 loss: 0.23562228679656982
Batch 19/64 loss: 0.22541022300720215
Batch 20/64 loss: 0.22879785299301147
Batch 21/64 loss: 0.21825039386749268
Batch 22/64 loss: 0.2200084924697876
Batch 23/64 loss: 0.2417818307876587
Batch 24/64 loss: 0.23756945133209229
Batch 25/64 loss: 0.22867608070373535
Batch 26/64 loss: 0.2277173399925232
Batch 27/64 loss: 0.2268604040145874
Batch 28/64 loss: 0.24303358793258667
Batch 29/64 loss: 0.2188553810119629
Batch 30/64 loss: 0.2276076078414917
Batch 31/64 loss: 0.22953081130981445
Batch 32/64 loss: 0.23550212383270264
Batch 33/64 loss: 0.23297220468521118
Batch 34/64 loss: 0.24214059114456177
Batch 35/64 loss: 0.2223672866821289
Batch 36/64 loss: 0.22807717323303223
Batch 37/64 loss: 0.2314557433128357
Batch 38/64 loss: 0.22940480709075928
Batch 39/64 loss: 0.22680485248565674
Batch 40/64 loss: 0.2337825894355774
Batch 41/64 loss: 0.23958098888397217
Batch 42/64 loss: 0.23702239990234375
Batch 43/64 loss: 0.2324122190475464
Batch 44/64 loss: 0.23264765739440918
Batch 45/64 loss: 0.227089524269104
Batch 46/64 loss: 0.22391599416732788
Batch 47/64 loss: 0.22394156455993652
Batch 48/64 loss: 0.2380509376525879
Batch 49/64 loss: 0.2225620150566101
Batch 50/64 loss: 0.22265124320983887
Batch 51/64 loss: 0.23675638437271118
Batch 52/64 loss: 0.21570950746536255
Batch 53/64 loss: 0.22020965814590454
Batch 54/64 loss: 0.2327149510383606
Batch 55/64 loss: 0.22518837451934814
Batch 56/64 loss: 0.22300589084625244
Batch 57/64 loss: 0.23767244815826416
Batch 58/64 loss: 0.233878493309021
Batch 59/64 loss: 0.2125566601753235
Batch 60/64 loss: 0.22324633598327637
Batch 61/64 loss: 0.2188040018081665
Batch 62/64 loss: 0.22548192739486694
Batch 63/64 loss: 0.2183823585510254
Batch 64/64 loss: 0.2233206033706665
Epoch 3  Train loss: 0.23020692385879218  Val loss: 0.23730581056621067
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 0.23392611742019653
Batch 2/64 loss: 0.2309665083885193
Batch 3/64 loss: 0.2238527536392212
Batch 4/64 loss: 0.22150558233261108
Batch 5/64 loss: 0.22081899642944336
Batch 6/64 loss: 0.22147023677825928
Batch 7/64 loss: 0.21929311752319336
Batch 8/64 loss: 0.23128652572631836
Batch 9/64 loss: 0.2225363850593567
Batch 10/64 loss: 0.23924791812896729
Batch 11/64 loss: 0.21702814102172852
Batch 12/64 loss: 0.22215789556503296
Batch 13/64 loss: 0.22174745798110962
Batch 14/64 loss: 0.20599228143692017
Batch 15/64 loss: 0.2196214199066162
Batch 16/64 loss: 0.21772515773773193
Batch 17/64 loss: 0.20097965002059937
Batch 18/64 loss: 0.2291398048400879
Batch 19/64 loss: 0.22694003582000732
Batch 20/64 loss: 0.2218838930130005
Batch 21/64 loss: 0.22512519359588623
Batch 22/64 loss: 0.21618080139160156
Batch 23/64 loss: 0.22510051727294922
Batch 24/64 loss: 0.2178189754486084
Batch 25/64 loss: 0.21868562698364258
Batch 26/64 loss: 0.21867704391479492
Batch 27/64 loss: 0.21851211786270142
Batch 28/64 loss: 0.22068268060684204
Batch 29/64 loss: 0.23135066032409668
Batch 30/64 loss: 0.2318875789642334
Batch 31/64 loss: 0.22298848628997803
Batch 32/64 loss: 0.2258455753326416
Batch 33/64 loss: 0.22796237468719482
Batch 34/64 loss: 0.22598767280578613
Batch 35/64 loss: 0.22471582889556885
Batch 36/64 loss: 0.22266066074371338
Batch 37/64 loss: 0.223749041557312
Batch 38/64 loss: 0.21808010339736938
Batch 39/64 loss: 0.2226516604423523
Batch 40/64 loss: 0.21980595588684082
Batch 41/64 loss: 0.21756649017333984
Batch 42/64 loss: 0.21935003995895386
Batch 43/64 loss: 0.22045886516571045
Batch 44/64 loss: 0.22520864009857178
Batch 45/64 loss: 0.22621965408325195
Batch 46/64 loss: 0.2189708948135376
Batch 47/64 loss: 0.21099376678466797
Batch 48/64 loss: 0.2148650884628296
Batch 49/64 loss: 0.21700835227966309
Batch 50/64 loss: 0.2329118251800537
Batch 51/64 loss: 0.211439847946167
Batch 52/64 loss: 0.21001851558685303
Batch 53/64 loss: 0.2184356451034546
Batch 54/64 loss: 0.21890640258789062
Batch 55/64 loss: 0.2268412709236145
Batch 56/64 loss: 0.22589731216430664
Batch 57/64 loss: 0.220345139503479
Batch 58/64 loss: 0.21207880973815918
Batch 59/64 loss: 0.21806329488754272
Batch 60/64 loss: 0.2162569761276245
Batch 61/64 loss: 0.21389293670654297
Batch 62/64 loss: 0.21524792909622192
Batch 63/64 loss: 0.22123122215270996
Batch 64/64 loss: 0.20643079280853271
Epoch 4  Train loss: 0.22107639920477773  Val loss: 0.22392013351532192
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 0.20990818738937378
Batch 2/64 loss: 0.22414898872375488
Batch 3/64 loss: 0.21413862705230713
Batch 4/64 loss: 0.21816325187683105
Batch 5/64 loss: 0.22101855278015137
Batch 6/64 loss: 0.20559561252593994
Batch 7/64 loss: 0.21492242813110352
Batch 8/64 loss: 0.20132803916931152
Batch 9/64 loss: 0.21213340759277344
Batch 10/64 loss: 0.2257765531539917
Batch 11/64 loss: 0.21241801977157593
Batch 12/64 loss: 0.21571969985961914
Batch 13/64 loss: 0.21384644508361816
Batch 14/64 loss: 0.2102717161178589
Batch 15/64 loss: 0.21697115898132324
Batch 16/64 loss: 0.18923473358154297
Batch 17/64 loss: 0.20795947313308716
Batch 18/64 loss: 0.19806623458862305
Batch 19/64 loss: 0.20731723308563232
Batch 20/64 loss: 0.21121621131896973
Batch 21/64 loss: 0.22601759433746338
Batch 22/64 loss: 0.21062588691711426
Batch 23/64 loss: 0.20768553018569946
Batch 24/64 loss: 0.20445823669433594
Batch 25/64 loss: 0.21317708492279053
Batch 26/64 loss: 0.20037555694580078
Batch 27/64 loss: 0.21040767431259155
Batch 28/64 loss: 0.21129679679870605
Batch 29/64 loss: 0.21777594089508057
Batch 30/64 loss: 0.22469449043273926
Batch 31/64 loss: 0.21472299098968506
Batch 32/64 loss: 0.19725406169891357
Batch 33/64 loss: 0.21388691663742065
Batch 34/64 loss: 0.2010323405265808
Batch 35/64 loss: 0.21051394939422607
Batch 36/64 loss: 0.20010697841644287
Batch 37/64 loss: 0.21624451875686646
Batch 38/64 loss: 0.20283687114715576
Batch 39/64 loss: 0.21381700038909912
Batch 40/64 loss: 0.19224131107330322
Batch 41/64 loss: 0.2092854380607605
Batch 42/64 loss: 0.20738154649734497
Batch 43/64 loss: 0.21362292766571045
Batch 44/64 loss: 0.222448468208313
Batch 45/64 loss: 0.19813024997711182
Batch 46/64 loss: 0.20821917057037354
Batch 47/64 loss: 0.20935237407684326
Batch 48/64 loss: 0.21569585800170898
Batch 49/64 loss: 0.20145320892333984
Batch 50/64 loss: 0.20302081108093262
Batch 51/64 loss: 0.2058330774307251
Batch 52/64 loss: 0.1969078779220581
Batch 53/64 loss: 0.20405220985412598
Batch 54/64 loss: 0.20312905311584473
Batch 55/64 loss: 0.1998540163040161
Batch 56/64 loss: 0.1917058825492859
Batch 57/64 loss: 0.2024315595626831
Batch 58/64 loss: 0.22077107429504395
Batch 59/64 loss: 0.1987752914428711
Batch 60/64 loss: 0.21116948127746582
Batch 61/64 loss: 0.19961845874786377
Batch 62/64 loss: 0.2077111005783081
Batch 63/64 loss: 0.19781804084777832
Batch 64/64 loss: 0.19575011730194092
Epoch 5  Train loss: 0.20854158541735482  Val loss: 0.20970612948702783
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 0.19043445587158203
Batch 2/64 loss: 0.1999913454055786
Batch 3/64 loss: 0.19355446100234985
Batch 4/64 loss: 0.1911487579345703
Batch 5/64 loss: 0.19982564449310303
Batch 6/64 loss: 0.20223575830459595
Batch 7/64 loss: 0.19616639614105225
Batch 8/64 loss: 0.1811814308166504
Batch 9/64 loss: 0.20023828744888306
Batch 10/64 loss: 0.1889548897743225
Batch 11/64 loss: 0.20292985439300537
Batch 12/64 loss: 0.2049347162246704
Batch 13/64 loss: 0.20135188102722168
Batch 14/64 loss: 0.202897310256958
Batch 15/64 loss: 0.20439720153808594
Batch 16/64 loss: 0.18983972072601318
Batch 17/64 loss: 0.19562405347824097
Batch 18/64 loss: 0.20302271842956543
Batch 19/64 loss: 0.20001542568206787
Batch 20/64 loss: 0.20379674434661865
Batch 21/64 loss: 0.2061651349067688
Batch 22/64 loss: 0.19997048377990723
Batch 23/64 loss: 0.18797290325164795
Batch 24/64 loss: 0.20144206285476685
Batch 25/64 loss: 0.1998659372329712
Batch 26/64 loss: 0.21065354347229004
Batch 27/64 loss: 0.2072964906692505
Batch 28/64 loss: 0.1986885666847229
Batch 29/64 loss: 0.20244622230529785
Batch 30/64 loss: 0.2022554874420166
Batch 31/64 loss: 0.22510582208633423
Batch 32/64 loss: 0.20132875442504883
Batch 33/64 loss: 0.20744454860687256
Batch 34/64 loss: 0.2007840871810913
Batch 35/64 loss: 0.20416319370269775
Batch 36/64 loss: 0.1921645998954773
Batch 37/64 loss: 0.1972554326057434
Batch 38/64 loss: 0.20364636182785034
Batch 39/64 loss: 0.21327126026153564
Batch 40/64 loss: 0.205682635307312
Batch 41/64 loss: 0.19198369979858398
Batch 42/64 loss: 0.18818426132202148
Batch 43/64 loss: 0.21801066398620605
Batch 44/64 loss: 0.20042550563812256
Batch 45/64 loss: 0.19625210762023926
Batch 46/64 loss: 0.1951746940612793
Batch 47/64 loss: 0.18824291229248047
Batch 48/64 loss: 0.19977378845214844
Batch 49/64 loss: 0.19233769178390503
Batch 50/64 loss: 0.18939101696014404
Batch 51/64 loss: 0.2085561752319336
Batch 52/64 loss: 0.19329673051834106
Batch 53/64 loss: 0.19016265869140625
Batch 54/64 loss: 0.19056928157806396
Batch 55/64 loss: 0.1837618350982666
Batch 56/64 loss: 0.20174914598464966
Batch 57/64 loss: 0.19126677513122559
Batch 58/64 loss: 0.17874741554260254
Batch 59/64 loss: 0.19846385717391968
Batch 60/64 loss: 0.19006872177124023
Batch 61/64 loss: 0.17800474166870117
Batch 62/64 loss: 0.19848418235778809
Batch 63/64 loss: 0.1786482334136963
Batch 64/64 loss: 0.18452119827270508
Epoch 6  Train loss: 0.19764849531884288  Val loss: 0.19600500437811888
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 0.1818385124206543
Batch 2/64 loss: 0.18274205923080444
Batch 3/64 loss: 0.21124708652496338
Batch 4/64 loss: 0.18925106525421143
Batch 5/64 loss: 0.18064409494400024
Batch 6/64 loss: 0.191139817237854
Batch 7/64 loss: 0.19679611921310425
Batch 8/64 loss: 0.19249391555786133
Batch 9/64 loss: 0.19494009017944336
Batch 10/64 loss: 0.20226818323135376
Batch 11/64 loss: 0.1705331802368164
Batch 12/64 loss: 0.19762641191482544
Batch 13/64 loss: 0.1878417730331421
Batch 14/64 loss: 0.17876005172729492
Batch 15/64 loss: 0.19225943088531494
Batch 16/64 loss: 0.17455434799194336
Batch 17/64 loss: 0.1839749813079834
Batch 18/64 loss: 0.18094033002853394
Batch 19/64 loss: 0.17331349849700928
Batch 20/64 loss: 0.1917870044708252
Batch 21/64 loss: 0.18009668588638306
Batch 22/64 loss: 0.17943990230560303
Batch 23/64 loss: 0.17040544748306274
Batch 24/64 loss: 0.17293739318847656
Batch 25/64 loss: 0.19794821739196777
Batch 26/64 loss: 0.18059206008911133
Batch 27/64 loss: 0.1802734136581421
Batch 28/64 loss: 0.19474327564239502
Batch 29/64 loss: 0.1746751070022583
Batch 30/64 loss: 0.19033867120742798
Batch 31/64 loss: 0.17633533477783203
Batch 32/64 loss: 0.19751620292663574
Batch 33/64 loss: 0.18703234195709229
Batch 34/64 loss: 0.188032865524292
Batch 35/64 loss: 0.1870361566543579
Batch 36/64 loss: 0.1824359893798828
Batch 37/64 loss: 0.17721188068389893
Batch 38/64 loss: 0.18610239028930664
Batch 39/64 loss: 0.21147143840789795
Batch 40/64 loss: 0.21173954010009766
Batch 41/64 loss: 0.182259202003479
Batch 42/64 loss: 0.1955561637878418
Batch 43/64 loss: 0.1880667805671692
Batch 44/64 loss: 0.1868898868560791
Batch 45/64 loss: 0.1817225217819214
Batch 46/64 loss: 0.19787979125976562
Batch 47/64 loss: 0.1969512701034546
Batch 48/64 loss: 0.17004573345184326
Batch 49/64 loss: 0.1834496259689331
Batch 50/64 loss: 0.18864035606384277
Batch 51/64 loss: 0.17385762929916382
Batch 52/64 loss: 0.19265234470367432
Batch 53/64 loss: 0.17174673080444336
Batch 54/64 loss: 0.17467129230499268
Batch 55/64 loss: 0.18477404117584229
Batch 56/64 loss: 0.18253731727600098
Batch 57/64 loss: 0.18555814027786255
Batch 58/64 loss: 0.18869131803512573
Batch 59/64 loss: 0.2021823525428772
Batch 60/64 loss: 0.1898508071899414
Batch 61/64 loss: 0.17485451698303223
Batch 62/64 loss: 0.1802668571472168
Batch 63/64 loss: 0.18915629386901855
Batch 64/64 loss: 0.17477929592132568
Epoch 7  Train loss: 0.18626920335433061  Val loss: 0.1915079605128757
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 0.1831943392753601
Batch 2/64 loss: 0.17794877290725708
Batch 3/64 loss: 0.17952251434326172
Batch 4/64 loss: 0.1901426911354065
Batch 5/64 loss: 0.17314636707305908
Batch 6/64 loss: 0.1777297854423523
Batch 7/64 loss: 0.19254004955291748
Batch 8/64 loss: 0.18554973602294922
Batch 9/64 loss: 0.17643702030181885
Batch 10/64 loss: 0.17944324016571045
Batch 11/64 loss: 0.16900473833084106
Batch 12/64 loss: 0.18666398525238037
Batch 13/64 loss: 0.1707698106765747
Batch 14/64 loss: 0.17625993490219116
Batch 15/64 loss: 0.18438959121704102
Batch 16/64 loss: 0.16619491577148438
Batch 17/64 loss: 0.19476759433746338
Batch 18/64 loss: 0.1650272011756897
Batch 19/64 loss: 0.1704387068748474
Batch 20/64 loss: 0.16232168674468994
Batch 21/64 loss: 0.14342355728149414
Batch 22/64 loss: 0.16417932510375977
Batch 23/64 loss: 0.15628880262374878
Batch 24/64 loss: 0.15492403507232666
Batch 25/64 loss: 0.16108465194702148
Batch 26/64 loss: 0.17263662815093994
Batch 27/64 loss: 0.17336446046829224
Batch 28/64 loss: 0.18622475862503052
Batch 29/64 loss: 0.17893129587173462
Batch 30/64 loss: 0.17413628101348877
Batch 31/64 loss: 0.1767808198928833
Batch 32/64 loss: 0.16268301010131836
Batch 33/64 loss: 0.17690658569335938
Batch 34/64 loss: 0.17705440521240234
Batch 35/64 loss: 0.16966384649276733
Batch 36/64 loss: 0.17311155796051025
Batch 37/64 loss: 0.18495869636535645
Batch 38/64 loss: 0.18606460094451904
Batch 39/64 loss: 0.15394890308380127
Batch 40/64 loss: 0.16511589288711548
Batch 41/64 loss: 0.16889172792434692
Batch 42/64 loss: 0.1899806261062622
Batch 43/64 loss: 0.18218374252319336
Batch 44/64 loss: 0.17547333240509033
Batch 45/64 loss: 0.1802767515182495
Batch 46/64 loss: 0.16471612453460693
Batch 47/64 loss: 0.17695832252502441
Batch 48/64 loss: 0.17875725030899048
Batch 49/64 loss: 0.18015789985656738
Batch 50/64 loss: 0.1979631781578064
Batch 51/64 loss: 0.17294055223464966
Batch 52/64 loss: 0.17749547958374023
Batch 53/64 loss: 0.20100879669189453
Batch 54/64 loss: 0.1738153100013733
Batch 55/64 loss: 0.18069851398468018
Batch 56/64 loss: 0.1894335150718689
Batch 57/64 loss: 0.19016975164413452
Batch 58/64 loss: 0.16699916124343872
Batch 59/64 loss: 0.18230372667312622
Batch 60/64 loss: 0.17589104175567627
Batch 61/64 loss: 0.17770981788635254
Batch 62/64 loss: 0.1662611961364746
Batch 63/64 loss: 0.16574668884277344
Batch 64/64 loss: 0.16252493858337402
Epoch 8  Train loss: 0.175540407966165  Val loss: 0.18070780206791723
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 0.18009662628173828
Batch 2/64 loss: 0.16172730922698975
Batch 3/64 loss: 0.16409730911254883
Batch 4/64 loss: 0.1647546887397766
Batch 5/64 loss: 0.17084741592407227
Batch 6/64 loss: 0.1654653549194336
Batch 7/64 loss: 0.18377673625946045
Batch 8/64 loss: 0.16447091102600098
Batch 9/64 loss: 0.15419995784759521
Batch 10/64 loss: 0.15871834754943848
Batch 11/64 loss: 0.18458116054534912
Batch 12/64 loss: 0.16894149780273438
Batch 13/64 loss: 0.16020071506500244
Batch 14/64 loss: 0.16324716806411743
Batch 15/64 loss: 0.1640625
Batch 16/64 loss: 0.16845232248306274
Batch 17/64 loss: 0.15580230951309204
Batch 18/64 loss: 0.16284465789794922
Batch 19/64 loss: 0.15291595458984375
Batch 20/64 loss: 0.15160584449768066
Batch 21/64 loss: 0.1740555763244629
Batch 22/64 loss: 0.1468040943145752
Batch 23/64 loss: 0.17261815071105957
Batch 24/64 loss: 0.14880406856536865
Batch 25/64 loss: 0.16513991355895996
Batch 26/64 loss: 0.15163075923919678
Batch 27/64 loss: 0.17255771160125732
Batch 28/64 loss: 0.1489708423614502
Batch 29/64 loss: 0.16820693016052246
Batch 30/64 loss: 0.16401028633117676
Batch 31/64 loss: 0.15460264682769775
Batch 32/64 loss: 0.16810351610183716
Batch 33/64 loss: 0.15545737743377686
Batch 34/64 loss: 0.14315426349639893
Batch 35/64 loss: 0.15519475936889648
Batch 36/64 loss: 0.16981035470962524
Batch 37/64 loss: 0.18225932121276855
Batch 38/64 loss: 0.18527144193649292
Batch 39/64 loss: 0.17106389999389648
Batch 40/64 loss: 0.1590888500213623
Batch 41/64 loss: 0.15180009603500366
Batch 42/64 loss: 0.1658533215522766
Batch 43/64 loss: 0.1605057716369629
Batch 44/64 loss: 0.17789632081985474
Batch 45/64 loss: 0.1560877561569214
Batch 46/64 loss: 0.15366733074188232
Batch 47/64 loss: 0.1879720687866211
Batch 48/64 loss: 0.15225231647491455
Batch 49/64 loss: 0.1633862853050232
Batch 50/64 loss: 0.14977169036865234
Batch 51/64 loss: 0.16151773929595947
Batch 52/64 loss: 0.15982341766357422
Batch 53/64 loss: 0.15456771850585938
Batch 54/64 loss: 0.16329479217529297
Batch 55/64 loss: 0.16044294834136963
Batch 56/64 loss: 0.15222346782684326
Batch 57/64 loss: 0.16685199737548828
Batch 58/64 loss: 0.15339607000350952
Batch 59/64 loss: 0.17733216285705566
Batch 60/64 loss: 0.1610255241394043
Batch 61/64 loss: 0.15898406505584717
Batch 62/64 loss: 0.17391502857208252
Batch 63/64 loss: 0.1532726287841797
Batch 64/64 loss: 0.17475545406341553
Epoch 9  Train loss: 0.1632081672257068  Val loss: 0.1678291399454333
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 0.14653939008712769
Batch 2/64 loss: 0.15976601839065552
Batch 3/64 loss: 0.16055691242218018
Batch 4/64 loss: 0.14205223321914673
Batch 5/64 loss: 0.16286373138427734
Batch 6/64 loss: 0.1503375768661499
Batch 7/64 loss: 0.14501738548278809
Batch 8/64 loss: 0.15108758211135864
Batch 9/64 loss: 0.16030287742614746
Batch 10/64 loss: 0.15036261081695557
Batch 11/64 loss: 0.15066921710968018
Batch 12/64 loss: 0.13799095153808594
Batch 13/64 loss: 0.14965516328811646
Batch 14/64 loss: 0.17126774787902832
Batch 15/64 loss: 0.15706825256347656
Batch 16/64 loss: 0.14337432384490967
Batch 17/64 loss: 0.14265942573547363
Batch 18/64 loss: 0.1485588550567627
Batch 19/64 loss: 0.14214098453521729
Batch 20/64 loss: 0.1609894037246704
Batch 21/64 loss: 0.15514463186264038
Batch 22/64 loss: 0.16282469034194946
Batch 23/64 loss: 0.15344971418380737
Batch 24/64 loss: 0.1624659299850464
Batch 25/64 loss: 0.15564346313476562
Batch 26/64 loss: 0.1532517671585083
Batch 27/64 loss: 0.14563089609146118
Batch 28/64 loss: 0.15034514665603638
Batch 29/64 loss: 0.15757399797439575
Batch 30/64 loss: 0.17284047603607178
Batch 31/64 loss: 0.15227192640304565
Batch 32/64 loss: 0.1471942663192749
Batch 33/64 loss: 0.14867812395095825
Batch 34/64 loss: 0.14429503679275513
Batch 35/64 loss: 0.15153050422668457
Batch 36/64 loss: 0.16042959690093994
Batch 37/64 loss: 0.1584606170654297
Batch 38/64 loss: 0.1480085849761963
Batch 39/64 loss: 0.15742480754852295
Batch 40/64 loss: 0.13844555616378784
Batch 41/64 loss: 0.14746922254562378
Batch 42/64 loss: 0.17825967073440552
Batch 43/64 loss: 0.13994508981704712
Batch 44/64 loss: 0.16615450382232666
Batch 45/64 loss: 0.13125216960906982
Batch 46/64 loss: 0.14600104093551636
Batch 47/64 loss: 0.15331512689590454
Batch 48/64 loss: 0.16131895780563354
Batch 49/64 loss: 0.1578909158706665
Batch 50/64 loss: 0.12743163108825684
Batch 51/64 loss: 0.1399167776107788
Batch 52/64 loss: 0.15267091989517212
Batch 53/64 loss: 0.1626386046409607
Batch 54/64 loss: 0.13539916276931763
Batch 55/64 loss: 0.16201388835906982
Batch 56/64 loss: 0.13914769887924194
Batch 57/64 loss: 0.1315975785255432
Batch 58/64 loss: 0.1540621519088745
Batch 59/64 loss: 0.133417010307312
Batch 60/64 loss: 0.13903892040252686
Batch 61/64 loss: 0.13196486234664917
Batch 62/64 loss: 0.1489393711090088
Batch 63/64 loss: 0.1587914228439331
Batch 64/64 loss: 0.1211199164390564
Epoch 10  Train loss: 0.15056701197343714  Val loss: 0.15355041489978016
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: 0.1658252477645874
Batch 2/64 loss: 0.14647483825683594
Batch 3/64 loss: 0.12164092063903809
Batch 4/64 loss: 0.12410151958465576
Batch 5/64 loss: 0.14949959516525269
Batch 6/64 loss: 0.1309884786605835
Batch 7/64 loss: 0.15083926916122437
Batch 8/64 loss: 0.13788217306137085
Batch 9/64 loss: 0.13536220788955688
Batch 10/64 loss: 0.11556816101074219
Batch 11/64 loss: 0.15558350086212158
Batch 12/64 loss: 0.116122305393219
Batch 13/64 loss: 0.12665051221847534
Batch 14/64 loss: 0.13236993551254272
Batch 15/64 loss: 0.13929373025894165
Batch 16/64 loss: 0.14100098609924316
Batch 17/64 loss: 0.15079015493392944
Batch 18/64 loss: 0.12503761053085327
Batch 19/64 loss: 0.1607096791267395
Batch 20/64 loss: 0.13663268089294434
Batch 21/64 loss: 0.14969658851623535
Batch 22/64 loss: 0.1391863226890564
Batch 23/64 loss: 0.1220054030418396
Batch 24/64 loss: 0.1508699655532837
Batch 25/64 loss: 0.12854552268981934
Batch 26/64 loss: 0.1316901445388794
Batch 27/64 loss: 0.12717187404632568
Batch 28/64 loss: 0.15958237648010254
Batch 29/64 loss: 0.15220654010772705
Batch 30/64 loss: 0.15187674760818481
Batch 31/64 loss: 0.14650988578796387
Batch 32/64 loss: 0.14359557628631592
Batch 33/64 loss: 0.14383423328399658
Batch 34/64 loss: 0.15223228931427002
Batch 35/64 loss: 0.1388651728630066
Batch 36/64 loss: 0.11587780714035034
Batch 37/64 loss: 0.1356884241104126
Batch 38/64 loss: 0.13898944854736328
Batch 39/64 loss: 0.13506191968917847
Batch 40/64 loss: 0.14026224613189697
Batch 41/64 loss: 0.15469986200332642
Batch 42/64 loss: 0.1499517560005188
Batch 43/64 loss: 0.14810967445373535
Batch 44/64 loss: 0.15508437156677246
Batch 45/64 loss: 0.14273762702941895
Batch 46/64 loss: 0.1416798233985901
Batch 47/64 loss: 0.13569849729537964
Batch 48/64 loss: 0.1378638744354248
Batch 49/64 loss: 0.11508578062057495
Batch 50/64 loss: 0.14471739530563354
Batch 51/64 loss: 0.13642632961273193
Batch 52/64 loss: 0.13050800561904907
Batch 53/64 loss: 0.12804925441741943
Batch 54/64 loss: 0.13957595825195312
Batch 55/64 loss: 0.13444095849990845
Batch 56/64 loss: 0.12317031621932983
Batch 57/64 loss: 0.140785813331604
Batch 58/64 loss: 0.1458137035369873
Batch 59/64 loss: 0.13280487060546875
Batch 60/64 loss: 0.13775670528411865
Batch 61/64 loss: 0.12197530269622803
Batch 62/64 loss: 0.13283056020736694
Batch 63/64 loss: 0.14264047145843506
Batch 64/64 loss: 0.12796640396118164
Epoch 11  Train loss: 0.13858045035717534  Val loss: 0.14816752557492338
Saving best model, epoch: 11
Epoch 12
-------------------------------
Batch 1/64 loss: 0.13186651468276978
Batch 2/64 loss: 0.11480951309204102
Batch 3/64 loss: 0.13854485750198364
Batch 4/64 loss: 0.12174320220947266
Batch 5/64 loss: 0.141995370388031
Batch 6/64 loss: 0.12089800834655762
Batch 7/64 loss: 0.12995225191116333
Batch 8/64 loss: 0.14703524112701416
Batch 9/64 loss: 0.12546026706695557
Batch 10/64 loss: 0.11679935455322266
Batch 11/64 loss: 0.12676215171813965
Batch 12/64 loss: 0.11293667554855347
Batch 13/64 loss: 0.1153329610824585
Batch 14/64 loss: 0.12060558795928955
Batch 15/64 loss: 0.13905131816864014
Batch 16/64 loss: 0.13927370309829712
Batch 17/64 loss: 0.13229715824127197
Batch 18/64 loss: 0.11349457502365112
Batch 19/64 loss: 0.1251446008682251
Batch 20/64 loss: 0.13311070203781128
Batch 21/64 loss: 0.1313430666923523
Batch 22/64 loss: 0.1404770016670227
Batch 23/64 loss: 0.11940699815750122
Batch 24/64 loss: 0.13974446058273315
Batch 25/64 loss: 0.12633812427520752
Batch 26/64 loss: 0.11367231607437134
Batch 27/64 loss: 0.12394523620605469
Batch 28/64 loss: 0.13549822568893433
Batch 29/64 loss: 0.13151061534881592
Batch 30/64 loss: 0.12828516960144043
Batch 31/64 loss: 0.09674203395843506
Batch 32/64 loss: 0.1251523494720459
Batch 33/64 loss: 0.12719041109085083
Batch 34/64 loss: 0.13524001836776733
Batch 35/64 loss: 0.12951087951660156
Batch 36/64 loss: 0.1276535987854004
Batch 37/64 loss: 0.12198758125305176
Batch 38/64 loss: 0.11504840850830078
Batch 39/64 loss: 0.1142038106918335
Batch 40/64 loss: 0.1215088963508606
Batch 41/64 loss: 0.11563777923583984
Batch 42/64 loss: 0.11636972427368164
Batch 43/64 loss: 0.12025618553161621
Batch 44/64 loss: 0.12659907341003418
Batch 45/64 loss: 0.13271385431289673
Batch 46/64 loss: 0.11557775735855103
Batch 47/64 loss: 0.11836951971054077
Batch 48/64 loss: 0.11353743076324463
Batch 49/64 loss: 0.11707437038421631
Batch 50/64 loss: 0.10665661096572876
Batch 51/64 loss: 0.12839972972869873
Batch 52/64 loss: 0.1252090334892273
Batch 53/64 loss: 0.13905620574951172
Batch 54/64 loss: 0.1355574131011963
Batch 55/64 loss: 0.1290806531906128
Batch 56/64 loss: 0.13935625553131104
Batch 57/64 loss: 0.12282717227935791
Batch 58/64 loss: 0.1255037784576416
Batch 59/64 loss: 0.11458724737167358
Batch 60/64 loss: 0.12428104877471924
Batch 61/64 loss: 0.1234394907951355
Batch 62/64 loss: 0.14292418956756592
Batch 63/64 loss: 0.10943418741226196
Batch 64/64 loss: 0.11881864070892334
Epoch 12  Train loss: 0.12522566131517  Val loss: 0.13487381214128738
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 0.12299752235412598
Batch 2/64 loss: 0.1075514554977417
Batch 3/64 loss: 0.10326939821243286
Batch 4/64 loss: 0.09901392459869385
Batch 5/64 loss: 0.1309605836868286
Batch 6/64 loss: 0.12168276309967041
Batch 7/64 loss: 0.1155327558517456
Batch 8/64 loss: 0.12641417980194092
Batch 9/64 loss: 0.11528903245925903
Batch 10/64 loss: 0.110015869140625
Batch 11/64 loss: 0.12765836715698242
Batch 12/64 loss: 0.1057051420211792
Batch 13/64 loss: 0.0981948971748352
Batch 14/64 loss: 0.11011683940887451
Batch 15/64 loss: 0.11176931858062744
Batch 16/64 loss: 0.13351774215698242
Batch 17/64 loss: 0.12362754344940186
Batch 18/64 loss: 0.13690274953842163
Batch 19/64 loss: 0.12060856819152832
Batch 20/64 loss: 0.09566402435302734
Batch 21/64 loss: 0.11773282289505005
Batch 22/64 loss: 0.11892896890640259
Batch 23/64 loss: 0.12667256593704224
Batch 24/64 loss: 0.12051677703857422
Batch 25/64 loss: 0.12163716554641724
Batch 26/64 loss: 0.1269192099571228
Batch 27/64 loss: 0.11586731672286987
Batch 28/64 loss: 0.11157363653182983
Batch 29/64 loss: 0.11148834228515625
Batch 30/64 loss: 0.10676974058151245
Batch 31/64 loss: 0.1036558747291565
Batch 32/64 loss: 0.12761175632476807
Batch 33/64 loss: 0.10441404581069946
Batch 34/64 loss: 0.11102348566055298
Batch 35/64 loss: 0.11037600040435791
Batch 36/64 loss: 0.10125446319580078
Batch 37/64 loss: 0.12683337926864624
Batch 38/64 loss: 0.10012996196746826
Batch 39/64 loss: 0.12806487083435059
Batch 40/64 loss: 0.11262059211730957
Batch 41/64 loss: 0.10452806949615479
Batch 42/64 loss: 0.12493085861206055
Batch 43/64 loss: 0.11563354730606079
Batch 44/64 loss: 0.09243226051330566
Batch 45/64 loss: 0.11583364009857178
Batch 46/64 loss: 0.10426610708236694
Batch 47/64 loss: 0.08780503273010254
Batch 48/64 loss: 0.13359606266021729
Batch 49/64 loss: 0.10900288820266724
Batch 50/64 loss: 0.12655788660049438
Batch 51/64 loss: 0.08723646402359009
Batch 52/64 loss: 0.10079866647720337
Batch 53/64 loss: 0.09167951345443726
Batch 54/64 loss: 0.10131555795669556
Batch 55/64 loss: 0.10200989246368408
Batch 56/64 loss: 0.1227407455444336
Batch 57/64 loss: 0.11337101459503174
Batch 58/64 loss: 0.11287093162536621
Batch 59/64 loss: 0.1067342758178711
Batch 60/64 loss: 0.09433138370513916
Batch 61/64 loss: 0.10549020767211914
Batch 62/64 loss: 0.10309696197509766
Batch 63/64 loss: 0.09918880462646484
Batch 64/64 loss: 0.081473708152771
Epoch 13  Train loss: 0.11195513267143099  Val loss: 0.11960163313088958
Saving best model, epoch: 13
Epoch 14
-------------------------------
Batch 1/64 loss: 0.10563981533050537
Batch 2/64 loss: 0.10571473836898804
Batch 3/64 loss: 0.09611135721206665
Batch 4/64 loss: 0.11738908290863037
Batch 5/64 loss: 0.10782390832901001
Batch 6/64 loss: 0.07999777793884277
Batch 7/64 loss: 0.11157995462417603
Batch 8/64 loss: 0.10436809062957764
Batch 9/64 loss: 0.1030920147895813
Batch 10/64 loss: 0.11188572645187378
Batch 11/64 loss: 0.1066516637802124
Batch 12/64 loss: 0.07941269874572754
Batch 13/64 loss: 0.10932689905166626
Batch 14/64 loss: 0.09912168979644775
Batch 15/64 loss: 0.08868777751922607
Batch 16/64 loss: 0.09261107444763184
Batch 17/64 loss: 0.08675515651702881
Batch 18/64 loss: 0.09562951326370239
Batch 19/64 loss: 0.09830176830291748
Batch 20/64 loss: 0.09050184488296509
Batch 21/64 loss: 0.11768084764480591
Batch 22/64 loss: 0.09797453880310059
Batch 23/64 loss: 0.08551549911499023
Batch 24/64 loss: 0.14077311754226685
Batch 25/64 loss: 0.11939918994903564
Batch 26/64 loss: 0.11482453346252441
Batch 27/64 loss: 0.09307640790939331
Batch 28/64 loss: 0.09372919797897339
Batch 29/64 loss: 0.09896248579025269
Batch 30/64 loss: 0.08812057971954346
Batch 31/64 loss: 0.11831378936767578
Batch 32/64 loss: 0.10644948482513428
Batch 33/64 loss: 0.1010594367980957
Batch 34/64 loss: 0.1172974705696106
Batch 35/64 loss: 0.09430694580078125
Batch 36/64 loss: 0.10735142230987549
Batch 37/64 loss: 0.08709156513214111
Batch 38/64 loss: 0.08948826789855957
Batch 39/64 loss: 0.1069951057434082
Batch 40/64 loss: 0.09614139795303345
Batch 41/64 loss: 0.09675717353820801
Batch 42/64 loss: 0.08186161518096924
Batch 43/64 loss: 0.1143454909324646
Batch 44/64 loss: 0.07031583786010742
Batch 45/64 loss: 0.09472143650054932
Batch 46/64 loss: 0.08542567491531372
Batch 47/64 loss: 0.10474735498428345
Batch 48/64 loss: 0.11396753787994385
Batch 49/64 loss: 0.10313206911087036
Batch 50/64 loss: 0.13589859008789062
Batch 51/64 loss: 0.10686439275741577
Batch 52/64 loss: 0.09359937906265259
Batch 53/64 loss: 0.07154953479766846
Batch 54/64 loss: 0.11448025703430176
Batch 55/64 loss: 0.10831677913665771
Batch 56/64 loss: 0.0942581295967102
Batch 57/64 loss: 0.08637499809265137
Batch 58/64 loss: 0.1012425422668457
Batch 59/64 loss: 0.07603812217712402
Batch 60/64 loss: 0.11333566904067993
Batch 61/64 loss: 0.11157739162445068
Batch 62/64 loss: 0.10648488998413086
Batch 63/64 loss: 0.08144986629486084
Batch 64/64 loss: 0.07911992073059082
Epoch 14  Train loss: 0.10025472173503801  Val loss: 0.11008102377665412
Saving best model, epoch: 14
Epoch 15
-------------------------------
Batch 1/64 loss: 0.08170515298843384
Batch 2/64 loss: 0.10352253913879395
Batch 3/64 loss: 0.08203661441802979
Batch 4/64 loss: 0.0982406735420227
Batch 5/64 loss: 0.08996987342834473
Batch 6/64 loss: 0.07363033294677734
Batch 7/64 loss: 0.09313350915908813
Batch 8/64 loss: 0.09317505359649658
Batch 9/64 loss: 0.10722529888153076
Batch 10/64 loss: 0.08411163091659546
Batch 11/64 loss: 0.09578347206115723
Batch 12/64 loss: 0.09116435050964355
Batch 13/64 loss: 0.10187917947769165
Batch 14/64 loss: 0.13715589046478271
Batch 15/64 loss: 0.09318870306015015
Batch 16/64 loss: 0.08291506767272949
Batch 17/64 loss: 0.08254033327102661
Batch 18/64 loss: 0.0868794322013855
Batch 19/64 loss: 0.10694289207458496
Batch 20/64 loss: 0.10994940996170044
Batch 21/64 loss: 0.08973819017410278
Batch 22/64 loss: 0.07692170143127441
Batch 23/64 loss: 0.07335561513900757
Batch 24/64 loss: 0.10698318481445312
Batch 25/64 loss: 0.09104835987091064
Batch 26/64 loss: 0.09614062309265137
Batch 27/64 loss: 0.10250622034072876
Batch 28/64 loss: 0.08214861154556274
Batch 29/64 loss: 0.07666325569152832
Batch 30/64 loss: 0.09966206550598145
Batch 31/64 loss: 0.09788632392883301
Batch 32/64 loss: 0.09743738174438477
Batch 33/64 loss: 0.09452223777770996
Batch 34/64 loss: 0.09596860408782959
Batch 35/64 loss: 0.09077560901641846
Batch 36/64 loss: 0.08287447690963745
Batch 37/64 loss: 0.06826233863830566
Batch 38/64 loss: 0.09076666831970215
Batch 39/64 loss: 0.10827159881591797
Batch 40/64 loss: 0.11699289083480835
Batch 41/64 loss: 0.10255396366119385
Batch 42/64 loss: 0.08639711141586304
Batch 43/64 loss: 0.0862228274345398
Batch 44/64 loss: 0.08906710147857666
Batch 45/64 loss: 0.10020172595977783
Batch 46/64 loss: 0.06860816478729248
Batch 47/64 loss: 0.0965164303779602
Batch 48/64 loss: 0.09569364786148071
Batch 49/64 loss: 0.07030761241912842
Batch 50/64 loss: 0.0782400369644165
Batch 51/64 loss: 0.05667603015899658
Batch 52/64 loss: 0.07813751697540283
Batch 53/64 loss: 0.06861412525177002
Batch 54/64 loss: 0.06686735153198242
Batch 55/64 loss: 0.09840679168701172
Batch 56/64 loss: 0.07329714298248291
Batch 57/64 loss: 0.09889698028564453
Batch 58/64 loss: 0.07680743932723999
Batch 59/64 loss: 0.07566344738006592
Batch 60/64 loss: 0.0850875973701477
Batch 61/64 loss: 0.09050166606903076
Batch 62/64 loss: 0.0908362865447998
Batch 63/64 loss: 0.07898449897766113
Batch 64/64 loss: 0.08366364240646362
Epoch 15  Train loss: 0.08955935052796907  Val loss: 0.10744158570299443
Saving best model, epoch: 15
Epoch 16
-------------------------------
Batch 1/64 loss: 0.07442909479141235
Batch 2/64 loss: 0.07045590877532959
Batch 3/64 loss: 0.06320297718048096
Batch 4/64 loss: 0.07862961292266846
Batch 5/64 loss: 0.06641310453414917
Batch 6/64 loss: 0.06156200170516968
Batch 7/64 loss: 0.0821031928062439
Batch 8/64 loss: 0.09993904829025269
Batch 9/64 loss: 0.09568566083908081
Batch 10/64 loss: 0.09772157669067383
Batch 11/64 loss: 0.07053446769714355
Batch 12/64 loss: 0.07051551342010498
Batch 13/64 loss: 0.09930533170700073
Batch 14/64 loss: 0.07583010196685791
Batch 15/64 loss: 0.0850566029548645
Batch 16/64 loss: 0.09636008739471436
Batch 17/64 loss: 0.058856070041656494
Batch 18/64 loss: 0.10627001523971558
Batch 19/64 loss: 0.09099525213241577
Batch 20/64 loss: 0.06332951784133911
Batch 21/64 loss: 0.07087916135787964
Batch 22/64 loss: 0.10738933086395264
Batch 23/64 loss: 0.08577799797058105
Batch 24/64 loss: 0.12283259630203247
Batch 25/64 loss: 0.08993351459503174
Batch 26/64 loss: 0.0898403525352478
Batch 27/64 loss: 0.08030378818511963
Batch 28/64 loss: 0.05314469337463379
Batch 29/64 loss: 0.06771683692932129
Batch 30/64 loss: 0.08753883838653564
Batch 31/64 loss: 0.08143311738967896
Batch 32/64 loss: 0.07785904407501221
Batch 33/64 loss: 0.08388179540634155
Batch 34/64 loss: 0.08498698472976685
Batch 35/64 loss: 0.06570327281951904
Batch 36/64 loss: 0.09333992004394531
Batch 37/64 loss: 0.07226622104644775
Batch 38/64 loss: 0.08051115274429321
Batch 39/64 loss: 0.09077036380767822
Batch 40/64 loss: 0.07852327823638916
Batch 41/64 loss: 0.08784735202789307
Batch 42/64 loss: 0.08806121349334717
Batch 43/64 loss: 0.09213638305664062
Batch 44/64 loss: 0.10810989141464233
Batch 45/64 loss: 0.09632694721221924
Batch 46/64 loss: 0.07965826988220215
Batch 47/64 loss: 0.07930022478103638
Batch 48/64 loss: 0.0905579924583435
Batch 49/64 loss: 0.09308189153671265
Batch 50/64 loss: 0.07005435228347778
Batch 51/64 loss: 0.09819406270980835
Batch 52/64 loss: 0.06571370363235474
Batch 53/64 loss: 0.06456077098846436
Batch 54/64 loss: 0.06093883514404297
Batch 55/64 loss: 0.07532739639282227
Batch 56/64 loss: 0.05473273992538452
Batch 57/64 loss: 0.07086104154586792
Batch 58/64 loss: 0.059198856353759766
Batch 59/64 loss: 0.06626540422439575
Batch 60/64 loss: 0.05847370624542236
Batch 61/64 loss: 0.07860982418060303
Batch 62/64 loss: 0.08008629083633423
Batch 63/64 loss: 0.11275672912597656
Batch 64/64 loss: 0.06993252038955688
Epoch 16  Train loss: 0.08086479481528787  Val loss: 0.09894571332997064
Saving best model, epoch: 16
Epoch 17
-------------------------------
Batch 1/64 loss: 0.07523387670516968
Batch 2/64 loss: 0.07172513008117676
Batch 3/64 loss: 0.082896888256073
Batch 4/64 loss: 0.07982182502746582
Batch 5/64 loss: 0.08194386959075928
Batch 6/64 loss: 0.05600243806838989
Batch 7/64 loss: 0.06589657068252563
Batch 8/64 loss: 0.09144234657287598
Batch 9/64 loss: 0.07241702079772949
Batch 10/64 loss: 0.09299218654632568
Batch 11/64 loss: 0.07530134916305542
Batch 12/64 loss: 0.08941137790679932
Batch 13/64 loss: 0.08699578046798706
Batch 14/64 loss: 0.07962089776992798
Batch 15/64 loss: 0.07577621936798096
Batch 16/64 loss: 0.07894128561019897
Batch 17/64 loss: 0.053303420543670654
Batch 18/64 loss: 0.09754937887191772
Batch 19/64 loss: 0.06032538414001465
Batch 20/64 loss: 0.06767290830612183
Batch 21/64 loss: 0.07699555158615112
Batch 22/64 loss: 0.07923328876495361
Batch 23/64 loss: 0.08259838819503784
Batch 24/64 loss: 0.08285129070281982
Batch 25/64 loss: 0.07640641927719116
Batch 26/64 loss: 0.05918765068054199
Batch 27/64 loss: 0.09373927116394043
Batch 28/64 loss: 0.08731949329376221
Batch 29/64 loss: 0.07392048835754395
Batch 30/64 loss: 0.07524657249450684
Batch 31/64 loss: 0.05913114547729492
Batch 32/64 loss: 0.05952787399291992
Batch 33/64 loss: 0.06360447406768799
Batch 34/64 loss: 0.0675804615020752
Batch 35/64 loss: 0.06898665428161621
Batch 36/64 loss: 0.09452420473098755
Batch 37/64 loss: 0.0663595199584961
Batch 38/64 loss: 0.07808864116668701
Batch 39/64 loss: 0.08020651340484619
Batch 40/64 loss: 0.08217132091522217
Batch 41/64 loss: 0.07416850328445435
Batch 42/64 loss: 0.06221526861190796
Batch 43/64 loss: 0.05973029136657715
Batch 44/64 loss: 0.07994604110717773
Batch 45/64 loss: 0.0936959981918335
Batch 46/64 loss: 0.07774138450622559
Batch 47/64 loss: 0.08207869529724121
Batch 48/64 loss: 0.06547558307647705
Batch 49/64 loss: 0.09074246883392334
Batch 50/64 loss: 0.06059843301773071
Batch 51/64 loss: 0.07070547342300415
Batch 52/64 loss: 0.09770232439041138
Batch 53/64 loss: 0.10858404636383057
Batch 54/64 loss: 0.07769936323165894
Batch 55/64 loss: 0.0797724723815918
Batch 56/64 loss: 0.08411586284637451
Batch 57/64 loss: 0.07865464687347412
Batch 58/64 loss: 0.05720728635787964
Batch 59/64 loss: 0.07239270210266113
Batch 60/64 loss: 0.06454789638519287
Batch 61/64 loss: 0.0681312084197998
Batch 62/64 loss: 0.05882465839385986
Batch 63/64 loss: 0.06107354164123535
Batch 64/64 loss: 0.059661805629730225
Epoch 17  Train loss: 0.07550588051478067  Val loss: 0.0999913522877644
Epoch 18
-------------------------------
Batch 1/64 loss: 0.06231224536895752
Batch 2/64 loss: 0.07435983419418335
Batch 3/64 loss: 0.07141709327697754
Batch 4/64 loss: 0.0775330662727356
Batch 5/64 loss: 0.053359925746917725
Batch 6/64 loss: 0.06977635622024536
Batch 7/64 loss: 0.06269979476928711
Batch 8/64 loss: 0.06064194440841675
Batch 9/64 loss: 0.07980448007583618
Batch 10/64 loss: 0.07114279270172119
Batch 11/64 loss: 0.08229714632034302
Batch 12/64 loss: 0.06859081983566284
Batch 13/64 loss: 0.07172590494155884
Batch 14/64 loss: 0.07122230529785156
Batch 15/64 loss: 0.0642620325088501
Batch 16/64 loss: 0.06177806854248047
Batch 17/64 loss: 0.08127301931381226
Batch 18/64 loss: 0.08254426717758179
Batch 19/64 loss: 0.04641491174697876
Batch 20/64 loss: 0.05916625261306763
Batch 21/64 loss: 0.06799423694610596
Batch 22/64 loss: 0.060293495655059814
Batch 23/64 loss: 0.0915064811706543
Batch 24/64 loss: 0.05858898162841797
Batch 25/64 loss: 0.07001668214797974
Batch 26/64 loss: 0.08364588022232056
Batch 27/64 loss: 0.05560493469238281
Batch 28/64 loss: 0.05939483642578125
Batch 29/64 loss: 0.0836254358291626
Batch 30/64 loss: 0.05018210411071777
Batch 31/64 loss: 0.06937259435653687
Batch 32/64 loss: 0.05374699831008911
Batch 33/64 loss: 0.07037109136581421
Batch 34/64 loss: 0.06156355142593384
Batch 35/64 loss: 0.07262426614761353
Batch 36/64 loss: 0.06892269849777222
Batch 37/64 loss: 0.04506492614746094
Batch 38/64 loss: 0.08535349369049072
Batch 39/64 loss: 0.05032867193222046
Batch 40/64 loss: 0.05131852626800537
Batch 41/64 loss: 0.05400717258453369
Batch 42/64 loss: 0.0779120922088623
Batch 43/64 loss: 0.05322468280792236
Batch 44/64 loss: 0.08140069246292114
Batch 45/64 loss: 0.05178183317184448
Batch 46/64 loss: 0.0668482780456543
Batch 47/64 loss: 0.06676095724105835
Batch 48/64 loss: 0.05357635021209717
Batch 49/64 loss: 0.05831557512283325
Batch 50/64 loss: 0.09808874130249023
Batch 51/64 loss: 0.03612774610519409
Batch 52/64 loss: 0.06908106803894043
Batch 53/64 loss: 0.04621171951293945
Batch 54/64 loss: 0.05933988094329834
Batch 55/64 loss: 0.054108500480651855
Batch 56/64 loss: 0.06147456169128418
Batch 57/64 loss: 0.09508717060089111
Batch 58/64 loss: 0.0653223991394043
Batch 59/64 loss: 0.09675610065460205
Batch 60/64 loss: 0.06695717573165894
Batch 61/64 loss: 0.06852138042449951
Batch 62/64 loss: 0.07058125734329224
Batch 63/64 loss: 0.051374197006225586
Batch 64/64 loss: 0.05663865804672241
Epoch 18  Train loss: 0.06630871646544513  Val loss: 0.08329366695430271
Saving best model, epoch: 18
Epoch 19
-------------------------------
Batch 1/64 loss: 0.052029430866241455
Batch 2/64 loss: 0.04953587055206299
Batch 3/64 loss: 0.05992305278778076
Batch 4/64 loss: 0.06288951635360718
Batch 5/64 loss: 0.08735674619674683
Batch 6/64 loss: 0.05258399248123169
Batch 7/64 loss: 0.043812572956085205
Batch 8/64 loss: 0.08307772874832153
Batch 9/64 loss: 0.04151713848114014
Batch 10/64 loss: 0.06456166505813599
Batch 11/64 loss: 0.022487759590148926
Batch 12/64 loss: 0.0662999153137207
Batch 13/64 loss: 0.044208824634552
Batch 14/64 loss: 0.06543838977813721
Batch 15/64 loss: 0.07329094409942627
Batch 16/64 loss: 0.05505990982055664
Batch 17/64 loss: 0.033514976501464844
Batch 18/64 loss: 0.04271984100341797
Batch 19/64 loss: 0.06300479173660278
Batch 20/64 loss: 0.06401044130325317
Batch 21/64 loss: 0.06244009733200073
Batch 22/64 loss: 0.057427287101745605
Batch 23/64 loss: 0.050221145153045654
Batch 24/64 loss: 0.05412161350250244
Batch 25/64 loss: 0.05879342555999756
Batch 26/64 loss: 0.05557286739349365
Batch 27/64 loss: 0.062057316303253174
Batch 28/64 loss: 0.05277061462402344
Batch 29/64 loss: 0.0629546046257019
Batch 30/64 loss: 0.07703685760498047
Batch 31/64 loss: 0.0611875057220459
Batch 32/64 loss: 0.049424171447753906
Batch 33/64 loss: 0.058782875537872314
Batch 34/64 loss: 0.054160237312316895
Batch 35/64 loss: 0.06376832723617554
Batch 36/64 loss: 0.05378305912017822
Batch 37/64 loss: 0.05806159973144531
Batch 38/64 loss: 0.05581331253051758
Batch 39/64 loss: 0.06770586967468262
Batch 40/64 loss: 0.0744936466217041
Batch 41/64 loss: 0.04848933219909668
Batch 42/64 loss: 0.05926293134689331
Batch 43/64 loss: 0.05860745906829834
Batch 44/64 loss: 0.060798048973083496
Batch 45/64 loss: 0.063679039478302
Batch 46/64 loss: 0.06602931022644043
Batch 47/64 loss: 0.08509981632232666
Batch 48/64 loss: 0.06163930892944336
Batch 49/64 loss: 0.052197396755218506
Batch 50/64 loss: 0.06377822160720825
Batch 51/64 loss: 0.05602210760116577
Batch 52/64 loss: 0.04958575963973999
Batch 53/64 loss: 0.04270094633102417
Batch 54/64 loss: 0.08692705631256104
Batch 55/64 loss: 0.06891262531280518
Batch 56/64 loss: 0.07541853189468384
Batch 57/64 loss: 0.07629948854446411
Batch 58/64 loss: 0.09832322597503662
Batch 59/64 loss: 0.06588143110275269
Batch 60/64 loss: 0.03613859415054321
Batch 61/64 loss: 0.056645870208740234
Batch 62/64 loss: 0.03816157579421997
Batch 63/64 loss: 0.0657150149345398
Batch 64/64 loss: 0.05955272912979126
Epoch 19  Train loss: 0.05959023657967063  Val loss: 0.08157114556564908
Saving best model, epoch: 19
Epoch 20
-------------------------------
Batch 1/64 loss: 0.050799667835235596
Batch 2/64 loss: 0.049349844455718994
Batch 3/64 loss: 0.036390483379364014
Batch 4/64 loss: 0.039768218994140625
Batch 5/64 loss: 0.05380803346633911
Batch 6/64 loss: 0.0682985782623291
Batch 7/64 loss: 0.07886719703674316
Batch 8/64 loss: 0.051120638847351074
Batch 9/64 loss: 0.060075461864471436
Batch 10/64 loss: 0.03991413116455078
Batch 11/64 loss: 0.07410037517547607
Batch 12/64 loss: 0.07932794094085693
Batch 13/64 loss: 0.04949963092803955
Batch 14/64 loss: 0.07341736555099487
Batch 15/64 loss: 0.05102437734603882
Batch 16/64 loss: 0.04861617088317871
Batch 17/64 loss: 0.06567525863647461
Batch 18/64 loss: 0.05489981174468994
Batch 19/64 loss: 0.04773527383804321
Batch 20/64 loss: 0.05315959453582764
Batch 21/64 loss: 0.05023515224456787
Batch 22/64 loss: 0.04232436418533325
Batch 23/64 loss: 0.04176360368728638
Batch 24/64 loss: 0.056239962577819824
Batch 25/64 loss: 0.07350987195968628
Batch 26/64 loss: 0.0523829460144043
Batch 27/64 loss: 0.05762362480163574
Batch 28/64 loss: 0.035317182540893555
Batch 29/64 loss: 0.05335795879364014
Batch 30/64 loss: 0.034034669399261475
Batch 31/64 loss: 0.0765882134437561
Batch 32/64 loss: 0.0773167610168457
Batch 33/64 loss: 0.06376421451568604
Batch 34/64 loss: 0.07192367315292358
Batch 35/64 loss: 0.06171315908432007
Batch 36/64 loss: 0.07102978229522705
Batch 37/64 loss: 0.04501926898956299
Batch 38/64 loss: 0.02473658323287964
Batch 39/64 loss: 0.03123396635055542
Batch 40/64 loss: 0.037543296813964844
Batch 41/64 loss: 0.07638990879058838
Batch 42/64 loss: 0.05939370393753052
Batch 43/64 loss: 0.04599183797836304
Batch 44/64 loss: 0.06367772817611694
Batch 45/64 loss: 0.05526554584503174
Batch 46/64 loss: 0.04898267984390259
Batch 47/64 loss: 0.05483376979827881
Batch 48/64 loss: 0.0522388219833374
Batch 49/64 loss: 0.06649738550186157
Batch 50/64 loss: 0.05694359540939331
Batch 51/64 loss: 0.023065567016601562
Batch 52/64 loss: 0.06913101673126221
Batch 53/64 loss: 0.05117630958557129
Batch 54/64 loss: 0.03417479991912842
Batch 55/64 loss: 0.0440523624420166
Batch 56/64 loss: 0.05205821990966797
Batch 57/64 loss: 0.059918761253356934
Batch 58/64 loss: 0.08532845973968506
Batch 59/64 loss: 0.052675724029541016
Batch 60/64 loss: 0.05443042516708374
Batch 61/64 loss: 0.06525766849517822
Batch 62/64 loss: 0.030821502208709717
Batch 63/64 loss: 0.06937915086746216
Batch 64/64 loss: 0.037944674491882324
Epoch 20  Train loss: 0.05464548643897561  Val loss: 0.07706601103556525
Saving best model, epoch: 20
Epoch 21
-------------------------------
Batch 1/64 loss: 0.05092042684555054
Batch 2/64 loss: 0.05391049385070801
Batch 3/64 loss: 0.04564028978347778
Batch 4/64 loss: 0.049048542976379395
Batch 5/64 loss: 0.029437720775604248
Batch 6/64 loss: 0.04204201698303223
Batch 7/64 loss: 0.05082947015762329
Batch 8/64 loss: 0.04636061191558838
Batch 9/64 loss: 0.051202237606048584
Batch 10/64 loss: 0.05090665817260742
Batch 11/64 loss: 0.0488775372505188
Batch 12/64 loss: 0.059584856033325195
Batch 13/64 loss: 0.036785006523132324
Batch 14/64 loss: 0.034573912620544434
Batch 15/64 loss: 0.063235342502594
Batch 16/64 loss: 0.0683336853981018
Batch 17/64 loss: 0.03827250003814697
Batch 18/64 loss: 0.06579530239105225
Batch 19/64 loss: 0.03727954626083374
Batch 20/64 loss: 0.02056676149368286
Batch 21/64 loss: 0.01968008279800415
Batch 22/64 loss: 0.034616708755493164
Batch 23/64 loss: 0.055887460708618164
Batch 24/64 loss: 0.06318855285644531
Batch 25/64 loss: 0.03617274761199951
Batch 26/64 loss: 0.026711225509643555
Batch 27/64 loss: 0.03750342130661011
Batch 28/64 loss: 0.07027363777160645
Batch 29/64 loss: 0.04525834321975708
Batch 30/64 loss: 0.031222939491271973
Batch 31/64 loss: 0.05902421474456787
Batch 32/64 loss: 0.05170845985412598
Batch 33/64 loss: 0.07616519927978516
Batch 34/64 loss: 0.07152009010314941
Batch 35/64 loss: 0.048702239990234375
Batch 36/64 loss: 0.03939884901046753
Batch 37/64 loss: 0.036018431186676025
Batch 38/64 loss: 0.05364024639129639
Batch 39/64 loss: 0.028089046478271484
Batch 40/64 loss: 0.06803423166275024
Batch 41/64 loss: 0.06188249588012695
Batch 42/64 loss: 0.017832517623901367
Batch 43/64 loss: 0.031205058097839355
Batch 44/64 loss: 0.03508138656616211
Batch 45/64 loss: 0.06763589382171631
Batch 46/64 loss: 0.05200904607772827
Batch 47/64 loss: 0.04997134208679199
Batch 48/64 loss: 0.03645271062850952
Batch 49/64 loss: 0.05580258369445801
Batch 50/64 loss: 0.06150633096694946
Batch 51/64 loss: 0.05343437194824219
Batch 52/64 loss: 0.03770875930786133
Batch 53/64 loss: 0.06389296054840088
Batch 54/64 loss: 0.04975533485412598
Batch 55/64 loss: 0.04936814308166504
Batch 56/64 loss: 0.05202353000640869
Batch 57/64 loss: 0.07223641872406006
Batch 58/64 loss: 0.07604825496673584
Batch 59/64 loss: 0.0516352653503418
Batch 60/64 loss: 0.046057701110839844
Batch 61/64 loss: 0.0589519739151001
Batch 62/64 loss: 0.030837297439575195
Batch 63/64 loss: 0.04381793737411499
Batch 64/64 loss: 0.04244142770767212
Epoch 21  Train loss: 0.048367018559399774  Val loss: 0.07577262875140738
Saving best model, epoch: 21
Epoch 22
-------------------------------
Batch 1/64 loss: 0.04022681713104248
Batch 2/64 loss: 0.0476229190826416
Batch 3/64 loss: 0.03855639696121216
Batch 4/64 loss: 0.03327745199203491
Batch 5/64 loss: 0.03362452983856201
Batch 6/64 loss: 0.027590692043304443
Batch 7/64 loss: 0.043940722942352295
Batch 8/64 loss: 0.04994213581085205
Batch 9/64 loss: 0.02234327793121338
Batch 10/64 loss: 0.06958651542663574
Batch 11/64 loss: 0.05579805374145508
Batch 12/64 loss: 0.05161237716674805
Batch 13/64 loss: 0.054357171058654785
Batch 14/64 loss: 0.03564232587814331
Batch 15/64 loss: 0.04568469524383545
Batch 16/64 loss: 0.03313499689102173
Batch 17/64 loss: 0.044403791427612305
Batch 18/64 loss: 0.0385286808013916
Batch 19/64 loss: 0.03263401985168457
Batch 20/64 loss: 0.047464966773986816
Batch 21/64 loss: 0.025614023208618164
Batch 22/64 loss: 0.0630149245262146
Batch 23/64 loss: 0.07016664743423462
Batch 24/64 loss: 0.024488389492034912
Batch 25/64 loss: 0.04713183641433716
Batch 26/64 loss: 0.04573458433151245
Batch 27/64 loss: 0.058743953704833984
Batch 28/64 loss: 0.03177797794342041
Batch 29/64 loss: 0.03534668684005737
Batch 30/64 loss: 0.03965401649475098
Batch 31/64 loss: 0.04596322774887085
Batch 32/64 loss: 0.014969825744628906
Batch 33/64 loss: 0.049161314964294434
Batch 34/64 loss: 0.05241096019744873
Batch 35/64 loss: 0.057731807231903076
Batch 36/64 loss: 0.04665642976760864
Batch 37/64 loss: 0.04683077335357666
Batch 38/64 loss: 0.038661181926727295
Batch 39/64 loss: 0.06422305107116699
Batch 40/64 loss: 0.06213736534118652
Batch 41/64 loss: 0.04758113622665405
Batch 42/64 loss: 0.05218517780303955
Batch 43/64 loss: 0.03771805763244629
Batch 44/64 loss: 0.031105399131774902
Batch 45/64 loss: 0.04243457317352295
Batch 46/64 loss: 0.03806900978088379
Batch 47/64 loss: 0.059777021408081055
Batch 48/64 loss: 0.044464111328125
Batch 49/64 loss: 0.04316288232803345
Batch 50/64 loss: 0.051400184631347656
Batch 51/64 loss: 0.04003798961639404
Batch 52/64 loss: 0.056380629539489746
Batch 53/64 loss: 0.04735279083251953
Batch 54/64 loss: 0.04806172847747803
Batch 55/64 loss: 0.04822123050689697
Batch 56/64 loss: 0.04194819927215576
Batch 57/64 loss: 0.032267868518829346
Batch 58/64 loss: 0.04899394512176514
Batch 59/64 loss: 0.040654122829437256
Batch 60/64 loss: 0.04017174243927002
Batch 61/64 loss: 0.03841090202331543
Batch 62/64 loss: 0.06497317552566528
Batch 63/64 loss: 0.03612571954727173
Batch 64/64 loss: 0.010968685150146484
Epoch 22  Train loss: 0.04401750003590303  Val loss: 0.07248338104523334
Saving best model, epoch: 22
Epoch 23
-------------------------------
Batch 1/64 loss: 0.05210393667221069
Batch 2/64 loss: 0.038560450077056885
Batch 3/64 loss: 0.05786335468292236
Batch 4/64 loss: 0.05517125129699707
Batch 5/64 loss: 0.04337024688720703
Batch 6/64 loss: 0.043544888496398926
Batch 7/64 loss: 0.03440672159194946
Batch 8/64 loss: 0.014422476291656494
Batch 9/64 loss: 0.03915905952453613
Batch 10/64 loss: 0.0315701961517334
Batch 11/64 loss: 0.042129337787628174
Batch 12/64 loss: 0.03609192371368408
Batch 13/64 loss: 0.05112868547439575
Batch 14/64 loss: 0.04378783702850342
Batch 15/64 loss: 0.03852766752243042
Batch 16/64 loss: 0.03145623207092285
Batch 17/64 loss: 0.06351852416992188
Batch 18/64 loss: 0.03676271438598633
Batch 19/64 loss: 0.036589860916137695
Batch 20/64 loss: 0.028712928295135498
Batch 21/64 loss: 0.052809834480285645
Batch 22/64 loss: 0.030403971672058105
Batch 23/64 loss: 0.05411064624786377
Batch 24/64 loss: 0.0543590784072876
Batch 25/64 loss: 0.03787517547607422
Batch 26/64 loss: 0.02174389362335205
Batch 27/64 loss: 0.014380335807800293
Batch 28/64 loss: 0.04201388359069824
Batch 29/64 loss: 0.08233225345611572
Batch 30/64 loss: 0.052677154541015625
Batch 31/64 loss: 0.048184752464294434
Batch 32/64 loss: 0.028354108333587646
Batch 33/64 loss: 0.03338509798049927
Batch 34/64 loss: 0.06553512811660767
Batch 35/64 loss: 0.049589455127716064
Batch 36/64 loss: 0.03811752796173096
Batch 37/64 loss: 0.01426088809967041
Batch 38/64 loss: 0.04769676923751831
Batch 39/64 loss: 0.04821288585662842
Batch 40/64 loss: 0.054949939250946045
Batch 41/64 loss: 0.05269348621368408
Batch 42/64 loss: 0.032527804374694824
Batch 43/64 loss: 0.03015577793121338
Batch 44/64 loss: 0.04009413719177246
Batch 45/64 loss: 0.03335607051849365
Batch 46/64 loss: 0.06065845489501953
Batch 47/64 loss: 0.01509922742843628
Batch 48/64 loss: 0.05285722017288208
Batch 49/64 loss: 0.037335336208343506
Batch 50/64 loss: 0.016211390495300293
Batch 51/64 loss: 0.04437988996505737
Batch 52/64 loss: 0.024037957191467285
Batch 53/64 loss: 0.04594242572784424
Batch 54/64 loss: 0.05212736129760742
Batch 55/64 loss: 0.034164488315582275
Batch 56/64 loss: 0.024156689643859863
Batch 57/64 loss: 0.03248858451843262
Batch 58/64 loss: 0.042092323303222656
Batch 59/64 loss: 0.043204665184020996
Batch 60/64 loss: 0.049799561500549316
Batch 61/64 loss: 0.03237509727478027
Batch 62/64 loss: 0.052091360092163086
Batch 63/64 loss: 0.02992570400238037
Batch 64/64 loss: 0.03104424476623535
Epoch 23  Train loss: 0.04064202776142195  Val loss: 0.06664163058566064
Saving best model, epoch: 23
Epoch 24
-------------------------------
Batch 1/64 loss: 0.016643643379211426
Batch 2/64 loss: 0.027959048748016357
Batch 3/64 loss: 0.023536860942840576
Batch 4/64 loss: 0.043551862239837646
Batch 5/64 loss: 0.060509443283081055
Batch 6/64 loss: 0.0633002519607544
Batch 7/64 loss: 0.06678009033203125
Batch 8/64 loss: 0.03479808568954468
Batch 9/64 loss: 0.009530067443847656
Batch 10/64 loss: 0.031824588775634766
Batch 11/64 loss: 0.027368664741516113
Batch 12/64 loss: 0.0476917028427124
Batch 13/64 loss: 0.048890113830566406
Batch 14/64 loss: 0.0461544394493103
Batch 15/64 loss: 0.032948315143585205
Batch 16/64 loss: 0.05550515651702881
Batch 17/64 loss: 0.045434415340423584
Batch 18/64 loss: 0.045882999897003174
Batch 19/64 loss: 0.025025546550750732
Batch 20/64 loss: 0.02057063579559326
Batch 21/64 loss: 0.02540123462677002
Batch 22/64 loss: 0.03944277763366699
Batch 23/64 loss: 0.031074941158294678
Batch 24/64 loss: 0.03177297115325928
Batch 25/64 loss: 0.02636510133743286
Batch 26/64 loss: 0.019102811813354492
Batch 27/64 loss: 0.02520275115966797
Batch 28/64 loss: 0.03555655479431152
Batch 29/64 loss: 0.03983807563781738
Batch 30/64 loss: 0.02760869264602661
Batch 31/64 loss: 0.014639735221862793
Batch 32/64 loss: 0.05105876922607422
Batch 33/64 loss: 0.022048652172088623
Batch 34/64 loss: 0.042028069496154785
Batch 35/64 loss: 0.04424780607223511
Batch 36/64 loss: 0.04614698886871338
Batch 37/64 loss: 0.05017554759979248
Batch 38/64 loss: 0.026804566383361816
Batch 39/64 loss: 0.023484647274017334
Batch 40/64 loss: 0.06209081411361694
Batch 41/64 loss: 0.05501657724380493
Batch 42/64 loss: 0.038240790367126465
Batch 43/64 loss: 0.03151184320449829
Batch 44/64 loss: 0.03526031970977783
Batch 45/64 loss: 0.04870319366455078
Batch 46/64 loss: 0.06409817934036255
Batch 47/64 loss: 0.04694169759750366
Batch 48/64 loss: 0.04242020845413208
Batch 49/64 loss: 0.04794865846633911
Batch 50/64 loss: 0.039739251136779785
Batch 51/64 loss: 0.031577348709106445
Batch 52/64 loss: 0.03630632162094116
Batch 53/64 loss: 0.03462088108062744
Batch 54/64 loss: 0.059239864349365234
Batch 55/64 loss: 0.028287172317504883
Batch 56/64 loss: 0.028465867042541504
Batch 57/64 loss: 0.061623454093933105
Batch 58/64 loss: 0.026413440704345703
Batch 59/64 loss: 0.03074932098388672
Batch 60/64 loss: 0.03231841325759888
Batch 61/64 loss: 0.02712273597717285
Batch 62/64 loss: 0.007233977317810059
Batch 63/64 loss: 0.027080059051513672
Batch 64/64 loss: 0.03839069604873657
Epoch 24  Train loss: 0.03714054963167976  Val loss: 0.07777624462068695
Epoch 25
-------------------------------
Batch 1/64 loss: 0.0511394739151001
Batch 2/64 loss: 0.06094920635223389
Batch 3/64 loss: 0.04262632131576538
Batch 4/64 loss: 0.024614572525024414
Batch 5/64 loss: 0.02759385108947754
Batch 6/64 loss: 0.04761624336242676
Batch 7/64 loss: 0.06932157278060913
Batch 8/64 loss: 0.039182424545288086
Batch 9/64 loss: 0.03267711400985718
Batch 10/64 loss: 0.044375479221343994
Batch 11/64 loss: 0.03705430030822754
Batch 12/64 loss: 0.026760339736938477
Batch 13/64 loss: 0.035109758377075195
Batch 14/64 loss: 0.03660380840301514
Batch 15/64 loss: 0.03497743606567383
Batch 16/64 loss: 0.009732484817504883
Batch 17/64 loss: 0.0053449273109436035
Batch 18/64 loss: 0.025542616844177246
Batch 19/64 loss: 0.025554001331329346
Batch 20/64 loss: 0.04311203956604004
Batch 21/64 loss: 0.027049124240875244
Batch 22/64 loss: 0.0459139347076416
Batch 23/64 loss: 0.025654613971710205
Batch 24/64 loss: 0.037951767444610596
Batch 25/64 loss: 0.031700193881988525
Batch 26/64 loss: 0.0479201078414917
Batch 27/64 loss: 0.039268553256988525
Batch 28/64 loss: 0.03250020742416382
Batch 29/64 loss: 0.04777979850769043
Batch 30/64 loss: 0.028142213821411133
Batch 31/64 loss: 0.04200083017349243
Batch 32/64 loss: 0.02914339303970337
Batch 33/64 loss: 0.03447848558425903
Batch 34/64 loss: 0.04010343551635742
Batch 35/64 loss: 0.0392378568649292
Batch 36/64 loss: 0.033728837966918945
Batch 37/64 loss: 0.03366345167160034
Batch 38/64 loss: 0.026794373989105225
Batch 39/64 loss: 0.03434056043624878
Batch 40/64 loss: 0.049376606941223145
Batch 41/64 loss: 0.03821688890457153
Batch 42/64 loss: 0.03422737121582031
Batch 43/64 loss: 0.040437400341033936
Batch 44/64 loss: 0.037856101989746094
Batch 45/64 loss: 0.034416794776916504
Batch 46/64 loss: 0.033297181129455566
Batch 47/64 loss: 0.0069342851638793945
Batch 48/64 loss: 0.032295823097229004
Batch 49/64 loss: 0.0442655086517334
Batch 50/64 loss: 0.03436779975891113
Batch 51/64 loss: 0.021014153957366943
Batch 52/64 loss: 0.044557154178619385
Batch 53/64 loss: 0.04697728157043457
Batch 54/64 loss: 0.023937225341796875
Batch 55/64 loss: 0.021548867225646973
Batch 56/64 loss: 0.02103179693222046
Batch 57/64 loss: 0.014382123947143555
Batch 58/64 loss: 0.06441754102706909
Batch 59/64 loss: 0.03823995590209961
Batch 60/64 loss: 0.03980302810668945
Batch 61/64 loss: 0.014253616333007812
Batch 62/64 loss: 0.018744587898254395
Batch 63/64 loss: 0.017159640789031982
Batch 64/64 loss: 0.009228646755218506
Epoch 25  Train loss: 0.034132391097498875  Val loss: 0.07047068376311731
Epoch 26
-------------------------------
Batch 1/64 loss: 0.037316322326660156
Batch 2/64 loss: 0.04180252552032471
Batch 3/64 loss: 0.0146598219871521
Batch 4/64 loss: 0.03665769100189209
Batch 5/64 loss: 0.011567831039428711
Batch 6/64 loss: 0.04422402381896973
Batch 7/64 loss: 0.035147905349731445
Batch 8/64 loss: 0.020337402820587158
Batch 9/64 loss: 0.03088235855102539
Batch 10/64 loss: 0.03341937065124512
Batch 11/64 loss: 0.03275483846664429
Batch 12/64 loss: 0.05948561429977417
Batch 13/64 loss: 0.03469717502593994
Batch 14/64 loss: 0.022123217582702637
Batch 15/64 loss: 0.02244269847869873
Batch 16/64 loss: 0.053725242614746094
Batch 17/64 loss: 0.03864830732345581
Batch 18/64 loss: 0.024514436721801758
Batch 19/64 loss: 0.009952008724212646
Batch 20/64 loss: 0.023244500160217285
Batch 21/64 loss: 0.02444911003112793
Batch 22/64 loss: 0.022137939929962158
Batch 23/64 loss: 0.02679365873336792
Batch 24/64 loss: 0.01362764835357666
Batch 25/64 loss: 0.002629876136779785
Batch 26/64 loss: 0.012467801570892334
Batch 27/64 loss: 0.04004240036010742
Batch 28/64 loss: 0.030025184154510498
Batch 29/64 loss: 0.01090627908706665
Batch 30/64 loss: 0.00948721170425415
Batch 31/64 loss: 0.018860578536987305
Batch 32/64 loss: 0.023638486862182617
Batch 33/64 loss: 0.02692192792892456
Batch 34/64 loss: 0.03633308410644531
Batch 35/64 loss: 0.039678990840911865
Batch 36/64 loss: 0.030200600624084473
Batch 37/64 loss: 0.037407100200653076
Batch 38/64 loss: 0.03875923156738281
Batch 39/64 loss: 0.015908360481262207
Batch 40/64 loss: 0.04453122615814209
Batch 41/64 loss: 0.04849117994308472
Batch 42/64 loss: 0.036537885665893555
Batch 43/64 loss: 0.019417166709899902
Batch 44/64 loss: 0.012288451194763184
Batch 45/64 loss: 0.019317567348480225
Batch 46/64 loss: 0.03694891929626465
Batch 47/64 loss: 0.02721923589706421
Batch 48/64 loss: 0.02037060260772705
Batch 49/64 loss: 0.006254673004150391
Batch 50/64 loss: 0.027035176753997803
Batch 51/64 loss: 0.037658512592315674
Batch 52/64 loss: 0.031126081943511963
Batch 53/64 loss: 0.04339766502380371
Batch 54/64 loss: 0.015392482280731201
Batch 55/64 loss: 0.009177923202514648
Batch 56/64 loss: 0.0370938777923584
Batch 57/64 loss: 0.035003066062927246
Batch 58/64 loss: 0.025689125061035156
Batch 59/64 loss: 0.031108081340789795
Batch 60/64 loss: 0.03780710697174072
Batch 61/64 loss: 0.028216302394866943
Batch 62/64 loss: 0.0822480320930481
Batch 63/64 loss: 0.02287822961807251
Batch 64/64 loss: 0.020745038986206055
Epoch 26  Train loss: 0.028841507668588676  Val loss: 0.05496515052015429
Saving best model, epoch: 26
Epoch 27
-------------------------------
Batch 1/64 loss: 0.0176200270652771
Batch 2/64 loss: 0.012010455131530762
Batch 3/64 loss: 0.02772831916809082
Batch 4/64 loss: 0.01242053508758545
Batch 5/64 loss: 0.013142049312591553
Batch 6/64 loss: 0.007997334003448486
Batch 7/64 loss: 0.03663593530654907
Batch 8/64 loss: -0.00045877695083618164
Batch 9/64 loss: 0.026480257511138916
Batch 10/64 loss: 0.06933259963989258
Batch 11/64 loss: 0.05064195394515991
Batch 12/64 loss: 0.008427560329437256
Batch 13/64 loss: 0.02461451292037964
Batch 14/64 loss: 0.021964848041534424
Batch 15/64 loss: -0.0052956342697143555
Batch 16/64 loss: 0.03727477788925171
Batch 17/64 loss: 0.02570486068725586
Batch 18/64 loss: 0.035232603549957275
Batch 19/64 loss: 0.013553023338317871
Batch 20/64 loss: 0.016355156898498535
Batch 21/64 loss: 0.02504873275756836
Batch 22/64 loss: 0.014734208583831787
Batch 23/64 loss: 0.015217304229736328
Batch 24/64 loss: 0.01945483684539795
Batch 25/64 loss: 0.04194086790084839
Batch 26/64 loss: 0.03935897350311279
Batch 27/64 loss: 0.027734875679016113
Batch 28/64 loss: 0.014201819896697998
Batch 29/64 loss: 0.0020084381103515625
Batch 30/64 loss: 0.033925533294677734
Batch 31/64 loss: 0.03267723321914673
Batch 32/64 loss: 0.022738337516784668
Batch 33/64 loss: 0.02934485673904419
Batch 34/64 loss: 0.03248763084411621
Batch 35/64 loss: 0.034887492656707764
Batch 36/64 loss: 0.009587645530700684
Batch 37/64 loss: 0.023526251316070557
Batch 38/64 loss: 0.011438250541687012
Batch 39/64 loss: 0.015903890132904053
Batch 40/64 loss: 0.026913940906524658
Batch 41/64 loss: 0.0432623028755188
Batch 42/64 loss: 0.022572636604309082
Batch 43/64 loss: 0.016346752643585205
Batch 44/64 loss: 0.02571117877960205
Batch 45/64 loss: 0.04187983274459839
Batch 46/64 loss: 0.03523212671279907
Batch 47/64 loss: 0.059295713901519775
Batch 48/64 loss: 0.038774847984313965
Batch 49/64 loss: 0.011859297752380371
Batch 50/64 loss: 0.027256786823272705
Batch 51/64 loss: 0.029263675212860107
Batch 52/64 loss: 0.01737123727798462
Batch 53/64 loss: 0.03333866596221924
Batch 54/64 loss: 0.022842764854431152
Batch 55/64 loss: 0.05654001235961914
Batch 56/64 loss: 0.005225539207458496
Batch 57/64 loss: 0.018650829792022705
Batch 58/64 loss: 0.029379725456237793
Batch 59/64 loss: 0.0258672833442688
Batch 60/64 loss: 0.045317888259887695
Batch 61/64 loss: 0.061788201332092285
Batch 62/64 loss: 0.0038247108459472656
Batch 63/64 loss: 0.02322208881378174
Batch 64/64 loss: 0.01759922504425049
Epoch 27  Train loss: 0.025608424111908556  Val loss: 0.06125339220479591
Epoch 28
-------------------------------
Batch 1/64 loss: 0.04756355285644531
Batch 2/64 loss: 0.008543133735656738
Batch 3/64 loss: 0.03040754795074463
Batch 4/64 loss: 0.032927632331848145
Batch 5/64 loss: 0.01931595802307129
Batch 6/64 loss: 0.03548794984817505
Batch 7/64 loss: 0.01744091510772705
Batch 8/64 loss: 0.033503949642181396
Batch 9/64 loss: 0.0024799108505249023
Batch 10/64 loss: 0.032750844955444336
Batch 11/64 loss: 0.04733240604400635
Batch 12/64 loss: 0.01699727773666382
Batch 13/64 loss: 0.035486698150634766
Batch 14/64 loss: 0.017646372318267822
Batch 15/64 loss: 0.026598989963531494
Batch 16/64 loss: 0.009010672569274902
Batch 17/64 loss: 0.022731542587280273
Batch 18/64 loss: 0.029272139072418213
Batch 19/64 loss: 0.044098496437072754
Batch 20/64 loss: 0.0018320083618164062
Batch 21/64 loss: 0.004221141338348389
Batch 22/64 loss: 0.00028383731842041016
Batch 23/64 loss: -0.006087303161621094
Batch 24/64 loss: 0.011309921741485596
Batch 25/64 loss: 0.028662443161010742
Batch 26/64 loss: 0.03417414426803589
Batch 27/64 loss: 0.015239536762237549
Batch 28/64 loss: 0.033566057682037354
Batch 29/64 loss: 0.0007260441780090332
Batch 30/64 loss: 0.0181005597114563
Batch 31/64 loss: 0.01716029644012451
Batch 32/64 loss: 0.019432008266448975
Batch 33/64 loss: 0.036615967750549316
Batch 34/64 loss: 0.03335946798324585
Batch 35/64 loss: 0.004767715930938721
Batch 36/64 loss: 0.029855728149414062
Batch 37/64 loss: 0.02218097448348999
Batch 38/64 loss: 0.01883554458618164
Batch 39/64 loss: 0.011821269989013672
Batch 40/64 loss: 0.021994352340698242
Batch 41/64 loss: 0.014289438724517822
Batch 42/64 loss: 0.02613753080368042
Batch 43/64 loss: 0.02329033613204956
Batch 44/64 loss: 0.02496880292892456
Batch 45/64 loss: 0.013501644134521484
Batch 46/64 loss: 0.01336604356765747
Batch 47/64 loss: 0.01917243003845215
Batch 48/64 loss: 0.020117878913879395
Batch 49/64 loss: 0.018356502056121826
Batch 50/64 loss: 0.007889807224273682
Batch 51/64 loss: 0.02831399440765381
Batch 52/64 loss: 0.013428986072540283
Batch 53/64 loss: 0.014298975467681885
Batch 54/64 loss: 0.037917256355285645
Batch 55/64 loss: 0.013588666915893555
Batch 56/64 loss: 0.016597628593444824
Batch 57/64 loss: 0.0006647109985351562
Batch 58/64 loss: 0.03439474105834961
Batch 59/64 loss: 0.06054878234863281
Batch 60/64 loss: 0.015213251113891602
Batch 61/64 loss: 0.015491247177124023
Batch 62/64 loss: 0.020693063735961914
Batch 63/64 loss: 0.03606688976287842
Batch 64/64 loss: 0.015590488910675049
Epoch 28  Train loss: 0.02145332051258461  Val loss: 0.050169465468101894
Saving best model, epoch: 28
Epoch 29
-------------------------------
Batch 1/64 loss: 0.01539921760559082
Batch 2/64 loss: 0.03187978267669678
Batch 3/64 loss: 0.026324450969696045
Batch 4/64 loss: 0.015535473823547363
Batch 5/64 loss: 0.02162301540374756
Batch 6/64 loss: 0.02533125877380371
Batch 7/64 loss: 0.023351013660430908
Batch 8/64 loss: 0.019337773323059082
Batch 9/64 loss: 0.011183500289916992
Batch 10/64 loss: -0.0013695359230041504
Batch 11/64 loss: 0.004942655563354492
Batch 12/64 loss: 0.03205817937850952
Batch 13/64 loss: 0.020143449306488037
Batch 14/64 loss: 0.023588597774505615
Batch 15/64 loss: 0.03531926870346069
Batch 16/64 loss: 0.018721342086791992
Batch 17/64 loss: 0.0011159181594848633
Batch 18/64 loss: -0.0017080307006835938
Batch 19/64 loss: 0.021854877471923828
Batch 20/64 loss: 0.03641730546951294
Batch 21/64 loss: 0.0002751946449279785
Batch 22/64 loss: 0.006448626518249512
Batch 23/64 loss: 0.025699913501739502
Batch 24/64 loss: 0.018523871898651123
Batch 25/64 loss: 0.017606377601623535
Batch 26/64 loss: 0.02281850576400757
Batch 27/64 loss: 0.001493990421295166
Batch 28/64 loss: 0.017873704433441162
Batch 29/64 loss: 0.002815544605255127
Batch 30/64 loss: 0.04224097728729248
Batch 31/64 loss: 0.012287020683288574
Batch 32/64 loss: 0.01089930534362793
Batch 33/64 loss: 0.02066701650619507
Batch 34/64 loss: 0.05961167812347412
Batch 35/64 loss: 0.03500443696975708
Batch 36/64 loss: -0.0026204586029052734
Batch 37/64 loss: 0.005911111831665039
Batch 38/64 loss: 0.007601141929626465
Batch 39/64 loss: 0.002391338348388672
Batch 40/64 loss: 0.017520010471343994
Batch 41/64 loss: 0.04378920793533325
Batch 42/64 loss: 0.054431378841400146
Batch 43/64 loss: 0.012601792812347412
Batch 44/64 loss: 0.026425182819366455
Batch 45/64 loss: 0.024409949779510498
Batch 46/64 loss: 0.016480445861816406
Batch 47/64 loss: 0.009346723556518555
Batch 48/64 loss: 0.0074707865715026855
Batch 49/64 loss: 0.01900196075439453
Batch 50/64 loss: -0.0008093714714050293
Batch 51/64 loss: 0.013691246509552002
Batch 52/64 loss: 0.002145111560821533
Batch 53/64 loss: 0.011597156524658203
Batch 54/64 loss: 0.016729235649108887
Batch 55/64 loss: 0.04326349496841431
Batch 56/64 loss: 0.007331252098083496
Batch 57/64 loss: 0.03548389673233032
Batch 58/64 loss: 0.016961216926574707
Batch 59/64 loss: 0.0036771297454833984
Batch 60/64 loss: 0.012214422225952148
Batch 61/64 loss: 0.03706169128417969
Batch 62/64 loss: 0.01760876178741455
Batch 63/64 loss: 0.012564897537231445
Batch 64/64 loss: 0.019904792308807373
Epoch 29  Train loss: 0.01826705862494076  Val loss: 0.050098140829617215
Saving best model, epoch: 29
Epoch 30
-------------------------------
Batch 1/64 loss: 0.01054227352142334
Batch 2/64 loss: 0.012079119682312012
Batch 3/64 loss: 0.011128067970275879
Batch 4/64 loss: 0.012086212635040283
Batch 5/64 loss: -0.008240461349487305
Batch 6/64 loss: 0.003481566905975342
Batch 7/64 loss: -0.007209122180938721
Batch 8/64 loss: 0.003603816032409668
Batch 9/64 loss: 0.008680164813995361
Batch 10/64 loss: -0.004096925258636475
Batch 11/64 loss: 0.029689788818359375
Batch 12/64 loss: 0.037175774574279785
Batch 13/64 loss: 0.00203549861907959
Batch 14/64 loss: -7.599592208862305e-05
Batch 15/64 loss: 0.04100596904754639
Batch 16/64 loss: 0.013784050941467285
Batch 17/64 loss: 0.01278144121170044
Batch 18/64 loss: -0.007504761219024658
Batch 19/64 loss: 0.030542194843292236
Batch 20/64 loss: 0.0181158185005188
Batch 21/64 loss: 0.026574373245239258
Batch 22/64 loss: 0.051564693450927734
Batch 23/64 loss: 0.005244851112365723
Batch 24/64 loss: 0.029803752899169922
Batch 25/64 loss: 0.0022188425064086914
Batch 26/64 loss: 0.005927979946136475
Batch 27/64 loss: 0.028584718704223633
Batch 28/64 loss: -0.00380021333694458
Batch 29/64 loss: 0.0022502541542053223
Batch 30/64 loss: 0.006292521953582764
Batch 31/64 loss: 0.025731563568115234
Batch 32/64 loss: -0.0020372867584228516
Batch 33/64 loss: 0.009502768516540527
Batch 34/64 loss: 0.012021124362945557
Batch 35/64 loss: 0.006913483142852783
Batch 36/64 loss: -0.0022696256637573242
Batch 37/64 loss: 0.007453858852386475
Batch 38/64 loss: 0.007237851619720459
Batch 39/64 loss: 0.013049662113189697
Batch 40/64 loss: -0.00721895694732666
Batch 41/64 loss: 0.022346556186676025
Batch 42/64 loss: 0.041008830070495605
Batch 43/64 loss: 0.026031076908111572
Batch 44/64 loss: 0.025541841983795166
Batch 45/64 loss: 0.036198556423187256
Batch 46/64 loss: 0.021655023097991943
Batch 47/64 loss: 0.034137189388275146
Batch 48/64 loss: 0.015048623085021973
Batch 49/64 loss: 0.019043564796447754
Batch 50/64 loss: 0.03537648916244507
Batch 51/64 loss: 0.02593982219696045
Batch 52/64 loss: 0.004412531852722168
Batch 53/64 loss: 0.02336740493774414
Batch 54/64 loss: 0.005511283874511719
Batch 55/64 loss: 0.041603028774261475
Batch 56/64 loss: 0.018059372901916504
Batch 57/64 loss: 0.00015842914581298828
Batch 58/64 loss: 0.04304403066635132
Batch 59/64 loss: 0.026316285133361816
Batch 60/64 loss: -0.00035369396209716797
Batch 61/64 loss: 0.01965177059173584
Batch 62/64 loss: 0.0009608268737792969
Batch 63/64 loss: 0.016299545764923096
Batch 64/64 loss: 0.01370173692703247
Epoch 30  Train loss: 0.015000555328294342  Val loss: 0.04899670926156323
Saving best model, epoch: 30
Epoch 31
-------------------------------
Batch 1/64 loss: 0.01834285259246826
Batch 2/64 loss: 0.024761319160461426
Batch 3/64 loss: 0.0057280659675598145
Batch 4/64 loss: 0.010580658912658691
Batch 5/64 loss: 0.0021271109580993652
Batch 6/64 loss: -0.0056334733963012695
Batch 7/64 loss: 0.009737253189086914
Batch 8/64 loss: 0.02964460849761963
Batch 9/64 loss: 0.03233224153518677
Batch 10/64 loss: 0.004364907741546631
Batch 11/64 loss: 0.01656705141067505
Batch 12/64 loss: 0.022160112857818604
Batch 13/64 loss: 0.01729816198348999
Batch 14/64 loss: 0.039590418338775635
Batch 15/64 loss: -0.0021644234657287598
Batch 16/64 loss: 0.004134774208068848
Batch 17/64 loss: 0.02680230140686035
Batch 18/64 loss: 0.011875689029693604
Batch 19/64 loss: 0.01760077476501465
Batch 20/64 loss: 0.006654679775238037
Batch 21/64 loss: 0.016809165477752686
Batch 22/64 loss: 0.019935250282287598
Batch 23/64 loss: 0.0073389410972595215
Batch 24/64 loss: 0.002132594585418701
Batch 25/64 loss: 0.0236055850982666
Batch 26/64 loss: 0.008168458938598633
Batch 27/64 loss: 0.0105057954788208
Batch 28/64 loss: 0.0017113089561462402
Batch 29/64 loss: 0.012974858283996582
Batch 30/64 loss: 0.03349018096923828
Batch 31/64 loss: 0.00945347547531128
Batch 32/64 loss: 0.02904146909713745
Batch 33/64 loss: 0.02425295114517212
Batch 34/64 loss: -0.006481051445007324
Batch 35/64 loss: 0.0059719085693359375
Batch 36/64 loss: 0.0004215836524963379
Batch 37/64 loss: 0.015364229679107666
Batch 38/64 loss: 0.020840346813201904
Batch 39/64 loss: 0.03249627351760864
Batch 40/64 loss: 0.010802209377288818
Batch 41/64 loss: -7.671117782592773e-05
Batch 42/64 loss: 0.014346063137054443
Batch 43/64 loss: 0.025347113609313965
Batch 44/64 loss: 0.01825559139251709
Batch 45/64 loss: 0.017509520053863525
Batch 46/64 loss: -0.01684868335723877
Batch 47/64 loss: 0.00802081823348999
Batch 48/64 loss: 0.02313023805618286
Batch 49/64 loss: -0.0033473968505859375
Batch 50/64 loss: 0.0276566743850708
Batch 51/64 loss: -0.0013883113861083984
Batch 52/64 loss: 0.02620089054107666
Batch 53/64 loss: -0.019092202186584473
Batch 54/64 loss: 0.004721999168395996
Batch 55/64 loss: 0.0040018558502197266
Batch 56/64 loss: 0.02376610040664673
Batch 57/64 loss: 0.0004929900169372559
Batch 58/64 loss: -0.005121350288391113
Batch 59/64 loss: 0.007311046123504639
Batch 60/64 loss: 0.0002715587615966797
Batch 61/64 loss: 0.015008091926574707
Batch 62/64 loss: 0.032456159591674805
Batch 63/64 loss: 0.0026967525482177734
Batch 64/64 loss: -0.0039307475090026855
Epoch 31  Train loss: 0.012136648916730693  Val loss: 0.04752900366930617
Saving best model, epoch: 31
Epoch 32
-------------------------------
Batch 1/64 loss: 0.02713555097579956
Batch 2/64 loss: 0.027470767498016357
Batch 3/64 loss: 0.020026445388793945
Batch 4/64 loss: 0.0016440153121948242
Batch 5/64 loss: 0.003432154655456543
Batch 6/64 loss: 0.032936275005340576
Batch 7/64 loss: 0.011118948459625244
Batch 8/64 loss: 0.013393759727478027
Batch 9/64 loss: 0.003960847854614258
Batch 10/64 loss: 0.014656543731689453
Batch 11/64 loss: 0.01507413387298584
Batch 12/64 loss: 0.02206861972808838
Batch 13/64 loss: 0.02593928575515747
Batch 14/64 loss: 0.013473808765411377
Batch 15/64 loss: 0.004379749298095703
Batch 16/64 loss: -0.0010338425636291504
Batch 17/64 loss: 0.03750795125961304
Batch 18/64 loss: 0.028855502605438232
Batch 19/64 loss: 0.005600690841674805
Batch 20/64 loss: 0.006386816501617432
Batch 21/64 loss: 0.01040428876876831
Batch 22/64 loss: -0.004228055477142334
Batch 23/64 loss: 0.004314720630645752
Batch 24/64 loss: -0.01881706714630127
Batch 25/64 loss: -0.008997559547424316
Batch 26/64 loss: 0.01551365852355957
Batch 27/64 loss: -0.007435142993927002
Batch 28/64 loss: -0.009760737419128418
Batch 29/64 loss: 0.028285980224609375
Batch 30/64 loss: -0.0021820068359375
Batch 31/64 loss: 0.0025311708450317383
Batch 32/64 loss: 0.015911102294921875
Batch 33/64 loss: -0.00484919548034668
Batch 34/64 loss: 0.019274652004241943
Batch 35/64 loss: -0.007700026035308838
Batch 36/64 loss: 0.03612881898880005
Batch 37/64 loss: 0.01690840721130371
Batch 38/64 loss: 0.004362702369689941
Batch 39/64 loss: 0.018311381340026855
Batch 40/64 loss: 0.004520058631896973
Batch 41/64 loss: 0.033707380294799805
Batch 42/64 loss: 0.017476141452789307
Batch 43/64 loss: -0.006503760814666748
Batch 44/64 loss: 0.027991533279418945
Batch 45/64 loss: 0.010415434837341309
Batch 46/64 loss: 0.0001265406608581543
Batch 47/64 loss: -0.0030350089073181152
Batch 48/64 loss: 0.014760375022888184
Batch 49/64 loss: 0.006606757640838623
Batch 50/64 loss: -0.0026232004165649414
Batch 51/64 loss: 0.011590182781219482
Batch 52/64 loss: 0.011913716793060303
Batch 53/64 loss: 0.011438131332397461
Batch 54/64 loss: 0.016155540943145752
Batch 55/64 loss: 0.012092947959899902
Batch 56/64 loss: -0.009887099266052246
Batch 57/64 loss: -0.00043487548828125
Batch 58/64 loss: -0.007520735263824463
Batch 59/64 loss: 0.021031677722930908
Batch 60/64 loss: 0.007530570030212402
Batch 61/64 loss: 0.02234429121017456
Batch 62/64 loss: -0.011127471923828125
Batch 63/64 loss: -0.015479087829589844
Batch 64/64 loss: 0.034561991691589355
Epoch 32  Train loss: 0.009741437668893852  Val loss: 0.043088093246381305
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: -0.01655101776123047
Batch 2/64 loss: -0.0019283890724182129
Batch 3/64 loss: -0.008695602416992188
Batch 4/64 loss: 0.043999671936035156
Batch 5/64 loss: -0.005530357360839844
Batch 6/64 loss: 0.00426095724105835
Batch 7/64 loss: -0.010955750942230225
Batch 8/64 loss: -0.005040347576141357
Batch 9/64 loss: 0.01345604658126831
Batch 10/64 loss: 0.0012329816818237305
Batch 11/64 loss: 0.011141777038574219
Batch 12/64 loss: 0.00014907121658325195
Batch 13/64 loss: -0.0033481717109680176
Batch 14/64 loss: 0.0165289044380188
Batch 15/64 loss: 0.028057456016540527
Batch 16/64 loss: -0.00016248226165771484
Batch 17/64 loss: 0.0024413466453552246
Batch 18/64 loss: -0.0167464017868042
Batch 19/64 loss: -0.006047546863555908
Batch 20/64 loss: 0.019106626510620117
Batch 21/64 loss: 0.0005612373352050781
Batch 22/64 loss: 0.015502214431762695
Batch 23/64 loss: -0.007092177867889404
Batch 24/64 loss: 0.055092811584472656
Batch 25/64 loss: 0.018253087997436523
Batch 26/64 loss: -0.0007376670837402344
Batch 27/64 loss: 0.017206668853759766
Batch 28/64 loss: 0.0022382736206054688
Batch 29/64 loss: 0.024225234985351562
Batch 30/64 loss: -0.004959702491760254
Batch 31/64 loss: -0.001035928726196289
Batch 32/64 loss: 0.020921289920806885
Batch 33/64 loss: 0.025819778442382812
Batch 34/64 loss: 0.020978212356567383
Batch 35/64 loss: 0.014365613460540771
Batch 36/64 loss: 0.01881849765777588
Batch 37/64 loss: 0.0066986083984375
Batch 38/64 loss: 0.026581168174743652
Batch 39/64 loss: 0.025587141513824463
Batch 40/64 loss: -0.016453564167022705
Batch 41/64 loss: 0.015619993209838867
Batch 42/64 loss: 0.015499413013458252
Batch 43/64 loss: 0.02629852294921875
Batch 44/64 loss: 0.002628803253173828
Batch 45/64 loss: 0.01631462574005127
Batch 46/64 loss: -0.00615614652633667
Batch 47/64 loss: 0.018095433712005615
Batch 48/64 loss: -0.0036846399307250977
Batch 49/64 loss: 0.025296509265899658
Batch 50/64 loss: -0.020093798637390137
Batch 51/64 loss: 0.00736159086227417
Batch 52/64 loss: -6.473064422607422e-05
Batch 53/64 loss: 0.001677393913269043
Batch 54/64 loss: 0.007518112659454346
Batch 55/64 loss: 0.004057884216308594
Batch 56/64 loss: -0.0015254020690917969
Batch 57/64 loss: -0.012409567832946777
Batch 58/64 loss: 0.005143463611602783
Batch 59/64 loss: 0.04150080680847168
Batch 60/64 loss: 0.011632323265075684
Batch 61/64 loss: 0.019413411617279053
Batch 62/64 loss: 0.015306711196899414
Batch 63/64 loss: 0.008466720581054688
Batch 64/64 loss: -0.004554450511932373
Epoch 33  Train loss: 0.00819484182432586  Val loss: 0.047481050196382185
Epoch 34
-------------------------------
Batch 1/64 loss: -0.0024874210357666016
Batch 2/64 loss: 0.020810246467590332
Batch 3/64 loss: -0.00010055303573608398
Batch 4/64 loss: 0.008625030517578125
Batch 5/64 loss: 0.011140108108520508
Batch 6/64 loss: -0.00402224063873291
Batch 7/64 loss: 0.008641481399536133
Batch 8/64 loss: 0.004336059093475342
Batch 9/64 loss: -0.013159334659576416
Batch 10/64 loss: 0.005652725696563721
Batch 11/64 loss: 0.018653810024261475
Batch 12/64 loss: 0.0015919804573059082
Batch 13/64 loss: -0.018628478050231934
Batch 14/64 loss: 0.020700275897979736
Batch 15/64 loss: -0.005850255489349365
Batch 16/64 loss: 0.00816035270690918
Batch 17/64 loss: 0.010369062423706055
Batch 18/64 loss: 0.007212996482849121
Batch 19/64 loss: 0.01325070858001709
Batch 20/64 loss: 0.016548514366149902
Batch 21/64 loss: -0.006417036056518555
Batch 22/64 loss: 0.005357861518859863
Batch 23/64 loss: 0.01623523235321045
Batch 24/64 loss: 0.004387855529785156
Batch 25/64 loss: -0.007346510887145996
Batch 26/64 loss: 0.011335968971252441
Batch 27/64 loss: -0.008068084716796875
Batch 28/64 loss: 0.000945746898651123
Batch 29/64 loss: 0.02049356698989868
Batch 30/64 loss: -0.013347327709197998
Batch 31/64 loss: -0.0148543119430542
Batch 32/64 loss: -0.01044541597366333
Batch 33/64 loss: 0.023810148239135742
Batch 34/64 loss: 0.0013701915740966797
Batch 35/64 loss: -0.0015453100204467773
Batch 36/64 loss: 0.004861712455749512
Batch 37/64 loss: -0.008351504802703857
Batch 38/64 loss: 0.010060727596282959
Batch 39/64 loss: 0.015191972255706787
Batch 40/64 loss: -0.002686142921447754
Batch 41/64 loss: 0.012442946434020996
Batch 42/64 loss: -0.010029077529907227
Batch 43/64 loss: -0.016230344772338867
Batch 44/64 loss: 0.016677379608154297
Batch 45/64 loss: -0.009185314178466797
Batch 46/64 loss: -0.004681825637817383
Batch 47/64 loss: 0.009213685989379883
Batch 48/64 loss: 0.0323675274848938
Batch 49/64 loss: 0.008990824222564697
Batch 50/64 loss: -0.00110703706741333
Batch 51/64 loss: -0.0008563399314880371
Batch 52/64 loss: -0.0019795894622802734
Batch 53/64 loss: 0.008119463920593262
Batch 54/64 loss: 0.0018551945686340332
Batch 55/64 loss: 0.011007189750671387
Batch 56/64 loss: 0.002889275550842285
Batch 57/64 loss: 0.010937750339508057
Batch 58/64 loss: -0.007478654384613037
Batch 59/64 loss: 0.02038508653640747
Batch 60/64 loss: -0.010509908199310303
Batch 61/64 loss: 0.022616147994995117
Batch 62/64 loss: 0.018651127815246582
Batch 63/64 loss: -0.0033801794052124023
Batch 64/64 loss: -0.01908695697784424
Epoch 34  Train loss: 0.0039032865973079906  Val loss: 0.04246191265656776
Saving best model, epoch: 34
Epoch 35
-------------------------------
Batch 1/64 loss: -0.004121243953704834
Batch 2/64 loss: 0.02257394790649414
Batch 3/64 loss: -0.002546072006225586
Batch 4/64 loss: 0.00995415449142456
Batch 5/64 loss: -0.003229498863220215
Batch 6/64 loss: -0.010735392570495605
Batch 7/64 loss: 0.009073913097381592
Batch 8/64 loss: -0.027074694633483887
Batch 9/64 loss: 0.0218508243560791
Batch 10/64 loss: 0.01266014575958252
Batch 11/64 loss: -0.009152770042419434
Batch 12/64 loss: 0.0034834742546081543
Batch 13/64 loss: -0.010686218738555908
Batch 14/64 loss: 0.009145617485046387
Batch 15/64 loss: -0.028269708156585693
Batch 16/64 loss: 0.01207643747329712
Batch 17/64 loss: -0.007014870643615723
Batch 18/64 loss: -0.002578139305114746
Batch 19/64 loss: 0.024415016174316406
Batch 20/64 loss: -0.008781492710113525
Batch 21/64 loss: 0.006064653396606445
Batch 22/64 loss: 0.005765736103057861
Batch 23/64 loss: 0.04052472114562988
Batch 24/64 loss: 0.007724940776824951
Batch 25/64 loss: -0.014695525169372559
Batch 26/64 loss: 0.01433652639389038
Batch 27/64 loss: -0.002663910388946533
Batch 28/64 loss: -0.005188941955566406
Batch 29/64 loss: 0.027330875396728516
Batch 30/64 loss: 0.028448104858398438
Batch 31/64 loss: -0.0114402174949646
Batch 32/64 loss: 0.019145727157592773
Batch 33/64 loss: 0.0016629695892333984
Batch 34/64 loss: -0.009812116622924805
Batch 35/64 loss: 0.008011341094970703
Batch 36/64 loss: 0.0070552825927734375
Batch 37/64 loss: -0.00780797004699707
Batch 38/64 loss: -0.0013744831085205078
Batch 39/64 loss: 0.012149393558502197
Batch 40/64 loss: 0.032797932624816895
Batch 41/64 loss: -0.018958628177642822
Batch 42/64 loss: 0.015902400016784668
Batch 43/64 loss: -0.014214158058166504
Batch 44/64 loss: -0.006919384002685547
Batch 45/64 loss: 0.017345428466796875
Batch 46/64 loss: -0.0036455392837524414
Batch 47/64 loss: 0.004353642463684082
Batch 48/64 loss: -0.002979457378387451
Batch 49/64 loss: 0.0022399425506591797
Batch 50/64 loss: 0.003998398780822754
Batch 51/64 loss: -0.003739595413208008
Batch 52/64 loss: -0.008106827735900879
Batch 53/64 loss: 0.000896751880645752
Batch 54/64 loss: 0.000947415828704834
Batch 55/64 loss: 0.02668827772140503
Batch 56/64 loss: -0.0018008947372436523
Batch 57/64 loss: 0.002161562442779541
Batch 58/64 loss: -0.015339493751525879
Batch 59/64 loss: -0.002965569496154785
Batch 60/64 loss: 0.003693819046020508
Batch 61/64 loss: 0.02335500717163086
Batch 62/64 loss: -0.014319658279418945
Batch 63/64 loss: 0.0006244182586669922
Batch 64/64 loss: -0.019414424896240234
Epoch 35  Train loss: 0.002568400139902152  Val loss: 0.040086782675018834
Saving best model, epoch: 35
Epoch 36
-------------------------------
Batch 1/64 loss: -0.007336676120758057
Batch 2/64 loss: 0.010937035083770752
Batch 3/64 loss: -0.028829634189605713
Batch 4/64 loss: -0.032099127769470215
Batch 5/64 loss: -0.01172637939453125
Batch 6/64 loss: -0.013312101364135742
Batch 7/64 loss: 0.03599804639816284
Batch 8/64 loss: -0.020816326141357422
Batch 9/64 loss: -0.009319484233856201
Batch 10/64 loss: 0.01252967119216919
Batch 11/64 loss: 0.020296037197113037
Batch 12/64 loss: 0.01985311508178711
Batch 13/64 loss: -0.00306546688079834
Batch 14/64 loss: -0.013808190822601318
Batch 15/64 loss: -0.00963127613067627
Batch 16/64 loss: 0.011533677577972412
Batch 17/64 loss: 0.008928894996643066
Batch 18/64 loss: -0.012054741382598877
Batch 19/64 loss: -0.02735757827758789
Batch 20/64 loss: -0.019195199012756348
Batch 21/64 loss: -0.0028756260871887207
Batch 22/64 loss: 0.007249355316162109
Batch 23/64 loss: 0.004047572612762451
Batch 24/64 loss: 0.017506062984466553
Batch 25/64 loss: 0.022194862365722656
Batch 26/64 loss: -0.010737776756286621
Batch 27/64 loss: -0.003712892532348633
Batch 28/64 loss: 0.0038138628005981445
Batch 29/64 loss: 0.03141707181930542
Batch 30/64 loss: 0.0016595125198364258
Batch 31/64 loss: -0.01732015609741211
Batch 32/64 loss: -0.011610746383666992
Batch 33/64 loss: -0.01564037799835205
Batch 34/64 loss: -0.007877111434936523
Batch 35/64 loss: 0.04274296760559082
Batch 36/64 loss: 0.014702856540679932
Batch 37/64 loss: -0.005421996116638184
Batch 38/64 loss: -0.001143336296081543
Batch 39/64 loss: 0.0002002716064453125
Batch 40/64 loss: 0.007425427436828613
Batch 41/64 loss: 0.002150416374206543
Batch 42/64 loss: 0.012391388416290283
Batch 43/64 loss: -0.014700174331665039
Batch 44/64 loss: -0.010950446128845215
Batch 45/64 loss: -0.006293654441833496
Batch 46/64 loss: 0.004564940929412842
Batch 47/64 loss: -0.002657949924468994
Batch 48/64 loss: -0.019802749156951904
Batch 49/64 loss: -0.00851982831954956
Batch 50/64 loss: -0.0035240650177001953
Batch 51/64 loss: -0.003652811050415039
Batch 52/64 loss: -0.008885622024536133
Batch 53/64 loss: 0.0004017949104309082
Batch 54/64 loss: 0.004316389560699463
Batch 55/64 loss: -0.013878822326660156
Batch 56/64 loss: -0.0008956789970397949
Batch 57/64 loss: 0.0004214048385620117
Batch 58/64 loss: 0.007654070854187012
Batch 59/64 loss: 0.00410228967666626
Batch 60/64 loss: 0.009992241859436035
Batch 61/64 loss: 0.0030666589736938477
Batch 62/64 loss: -0.0050980448722839355
Batch 63/64 loss: 0.015992462635040283
Batch 64/64 loss: 0.028442740440368652
Epoch 36  Train loss: -0.00038164129444197113  Val loss: 0.04106359444942671
Epoch 37
-------------------------------
Batch 1/64 loss: -0.01694774627685547
Batch 2/64 loss: -0.0006429553031921387
Batch 3/64 loss: -0.021061480045318604
Batch 4/64 loss: -0.02359175682067871
Batch 5/64 loss: 0.0019180774688720703
Batch 6/64 loss: 0.0006573796272277832
Batch 7/64 loss: -0.01202303171157837
Batch 8/64 loss: 0.004569649696350098
Batch 9/64 loss: -0.018690407276153564
Batch 10/64 loss: 0.00837165117263794
Batch 11/64 loss: -0.02827763557434082
Batch 12/64 loss: -0.0062514543533325195
Batch 13/64 loss: 0.0012983083724975586
Batch 14/64 loss: -0.012536585330963135
Batch 15/64 loss: 0.005333006381988525
Batch 16/64 loss: -0.009117305278778076
Batch 17/64 loss: -0.01478564739227295
Batch 18/64 loss: 0.04268002510070801
Batch 19/64 loss: 0.013685345649719238
Batch 20/64 loss: -0.021687805652618408
Batch 21/64 loss: -0.002163231372833252
Batch 22/64 loss: 0.015058577060699463
Batch 23/64 loss: -0.0018970966339111328
Batch 24/64 loss: 0.012247979640960693
Batch 25/64 loss: -0.02477508783340454
Batch 26/64 loss: 0.004839420318603516
Batch 27/64 loss: -0.00851595401763916
Batch 28/64 loss: -0.014256477355957031
Batch 29/64 loss: -0.0003458857536315918
Batch 30/64 loss: -0.003449082374572754
Batch 31/64 loss: -0.003962278366088867
Batch 32/64 loss: 0.01778435707092285
Batch 33/64 loss: 0.006638467311859131
Batch 34/64 loss: -0.018245935440063477
Batch 35/64 loss: -0.02901679277420044
Batch 36/64 loss: -0.008262097835540771
Batch 37/64 loss: 0.0035140514373779297
Batch 38/64 loss: -0.008478701114654541
Batch 39/64 loss: 0.0019645094871520996
Batch 40/64 loss: 0.009947061538696289
Batch 41/64 loss: -0.00021791458129882812
Batch 42/64 loss: 0.003971099853515625
Batch 43/64 loss: -0.0015344619750976562
Batch 44/64 loss: 0.004319369792938232
Batch 45/64 loss: -0.009951591491699219
Batch 46/64 loss: 0.012590885162353516
Batch 47/64 loss: -0.007840454578399658
Batch 48/64 loss: 0.014384210109710693
Batch 49/64 loss: 0.0035526156425476074
Batch 50/64 loss: 0.004937410354614258
Batch 51/64 loss: 0.000925898551940918
Batch 52/64 loss: -0.018184304237365723
Batch 53/64 loss: -0.012239336967468262
Batch 54/64 loss: 0.0007367134094238281
Batch 55/64 loss: -0.00038236379623413086
Batch 56/64 loss: 0.009320259094238281
Batch 57/64 loss: 0.011590540409088135
Batch 58/64 loss: 0.0002509951591491699
Batch 59/64 loss: -0.015185534954071045
Batch 60/64 loss: 0.0007188320159912109
Batch 61/64 loss: 0.01213693618774414
Batch 62/64 loss: -0.0057776570320129395
Batch 63/64 loss: 0.00398629903793335
Batch 64/64 loss: 0.0115966796875
Epoch 37  Train loss: -0.0021595076018688726  Val loss: 0.038170859166437
Saving best model, epoch: 37
Epoch 38
-------------------------------
Batch 1/64 loss: -0.011927008628845215
Batch 2/64 loss: 0.006408870220184326
Batch 3/64 loss: -0.004976034164428711
Batch 4/64 loss: 0.0062103271484375
Batch 5/64 loss: -0.020933568477630615
Batch 6/64 loss: -0.025105535984039307
Batch 7/64 loss: -0.013799726963043213
Batch 8/64 loss: -0.025351345539093018
Batch 9/64 loss: 0.004357337951660156
Batch 10/64 loss: -0.0035382509231567383
Batch 11/64 loss: -0.0019381046295166016
Batch 12/64 loss: 0.02033555507659912
Batch 13/64 loss: -0.023515164852142334
Batch 14/64 loss: -0.002772212028503418
Batch 15/64 loss: -0.014140307903289795
Batch 16/64 loss: 0.02144622802734375
Batch 17/64 loss: -0.012829899787902832
Batch 18/64 loss: 0.008179187774658203
Batch 19/64 loss: -0.017020702362060547
Batch 20/64 loss: -0.009678304195404053
Batch 21/64 loss: -0.027674436569213867
Batch 22/64 loss: -0.023756980895996094
Batch 23/64 loss: -0.0002956390380859375
Batch 24/64 loss: -0.02761012315750122
Batch 25/64 loss: -0.0007232427597045898
Batch 26/64 loss: 0.010098099708557129
Batch 27/64 loss: 0.015611827373504639
Batch 28/64 loss: 0.02011185884475708
Batch 29/64 loss: 0.00851529836654663
Batch 30/64 loss: -0.014497041702270508
Batch 31/64 loss: -0.014948606491088867
Batch 32/64 loss: -0.019728243350982666
Batch 33/64 loss: -0.01130765676498413
Batch 34/64 loss: 0.01575016975402832
Batch 35/64 loss: -0.036580443382263184
Batch 36/64 loss: -0.011795461177825928
Batch 37/64 loss: 0.009405434131622314
Batch 38/64 loss: 0.0023790597915649414
Batch 39/64 loss: 0.004443049430847168
Batch 40/64 loss: -0.021657586097717285
Batch 41/64 loss: 0.0001308917999267578
Batch 42/64 loss: 0.019532978534698486
Batch 43/64 loss: 0.012974023818969727
Batch 44/64 loss: -0.0007635354995727539
Batch 45/64 loss: 0.004215598106384277
Batch 46/64 loss: -0.00969398021697998
Batch 47/64 loss: -0.009564042091369629
Batch 48/64 loss: 0.0006464123725891113
Batch 49/64 loss: -0.025331497192382812
Batch 50/64 loss: -0.020454108715057373
Batch 51/64 loss: 0.010444164276123047
Batch 52/64 loss: -0.012149691581726074
Batch 53/64 loss: 0.0006465911865234375
Batch 54/64 loss: -0.012826263904571533
Batch 55/64 loss: 0.006023406982421875
Batch 56/64 loss: -0.011531174182891846
Batch 57/64 loss: -0.0033225417137145996
Batch 58/64 loss: -0.016280531883239746
Batch 59/64 loss: 0.010792970657348633
Batch 60/64 loss: 0.0022560954093933105
Batch 61/64 loss: -0.006905972957611084
Batch 62/64 loss: -0.024599730968475342
Batch 63/64 loss: -0.007842779159545898
Batch 64/64 loss: -0.0016061663627624512
Epoch 38  Train loss: -0.005327947700724882  Val loss: 0.0386645501012245
Epoch 39
-------------------------------
Batch 1/64 loss: -0.01584041118621826
Batch 2/64 loss: 0.0067220330238342285
Batch 3/64 loss: -0.02199733257293701
Batch 4/64 loss: -0.00407564640045166
Batch 5/64 loss: -0.017252862453460693
Batch 6/64 loss: -0.03452634811401367
Batch 7/64 loss: -0.007598817348480225
Batch 8/64 loss: -0.021193861961364746
Batch 9/64 loss: 0.002603769302368164
Batch 10/64 loss: -0.019523024559020996
Batch 11/64 loss: -0.003370225429534912
Batch 12/64 loss: 0.0015453696250915527
Batch 13/64 loss: -0.009590208530426025
Batch 14/64 loss: 0.000941932201385498
Batch 15/64 loss: -0.01760774850845337
Batch 16/64 loss: -0.01512753963470459
Batch 17/64 loss: -0.011640369892120361
Batch 18/64 loss: -0.01586604118347168
Batch 19/64 loss: -0.015181779861450195
Batch 20/64 loss: -0.014551997184753418
Batch 21/64 loss: -0.02042365074157715
Batch 22/64 loss: -0.00020486116409301758
Batch 23/64 loss: 0.011074960231781006
Batch 24/64 loss: -0.01840388774871826
Batch 25/64 loss: 0.017983675003051758
Batch 26/64 loss: -0.022359132766723633
Batch 27/64 loss: -0.0045816898345947266
Batch 28/64 loss: -0.017858803272247314
Batch 29/64 loss: -0.0023893117904663086
Batch 30/64 loss: -0.008682966232299805
Batch 31/64 loss: 0.009717166423797607
Batch 32/64 loss: -0.027849256992340088
Batch 33/64 loss: -0.005112409591674805
Batch 34/64 loss: 0.005117475986480713
Batch 35/64 loss: -0.013034403324127197
Batch 36/64 loss: 0.0103834867477417
Batch 37/64 loss: -0.018938004970550537
Batch 38/64 loss: -0.014436066150665283
Batch 39/64 loss: -0.024372577667236328
Batch 40/64 loss: 0.0076583027839660645
Batch 41/64 loss: -0.0017472505569458008
Batch 42/64 loss: 0.016901791095733643
Batch 43/64 loss: -0.019299864768981934
Batch 44/64 loss: 0.016033709049224854
Batch 45/64 loss: -0.011737644672393799
Batch 46/64 loss: 0.011327087879180908
Batch 47/64 loss: 0.0003559589385986328
Batch 48/64 loss: -0.01207435131072998
Batch 49/64 loss: -0.008391499519348145
Batch 50/64 loss: -0.007877349853515625
Batch 51/64 loss: -0.026928067207336426
Batch 52/64 loss: -0.003536045551300049
Batch 53/64 loss: -0.02475959062576294
Batch 54/64 loss: -0.010782599449157715
Batch 55/64 loss: 0.007047891616821289
Batch 56/64 loss: -0.008933305740356445
Batch 57/64 loss: -0.014534473419189453
Batch 58/64 loss: -0.007886052131652832
Batch 59/64 loss: -0.016430974006652832
Batch 60/64 loss: -0.011652469635009766
Batch 61/64 loss: 0.01353919506072998
Batch 62/64 loss: -0.006626248359680176
Batch 63/64 loss: -0.011363387107849121
Batch 64/64 loss: -0.014844119548797607
Epoch 39  Train loss: -0.008162065814523136  Val loss: 0.03620589230068771
Saving best model, epoch: 39
Epoch 40
-------------------------------
Batch 1/64 loss: -0.01838827133178711
Batch 2/64 loss: -0.029723405838012695
Batch 3/64 loss: -0.008401274681091309
Batch 4/64 loss: -0.01814854145050049
Batch 5/64 loss: -0.001495361328125
Batch 6/64 loss: -0.025646090507507324
Batch 7/64 loss: 0.007921159267425537
Batch 8/64 loss: 0.004585742950439453
Batch 9/64 loss: 0.016878604888916016
Batch 10/64 loss: -0.036811649799346924
Batch 11/64 loss: -0.012951254844665527
Batch 12/64 loss: -0.013900279998779297
Batch 13/64 loss: -0.021883010864257812
Batch 14/64 loss: -0.005954146385192871
Batch 15/64 loss: -0.016048192977905273
Batch 16/64 loss: -0.02297729253768921
Batch 17/64 loss: -0.023833930492401123
Batch 18/64 loss: -0.029291987419128418
Batch 19/64 loss: 0.0015717148780822754
Batch 20/64 loss: 0.0037792325019836426
Batch 21/64 loss: -0.0027276277542114258
Batch 22/64 loss: -0.007832765579223633
Batch 23/64 loss: -0.02641451358795166
Batch 24/64 loss: -0.008172452449798584
Batch 25/64 loss: -0.03488743305206299
Batch 26/64 loss: -0.014444172382354736
Batch 27/64 loss: -0.018460392951965332
Batch 28/64 loss: -0.004063069820404053
Batch 29/64 loss: 0.0067771077156066895
Batch 30/64 loss: -0.02215862274169922
Batch 31/64 loss: -0.0029610395431518555
Batch 32/64 loss: -0.0173833966255188
Batch 33/64 loss: 0.006330728530883789
Batch 34/64 loss: -0.03567284345626831
Batch 35/64 loss: -0.005456864833831787
Batch 36/64 loss: -0.028251051902770996
Batch 37/64 loss: -0.013896346092224121
Batch 38/64 loss: -0.008231818675994873
Batch 39/64 loss: -0.011783003807067871
Batch 40/64 loss: 0.0214155912399292
Batch 41/64 loss: 0.00658571720123291
Batch 42/64 loss: -0.005705535411834717
Batch 43/64 loss: -0.028190791606903076
Batch 44/64 loss: -0.00380706787109375
Batch 45/64 loss: -0.02840578556060791
Batch 46/64 loss: -0.010138869285583496
Batch 47/64 loss: -0.008257687091827393
Batch 48/64 loss: 0.007204890251159668
Batch 49/64 loss: -0.023509681224822998
Batch 50/64 loss: 0.011295080184936523
Batch 51/64 loss: -0.014462471008300781
Batch 52/64 loss: -0.019203484058380127
Batch 53/64 loss: -0.01741117238998413
Batch 54/64 loss: -0.01266258955001831
Batch 55/64 loss: -0.003962576389312744
Batch 56/64 loss: 0.02167612314224243
Batch 57/64 loss: -0.011298418045043945
Batch 58/64 loss: -0.0012025833129882812
Batch 59/64 loss: -0.005752682685852051
Batch 60/64 loss: -0.003281235694885254
Batch 61/64 loss: -0.005930542945861816
Batch 62/64 loss: -0.01171964406967163
Batch 63/64 loss: -0.0132979154586792
Batch 64/64 loss: -0.019243597984313965
Epoch 40  Train loss: -0.010586099531136307  Val loss: 0.04399115441181406
Epoch 41
-------------------------------
Batch 1/64 loss: -0.035269320011138916
Batch 2/64 loss: -0.01488196849822998
Batch 3/64 loss: -0.02717822790145874
Batch 4/64 loss: -0.007484138011932373
Batch 5/64 loss: 0.018039822578430176
Batch 6/64 loss: -0.030327677726745605
Batch 7/64 loss: -0.006365418434143066
Batch 8/64 loss: -0.007930159568786621
Batch 9/64 loss: -0.016819357872009277
Batch 10/64 loss: -0.0006083250045776367
Batch 11/64 loss: -0.014091551303863525
Batch 12/64 loss: -0.004947364330291748
Batch 13/64 loss: -0.008010625839233398
Batch 14/64 loss: -0.004039943218231201
Batch 15/64 loss: -0.029503345489501953
Batch 16/64 loss: -0.008804380893707275
Batch 17/64 loss: -0.01773536205291748
Batch 18/64 loss: -0.004906773567199707
Batch 19/64 loss: -0.01885885000228882
Batch 20/64 loss: -0.013522982597351074
Batch 21/64 loss: -0.01763904094696045
Batch 22/64 loss: -0.010974884033203125
Batch 23/64 loss: -0.014822840690612793
Batch 24/64 loss: 0.005318641662597656
Batch 25/64 loss: -0.004330575466156006
Batch 26/64 loss: -0.022173821926116943
Batch 27/64 loss: -0.019905030727386475
Batch 28/64 loss: -0.016056418418884277
Batch 29/64 loss: -0.01249462366104126
Batch 30/64 loss: 0.00587773323059082
Batch 31/64 loss: -0.004194021224975586
Batch 32/64 loss: -0.002121448516845703
Batch 33/64 loss: -0.000905454158782959
Batch 34/64 loss: -0.015049993991851807
Batch 35/64 loss: -0.010792016983032227
Batch 36/64 loss: -0.015236437320709229
Batch 37/64 loss: -0.00686490535736084
Batch 38/64 loss: 0.008316755294799805
Batch 39/64 loss: 0.009431421756744385
Batch 40/64 loss: 0.004735589027404785
Batch 41/64 loss: -0.013236761093139648
Batch 42/64 loss: -0.0005806684494018555
Batch 43/64 loss: -0.029796898365020752
Batch 44/64 loss: -0.02076125144958496
Batch 45/64 loss: -0.01860940456390381
Batch 46/64 loss: 0.0012208223342895508
Batch 47/64 loss: 0.038929879665374756
Batch 48/64 loss: -0.012661278247833252
Batch 49/64 loss: -0.016590356826782227
Batch 50/64 loss: -0.02451711893081665
Batch 51/64 loss: -0.024483144283294678
Batch 52/64 loss: -0.04105091094970703
Batch 53/64 loss: 0.004513263702392578
Batch 54/64 loss: -0.0037149786949157715
Batch 55/64 loss: -0.015614449977874756
Batch 56/64 loss: -0.0145493745803833
Batch 57/64 loss: -0.0025786161422729492
Batch 58/64 loss: -0.007455587387084961
Batch 59/64 loss: 0.008356630802154541
Batch 60/64 loss: -0.03564852476119995
Batch 61/64 loss: -0.013131380081176758
Batch 62/64 loss: -0.009663283824920654
Batch 63/64 loss: -0.0225602388381958
Batch 64/64 loss: -0.03743171691894531
Epoch 41  Train loss: -0.010907995934579886  Val loss: 0.03391231560625162
Saving best model, epoch: 41
Epoch 42
-------------------------------
Batch 1/64 loss: -0.005217552185058594
Batch 2/64 loss: -0.0250704288482666
Batch 3/64 loss: -0.021101534366607666
Batch 4/64 loss: -0.02803868055343628
Batch 5/64 loss: 0.002832651138305664
Batch 6/64 loss: -0.02821183204650879
Batch 7/64 loss: -0.03479945659637451
Batch 8/64 loss: 0.001034080982208252
Batch 9/64 loss: -0.03345942497253418
Batch 10/64 loss: -0.0007926225662231445
Batch 11/64 loss: -0.011548638343811035
Batch 12/64 loss: -0.04824256896972656
Batch 13/64 loss: 0.00979924201965332
Batch 14/64 loss: -0.02636665105819702
Batch 15/64 loss: -0.0047245025634765625
Batch 16/64 loss: 0.010917425155639648
Batch 17/64 loss: -0.033453285694122314
Batch 18/64 loss: -0.02890312671661377
Batch 19/64 loss: 0.004856467247009277
Batch 20/64 loss: -0.003773808479309082
Batch 21/64 loss: -0.02094799280166626
Batch 22/64 loss: -0.017409563064575195
Batch 23/64 loss: -0.0070095062255859375
Batch 24/64 loss: -0.03448891639709473
Batch 25/64 loss: -0.03990662097930908
Batch 26/64 loss: -0.0010268688201904297
Batch 27/64 loss: -0.0300443172454834
Batch 28/64 loss: -0.01840031147003174
Batch 29/64 loss: -0.014102518558502197
Batch 30/64 loss: -0.012089073657989502
Batch 31/64 loss: -0.028685331344604492
Batch 32/64 loss: -0.008771061897277832
Batch 33/64 loss: -0.022317349910736084
Batch 34/64 loss: -0.018085002899169922
Batch 35/64 loss: -0.00736081600189209
Batch 36/64 loss: -0.0037178993225097656
Batch 37/64 loss: -0.02930450439453125
Batch 38/64 loss: -0.005963623523712158
Batch 39/64 loss: -0.028531372547149658
Batch 40/64 loss: 0.00016236305236816406
Batch 41/64 loss: -0.034830451011657715
Batch 42/64 loss: -0.015112042427062988
Batch 43/64 loss: -0.018950581550598145
Batch 44/64 loss: -0.009187161922454834
Batch 45/64 loss: -0.0051721930503845215
Batch 46/64 loss: -0.012916088104248047
Batch 47/64 loss: -0.034086525440216064
Batch 48/64 loss: -0.02514779567718506
Batch 49/64 loss: -0.00611037015914917
Batch 50/64 loss: -0.011101484298706055
Batch 51/64 loss: 0.0023189783096313477
Batch 52/64 loss: -0.005846798419952393
Batch 53/64 loss: -0.022377729415893555
Batch 54/64 loss: -0.008768975734710693
Batch 55/64 loss: -0.022061288356781006
Batch 56/64 loss: -0.03555804491043091
Batch 57/64 loss: -0.005076229572296143
Batch 58/64 loss: -0.005064249038696289
Batch 59/64 loss: -0.016006112098693848
Batch 60/64 loss: -0.006482243537902832
Batch 61/64 loss: -0.02926713228225708
Batch 62/64 loss: -0.015513598918914795
Batch 63/64 loss: -0.01747649908065796
Batch 64/64 loss: -0.014146149158477783
Epoch 42  Train loss: -0.016041862964630126  Val loss: 0.03322240085536262
Saving best model, epoch: 42
Epoch 43
-------------------------------
Batch 1/64 loss: -0.010830044746398926
Batch 2/64 loss: -0.02245396375656128
Batch 3/64 loss: -0.01918238401412964
Batch 4/64 loss: -0.01602792739868164
Batch 5/64 loss: -0.03427731990814209
Batch 6/64 loss: 0.008435666561126709
Batch 7/64 loss: -0.03098064661026001
Batch 8/64 loss: -0.012222886085510254
Batch 9/64 loss: -0.018393278121948242
Batch 10/64 loss: -0.0207216739654541
Batch 11/64 loss: 0.0001869797706604004
Batch 12/64 loss: -0.02454197406768799
Batch 13/64 loss: -0.027463436126708984
Batch 14/64 loss: -0.018604815006256104
Batch 15/64 loss: -0.031964659690856934
Batch 16/64 loss: -0.02434563636779785
Batch 17/64 loss: -0.031085491180419922
Batch 18/64 loss: -0.040968358516693115
Batch 19/64 loss: -0.02842867374420166
Batch 20/64 loss: -0.0069800615310668945
Batch 21/64 loss: -0.02654784917831421
Batch 22/64 loss: -0.027399539947509766
Batch 23/64 loss: -0.028670907020568848
Batch 24/64 loss: -0.04098385572433472
Batch 25/64 loss: -0.017202138900756836
Batch 26/64 loss: -0.028203368186950684
Batch 27/64 loss: -0.025933265686035156
Batch 28/64 loss: -0.0025663375854492188
Batch 29/64 loss: -0.022665441036224365
Batch 30/64 loss: 0.006839394569396973
Batch 31/64 loss: -0.012886464595794678
Batch 32/64 loss: -0.03623312711715698
Batch 33/64 loss: -0.019472956657409668
Batch 34/64 loss: -0.02674269676208496
Batch 35/64 loss: 0.0003343820571899414
Batch 36/64 loss: -0.03884965181350708
Batch 37/64 loss: -0.0021649599075317383
Batch 38/64 loss: -0.03495997190475464
Batch 39/64 loss: -0.03863292932510376
Batch 40/64 loss: -0.025818705558776855
Batch 41/64 loss: -0.01966649293899536
Batch 42/64 loss: -0.015594244003295898
Batch 43/64 loss: -0.021729230880737305
Batch 44/64 loss: -0.013336181640625
Batch 45/64 loss: -0.027469396591186523
Batch 46/64 loss: -0.022904515266418457
Batch 47/64 loss: -0.009237587451934814
Batch 48/64 loss: -0.02772068977355957
Batch 49/64 loss: -0.01967155933380127
Batch 50/64 loss: -0.006665587425231934
Batch 51/64 loss: -0.018228232860565186
Batch 52/64 loss: -0.005753457546234131
Batch 53/64 loss: -0.019510984420776367
Batch 54/64 loss: -0.026955723762512207
Batch 55/64 loss: -0.0032961368560791016
Batch 56/64 loss: 0.005232334136962891
Batch 57/64 loss: -0.006849944591522217
Batch 58/64 loss: -0.031019747257232666
Batch 59/64 loss: -0.0019252300262451172
Batch 60/64 loss: 0.0023622512817382812
Batch 61/64 loss: -0.020250380039215088
Batch 62/64 loss: 0.005327343940734863
Batch 63/64 loss: -0.0040152668952941895
Batch 64/64 loss: -0.013619303703308105
Epoch 43  Train loss: -0.018489476278716444  Val loss: 0.033189061581064334
Saving best model, epoch: 43
Epoch 44
-------------------------------
Batch 1/64 loss: 0.009946763515472412
Batch 2/64 loss: -0.009146690368652344
Batch 3/64 loss: -0.021509408950805664
Batch 4/64 loss: -0.0017713308334350586
Batch 5/64 loss: -0.012562453746795654
Batch 6/64 loss: -0.03371185064315796
Batch 7/64 loss: -0.04280346632003784
Batch 8/64 loss: -0.013992249965667725
Batch 9/64 loss: -0.028361201286315918
Batch 10/64 loss: -0.006842613220214844
Batch 11/64 loss: -0.027356505393981934
Batch 12/64 loss: -0.029047727584838867
Batch 13/64 loss: -0.004262447357177734
Batch 14/64 loss: -0.011778175830841064
Batch 15/64 loss: -0.02774059772491455
Batch 16/64 loss: -0.02025085687637329
Batch 17/64 loss: -0.028013169765472412
Batch 18/64 loss: -0.016506075859069824
Batch 19/64 loss: -0.010325312614440918
Batch 20/64 loss: -0.02917248010635376
Batch 21/64 loss: -0.007458686828613281
Batch 22/64 loss: -0.012799620628356934
Batch 23/64 loss: -0.007904231548309326
Batch 24/64 loss: -0.001225113868713379
Batch 25/64 loss: -0.024080514907836914
Batch 26/64 loss: -0.024841248989105225
Batch 27/64 loss: -0.006078124046325684
Batch 28/64 loss: -0.04108911752700806
Batch 29/64 loss: -0.009112954139709473
Batch 30/64 loss: -0.03787308931350708
Batch 31/64 loss: -0.01514369249343872
Batch 32/64 loss: -0.02908766269683838
Batch 33/64 loss: 0.0120316743850708
Batch 34/64 loss: -0.011269152164459229
Batch 35/64 loss: -0.04232227802276611
Batch 36/64 loss: -0.02742922306060791
Batch 37/64 loss: -0.016505837440490723
Batch 38/64 loss: -0.018891334533691406
Batch 39/64 loss: -0.03884333372116089
Batch 40/64 loss: -0.003068089485168457
Batch 41/64 loss: -0.01908242702484131
Batch 42/64 loss: -0.032865047454833984
Batch 43/64 loss: -0.03518545627593994
Batch 44/64 loss: -0.030984997749328613
Batch 45/64 loss: -0.006955504417419434
Batch 46/64 loss: -0.01960080862045288
Batch 47/64 loss: -0.029973924160003662
Batch 48/64 loss: -0.02515566349029541
Batch 49/64 loss: -0.014369189739227295
Batch 50/64 loss: -0.03404909372329712
Batch 51/64 loss: -0.03214704990386963
Batch 52/64 loss: -0.005772054195404053
Batch 53/64 loss: 0.012394368648529053
Batch 54/64 loss: -0.009355902671813965
Batch 55/64 loss: 0.00972294807434082
Batch 56/64 loss: -0.0025935769081115723
Batch 57/64 loss: -0.04329657554626465
Batch 58/64 loss: -0.01279836893081665
Batch 59/64 loss: -0.017787575721740723
Batch 60/64 loss: -0.0165899395942688
Batch 61/64 loss: -0.012075841426849365
Batch 62/64 loss: -0.00821012258529663
Batch 63/64 loss: -0.028258562088012695
Batch 64/64 loss: 0.0017660856246948242
Epoch 44  Train loss: -0.017911612286287196  Val loss: 0.03342525881180648
Epoch 45
-------------------------------
Batch 1/64 loss: -0.019180238246917725
Batch 2/64 loss: -0.03583836555480957
Batch 3/64 loss: -0.016335725784301758
Batch 4/64 loss: -0.0360112190246582
Batch 5/64 loss: -0.044092774391174316
Batch 6/64 loss: -0.019924044609069824
Batch 7/64 loss: -0.0419543981552124
Batch 8/64 loss: -0.02195221185684204
Batch 9/64 loss: -0.026271045207977295
Batch 10/64 loss: -0.014393806457519531
Batch 11/64 loss: -0.0010834336280822754
Batch 12/64 loss: -0.015092849731445312
Batch 13/64 loss: -0.0533939003944397
Batch 14/64 loss: -0.01373833417892456
Batch 15/64 loss: -0.034708499908447266
Batch 16/64 loss: -0.033544301986694336
Batch 17/64 loss: -0.020135819911956787
Batch 18/64 loss: -0.024324417114257812
Batch 19/64 loss: -0.032665371894836426
Batch 20/64 loss: -0.02444148063659668
Batch 21/64 loss: -0.004198551177978516
Batch 22/64 loss: -0.007580101490020752
Batch 23/64 loss: -0.035704731941223145
Batch 24/64 loss: -0.02888619899749756
Batch 25/64 loss: -0.040069520473480225
Batch 26/64 loss: -0.023750603199005127
Batch 27/64 loss: -0.03272527456283569
Batch 28/64 loss: -0.01910299062728882
Batch 29/64 loss: -0.026498138904571533
Batch 30/64 loss: 0.005022406578063965
Batch 31/64 loss: -0.01594454050064087
Batch 32/64 loss: -0.04275447130203247
Batch 33/64 loss: -0.035426437854766846
Batch 34/64 loss: -0.009633243083953857
Batch 35/64 loss: -0.023403942584991455
Batch 36/64 loss: -0.04167604446411133
Batch 37/64 loss: -0.02228844165802002
Batch 38/64 loss: -0.02580440044403076
Batch 39/64 loss: -0.05114823579788208
Batch 40/64 loss: -0.037692129611968994
Batch 41/64 loss: -0.03184032440185547
Batch 42/64 loss: -0.03642404079437256
Batch 43/64 loss: -0.01984715461730957
Batch 44/64 loss: 0.00021713972091674805
Batch 45/64 loss: -0.012439370155334473
Batch 46/64 loss: -0.01642829179763794
Batch 47/64 loss: -0.04251563549041748
Batch 48/64 loss: -0.013776063919067383
Batch 49/64 loss: -0.0424189567565918
Batch 50/64 loss: -0.04921317100524902
Batch 51/64 loss: -0.014448046684265137
Batch 52/64 loss: -0.010447859764099121
Batch 53/64 loss: -0.013268768787384033
Batch 54/64 loss: -0.009711861610412598
Batch 55/64 loss: -0.03007739782333374
Batch 56/64 loss: -0.026133835315704346
Batch 57/64 loss: -0.010381162166595459
Batch 58/64 loss: -0.018024027347564697
Batch 59/64 loss: -0.000972747802734375
Batch 60/64 loss: -0.034273624420166016
Batch 61/64 loss: -0.011177480220794678
Batch 62/64 loss: -0.00290757417678833
Batch 63/64 loss: -0.025723695755004883
Batch 64/64 loss: -0.009192585945129395
Epoch 45  Train loss: -0.023960411314870797  Val loss: 0.029049043598043958
Saving best model, epoch: 45
Epoch 46
-------------------------------
Batch 1/64 loss: -0.0456584095954895
Batch 2/64 loss: 0.002895832061767578
Batch 3/64 loss: -0.03111112117767334
Batch 4/64 loss: -0.01728522777557373
Batch 5/64 loss: -0.047194838523864746
Batch 6/64 loss: -0.02406388521194458
Batch 7/64 loss: -0.0015048384666442871
Batch 8/64 loss: -0.03558152914047241
Batch 9/64 loss: -0.05262702703475952
Batch 10/64 loss: -0.0365300178527832
Batch 11/64 loss: -0.02749413251876831
Batch 12/64 loss: -0.027641117572784424
Batch 13/64 loss: -0.035856544971466064
Batch 14/64 loss: -0.030531108379364014
Batch 15/64 loss: -0.027819693088531494
Batch 16/64 loss: -0.046329498291015625
Batch 17/64 loss: -0.02354496717453003
Batch 18/64 loss: -0.03829801082611084
Batch 19/64 loss: -0.039229393005371094
Batch 20/64 loss: -0.03331613540649414
Batch 21/64 loss: -0.02251952886581421
Batch 22/64 loss: -0.025991737842559814
Batch 23/64 loss: -0.007430911064147949
Batch 24/64 loss: -0.03881847858428955
Batch 25/64 loss: -0.014425992965698242
Batch 26/64 loss: -0.028882741928100586
Batch 27/64 loss: -0.02984410524368286
Batch 28/64 loss: -0.029402434825897217
Batch 29/64 loss: -0.023923277854919434
Batch 30/64 loss: -0.017971158027648926
Batch 31/64 loss: -0.03460341691970825
Batch 32/64 loss: -0.015996456146240234
Batch 33/64 loss: -0.030240178108215332
Batch 34/64 loss: -0.025511324405670166
Batch 35/64 loss: -0.032466113567352295
Batch 36/64 loss: -0.015199661254882812
Batch 37/64 loss: -0.03534740209579468
Batch 38/64 loss: -0.039126694202423096
Batch 39/64 loss: -0.03734511137008667
Batch 40/64 loss: -0.029815256595611572
Batch 41/64 loss: -0.02065122127532959
Batch 42/64 loss: -0.03771054744720459
Batch 43/64 loss: -0.03653395175933838
Batch 44/64 loss: -0.03973108530044556
Batch 45/64 loss: -0.02337658405303955
Batch 46/64 loss: -0.03129172325134277
Batch 47/64 loss: -0.015094280242919922
Batch 48/64 loss: -0.02144479751586914
Batch 49/64 loss: -0.03383916616439819
Batch 50/64 loss: 0.00969022512435913
Batch 51/64 loss: 0.018871605396270752
Batch 52/64 loss: -0.01661473512649536
Batch 53/64 loss: -0.007471621036529541
Batch 54/64 loss: -0.0044345855712890625
Batch 55/64 loss: -0.017195940017700195
Batch 56/64 loss: -0.02984708547592163
Batch 57/64 loss: -0.02176058292388916
Batch 58/64 loss: -0.009602963924407959
Batch 59/64 loss: -0.02453404664993286
Batch 60/64 loss: -0.014048397541046143
Batch 61/64 loss: -0.01124584674835205
Batch 62/64 loss: -0.03743523359298706
Batch 63/64 loss: -0.015670299530029297
Batch 64/64 loss: -0.012975633144378662
Epoch 46  Train loss: -0.025165305651870428  Val loss: 0.02674479337082696
Saving best model, epoch: 46
Epoch 47
-------------------------------
Batch 1/64 loss: -0.007688760757446289
Batch 2/64 loss: -0.03725731372833252
Batch 3/64 loss: -0.026457488536834717
Batch 4/64 loss: -0.023793518543243408
Batch 5/64 loss: -0.04059255123138428
Batch 6/64 loss: -0.04071664810180664
Batch 7/64 loss: -0.03667867183685303
Batch 8/64 loss: -0.02424567937850952
Batch 9/64 loss: -0.04026252031326294
Batch 10/64 loss: -0.008484959602355957
Batch 11/64 loss: -0.005497336387634277
Batch 12/64 loss: -0.03346031904220581
Batch 13/64 loss: -0.043889760971069336
Batch 14/64 loss: -0.00291287899017334
Batch 15/64 loss: -0.00918048620223999
Batch 16/64 loss: -0.018821239471435547
Batch 17/64 loss: -0.03999531269073486
Batch 18/64 loss: -0.003537118434906006
Batch 19/64 loss: -0.0009227991104125977
Batch 20/64 loss: -0.005375206470489502
Batch 21/64 loss: -0.0353168249130249
Batch 22/64 loss: -0.02894890308380127
Batch 23/64 loss: -0.03040289878845215
Batch 24/64 loss: 7.587671279907227e-05
Batch 25/64 loss: -0.03703683614730835
Batch 26/64 loss: -0.016873598098754883
Batch 27/64 loss: -0.05422091484069824
Batch 28/64 loss: -0.027785181999206543
Batch 29/64 loss: -0.030142545700073242
Batch 30/64 loss: -0.03207802772521973
Batch 31/64 loss: -0.03259807825088501
Batch 32/64 loss: -0.020766735076904297
Batch 33/64 loss: -0.048278868198394775
Batch 34/64 loss: -0.03130573034286499
Batch 35/64 loss: -0.020232796669006348
Batch 36/64 loss: -0.028276503086090088
Batch 37/64 loss: -0.029883921146392822
Batch 38/64 loss: -0.03772318363189697
Batch 39/64 loss: -0.04774284362792969
Batch 40/64 loss: -0.011296331882476807
Batch 41/64 loss: -0.028537988662719727
Batch 42/64 loss: -0.03675484657287598
Batch 43/64 loss: -0.01634359359741211
Batch 44/64 loss: -0.02805238962173462
Batch 45/64 loss: -0.026423215866088867
Batch 46/64 loss: -0.008463740348815918
Batch 47/64 loss: -0.02734661102294922
Batch 48/64 loss: -0.04269278049468994
Batch 49/64 loss: -0.022208213806152344
Batch 50/64 loss: -0.03494894504547119
Batch 51/64 loss: -0.03594064712524414
Batch 52/64 loss: -0.009667575359344482
Batch 53/64 loss: -0.03020155429840088
Batch 54/64 loss: -0.04294341802597046
Batch 55/64 loss: -0.04840433597564697
Batch 56/64 loss: -0.023851335048675537
Batch 57/64 loss: -0.00963902473449707
Batch 58/64 loss: -0.025693655014038086
Batch 59/64 loss: -0.02840632200241089
Batch 60/64 loss: -0.01573234796524048
Batch 61/64 loss: -0.013629913330078125
Batch 62/64 loss: -0.021974623203277588
Batch 63/64 loss: -0.0035164356231689453
Batch 64/64 loss: -0.014440059661865234
Epoch 47  Train loss: -0.025769552530026902  Val loss: 0.026493269962953127
Saving best model, epoch: 47
Epoch 48
-------------------------------
Batch 1/64 loss: -0.03680497407913208
Batch 2/64 loss: -0.030657470226287842
Batch 3/64 loss: -0.03863745927810669
Batch 4/64 loss: -0.04991292953491211
Batch 5/64 loss: -0.034503042697906494
Batch 6/64 loss: -0.03352975845336914
Batch 7/64 loss: -0.028774023056030273
Batch 8/64 loss: 0.0035753250122070312
Batch 9/64 loss: -0.03787040710449219
Batch 10/64 loss: -0.054211556911468506
Batch 11/64 loss: -0.03046286106109619
Batch 12/64 loss: -0.016189634799957275
Batch 13/64 loss: -0.02908879518508911
Batch 14/64 loss: -0.03522646427154541
Batch 15/64 loss: -0.0020794272422790527
Batch 16/64 loss: -0.045822739601135254
Batch 17/64 loss: -0.03450697660446167
Batch 18/64 loss: -0.018004179000854492
Batch 19/64 loss: -0.025493860244750977
Batch 20/64 loss: -0.046146273612976074
Batch 21/64 loss: -0.03669416904449463
Batch 22/64 loss: -0.018821239471435547
Batch 23/64 loss: -0.024433791637420654
Batch 24/64 loss: -0.031730830669403076
Batch 25/64 loss: -0.0281907320022583
Batch 26/64 loss: -0.0025977492332458496
Batch 27/64 loss: -0.030461609363555908
Batch 28/64 loss: -0.018330395221710205
Batch 29/64 loss: 0.0020560026168823242
Batch 30/64 loss: -0.023968636989593506
Batch 31/64 loss: -0.031192302703857422
Batch 32/64 loss: -0.0407177209854126
Batch 33/64 loss: -0.03086566925048828
Batch 34/64 loss: -0.028728723526000977
Batch 35/64 loss: -0.026935279369354248
Batch 36/64 loss: -0.022799134254455566
Batch 37/64 loss: -0.03287184238433838
Batch 38/64 loss: -0.04786825180053711
Batch 39/64 loss: -0.029087543487548828
Batch 40/64 loss: -0.0389859676361084
Batch 41/64 loss: -0.017647504806518555
Batch 42/64 loss: -0.03471869230270386
Batch 43/64 loss: -0.03273499011993408
Batch 44/64 loss: -0.03475773334503174
Batch 45/64 loss: -0.02412205934524536
Batch 46/64 loss: -0.019576191902160645
Batch 47/64 loss: -0.005961716175079346
Batch 48/64 loss: -0.042169272899627686
Batch 49/64 loss: -0.02411210536956787
Batch 50/64 loss: -0.04372447729110718
Batch 51/64 loss: -0.051479339599609375
Batch 52/64 loss: -0.009592771530151367
Batch 53/64 loss: -0.029894649982452393
Batch 54/64 loss: -0.03202462196350098
Batch 55/64 loss: -0.005905807018280029
Batch 56/64 loss: 0.0066124796867370605
Batch 57/64 loss: -0.03024113178253174
Batch 58/64 loss: -0.016019582748413086
Batch 59/64 loss: -0.04637366533279419
Batch 60/64 loss: -0.02754366397857666
Batch 61/64 loss: -0.042455077171325684
Batch 62/64 loss: -0.03401780128479004
Batch 63/64 loss: -0.041989803314208984
Batch 64/64 loss: -0.029471516609191895
Epoch 48  Train loss: -0.02870787312002743  Val loss: 0.03009298904654906
Epoch 49
-------------------------------
Batch 1/64 loss: -0.06695085763931274
Batch 2/64 loss: -0.019613921642303467
Batch 3/64 loss: -0.039953410625457764
Batch 4/64 loss: -0.049401044845581055
Batch 5/64 loss: -0.04498928785324097
Batch 6/64 loss: -0.034751176834106445
Batch 7/64 loss: -0.020502090454101562
Batch 8/64 loss: -0.029932796955108643
Batch 9/64 loss: -0.044763922691345215
Batch 10/64 loss: -0.027670562267303467
Batch 11/64 loss: -0.02960902452468872
Batch 12/64 loss: -0.05415886640548706
Batch 13/64 loss: -0.02438873052597046
Batch 14/64 loss: -0.0558132529258728
Batch 15/64 loss: -0.04816025495529175
Batch 16/64 loss: -0.02946072816848755
Batch 17/64 loss: 0.007030308246612549
Batch 18/64 loss: -0.026588618755340576
Batch 19/64 loss: -0.022696614265441895
Batch 20/64 loss: -0.011204898357391357
Batch 21/64 loss: -0.03099822998046875
Batch 22/64 loss: -0.04065072536468506
Batch 23/64 loss: -0.030724704265594482
Batch 24/64 loss: -0.035472333431243896
Batch 25/64 loss: -0.028785884380340576
Batch 26/64 loss: -0.0338626503944397
Batch 27/64 loss: -0.019459784030914307
Batch 28/64 loss: -0.0373845100402832
Batch 29/64 loss: -0.05507254600524902
Batch 30/64 loss: -0.04551589488983154
Batch 31/64 loss: -0.03728550672531128
Batch 32/64 loss: -0.042501211166381836
Batch 33/64 loss: -0.02886253595352173
Batch 34/64 loss: -0.028318464756011963
Batch 35/64 loss: -0.036195337772369385
Batch 36/64 loss: -0.024187028408050537
Batch 37/64 loss: -0.020846247673034668
Batch 38/64 loss: -0.04501771926879883
Batch 39/64 loss: -0.031222760677337646
Batch 40/64 loss: -0.026965677738189697
Batch 41/64 loss: -0.018071353435516357
Batch 42/64 loss: -0.019609272480010986
Batch 43/64 loss: -0.039852142333984375
Batch 44/64 loss: -0.018097341060638428
Batch 45/64 loss: -0.025522708892822266
Batch 46/64 loss: -0.015446960926055908
Batch 47/64 loss: -0.02146768569946289
Batch 48/64 loss: -0.045155227184295654
Batch 49/64 loss: -0.04136604070663452
Batch 50/64 loss: -0.005442917346954346
Batch 51/64 loss: -0.02989274263381958
Batch 52/64 loss: -0.02536708116531372
Batch 53/64 loss: -0.018598198890686035
Batch 54/64 loss: -0.03295189142227173
Batch 55/64 loss: -0.04432648420333862
Batch 56/64 loss: -0.03616595268249512
Batch 57/64 loss: -0.03594487905502319
Batch 58/64 loss: -0.03630560636520386
Batch 59/64 loss: -0.02745962142944336
Batch 60/64 loss: -0.04694491624832153
Batch 61/64 loss: -0.016395926475524902
Batch 62/64 loss: -0.06690347194671631
Batch 63/64 loss: -0.04352724552154541
Batch 64/64 loss: -0.020718753337860107
Epoch 49  Train loss: -0.032615909623164754  Val loss: 0.026851964365575732
Epoch 50
-------------------------------
Batch 1/64 loss: -0.03170585632324219
Batch 2/64 loss: -0.05382722616195679
Batch 3/64 loss: -0.04343903064727783
Batch 4/64 loss: -0.05113035440444946
Batch 5/64 loss: -0.023671090602874756
Batch 6/64 loss: -0.021661102771759033
Batch 7/64 loss: -0.04039287567138672
Batch 8/64 loss: -0.01109236478805542
Batch 9/64 loss: -0.025915145874023438
Batch 10/64 loss: -0.03488045930862427
Batch 11/64 loss: -0.030099153518676758
Batch 12/64 loss: -0.03464841842651367
Batch 13/64 loss: -0.04730713367462158
Batch 14/64 loss: -0.05735963582992554
Batch 15/64 loss: -0.04555481672286987
Batch 16/64 loss: -0.04973018169403076
Batch 17/64 loss: -0.049940288066864014
Batch 18/64 loss: -0.035878896713256836
Batch 19/64 loss: -0.020998597145080566
Batch 20/64 loss: 0.004799842834472656
Batch 21/64 loss: -0.005067110061645508
Batch 22/64 loss: -0.020498275756835938
Batch 23/64 loss: -0.058308303356170654
Batch 24/64 loss: -0.014064431190490723
Batch 25/64 loss: -0.05948019027709961
Batch 26/64 loss: -0.035290658473968506
Batch 27/64 loss: -0.039564669132232666
Batch 28/64 loss: -0.04284501075744629
Batch 29/64 loss: -0.04961287975311279
Batch 30/64 loss: -0.024817705154418945
Batch 31/64 loss: -0.02040410041809082
Batch 32/64 loss: -0.02028679847717285
Batch 33/64 loss: -0.042226970195770264
Batch 34/64 loss: -0.02316385507583618
Batch 35/64 loss: -0.036257028579711914
Batch 36/64 loss: -0.01822429895401001
Batch 37/64 loss: -0.019308149814605713
Batch 38/64 loss: -0.042525529861450195
Batch 39/64 loss: -0.033908188343048096
Batch 40/64 loss: -0.033628225326538086
Batch 41/64 loss: -0.032470107078552246
Batch 42/64 loss: -0.044547080993652344
Batch 43/64 loss: -0.05966562032699585
Batch 44/64 loss: -0.036902785301208496
Batch 45/64 loss: -0.02874678373336792
Batch 46/64 loss: -0.014199554920196533
Batch 47/64 loss: -0.01809060573577881
Batch 48/64 loss: -0.02595829963684082
Batch 49/64 loss: -0.06780505180358887
Batch 50/64 loss: -0.03046804666519165
Batch 51/64 loss: -0.012152910232543945
Batch 52/64 loss: -0.045595645904541016
Batch 53/64 loss: -0.05349665880203247
Batch 54/64 loss: -0.010330557823181152
Batch 55/64 loss: -0.03168010711669922
Batch 56/64 loss: -0.026569128036499023
Batch 57/64 loss: -0.015826404094696045
Batch 58/64 loss: -0.0243682861328125
Batch 59/64 loss: -0.02831333875656128
Batch 60/64 loss: -0.03786051273345947
Batch 61/64 loss: -0.0347365140914917
Batch 62/64 loss: -0.04131627082824707
Batch 63/64 loss: -0.015461206436157227
Batch 64/64 loss: -0.057218074798583984
Epoch 50  Train loss: -0.03330808153339461  Val loss: 0.0315788750795974
Epoch 51
-------------------------------
Batch 1/64 loss: -0.03751242160797119
Batch 2/64 loss: -0.015365540981292725
Batch 3/64 loss: -0.03745681047439575
Batch 4/64 loss: -0.016392827033996582
Batch 5/64 loss: -0.022866904735565186
Batch 6/64 loss: -0.040161848068237305
Batch 7/64 loss: -0.049388349056243896
Batch 8/64 loss: -0.05123305320739746
Batch 9/64 loss: -0.04053372144699097
Batch 10/64 loss: -0.025223255157470703
Batch 11/64 loss: -0.032409608364105225
Batch 12/64 loss: -0.042273640632629395
Batch 13/64 loss: -0.046352267265319824
Batch 14/64 loss: -0.029873371124267578
Batch 15/64 loss: -0.04140371084213257
Batch 16/64 loss: 0.005170702934265137
Batch 17/64 loss: -0.04015737771987915
Batch 18/64 loss: -0.027294456958770752
Batch 19/64 loss: -0.04536771774291992
Batch 20/64 loss: -0.025061070919036865
Batch 21/64 loss: -0.04235255718231201
Batch 22/64 loss: -0.041409432888031006
Batch 23/64 loss: -0.023804068565368652
Batch 24/64 loss: -0.06510835886001587
Batch 25/64 loss: -0.0453418493270874
Batch 26/64 loss: -0.03485417366027832
Batch 27/64 loss: 0.00868290662765503
Batch 28/64 loss: -0.039096057415008545
Batch 29/64 loss: -0.03638118505477905
Batch 30/64 loss: -0.013392031192779541
Batch 31/64 loss: -0.026323318481445312
Batch 32/64 loss: -0.01600700616836548
Batch 33/64 loss: -0.058268189430236816
Batch 34/64 loss: -0.027377009391784668
Batch 35/64 loss: -0.03118157386779785
Batch 36/64 loss: -0.045906662940979004
Batch 37/64 loss: -0.04036545753479004
Batch 38/64 loss: -0.03596925735473633
Batch 39/64 loss: -0.05543917417526245
Batch 40/64 loss: -0.05675691366195679
Batch 41/64 loss: -0.03934180736541748
Batch 42/64 loss: -0.01576054096221924
Batch 43/64 loss: 0.006577908992767334
Batch 44/64 loss: -0.04403984546661377
Batch 45/64 loss: -0.04851800203323364
Batch 46/64 loss: -0.022395610809326172
Batch 47/64 loss: -0.03618365526199341
Batch 48/64 loss: -0.014718115329742432
Batch 49/64 loss: -0.03696233034133911
Batch 50/64 loss: -0.04777514934539795
Batch 51/64 loss: -0.03637295961380005
Batch 52/64 loss: -0.05270189046859741
Batch 53/64 loss: -0.027258336544036865
Batch 54/64 loss: 0.0058623552322387695
Batch 55/64 loss: -0.045944929122924805
Batch 56/64 loss: -0.030592799186706543
Batch 57/64 loss: -0.04125744104385376
Batch 58/64 loss: -0.00962764024734497
Batch 59/64 loss: -0.04443705081939697
Batch 60/64 loss: -0.04356426000595093
Batch 61/64 loss: -0.018531322479248047
Batch 62/64 loss: -0.038466036319732666
Batch 63/64 loss: -0.028261125087738037
Batch 64/64 loss: -0.03794437646865845
Epoch 51  Train loss: -0.033357450775071684  Val loss: 0.023822004852426013
Saving best model, epoch: 51
Epoch 52
-------------------------------
Batch 1/64 loss: -0.06291085481643677
Batch 2/64 loss: -0.05709296464920044
Batch 3/64 loss: -0.04522979259490967
Batch 4/64 loss: -0.025357723236083984
Batch 5/64 loss: -0.00908571481704712
Batch 6/64 loss: -0.04107379913330078
Batch 7/64 loss: -0.014528334140777588
Batch 8/64 loss: -0.03836798667907715
Batch 9/64 loss: -0.030158817768096924
Batch 10/64 loss: -0.05303835868835449
Batch 11/64 loss: -0.0344921350479126
Batch 12/64 loss: -0.05027306079864502
Batch 13/64 loss: -0.04728299379348755
Batch 14/64 loss: -0.026831090450286865
Batch 15/64 loss: -0.04554480314254761
Batch 16/64 loss: -0.04847538471221924
Batch 17/64 loss: -0.05367928743362427
Batch 18/64 loss: -0.025258302688598633
Batch 19/64 loss: -0.03765970468521118
Batch 20/64 loss: -0.07034969329833984
Batch 21/64 loss: -0.05695521831512451
Batch 22/64 loss: -0.053115904331207275
Batch 23/64 loss: -0.04209202527999878
Batch 24/64 loss: 0.00044798851013183594
Batch 25/64 loss: -0.021022319793701172
Batch 26/64 loss: -0.027067899703979492
Batch 27/64 loss: -0.06148934364318848
Batch 28/64 loss: -0.029431462287902832
Batch 29/64 loss: -0.022415757179260254
Batch 30/64 loss: -0.04113948345184326
Batch 31/64 loss: -0.04715681076049805
Batch 32/64 loss: -0.04636943340301514
Batch 33/64 loss: -0.02923405170440674
Batch 34/64 loss: -0.030226826667785645
Batch 35/64 loss: -0.01869422197341919
Batch 36/64 loss: -0.05237853527069092
Batch 37/64 loss: -0.04942125082015991
Batch 38/64 loss: -0.021568238735198975
Batch 39/64 loss: -0.031790733337402344
Batch 40/64 loss: -0.046842217445373535
Batch 41/64 loss: -0.048384904861450195
Batch 42/64 loss: -0.06241583824157715
Batch 43/64 loss: -0.028016626834869385
Batch 44/64 loss: -0.010175585746765137
Batch 45/64 loss: -0.01952970027923584
Batch 46/64 loss: -0.03939652442932129
Batch 47/64 loss: -0.05934542417526245
Batch 48/64 loss: -0.03551530838012695
Batch 49/64 loss: -0.041542232036590576
Batch 50/64 loss: -0.030286431312561035
Batch 51/64 loss: -0.029723048210144043
Batch 52/64 loss: -0.027063488960266113
Batch 53/64 loss: -0.049442410469055176
Batch 54/64 loss: -0.013793230056762695
Batch 55/64 loss: -0.044808268547058105
Batch 56/64 loss: -0.012202858924865723
Batch 57/64 loss: -0.037567198276519775
Batch 58/64 loss: -0.04539453983306885
Batch 59/64 loss: -0.06662160158157349
Batch 60/64 loss: -0.056853532791137695
Batch 61/64 loss: -0.04088026285171509
Batch 62/64 loss: -0.02257317304611206
Batch 63/64 loss: -0.03790843486785889
Batch 64/64 loss: -0.03143996000289917
Epoch 52  Train loss: -0.03808117871190987  Val loss: 0.02647655710731585
Epoch 53
-------------------------------
Batch 1/64 loss: -0.031095147132873535
Batch 2/64 loss: -0.029658079147338867
Batch 3/64 loss: -0.030077099800109863
Batch 4/64 loss: -0.048859596252441406
Batch 5/64 loss: -0.05336707830429077
Batch 6/64 loss: -0.04742234945297241
Batch 7/64 loss: -0.040995121002197266
Batch 8/64 loss: -0.0406109094619751
Batch 9/64 loss: -0.07711911201477051
Batch 10/64 loss: -0.02590388059616089
Batch 11/64 loss: -0.06642913818359375
Batch 12/64 loss: -0.03684788942337036
Batch 13/64 loss: -0.02963656187057495
Batch 14/64 loss: -0.04611402750015259
Batch 15/64 loss: -0.037612855434417725
Batch 16/64 loss: -0.018783748149871826
Batch 17/64 loss: -0.0523342490196228
Batch 18/64 loss: -0.047054290771484375
Batch 19/64 loss: -0.0456773042678833
Batch 20/64 loss: -0.060514211654663086
Batch 21/64 loss: -0.03683876991271973
Batch 22/64 loss: -0.034342288970947266
Batch 23/64 loss: -0.057144761085510254
Batch 24/64 loss: -0.049126267433166504
Batch 25/64 loss: -0.020884454250335693
Batch 26/64 loss: -0.05583059787750244
Batch 27/64 loss: -0.023594200611114502
Batch 28/64 loss: -0.05057847499847412
Batch 29/64 loss: -0.03475213050842285
Batch 30/64 loss: -0.045606255531311035
Batch 31/64 loss: -0.024394452571868896
Batch 32/64 loss: -0.03955960273742676
Batch 33/64 loss: -0.041935086250305176
Batch 34/64 loss: -0.04265439510345459
Batch 35/64 loss: -0.025023043155670166
Batch 36/64 loss: -0.0187075138092041
Batch 37/64 loss: -0.049238741397857666
Batch 38/64 loss: -0.006518065929412842
Batch 39/64 loss: -0.037934720516204834
Batch 40/64 loss: -0.02027183771133423
Batch 41/64 loss: -0.0415722131729126
Batch 42/64 loss: -0.04437440633773804
Batch 43/64 loss: -0.033986032009124756
Batch 44/64 loss: -0.028165340423583984
Batch 45/64 loss: -0.04243677854537964
Batch 46/64 loss: -0.056936025619506836
Batch 47/64 loss: -0.029237985610961914
Batch 48/64 loss: -0.057701051235198975
Batch 49/64 loss: -0.031141817569732666
Batch 50/64 loss: -0.0627332329750061
Batch 51/64 loss: -0.0373387336730957
Batch 52/64 loss: -0.03436887264251709
Batch 53/64 loss: -0.04472529888153076
Batch 54/64 loss: -0.037687063217163086
Batch 55/64 loss: -0.018871307373046875
Batch 56/64 loss: -0.048758089542388916
Batch 57/64 loss: -0.042640745639801025
Batch 58/64 loss: -0.034743666648864746
Batch 59/64 loss: -0.028587281703948975
Batch 60/64 loss: -0.052677154541015625
Batch 61/64 loss: -0.05957460403442383
Batch 62/64 loss: -0.01845550537109375
Batch 63/64 loss: -0.04648232460021973
Batch 64/64 loss: -0.035236358642578125
Epoch 53  Train loss: -0.039853664472991344  Val loss: 0.02312387305846329
Saving best model, epoch: 53
Epoch 54
-------------------------------
Batch 1/64 loss: -0.029619336128234863
Batch 2/64 loss: -0.04301828145980835
Batch 3/64 loss: -0.028538644313812256
Batch 4/64 loss: -0.04934895038604736
Batch 5/64 loss: -0.0342029333114624
Batch 6/64 loss: -0.04497861862182617
Batch 7/64 loss: -0.04829198122024536
Batch 8/64 loss: -0.04857802391052246
Batch 9/64 loss: -0.045862674713134766
Batch 10/64 loss: -0.0620456337928772
Batch 11/64 loss: -0.036802828311920166
Batch 12/64 loss: -0.053208887577056885
Batch 13/64 loss: -0.047049641609191895
Batch 14/64 loss: -0.04274284839630127
Batch 15/64 loss: -0.035701096057891846
Batch 16/64 loss: -0.025268197059631348
Batch 17/64 loss: -0.038308560848236084
Batch 18/64 loss: -0.051184892654418945
Batch 19/64 loss: -0.05281168222427368
Batch 20/64 loss: -0.03156113624572754
Batch 21/64 loss: -0.04631727933883667
Batch 22/64 loss: -0.037945449352264404
Batch 23/64 loss: -0.028670668601989746
Batch 24/64 loss: -0.05006372928619385
Batch 25/64 loss: -0.05137532949447632
Batch 26/64 loss: -0.0420190691947937
Batch 27/64 loss: -0.03943514823913574
Batch 28/64 loss: -0.032660841941833496
Batch 29/64 loss: -0.04644298553466797
Batch 30/64 loss: -0.03294116258621216
Batch 31/64 loss: -0.0472182035446167
Batch 32/64 loss: -0.018617570400238037
Batch 33/64 loss: -0.03613317012786865
Batch 34/64 loss: -0.0408397912979126
Batch 35/64 loss: -0.02693265676498413
Batch 36/64 loss: -0.04932647943496704
Batch 37/64 loss: -0.043472230434417725
Batch 38/64 loss: -0.053391098976135254
Batch 39/64 loss: -0.048601388931274414
Batch 40/64 loss: -0.040356576442718506
Batch 41/64 loss: -0.015034794807434082
Batch 42/64 loss: -0.06395256519317627
Batch 43/64 loss: -0.04002940654754639
Batch 44/64 loss: -0.05385470390319824
Batch 45/64 loss: -0.054967641830444336
Batch 46/64 loss: -0.04702180624008179
Batch 47/64 loss: -0.03399848937988281
Batch 48/64 loss: -0.039438605308532715
Batch 49/64 loss: -0.0217096209526062
Batch 50/64 loss: -0.057548701763153076
Batch 51/64 loss: -0.06505173444747925
Batch 52/64 loss: -0.041695237159729004
Batch 53/64 loss: -0.010685861110687256
Batch 54/64 loss: -0.04292696714401245
Batch 55/64 loss: -0.04677093029022217
Batch 56/64 loss: -0.038958072662353516
Batch 57/64 loss: -0.03295546770095825
Batch 58/64 loss: -0.054915666580200195
Batch 59/64 loss: -0.06515157222747803
Batch 60/64 loss: -0.03539508581161499
Batch 61/64 loss: -0.02181476354598999
Batch 62/64 loss: -0.0004875659942626953
Batch 63/64 loss: -0.05382227897644043
Batch 64/64 loss: -0.06445884704589844
Epoch 54  Train loss: -0.041543801625569664  Val loss: 0.024799806350694897
Epoch 55
-------------------------------
Batch 1/64 loss: -0.048970043659210205
Batch 2/64 loss: 0.005576968193054199
Batch 3/64 loss: -0.072529137134552
Batch 4/64 loss: -0.04231339693069458
Batch 5/64 loss: -0.030804216861724854
Batch 6/64 loss: -0.07232081890106201
Batch 7/64 loss: -0.05099213123321533
Batch 8/64 loss: -0.08027660846710205
Batch 9/64 loss: -0.06351375579833984
Batch 10/64 loss: -0.03538268804550171
Batch 11/64 loss: -0.04722440242767334
Batch 12/64 loss: -0.04773426055908203
Batch 13/64 loss: -0.06654489040374756
Batch 14/64 loss: -0.009604215621948242
Batch 15/64 loss: -0.06594914197921753
Batch 16/64 loss: -0.046206772327423096
Batch 17/64 loss: -0.060409486293792725
Batch 18/64 loss: -0.05403703451156616
Batch 19/64 loss: -0.05582922697067261
Batch 20/64 loss: -0.03884994983673096
Batch 21/64 loss: -0.034266889095306396
Batch 22/64 loss: -0.07058990001678467
Batch 23/64 loss: -0.04495656490325928
Batch 24/64 loss: -0.046746134757995605
Batch 25/64 loss: -0.05547738075256348
Batch 26/64 loss: -0.029212355613708496
Batch 27/64 loss: -0.04236125946044922
Batch 28/64 loss: -0.03933137655258179
Batch 29/64 loss: -0.058029234409332275
Batch 30/64 loss: -0.052536845207214355
Batch 31/64 loss: -0.07203304767608643
Batch 32/64 loss: -0.06648880243301392
Batch 33/64 loss: -0.04289275407791138
Batch 34/64 loss: -0.04298996925354004
Batch 35/64 loss: -0.0025541186332702637
Batch 36/64 loss: -0.05437123775482178
Batch 37/64 loss: -0.009505748748779297
Batch 38/64 loss: -0.03288447856903076
Batch 39/64 loss: -0.029242396354675293
Batch 40/64 loss: -0.039097726345062256
Batch 41/64 loss: -0.010945737361907959
Batch 42/64 loss: -0.004523515701293945
Batch 43/64 loss: -0.02340400218963623
Batch 44/64 loss: -0.054664671421051025
Batch 45/64 loss: -0.04813849925994873
Batch 46/64 loss: -0.03756171464920044
Batch 47/64 loss: -0.052191972732543945
Batch 48/64 loss: -0.03572124242782593
Batch 49/64 loss: -0.046672284603118896
Batch 50/64 loss: -0.037958383560180664
Batch 51/64 loss: -0.03137695789337158
Batch 52/64 loss: -0.012369990348815918
Batch 53/64 loss: -0.06850236654281616
Batch 54/64 loss: -0.029575586318969727
Batch 55/64 loss: -0.05390751361846924
Batch 56/64 loss: -0.007952749729156494
Batch 57/64 loss: -0.028149008750915527
Batch 58/64 loss: -0.03631913661956787
Batch 59/64 loss: -0.042899370193481445
Batch 60/64 loss: -0.03510701656341553
Batch 61/64 loss: -0.03495323657989502
Batch 62/64 loss: -0.04552203416824341
Batch 63/64 loss: -0.06124395132064819
Batch 64/64 loss: -0.04119408130645752
Epoch 55  Train loss: -0.042667293081096576  Val loss: 0.02337418819211193
Epoch 56
-------------------------------
Batch 1/64 loss: -0.08323639631271362
Batch 2/64 loss: -0.05427706241607666
Batch 3/64 loss: -0.0488010048866272
Batch 4/64 loss: -0.05790334939956665
Batch 5/64 loss: -0.03644669055938721
Batch 6/64 loss: -0.027799367904663086
Batch 7/64 loss: -0.03751349449157715
Batch 8/64 loss: -0.03749275207519531
Batch 9/64 loss: -0.027240276336669922
Batch 10/64 loss: -0.04602646827697754
Batch 11/64 loss: -0.043305814266204834
Batch 12/64 loss: -0.05389964580535889
Batch 13/64 loss: -0.06170046329498291
Batch 14/64 loss: -0.06053602695465088
Batch 15/64 loss: -0.0677257776260376
Batch 16/64 loss: -0.05567079782485962
Batch 17/64 loss: -0.052439093589782715
Batch 18/64 loss: -0.05635195970535278
Batch 19/64 loss: -0.08617901802062988
Batch 20/64 loss: -0.051049649715423584
Batch 21/64 loss: -0.062147676944732666
Batch 22/64 loss: -0.05613243579864502
Batch 23/64 loss: -0.05691683292388916
Batch 24/64 loss: -0.03413659334182739
Batch 25/64 loss: -0.04038834571838379
Batch 26/64 loss: -0.04662972688674927
Batch 27/64 loss: -0.03448289632797241
Batch 28/64 loss: -0.0663524866104126
Batch 29/64 loss: -0.05080592632293701
Batch 30/64 loss: -0.05153822898864746
Batch 31/64 loss: -0.059933483600616455
Batch 32/64 loss: -0.021065592765808105
Batch 33/64 loss: -0.02754950523376465
Batch 34/64 loss: -0.03234696388244629
Batch 35/64 loss: -0.046872496604919434
Batch 36/64 loss: -0.029495954513549805
Batch 37/64 loss: -0.04719895124435425
Batch 38/64 loss: -0.05705815553665161
Batch 39/64 loss: -0.04613173007965088
Batch 40/64 loss: -0.054859161376953125
Batch 41/64 loss: -0.05361455678939819
Batch 42/64 loss: -0.05656933784484863
Batch 43/64 loss: -0.05650961399078369
Batch 44/64 loss: -0.03879892826080322
Batch 45/64 loss: -0.04685330390930176
Batch 46/64 loss: -0.03117692470550537
Batch 47/64 loss: -0.04765993356704712
Batch 48/64 loss: -0.03499424457550049
Batch 49/64 loss: -0.004527270793914795
Batch 50/64 loss: -0.032032132148742676
Batch 51/64 loss: -0.03186023235321045
Batch 52/64 loss: -0.05655473470687866
Batch 53/64 loss: -0.03616851568222046
Batch 54/64 loss: -0.05490165948867798
Batch 55/64 loss: -0.05125892162322998
Batch 56/64 loss: -0.02940976619720459
Batch 57/64 loss: -0.04032975435256958
Batch 58/64 loss: -0.039584577083587646
Batch 59/64 loss: -0.05094790458679199
Batch 60/64 loss: -0.039934754371643066
Batch 61/64 loss: -0.03288877010345459
Batch 62/64 loss: -0.05017977952957153
Batch 63/64 loss: -0.03716188669204712
Batch 64/64 loss: -0.04614675045013428
Epoch 56  Train loss: -0.04637122855466955  Val loss: 0.022148588473854196
Saving best model, epoch: 56
Epoch 57
-------------------------------
Batch 1/64 loss: -0.05271303653717041
Batch 2/64 loss: -0.07072746753692627
Batch 3/64 loss: -0.04239380359649658
Batch 4/64 loss: -0.057451486587524414
Batch 5/64 loss: -0.043779969215393066
Batch 6/64 loss: -0.055652618408203125
Batch 7/64 loss: -0.053538739681243896
Batch 8/64 loss: -0.06014609336853027
Batch 9/64 loss: -0.03821861743927002
Batch 10/64 loss: -0.06948715448379517
Batch 11/64 loss: -0.07029420137405396
Batch 12/64 loss: -0.04076671600341797
Batch 13/64 loss: -0.03853940963745117
Batch 14/64 loss: -0.042472243309020996
Batch 15/64 loss: -0.03222858905792236
Batch 16/64 loss: -0.05709350109100342
Batch 17/64 loss: -0.05039405822753906
Batch 18/64 loss: -0.0625382661819458
Batch 19/64 loss: -0.04377388954162598
Batch 20/64 loss: -0.053617894649505615
Batch 21/64 loss: -0.04671168327331543
Batch 22/64 loss: -0.058812499046325684
Batch 23/64 loss: -0.06926727294921875
Batch 24/64 loss: -0.018619179725646973
Batch 25/64 loss: -0.07086467742919922
Batch 26/64 loss: -0.06199169158935547
Batch 27/64 loss: -0.055577099323272705
Batch 28/64 loss: -0.049164533615112305
Batch 29/64 loss: -0.0488777756690979
Batch 30/64 loss: -0.046720921993255615
Batch 31/64 loss: -0.017634570598602295
Batch 32/64 loss: -0.07380270957946777
Batch 33/64 loss: -0.011745154857635498
Batch 34/64 loss: -0.053333163261413574
Batch 35/64 loss: -0.029469609260559082
Batch 36/64 loss: -0.0641624927520752
Batch 37/64 loss: -0.03242945671081543
Batch 38/64 loss: -0.06359070539474487
Batch 39/64 loss: -0.04477047920227051
Batch 40/64 loss: -0.0004106760025024414
Batch 41/64 loss: -0.05735057592391968
Batch 42/64 loss: -0.06736886501312256
Batch 43/64 loss: -0.05811220407485962
Batch 44/64 loss: -0.058215558528900146
Batch 45/64 loss: -0.0552445650100708
Batch 46/64 loss: -0.042891502380371094
Batch 47/64 loss: -0.04818439483642578
Batch 48/64 loss: -0.055939674377441406
Batch 49/64 loss: -0.060193777084350586
Batch 50/64 loss: -0.028157413005828857
Batch 51/64 loss: -0.052359700202941895
Batch 52/64 loss: -0.04592621326446533
Batch 53/64 loss: -0.046007633209228516
Batch 54/64 loss: -0.04989981651306152
Batch 55/64 loss: -0.044285476207733154
Batch 56/64 loss: -0.03942364454269409
Batch 57/64 loss: -0.04634690284729004
Batch 58/64 loss: -0.03727591037750244
Batch 59/64 loss: -0.042653560638427734
Batch 60/64 loss: -0.06947559118270874
Batch 61/64 loss: -0.06367141008377075
Batch 62/64 loss: -0.05249953269958496
Batch 63/64 loss: -0.031619369983673096
Batch 64/64 loss: -0.05562227964401245
Epoch 57  Train loss: -0.04938986839032641  Val loss: 0.02138519696763291
Saving best model, epoch: 57
Epoch 58
-------------------------------
Batch 1/64 loss: -0.066561758518219
Batch 2/64 loss: -0.05518919229507446
Batch 3/64 loss: -0.07598423957824707
Batch 4/64 loss: -0.061950862407684326
Batch 5/64 loss: -0.08431768417358398
Batch 6/64 loss: -0.055232107639312744
Batch 7/64 loss: -0.056336939334869385
Batch 8/64 loss: -0.04191458225250244
Batch 9/64 loss: -0.06958979368209839
Batch 10/64 loss: -0.04353386163711548
Batch 11/64 loss: -0.05814957618713379
Batch 12/64 loss: -0.06928658485412598
Batch 13/64 loss: -0.0711289644241333
Batch 14/64 loss: -0.04011869430541992
Batch 15/64 loss: -0.03767657279968262
Batch 16/64 loss: -0.04111605882644653
Batch 17/64 loss: -0.040488481521606445
Batch 18/64 loss: -0.03527545928955078
Batch 19/64 loss: -0.038163602352142334
Batch 20/64 loss: -0.03009939193725586
Batch 21/64 loss: -0.03899276256561279
Batch 22/64 loss: -0.03475749492645264
Batch 23/64 loss: -0.07741928100585938
Batch 24/64 loss: -0.05585712194442749
Batch 25/64 loss: -0.055010437965393066
Batch 26/64 loss: -0.049372076988220215
Batch 27/64 loss: -0.030299484729766846
Batch 28/64 loss: -0.051865339279174805
Batch 29/64 loss: -0.03519231081008911
Batch 30/64 loss: -0.03728830814361572
Batch 31/64 loss: -0.04837143421173096
Batch 32/64 loss: -0.03942251205444336
Batch 33/64 loss: -0.07639908790588379
Batch 34/64 loss: -0.05302184820175171
Batch 35/64 loss: -0.010894596576690674
Batch 36/64 loss: -0.06997126340866089
Batch 37/64 loss: -0.0502392053604126
Batch 38/64 loss: -0.055152058601379395
Batch 39/64 loss: -0.06549549102783203
Batch 40/64 loss: -0.05430716276168823
Batch 41/64 loss: -0.03695094585418701
Batch 42/64 loss: -0.07719075679779053
Batch 43/64 loss: -0.05978500843048096
Batch 44/64 loss: -0.02881145477294922
Batch 45/64 loss: -0.06722593307495117
Batch 46/64 loss: -0.05331176519393921
Batch 47/64 loss: -0.06453180313110352
Batch 48/64 loss: -0.05038577318191528
Batch 49/64 loss: -0.0541839599609375
Batch 50/64 loss: -0.05614340305328369
Batch 51/64 loss: -0.04667395353317261
Batch 52/64 loss: -0.04477965831756592
Batch 53/64 loss: -0.014920294284820557
Batch 54/64 loss: -0.07389205694198608
Batch 55/64 loss: -0.04020422697067261
Batch 56/64 loss: -0.033779263496398926
Batch 57/64 loss: -0.0073893070220947266
Batch 58/64 loss: -0.05376636981964111
Batch 59/64 loss: -0.05012744665145874
Batch 60/64 loss: -0.05132359266281128
Batch 61/64 loss: -0.039515912532806396
Batch 62/64 loss: -0.07329601049423218
Batch 63/64 loss: -0.05546504259109497
Batch 64/64 loss: -0.03761261701583862
Epoch 58  Train loss: -0.05056167981203864  Val loss: 0.023615539483598007
Epoch 59
-------------------------------
Batch 1/64 loss: -0.05202895402908325
Batch 2/64 loss: -0.06157410144805908
Batch 3/64 loss: -0.047814786434173584
Batch 4/64 loss: -0.040599822998046875
Batch 5/64 loss: -0.08473318815231323
Batch 6/64 loss: -0.052409827709198
Batch 7/64 loss: -0.06952393054962158
Batch 8/64 loss: -0.04731178283691406
Batch 9/64 loss: -0.04632830619812012
Batch 10/64 loss: -0.06341087818145752
Batch 11/64 loss: -0.049702346324920654
Batch 12/64 loss: -0.021260738372802734
Batch 13/64 loss: -0.07489180564880371
Batch 14/64 loss: -0.04536348581314087
Batch 15/64 loss: -0.054016053676605225
Batch 16/64 loss: -0.07475548982620239
Batch 17/64 loss: -0.050795912742614746
Batch 18/64 loss: -0.012902498245239258
Batch 19/64 loss: -0.04501748085021973
Batch 20/64 loss: -0.042820632457733154
Batch 21/64 loss: -0.061088740825653076
Batch 22/64 loss: -0.04355287551879883
Batch 23/64 loss: -0.04770362377166748
Batch 24/64 loss: -0.042311906814575195
Batch 25/64 loss: -0.05855363607406616
Batch 26/64 loss: -0.044905245304107666
Batch 27/64 loss: -0.058127760887145996
Batch 28/64 loss: -0.04512983560562134
Batch 29/64 loss: -0.07070416212081909
Batch 30/64 loss: -0.04707682132720947
Batch 31/64 loss: -0.06285524368286133
Batch 32/64 loss: -0.06450557708740234
Batch 33/64 loss: -0.033476948738098145
Batch 34/64 loss: -0.05116093158721924
Batch 35/64 loss: -0.05968278646469116
Batch 36/64 loss: -0.0682976245880127
Batch 37/64 loss: -0.05044299364089966
Batch 38/64 loss: -0.06424617767333984
Batch 39/64 loss: -0.034910738468170166
Batch 40/64 loss: -0.018797218799591064
Batch 41/64 loss: -0.028630316257476807
Batch 42/64 loss: -0.06811279058456421
Batch 43/64 loss: -0.05864572525024414
Batch 44/64 loss: -0.07311826944351196
Batch 45/64 loss: -0.05182915925979614
Batch 46/64 loss: -0.052607953548431396
Batch 47/64 loss: -0.061542630195617676
Batch 48/64 loss: -0.04378688335418701
Batch 49/64 loss: -0.020753026008605957
Batch 50/64 loss: -0.032437920570373535
Batch 51/64 loss: -0.05858016014099121
Batch 52/64 loss: -0.05305194854736328
Batch 53/64 loss: -0.056419432163238525
Batch 54/64 loss: -0.04685568809509277
Batch 55/64 loss: -0.0702279806137085
Batch 56/64 loss: -0.05732947587966919
Batch 57/64 loss: -0.04419344663619995
Batch 58/64 loss: -0.02815955877304077
Batch 59/64 loss: -0.06055140495300293
Batch 60/64 loss: -0.05735665559768677
Batch 61/64 loss: -0.04236769676208496
Batch 62/64 loss: -0.03599601984024048
Batch 63/64 loss: -0.047364890575408936
Batch 64/64 loss: -0.042869746685028076
Epoch 59  Train loss: -0.05093120336532593  Val loss: 0.024355104698757946
Epoch 60
-------------------------------
Batch 1/64 loss: -0.05547970533370972
Batch 2/64 loss: -0.07931607961654663
Batch 3/64 loss: -0.06753838062286377
Batch 4/64 loss: -0.05005645751953125
Batch 5/64 loss: -0.05564147233963013
Batch 6/64 loss: -0.05088835954666138
Batch 7/64 loss: -0.06507992744445801
Batch 8/64 loss: -0.04466593265533447
Batch 9/64 loss: -0.04808390140533447
Batch 10/64 loss: -0.05423098802566528
Batch 11/64 loss: -0.07845354080200195
Batch 12/64 loss: -0.03016453981399536
Batch 13/64 loss: -0.04138308763504028
Batch 14/64 loss: -0.02395153045654297
Batch 15/64 loss: -0.05573892593383789
Batch 16/64 loss: -0.0429837703704834
Batch 17/64 loss: -0.053912997245788574
Batch 18/64 loss: -0.06608068943023682
Batch 19/64 loss: -0.06458830833435059
Batch 20/64 loss: -0.07041102647781372
Batch 21/64 loss: -0.047919273376464844
Batch 22/64 loss: -0.07116115093231201
Batch 23/64 loss: -0.07128185033798218
Batch 24/64 loss: -0.034840941429138184
Batch 25/64 loss: -0.07263010740280151
Batch 26/64 loss: -0.06021970510482788
Batch 27/64 loss: -0.054575324058532715
Batch 28/64 loss: -0.05879855155944824
Batch 29/64 loss: -0.05840092897415161
Batch 30/64 loss: -0.05827188491821289
Batch 31/64 loss: -0.04771065711975098
Batch 32/64 loss: -0.0589679479598999
Batch 33/64 loss: -0.06912773847579956
Batch 34/64 loss: -0.0697479248046875
Batch 35/64 loss: -0.04041182994842529
Batch 36/64 loss: -0.051441073417663574
Batch 37/64 loss: -0.04015922546386719
Batch 38/64 loss: -0.04469919204711914
Batch 39/64 loss: -0.04598557949066162
Batch 40/64 loss: -0.046263158321380615
Batch 41/64 loss: -0.06262415647506714
Batch 42/64 loss: -0.06336981058120728
Batch 43/64 loss: -0.07950043678283691
Batch 44/64 loss: -0.04634732007980347
Batch 45/64 loss: -0.07392913103103638
Batch 46/64 loss: -0.03839755058288574
Batch 47/64 loss: -0.040383756160736084
Batch 48/64 loss: -0.07750487327575684
Batch 49/64 loss: -0.0353274941444397
Batch 50/64 loss: -0.020697712898254395
Batch 51/64 loss: -0.058765530586242676
Batch 52/64 loss: -0.05266648530960083
Batch 53/64 loss: -0.07859784364700317
Batch 54/64 loss: -0.031124413013458252
Batch 55/64 loss: -0.037709832191467285
Batch 56/64 loss: -0.04780614376068115
Batch 57/64 loss: -0.07028216123580933
Batch 58/64 loss: -0.06430214643478394
Batch 59/64 loss: -0.050529539585113525
Batch 60/64 loss: -0.057749927043914795
Batch 61/64 loss: -0.03564852476119995
Batch 62/64 loss: -0.013766884803771973
Batch 63/64 loss: -0.04589301347732544
Batch 64/64 loss: -0.0480954647064209
Epoch 60  Train loss: -0.053651136510512405  Val loss: 0.02247199569780802
Epoch 61
-------------------------------
Batch 1/64 loss: -0.07535380125045776
Batch 2/64 loss: -0.04793423414230347
Batch 3/64 loss: -0.06263971328735352
Batch 4/64 loss: -0.050849080085754395
Batch 5/64 loss: -0.057535648345947266
Batch 6/64 loss: -0.06542783975601196
Batch 7/64 loss: -0.06677097082138062
Batch 8/64 loss: -0.008215725421905518
Batch 9/64 loss: -0.060957252979278564
Batch 10/64 loss: -0.05716753005981445
Batch 11/64 loss: -0.06326889991760254
Batch 12/64 loss: -0.018948614597320557
Batch 13/64 loss: -0.05160480737686157
Batch 14/64 loss: -0.08084017038345337
Batch 15/64 loss: -0.04370933771133423
Batch 16/64 loss: -0.024365544319152832
Batch 17/64 loss: -0.08223319053649902
Batch 18/64 loss: -0.07880151271820068
Batch 19/64 loss: -0.06559675931930542
Batch 20/64 loss: -0.05061018466949463
Batch 21/64 loss: -0.06737339496612549
Batch 22/64 loss: -0.04764425754547119
Batch 23/64 loss: -0.08473742008209229
Batch 24/64 loss: -0.07459205389022827
Batch 25/64 loss: -0.05932962894439697
Batch 26/64 loss: -0.05854988098144531
Batch 27/64 loss: -0.07491409778594971
Batch 28/64 loss: -0.02501809597015381
Batch 29/64 loss: -0.0366702675819397
Batch 30/64 loss: -0.05082380771636963
Batch 31/64 loss: -0.06700330972671509
Batch 32/64 loss: -0.03627711534500122
Batch 33/64 loss: -0.0716780424118042
Batch 34/64 loss: -0.053063809871673584
Batch 35/64 loss: -0.05272209644317627
Batch 36/64 loss: -0.0752338171005249
Batch 37/64 loss: -0.05272454023361206
Batch 38/64 loss: -0.07255148887634277
Batch 39/64 loss: -0.043933987617492676
Batch 40/64 loss: -0.03626847267150879
Batch 41/64 loss: -0.039217472076416016
Batch 42/64 loss: -0.05402112007141113
Batch 43/64 loss: -0.03729712963104248
Batch 44/64 loss: -0.03562116622924805
Batch 45/64 loss: -0.035565733909606934
Batch 46/64 loss: -0.0418437123298645
Batch 47/64 loss: -0.08210998773574829
Batch 48/64 loss: -0.0627448558807373
Batch 49/64 loss: -0.06796902418136597
Batch 50/64 loss: -0.06746327877044678
Batch 51/64 loss: -0.06383633613586426
Batch 52/64 loss: -0.07247930765151978
Batch 53/64 loss: -0.049788832664489746
Batch 54/64 loss: -0.04402565956115723
Batch 55/64 loss: -0.06480652093887329
Batch 56/64 loss: -0.06119805574417114
Batch 57/64 loss: -0.0314059853553772
Batch 58/64 loss: -0.06592702865600586
Batch 59/64 loss: -0.059200406074523926
Batch 60/64 loss: -0.06732845306396484
Batch 61/64 loss: -0.07675892114639282
Batch 62/64 loss: -0.029850780963897705
Batch 63/64 loss: -0.04689061641693115
Batch 64/64 loss: -0.054925620555877686
Epoch 61  Train loss: -0.055725254965763464  Val loss: 0.02136191449214503
Saving best model, epoch: 61
Epoch 62
-------------------------------
Batch 1/64 loss: -0.0760989785194397
Batch 2/64 loss: -0.0383419394493103
Batch 3/64 loss: -0.06351208686828613
Batch 4/64 loss: -0.0559578537940979
Batch 5/64 loss: -0.09274744987487793
Batch 6/64 loss: -0.06002199649810791
Batch 7/64 loss: -0.03176867961883545
Batch 8/64 loss: -0.08217006921768188
Batch 9/64 loss: -0.05961954593658447
Batch 10/64 loss: -0.06374895572662354
Batch 11/64 loss: -0.08081209659576416
Batch 12/64 loss: -0.0673590898513794
Batch 13/64 loss: -0.051065146923065186
Batch 14/64 loss: -0.05102407932281494
Batch 15/64 loss: -0.04807865619659424
Batch 16/64 loss: -0.040368735790252686
Batch 17/64 loss: -0.07077455520629883
Batch 18/64 loss: -0.046627700328826904
Batch 19/64 loss: -0.06547683477401733
Batch 20/64 loss: -0.04577910900115967
Batch 21/64 loss: -0.06682097911834717
Batch 22/64 loss: -0.052674710750579834
Batch 23/64 loss: -0.048326849937438965
Batch 24/64 loss: -0.07865619659423828
Batch 25/64 loss: -0.04169154167175293
Batch 26/64 loss: -0.07232862710952759
Batch 27/64 loss: -0.05194121599197388
Batch 28/64 loss: -0.06431013345718384
Batch 29/64 loss: -0.05016869306564331
Batch 30/64 loss: -0.057248711585998535
Batch 31/64 loss: -0.07631611824035645
Batch 32/64 loss: -0.056734561920166016
Batch 33/64 loss: -0.0805739164352417
Batch 34/64 loss: -0.054401934146881104
Batch 35/64 loss: -0.04959285259246826
Batch 36/64 loss: -0.05799442529678345
Batch 37/64 loss: -0.05229341983795166
Batch 38/64 loss: -0.07092714309692383
Batch 39/64 loss: -0.0627051591873169
Batch 40/64 loss: -0.07426279783248901
Batch 41/64 loss: -0.04757148027420044
Batch 42/64 loss: -0.0700981616973877
Batch 43/64 loss: -0.044751524925231934
Batch 44/64 loss: -0.033201515674591064
Batch 45/64 loss: -0.06389284133911133
Batch 46/64 loss: -0.04853260517120361
Batch 47/64 loss: -0.04515731334686279
Batch 48/64 loss: -0.057625651359558105
Batch 49/64 loss: -0.05627375841140747
Batch 50/64 loss: -0.018211066722869873
Batch 51/64 loss: -0.06359308958053589
Batch 52/64 loss: -0.038492441177368164
Batch 53/64 loss: -0.06554508209228516
Batch 54/64 loss: -0.05553227663040161
Batch 55/64 loss: -0.038626790046691895
Batch 56/64 loss: -0.043352723121643066
Batch 57/64 loss: -0.05445373058319092
Batch 58/64 loss: -0.0772365927696228
Batch 59/64 loss: -0.06613308191299438
Batch 60/64 loss: -0.07278919219970703
Batch 61/64 loss: -0.07616174221038818
Batch 62/64 loss: -0.06886041164398193
Batch 63/64 loss: -0.06020456552505493
Batch 64/64 loss: -0.05573350191116333
Epoch 62  Train loss: -0.058343863720987356  Val loss: 0.025205957930522275
Epoch 63
-------------------------------
Batch 1/64 loss: -0.06157737970352173
Batch 2/64 loss: -0.07320904731750488
Batch 3/64 loss: -0.07909846305847168
Batch 4/64 loss: -0.05560070276260376
Batch 5/64 loss: -0.07185745239257812
Batch 6/64 loss: -0.06281924247741699
Batch 7/64 loss: -0.049012064933776855
Batch 8/64 loss: -0.05188703536987305
Batch 9/64 loss: -0.03570902347564697
Batch 10/64 loss: -0.04844146966934204
Batch 11/64 loss: -0.06845825910568237
Batch 12/64 loss: -0.06925970315933228
Batch 13/64 loss: -0.06831294298171997
Batch 14/64 loss: -0.0500333309173584
Batch 15/64 loss: -0.06016486883163452
Batch 16/64 loss: -0.03087007999420166
Batch 17/64 loss: -0.08025556802749634
Batch 18/64 loss: -0.03966796398162842
Batch 19/64 loss: -0.056424736976623535
Batch 20/64 loss: -0.08234071731567383
Batch 21/64 loss: -0.05182468891143799
Batch 22/64 loss: -0.05735325813293457
Batch 23/64 loss: -0.07090049982070923
Batch 24/64 loss: -0.062204599380493164
Batch 25/64 loss: -0.04306435585021973
Batch 26/64 loss: -0.06337869167327881
Batch 27/64 loss: -0.0730711817741394
Batch 28/64 loss: -0.05594515800476074
Batch 29/64 loss: -0.07215607166290283
Batch 30/64 loss: -0.06635075807571411
Batch 31/64 loss: -0.08614557981491089
Batch 32/64 loss: -0.060727834701538086
Batch 33/64 loss: -0.048069775104522705
Batch 34/64 loss: -0.05169779062271118
Batch 35/64 loss: -0.07664752006530762
Batch 36/64 loss: -0.038843393325805664
Batch 37/64 loss: -0.07980203628540039
Batch 38/64 loss: -0.06760895252227783
Batch 39/64 loss: -0.07227623462677002
Batch 40/64 loss: -0.08015316724777222
Batch 41/64 loss: -0.04642987251281738
Batch 42/64 loss: -0.05003225803375244
Batch 43/64 loss: -0.03875863552093506
Batch 44/64 loss: -0.07124686241149902
Batch 45/64 loss: -0.04684269428253174
Batch 46/64 loss: -0.057973265647888184
Batch 47/64 loss: -0.0797964334487915
Batch 48/64 loss: -0.08462530374526978
Batch 49/64 loss: -0.03662842512130737
Batch 50/64 loss: -0.06866919994354248
Batch 51/64 loss: -0.06566816568374634
Batch 52/64 loss: -0.03827691078186035
Batch 53/64 loss: -0.07448899745941162
Batch 54/64 loss: -0.060366928577423096
Batch 55/64 loss: -0.04044228792190552
Batch 56/64 loss: -0.03957873582839966
Batch 57/64 loss: -0.0754351019859314
Batch 58/64 loss: -0.07434475421905518
Batch 59/64 loss: -0.06781667470932007
Batch 60/64 loss: -0.04387319087982178
Batch 61/64 loss: -0.0446697473526001
Batch 62/64 loss: -0.03101569414138794
Batch 63/64 loss: -0.060636281967163086
Batch 64/64 loss: -0.0631914734840393
Epoch 63  Train loss: -0.05989382944855035  Val loss: 0.02100907158605831
Saving best model, epoch: 63
Epoch 64
-------------------------------
Batch 1/64 loss: -0.06089073419570923
Batch 2/64 loss: -0.04433053731918335
Batch 3/64 loss: -0.08126622438430786
Batch 4/64 loss: -0.051115453243255615
Batch 5/64 loss: -0.06829994916915894
Batch 6/64 loss: -0.05252432823181152
Batch 7/64 loss: -0.05980515480041504
Batch 8/64 loss: -0.06758850812911987
Batch 9/64 loss: -0.04266703128814697
Batch 10/64 loss: -0.05867820978164673
Batch 11/64 loss: -0.02294623851776123
Batch 12/64 loss: -0.07032108306884766
Batch 13/64 loss: -0.06567400693893433
Batch 14/64 loss: -0.06545823812484741
Batch 15/64 loss: -0.0749853253364563
Batch 16/64 loss: -0.06999737024307251
Batch 17/64 loss: -0.05972832441329956
Batch 18/64 loss: -0.06591153144836426
Batch 19/64 loss: -0.07772386074066162
Batch 20/64 loss: -0.05838334560394287
Batch 21/64 loss: -0.03565406799316406
Batch 22/64 loss: -0.05949586629867554
Batch 23/64 loss: -0.07398432493209839
Batch 24/64 loss: -0.05549466609954834
Batch 25/64 loss: -0.0701136589050293
Batch 26/64 loss: -0.056388139724731445
Batch 27/64 loss: -0.0730661153793335
Batch 28/64 loss: -0.06331270933151245
Batch 29/64 loss: -0.06206715106964111
Batch 30/64 loss: -0.07065725326538086
Batch 31/64 loss: -0.05472278594970703
Batch 32/64 loss: -0.09525531530380249
Batch 33/64 loss: -0.05941706895828247
Batch 34/64 loss: -0.06104850769042969
Batch 35/64 loss: -0.07682877779006958
Batch 36/64 loss: -0.07961678504943848
Batch 37/64 loss: -0.08855545520782471
Batch 38/64 loss: -0.06762099266052246
Batch 39/64 loss: -0.05610513687133789
Batch 40/64 loss: -0.07651150226593018
Batch 41/64 loss: -0.06777942180633545
Batch 42/64 loss: -0.04998844861984253
Batch 43/64 loss: -0.05049210786819458
Batch 44/64 loss: -0.0791119933128357
Batch 45/64 loss: -0.055904388427734375
Batch 46/64 loss: -0.09092056751251221
Batch 47/64 loss: -0.08246380090713501
Batch 48/64 loss: -0.07318401336669922
Batch 49/64 loss: -0.04304462671279907
Batch 50/64 loss: -0.04400193691253662
Batch 51/64 loss: -0.05349534749984741
Batch 52/64 loss: -0.05172485113143921
Batch 53/64 loss: -0.06614309549331665
Batch 54/64 loss: -0.04913043975830078
Batch 55/64 loss: -0.033801257610321045
Batch 56/64 loss: -0.006794631481170654
Batch 57/64 loss: -0.07158470153808594
Batch 58/64 loss: -0.060162901878356934
Batch 59/64 loss: -0.06890535354614258
Batch 60/64 loss: -0.054051876068115234
Batch 61/64 loss: -0.06998801231384277
Batch 62/64 loss: -0.040628910064697266
Batch 63/64 loss: -0.08257067203521729
Batch 64/64 loss: -0.048866212368011475
Epoch 64  Train loss: -0.06175270197438259  Val loss: 0.0183828460801508
Saving best model, epoch: 64
Epoch 65
-------------------------------
Batch 1/64 loss: -0.08194059133529663
Batch 2/64 loss: -0.08149391412734985
Batch 3/64 loss: -0.0765918493270874
Batch 4/64 loss: -0.032132625579833984
Batch 5/64 loss: -0.07323968410491943
Batch 6/64 loss: -0.08543252944946289
Batch 7/64 loss: -0.07402914762496948
Batch 8/64 loss: -0.061419010162353516
Batch 9/64 loss: -0.06242990493774414
Batch 10/64 loss: -0.05406850576400757
Batch 11/64 loss: -0.03827559947967529
Batch 12/64 loss: -0.06999194622039795
Batch 13/64 loss: -0.05539983510971069
Batch 14/64 loss: -0.05843466520309448
Batch 15/64 loss: -0.06470823287963867
Batch 16/64 loss: -0.09618210792541504
Batch 17/64 loss: -0.06383216381072998
Batch 18/64 loss: -0.06381094455718994
Batch 19/64 loss: -0.07210636138916016
Batch 20/64 loss: -0.07750385999679565
Batch 21/64 loss: -0.07500970363616943
Batch 22/64 loss: -0.07059770822525024
Batch 23/64 loss: -0.06625354290008545
Batch 24/64 loss: -0.08587777614593506
Batch 25/64 loss: -0.08222633600234985
Batch 26/64 loss: -0.08091908693313599
Batch 27/64 loss: -0.050206542015075684
Batch 28/64 loss: -0.07851332426071167
Batch 29/64 loss: -0.08043843507766724
Batch 30/64 loss: -0.06581777334213257
Batch 31/64 loss: -0.06493282318115234
Batch 32/64 loss: -0.08506464958190918
Batch 33/64 loss: -0.0405421257019043
Batch 34/64 loss: -0.05381673574447632
Batch 35/64 loss: -0.06180542707443237
Batch 36/64 loss: -0.03777414560317993
Batch 37/64 loss: -0.055342912673950195
Batch 38/64 loss: -0.04690974950790405
Batch 39/64 loss: -0.06458663940429688
Batch 40/64 loss: -0.06270027160644531
Batch 41/64 loss: -0.07548075914382935
Batch 42/64 loss: -0.07711923122406006
Batch 43/64 loss: -0.04601597785949707
Batch 44/64 loss: -0.03742849826812744
Batch 45/64 loss: -0.06170451641082764
Batch 46/64 loss: -0.06260514259338379
Batch 47/64 loss: -0.04196584224700928
Batch 48/64 loss: -0.07358169555664062
Batch 49/64 loss: -0.04725295305252075
Batch 50/64 loss: -0.05073976516723633
Batch 51/64 loss: -0.05585169792175293
Batch 52/64 loss: -0.05201828479766846
Batch 53/64 loss: -0.047482430934906006
Batch 54/64 loss: -0.06872308254241943
Batch 55/64 loss: -0.034991323947906494
Batch 56/64 loss: -0.09096634387969971
Batch 57/64 loss: -0.0386958122253418
Batch 58/64 loss: -0.0778963565826416
Batch 59/64 loss: -0.08256185054779053
Batch 60/64 loss: -0.07232177257537842
Batch 61/64 loss: -0.04045373201370239
Batch 62/64 loss: -0.057060837745666504
Batch 63/64 loss: -0.08909237384796143
Batch 64/64 loss: -0.043637752532958984
Epoch 65  Train loss: -0.06379761228374406  Val loss: 0.018596106788137116
Epoch 66
-------------------------------
Batch 1/64 loss: -0.05597203969955444
Batch 2/64 loss: -0.08713871240615845
Batch 3/64 loss: -0.06439000368118286
Batch 4/64 loss: -0.06999260187149048
Batch 5/64 loss: -0.08167374134063721
Batch 6/64 loss: -0.05070781707763672
Batch 7/64 loss: -0.07789391279220581
Batch 8/64 loss: -0.07895714044570923
Batch 9/64 loss: -0.08111059665679932
Batch 10/64 loss: -0.072140634059906
Batch 11/64 loss: -0.06806445121765137
Batch 12/64 loss: -0.08124721050262451
Batch 13/64 loss: -0.0648961067199707
Batch 14/64 loss: -0.07352209091186523
Batch 15/64 loss: -0.07599061727523804
Batch 16/64 loss: -0.07630276679992676
Batch 17/64 loss: -0.062103867530822754
Batch 18/64 loss: -0.05663132667541504
Batch 19/64 loss: -0.08551758527755737
Batch 20/64 loss: -0.066977858543396
Batch 21/64 loss: -0.07905936241149902
Batch 22/64 loss: -0.052400171756744385
Batch 23/64 loss: -0.06615936756134033
Batch 24/64 loss: -0.061649858951568604
Batch 25/64 loss: -0.06754004955291748
Batch 26/64 loss: -0.07204967737197876
Batch 27/64 loss: -0.08262008428573608
Batch 28/64 loss: -0.06924217939376831
Batch 29/64 loss: -0.0743570327758789
Batch 30/64 loss: -0.0650339126586914
Batch 31/64 loss: -0.09712326526641846
Batch 32/64 loss: -0.05065792798995972
Batch 33/64 loss: -0.07996499538421631
Batch 34/64 loss: -0.042491912841796875
Batch 35/64 loss: -0.050025999546051025
Batch 36/64 loss: -0.07729572057723999
Batch 37/64 loss: -0.05614227056503296
Batch 38/64 loss: -0.038880348205566406
Batch 39/64 loss: -0.06591308116912842
Batch 40/64 loss: -0.06335312128067017
Batch 41/64 loss: -0.07161396741867065
Batch 42/64 loss: -0.06442493200302124
Batch 43/64 loss: -0.06923872232437134
Batch 44/64 loss: -0.049266517162323
Batch 45/64 loss: -0.04505652189254761
Batch 46/64 loss: -0.05926012992858887
Batch 47/64 loss: -0.05524647235870361
Batch 48/64 loss: -0.054200828075408936
Batch 49/64 loss: -0.08640003204345703
Batch 50/64 loss: -0.07540535926818848
Batch 51/64 loss: -0.07834774255752563
Batch 52/64 loss: -0.08661770820617676
Batch 53/64 loss: -0.09114956855773926
Batch 54/64 loss: -0.054821133613586426
Batch 55/64 loss: -0.04174816608428955
Batch 56/64 loss: -0.07736945152282715
Batch 57/64 loss: -0.07037532329559326
Batch 58/64 loss: -0.0548664927482605
Batch 59/64 loss: -0.034355342388153076
Batch 60/64 loss: -0.01956021785736084
Batch 61/64 loss: -0.09104382991790771
Batch 62/64 loss: -0.06376290321350098
Batch 63/64 loss: -0.07628178596496582
Batch 64/64 loss: -0.046497464179992676
Epoch 66  Train loss: -0.0666427869422763  Val loss: 0.018305227518900975
Saving best model, epoch: 66
Epoch 67
-------------------------------
Batch 1/64 loss: -0.05875742435455322
Batch 2/64 loss: -0.062204599380493164
Batch 3/64 loss: -0.04875469207763672
Batch 4/64 loss: -0.05610591173171997
Batch 5/64 loss: -0.07575428485870361
Batch 6/64 loss: -0.0795145034790039
Batch 7/64 loss: -0.06953966617584229
Batch 8/64 loss: -0.05220520496368408
Batch 9/64 loss: -0.08834677934646606
Batch 10/64 loss: -0.06961172819137573
Batch 11/64 loss: -0.04734581708908081
Batch 12/64 loss: -0.04077887535095215
Batch 13/64 loss: -0.07090914249420166
Batch 14/64 loss: -0.049169063568115234
Batch 15/64 loss: -0.058976590633392334
Batch 16/64 loss: -0.0768314003944397
Batch 17/64 loss: -0.0580439567565918
Batch 18/64 loss: -0.06442534923553467
Batch 19/64 loss: -0.0872429609298706
Batch 20/64 loss: -0.049784183502197266
Batch 21/64 loss: -0.04174625873565674
Batch 22/64 loss: -0.06320345401763916
Batch 23/64 loss: -0.07618653774261475
Batch 24/64 loss: -0.06640726327896118
Batch 25/64 loss: -0.08239072561264038
Batch 26/64 loss: -0.06782877445220947
Batch 27/64 loss: -0.09416961669921875
Batch 28/64 loss: -0.10629653930664062
Batch 29/64 loss: -0.06423014402389526
Batch 30/64 loss: -0.07197725772857666
Batch 31/64 loss: -0.07299095392227173
Batch 32/64 loss: -0.054434895515441895
Batch 33/64 loss: -0.07269793748855591
Batch 34/64 loss: -0.07983732223510742
Batch 35/64 loss: -0.08288824558258057
Batch 36/64 loss: -0.07913482189178467
Batch 37/64 loss: -0.08000832796096802
Batch 38/64 loss: -0.0785064697265625
Batch 39/64 loss: -0.0651131272315979
Batch 40/64 loss: -0.06534093618392944
Batch 41/64 loss: -0.04502558708190918
Batch 42/64 loss: -0.07285523414611816
Batch 43/64 loss: -0.05397367477416992
Batch 44/64 loss: -0.05127251148223877
Batch 45/64 loss: -0.05993390083312988
Batch 46/64 loss: -0.08398574590682983
Batch 47/64 loss: -0.06514298915863037
Batch 48/64 loss: -0.04573321342468262
Batch 49/64 loss: -0.08382916450500488
Batch 50/64 loss: -0.07396245002746582
Batch 51/64 loss: -0.10031652450561523
Batch 52/64 loss: -0.07518231868743896
Batch 53/64 loss: -0.04641908407211304
Batch 54/64 loss: -0.08907145261764526
Batch 55/64 loss: -0.0692434310913086
Batch 56/64 loss: -0.08655595779418945
Batch 57/64 loss: -0.09631466865539551
Batch 58/64 loss: -0.062166571617126465
Batch 59/64 loss: -0.06615447998046875
Batch 60/64 loss: -0.07297080755233765
Batch 61/64 loss: -0.07621431350708008
Batch 62/64 loss: -0.07137072086334229
Batch 63/64 loss: -0.06504976749420166
Batch 64/64 loss: -0.0659404993057251
Epoch 67  Train loss: -0.0688924186369952  Val loss: 0.01777017177994718
Saving best model, epoch: 67
Epoch 68
-------------------------------
Batch 1/64 loss: -0.0809788703918457
Batch 2/64 loss: -0.06360256671905518
Batch 3/64 loss: -0.061430931091308594
Batch 4/64 loss: -0.07685196399688721
Batch 5/64 loss: -0.08901345729827881
Batch 6/64 loss: -0.07013648748397827
Batch 7/64 loss: -0.08578038215637207
Batch 8/64 loss: -0.09770756959915161
Batch 9/64 loss: -0.07463139295578003
Batch 10/64 loss: -0.09956526756286621
Batch 11/64 loss: -0.06650447845458984
Batch 12/64 loss: -0.07909971475601196
Batch 13/64 loss: -0.08785563707351685
Batch 14/64 loss: -0.06728494167327881
Batch 15/64 loss: -0.07676458358764648
Batch 16/64 loss: -0.09671944379806519
Batch 17/64 loss: -0.07929646968841553
Batch 18/64 loss: -0.08240765333175659
Batch 19/64 loss: -0.06570887565612793
Batch 20/64 loss: -0.08897137641906738
Batch 21/64 loss: -0.057312965393066406
Batch 22/64 loss: -0.06224024295806885
Batch 23/64 loss: -0.07907599210739136
Batch 24/64 loss: -0.057268619537353516
Batch 25/64 loss: -0.04953920841217041
Batch 26/64 loss: -0.0610845685005188
Batch 27/64 loss: -0.031183838844299316
Batch 28/64 loss: -0.07722532749176025
Batch 29/64 loss: -0.08747535943984985
Batch 30/64 loss: -0.061013638973236084
Batch 31/64 loss: -0.08016818761825562
Batch 32/64 loss: -0.07616406679153442
Batch 33/64 loss: -0.06303775310516357
Batch 34/64 loss: -0.07748883962631226
Batch 35/64 loss: -0.0670812726020813
Batch 36/64 loss: -0.0579150915145874
Batch 37/64 loss: -0.0577356219291687
Batch 38/64 loss: -0.06629222631454468
Batch 39/64 loss: -0.08920186758041382
Batch 40/64 loss: -0.05820012092590332
Batch 41/64 loss: -0.07453691959381104
Batch 42/64 loss: -0.0685303807258606
Batch 43/64 loss: -0.07750773429870605
Batch 44/64 loss: -0.0676606297492981
Batch 45/64 loss: -0.056758999824523926
Batch 46/64 loss: -0.08449876308441162
Batch 47/64 loss: -0.08346438407897949
Batch 48/64 loss: -0.09286844730377197
Batch 49/64 loss: -0.0806235671043396
Batch 50/64 loss: -0.08765637874603271
Batch 51/64 loss: -0.05388331413269043
Batch 52/64 loss: -0.06862097978591919
Batch 53/64 loss: -0.06812673807144165
Batch 54/64 loss: -0.08157467842102051
Batch 55/64 loss: -0.07968556880950928
Batch 56/64 loss: -0.07978534698486328
Batch 57/64 loss: -0.06392663717269897
Batch 58/64 loss: -0.07482105493545532
Batch 59/64 loss: -0.060953497886657715
Batch 60/64 loss: -0.028295397758483887
Batch 61/64 loss: -0.06788432598114014
Batch 62/64 loss: -0.07155400514602661
Batch 63/64 loss: -0.0382809042930603
Batch 64/64 loss: -0.042725563049316406
Epoch 68  Train loss: -0.07138132862016267  Val loss: 0.023171369767270956
Epoch 69
-------------------------------
Batch 1/64 loss: -0.08927243947982788
Batch 2/64 loss: -0.06338930130004883
Batch 3/64 loss: -0.0882195234298706
Batch 4/64 loss: -0.0793309211730957
Batch 5/64 loss: -0.09804892539978027
Batch 6/64 loss: -0.09156626462936401
Batch 7/64 loss: -0.08527630567550659
Batch 8/64 loss: -0.03225851058959961
Batch 9/64 loss: -0.07477730512619019
Batch 10/64 loss: -0.08699584007263184
Batch 11/64 loss: -0.0782155990600586
Batch 12/64 loss: -0.0873798131942749
Batch 13/64 loss: -0.08218878507614136
Batch 14/64 loss: -0.07522302865982056
Batch 15/64 loss: -0.06422007083892822
Batch 16/64 loss: -0.06596815586090088
Batch 17/64 loss: -0.027449369430541992
Batch 18/64 loss: -0.0531611442565918
Batch 19/64 loss: -0.0762435793876648
Batch 20/64 loss: -0.058363378047943115
Batch 21/64 loss: -0.05045372247695923
Batch 22/64 loss: -0.06238824129104614
Batch 23/64 loss: -0.07212352752685547
Batch 24/64 loss: -0.07749897241592407
Batch 25/64 loss: -0.05526471138000488
Batch 26/64 loss: -0.06444871425628662
Batch 27/64 loss: -0.06525826454162598
Batch 28/64 loss: -0.051229774951934814
Batch 29/64 loss: -0.050296664237976074
Batch 30/64 loss: -0.07015872001647949
Batch 31/64 loss: -0.07757949829101562
Batch 32/64 loss: -0.07059997320175171
Batch 33/64 loss: -0.05410265922546387
Batch 34/64 loss: -0.07239407300949097
Batch 35/64 loss: -0.04949373006820679
Batch 36/64 loss: -0.07590115070343018
Batch 37/64 loss: -0.06040471792221069
Batch 38/64 loss: -0.079781174659729
Batch 39/64 loss: -0.06991684436798096
Batch 40/64 loss: -0.08207488059997559
Batch 41/64 loss: -0.09092354774475098
Batch 42/64 loss: -0.060690224170684814
Batch 43/64 loss: -0.0737999677658081
Batch 44/64 loss: -0.08119368553161621
Batch 45/64 loss: -0.07495927810668945
Batch 46/64 loss: -0.07348138093948364
Batch 47/64 loss: -0.08026999235153198
Batch 48/64 loss: -0.05351299047470093
Batch 49/64 loss: -0.07801753282546997
Batch 50/64 loss: -0.07512736320495605
Batch 51/64 loss: -0.08449572324752808
Batch 52/64 loss: -0.03917783498764038
Batch 53/64 loss: -0.06851977109909058
Batch 54/64 loss: -0.07358574867248535
Batch 55/64 loss: -0.07444441318511963
Batch 56/64 loss: -0.08044558763504028
Batch 57/64 loss: -0.0893632173538208
Batch 58/64 loss: -0.10574615001678467
Batch 59/64 loss: -0.07058912515640259
Batch 60/64 loss: -0.04813659191131592
Batch 61/64 loss: -0.061565935611724854
Batch 62/64 loss: -0.05359166860580444
Batch 63/64 loss: -0.08772385120391846
Batch 64/64 loss: -0.0834624171257019
Epoch 69  Train loss: -0.07075884973301608  Val loss: 0.01753109980284963
Saving best model, epoch: 69
Epoch 70
-------------------------------
Batch 1/64 loss: -0.09149980545043945
Batch 2/64 loss: -0.06778419017791748
Batch 3/64 loss: -0.0327378511428833
Batch 4/64 loss: -0.08519721031188965
Batch 5/64 loss: -0.09853839874267578
Batch 6/64 loss: -0.09112739562988281
Batch 7/64 loss: -0.06750369071960449
Batch 8/64 loss: -0.08886778354644775
Batch 9/64 loss: -0.08657598495483398
Batch 10/64 loss: -0.08271265029907227
Batch 11/64 loss: -0.09505194425582886
Batch 12/64 loss: -0.07311546802520752
Batch 13/64 loss: -0.07244253158569336
Batch 14/64 loss: -0.07123738527297974
Batch 15/64 loss: -0.07010573148727417
Batch 16/64 loss: -0.07181018590927124
Batch 17/64 loss: -0.07989424467086792
Batch 18/64 loss: -0.07349717617034912
Batch 19/64 loss: -0.054758310317993164
Batch 20/64 loss: -0.041385531425476074
Batch 21/64 loss: -0.06155651807785034
Batch 22/64 loss: -0.07158207893371582
Batch 23/64 loss: -0.09169584512710571
Batch 24/64 loss: -0.10450667142868042
Batch 25/64 loss: -0.09052771329879761
Batch 26/64 loss: -0.06465274095535278
Batch 27/64 loss: -0.07009142637252808
Batch 28/64 loss: -0.0731191635131836
Batch 29/64 loss: -0.08411478996276855
Batch 30/64 loss: -0.06396478414535522
Batch 31/64 loss: -0.024717271327972412
Batch 32/64 loss: -0.07715725898742676
Batch 33/64 loss: -0.05774271488189697
Batch 34/64 loss: -0.07571756839752197
Batch 35/64 loss: -0.06678402423858643
Batch 36/64 loss: -0.07941323518753052
Batch 37/64 loss: -0.06958860158920288
Batch 38/64 loss: -0.06606513261795044
Batch 39/64 loss: -0.09076488018035889
Batch 40/64 loss: -0.09029102325439453
Batch 41/64 loss: -0.09214013814926147
Batch 42/64 loss: -0.0700521469116211
Batch 43/64 loss: -0.06573837995529175
Batch 44/64 loss: -0.10599952936172485
Batch 45/64 loss: -0.04583871364593506
Batch 46/64 loss: -0.04127800464630127
Batch 47/64 loss: -0.06693673133850098
Batch 48/64 loss: -0.06495976448059082
Batch 49/64 loss: -0.09695476293563843
Batch 50/64 loss: -0.0847126841545105
Batch 51/64 loss: -0.08220535516738892
Batch 52/64 loss: -0.08139902353286743
Batch 53/64 loss: -0.03901851177215576
Batch 54/64 loss: -0.11099356412887573
Batch 55/64 loss: -0.06574827432632446
Batch 56/64 loss: -0.04880821704864502
Batch 57/64 loss: -0.0977022647857666
Batch 58/64 loss: -0.06742560863494873
Batch 59/64 loss: -0.04863077402114868
Batch 60/64 loss: -0.0723767876625061
Batch 61/64 loss: -0.0695069432258606
Batch 62/64 loss: -0.065548837184906
Batch 63/64 loss: -0.07411062717437744
Batch 64/64 loss: -0.07796746492385864
Epoch 70  Train loss: -0.0735130691060833  Val loss: 0.019249613752070162
Epoch 71
-------------------------------
Batch 1/64 loss: -0.10908102989196777
Batch 2/64 loss: -0.08392173051834106
Batch 3/64 loss: -0.09620708227157593
Batch 4/64 loss: -0.06451338529586792
Batch 5/64 loss: -0.06273603439331055
Batch 6/64 loss: -0.10841166973114014
Batch 7/64 loss: -0.09920328855514526
Batch 8/64 loss: -0.075847327709198
Batch 9/64 loss: -0.08331918716430664
Batch 10/64 loss: -0.0814657211303711
Batch 11/64 loss: -0.07349634170532227
Batch 12/64 loss: -0.06476867198944092
Batch 13/64 loss: -0.05953264236450195
Batch 14/64 loss: -0.07182431221008301
Batch 15/64 loss: -0.05393564701080322
Batch 16/64 loss: -0.09557223320007324
Batch 17/64 loss: -0.07478594779968262
Batch 18/64 loss: -0.09352409839630127
Batch 19/64 loss: -0.08255648612976074
Batch 20/64 loss: -0.08894157409667969
Batch 21/64 loss: -0.07470285892486572
Batch 22/64 loss: -0.08444499969482422
Batch 23/64 loss: -0.0861998200416565
Batch 24/64 loss: -0.06518346071243286
Batch 25/64 loss: -0.06374388933181763
Batch 26/64 loss: -0.09482109546661377
Batch 27/64 loss: -0.0524255633354187
Batch 28/64 loss: -0.06583166122436523
Batch 29/64 loss: -0.09689199924468994
Batch 30/64 loss: -0.08668756484985352
Batch 31/64 loss: -0.07525181770324707
Batch 32/64 loss: -0.08948975801467896
Batch 33/64 loss: -0.04867285490036011
Batch 34/64 loss: -0.05544281005859375
Batch 35/64 loss: -0.08658146858215332
Batch 36/64 loss: -0.07940107583999634
Batch 37/64 loss: -0.05344593524932861
Batch 38/64 loss: -0.06356382369995117
Batch 39/64 loss: -0.06451034545898438
Batch 40/64 loss: -0.056383490562438965
Batch 41/64 loss: -0.07088518142700195
Batch 42/64 loss: -0.07333481311798096
Batch 43/64 loss: -0.08619248867034912
Batch 44/64 loss: -0.08620846271514893
Batch 45/64 loss: -0.060426950454711914
Batch 46/64 loss: -0.05074948072433472
Batch 47/64 loss: -0.0725141167640686
Batch 48/64 loss: -0.05338186025619507
Batch 49/64 loss: -0.0728653073310852
Batch 50/64 loss: -0.06130790710449219
Batch 51/64 loss: -0.0577319860458374
Batch 52/64 loss: -0.0842241644859314
Batch 53/64 loss: -0.07542896270751953
Batch 54/64 loss: -0.051108717918395996
Batch 55/64 loss: -0.08017480373382568
Batch 56/64 loss: -0.06115853786468506
Batch 57/64 loss: -0.054752349853515625
Batch 58/64 loss: -0.08176010847091675
Batch 59/64 loss: -0.07812660932540894
Batch 60/64 loss: -0.09228390455245972
Batch 61/64 loss: -0.050635337829589844
Batch 62/64 loss: -0.0900687575340271
Batch 63/64 loss: -0.08057177066802979
Batch 64/64 loss: -0.08959650993347168
Epoch 71  Train loss: -0.07467310849358054  Val loss: 0.017564674628149604
Epoch 72
-------------------------------
Batch 1/64 loss: -0.09153836965560913
Batch 2/64 loss: -0.07105076313018799
Batch 3/64 loss: -0.07415515184402466
Batch 4/64 loss: -0.06055575609207153
Batch 5/64 loss: -0.04320359230041504
Batch 6/64 loss: -0.0887792706489563
Batch 7/64 loss: -0.07545650005340576
Batch 8/64 loss: -0.10202932357788086
Batch 9/64 loss: -0.09770750999450684
Batch 10/64 loss: -0.0875888466835022
Batch 11/64 loss: -0.10391831398010254
Batch 12/64 loss: -0.09431761503219604
Batch 13/64 loss: -0.05828827619552612
Batch 14/64 loss: -0.09187036752700806
Batch 15/64 loss: -0.08745235204696655
Batch 16/64 loss: -0.06540250778198242
Batch 17/64 loss: -0.0861251950263977
Batch 18/64 loss: -0.045859456062316895
Batch 19/64 loss: -0.083060622215271
Batch 20/64 loss: -0.07937628030776978
Batch 21/64 loss: -0.09605038166046143
Batch 22/64 loss: -0.08886981010437012
Batch 23/64 loss: -0.07960450649261475
Batch 24/64 loss: -0.0630301833152771
Batch 25/64 loss: -0.07358938455581665
Batch 26/64 loss: -0.08245646953582764
Batch 27/64 loss: -0.06782013177871704
Batch 28/64 loss: -0.07590556144714355
Batch 29/64 loss: -0.070148766040802
Batch 30/64 loss: -0.08700358867645264
Batch 31/64 loss: -0.05518746376037598
Batch 32/64 loss: -0.0871589183807373
Batch 33/64 loss: -0.05465102195739746
Batch 34/64 loss: -0.08706629276275635
Batch 35/64 loss: -0.07565200328826904
Batch 36/64 loss: -0.0665673017501831
Batch 37/64 loss: -0.08962798118591309
Batch 38/64 loss: -0.05087685585021973
Batch 39/64 loss: -0.06005924940109253
Batch 40/64 loss: -0.07817482948303223
Batch 41/64 loss: -0.06522953510284424
Batch 42/64 loss: -0.06533098220825195
Batch 43/64 loss: -0.06978517770767212
Batch 44/64 loss: -0.061732590198516846
Batch 45/64 loss: -0.07445275783538818
Batch 46/64 loss: -0.052582621574401855
Batch 47/64 loss: -0.07828962802886963
Batch 48/64 loss: -0.0792880654335022
Batch 49/64 loss: -0.07377326488494873
Batch 50/64 loss: -0.08262252807617188
Batch 51/64 loss: -0.057985782623291016
Batch 52/64 loss: -0.0926465392112732
Batch 53/64 loss: -0.08125561475753784
Batch 54/64 loss: -0.09710776805877686
Batch 55/64 loss: -0.08388793468475342
Batch 56/64 loss: -0.07847386598587036
Batch 57/64 loss: -0.0829724669456482
Batch 58/64 loss: -0.07153189182281494
Batch 59/64 loss: -0.07951390743255615
Batch 60/64 loss: -0.08056116104125977
Batch 61/64 loss: -0.07064127922058105
Batch 62/64 loss: -0.08563858270645142
Batch 63/64 loss: -0.06717890501022339
Batch 64/64 loss: -0.09627020359039307
Epoch 72  Train loss: -0.07657917293847775  Val loss: 0.017300517493506886
Saving best model, epoch: 72
Epoch 73
-------------------------------
Batch 1/64 loss: -0.08238881826400757
Batch 2/64 loss: -0.09387505054473877
Batch 3/64 loss: -0.06139659881591797
Batch 4/64 loss: -0.07690775394439697
Batch 5/64 loss: -0.09502798318862915
Batch 6/64 loss: -0.09306007623672485
Batch 7/64 loss: -0.07617449760437012
Batch 8/64 loss: -0.05833423137664795
Batch 9/64 loss: -0.06436467170715332
Batch 10/64 loss: -0.07328426837921143
Batch 11/64 loss: -0.08949661254882812
Batch 12/64 loss: -0.09201449155807495
Batch 13/64 loss: -0.10998731851577759
Batch 14/64 loss: -0.0790109634399414
Batch 15/64 loss: -0.0468602180480957
Batch 16/64 loss: -0.09277600049972534
Batch 17/64 loss: -0.08827126026153564
Batch 18/64 loss: -0.05955052375793457
Batch 19/64 loss: -0.08543163537979126
Batch 20/64 loss: -0.06891077756881714
Batch 21/64 loss: -0.10884809494018555
Batch 22/64 loss: -0.0951465368270874
Batch 23/64 loss: -0.08007287979125977
Batch 24/64 loss: -0.06165623664855957
Batch 25/64 loss: -0.08120673894882202
Batch 26/64 loss: -0.09409129619598389
Batch 27/64 loss: -0.10107946395874023
Batch 28/64 loss: -0.06197857856750488
Batch 29/64 loss: -0.07514911890029907
Batch 30/64 loss: -0.06117093563079834
Batch 31/64 loss: -0.08276021480560303
Batch 32/64 loss: -0.04556739330291748
Batch 33/64 loss: -0.08160161972045898
Batch 34/64 loss: -0.0999823808670044
Batch 35/64 loss: -0.08461511135101318
Batch 36/64 loss: -0.101565420627594
Batch 37/64 loss: -0.09393805265426636
Batch 38/64 loss: -0.07725799083709717
Batch 39/64 loss: -0.0682559609413147
Batch 40/64 loss: -0.07368242740631104
Batch 41/64 loss: -0.10486507415771484
Batch 42/64 loss: -0.08742296695709229
Batch 43/64 loss: -0.08461374044418335
Batch 44/64 loss: -0.09819686412811279
Batch 45/64 loss: -0.08953720331192017
Batch 46/64 loss: -0.09326601028442383
Batch 47/64 loss: -0.058422982692718506
Batch 48/64 loss: -0.10360527038574219
Batch 49/64 loss: -0.08774447441101074
Batch 50/64 loss: -0.09225094318389893
Batch 51/64 loss: -0.0504685640335083
Batch 52/64 loss: -0.08301359415054321
Batch 53/64 loss: -0.06697314977645874
Batch 54/64 loss: -0.0934680700302124
Batch 55/64 loss: -0.09847968816757202
Batch 56/64 loss: -0.058407485485076904
Batch 57/64 loss: -0.06878525018692017
Batch 58/64 loss: -0.028927505016326904
Batch 59/64 loss: -0.0782080888748169
Batch 60/64 loss: -0.07424694299697876
Batch 61/64 loss: -0.08667778968811035
Batch 62/64 loss: -0.08999836444854736
Batch 63/64 loss: -0.05476856231689453
Batch 64/64 loss: -0.07530885934829712
Epoch 73  Train loss: -0.08008753622279448  Val loss: 0.015688057412806246
Saving best model, epoch: 73
Epoch 74
-------------------------------
Batch 1/64 loss: -0.10614830255508423
Batch 2/64 loss: -0.08351939916610718
Batch 3/64 loss: -0.05884408950805664
Batch 4/64 loss: -0.068156898021698
Batch 5/64 loss: -0.10192829370498657
Batch 6/64 loss: -0.10663974285125732
Batch 7/64 loss: -0.08626121282577515
Batch 8/64 loss: -0.06039923429489136
Batch 9/64 loss: -0.10728806257247925
Batch 10/64 loss: -0.11445528268814087
Batch 11/64 loss: -0.06651222705841064
Batch 12/64 loss: -0.08226257562637329
Batch 13/64 loss: -0.09759396314620972
Batch 14/64 loss: -0.07852047681808472
Batch 15/64 loss: -0.10487741231918335
Batch 16/64 loss: -0.10382187366485596
Batch 17/64 loss: -0.09809553623199463
Batch 18/64 loss: -0.08633559942245483
Batch 19/64 loss: -0.0900353193283081
Batch 20/64 loss: -0.050981342792510986
Batch 21/64 loss: -0.0917205810546875
Batch 22/64 loss: -0.09316813945770264
Batch 23/64 loss: -0.06398284435272217
Batch 24/64 loss: -0.07852834463119507
Batch 25/64 loss: -0.07398062944412231
Batch 26/64 loss: -0.08868122100830078
Batch 27/64 loss: -0.09899306297302246
Batch 28/64 loss: -0.07804083824157715
Batch 29/64 loss: -0.075600266456604
Batch 30/64 loss: -0.08717191219329834
Batch 31/64 loss: -0.08889985084533691
Batch 32/64 loss: -0.0645456314086914
Batch 33/64 loss: -0.07700204849243164
Batch 34/64 loss: -0.08099114894866943
Batch 35/64 loss: -0.09473270177841187
Batch 36/64 loss: -0.09785759449005127
Batch 37/64 loss: -0.04894644021987915
Batch 38/64 loss: -0.07444334030151367
Batch 39/64 loss: -0.0698668360710144
Batch 40/64 loss: -0.08755004405975342
Batch 41/64 loss: -0.08621597290039062
Batch 42/64 loss: -0.10179942846298218
Batch 43/64 loss: -0.05876797437667847
Batch 44/64 loss: -0.09023600816726685
Batch 45/64 loss: -0.09809088706970215
Batch 46/64 loss: -0.07632124423980713
Batch 47/64 loss: -0.11074167490005493
Batch 48/64 loss: -0.08810281753540039
Batch 49/64 loss: -0.05850672721862793
Batch 50/64 loss: -0.09313303232192993
Batch 51/64 loss: -0.08073079586029053
Batch 52/64 loss: -0.07473766803741455
Batch 53/64 loss: -0.08659428358078003
Batch 54/64 loss: -0.10026049613952637
Batch 55/64 loss: -0.05926394462585449
Batch 56/64 loss: -0.05734211206436157
Batch 57/64 loss: -0.07186681032180786
Batch 58/64 loss: -0.08936971426010132
Batch 59/64 loss: -0.0785704255104065
Batch 60/64 loss: -0.04249173402786255
Batch 61/64 loss: -0.08507120609283447
Batch 62/64 loss: -0.08546161651611328
Batch 63/64 loss: -0.09207165241241455
Batch 64/64 loss: -0.10388481616973877
Epoch 74  Train loss: -0.08331046525169822  Val loss: 0.017718556820322147
Epoch 75
-------------------------------
Batch 1/64 loss: -0.07447183132171631
Batch 2/64 loss: -0.09983110427856445
Batch 3/64 loss: -0.10234904289245605
Batch 4/64 loss: -0.09623163938522339
Batch 5/64 loss: -0.10760807991027832
Batch 6/64 loss: -0.1005328893661499
Batch 7/64 loss: -0.06452345848083496
Batch 8/64 loss: -0.09141302108764648
Batch 9/64 loss: -0.11306631565093994
Batch 10/64 loss: -0.07952910661697388
Batch 11/64 loss: -0.07685071229934692
Batch 12/64 loss: -0.09542757272720337
Batch 13/64 loss: -0.07605952024459839
Batch 14/64 loss: -0.09974312782287598
Batch 15/64 loss: -0.07290136814117432
Batch 16/64 loss: -0.06973797082901001
Batch 17/64 loss: -0.09805411100387573
Batch 18/64 loss: -0.11379504203796387
Batch 19/64 loss: -0.08418750762939453
Batch 20/64 loss: -0.10627847909927368
Batch 21/64 loss: -0.098990797996521
Batch 22/64 loss: -0.09374195337295532
Batch 23/64 loss: -0.07266664505004883
Batch 24/64 loss: -0.07538992166519165
Batch 25/64 loss: -0.08752644062042236
Batch 26/64 loss: -0.0691947340965271
Batch 27/64 loss: -0.10875964164733887
Batch 28/64 loss: -0.07994139194488525
Batch 29/64 loss: -0.08236074447631836
Batch 30/64 loss: -0.07922446727752686
Batch 31/64 loss: -0.07236093282699585
Batch 32/64 loss: -0.0831913948059082
Batch 33/64 loss: -0.07728362083435059
Batch 34/64 loss: -0.09370666742324829
Batch 35/64 loss: -0.07731115818023682
Batch 36/64 loss: -0.04045218229293823
Batch 37/64 loss: -0.09272915124893188
Batch 38/64 loss: -0.09036809206008911
Batch 39/64 loss: -0.06511324644088745
Batch 40/64 loss: -0.08404111862182617
Batch 41/64 loss: -0.07654041051864624
Batch 42/64 loss: -0.04510730504989624
Batch 43/64 loss: -0.06478792428970337
Batch 44/64 loss: -0.07853370904922485
Batch 45/64 loss: -0.09564679861068726
Batch 46/64 loss: -0.0985192060470581
Batch 47/64 loss: -0.07155072689056396
Batch 48/64 loss: -0.09268957376480103
Batch 49/64 loss: -0.09706157445907593
Batch 50/64 loss: -0.11515051126480103
Batch 51/64 loss: -0.10478091239929199
Batch 52/64 loss: -0.04426610469818115
Batch 53/64 loss: -0.07975238561630249
Batch 54/64 loss: -0.07189923524856567
Batch 55/64 loss: -0.10606777667999268
Batch 56/64 loss: -0.08163118362426758
Batch 57/64 loss: -0.08341860771179199
Batch 58/64 loss: -0.0675508975982666
Batch 59/64 loss: -0.107399582862854
Batch 60/64 loss: -0.08015340566635132
Batch 61/64 loss: -0.07746446132659912
Batch 62/64 loss: -0.04600644111633301
Batch 63/64 loss: -0.08041393756866455
Batch 64/64 loss: -0.0411648154258728
Epoch 75  Train loss: -0.08383078411513684  Val loss: 0.02115628686557521
Epoch 76
-------------------------------
Batch 1/64 loss: -0.05440801382064819
Batch 2/64 loss: -0.08537715673446655
Batch 3/64 loss: -0.06925731897354126
Batch 4/64 loss: -0.0941624641418457
Batch 5/64 loss: -0.10213297605514526
Batch 6/64 loss: -0.10428464412689209
Batch 7/64 loss: -0.07087850570678711
Batch 8/64 loss: -0.1103132963180542
Batch 9/64 loss: -0.09303569793701172
Batch 10/64 loss: -0.06786131858825684
Batch 11/64 loss: -0.0980910062789917
Batch 12/64 loss: -0.09641683101654053
Batch 13/64 loss: -0.10356742143630981
Batch 14/64 loss: -0.09588700532913208
Batch 15/64 loss: -0.0993727445602417
Batch 16/64 loss: -0.06984806060791016
Batch 17/64 loss: -0.11536562442779541
Batch 18/64 loss: -0.08076494932174683
Batch 19/64 loss: -0.09138292074203491
Batch 20/64 loss: -0.0679367184638977
Batch 21/64 loss: -0.0962151288986206
Batch 22/64 loss: -0.08483070135116577
Batch 23/64 loss: -0.0801471471786499
Batch 24/64 loss: -0.09015357494354248
Batch 25/64 loss: -0.0732581615447998
Batch 26/64 loss: -0.09077155590057373
Batch 27/64 loss: -0.07788801193237305
Batch 28/64 loss: -0.07190215587615967
Batch 29/64 loss: -0.09308946132659912
Batch 30/64 loss: -0.11653900146484375
Batch 31/64 loss: -0.07934266328811646
Batch 32/64 loss: -0.05268442630767822
Batch 33/64 loss: -0.09097665548324585
Batch 34/64 loss: -0.08301258087158203
Batch 35/64 loss: -0.08489370346069336
Batch 36/64 loss: -0.07782667875289917
Batch 37/64 loss: -0.10956299304962158
Batch 38/64 loss: -0.09956759214401245
Batch 39/64 loss: -0.08584386110305786
Batch 40/64 loss: -0.0877225399017334
Batch 41/64 loss: -0.11046761274337769
Batch 42/64 loss: -0.06821948289871216
Batch 43/64 loss: -0.0812787413597107
Batch 44/64 loss: -0.107532799243927
Batch 45/64 loss: -0.07491481304168701
Batch 46/64 loss: -0.10195446014404297
Batch 47/64 loss: -0.08105003833770752
Batch 48/64 loss: -0.07893538475036621
Batch 49/64 loss: -0.09226953983306885
Batch 50/64 loss: -0.09495663642883301
Batch 51/64 loss: -0.08217942714691162
Batch 52/64 loss: -0.09442853927612305
Batch 53/64 loss: -0.09521478414535522
Batch 54/64 loss: -0.09398305416107178
Batch 55/64 loss: -0.07491433620452881
Batch 56/64 loss: -0.08804869651794434
Batch 57/64 loss: -0.08407914638519287
Batch 58/64 loss: -0.07155364751815796
Batch 59/64 loss: -0.058164000511169434
Batch 60/64 loss: -0.08377701044082642
Batch 61/64 loss: -0.10174137353897095
Batch 62/64 loss: -0.08241891860961914
Batch 63/64 loss: -0.07104021310806274
Batch 64/64 loss: -0.08590888977050781
Epoch 76  Train loss: -0.0868098445967132  Val loss: 0.01730361140470734
Epoch 77
-------------------------------
Batch 1/64 loss: -0.09207630157470703
Batch 2/64 loss: -0.09918224811553955
Batch 3/64 loss: -0.10044491291046143
Batch 4/64 loss: -0.07926088571548462
Batch 5/64 loss: -0.07948797941207886
Batch 6/64 loss: -0.11700981855392456
Batch 7/64 loss: -0.10630899667739868
Batch 8/64 loss: -0.10042941570281982
Batch 9/64 loss: -0.11088991165161133
Batch 10/64 loss: -0.08078622817993164
Batch 11/64 loss: -0.10689711570739746
Batch 12/64 loss: -0.09737157821655273
Batch 13/64 loss: -0.11338943243026733
Batch 14/64 loss: -0.09594190120697021
Batch 15/64 loss: -0.09734112024307251
Batch 16/64 loss: -0.10183125734329224
Batch 17/64 loss: -0.07957130670547485
Batch 18/64 loss: -0.0907706618309021
Batch 19/64 loss: -0.11034399271011353
Batch 20/64 loss: -0.10118037462234497
Batch 21/64 loss: -0.08023566007614136
Batch 22/64 loss: -0.10827082395553589
Batch 23/64 loss: -0.07985258102416992
Batch 24/64 loss: -0.10620129108428955
Batch 25/64 loss: -0.10576152801513672
Batch 26/64 loss: -0.09220349788665771
Batch 27/64 loss: -0.10104185342788696
Batch 28/64 loss: -0.08791470527648926
Batch 29/64 loss: -0.11345052719116211
Batch 30/64 loss: -0.06343120336532593
Batch 31/64 loss: -0.09639644622802734
Batch 32/64 loss: -0.10500890016555786
Batch 33/64 loss: -0.06041938066482544
Batch 34/64 loss: -0.08508104085922241
Batch 35/64 loss: -0.10041183233261108
Batch 36/64 loss: -0.09407705068588257
Batch 37/64 loss: -0.09478771686553955
Batch 38/64 loss: -0.07349973917007446
Batch 39/64 loss: -0.05372554063796997
Batch 40/64 loss: -0.07436525821685791
Batch 41/64 loss: -0.05318725109100342
Batch 42/64 loss: -0.08531641960144043
Batch 43/64 loss: -0.08340787887573242
Batch 44/64 loss: -0.08847415447235107
Batch 45/64 loss: -0.05182039737701416
Batch 46/64 loss: -0.06982302665710449
Batch 47/64 loss: -0.04476529359817505
Batch 48/64 loss: -0.08514738082885742
Batch 49/64 loss: -0.05920076370239258
Batch 50/64 loss: -0.08954799175262451
Batch 51/64 loss: -0.09988188743591309
Batch 52/64 loss: -0.06650924682617188
Batch 53/64 loss: -0.07522660493850708
Batch 54/64 loss: -0.0759134292602539
Batch 55/64 loss: -0.10022276639938354
Batch 56/64 loss: -0.09870535135269165
Batch 57/64 loss: -0.07917064428329468
Batch 58/64 loss: -0.0808253288269043
Batch 59/64 loss: -0.07286763191223145
Batch 60/64 loss: -0.06052219867706299
Batch 61/64 loss: -0.09543514251708984
Batch 62/64 loss: -0.07089340686798096
Batch 63/64 loss: -0.06782740354537964
Batch 64/64 loss: -0.10930502414703369
Epoch 77  Train loss: -0.08742466487136541  Val loss: 0.01739092299209018
Epoch 78
-------------------------------
Batch 1/64 loss: -0.08320897817611694
Batch 2/64 loss: -0.08428668975830078
Batch 3/64 loss: -0.07519650459289551
Batch 4/64 loss: -0.11946845054626465
Batch 5/64 loss: -0.08323538303375244
Batch 6/64 loss: -0.11036539077758789
Batch 7/64 loss: -0.11794722080230713
Batch 8/64 loss: -0.07952433824539185
Batch 9/64 loss: -0.088570237159729
Batch 10/64 loss: -0.08223634958267212
Batch 11/64 loss: -0.10117751359939575
Batch 12/64 loss: -0.10045427083969116
Batch 13/64 loss: -0.09182816743850708
Batch 14/64 loss: -0.09862840175628662
Batch 15/64 loss: -0.09731513261795044
Batch 16/64 loss: -0.08899807929992676
Batch 17/64 loss: -0.08153307437896729
Batch 18/64 loss: -0.09120184183120728
Batch 19/64 loss: -0.09019136428833008
Batch 20/64 loss: -0.0814390778541565
Batch 21/64 loss: -0.08858007192611694
Batch 22/64 loss: -0.09900814294815063
Batch 23/64 loss: -0.05469679832458496
Batch 24/64 loss: -0.11628031730651855
Batch 25/64 loss: -0.1094399094581604
Batch 26/64 loss: -0.0827757716178894
Batch 27/64 loss: -0.10844606161117554
Batch 28/64 loss: -0.04878854751586914
Batch 29/64 loss: -0.10445666313171387
Batch 30/64 loss: -0.0794229507446289
Batch 31/64 loss: -0.07327032089233398
Batch 32/64 loss: -0.10955911874771118
Batch 33/64 loss: -0.07270067930221558
Batch 34/64 loss: -0.07701146602630615
Batch 35/64 loss: -0.08922171592712402
Batch 36/64 loss: -0.0959324836730957
Batch 37/64 loss: -0.09909427165985107
Batch 38/64 loss: -0.10155141353607178
Batch 39/64 loss: -0.0694156289100647
Batch 40/64 loss: -0.10407829284667969
Batch 41/64 loss: -0.10022825002670288
Batch 42/64 loss: -0.10007601976394653
Batch 43/64 loss: -0.07119709253311157
Batch 44/64 loss: -0.09058165550231934
Batch 45/64 loss: -0.10421973466873169
Batch 46/64 loss: -0.0832359790802002
Batch 47/64 loss: -0.09421181678771973
Batch 48/64 loss: -0.1035659909248352
Batch 49/64 loss: -0.1045159101486206
Batch 50/64 loss: -0.09028631448745728
Batch 51/64 loss: -0.06703120470046997
Batch 52/64 loss: -0.09193670749664307
Batch 53/64 loss: -0.07490432262420654
Batch 54/64 loss: -0.07762748003005981
Batch 55/64 loss: -0.09727156162261963
Batch 56/64 loss: -0.09277176856994629
Batch 57/64 loss: -0.07377749681472778
Batch 58/64 loss: -0.05563008785247803
Batch 59/64 loss: -0.09542477130889893
Batch 60/64 loss: -0.09269767999649048
Batch 61/64 loss: -0.06871950626373291
Batch 62/64 loss: -0.10408180952072144
Batch 63/64 loss: -0.10557389259338379
Batch 64/64 loss: -0.1143532395362854
Epoch 78  Train loss: -0.09028814236323039  Val loss: 0.018445054485216175
Epoch 79
-------------------------------
Batch 1/64 loss: -0.06970101594924927
Batch 2/64 loss: -0.10111850500106812
Batch 3/64 loss: -0.10667037963867188
Batch 4/64 loss: -0.11351710557937622
Batch 5/64 loss: -0.12122505903244019
Batch 6/64 loss: -0.10732954740524292
Batch 7/64 loss: -0.0852048397064209
Batch 8/64 loss: -0.07352185249328613
Batch 9/64 loss: -0.11735641956329346
Batch 10/64 loss: -0.10498487949371338
Batch 11/64 loss: -0.1003183126449585
Batch 12/64 loss: -0.07806211709976196
Batch 13/64 loss: -0.10718244314193726
Batch 14/64 loss: -0.10100120306015015
Batch 15/64 loss: -0.08906453847885132
Batch 16/64 loss: -0.10594499111175537
Batch 17/64 loss: -0.10322082042694092
Batch 18/64 loss: -0.10592281818389893
Batch 19/64 loss: -0.11463749408721924
Batch 20/64 loss: -0.10005688667297363
Batch 21/64 loss: -0.08640128374099731
Batch 22/64 loss: -0.102317214012146
Batch 23/64 loss: -0.09839588403701782
Batch 24/64 loss: -0.08724761009216309
Batch 25/64 loss: -0.07642889022827148
Batch 26/64 loss: -0.08592027425765991
Batch 27/64 loss: -0.1139143705368042
Batch 28/64 loss: -0.10095477104187012
Batch 29/64 loss: -0.10095447301864624
Batch 30/64 loss: -0.11654782295227051
Batch 31/64 loss: -0.08124220371246338
Batch 32/64 loss: -0.07866919040679932
Batch 33/64 loss: -0.06812137365341187
Batch 34/64 loss: -0.08297932147979736
Batch 35/64 loss: -0.08669131994247437
Batch 36/64 loss: -0.0988582968711853
Batch 37/64 loss: -0.09102237224578857
Batch 38/64 loss: -0.10465562343597412
Batch 39/64 loss: -0.06561410427093506
Batch 40/64 loss: -0.08785593509674072
Batch 41/64 loss: -0.09799933433532715
Batch 42/64 loss: -0.10613924264907837
Batch 43/64 loss: -0.0758785605430603
Batch 44/64 loss: -0.0960468053817749
Batch 45/64 loss: -0.08544051647186279
Batch 46/64 loss: -0.08410632610321045
Batch 47/64 loss: -0.08695608377456665
Batch 48/64 loss: -0.12316739559173584
Batch 49/64 loss: -0.09850424528121948
Batch 50/64 loss: -0.06480151414871216
Batch 51/64 loss: -0.06725078821182251
Batch 52/64 loss: -0.10773879289627075
Batch 53/64 loss: -0.08154124021530151
Batch 54/64 loss: -0.07553911209106445
Batch 55/64 loss: -0.04252481460571289
Batch 56/64 loss: -0.07686132192611694
Batch 57/64 loss: -0.09577029943466187
Batch 58/64 loss: -0.06572103500366211
Batch 59/64 loss: -0.09497076272964478
Batch 60/64 loss: -0.09154343605041504
Batch 61/64 loss: -0.10714954137802124
Batch 62/64 loss: -0.05012732744216919
Batch 63/64 loss: -0.07567256689071655
Batch 64/64 loss: -0.09491872787475586
Epoch 79  Train loss: -0.09166233212340112  Val loss: 0.015639467337696823
Saving best model, epoch: 79
Epoch 80
-------------------------------
Batch 1/64 loss: -0.08585113286972046
Batch 2/64 loss: -0.10374325513839722
Batch 3/64 loss: -0.10613381862640381
Batch 4/64 loss: -0.08276712894439697
Batch 5/64 loss: -0.0871087908744812
Batch 6/64 loss: -0.09422117471694946
Batch 7/64 loss: -0.08870148658752441
Batch 8/64 loss: -0.10891842842102051
Batch 9/64 loss: -0.1337987780570984
Batch 10/64 loss: -0.0765870213508606
Batch 11/64 loss: -0.10296308994293213
Batch 12/64 loss: -0.07996177673339844
Batch 13/64 loss: -0.09598785638809204
Batch 14/64 loss: -0.09793460369110107
Batch 15/64 loss: -0.0787959098815918
Batch 16/64 loss: -0.08857274055480957
Batch 17/64 loss: -0.1076996922492981
Batch 18/64 loss: -0.09792351722717285
Batch 19/64 loss: -0.10858404636383057
Batch 20/64 loss: -0.08002126216888428
Batch 21/64 loss: -0.11390149593353271
Batch 22/64 loss: -0.08565682172775269
Batch 23/64 loss: -0.10430377721786499
Batch 24/64 loss: -0.07785028219223022
Batch 25/64 loss: -0.08624738454818726
Batch 26/64 loss: -0.07581055164337158
Batch 27/64 loss: -0.09439617395401001
Batch 28/64 loss: -0.10820978879928589
Batch 29/64 loss: -0.09805190563201904
Batch 30/64 loss: -0.096859872341156
Batch 31/64 loss: -0.10379195213317871
Batch 32/64 loss: -0.075996994972229
Batch 33/64 loss: -0.08223533630371094
Batch 34/64 loss: -0.059395551681518555
Batch 35/64 loss: -0.08042693138122559
Batch 36/64 loss: -0.09950006008148193
Batch 37/64 loss: -0.118782639503479
Batch 38/64 loss: -0.0944589376449585
Batch 39/64 loss: -0.10941767692565918
Batch 40/64 loss: -0.07038915157318115
Batch 41/64 loss: -0.11420238018035889
Batch 42/64 loss: -0.1132928729057312
Batch 43/64 loss: -0.10969716310501099
Batch 44/64 loss: -0.09113806486129761
Batch 45/64 loss: -0.07781684398651123
Batch 46/64 loss: -0.06887787580490112
Batch 47/64 loss: -0.08416545391082764
Batch 48/64 loss: -0.10299009084701538
Batch 49/64 loss: -0.08277338743209839
Batch 50/64 loss: -0.11706042289733887
Batch 51/64 loss: -0.08760905265808105
Batch 52/64 loss: -0.10698562860488892
Batch 53/64 loss: -0.09248793125152588
Batch 54/64 loss: -0.09982621669769287
Batch 55/64 loss: -0.09384965896606445
Batch 56/64 loss: -0.10326230525970459
Batch 57/64 loss: -0.10954201221466064
Batch 58/64 loss: -0.06859880685806274
Batch 59/64 loss: -0.08477294445037842
Batch 60/64 loss: -0.09842920303344727
Batch 61/64 loss: -0.07167530059814453
Batch 62/64 loss: -0.052293598651885986
Batch 63/64 loss: -0.10196340084075928
Batch 64/64 loss: -0.1018984317779541
Epoch 80  Train loss: -0.0933598939110251  Val loss: 0.016837729211525407
Epoch 81
-------------------------------
Batch 1/64 loss: -0.06896674633026123
Batch 2/64 loss: -0.0979001522064209
Batch 3/64 loss: -0.0770842432975769
Batch 4/64 loss: -0.11304640769958496
Batch 5/64 loss: -0.11296296119689941
Batch 6/64 loss: -0.08979982137680054
Batch 7/64 loss: -0.07840150594711304
Batch 8/64 loss: -0.09417617321014404
Batch 9/64 loss: -0.12183618545532227
Batch 10/64 loss: -0.07702666521072388
Batch 11/64 loss: -0.10125148296356201
Batch 12/64 loss: -0.07605278491973877
Batch 13/64 loss: -0.11446398496627808
Batch 14/64 loss: -0.09334170818328857
Batch 15/64 loss: -0.10794860124588013
Batch 16/64 loss: -0.08801090717315674
Batch 17/64 loss: -0.10723263025283813
Batch 18/64 loss: -0.0836992859840393
Batch 19/64 loss: -0.10814785957336426
Batch 20/64 loss: -0.06147700548171997
Batch 21/64 loss: -0.11034047603607178
Batch 22/64 loss: -0.09282052516937256
Batch 23/64 loss: -0.08220022916793823
Batch 24/64 loss: -0.07569020986557007
Batch 25/64 loss: -0.07358437776565552
Batch 26/64 loss: -0.07818490266799927
Batch 27/64 loss: -0.09181439876556396
Batch 28/64 loss: -0.10090935230255127
Batch 29/64 loss: -0.082369863986969
Batch 30/64 loss: -0.08477675914764404
Batch 31/64 loss: -0.04075789451599121
Batch 32/64 loss: -0.1301470398902893
Batch 33/64 loss: -0.10029232501983643
Batch 34/64 loss: -0.09393072128295898
Batch 35/64 loss: -0.07047301530838013
Batch 36/64 loss: -0.09251987934112549
Batch 37/64 loss: -0.06222182512283325
Batch 38/64 loss: -0.0800812840461731
Batch 39/64 loss: -0.10333496332168579
Batch 40/64 loss: -0.11031997203826904
Batch 41/64 loss: -0.113983154296875
Batch 42/64 loss: -0.09414499998092651
Batch 43/64 loss: -0.11051511764526367
Batch 44/64 loss: -0.0831383466720581
Batch 45/64 loss: -0.08866018056869507
Batch 46/64 loss: -0.08432948589324951
Batch 47/64 loss: -0.0986102819442749
Batch 48/64 loss: -0.11429566144943237
Batch 49/64 loss: -0.06278598308563232
Batch 50/64 loss: -0.10743391513824463
Batch 51/64 loss: -0.0933648943901062
Batch 52/64 loss: -0.11740577220916748
Batch 53/64 loss: -0.10991334915161133
Batch 54/64 loss: -0.08925449848175049
Batch 55/64 loss: -0.10496872663497925
Batch 56/64 loss: -0.13492482900619507
Batch 57/64 loss: -0.07951074838638306
Batch 58/64 loss: -0.08545267581939697
Batch 59/64 loss: -0.09561169147491455
Batch 60/64 loss: -0.10862338542938232
Batch 61/64 loss: -0.10932326316833496
Batch 62/64 loss: -0.09409809112548828
Batch 63/64 loss: -0.1199028491973877
Batch 64/64 loss: -0.09114456176757812
Epoch 81  Train loss: -0.09408952862608666  Val loss: 0.013918758667621416
Saving best model, epoch: 81
Epoch 82
-------------------------------
Batch 1/64 loss: -0.09091109037399292
Batch 2/64 loss: -0.10595905780792236
Batch 3/64 loss: -0.07665693759918213
Batch 4/64 loss: -0.10352927446365356
Batch 5/64 loss: -0.12534606456756592
Batch 6/64 loss: -0.09000593423843384
Batch 7/64 loss: -0.08000648021697998
Batch 8/64 loss: -0.09304827451705933
Batch 9/64 loss: -0.09502506256103516
Batch 10/64 loss: -0.09510475397109985
Batch 11/64 loss: -0.0981370210647583
Batch 12/64 loss: -0.11186826229095459
Batch 13/64 loss: -0.05992239713668823
Batch 14/64 loss: -0.08364325761795044
Batch 15/64 loss: -0.06411564350128174
Batch 16/64 loss: -0.07562166452407837
Batch 17/64 loss: -0.10082805156707764
Batch 18/64 loss: -0.11100435256958008
Batch 19/64 loss: -0.09265357255935669
Batch 20/64 loss: -0.09373044967651367
Batch 21/64 loss: -0.08297997713088989
Batch 22/64 loss: -0.09134817123413086
Batch 23/64 loss: -0.08849173784255981
Batch 24/64 loss: -0.08157020807266235
Batch 25/64 loss: -0.09082216024398804
Batch 26/64 loss: -0.10194611549377441
Batch 27/64 loss: -0.08798778057098389
Batch 28/64 loss: -0.1058950424194336
Batch 29/64 loss: -0.09989070892333984
Batch 30/64 loss: -0.10566484928131104
Batch 31/64 loss: -0.09372001886367798
Batch 32/64 loss: -0.10080033540725708
Batch 33/64 loss: -0.08991849422454834
Batch 34/64 loss: -0.09534990787506104
Batch 35/64 loss: -0.0938485860824585
Batch 36/64 loss: -0.0842592716217041
Batch 37/64 loss: -0.11660832166671753
Batch 38/64 loss: -0.0913129448890686
Batch 39/64 loss: -0.09820824861526489
Batch 40/64 loss: -0.11118757724761963
Batch 41/64 loss: -0.06981205940246582
Batch 42/64 loss: -0.10153472423553467
Batch 43/64 loss: -0.11503314971923828
Batch 44/64 loss: -0.11693811416625977
Batch 45/64 loss: -0.10021883249282837
Batch 46/64 loss: -0.06823629140853882
Batch 47/64 loss: -0.10316324234008789
Batch 48/64 loss: -0.13761568069458008
Batch 49/64 loss: -0.11138629913330078
Batch 50/64 loss: -0.09168320894241333
Batch 51/64 loss: -0.08189696073532104
Batch 52/64 loss: -0.09861993789672852
Batch 53/64 loss: -0.13347947597503662
Batch 54/64 loss: -0.08504164218902588
Batch 55/64 loss: -0.10704684257507324
Batch 56/64 loss: -0.11698150634765625
Batch 57/64 loss: -0.127341628074646
Batch 58/64 loss: -0.10516291856765747
Batch 59/64 loss: -0.10231584310531616
Batch 60/64 loss: -0.08322155475616455
Batch 61/64 loss: -0.09129452705383301
Batch 62/64 loss: -0.06592816114425659
Batch 63/64 loss: -0.12324631214141846
Batch 64/64 loss: -0.09740853309631348
Epoch 82  Train loss: -0.09677150389727424  Val loss: 0.01622234505066757
Epoch 83
-------------------------------
Batch 1/64 loss: -0.12442600727081299
Batch 2/64 loss: -0.09621310234069824
Batch 3/64 loss: -0.11656951904296875
Batch 4/64 loss: -0.1304081678390503
Batch 5/64 loss: -0.1145179271697998
Batch 6/64 loss: -0.12593203783035278
Batch 7/64 loss: -0.1183176040649414
Batch 8/64 loss: -0.09304028749465942
Batch 9/64 loss: -0.09099805355072021
Batch 10/64 loss: -0.09588861465454102
Batch 11/64 loss: -0.10085582733154297
Batch 12/64 loss: -0.11243319511413574
Batch 13/64 loss: -0.09167373180389404
Batch 14/64 loss: -0.11131083965301514
Batch 15/64 loss: -0.09417754411697388
Batch 16/64 loss: -0.09430438280105591
Batch 17/64 loss: -0.10234206914901733
Batch 18/64 loss: -0.10123282670974731
Batch 19/64 loss: -0.09415090084075928
Batch 20/64 loss: -0.06650185585021973
Batch 21/64 loss: -0.11926132440567017
Batch 22/64 loss: -0.0782623291015625
Batch 23/64 loss: -0.08237898349761963
Batch 24/64 loss: -0.09617596864700317
Batch 25/64 loss: -0.11445599794387817
Batch 26/64 loss: -0.11131787300109863
Batch 27/64 loss: -0.10566073656082153
Batch 28/64 loss: -0.10174190998077393
Batch 29/64 loss: -0.09072399139404297
Batch 30/64 loss: -0.1041073203086853
Batch 31/64 loss: -0.08182406425476074
Batch 32/64 loss: -0.055550992488861084
Batch 33/64 loss: -0.07205361127853394
Batch 34/64 loss: -0.10043811798095703
Batch 35/64 loss: -0.11603784561157227
Batch 36/64 loss: -0.09631186723709106
Batch 37/64 loss: -0.10289055109024048
Batch 38/64 loss: -0.089069664478302
Batch 39/64 loss: -0.06987148523330688
Batch 40/64 loss: -0.10963422060012817
Batch 41/64 loss: -0.1007084846496582
Batch 42/64 loss: -0.10619944334030151
Batch 43/64 loss: -0.10847020149230957
Batch 44/64 loss: -0.09880119562149048
Batch 45/64 loss: -0.07690644264221191
Batch 46/64 loss: -0.07063412666320801
Batch 47/64 loss: -0.115264892578125
Batch 48/64 loss: -0.09315645694732666
Batch 49/64 loss: -0.08749043941497803
Batch 50/64 loss: -0.08693796396255493
Batch 51/64 loss: -0.11004430055618286
Batch 52/64 loss: -0.10748720169067383
Batch 53/64 loss: -0.06979978084564209
Batch 54/64 loss: -0.12439227104187012
Batch 55/64 loss: -0.12144690752029419
Batch 56/64 loss: -0.0820380449295044
Batch 57/64 loss: -0.10218459367752075
Batch 58/64 loss: -0.11688727140426636
Batch 59/64 loss: -0.08660584688186646
Batch 60/64 loss: -0.11278021335601807
Batch 61/64 loss: -0.11053192615509033
Batch 62/64 loss: -0.09984856843948364
Batch 63/64 loss: -0.08968609571456909
Batch 64/64 loss: -0.08658063411712646
Epoch 83  Train loss: -0.09907923913469502  Val loss: 0.018575025792793718
Epoch 84
-------------------------------
Batch 1/64 loss: -0.1007375717163086
Batch 2/64 loss: -0.10824447870254517
Batch 3/64 loss: -0.13203787803649902
Batch 4/64 loss: -0.07448214292526245
Batch 5/64 loss: -0.11323714256286621
Batch 6/64 loss: -0.14626747369766235
Batch 7/64 loss: -0.10491907596588135
Batch 8/64 loss: -0.1147078275680542
Batch 9/64 loss: -0.10374021530151367
Batch 10/64 loss: -0.12508302927017212
Batch 11/64 loss: -0.08405190706253052
Batch 12/64 loss: -0.09556466341018677
Batch 13/64 loss: -0.10559743642807007
Batch 14/64 loss: -0.07710736989974976
Batch 15/64 loss: -0.11280381679534912
Batch 16/64 loss: -0.09583151340484619
Batch 17/64 loss: -0.10850870609283447
Batch 18/64 loss: -0.10766977071762085
Batch 19/64 loss: -0.06674027442932129
Batch 20/64 loss: -0.09652495384216309
Batch 21/64 loss: -0.10236591100692749
Batch 22/64 loss: -0.12169700860977173
Batch 23/64 loss: -0.08333075046539307
Batch 24/64 loss: -0.10855257511138916
Batch 25/64 loss: -0.11524665355682373
Batch 26/64 loss: -0.09141343832015991
Batch 27/64 loss: -0.11045330762863159
Batch 28/64 loss: -0.12163633108139038
Batch 29/64 loss: -0.09217047691345215
Batch 30/64 loss: -0.055870890617370605
Batch 31/64 loss: -0.09941494464874268
Batch 32/64 loss: -0.10159569978713989
Batch 33/64 loss: -0.12412762641906738
Batch 34/64 loss: -0.09055042266845703
Batch 35/64 loss: -0.11931377649307251
Batch 36/64 loss: -0.07639598846435547
Batch 37/64 loss: -0.10498464107513428
Batch 38/64 loss: -0.08303594589233398
Batch 39/64 loss: -0.12242686748504639
Batch 40/64 loss: -0.09918928146362305
Batch 41/64 loss: -0.08269542455673218
Batch 42/64 loss: -0.10209763050079346
Batch 43/64 loss: -0.10504961013793945
Batch 44/64 loss: -0.08947527408599854
Batch 45/64 loss: -0.0781865119934082
Batch 46/64 loss: -0.10765326023101807
Batch 47/64 loss: -0.09178471565246582
Batch 48/64 loss: -0.11739903688430786
Batch 49/64 loss: -0.1219402551651001
Batch 50/64 loss: -0.10104405879974365
Batch 51/64 loss: -0.0974205732345581
Batch 52/64 loss: -0.11908620595932007
Batch 53/64 loss: -0.07952260971069336
Batch 54/64 loss: -0.0792456865310669
Batch 55/64 loss: -0.09693324565887451
Batch 56/64 loss: -0.10773712396621704
Batch 57/64 loss: -0.07855063676834106
Batch 58/64 loss: -0.06699466705322266
Batch 59/64 loss: -0.09546494483947754
Batch 60/64 loss: -0.10513603687286377
Batch 61/64 loss: -0.0996854305267334
Batch 62/64 loss: -0.11231136322021484
Batch 63/64 loss: -0.09881448745727539
Batch 64/64 loss: -0.09747087955474854
Epoch 84  Train loss: -0.10046998800015917  Val loss: 0.016741501097007307
Epoch 85
-------------------------------
Batch 1/64 loss: -0.10881823301315308
Batch 2/64 loss: -0.09595680236816406
Batch 3/64 loss: -0.12016934156417847
Batch 4/64 loss: -0.10324007272720337
Batch 5/64 loss: -0.1221846342086792
Batch 6/64 loss: -0.09506559371948242
Batch 7/64 loss: -0.08590489625930786
Batch 8/64 loss: -0.08674323558807373
Batch 9/64 loss: -0.06290560960769653
Batch 10/64 loss: -0.09960830211639404
Batch 11/64 loss: -0.09865033626556396
Batch 12/64 loss: -0.10934758186340332
Batch 13/64 loss: -0.09970235824584961
Batch 14/64 loss: -0.12020993232727051
Batch 15/64 loss: -0.10869717597961426
Batch 16/64 loss: -0.11274564266204834
Batch 17/64 loss: -0.08911573886871338
Batch 18/64 loss: -0.09067857265472412
Batch 19/64 loss: -0.12145334482192993
Batch 20/64 loss: -0.12185502052307129
Batch 21/64 loss: -0.10471457242965698
Batch 22/64 loss: -0.10406184196472168
Batch 23/64 loss: -0.10507375001907349
Batch 24/64 loss: -0.11916911602020264
Batch 25/64 loss: -0.0791463851928711
Batch 26/64 loss: -0.0931481122970581
Batch 27/64 loss: -0.07644009590148926
Batch 28/64 loss: -0.07436954975128174
Batch 29/64 loss: -0.09969282150268555
Batch 30/64 loss: -0.10039573907852173
Batch 31/64 loss: -0.1124773621559143
Batch 32/64 loss: -0.11312466859817505
Batch 33/64 loss: -0.12402200698852539
Batch 34/64 loss: -0.06113719940185547
Batch 35/64 loss: -0.10351002216339111
Batch 36/64 loss: -0.11460500955581665
Batch 37/64 loss: -0.10787522792816162
Batch 38/64 loss: -0.08611667156219482
Batch 39/64 loss: -0.0892067551612854
Batch 40/64 loss: -0.1056746244430542
Batch 41/64 loss: -0.10923504829406738
Batch 42/64 loss: -0.09701013565063477
Batch 43/64 loss: -0.11236834526062012
Batch 44/64 loss: -0.12198376655578613
Batch 45/64 loss: -0.11816650629043579
Batch 46/64 loss: -0.11629807949066162
Batch 47/64 loss: -0.10165208578109741
Batch 48/64 loss: -0.10637426376342773
Batch 49/64 loss: -0.06815338134765625
Batch 50/64 loss: -0.08398711681365967
Batch 51/64 loss: -0.10535955429077148
Batch 52/64 loss: -0.1117624044418335
Batch 53/64 loss: -0.11856234073638916
Batch 54/64 loss: -0.10417389869689941
Batch 55/64 loss: -0.11410093307495117
Batch 56/64 loss: -0.10427957773208618
Batch 57/64 loss: -0.08298975229263306
Batch 58/64 loss: -0.12416642904281616
Batch 59/64 loss: -0.08530598878860474
Batch 60/64 loss: -0.07247710227966309
Batch 61/64 loss: -0.09016740322113037
Batch 62/64 loss: -0.10201370716094971
Batch 63/64 loss: -0.10181272029876709
Batch 64/64 loss: -0.11831796169281006
Epoch 85  Train loss: -0.10146122306000953  Val loss: 0.02003840646383279
Epoch 86
-------------------------------
Batch 1/64 loss: -0.1348198652267456
Batch 2/64 loss: -0.11023259162902832
Batch 3/64 loss: -0.09538424015045166
Batch 4/64 loss: -0.11026090383529663
Batch 5/64 loss: -0.13341432809829712
Batch 6/64 loss: -0.08787435293197632
Batch 7/64 loss: -0.09214639663696289
Batch 8/64 loss: -0.12640875577926636
Batch 9/64 loss: -0.083668053150177
Batch 10/64 loss: -0.13280022144317627
Batch 11/64 loss: -0.13679802417755127
Batch 12/64 loss: -0.126143217086792
Batch 13/64 loss: -0.13012796640396118
Batch 14/64 loss: -0.0799608826637268
Batch 15/64 loss: -0.09357643127441406
Batch 16/64 loss: -0.1097111701965332
Batch 17/64 loss: -0.10940617322921753
Batch 18/64 loss: -0.11032027006149292
Batch 19/64 loss: -0.0910031795501709
Batch 20/64 loss: -0.08484005928039551
Batch 21/64 loss: -0.07746857404708862
Batch 22/64 loss: -0.07591116428375244
Batch 23/64 loss: -0.08622968196868896
Batch 24/64 loss: -0.1065288782119751
Batch 25/64 loss: -0.11068987846374512
Batch 26/64 loss: -0.10144561529159546
Batch 27/64 loss: -0.07133424282073975
Batch 28/64 loss: -0.07704168558120728
Batch 29/64 loss: -0.1069403886795044
Batch 30/64 loss: -0.11869800090789795
Batch 31/64 loss: -0.10658568143844604
Batch 32/64 loss: -0.122630774974823
Batch 33/64 loss: -0.13720673322677612
Batch 34/64 loss: -0.11974537372589111
Batch 35/64 loss: -0.11505961418151855
Batch 36/64 loss: -0.08965623378753662
Batch 37/64 loss: -0.06986910104751587
Batch 38/64 loss: -0.10749167203903198
Batch 39/64 loss: -0.08014672994613647
Batch 40/64 loss: -0.12329083681106567
Batch 41/64 loss: -0.10546290874481201
Batch 42/64 loss: -0.10351991653442383
Batch 43/64 loss: -0.10513734817504883
Batch 44/64 loss: -0.08038777112960815
Batch 45/64 loss: -0.10630249977111816
Batch 46/64 loss: -0.11901164054870605
Batch 47/64 loss: -0.09473669528961182
Batch 48/64 loss: -0.1096348762512207
Batch 49/64 loss: -0.11819207668304443
Batch 50/64 loss: -0.12165939807891846
Batch 51/64 loss: -0.10988986492156982
Batch 52/64 loss: -0.10670316219329834
Batch 53/64 loss: -0.09148788452148438
Batch 54/64 loss: -0.1215062141418457
Batch 55/64 loss: -0.10247182846069336
Batch 56/64 loss: -0.07463252544403076
Batch 57/64 loss: -0.12338846921920776
Batch 58/64 loss: -0.0861506462097168
Batch 59/64 loss: -0.09087914228439331
Batch 60/64 loss: -0.11146610975265503
Batch 61/64 loss: -0.11319774389266968
Batch 62/64 loss: -0.09297293424606323
Batch 63/64 loss: -0.09054052829742432
Batch 64/64 loss: -0.09475016593933105
Epoch 86  Train loss: -0.10405118044684915  Val loss: 0.016121530246079173
Epoch 87
-------------------------------
Batch 1/64 loss: -0.12823021411895752
Batch 2/64 loss: -0.0683281421661377
Batch 3/64 loss: -0.11483389139175415
Batch 4/64 loss: -0.10088974237442017
Batch 5/64 loss: -0.10486620664596558
Batch 6/64 loss: -0.12236666679382324
Batch 7/64 loss: -0.11548757553100586
Batch 8/64 loss: -0.11269402503967285
Batch 9/64 loss: -0.12346869707107544
Batch 10/64 loss: -0.10955286026000977
Batch 11/64 loss: -0.08114451169967651
Batch 12/64 loss: -0.1276867389678955
Batch 13/64 loss: -0.11360067129135132
Batch 14/64 loss: -0.13440370559692383
Batch 15/64 loss: -0.11654007434844971
Batch 16/64 loss: -0.12566721439361572
Batch 17/64 loss: -0.07179594039916992
Batch 18/64 loss: -0.11565899848937988
Batch 19/64 loss: -0.11057519912719727
Batch 20/64 loss: -0.11304432153701782
Batch 21/64 loss: -0.08844274282455444
Batch 22/64 loss: -0.09569072723388672
Batch 23/64 loss: -0.11203324794769287
Batch 24/64 loss: -0.10412704944610596
Batch 25/64 loss: -0.10864424705505371
Batch 26/64 loss: -0.10264277458190918
Batch 27/64 loss: -0.07161903381347656
Batch 28/64 loss: -0.10246032476425171
Batch 29/64 loss: -0.09109997749328613
Batch 30/64 loss: -0.12097668647766113
Batch 31/64 loss: -0.1284591555595398
Batch 32/64 loss: -0.13566625118255615
Batch 33/64 loss: -0.06655222177505493
Batch 34/64 loss: -0.10107851028442383
Batch 35/64 loss: -0.10968512296676636
Batch 36/64 loss: -0.12051403522491455
Batch 37/64 loss: -0.1157563328742981
Batch 38/64 loss: -0.1080583930015564
Batch 39/64 loss: -0.10502910614013672
Batch 40/64 loss: -0.13882344961166382
Batch 41/64 loss: -0.07498162984848022
Batch 42/64 loss: -0.13284629583358765
Batch 43/64 loss: -0.1044110655784607
Batch 44/64 loss: -0.10707783699035645
Batch 45/64 loss: -0.1185256838798523
Batch 46/64 loss: -0.08541524410247803
Batch 47/64 loss: -0.10919791460037231
Batch 48/64 loss: -0.07977479696273804
Batch 49/64 loss: -0.08932125568389893
Batch 50/64 loss: -0.08946335315704346
Batch 51/64 loss: -0.11407756805419922
Batch 52/64 loss: -0.11751061677932739
Batch 53/64 loss: -0.11807918548583984
Batch 54/64 loss: -0.09778249263763428
Batch 55/64 loss: -0.11017078161239624
Batch 56/64 loss: -0.11677658557891846
Batch 57/64 loss: -0.13442343473434448
Batch 58/64 loss: -0.0843360424041748
Batch 59/64 loss: -0.08906811475753784
Batch 60/64 loss: -0.09762126207351685
Batch 61/64 loss: -0.1100085973739624
Batch 62/64 loss: -0.1270751953125
Batch 63/64 loss: -0.10637569427490234
Batch 64/64 loss: -0.11984133720397949
Epoch 87  Train loss: -0.10733170883328307  Val loss: 0.01695199172521375
Epoch 88
-------------------------------
Batch 1/64 loss: -0.13995707035064697
Batch 2/64 loss: -0.13401633501052856
Batch 3/64 loss: -0.10598015785217285
Batch 4/64 loss: -0.08324289321899414
Batch 5/64 loss: -0.12085723876953125
Batch 6/64 loss: -0.12514346837997437
Batch 7/64 loss: -0.12765461206436157
Batch 8/64 loss: -0.11513161659240723
Batch 9/64 loss: -0.11360037326812744
Batch 10/64 loss: -0.1130097508430481
Batch 11/64 loss: -0.09342485666275024
Batch 12/64 loss: -0.09745627641677856
Batch 13/64 loss: -0.08037173748016357
Batch 14/64 loss: -0.057980358600616455
Batch 15/64 loss: -0.15183740854263306
Batch 16/64 loss: -0.1119692325592041
Batch 17/64 loss: -0.10549932718276978
Batch 18/64 loss: -0.13805806636810303
Batch 19/64 loss: -0.09272044897079468
Batch 20/64 loss: -0.11103415489196777
Batch 21/64 loss: -0.09681153297424316
Batch 22/64 loss: -0.12317585945129395
Batch 23/64 loss: -0.1216731071472168
Batch 24/64 loss: -0.1096193790435791
Batch 25/64 loss: -0.11216723918914795
Batch 26/64 loss: -0.12179666757583618
Batch 27/64 loss: -0.09203791618347168
Batch 28/64 loss: -0.08410274982452393
Batch 29/64 loss: -0.11118435859680176
Batch 30/64 loss: -0.11086970567703247
Batch 31/64 loss: -0.11639028787612915
Batch 32/64 loss: -0.1312592625617981
Batch 33/64 loss: -0.10241061449050903
Batch 34/64 loss: -0.09498703479766846
Batch 35/64 loss: -0.11286979913711548
Batch 36/64 loss: -0.12477874755859375
Batch 37/64 loss: -0.10650384426116943
Batch 38/64 loss: -0.13672959804534912
Batch 39/64 loss: -0.10903781652450562
Batch 40/64 loss: -0.14672940969467163
Batch 41/64 loss: -0.0986025333404541
Batch 42/64 loss: -0.10636186599731445
Batch 43/64 loss: -0.13060420751571655
Batch 44/64 loss: -0.1256687045097351
Batch 45/64 loss: -0.10104143619537354
Batch 46/64 loss: -0.08631694316864014
Batch 47/64 loss: -0.13789701461791992
Batch 48/64 loss: -0.11908614635467529
Batch 49/64 loss: -0.11083060503005981
Batch 50/64 loss: -0.11116212606430054
Batch 51/64 loss: -0.10468024015426636
Batch 52/64 loss: -0.11716097593307495
Batch 53/64 loss: -0.08203989267349243
Batch 54/64 loss: -0.11709171533584595
Batch 55/64 loss: -0.07275056838989258
Batch 56/64 loss: -0.11349672079086304
Batch 57/64 loss: -0.08244705200195312
Batch 58/64 loss: -0.11343157291412354
Batch 59/64 loss: -0.11939960718154907
Batch 60/64 loss: -0.08271396160125732
Batch 61/64 loss: -0.13401579856872559
Batch 62/64 loss: -0.08810573816299438
Batch 63/64 loss: -0.06808352470397949
Batch 64/64 loss: -0.12124919891357422
Epoch 88  Train loss: -0.10977264572592342  Val loss: 0.018415857836143256
Epoch 89
-------------------------------
Batch 1/64 loss: -0.09928297996520996
Batch 2/64 loss: -0.12163054943084717
Batch 3/64 loss: -0.10459333658218384
Batch 4/64 loss: -0.11494225263595581
Batch 5/64 loss: -0.1316664218902588
Batch 6/64 loss: -0.10864555835723877
Batch 7/64 loss: -0.13532185554504395
Batch 8/64 loss: -0.1181451678276062
Batch 9/64 loss: -0.12107598781585693
Batch 10/64 loss: -0.10637015104293823
Batch 11/64 loss: -0.11034458875656128
Batch 12/64 loss: -0.11579680442810059
Batch 13/64 loss: -0.12647879123687744
Batch 14/64 loss: -0.10920518636703491
Batch 15/64 loss: -0.12882739305496216
Batch 16/64 loss: -0.09260934591293335
Batch 17/64 loss: -0.12419342994689941
Batch 18/64 loss: -0.11345022916793823
Batch 19/64 loss: -0.1251508593559265
Batch 20/64 loss: -0.11244386434555054
Batch 21/64 loss: -0.13243603706359863
Batch 22/64 loss: -0.0942147970199585
Batch 23/64 loss: -0.1201435923576355
Batch 24/64 loss: -0.12150824069976807
Batch 25/64 loss: -0.12868237495422363
Batch 26/64 loss: -0.1407243013381958
Batch 27/64 loss: -0.11449140310287476
Batch 28/64 loss: -0.08531695604324341
Batch 29/64 loss: -0.1433088183403015
Batch 30/64 loss: -0.14275455474853516
Batch 31/64 loss: -0.11391639709472656
Batch 32/64 loss: -0.09462952613830566
Batch 33/64 loss: -0.08920931816101074
Batch 34/64 loss: -0.12814772129058838
Batch 35/64 loss: -0.11381411552429199
Batch 36/64 loss: -0.10916852951049805
Batch 37/64 loss: -0.10423719882965088
Batch 38/64 loss: -0.10702335834503174
Batch 39/64 loss: -0.10235428810119629
Batch 40/64 loss: -0.11827689409255981
Batch 41/64 loss: -0.11921930313110352
Batch 42/64 loss: -0.10712569952011108
Batch 43/64 loss: -0.1005326509475708
Batch 44/64 loss: -0.11096405982971191
Batch 45/64 loss: -0.09472590684890747
Batch 46/64 loss: -0.10697603225708008
Batch 47/64 loss: -0.13284695148468018
Batch 48/64 loss: -0.06837528944015503
Batch 49/64 loss: -0.12083196640014648
Batch 50/64 loss: -0.12855350971221924
Batch 51/64 loss: -0.08537256717681885
Batch 52/64 loss: -0.07977968454360962
Batch 53/64 loss: -0.09834569692611694
Batch 54/64 loss: -0.09840482473373413
Batch 55/64 loss: -0.10670685768127441
Batch 56/64 loss: -0.07711058855056763
Batch 57/64 loss: -0.09544450044631958
Batch 58/64 loss: -0.1265639066696167
Batch 59/64 loss: -0.1007797122001648
Batch 60/64 loss: -0.09124630689620972
Batch 61/64 loss: -0.08586305379867554
Batch 62/64 loss: -0.08564925193786621
Batch 63/64 loss: -0.10121756792068481
Batch 64/64 loss: -0.09881752729415894
Epoch 89  Train loss: -0.11013776017170326  Val loss: 0.017279332241241875
Epoch 90
-------------------------------
Batch 1/64 loss: -0.12880289554595947
Batch 2/64 loss: -0.10881835222244263
Batch 3/64 loss: -0.1285790205001831
Batch 4/64 loss: -0.11678189039230347
Batch 5/64 loss: -0.13221001625061035
Batch 6/64 loss: -0.12614309787750244
Batch 7/64 loss: -0.10051524639129639
Batch 8/64 loss: -0.11906790733337402
Batch 9/64 loss: -0.10254162549972534
Batch 10/64 loss: -0.10801362991333008
Batch 11/64 loss: -0.11340773105621338
Batch 12/64 loss: -0.10333132743835449
Batch 13/64 loss: -0.08719241619110107
Batch 14/64 loss: -0.09719967842102051
Batch 15/64 loss: -0.08828502893447876
Batch 16/64 loss: -0.10686248540878296
Batch 17/64 loss: -0.10965907573699951
Batch 18/64 loss: -0.10220694541931152
Batch 19/64 loss: -0.13358521461486816
Batch 20/64 loss: -0.10669738054275513
Batch 21/64 loss: -0.10773533582687378
Batch 22/64 loss: -0.10789662599563599
Batch 23/64 loss: -0.09490913152694702
Batch 24/64 loss: -0.14554190635681152
Batch 25/64 loss: -0.11447155475616455
Batch 26/64 loss: -0.11899071931838989
Batch 27/64 loss: -0.09848570823669434
Batch 28/64 loss: -0.11328989267349243
Batch 29/64 loss: -0.0974588394165039
Batch 30/64 loss: -0.11108416318893433
Batch 31/64 loss: -0.10852622985839844
Batch 32/64 loss: -0.12346518039703369
Batch 33/64 loss: -0.093894362449646
Batch 34/64 loss: -0.09714925289154053
Batch 35/64 loss: -0.1372508406639099
Batch 36/64 loss: -0.11356925964355469
Batch 37/64 loss: -0.10780709981918335
Batch 38/64 loss: -0.12139719724655151
Batch 39/64 loss: -0.1109246015548706
Batch 40/64 loss: -0.07555466890335083
Batch 41/64 loss: -0.12447726726531982
Batch 42/64 loss: -0.09309607744216919
Batch 43/64 loss: -0.0947721004486084
Batch 44/64 loss: -0.09778213500976562
Batch 45/64 loss: -0.10049879550933838
Batch 46/64 loss: -0.10499441623687744
Batch 47/64 loss: -0.12457460165023804
Batch 48/64 loss: -0.1346113681793213
Batch 49/64 loss: -0.09230250120162964
Batch 50/64 loss: -0.13587439060211182
Batch 51/64 loss: -0.12418687343597412
Batch 52/64 loss: -0.11912751197814941
Batch 53/64 loss: -0.1215510368347168
Batch 54/64 loss: -0.11120867729187012
Batch 55/64 loss: -0.09610676765441895
Batch 56/64 loss: -0.09340095520019531
Batch 57/64 loss: -0.11794859170913696
Batch 58/64 loss: -0.10074436664581299
Batch 59/64 loss: -0.11582446098327637
Batch 60/64 loss: -0.14920711517333984
Batch 61/64 loss: -0.11092609167098999
Batch 62/64 loss: -0.10666501522064209
Batch 63/64 loss: -0.14753270149230957
Batch 64/64 loss: -0.12328857183456421
Epoch 90  Train loss: -0.11183033386866252  Val loss: 0.01725621608524388
Epoch 91
-------------------------------
Batch 1/64 loss: -0.14405107498168945
Batch 2/64 loss: -0.10926783084869385
Batch 3/64 loss: -0.14898717403411865
Batch 4/64 loss: -0.11998921632766724
Batch 5/64 loss: -0.0837242603302002
Batch 6/64 loss: -0.08919847011566162
Batch 7/64 loss: -0.1218036413192749
Batch 8/64 loss: -0.13365012407302856
Batch 9/64 loss: -0.11853903532028198
Batch 10/64 loss: -0.1353667974472046
Batch 11/64 loss: -0.08447611331939697
Batch 12/64 loss: -0.11673635244369507
Batch 13/64 loss: -0.12104398012161255
Batch 14/64 loss: -0.13243317604064941
Batch 15/64 loss: -0.10267579555511475
Batch 16/64 loss: -0.12724220752716064
Batch 17/64 loss: -0.12477260828018188
Batch 18/64 loss: -0.1313380002975464
Batch 19/64 loss: -0.0919378399848938
Batch 20/64 loss: -0.14068084955215454
Batch 21/64 loss: -0.11472117900848389
Batch 22/64 loss: -0.08610445261001587
Batch 23/64 loss: -0.1051294207572937
Batch 24/64 loss: -0.12046235799789429
Batch 25/64 loss: -0.12405675649642944
Batch 26/64 loss: -0.13762903213500977
Batch 27/64 loss: -0.0971490740776062
Batch 28/64 loss: -0.134626567363739
Batch 29/64 loss: -0.13601750135421753
Batch 30/64 loss: -0.10288727283477783
Batch 31/64 loss: -0.10711485147476196
Batch 32/64 loss: -0.08644956350326538
Batch 33/64 loss: -0.10253435373306274
Batch 34/64 loss: -0.1399322748184204
Batch 35/64 loss: -0.11213231086730957
Batch 36/64 loss: -0.055033862590789795
Batch 37/64 loss: -0.10050958395004272
Batch 38/64 loss: -0.09894412755966187
Batch 39/64 loss: -0.11234098672866821
Batch 40/64 loss: -0.10360908508300781
Batch 41/64 loss: -0.08796310424804688
Batch 42/64 loss: -0.11087679862976074
Batch 43/64 loss: -0.12251418828964233
Batch 44/64 loss: -0.11126542091369629
Batch 45/64 loss: -0.12088412046432495
Batch 46/64 loss: -0.10803639888763428
Batch 47/64 loss: -0.13451087474822998
Batch 48/64 loss: -0.13948118686676025
Batch 49/64 loss: -0.13445210456848145
Batch 50/64 loss: -0.1078484058380127
Batch 51/64 loss: -0.1040412187576294
Batch 52/64 loss: -0.06568402051925659
Batch 53/64 loss: -0.10461628437042236
Batch 54/64 loss: -0.12140250205993652
Batch 55/64 loss: -0.11335331201553345
Batch 56/64 loss: -0.11166113615036011
Batch 57/64 loss: -0.10039085149765015
Batch 58/64 loss: -0.12772846221923828
Batch 59/64 loss: -0.12214791774749756
Batch 60/64 loss: -0.10524582862854004
Batch 61/64 loss: -0.11122745275497437
Batch 62/64 loss: -0.11249279975891113
Batch 63/64 loss: -0.08650517463684082
Batch 64/64 loss: -0.12376540899276733
Epoch 91  Train loss: -0.11313651426165712  Val loss: 0.014870465621096162
Epoch 92
-------------------------------
Batch 1/64 loss: -0.15782231092453003
Batch 2/64 loss: -0.12522727251052856
Batch 3/64 loss: -0.1088186502456665
Batch 4/64 loss: -0.1131858229637146
Batch 5/64 loss: -0.11774969100952148
Batch 6/64 loss: -0.12095320224761963
Batch 7/64 loss: -0.13004988431930542
Batch 8/64 loss: -0.12913453578948975
Batch 9/64 loss: -0.09995627403259277
Batch 10/64 loss: -0.13097518682479858
Batch 11/64 loss: -0.1086430549621582
Batch 12/64 loss: -0.12483096122741699
Batch 13/64 loss: -0.11268007755279541
Batch 14/64 loss: -0.11508673429489136
Batch 15/64 loss: -0.07880759239196777
Batch 16/64 loss: -0.06341433525085449
Batch 17/64 loss: -0.1335892677307129
Batch 18/64 loss: -0.09332215785980225
Batch 19/64 loss: -0.0997551679611206
Batch 20/64 loss: -0.11628866195678711
Batch 21/64 loss: -0.13598251342773438
Batch 22/64 loss: -0.10294121503829956
Batch 23/64 loss: -0.1163489818572998
Batch 24/64 loss: -0.1008155345916748
Batch 25/64 loss: -0.12526649236679077
Batch 26/64 loss: -0.0845949649810791
Batch 27/64 loss: -0.14107859134674072
Batch 28/64 loss: -0.09905898571014404
Batch 29/64 loss: -0.11420422792434692
Batch 30/64 loss: -0.11104381084442139
Batch 31/64 loss: -0.11356383562088013
Batch 32/64 loss: -0.12130045890808105
Batch 33/64 loss: -0.10610735416412354
Batch 34/64 loss: -0.12512385845184326
Batch 35/64 loss: -0.11904549598693848
Batch 36/64 loss: -0.12180352210998535
Batch 37/64 loss: -0.14348554611206055
Batch 38/64 loss: -0.12202924489974976
Batch 39/64 loss: -0.1249430775642395
Batch 40/64 loss: -0.1230270266532898
Batch 41/64 loss: -0.13932037353515625
Batch 42/64 loss: -0.11992156505584717
Batch 43/64 loss: -0.13309651613235474
Batch 44/64 loss: -0.13278114795684814
Batch 45/64 loss: -0.12912678718566895
Batch 46/64 loss: -0.11717355251312256
Batch 47/64 loss: -0.1308048963546753
Batch 48/64 loss: -0.07784277200698853
Batch 49/64 loss: -0.07295608520507812
Batch 50/64 loss: -0.10044938325881958
Batch 51/64 loss: -0.1191747784614563
Batch 52/64 loss: -0.10043901205062866
Batch 53/64 loss: -0.117054283618927
Batch 54/64 loss: -0.10703456401824951
Batch 55/64 loss: -0.12432587146759033
Batch 56/64 loss: -0.10966318845748901
Batch 57/64 loss: -0.1371026635169983
Batch 58/64 loss: -0.12739723920822144
Batch 59/64 loss: -0.08908331394195557
Batch 60/64 loss: -0.10215222835540771
Batch 61/64 loss: -0.12465512752532959
Batch 62/64 loss: -0.09374356269836426
Batch 63/64 loss: -0.13440722227096558
Batch 64/64 loss: -0.09097844362258911
Epoch 92  Train loss: -0.11513718506869147  Val loss: 0.013873429847337127
Saving best model, epoch: 92
Epoch 93
-------------------------------
Batch 1/64 loss: -0.12688565254211426
Batch 2/64 loss: -0.11609524488449097
Batch 3/64 loss: -0.13044053316116333
Batch 4/64 loss: -0.0979623794555664
Batch 5/64 loss: -0.08506357669830322
Batch 6/64 loss: -0.12631773948669434
Batch 7/64 loss: -0.14265167713165283
Batch 8/64 loss: -0.1567378044128418
Batch 9/64 loss: -0.12455600500106812
Batch 10/64 loss: -0.12575554847717285
Batch 11/64 loss: -0.14356094598770142
Batch 12/64 loss: -0.12936973571777344
Batch 13/64 loss: -0.1095118522644043
Batch 14/64 loss: -0.11684072017669678
Batch 15/64 loss: -0.07997965812683105
Batch 16/64 loss: -0.10717755556106567
Batch 17/64 loss: -0.12050509452819824
Batch 18/64 loss: -0.13175052404403687
Batch 19/64 loss: -0.12443214654922485
Batch 20/64 loss: -0.13544994592666626
Batch 21/64 loss: -0.11063134670257568
Batch 22/64 loss: -0.12719964981079102
Batch 23/64 loss: -0.10502606630325317
Batch 24/64 loss: -0.10794508457183838
Batch 25/64 loss: -0.11861145496368408
Batch 26/64 loss: -0.12992089986801147
Batch 27/64 loss: -0.09120333194732666
Batch 28/64 loss: -0.0926017165184021
Batch 29/64 loss: -0.07460963726043701
Batch 30/64 loss: -0.1321638822555542
Batch 31/64 loss: -0.11766934394836426
Batch 32/64 loss: -0.1318393349647522
Batch 33/64 loss: -0.11362004280090332
Batch 34/64 loss: -0.11973786354064941
Batch 35/64 loss: -0.1134304404258728
Batch 36/64 loss: -0.11977434158325195
Batch 37/64 loss: -0.12114876508712769
Batch 38/64 loss: -0.1051177978515625
Batch 39/64 loss: -0.11902230978012085
Batch 40/64 loss: -0.11849719285964966
Batch 41/64 loss: -0.10889244079589844
Batch 42/64 loss: -0.10656535625457764
Batch 43/64 loss: -0.10153031349182129
Batch 44/64 loss: -0.13140028715133667
Batch 45/64 loss: -0.1473284363746643
Batch 46/64 loss: -0.13513869047164917
Batch 47/64 loss: -0.10732114315032959
Batch 48/64 loss: -0.11839020252227783
Batch 49/64 loss: -0.0857163667678833
Batch 50/64 loss: -0.11373323202133179
Batch 51/64 loss: -0.12979894876480103
Batch 52/64 loss: -0.11299830675125122
Batch 53/64 loss: -0.12795114517211914
Batch 54/64 loss: -0.11100870370864868
Batch 55/64 loss: -0.1280510425567627
Batch 56/64 loss: -0.11643904447555542
Batch 57/64 loss: -0.11078613996505737
Batch 58/64 loss: -0.10888749361038208
Batch 59/64 loss: -0.11518985033035278
Batch 60/64 loss: -0.14025908708572388
Batch 61/64 loss: -0.11288213729858398
Batch 62/64 loss: -0.11224126815795898
Batch 63/64 loss: -0.13448011875152588
Batch 64/64 loss: -0.13296741247177124
Epoch 93  Train loss: -0.11792204169666066  Val loss: 0.01571621538437519
Epoch 94
-------------------------------
Batch 1/64 loss: -0.12932121753692627
Batch 2/64 loss: -0.12731009721755981
Batch 3/64 loss: -0.09676170349121094
Batch 4/64 loss: -0.14609181880950928
Batch 5/64 loss: -0.13065576553344727
Batch 6/64 loss: -0.08928287029266357
Batch 7/64 loss: -0.11540448665618896
Batch 8/64 loss: -0.048264265060424805
Batch 9/64 loss: -0.12683463096618652
Batch 10/64 loss: -0.11064183712005615
Batch 11/64 loss: -0.12206953763961792
Batch 12/64 loss: -0.12306004762649536
Batch 13/64 loss: -0.10996365547180176
Batch 14/64 loss: -0.12542152404785156
Batch 15/64 loss: -0.1295229196548462
Batch 16/64 loss: -0.1252833604812622
Batch 17/64 loss: -0.11894106864929199
Batch 18/64 loss: -0.14431285858154297
Batch 19/64 loss: -0.12215161323547363
Batch 20/64 loss: -0.1429477334022522
Batch 21/64 loss: -0.12684673070907593
Batch 22/64 loss: -0.11750161647796631
Batch 23/64 loss: -0.15306341648101807
Batch 24/64 loss: -0.14226257801055908
Batch 25/64 loss: -0.11719566583633423
Batch 26/64 loss: -0.12628960609436035
Batch 27/64 loss: -0.12756013870239258
Batch 28/64 loss: -0.10530972480773926
Batch 29/64 loss: -0.11755865812301636
Batch 30/64 loss: -0.12541043758392334
Batch 31/64 loss: -0.14154213666915894
Batch 32/64 loss: -0.11964195966720581
Batch 33/64 loss: -0.1321953535079956
Batch 34/64 loss: -0.09226745367050171
Batch 35/64 loss: -0.10431450605392456
Batch 36/64 loss: -0.1186370849609375
Batch 37/64 loss: -0.1253911256790161
Batch 38/64 loss: -0.1144704818725586
Batch 39/64 loss: -0.09459388256072998
Batch 40/64 loss: -0.14872103929519653
Batch 41/64 loss: -0.12956780195236206
Batch 42/64 loss: -0.07698899507522583
Batch 43/64 loss: -0.13365060091018677
Batch 44/64 loss: -0.12892389297485352
Batch 45/64 loss: -0.12807154655456543
Batch 46/64 loss: -0.1297140121459961
Batch 47/64 loss: -0.10995304584503174
Batch 48/64 loss: -0.12508904933929443
Batch 49/64 loss: -0.11028879880905151
Batch 50/64 loss: -0.09141337871551514
Batch 51/64 loss: -0.08845245838165283
Batch 52/64 loss: -0.09937471151351929
Batch 53/64 loss: -0.11614680290222168
Batch 54/64 loss: -0.1043858528137207
Batch 55/64 loss: -0.1089632511138916
Batch 56/64 loss: -0.12190920114517212
Batch 57/64 loss: -0.10634857416152954
Batch 58/64 loss: -0.13731175661087036
Batch 59/64 loss: -0.11156439781188965
Batch 60/64 loss: -0.11727774143218994
Batch 61/64 loss: -0.11356329917907715
Batch 62/64 loss: -0.12172973155975342
Batch 63/64 loss: -0.11513394117355347
Batch 64/64 loss: -0.08734136819839478
Epoch 94  Train loss: -0.1180603212001277  Val loss: 0.01427717675867769
Epoch 95
-------------------------------
Batch 1/64 loss: -0.1157732605934143
Batch 2/64 loss: -0.14931237697601318
Batch 3/64 loss: -0.14677447080612183
Batch 4/64 loss: -0.14911997318267822
Batch 5/64 loss: -0.12635421752929688
Batch 6/64 loss: -0.11626929044723511
Batch 7/64 loss: -0.13072431087493896
Batch 8/64 loss: -0.1539926528930664
Batch 9/64 loss: -0.13506650924682617
Batch 10/64 loss: -0.12928181886672974
Batch 11/64 loss: -0.11455357074737549
Batch 12/64 loss: -0.13704895973205566
Batch 13/64 loss: -0.09823441505432129
Batch 14/64 loss: -0.15280616283416748
Batch 15/64 loss: -0.1396772861480713
Batch 16/64 loss: -0.12760454416275024
Batch 17/64 loss: -0.08803129196166992
Batch 18/64 loss: -0.0987086296081543
Batch 19/64 loss: -0.10784387588500977
Batch 20/64 loss: -0.11802834272384644
Batch 21/64 loss: -0.12529844045639038
Batch 22/64 loss: -0.11679691076278687
Batch 23/64 loss: -0.09302592277526855
Batch 24/64 loss: -0.14659172296524048
Batch 25/64 loss: -0.12360012531280518
Batch 26/64 loss: -0.13032656908035278
Batch 27/64 loss: -0.1223335862159729
Batch 28/64 loss: -0.06270897388458252
Batch 29/64 loss: -0.1402454376220703
Batch 30/64 loss: -0.134100079536438
Batch 31/64 loss: -0.11635196208953857
Batch 32/64 loss: -0.11100506782531738
Batch 33/64 loss: -0.09274834394454956
Batch 34/64 loss: -0.13833045959472656
Batch 35/64 loss: -0.11818361282348633
Batch 36/64 loss: -0.11976248025894165
Batch 37/64 loss: -0.09917521476745605
Batch 38/64 loss: -0.12088936567306519
Batch 39/64 loss: -0.11527323722839355
Batch 40/64 loss: -0.14022034406661987
Batch 41/64 loss: -0.13847136497497559
Batch 42/64 loss: -0.1155121922492981
Batch 43/64 loss: -0.13737183809280396
Batch 44/64 loss: -0.10155439376831055
Batch 45/64 loss: -0.13493329286575317
Batch 46/64 loss: -0.12827885150909424
Batch 47/64 loss: -0.13909542560577393
Batch 48/64 loss: -0.0698314905166626
Batch 49/64 loss: -0.12736070156097412
Batch 50/64 loss: -0.13103997707366943
Batch 51/64 loss: -0.13954836130142212
Batch 52/64 loss: -0.12201470136642456
Batch 53/64 loss: -0.1214856505393982
Batch 54/64 loss: -0.11877197027206421
Batch 55/64 loss: -0.11988657712936401
Batch 56/64 loss: -0.10703146457672119
Batch 57/64 loss: -0.12529009580612183
Batch 58/64 loss: -0.12815868854522705
Batch 59/64 loss: -0.12881433963775635
Batch 60/64 loss: -0.1345040202140808
Batch 61/64 loss: -0.11708188056945801
Batch 62/64 loss: -0.09202677011489868
Batch 63/64 loss: -0.12540030479431152
Batch 64/64 loss: -0.09795868396759033
Epoch 95  Train loss: -0.12202521071714513  Val loss: 0.016137395322937325
Epoch 96
-------------------------------
Batch 1/64 loss: -0.11946582794189453
Batch 2/64 loss: -0.14954298734664917
Batch 3/64 loss: -0.1511014699935913
Batch 4/64 loss: -0.12586307525634766
Batch 5/64 loss: -0.11754125356674194
Batch 6/64 loss: -0.1171034574508667
Batch 7/64 loss: -0.09158885478973389
Batch 8/64 loss: -0.1502334475517273
Batch 9/64 loss: -0.1583118438720703
Batch 10/64 loss: -0.12071347236633301
Batch 11/64 loss: -0.14823800325393677
Batch 12/64 loss: -0.12359273433685303
Batch 13/64 loss: -0.13646996021270752
Batch 14/64 loss: -0.13535857200622559
Batch 15/64 loss: -0.13842976093292236
Batch 16/64 loss: -0.1550358533859253
Batch 17/64 loss: -0.1334075927734375
Batch 18/64 loss: -0.14125484228134155
Batch 19/64 loss: -0.09105414152145386
Batch 20/64 loss: -0.1226034164428711
Batch 21/64 loss: -0.12237656116485596
Batch 22/64 loss: -0.12310057878494263
Batch 23/64 loss: -0.13787031173706055
Batch 24/64 loss: -0.09209173917770386
Batch 25/64 loss: -0.10216039419174194
Batch 26/64 loss: -0.10315549373626709
Batch 27/64 loss: -0.1162109375
Batch 28/64 loss: -0.10592800378799438
Batch 29/64 loss: -0.11902326345443726
Batch 30/64 loss: -0.12792736291885376
Batch 31/64 loss: -0.1277540922164917
Batch 32/64 loss: -0.12292414903640747
Batch 33/64 loss: -0.13108789920806885
Batch 34/64 loss: -0.14253783226013184
Batch 35/64 loss: -0.07442361116409302
Batch 36/64 loss: -0.10884559154510498
Batch 37/64 loss: -0.1439143419265747
Batch 38/64 loss: -0.11874282360076904
Batch 39/64 loss: -0.12637794017791748
Batch 40/64 loss: -0.10926342010498047
Batch 41/64 loss: -0.09201192855834961
Batch 42/64 loss: -0.14988845586776733
Batch 43/64 loss: -0.1464274525642395
Batch 44/64 loss: -0.1128813624382019
Batch 45/64 loss: -0.13125449419021606
Batch 46/64 loss: -0.12219184637069702
Batch 47/64 loss: -0.11986750364303589
Batch 48/64 loss: -0.13005739450454712
Batch 49/64 loss: -0.139667809009552
Batch 50/64 loss: -0.08545809984207153
Batch 51/64 loss: -0.0936095118522644
Batch 52/64 loss: -0.1433313488960266
Batch 53/64 loss: -0.11990547180175781
Batch 54/64 loss: -0.11278021335601807
Batch 55/64 loss: -0.13864558935165405
Batch 56/64 loss: -0.11017674207687378
Batch 57/64 loss: -0.10639297962188721
Batch 58/64 loss: -0.09951388835906982
Batch 59/64 loss: -0.12645697593688965
Batch 60/64 loss: -0.14108580350875854
Batch 61/64 loss: -0.13896173238754272
Batch 62/64 loss: -0.12027525901794434
Batch 63/64 loss: -0.08019185066223145
Batch 64/64 loss: -0.14984798431396484
Epoch 96  Train loss: -0.12323210379656624  Val loss: 0.015332649458724609
Epoch 97
-------------------------------
Batch 1/64 loss: -0.14538729190826416
Batch 2/64 loss: -0.12132406234741211
Batch 3/64 loss: -0.131921648979187
Batch 4/64 loss: -0.12862569093704224
Batch 5/64 loss: -0.14640527963638306
Batch 6/64 loss: -0.12484115362167358
Batch 7/64 loss: -0.11294722557067871
Batch 8/64 loss: -0.1079971194267273
Batch 9/64 loss: -0.12662827968597412
Batch 10/64 loss: -0.13246899843215942
Batch 11/64 loss: -0.14657825231552124
Batch 12/64 loss: -0.13825345039367676
Batch 13/64 loss: -0.11097526550292969
Batch 14/64 loss: -0.1351485252380371
Batch 15/64 loss: -0.13548922538757324
Batch 16/64 loss: -0.10929489135742188
Batch 17/64 loss: -0.14215868711471558
Batch 18/64 loss: -0.09837532043457031
Batch 19/64 loss: -0.13445013761520386
Batch 20/64 loss: -0.13852888345718384
Batch 21/64 loss: -0.1329416036605835
Batch 22/64 loss: -0.11592322587966919
Batch 23/64 loss: -0.12522470951080322
Batch 24/64 loss: -0.1261228322982788
Batch 25/64 loss: -0.13828086853027344
Batch 26/64 loss: -0.09590721130371094
Batch 27/64 loss: -0.07672345638275146
Batch 28/64 loss: -0.11507964134216309
Batch 29/64 loss: -0.0989331603050232
Batch 30/64 loss: -0.1418137550354004
Batch 31/64 loss: -0.16044044494628906
Batch 32/64 loss: -0.12129032611846924
Batch 33/64 loss: -0.1210784912109375
Batch 34/64 loss: -0.1524031162261963
Batch 35/64 loss: -0.12435179948806763
Batch 36/64 loss: -0.13417989015579224
Batch 37/64 loss: -0.11366528272628784
Batch 38/64 loss: -0.12371385097503662
Batch 39/64 loss: -0.11069059371948242
Batch 40/64 loss: -0.13829469680786133
Batch 41/64 loss: -0.1082373857498169
Batch 42/64 loss: -0.13503670692443848
Batch 43/64 loss: -0.11961096525192261
Batch 44/64 loss: -0.13687723875045776
Batch 45/64 loss: -0.12617719173431396
Batch 46/64 loss: -0.12723642587661743
Batch 47/64 loss: -0.12901031970977783
Batch 48/64 loss: -0.12071537971496582
Batch 49/64 loss: -0.12866413593292236
Batch 50/64 loss: -0.12434989213943481
Batch 51/64 loss: -0.13817471265792847
Batch 52/64 loss: -0.1282789707183838
Batch 53/64 loss: -0.14333021640777588
Batch 54/64 loss: -0.1428707242012024
Batch 55/64 loss: -0.09922641515731812
Batch 56/64 loss: -0.12193858623504639
Batch 57/64 loss: -0.12819576263427734
Batch 58/64 loss: -0.10410845279693604
Batch 59/64 loss: -0.0957176685333252
Batch 60/64 loss: -0.10697221755981445
Batch 61/64 loss: -0.13953256607055664
Batch 62/64 loss: -0.0972757339477539
Batch 63/64 loss: -0.08489370346069336
Batch 64/64 loss: -0.1304551362991333
Epoch 97  Train loss: -0.12422166384902655  Val loss: 0.01663825540608147
Epoch 98
-------------------------------
Batch 1/64 loss: -0.12807369232177734
Batch 2/64 loss: -0.13915938138961792
Batch 3/64 loss: -0.1100015640258789
Batch 4/64 loss: -0.116466224193573
Batch 5/64 loss: -0.15301638841629028
Batch 6/64 loss: -0.09470832347869873
Batch 7/64 loss: -0.11385244131088257
Batch 8/64 loss: -0.1380864977836609
Batch 9/64 loss: -0.13021546602249146
Batch 10/64 loss: -0.13802504539489746
Batch 11/64 loss: -0.1262967586517334
Batch 12/64 loss: -0.11767011880874634
Batch 13/64 loss: -0.14040279388427734
Batch 14/64 loss: -0.15265107154846191
Batch 15/64 loss: -0.11639988422393799
Batch 16/64 loss: -0.13091105222702026
Batch 17/64 loss: -0.1366611123085022
Batch 18/64 loss: -0.12882506847381592
Batch 19/64 loss: -0.15386927127838135
Batch 20/64 loss: -0.1272038221359253
Batch 21/64 loss: -0.10538411140441895
Batch 22/64 loss: -0.14744752645492554
Batch 23/64 loss: -0.11183398962020874
Batch 24/64 loss: -0.1009642481803894
Batch 25/64 loss: -0.14642810821533203
Batch 26/64 loss: -0.13698506355285645
Batch 27/64 loss: -0.1301184892654419
Batch 28/64 loss: -0.12391358613967896
Batch 29/64 loss: -0.12316763401031494
Batch 30/64 loss: -0.0890740156173706
Batch 31/64 loss: -0.10634642839431763
Batch 32/64 loss: -0.1460077166557312
Batch 33/64 loss: -0.1280437707901001
Batch 34/64 loss: -0.12273526191711426
Batch 35/64 loss: -0.11819589138031006
Batch 36/64 loss: -0.12781471014022827
Batch 37/64 loss: -0.10305595397949219
Batch 38/64 loss: -0.12443727254867554
Batch 39/64 loss: -0.13753902912139893
Batch 40/64 loss: -0.13335174322128296
Batch 41/64 loss: -0.1557024121284485
Batch 42/64 loss: -0.1340322494506836
Batch 43/64 loss: -0.12649160623550415
Batch 44/64 loss: -0.13181138038635254
Batch 45/64 loss: -0.13725519180297852
Batch 46/64 loss: -0.1147909164428711
Batch 47/64 loss: -0.12359344959259033
Batch 48/64 loss: -0.12314492464065552
Batch 49/64 loss: -0.11148959398269653
Batch 50/64 loss: -0.14195066690444946
Batch 51/64 loss: -0.12641286849975586
Batch 52/64 loss: -0.13074177503585815
Batch 53/64 loss: -0.0976262092590332
Batch 54/64 loss: -0.14290720224380493
Batch 55/64 loss: -0.12774240970611572
Batch 56/64 loss: -0.13377732038497925
Batch 57/64 loss: -0.1392989158630371
Batch 58/64 loss: -0.1439206600189209
Batch 59/64 loss: -0.1219872236251831
Batch 60/64 loss: -0.13123828172683716
Batch 61/64 loss: -0.14544755220413208
Batch 62/64 loss: -0.12674212455749512
Batch 63/64 loss: -0.13157176971435547
Batch 64/64 loss: -0.14603710174560547
Epoch 98  Train loss: -0.12807129504633885  Val loss: 0.01641801132778941
Epoch 99
-------------------------------
Batch 1/64 loss: -0.11405825614929199
Batch 2/64 loss: -0.12502062320709229
Batch 3/64 loss: -0.13156872987747192
Batch 4/64 loss: -0.13665664196014404
Batch 5/64 loss: -0.12127029895782471
Batch 6/64 loss: -0.14866065979003906
Batch 7/64 loss: -0.14570379257202148
Batch 8/64 loss: -0.12462276220321655
Batch 9/64 loss: -0.14047962427139282
Batch 10/64 loss: -0.12473762035369873
Batch 11/64 loss: -0.1246342658996582
Batch 12/64 loss: -0.1379227638244629
Batch 13/64 loss: -0.12992453575134277
Batch 14/64 loss: -0.1330627202987671
Batch 15/64 loss: -0.13251334428787231
Batch 16/64 loss: -0.11246597766876221
Batch 17/64 loss: -0.13041341304779053
Batch 18/64 loss: -0.1474074125289917
Batch 19/64 loss: -0.09367197751998901
Batch 20/64 loss: -0.1246182918548584
Batch 21/64 loss: -0.11805778741836548
Batch 22/64 loss: -0.13297683000564575
Batch 23/64 loss: -0.14041227102279663
Batch 24/64 loss: -0.13529640436172485
Batch 25/64 loss: -0.14465278387069702
Batch 26/64 loss: -0.14561885595321655
Batch 27/64 loss: -0.13196241855621338
Batch 28/64 loss: -0.11507701873779297
Batch 29/64 loss: -0.1195831298828125
Batch 30/64 loss: -0.16508805751800537
Batch 31/64 loss: -0.13900285959243774
Batch 32/64 loss: -0.13830536603927612
Batch 33/64 loss: -0.11146372556686401
Batch 34/64 loss: -0.12914037704467773
Batch 35/64 loss: -0.13213860988616943
Batch 36/64 loss: -0.14550364017486572
Batch 37/64 loss: -0.1379256248474121
Batch 38/64 loss: -0.13949865102767944
Batch 39/64 loss: -0.1421467661857605
Batch 40/64 loss: -0.14401555061340332
Batch 41/64 loss: -0.12503618001937866
Batch 42/64 loss: -0.12865865230560303
Batch 43/64 loss: -0.14042776823043823
Batch 44/64 loss: -0.1400476098060608
Batch 45/64 loss: -0.16266030073165894
Batch 46/64 loss: -0.14089679718017578
Batch 47/64 loss: -0.12472891807556152
Batch 48/64 loss: -0.14535874128341675
Batch 49/64 loss: -0.11986696720123291
Batch 50/64 loss: -0.11253511905670166
Batch 51/64 loss: -0.117922842502594
Batch 52/64 loss: -0.12158894538879395
Batch 53/64 loss: -0.08965879678726196
Batch 54/64 loss: -0.09393268823623657
Batch 55/64 loss: -0.14693140983581543
Batch 56/64 loss: -0.12157338857650757
Batch 57/64 loss: -0.13267940282821655
Batch 58/64 loss: -0.13023847341537476
Batch 59/64 loss: -0.11767935752868652
Batch 60/64 loss: -0.1659250259399414
Batch 61/64 loss: -0.10927391052246094
Batch 62/64 loss: -0.13218796253204346
Batch 63/64 loss: -0.11757522821426392
Batch 64/64 loss: -0.1431688666343689
Epoch 99  Train loss: -0.13069869139615228  Val loss: 0.014948412724786607
Epoch 100
-------------------------------
Batch 1/64 loss: -0.13675802946090698
Batch 2/64 loss: -0.1632598638534546
Batch 3/64 loss: -0.12411385774612427
Batch 4/64 loss: -0.14688396453857422
Batch 5/64 loss: -0.13502806425094604
Batch 6/64 loss: -0.15383845567703247
Batch 7/64 loss: -0.15749329328536987
Batch 8/64 loss: -0.14843451976776123
Batch 9/64 loss: -0.13421696424484253
Batch 10/64 loss: -0.061316728591918945
Batch 11/64 loss: -0.152082622051239
Batch 12/64 loss: -0.14401555061340332
Batch 13/64 loss: -0.1517733335494995
Batch 14/64 loss: -0.12683427333831787
Batch 15/64 loss: -0.14108335971832275
Batch 16/64 loss: -0.15111064910888672
Batch 17/64 loss: -0.14845162630081177
Batch 18/64 loss: -0.13390189409255981
Batch 19/64 loss: -0.17302507162094116
Batch 20/64 loss: -0.15200239419937134
Batch 21/64 loss: -0.1413942575454712
Batch 22/64 loss: -0.12395715713500977
Batch 23/64 loss: -0.14799946546554565
Batch 24/64 loss: -0.1398075819015503
Batch 25/64 loss: -0.13894927501678467
Batch 26/64 loss: -0.1365818977355957
Batch 27/64 loss: -0.13062119483947754
Batch 28/64 loss: -0.134682297706604
Batch 29/64 loss: -0.12644141912460327
Batch 30/64 loss: -0.12399542331695557
Batch 31/64 loss: -0.11552006006240845
Batch 32/64 loss: -0.13102567195892334
Batch 33/64 loss: -0.1402806043624878
Batch 34/64 loss: -0.10756146907806396
Batch 35/64 loss: -0.14699137210845947
Batch 36/64 loss: -0.09802889823913574
Batch 37/64 loss: -0.13049739599227905
Batch 38/64 loss: -0.13750606775283813
Batch 39/64 loss: -0.10462868213653564
Batch 40/64 loss: -0.11616301536560059
Batch 41/64 loss: -0.11160528659820557
Batch 42/64 loss: -0.1407712697982788
Batch 43/64 loss: -0.11698877811431885
Batch 44/64 loss: -0.1324080228805542
Batch 45/64 loss: -0.12485748529434204
Batch 46/64 loss: -0.08582860231399536
Batch 47/64 loss: -0.11521422863006592
Batch 48/64 loss: -0.13727223873138428
Batch 49/64 loss: -0.1453688144683838
Batch 50/64 loss: -0.10966390371322632
Batch 51/64 loss: -0.12709885835647583
Batch 52/64 loss: -0.08380812406539917
Batch 53/64 loss: -0.1347392201423645
Batch 54/64 loss: -0.1353704333305359
Batch 55/64 loss: -0.11395293474197388
Batch 56/64 loss: -0.13226819038391113
Batch 57/64 loss: -0.1001894474029541
Batch 58/64 loss: -0.15420269966125488
Batch 59/64 loss: -0.11004555225372314
Batch 60/64 loss: -0.08609014749526978
Batch 61/64 loss: -0.1326441764831543
Batch 62/64 loss: -0.13437354564666748
Batch 63/64 loss: -0.15352118015289307
Batch 64/64 loss: -0.11221212148666382
Epoch 100  Train loss: -0.1303639208569246  Val loss: 0.014032634468013068
Epoch 101
-------------------------------
Batch 1/64 loss: -0.12607455253601074
Batch 2/64 loss: -0.13262462615966797
Batch 3/64 loss: -0.12683546543121338
Batch 4/64 loss: -0.13998472690582275
Batch 5/64 loss: -0.14541304111480713
Batch 6/64 loss: -0.1200176477432251
Batch 7/64 loss: -0.15006816387176514
Batch 8/64 loss: -0.1524449586868286
Batch 9/64 loss: -0.12270444631576538
Batch 10/64 loss: -0.12939667701721191
Batch 11/64 loss: -0.15523701906204224
Batch 12/64 loss: -0.1266111135482788
Batch 13/64 loss: -0.11708986759185791
Batch 14/64 loss: -0.1539105772972107
Batch 15/64 loss: -0.15537136793136597
Batch 16/64 loss: -0.12899792194366455
Batch 17/64 loss: -0.12288457155227661
Batch 18/64 loss: -0.14557528495788574
Batch 19/64 loss: -0.15040427446365356
Batch 20/64 loss: -0.12271815538406372
Batch 21/64 loss: -0.1410314440727234
Batch 22/64 loss: -0.10701543092727661
Batch 23/64 loss: -0.09561145305633545
Batch 24/64 loss: -0.12135028839111328
Batch 25/64 loss: -0.1384480595588684
Batch 26/64 loss: -0.11322546005249023
Batch 27/64 loss: -0.15086239576339722
Batch 28/64 loss: -0.1407049298286438
Batch 29/64 loss: -0.1007876992225647
Batch 30/64 loss: -0.1546139121055603
Batch 31/64 loss: -0.12161034345626831
Batch 32/64 loss: -0.14596831798553467
Batch 33/64 loss: -0.15137451887130737
Batch 34/64 loss: -0.12672168016433716
Batch 35/64 loss: -0.10983294248580933
Batch 36/64 loss: -0.12347275018692017
Batch 37/64 loss: -0.13808900117874146
Batch 38/64 loss: -0.10820168256759644
Batch 39/64 loss: -0.1183614730834961
Batch 40/64 loss: -0.120292067527771
Batch 41/64 loss: -0.1229981780052185
Batch 42/64 loss: -0.13252174854278564
Batch 43/64 loss: -0.16141974925994873
Batch 44/64 loss: -0.12006360292434692
Batch 45/64 loss: -0.12633532285690308
Batch 46/64 loss: -0.14522629976272583
Batch 47/64 loss: -0.1346815824508667
Batch 48/64 loss: -0.14412420988082886
Batch 49/64 loss: -0.13252204656600952
Batch 50/64 loss: -0.1105351448059082
Batch 51/64 loss: -0.11994469165802002
Batch 52/64 loss: -0.15824902057647705
Batch 53/64 loss: -0.15068131685256958
Batch 54/64 loss: -0.15264058113098145
Batch 55/64 loss: -0.14443010091781616
Batch 56/64 loss: -0.1429537534713745
Batch 57/64 loss: -0.1339835524559021
Batch 58/64 loss: -0.13514947891235352
Batch 59/64 loss: -0.1257932186126709
Batch 60/64 loss: -0.12085318565368652
Batch 61/64 loss: -0.11167538166046143
Batch 62/64 loss: -0.09185469150543213
Batch 63/64 loss: -0.12267345190048218
Batch 64/64 loss: -0.1392505168914795
Epoch 101  Train loss: -0.13172844718484317  Val loss: 0.015079091299850097
Epoch 102
-------------------------------
Batch 1/64 loss: -0.1149512529373169
Batch 2/64 loss: -0.153844952583313
Batch 3/64 loss: -0.1679784655570984
Batch 4/64 loss: -0.09434700012207031
Batch 5/64 loss: -0.1458379626274109
Batch 6/64 loss: -0.13993269205093384
Batch 7/64 loss: -0.12919926643371582
Batch 8/64 loss: -0.09622269868850708
Batch 9/64 loss: -0.14592623710632324
Batch 10/64 loss: -0.1335086226463318
Batch 11/64 loss: -0.12455976009368896
Batch 12/64 loss: -0.13629138469696045
Batch 13/64 loss: -0.11675947904586792
Batch 14/64 loss: -0.136027991771698
Batch 15/64 loss: -0.12911731004714966
Batch 16/64 loss: -0.13431644439697266
Batch 17/64 loss: -0.13679641485214233
Batch 18/64 loss: -0.12192869186401367
Batch 19/64 loss: -0.09993308782577515
Batch 20/64 loss: -0.16022545099258423
Batch 21/64 loss: -0.10088825225830078
Batch 22/64 loss: -0.13928765058517456
Batch 23/64 loss: -0.1474912166595459
Batch 24/64 loss: -0.1410406231880188
Batch 25/64 loss: -0.1506648063659668
Batch 26/64 loss: -0.1581065058708191
Batch 27/64 loss: -0.13925766944885254
Batch 28/64 loss: -0.11704301834106445
Batch 29/64 loss: -0.12072902917861938
Batch 30/64 loss: -0.10034483671188354
Batch 31/64 loss: -0.11910474300384521
Batch 32/64 loss: -0.1260741949081421
Batch 33/64 loss: -0.1463286280632019
Batch 34/64 loss: -0.16108906269073486
Batch 35/64 loss: -0.13419979810714722
Batch 36/64 loss: -0.17642742395401
Batch 37/64 loss: -0.1461765170097351
Batch 38/64 loss: -0.14556890726089478
Batch 39/64 loss: -0.1347251534461975
Batch 40/64 loss: -0.15506893396377563
Batch 41/64 loss: -0.1486889123916626
Batch 42/64 loss: -0.11492973566055298
Batch 43/64 loss: -0.13193732500076294
Batch 44/64 loss: -0.1165420413017273
Batch 45/64 loss: -0.14933288097381592
Batch 46/64 loss: -0.14806455373764038
Batch 47/64 loss: -0.14224845170974731
Batch 48/64 loss: -0.11887753009796143
Batch 49/64 loss: -0.1478492021560669
Batch 50/64 loss: -0.11564850807189941
Batch 51/64 loss: -0.12165850400924683
Batch 52/64 loss: -0.1235353946685791
Batch 53/64 loss: -0.13736194372177124
Batch 54/64 loss: -0.1417277455329895
Batch 55/64 loss: -0.1490551233291626
Batch 56/64 loss: -0.14117342233657837
Batch 57/64 loss: -0.14037907123565674
Batch 58/64 loss: -0.12278199195861816
Batch 59/64 loss: -0.13979458808898926
Batch 60/64 loss: -0.12094712257385254
Batch 61/64 loss: -0.13905739784240723
Batch 62/64 loss: -0.13403773307800293
Batch 63/64 loss: -0.08371728658676147
Batch 64/64 loss: -0.15008479356765747
Epoch 102  Train loss: -0.13363501487993726  Val loss: 0.01721312950566872
Epoch 103
-------------------------------
Batch 1/64 loss: -0.15773475170135498
Batch 2/64 loss: -0.1264393925666809
Batch 3/64 loss: -0.12789827585220337
Batch 4/64 loss: -0.14800524711608887
Batch 5/64 loss: -0.15211862325668335
Batch 6/64 loss: -0.11311537027359009
Batch 7/64 loss: -0.15277099609375
Batch 8/64 loss: -0.12897133827209473
Batch 9/64 loss: -0.10481894016265869
Batch 10/64 loss: -0.13418036699295044
Batch 11/64 loss: -0.1344759464263916
Batch 12/64 loss: -0.11982274055480957
Batch 13/64 loss: -0.132285475730896
Batch 14/64 loss: -0.14741581678390503
Batch 15/64 loss: -0.16048848628997803
Batch 16/64 loss: -0.13785558938980103
Batch 17/64 loss: -0.13551044464111328
Batch 18/64 loss: -0.10566544532775879
Batch 19/64 loss: -0.1417539119720459
Batch 20/64 loss: -0.15443801879882812
Batch 21/64 loss: -0.13172763586044312
Batch 22/64 loss: -0.16361993551254272
Batch 23/64 loss: -0.13337236642837524
Batch 24/64 loss: -0.13785487413406372
Batch 25/64 loss: -0.11462384462356567
Batch 26/64 loss: -0.10660398006439209
Batch 27/64 loss: -0.1466602087020874
Batch 28/64 loss: -0.13124877214431763
Batch 29/64 loss: -0.11532789468765259
Batch 30/64 loss: -0.14833134412765503
Batch 31/64 loss: -0.150107741355896
Batch 32/64 loss: -0.13222312927246094
Batch 33/64 loss: -0.14665436744689941
Batch 34/64 loss: -0.1251075267791748
Batch 35/64 loss: -0.1402032971382141
Batch 36/64 loss: -0.11099100112915039
Batch 37/64 loss: -0.13682907819747925
Batch 38/64 loss: -0.1251283884048462
Batch 39/64 loss: -0.13892346620559692
Batch 40/64 loss: -0.11293023824691772
Batch 41/64 loss: -0.11607682704925537
Batch 42/64 loss: -0.14844560623168945
Batch 43/64 loss: -0.14722418785095215
Batch 44/64 loss: -0.11421668529510498
Batch 45/64 loss: -0.12924069166183472
Batch 46/64 loss: -0.12762445211410522
Batch 47/64 loss: -0.11878681182861328
Batch 48/64 loss: -0.10881954431533813
Batch 49/64 loss: -0.14832091331481934
Batch 50/64 loss: -0.1265808343887329
Batch 51/64 loss: -0.15114480257034302
Batch 52/64 loss: -0.1449156403541565
Batch 53/64 loss: -0.12582260370254517
Batch 54/64 loss: -0.15335756540298462
Batch 55/64 loss: -0.13584333658218384
Batch 56/64 loss: -0.13782435655593872
Batch 57/64 loss: -0.12346440553665161
Batch 58/64 loss: -0.15573710203170776
Batch 59/64 loss: -0.15681695938110352
Batch 60/64 loss: -0.1571875810623169
Batch 61/64 loss: -0.15152239799499512
Batch 62/64 loss: -0.1392425298690796
Batch 63/64 loss: -0.11903470754623413
Batch 64/64 loss: -0.11974817514419556
Epoch 103  Train loss: -0.13476542655159446  Val loss: 0.016864373511874798
Epoch 104
-------------------------------
Batch 1/64 loss: -0.15175998210906982
Batch 2/64 loss: -0.11571383476257324
Batch 3/64 loss: -0.14592957496643066
Batch 4/64 loss: -0.13565337657928467
Batch 5/64 loss: -0.12328243255615234
Batch 6/64 loss: -0.1603705883026123
Batch 7/64 loss: -0.10370784997940063
Batch 8/64 loss: -0.13442403078079224
Batch 9/64 loss: -0.14573967456817627
Batch 10/64 loss: -0.13051915168762207
Batch 11/64 loss: -0.13213402032852173
Batch 12/64 loss: -0.14479106664657593
Batch 13/64 loss: -0.13458788394927979
Batch 14/64 loss: -0.12751400470733643
Batch 15/64 loss: -0.12735825777053833
Batch 16/64 loss: -0.1442939043045044
Batch 17/64 loss: -0.1479332447052002
Batch 18/64 loss: -0.14981788396835327
Batch 19/64 loss: -0.14954376220703125
Batch 20/64 loss: -0.15007615089416504
Batch 21/64 loss: -0.14123284816741943
Batch 22/64 loss: -0.1333133578300476
Batch 23/64 loss: -0.12641209363937378
Batch 24/64 loss: -0.1445178985595703
Batch 25/64 loss: -0.16196948289871216
Batch 26/64 loss: -0.12505978345870972
Batch 27/64 loss: -0.10373330116271973
Batch 28/64 loss: -0.1620740294456482
Batch 29/64 loss: -0.14716410636901855
Batch 30/64 loss: -0.15210843086242676
Batch 31/64 loss: -0.08591175079345703
Batch 32/64 loss: -0.14308536052703857
Batch 33/64 loss: -0.11973607540130615
Batch 34/64 loss: -0.12177455425262451
Batch 35/64 loss: -0.1273166537284851
Batch 36/64 loss: -0.11664652824401855
Batch 37/64 loss: -0.15985578298568726
Batch 38/64 loss: -0.1484978199005127
Batch 39/64 loss: -0.14564871788024902
Batch 40/64 loss: -0.14974737167358398
Batch 41/64 loss: -0.14679253101348877
Batch 42/64 loss: -0.14534401893615723
Batch 43/64 loss: -0.12358605861663818
Batch 44/64 loss: -0.11862534284591675
Batch 45/64 loss: -0.1162070631980896
Batch 46/64 loss: -0.14201045036315918
Batch 47/64 loss: -0.1498158574104309
Batch 48/64 loss: -0.14450383186340332
Batch 49/64 loss: -0.11979842185974121
Batch 50/64 loss: -0.14171969890594482
Batch 51/64 loss: -0.10706734657287598
Batch 52/64 loss: -0.1158592700958252
Batch 53/64 loss: -0.14007568359375
Batch 54/64 loss: -0.13364994525909424
Batch 55/64 loss: -0.11811208724975586
Batch 56/64 loss: -0.13989782333374023
Batch 57/64 loss: -0.14395135641098022
Batch 58/64 loss: -0.15201377868652344
Batch 59/64 loss: -0.10921615362167358
Batch 60/64 loss: -0.1527753472328186
Batch 61/64 loss: -0.12538766860961914
Batch 62/64 loss: -0.15494823455810547
Batch 63/64 loss: -0.1362607479095459
Batch 64/64 loss: -0.1555144190788269
Epoch 104  Train loss: -0.13592488087859808  Val loss: 0.01630340040344553
Epoch 105
-------------------------------
Batch 1/64 loss: -0.1299014687538147
Batch 2/64 loss: -0.1602855920791626
Batch 3/64 loss: -0.15882670879364014
Batch 4/64 loss: -0.09829705953598022
Batch 5/64 loss: -0.10979568958282471
Batch 6/64 loss: -0.1764102578163147
Batch 7/64 loss: -0.15597856044769287
Batch 8/64 loss: -0.17204177379608154
Batch 9/64 loss: -0.15883994102478027
Batch 10/64 loss: -0.1565898060798645
Batch 11/64 loss: -0.1344754695892334
Batch 12/64 loss: -0.12908679246902466
Batch 13/64 loss: -0.14564251899719238
Batch 14/64 loss: -0.16973721981048584
Batch 15/64 loss: -0.14907312393188477
Batch 16/64 loss: -0.14419573545455933
Batch 17/64 loss: -0.1427597999572754
Batch 18/64 loss: -0.14532405138015747
Batch 19/64 loss: -0.12920069694519043
Batch 20/64 loss: -0.13034182786941528
Batch 21/64 loss: -0.12997233867645264
Batch 22/64 loss: -0.16062504053115845
Batch 23/64 loss: -0.14386451244354248
Batch 24/64 loss: -0.12202572822570801
Batch 25/64 loss: -0.1331620216369629
Batch 26/64 loss: -0.14422345161437988
Batch 27/64 loss: -0.12517660856246948
Batch 28/64 loss: -0.13641369342803955
Batch 29/64 loss: -0.1261383295059204
Batch 30/64 loss: -0.12946468591690063
Batch 31/64 loss: -0.16081511974334717
Batch 32/64 loss: -0.1183590292930603
Batch 33/64 loss: -0.12689977884292603
Batch 34/64 loss: -0.1582167148590088
Batch 35/64 loss: -0.11750859022140503
Batch 36/64 loss: -0.11644911766052246
Batch 37/64 loss: -0.12574005126953125
Batch 38/64 loss: -0.1564461588859558
Batch 39/64 loss: -0.15908312797546387
Batch 40/64 loss: -0.10582810640335083
Batch 41/64 loss: -0.1196906566619873
Batch 42/64 loss: -0.13891011476516724
Batch 43/64 loss: -0.13550877571105957
Batch 44/64 loss: -0.16608422994613647
Batch 45/64 loss: -0.1620686650276184
Batch 46/64 loss: -0.167375385761261
Batch 47/64 loss: -0.142051100730896
Batch 48/64 loss: -0.13494789600372314
Batch 49/64 loss: -0.14027392864227295
Batch 50/64 loss: -0.14643102884292603
Batch 51/64 loss: -0.12621170282363892
Batch 52/64 loss: -0.14687001705169678
Batch 53/64 loss: -0.12656527757644653
Batch 54/64 loss: -0.16154998540878296
Batch 55/64 loss: -0.12730896472930908
Batch 56/64 loss: -0.13610059022903442
Batch 57/64 loss: -0.15486598014831543
Batch 58/64 loss: -0.14291709661483765
Batch 59/64 loss: -0.16464418172836304
Batch 60/64 loss: -0.13020098209381104
Batch 61/64 loss: -0.14662957191467285
Batch 62/64 loss: -0.12353718280792236
Batch 63/64 loss: -0.12951445579528809
Batch 64/64 loss: -0.12022823095321655
Epoch 105  Train loss: -0.14044933716456096  Val loss: 0.015262009966414409
Epoch 106
-------------------------------
Batch 1/64 loss: -0.13654106855392456
Batch 2/64 loss: -0.12180560827255249
Batch 3/64 loss: -0.16347992420196533
Batch 4/64 loss: -0.16127997636795044
Batch 5/64 loss: -0.13281697034835815
Batch 6/64 loss: -0.10698056221008301
Batch 7/64 loss: -0.12692034244537354
Batch 8/64 loss: -0.1524066925048828
Batch 9/64 loss: -0.09839534759521484
Batch 10/64 loss: -0.17390358448028564
Batch 11/64 loss: -0.14771497249603271
Batch 12/64 loss: -0.16059988737106323
Batch 13/64 loss: -0.14379620552062988
Batch 14/64 loss: -0.16245031356811523
Batch 15/64 loss: -0.1435123085975647
Batch 16/64 loss: -0.13495397567749023
Batch 17/64 loss: -0.15202456712722778
Batch 18/64 loss: -0.15102577209472656
Batch 19/64 loss: -0.16378605365753174
Batch 20/64 loss: -0.1667817234992981
Batch 21/64 loss: -0.138988196849823
Batch 22/64 loss: -0.1661771535873413
Batch 23/64 loss: -0.15774130821228027
Batch 24/64 loss: -0.1546657681465149
Batch 25/64 loss: -0.14685726165771484
Batch 26/64 loss: -0.14173799753189087
Batch 27/64 loss: -0.1136583685874939
Batch 28/64 loss: -0.10510110855102539
Batch 29/64 loss: -0.14014595746994019
Batch 30/64 loss: -0.1449277400970459
Batch 31/64 loss: -0.1444646716117859
Batch 32/64 loss: -0.12776339054107666
Batch 33/64 loss: -0.16596925258636475
Batch 34/64 loss: -0.13473498821258545
Batch 35/64 loss: -0.13561147451400757
Batch 36/64 loss: -0.11030077934265137
Batch 37/64 loss: -0.1080622673034668
Batch 38/64 loss: -0.17614173889160156
Batch 39/64 loss: -0.1650891900062561
Batch 40/64 loss: -0.12062668800354004
Batch 41/64 loss: -0.1588495373725891
Batch 42/64 loss: -0.1287727952003479
Batch 43/64 loss: -0.16214656829833984
Batch 44/64 loss: -0.13970822095870972
Batch 45/64 loss: -0.1374555230140686
Batch 46/64 loss: -0.13408023118972778
Batch 47/64 loss: -0.14624005556106567
Batch 48/64 loss: -0.13714873790740967
Batch 49/64 loss: -0.15270304679870605
Batch 50/64 loss: -0.1305796504020691
Batch 51/64 loss: -0.12225043773651123
Batch 52/64 loss: -0.15154516696929932
Batch 53/64 loss: -0.12662434577941895
Batch 54/64 loss: -0.13594657182693481
Batch 55/64 loss: -0.1124410629272461
Batch 56/64 loss: -0.1175764799118042
Batch 57/64 loss: -0.12831562757492065
Batch 58/64 loss: -0.1272321343421936
Batch 59/64 loss: -0.13511335849761963
Batch 60/64 loss: -0.09969979524612427
Batch 61/64 loss: -0.12999790906906128
Batch 62/64 loss: -0.085105299949646
Batch 63/64 loss: -0.16653800010681152
Batch 64/64 loss: -0.14098525047302246
Epoch 106  Train loss: -0.13916471612219716  Val loss: 0.015812106968201314
Epoch 107
-------------------------------
Batch 1/64 loss: -0.14076566696166992
Batch 2/64 loss: -0.13151144981384277
Batch 3/64 loss: -0.1637689471244812
Batch 4/64 loss: -0.13560575246810913
Batch 5/64 loss: -0.13945329189300537
Batch 6/64 loss: -0.12104082107543945
Batch 7/64 loss: -0.11346465349197388
Batch 8/64 loss: -0.1471954584121704
Batch 9/64 loss: -0.14094901084899902
Batch 10/64 loss: -0.16737693548202515
Batch 11/64 loss: -0.16297638416290283
Batch 12/64 loss: -0.1455627679824829
Batch 13/64 loss: -0.13425815105438232
Batch 14/64 loss: -0.15159589052200317
Batch 15/64 loss: -0.15792715549468994
Batch 16/64 loss: -0.14134281873703003
Batch 17/64 loss: -0.11375021934509277
Batch 18/64 loss: -0.14914685487747192
Batch 19/64 loss: -0.15133309364318848
Batch 20/64 loss: -0.1544739007949829
Batch 21/64 loss: -0.12690645456314087
Batch 22/64 loss: -0.1481332778930664
Batch 23/64 loss: -0.143815815448761
Batch 24/64 loss: -0.14825284481048584
Batch 25/64 loss: -0.14588916301727295
Batch 26/64 loss: -0.14996790885925293
Batch 27/64 loss: -0.137282133102417
Batch 28/64 loss: -0.13862907886505127
Batch 29/64 loss: -0.1434994339942932
Batch 30/64 loss: -0.14068281650543213
Batch 31/64 loss: -0.13060259819030762
Batch 32/64 loss: -0.15378618240356445
Batch 33/64 loss: -0.1325170397758484
Batch 34/64 loss: -0.1487870216369629
Batch 35/64 loss: -0.12755531072616577
Batch 36/64 loss: -0.14834582805633545
Batch 37/64 loss: -0.16225934028625488
Batch 38/64 loss: -0.13781166076660156
Batch 39/64 loss: -0.16959816217422485
Batch 40/64 loss: -0.1695360541343689
Batch 41/64 loss: -0.13861459493637085
Batch 42/64 loss: -0.15248644351959229
Batch 43/64 loss: -0.12767058610916138
Batch 44/64 loss: -0.13697749376296997
Batch 45/64 loss: -0.13974899053573608
Batch 46/64 loss: -0.14619910717010498
Batch 47/64 loss: -0.1264895796775818
Batch 48/64 loss: -0.15225207805633545
Batch 49/64 loss: -0.12811732292175293
Batch 50/64 loss: -0.13654446601867676
Batch 51/64 loss: -0.10435587167739868
Batch 52/64 loss: -0.10313284397125244
Batch 53/64 loss: -0.16483670473098755
Batch 54/64 loss: -0.16038179397583008
Batch 55/64 loss: -0.08885079622268677
Batch 56/64 loss: -0.1294395923614502
Batch 57/64 loss: -0.1415751576423645
Batch 58/64 loss: -0.1430603265762329
Batch 59/64 loss: -0.14763247966766357
Batch 60/64 loss: -0.16844326257705688
Batch 61/64 loss: -0.11135542392730713
Batch 62/64 loss: -0.11956906318664551
Batch 63/64 loss: -0.13215315341949463
Batch 64/64 loss: -0.15696406364440918
Epoch 107  Train loss: -0.1409406671337053  Val loss: 0.01992573033493409
Epoch 108
-------------------------------
Batch 1/64 loss: -0.12601596117019653
Batch 2/64 loss: -0.12739980220794678
Batch 3/64 loss: -0.11989766359329224
Batch 4/64 loss: -0.11832201480865479
Batch 5/64 loss: -0.13303542137145996
Batch 6/64 loss: -0.11963677406311035
Batch 7/64 loss: -0.14736342430114746
Batch 8/64 loss: -0.15673381090164185
Batch 9/64 loss: -0.14964187145233154
Batch 10/64 loss: -0.08684885501861572
Batch 11/64 loss: -0.1596800684928894
Batch 12/64 loss: -0.15041643381118774
Batch 13/64 loss: -0.153967022895813
Batch 14/64 loss: -0.16756510734558105
Batch 15/64 loss: -0.1431223750114441
Batch 16/64 loss: -0.15697336196899414
Batch 17/64 loss: -0.144672691822052
Batch 18/64 loss: -0.17220288515090942
Batch 19/64 loss: -0.15369898080825806
Batch 20/64 loss: -0.15806132555007935
Batch 21/64 loss: -0.14511644840240479
Batch 22/64 loss: -0.17191660404205322
Batch 23/64 loss: -0.12207663059234619
Batch 24/64 loss: -0.15214532613754272
Batch 25/64 loss: -0.13699012994766235
Batch 26/64 loss: -0.1419880986213684
Batch 27/64 loss: -0.14392882585525513
Batch 28/64 loss: -0.15657132863998413
Batch 29/64 loss: -0.16620492935180664
Batch 30/64 loss: -0.14938825368881226
Batch 31/64 loss: -0.1677013635635376
Batch 32/64 loss: -0.13798904418945312
Batch 33/64 loss: -0.12461018562316895
Batch 34/64 loss: -0.126106858253479
Batch 35/64 loss: -0.1178404688835144
Batch 36/64 loss: -0.1284419298171997
Batch 37/64 loss: -0.14210718870162964
Batch 38/64 loss: -0.15784573554992676
Batch 39/64 loss: -0.14487868547439575
Batch 40/64 loss: -0.12960398197174072
Batch 41/64 loss: -0.11882412433624268
Batch 42/64 loss: -0.14128220081329346
Batch 43/64 loss: -0.14830684661865234
Batch 44/64 loss: -0.13575714826583862
Batch 45/64 loss: -0.1464824080467224
Batch 46/64 loss: -0.15234875679016113
Batch 47/64 loss: -0.16634440422058105
Batch 48/64 loss: -0.18222934007644653
Batch 49/64 loss: -0.16253739595413208
Batch 50/64 loss: -0.16039687395095825
Batch 51/64 loss: -0.1669483184814453
Batch 52/64 loss: -0.14564985036849976
Batch 53/64 loss: -0.14492034912109375
Batch 54/64 loss: -0.1663522720336914
Batch 55/64 loss: -0.12512165307998657
Batch 56/64 loss: -0.12671619653701782
Batch 57/64 loss: -0.12569576501846313
Batch 58/64 loss: -0.14046871662139893
Batch 59/64 loss: -0.1417505145072937
Batch 60/64 loss: -0.14775127172470093
Batch 61/64 loss: -0.09072351455688477
Batch 62/64 loss: -0.10935759544372559
Batch 63/64 loss: -0.13976120948791504
Batch 64/64 loss: -0.13514220714569092
Epoch 108  Train loss: -0.14267907843870276  Val loss: 0.015313396134327367
Epoch 109
-------------------------------
Batch 1/64 loss: -0.1703154444694519
Batch 2/64 loss: -0.15389758348464966
Batch 3/64 loss: -0.15853172540664673
Batch 4/64 loss: -0.17394095659255981
Batch 5/64 loss: -0.15753883123397827
Batch 6/64 loss: -0.1303911805152893
Batch 7/64 loss: -0.15706384181976318
Batch 8/64 loss: -0.1436471939086914
Batch 9/64 loss: -0.12702727317810059
Batch 10/64 loss: -0.1351497769355774
Batch 11/64 loss: -0.11801648139953613
Batch 12/64 loss: -0.11299490928649902
Batch 13/64 loss: -0.16552364826202393
Batch 14/64 loss: -0.15055298805236816
Batch 15/64 loss: -0.15494143962860107
Batch 16/64 loss: -0.11684828996658325
Batch 17/64 loss: -0.1550084352493286
Batch 18/64 loss: -0.15317463874816895
Batch 19/64 loss: -0.15809893608093262
Batch 20/64 loss: -0.13707387447357178
Batch 21/64 loss: -0.15148401260375977
Batch 22/64 loss: -0.1209179162979126
Batch 23/64 loss: -0.13862574100494385
Batch 24/64 loss: -0.1649991273880005
Batch 25/64 loss: -0.12964165210723877
Batch 26/64 loss: -0.13136762380599976
Batch 27/64 loss: -0.1456223726272583
Batch 28/64 loss: -0.1732109785079956
Batch 29/64 loss: -0.14757835865020752
Batch 30/64 loss: -0.1662883758544922
Batch 31/64 loss: -0.09296679496765137
Batch 32/64 loss: -0.13600754737854004
Batch 33/64 loss: -0.17400389909744263
Batch 34/64 loss: -0.1266613006591797
Batch 35/64 loss: -0.15503060817718506
Batch 36/64 loss: -0.1414143443107605
Batch 37/64 loss: -0.11547589302062988
Batch 38/64 loss: -0.15380942821502686
Batch 39/64 loss: -0.15080559253692627
Batch 40/64 loss: -0.14441972970962524
Batch 41/64 loss: -0.16824841499328613
Batch 42/64 loss: -0.14445632696151733
Batch 43/64 loss: -0.12797260284423828
Batch 44/64 loss: -0.12049663066864014
Batch 45/64 loss: -0.15988510847091675
Batch 46/64 loss: -0.15424448251724243
Batch 47/64 loss: -0.15536391735076904
Batch 48/64 loss: -0.11687296628952026
Batch 49/64 loss: -0.1416957974433899
Batch 50/64 loss: -0.14135056734085083
Batch 51/64 loss: -0.1325351595878601
Batch 52/64 loss: -0.16504114866256714
Batch 53/64 loss: -0.12670379877090454
Batch 54/64 loss: -0.13857018947601318
Batch 55/64 loss: -0.13537633419036865
Batch 56/64 loss: -0.17756551504135132
Batch 57/64 loss: -0.1453593373298645
Batch 58/64 loss: -0.1513463258743286
Batch 59/64 loss: -0.1246873140335083
Batch 60/64 loss: -0.14007139205932617
Batch 61/64 loss: -0.14543741941452026
Batch 62/64 loss: -0.1396384835243225
Batch 63/64 loss: -0.1827273964881897
Batch 64/64 loss: -0.1470128893852234
Epoch 109  Train loss: -0.14487803987428255  Val loss: 0.016678853952597918
Epoch 110
-------------------------------
Batch 1/64 loss: -0.14773792028427124
Batch 2/64 loss: -0.1514960527420044
Batch 3/64 loss: -0.12340867519378662
Batch 4/64 loss: -0.1388002634048462
Batch 5/64 loss: -0.1657058596611023
Batch 6/64 loss: -0.14717227220535278
Batch 7/64 loss: -0.14524507522583008
Batch 8/64 loss: -0.15116333961486816
Batch 9/64 loss: -0.15325576066970825
Batch 10/64 loss: -0.12769430875778198
Batch 11/64 loss: -0.09641474485397339
Batch 12/64 loss: -0.15361440181732178
Batch 13/64 loss: -0.15856808423995972
Batch 14/64 loss: -0.14079701900482178
Batch 15/64 loss: -0.16137909889221191
Batch 16/64 loss: -0.13170278072357178
Batch 17/64 loss: -0.15752726793289185
Batch 18/64 loss: -0.16022872924804688
Batch 19/64 loss: -0.13485866785049438
Batch 20/64 loss: -0.1249847412109375
Batch 21/64 loss: -0.1645621657371521
Batch 22/64 loss: -0.12807899713516235
Batch 23/64 loss: -0.10074716806411743
Batch 24/64 loss: -0.16422748565673828
Batch 25/64 loss: -0.13746261596679688
Batch 26/64 loss: -0.14775049686431885
Batch 27/64 loss: -0.15283983945846558
Batch 28/64 loss: -0.14004355669021606
Batch 29/64 loss: -0.16854453086853027
Batch 30/64 loss: -0.1492147445678711
Batch 31/64 loss: -0.1568271517753601
Batch 32/64 loss: -0.15185260772705078
Batch 33/64 loss: -0.13625586032867432
Batch 34/64 loss: -0.172122061252594
Batch 35/64 loss: -0.1575256586074829
Batch 36/64 loss: -0.14148008823394775
Batch 37/64 loss: -0.15291696786880493
Batch 38/64 loss: -0.16544067859649658
Batch 39/64 loss: -0.15940707921981812
Batch 40/64 loss: -0.15742075443267822
Batch 41/64 loss: -0.15894317626953125
Batch 42/64 loss: -0.13909560441970825
Batch 43/64 loss: -0.17783796787261963
Batch 44/64 loss: -0.11166262626647949
Batch 45/64 loss: -0.17862474918365479
Batch 46/64 loss: -0.13758492469787598
Batch 47/64 loss: -0.13911694288253784
Batch 48/64 loss: -0.1375877857208252
Batch 49/64 loss: -0.13844865560531616
Batch 50/64 loss: -0.16357606649398804
Batch 51/64 loss: -0.1313028335571289
Batch 52/64 loss: -0.14636331796646118
Batch 53/64 loss: -0.10933196544647217
Batch 54/64 loss: -0.11343443393707275
Batch 55/64 loss: -0.1414143443107605
Batch 56/64 loss: -0.1525716781616211
Batch 57/64 loss: -0.1208486557006836
Batch 58/64 loss: -0.11374717950820923
Batch 59/64 loss: -0.16513019800186157
Batch 60/64 loss: -0.10212397575378418
Batch 61/64 loss: -0.1534653902053833
Batch 62/64 loss: -0.17455148696899414
Batch 63/64 loss: -0.15162575244903564
Batch 64/64 loss: -0.14207148551940918
Epoch 110  Train loss: -0.14496343369577444  Val loss: 0.014201972287954744
Epoch 111
-------------------------------
Batch 1/64 loss: -0.14284920692443848
Batch 2/64 loss: -0.18529897928237915
Batch 3/64 loss: -0.11749935150146484
Batch 4/64 loss: -0.1488930583000183
Batch 5/64 loss: -0.15180015563964844
Batch 6/64 loss: -0.13698536157608032
Batch 7/64 loss: -0.1820605993270874
Batch 8/64 loss: -0.13742411136627197
Batch 9/64 loss: -0.12587398290634155
Batch 10/64 loss: -0.15382957458496094
Batch 11/64 loss: -0.15811288356781006
Batch 12/64 loss: -0.14733123779296875
Batch 13/64 loss: -0.1564461588859558
Batch 14/64 loss: -0.16298454999923706
Batch 15/64 loss: -0.12864118814468384
Batch 16/64 loss: -0.17235839366912842
Batch 17/64 loss: -0.13582313060760498
Batch 18/64 loss: -0.1659509539604187
Batch 19/64 loss: -0.16719269752502441
Batch 20/64 loss: -0.13621914386749268
Batch 21/64 loss: -0.13047170639038086
Batch 22/64 loss: -0.15001064538955688
Batch 23/64 loss: -0.16014337539672852
Batch 24/64 loss: -0.17886793613433838
Batch 25/64 loss: -0.15483736991882324
Batch 26/64 loss: -0.14647459983825684
Batch 27/64 loss: -0.10900819301605225
Batch 28/64 loss: -0.15015923976898193
Batch 29/64 loss: -0.16083425283432007
Batch 30/64 loss: -0.14764106273651123
Batch 31/64 loss: -0.11222803592681885
Batch 32/64 loss: -0.16456735134124756
Batch 33/64 loss: -0.14959615468978882
Batch 34/64 loss: -0.12313133478164673
Batch 35/64 loss: -0.10551619529724121
Batch 36/64 loss: -0.14727258682250977
Batch 37/64 loss: -0.12773871421813965
Batch 38/64 loss: -0.12769466638565063
Batch 39/64 loss: -0.1553729772567749
Batch 40/64 loss: -0.16125518083572388
Batch 41/64 loss: -0.15956801176071167
Batch 42/64 loss: -0.152729332447052
Batch 43/64 loss: -0.1535833477973938
Batch 44/64 loss: -0.13636231422424316
Batch 45/64 loss: -0.10600662231445312
Batch 46/64 loss: -0.14968305826187134
Batch 47/64 loss: -0.13752377033233643
Batch 48/64 loss: -0.12472158670425415
Batch 49/64 loss: -0.12856614589691162
Batch 50/64 loss: -0.14629381895065308
Batch 51/64 loss: -0.13034743070602417
Batch 52/64 loss: -0.1556386947631836
Batch 53/64 loss: -0.15682101249694824
Batch 54/64 loss: -0.16013872623443604
Batch 55/64 loss: -0.1581098437309265
Batch 56/64 loss: -0.14187520742416382
Batch 57/64 loss: -0.16935443878173828
Batch 58/64 loss: -0.14740800857543945
Batch 59/64 loss: -0.133203387260437
Batch 60/64 loss: -0.16357719898223877
Batch 61/64 loss: -0.12835323810577393
Batch 62/64 loss: -0.14654278755187988
Batch 63/64 loss: -0.16173160076141357
Batch 64/64 loss: -0.13197779655456543
Epoch 111  Train loss: -0.1462512820374732  Val loss: 0.016677936737480033
Epoch 112
-------------------------------
Batch 1/64 loss: -0.16899901628494263
Batch 2/64 loss: -0.16697418689727783
Batch 3/64 loss: -0.15544497966766357
Batch 4/64 loss: -0.15418976545333862
Batch 5/64 loss: -0.1776367425918579
Batch 6/64 loss: -0.1475292444229126
Batch 7/64 loss: -0.1337530016899109
Batch 8/64 loss: -0.15058881044387817
Batch 9/64 loss: -0.12828290462493896
Batch 10/64 loss: -0.15320897102355957
Batch 11/64 loss: -0.14516210556030273
Batch 12/64 loss: -0.14953196048736572
Batch 13/64 loss: -0.14388668537139893
Batch 14/64 loss: -0.13327085971832275
Batch 15/64 loss: -0.12429565191268921
Batch 16/64 loss: -0.16808068752288818
Batch 17/64 loss: -0.13811278343200684
Batch 18/64 loss: -0.16330647468566895
Batch 19/64 loss: -0.1435098648071289
Batch 20/64 loss: -0.12918144464492798
Batch 21/64 loss: -0.12942612171173096
Batch 22/64 loss: -0.1467599868774414
Batch 23/64 loss: -0.13826340436935425
Batch 24/64 loss: -0.16926836967468262
Batch 25/64 loss: -0.13897091150283813
Batch 26/64 loss: -0.1577802300453186
Batch 27/64 loss: -0.13532638549804688
Batch 28/64 loss: -0.1383981704711914
Batch 29/64 loss: -0.13898706436157227
Batch 30/64 loss: -0.12489748001098633
Batch 31/64 loss: -0.13199806213378906
Batch 32/64 loss: -0.14087921380996704
Batch 33/64 loss: -0.15123677253723145
Batch 34/64 loss: -0.17517685890197754
Batch 35/64 loss: -0.16527318954467773
Batch 36/64 loss: -0.15649867057800293
Batch 37/64 loss: -0.15297067165374756
Batch 38/64 loss: -0.15283209085464478
Batch 39/64 loss: -0.16086572408676147
Batch 40/64 loss: -0.17521756887435913
Batch 41/64 loss: -0.17251121997833252
Batch 42/64 loss: -0.16422343254089355
Batch 43/64 loss: -0.15061497688293457
Batch 44/64 loss: -0.16112250089645386
Batch 45/64 loss: -0.13955813646316528
Batch 46/64 loss: -0.13215148448944092
Batch 47/64 loss: -0.1308087706565857
Batch 48/64 loss: -0.16105139255523682
Batch 49/64 loss: -0.1461889147758484
Batch 50/64 loss: -0.12607771158218384
Batch 51/64 loss: -0.15163308382034302
Batch 52/64 loss: -0.0920822024345398
Batch 53/64 loss: -0.14896631240844727
Batch 54/64 loss: -0.1537347435951233
Batch 55/64 loss: -0.11631983518600464
Batch 56/64 loss: -0.14710980653762817
Batch 57/64 loss: -0.13413971662521362
Batch 58/64 loss: -0.151824951171875
Batch 59/64 loss: -0.1626749038696289
Batch 60/64 loss: -0.14091026782989502
Batch 61/64 loss: -0.14700287580490112
Batch 62/64 loss: -0.16851502656936646
Batch 63/64 loss: -0.15882658958435059
Batch 64/64 loss: -0.15366709232330322
Epoch 112  Train loss: -0.14791015316458309  Val loss: 0.014066791411527653
Epoch 113
-------------------------------
Batch 1/64 loss: -0.15581536293029785
Batch 2/64 loss: -0.1722373366355896
Batch 3/64 loss: -0.1454593539237976
Batch 4/64 loss: -0.16208428144454956
Batch 5/64 loss: -0.1717895269393921
Batch 6/64 loss: -0.1610996127128601
Batch 7/64 loss: -0.17298346757888794
Batch 8/64 loss: -0.13862240314483643
Batch 9/64 loss: -0.14197957515716553
Batch 10/64 loss: -0.181898832321167
Batch 11/64 loss: -0.14135658740997314
Batch 12/64 loss: -0.16253596544265747
Batch 13/64 loss: -0.16200155019760132
Batch 14/64 loss: -0.1453205943107605
Batch 15/64 loss: -0.1505109667778015
Batch 16/64 loss: -0.15597373247146606
Batch 17/64 loss: -0.17888003587722778
Batch 18/64 loss: -0.15440726280212402
Batch 19/64 loss: -0.19318139553070068
Batch 20/64 loss: -0.16350162029266357
Batch 21/64 loss: -0.19253134727478027
Batch 22/64 loss: -0.17122668027877808
Batch 23/64 loss: -0.15316081047058105
Batch 24/64 loss: -0.1613377332687378
Batch 25/64 loss: -0.18698984384536743
Batch 26/64 loss: -0.16330236196517944
Batch 27/64 loss: -0.14418524503707886
Batch 28/64 loss: -0.11353552341461182
Batch 29/64 loss: -0.15449607372283936
Batch 30/64 loss: -0.12810087203979492
Batch 31/64 loss: -0.12143862247467041
Batch 32/64 loss: -0.14819657802581787
Batch 33/64 loss: -0.11001855134963989
Batch 34/64 loss: -0.1269845962524414
Batch 35/64 loss: -0.1386866569519043
Batch 36/64 loss: -0.09918582439422607
Batch 37/64 loss: -0.1437097191810608
Batch 38/64 loss: -0.13399851322174072
Batch 39/64 loss: -0.1329737901687622
Batch 40/64 loss: -0.10600000619888306
Batch 41/64 loss: -0.16498816013336182
Batch 42/64 loss: -0.15932559967041016
Batch 43/64 loss: -0.146695077419281
Batch 44/64 loss: -0.11069673299789429
Batch 45/64 loss: -0.1399431824684143
Batch 46/64 loss: -0.162553071975708
Batch 47/64 loss: -0.12357401847839355
Batch 48/64 loss: -0.1312100887298584
Batch 49/64 loss: -0.16406255960464478
Batch 50/64 loss: -0.15709638595581055
Batch 51/64 loss: -0.15822505950927734
Batch 52/64 loss: -0.1408061981201172
Batch 53/64 loss: -0.18049216270446777
Batch 54/64 loss: -0.13576793670654297
Batch 55/64 loss: -0.14903181791305542
Batch 56/64 loss: -0.170271635055542
Batch 57/64 loss: -0.17682373523712158
Batch 58/64 loss: -0.15557849407196045
Batch 59/64 loss: -0.15424275398254395
Batch 60/64 loss: -0.13531982898712158
Batch 61/64 loss: -0.16592997312545776
Batch 62/64 loss: -0.13690412044525146
Batch 63/64 loss: -0.12242120504379272
Batch 64/64 loss: -0.14774078130722046
Epoch 113  Train loss: -0.1505013991804684  Val loss: 0.013955114633356993
Epoch 114
-------------------------------
Batch 1/64 loss: -0.18148380517959595
Batch 2/64 loss: -0.15415418148040771
Batch 3/64 loss: -0.1501409411430359
Batch 4/64 loss: -0.15626639127731323
Batch 5/64 loss: -0.1577441692352295
Batch 6/64 loss: -0.15648043155670166
Batch 7/64 loss: -0.15394443273544312
Batch 8/64 loss: -0.18390923738479614
Batch 9/64 loss: -0.16563045978546143
Batch 10/64 loss: -0.13962960243225098
Batch 11/64 loss: -0.10933685302734375
Batch 12/64 loss: -0.18362516164779663
Batch 13/64 loss: -0.1722276210784912
Batch 14/64 loss: -0.1350618600845337
Batch 15/64 loss: -0.14761728048324585
Batch 16/64 loss: -0.159010112285614
Batch 17/64 loss: -0.12708282470703125
Batch 18/64 loss: -0.1553400158882141
Batch 19/64 loss: -0.14164245128631592
Batch 20/64 loss: -0.17154055833816528
Batch 21/64 loss: -0.17513823509216309
Batch 22/64 loss: -0.14975738525390625
Batch 23/64 loss: -0.15429222583770752
Batch 24/64 loss: -0.13120537996292114
Batch 25/64 loss: -0.1545395851135254
Batch 26/64 loss: -0.17741209268569946
Batch 27/64 loss: -0.16170424222946167
Batch 28/64 loss: -0.142015278339386
Batch 29/64 loss: -0.1315343976020813
Batch 30/64 loss: -0.1307365894317627
Batch 31/64 loss: -0.10986387729644775
Batch 32/64 loss: -0.14839082956314087
Batch 33/64 loss: -0.16597896814346313
Batch 34/64 loss: -0.15337669849395752
Batch 35/64 loss: -0.16015219688415527
Batch 36/64 loss: -0.16027098894119263
Batch 37/64 loss: -0.14328044652938843
Batch 38/64 loss: -0.11832946538925171
Batch 39/64 loss: -0.13910138607025146
Batch 40/64 loss: -0.1553058624267578
Batch 41/64 loss: -0.15673714876174927
Batch 42/64 loss: -0.1467474102973938
Batch 43/64 loss: -0.16488534212112427
Batch 44/64 loss: -0.16717612743377686
Batch 45/64 loss: -0.17049318552017212
Batch 46/64 loss: -0.16713839769363403
Batch 47/64 loss: -0.16654455661773682
Batch 48/64 loss: -0.15366148948669434
Batch 49/64 loss: -0.13145464658737183
Batch 50/64 loss: -0.1619367003440857
Batch 51/64 loss: -0.16495907306671143
Batch 52/64 loss: -0.14740920066833496
Batch 53/64 loss: -0.1722886562347412
Batch 54/64 loss: -0.1389816403388977
Batch 55/64 loss: -0.14786553382873535
Batch 56/64 loss: -0.15170109272003174
Batch 57/64 loss: -0.14060330390930176
Batch 58/64 loss: -0.16189926862716675
Batch 59/64 loss: -0.0895305871963501
Batch 60/64 loss: -0.13062584400177002
Batch 61/64 loss: -0.1352144479751587
Batch 62/64 loss: -0.1454099416732788
Batch 63/64 loss: -0.16145402193069458
Batch 64/64 loss: -0.12638318538665771
Epoch 114  Train loss: -0.15111779652389826  Val loss: 0.013988695398638748
Epoch 115
-------------------------------
Batch 1/64 loss: -0.1648099422454834
Batch 2/64 loss: -0.1629807949066162
Batch 3/64 loss: -0.15204298496246338
Batch 4/64 loss: -0.18761098384857178
Batch 5/64 loss: -0.18556737899780273
Batch 6/64 loss: -0.15700089931488037
Batch 7/64 loss: -0.16857361793518066
Batch 8/64 loss: -0.16542840003967285
Batch 9/64 loss: -0.1748521327972412
Batch 10/64 loss: -0.14666682481765747
Batch 11/64 loss: -0.14699983596801758
Batch 12/64 loss: -0.15604859590530396
Batch 13/64 loss: -0.13291466236114502
Batch 14/64 loss: -0.1734774112701416
Batch 15/64 loss: -0.16954517364501953
Batch 16/64 loss: -0.14324140548706055
Batch 17/64 loss: -0.14515447616577148
Batch 18/64 loss: -0.17368090152740479
Batch 19/64 loss: -0.16022688150405884
Batch 20/64 loss: -0.19035959243774414
Batch 21/64 loss: -0.16298514604568481
Batch 22/64 loss: -0.1380797028541565
Batch 23/64 loss: -0.13451451063156128
Batch 24/64 loss: -0.14836430549621582
Batch 25/64 loss: -0.13868504762649536
Batch 26/64 loss: -0.15920740365982056
Batch 27/64 loss: -0.14905983209609985
Batch 28/64 loss: -0.14672303199768066
Batch 29/64 loss: -0.13358759880065918
Batch 30/64 loss: -0.13567721843719482
Batch 31/64 loss: -0.1229671835899353
Batch 32/64 loss: -0.1665075421333313
Batch 33/64 loss: -0.15946781635284424
Batch 34/64 loss: -0.12236654758453369
Batch 35/64 loss: -0.15911531448364258
Batch 36/64 loss: -0.13611537218093872
Batch 37/64 loss: -0.15946465730667114
Batch 38/64 loss: -0.15272903442382812
Batch 39/64 loss: -0.1477121114730835
Batch 40/64 loss: -0.14148551225662231
Batch 41/64 loss: -0.13242566585540771
Batch 42/64 loss: -0.1281886100769043
Batch 43/64 loss: -0.1560359001159668
Batch 44/64 loss: -0.16719448566436768
Batch 45/64 loss: -0.1688583493232727
Batch 46/64 loss: -0.13222670555114746
Batch 47/64 loss: -0.16028088331222534
Batch 48/64 loss: -0.13852578401565552
Batch 49/64 loss: -0.17295145988464355
Batch 50/64 loss: -0.15256649255752563
Batch 51/64 loss: -0.15175354480743408
Batch 52/64 loss: -0.16814017295837402
Batch 53/64 loss: -0.13749569654464722
Batch 54/64 loss: -0.18570303916931152
Batch 55/64 loss: -0.13131755590438843
Batch 56/64 loss: -0.1387021541595459
Batch 57/64 loss: -0.17729425430297852
Batch 58/64 loss: -0.14597368240356445
Batch 59/64 loss: -0.16496193408966064
Batch 60/64 loss: -0.15877127647399902
Batch 61/64 loss: -0.15065407752990723
Batch 62/64 loss: -0.14258772134780884
Batch 63/64 loss: -0.11978316307067871
Batch 64/64 loss: -0.13328427076339722
Epoch 115  Train loss: -0.15300946820016  Val loss: 0.01555928188500945
Epoch 116
-------------------------------
Batch 1/64 loss: -0.17882072925567627
Batch 2/64 loss: -0.12078511714935303
Batch 3/64 loss: -0.1528087854385376
Batch 4/64 loss: -0.17130357027053833
Batch 5/64 loss: -0.146528959274292
Batch 6/64 loss: -0.13435965776443481
Batch 7/64 loss: -0.1622014045715332
Batch 8/64 loss: -0.14888834953308105
Batch 9/64 loss: -0.17988628149032593
Batch 10/64 loss: -0.1458796262741089
Batch 11/64 loss: -0.1844998598098755
Batch 12/64 loss: -0.12031614780426025
Batch 13/64 loss: -0.15023356676101685
Batch 14/64 loss: -0.15515071153640747
Batch 15/64 loss: -0.17645049095153809
Batch 16/64 loss: -0.1727147102355957
Batch 17/64 loss: -0.167890727519989
Batch 18/64 loss: -0.14155399799346924
Batch 19/64 loss: -0.16732317209243774
Batch 20/64 loss: -0.1769145131111145
Batch 21/64 loss: -0.1732274889945984
Batch 22/64 loss: -0.15476614236831665
Batch 23/64 loss: -0.14903932809829712
Batch 24/64 loss: -0.15742826461791992
Batch 25/64 loss: -0.1710612177848816
Batch 26/64 loss: -0.13791930675506592
Batch 27/64 loss: -0.17436164617538452
Batch 28/64 loss: -0.13807135820388794
Batch 29/64 loss: -0.15242964029312134
Batch 30/64 loss: -0.16738003492355347
Batch 31/64 loss: -0.17046034336090088
Batch 32/64 loss: -0.1930851936340332
Batch 33/64 loss: -0.15371257066726685
Batch 34/64 loss: -0.12846636772155762
Batch 35/64 loss: -0.15490740537643433
Batch 36/64 loss: -0.15471893548965454
Batch 37/64 loss: -0.16253960132598877
Batch 38/64 loss: -0.1680682897567749
Batch 39/64 loss: -0.1684076189994812
Batch 40/64 loss: -0.15758615732192993
Batch 41/64 loss: -0.1307629942893982
Batch 42/64 loss: -0.14856946468353271
Batch 43/64 loss: -0.17837631702423096
Batch 44/64 loss: -0.1568453311920166
Batch 45/64 loss: -0.15206634998321533
Batch 46/64 loss: -0.13001352548599243
Batch 47/64 loss: -0.166789710521698
Batch 48/64 loss: -0.1426234245300293
Batch 49/64 loss: -0.16934537887573242
Batch 50/64 loss: -0.0998803973197937
Batch 51/64 loss: -0.1459956169128418
Batch 52/64 loss: -0.15327835083007812
Batch 53/64 loss: -0.15232539176940918
Batch 54/64 loss: -0.13673442602157593
Batch 55/64 loss: -0.1521611213684082
Batch 56/64 loss: -0.15827953815460205
Batch 57/64 loss: -0.14998161792755127
Batch 58/64 loss: -0.15786010026931763
Batch 59/64 loss: -0.1395319700241089
Batch 60/64 loss: -0.15431052446365356
Batch 61/64 loss: -0.18010151386260986
Batch 62/64 loss: -0.14496439695358276
Batch 63/64 loss: -0.16838490962982178
Batch 64/64 loss: -0.1654191017150879
Epoch 116  Train loss: -0.1558493174758612  Val loss: 0.013923304392300112
Epoch 117
-------------------------------
Batch 1/64 loss: -0.19172614812850952
Batch 2/64 loss: -0.14960938692092896
Batch 3/64 loss: -0.18995219469070435
Batch 4/64 loss: -0.16806262731552124
Batch 5/64 loss: -0.130082368850708
Batch 6/64 loss: -0.1374717354774475
Batch 7/64 loss: -0.12104320526123047
Batch 8/64 loss: -0.1945074200630188
Batch 9/64 loss: -0.1603376269340515
Batch 10/64 loss: -0.1706998348236084
Batch 11/64 loss: -0.1870335340499878
Batch 12/64 loss: -0.12621283531188965
Batch 13/64 loss: -0.17591720819473267
Batch 14/64 loss: -0.168160080909729
Batch 15/64 loss: -0.15129035711288452
Batch 16/64 loss: -0.14531540870666504
Batch 17/64 loss: -0.1723693609237671
Batch 18/64 loss: -0.13527953624725342
Batch 19/64 loss: -0.15165454149246216
Batch 20/64 loss: -0.18185752630233765
Batch 21/64 loss: -0.17236459255218506
Batch 22/64 loss: -0.17940086126327515
Batch 23/64 loss: -0.1688622236251831
Batch 24/64 loss: -0.1591305136680603
Batch 25/64 loss: -0.14775776863098145
Batch 26/64 loss: -0.14017927646636963
Batch 27/64 loss: -0.16099411249160767
Batch 28/64 loss: -0.1744619607925415
Batch 29/64 loss: -0.1458340883255005
Batch 30/64 loss: -0.19772249460220337
Batch 31/64 loss: -0.13555657863616943
Batch 32/64 loss: -0.18118345737457275
Batch 33/64 loss: -0.1505306363105774
Batch 34/64 loss: -0.15123307704925537
Batch 35/64 loss: -0.11900556087493896
Batch 36/64 loss: -0.16254150867462158
Batch 37/64 loss: -0.17311888933181763
Batch 38/64 loss: -0.15557247400283813
Batch 39/64 loss: -0.1808111071586609
Batch 40/64 loss: -0.18651854991912842
Batch 41/64 loss: -0.13627511262893677
Batch 42/64 loss: -0.19072824716567993
Batch 43/64 loss: -0.14794880151748657
Batch 44/64 loss: -0.19401180744171143
Batch 45/64 loss: -0.17217469215393066
Batch 46/64 loss: -0.17202293872833252
Batch 47/64 loss: -0.16813337802886963
Batch 48/64 loss: -0.13774341344833374
Batch 49/64 loss: -0.17215675115585327
Batch 50/64 loss: -0.12693285942077637
Batch 51/64 loss: -0.15872281789779663
Batch 52/64 loss: -0.15436965227127075
Batch 53/64 loss: -0.1693127155303955
Batch 54/64 loss: -0.12321370840072632
Batch 55/64 loss: -0.17028093338012695
Batch 56/64 loss: -0.13836848735809326
Batch 57/64 loss: -0.14406508207321167
Batch 58/64 loss: -0.1450357437133789
Batch 59/64 loss: -0.15465056896209717
Batch 60/64 loss: -0.1480022668838501
Batch 61/64 loss: -0.14501041173934937
Batch 62/64 loss: -0.18386632204055786
Batch 63/64 loss: -0.12916821241378784
Batch 64/64 loss: -0.15008199214935303
Epoch 117  Train loss: -0.15915480922250186  Val loss: 0.015701193784930043
Epoch 118
-------------------------------
Batch 1/64 loss: -0.17348623275756836
Batch 2/64 loss: -0.17396366596221924
Batch 3/64 loss: -0.17752301692962646
Batch 4/64 loss: -0.15434247255325317
Batch 5/64 loss: -0.16530364751815796
Batch 6/64 loss: -0.14591020345687866
Batch 7/64 loss: -0.12110918760299683
Batch 8/64 loss: -0.15058499574661255
Batch 9/64 loss: -0.16370654106140137
Batch 10/64 loss: -0.15917551517486572
Batch 11/64 loss: -0.17206746339797974
Batch 12/64 loss: -0.1485830545425415
Batch 13/64 loss: -0.15047162771224976
Batch 14/64 loss: -0.136519193649292
Batch 15/64 loss: -0.18721312284469604
Batch 16/64 loss: -0.16003483533859253
Batch 17/64 loss: -0.12074530124664307
Batch 18/64 loss: -0.18450301885604858
Batch 19/64 loss: -0.16791725158691406
Batch 20/64 loss: -0.16680872440338135
Batch 21/64 loss: -0.1312776803970337
Batch 22/64 loss: -0.1752927303314209
Batch 23/64 loss: -0.18425434827804565
Batch 24/64 loss: -0.1614895462989807
Batch 25/64 loss: -0.1609782576560974
Batch 26/64 loss: -0.17030495405197144
Batch 27/64 loss: -0.17344558238983154
Batch 28/64 loss: -0.1549515724182129
Batch 29/64 loss: -0.14286595582962036
Batch 30/64 loss: -0.17618536949157715
Batch 31/64 loss: -0.1872180700302124
Batch 32/64 loss: -0.15232187509536743
Batch 33/64 loss: -0.1395554542541504
Batch 34/64 loss: -0.15446388721466064
Batch 35/64 loss: -0.1771795153617859
Batch 36/64 loss: -0.15461236238479614
Batch 37/64 loss: -0.14823901653289795
Batch 38/64 loss: -0.16383081674575806
Batch 39/64 loss: -0.16779768466949463
Batch 40/64 loss: -0.17132216691970825
Batch 41/64 loss: -0.14631152153015137
Batch 42/64 loss: -0.16014015674591064
Batch 43/64 loss: -0.116161048412323
Batch 44/64 loss: -0.16831886768341064
Batch 45/64 loss: -0.15431559085845947
Batch 46/64 loss: -0.17012465000152588
Batch 47/64 loss: -0.1631917953491211
Batch 48/64 loss: -0.16754156351089478
Batch 49/64 loss: -0.15402543544769287
Batch 50/64 loss: -0.15187197923660278
Batch 51/64 loss: -0.1732437014579773
Batch 52/64 loss: -0.1544947624206543
Batch 53/64 loss: -0.1212570071220398
Batch 54/64 loss: -0.1602628231048584
Batch 55/64 loss: -0.15053921937942505
Batch 56/64 loss: -0.1601555347442627
Batch 57/64 loss: -0.17403817176818848
Batch 58/64 loss: -0.14859777688980103
Batch 59/64 loss: -0.13246756792068481
Batch 60/64 loss: -0.16966164112091064
Batch 61/64 loss: -0.15775465965270996
Batch 62/64 loss: -0.17072534561157227
Batch 63/64 loss: -0.1641107201576233
Batch 64/64 loss: -0.14641863107681274
Epoch 118  Train loss: -0.15884990481769337  Val loss: 0.01880435312736485
Epoch 119
-------------------------------
Batch 1/64 loss: -0.1626957654953003
Batch 2/64 loss: -0.17674648761749268
Batch 3/64 loss: -0.17459160089492798
Batch 4/64 loss: -0.18430304527282715
Batch 5/64 loss: -0.1719820499420166
Batch 6/64 loss: -0.12295335531234741
Batch 7/64 loss: -0.17742925882339478
Batch 8/64 loss: -0.15491998195648193
Batch 9/64 loss: -0.18462353944778442
Batch 10/64 loss: -0.13601720333099365
Batch 11/64 loss: -0.15675878524780273
Batch 12/64 loss: -0.1783827543258667
Batch 13/64 loss: -0.1706942915916443
Batch 14/64 loss: -0.16256284713745117
Batch 15/64 loss: -0.13443320989608765
Batch 16/64 loss: -0.15485435724258423
Batch 17/64 loss: -0.1597205400466919
Batch 18/64 loss: -0.17525827884674072
Batch 19/64 loss: -0.16034090518951416
Batch 20/64 loss: -0.19611263275146484
Batch 21/64 loss: -0.14336389303207397
Batch 22/64 loss: -0.11743032932281494
Batch 23/64 loss: -0.1454433798789978
Batch 24/64 loss: -0.12828660011291504
Batch 25/64 loss: -0.12962228059768677
Batch 26/64 loss: -0.17774206399917603
Batch 27/64 loss: -0.17257630825042725
Batch 28/64 loss: -0.1475580930709839
Batch 29/64 loss: -0.1590498685836792
Batch 30/64 loss: -0.18316853046417236
Batch 31/64 loss: -0.12928777933120728
Batch 32/64 loss: -0.14350742101669312
Batch 33/64 loss: -0.17025065422058105
Batch 34/64 loss: -0.15347588062286377
Batch 35/64 loss: -0.16726988554000854
Batch 36/64 loss: -0.10289764404296875
Batch 37/64 loss: -0.14265751838684082
Batch 38/64 loss: -0.16519200801849365
Batch 39/64 loss: -0.15592139959335327
Batch 40/64 loss: -0.16232651472091675
Batch 41/64 loss: -0.16649377346038818
Batch 42/64 loss: -0.15402090549468994
Batch 43/64 loss: -0.15084373950958252
Batch 44/64 loss: -0.16600364446640015
Batch 45/64 loss: -0.13153481483459473
Batch 46/64 loss: -0.16081386804580688
Batch 47/64 loss: -0.1700880527496338
Batch 48/64 loss: -0.16338956356048584
Batch 49/64 loss: -0.16066288948059082
Batch 50/64 loss: -0.16964566707611084
Batch 51/64 loss: -0.18049687147140503
Batch 52/64 loss: -0.1606793999671936
Batch 53/64 loss: -0.15241432189941406
Batch 54/64 loss: -0.16384273767471313
Batch 55/64 loss: -0.18150639533996582
Batch 56/64 loss: -0.16580557823181152
Batch 57/64 loss: -0.1702675223350525
Batch 58/64 loss: -0.1645248532295227
Batch 59/64 loss: -0.14740359783172607
Batch 60/64 loss: -0.17200881242752075
Batch 61/64 loss: -0.17110347747802734
Batch 62/64 loss: -0.18272823095321655
Batch 63/64 loss: -0.18344587087631226
Batch 64/64 loss: -0.18119454383850098
Epoch 119  Train loss: -0.16028281473645978  Val loss: 0.01188847268979574
Saving best model, epoch: 119
Epoch 120
-------------------------------
Batch 1/64 loss: -0.1879672408103943
Batch 2/64 loss: -0.1369590163230896
Batch 3/64 loss: -0.17617392539978027
Batch 4/64 loss: -0.1977803111076355
Batch 5/64 loss: -0.13466835021972656
Batch 6/64 loss: -0.13791853189468384
Batch 7/64 loss: -0.16265445947647095
Batch 8/64 loss: -0.18187975883483887
Batch 9/64 loss: -0.1573270559310913
Batch 10/64 loss: -0.1784788966178894
Batch 11/64 loss: -0.15743041038513184
Batch 12/64 loss: -0.1675257682800293
Batch 13/64 loss: -0.18227159976959229
Batch 14/64 loss: -0.17407304048538208
Batch 15/64 loss: -0.16902554035186768
Batch 16/64 loss: -0.12334853410720825
Batch 17/64 loss: -0.1875893473625183
Batch 18/64 loss: -0.17535459995269775
Batch 19/64 loss: -0.1619175672531128
Batch 20/64 loss: -0.15941572189331055
Batch 21/64 loss: -0.15710532665252686
Batch 22/64 loss: -0.16989946365356445
Batch 23/64 loss: -0.1528778076171875
Batch 24/64 loss: -0.18463951349258423
Batch 25/64 loss: -0.15370702743530273
Batch 26/64 loss: -0.16509002447128296
Batch 27/64 loss: -0.18562370538711548
Batch 28/64 loss: -0.1403607726097107
Batch 29/64 loss: -0.15646976232528687
Batch 30/64 loss: -0.1568758487701416
Batch 31/64 loss: -0.1934911012649536
Batch 32/64 loss: -0.1258975863456726
Batch 33/64 loss: -0.15919315814971924
Batch 34/64 loss: -0.1417841911315918
Batch 35/64 loss: -0.15662038326263428
Batch 36/64 loss: -0.16644179821014404
Batch 37/64 loss: -0.1754319667816162
Batch 38/64 loss: -0.14306706190109253
Batch 39/64 loss: -0.13533753156661987
Batch 40/64 loss: -0.14398092031478882
Batch 41/64 loss: -0.1648436188697815
Batch 42/64 loss: -0.15151548385620117
Batch 43/64 loss: -0.17769086360931396
Batch 44/64 loss: -0.17212533950805664
Batch 45/64 loss: -0.17154306173324585
Batch 46/64 loss: -0.17772984504699707
Batch 47/64 loss: -0.16720670461654663
Batch 48/64 loss: -0.14242368936538696
Batch 49/64 loss: -0.16806340217590332
Batch 50/64 loss: -0.15575921535491943
Batch 51/64 loss: -0.17197710275650024
Batch 52/64 loss: -0.17794716358184814
Batch 53/64 loss: -0.1893482208251953
Batch 54/64 loss: -0.17182815074920654
Batch 55/64 loss: -0.15596437454223633
Batch 56/64 loss: -0.15004634857177734
Batch 57/64 loss: -0.17514818906784058
Batch 58/64 loss: -0.1560516357421875
Batch 59/64 loss: -0.12908130884170532
Batch 60/64 loss: -0.14855027198791504
Batch 61/64 loss: -0.16328364610671997
Batch 62/64 loss: -0.17378515005111694
Batch 63/64 loss: -0.1571081280708313
Batch 64/64 loss: -0.12359219789505005
Epoch 120  Train loss: -0.1621234461372974  Val loss: 0.016988922025739532
Epoch 121
-------------------------------
Batch 1/64 loss: -0.1370721459388733
Batch 2/64 loss: -0.17585277557373047
Batch 3/64 loss: -0.17511898279190063
Batch 4/64 loss: -0.16649413108825684
Batch 5/64 loss: -0.19781923294067383
Batch 6/64 loss: -0.1362457275390625
Batch 7/64 loss: -0.18760621547698975
Batch 8/64 loss: -0.18128126859664917
Batch 9/64 loss: -0.1773936152458191
Batch 10/64 loss: -0.18440240621566772
Batch 11/64 loss: -0.18660211563110352
Batch 12/64 loss: -0.18693971633911133
Batch 13/64 loss: -0.12572818994522095
Batch 14/64 loss: -0.1470291018486023
Batch 15/64 loss: -0.17681527137756348
Batch 16/64 loss: -0.16098684072494507
Batch 17/64 loss: -0.1644347906112671
Batch 18/64 loss: -0.17577260732650757
Batch 19/64 loss: -0.16251236200332642
Batch 20/64 loss: -0.16702085733413696
Batch 21/64 loss: -0.146323561668396
Batch 22/64 loss: -0.1353474259376526
Batch 23/64 loss: -0.17401039600372314
Batch 24/64 loss: -0.14439058303833008
Batch 25/64 loss: -0.1479489803314209
Batch 26/64 loss: -0.17772507667541504
Batch 27/64 loss: -0.17406761646270752
Batch 28/64 loss: -0.16686010360717773
Batch 29/64 loss: -0.16789788007736206
Batch 30/64 loss: -0.19617271423339844
Batch 31/64 loss: -0.1421336531639099
Batch 32/64 loss: -0.1808241605758667
Batch 33/64 loss: -0.14278340339660645
Batch 34/64 loss: -0.16423118114471436
Batch 35/64 loss: -0.16878795623779297
Batch 36/64 loss: -0.14039528369903564
Batch 37/64 loss: -0.17953777313232422
Batch 38/64 loss: -0.14639681577682495
Batch 39/64 loss: -0.13892090320587158
Batch 40/64 loss: -0.15974998474121094
Batch 41/64 loss: -0.18042325973510742
Batch 42/64 loss: -0.09664952754974365
Batch 43/64 loss: -0.1481207013130188
Batch 44/64 loss: -0.18785810470581055
Batch 45/64 loss: -0.18397831916809082
Batch 46/64 loss: -0.18021631240844727
Batch 47/64 loss: -0.13332492113113403
Batch 48/64 loss: -0.15944701433181763
Batch 49/64 loss: -0.15953350067138672
Batch 50/64 loss: -0.16226112842559814
Batch 51/64 loss: -0.17571759223937988
Batch 52/64 loss: -0.15818458795547485
Batch 53/64 loss: -0.1370985507965088
Batch 54/64 loss: -0.16651946306228638
Batch 55/64 loss: -0.1840144395828247
Batch 56/64 loss: -0.20075851678848267
Batch 57/64 loss: -0.13425606489181519
Batch 58/64 loss: -0.1833173632621765
Batch 59/64 loss: -0.17475199699401855
Batch 60/64 loss: -0.19111323356628418
Batch 61/64 loss: -0.16922491788864136
Batch 62/64 loss: -0.1502223014831543
Batch 63/64 loss: -0.1273464560508728
Batch 64/64 loss: -0.15807056427001953
Epoch 121  Train loss: -0.1636160785076665  Val loss: 0.017307287843776324
Epoch 122
-------------------------------
Batch 1/64 loss: -0.14518648386001587
Batch 2/64 loss: -0.17777246236801147
Batch 3/64 loss: -0.17263829708099365
Batch 4/64 loss: -0.17798268795013428
Batch 5/64 loss: -0.18975329399108887
Batch 6/64 loss: -0.17649543285369873
Batch 7/64 loss: -0.1129080057144165
Batch 8/64 loss: -0.1793767213821411
Batch 9/64 loss: -0.17540621757507324
Batch 10/64 loss: -0.21599507331848145
Batch 11/64 loss: -0.19004356861114502
Batch 12/64 loss: -0.1810286045074463
Batch 13/64 loss: -0.178849458694458
Batch 14/64 loss: -0.14923113584518433
Batch 15/64 loss: -0.18931055068969727
Batch 16/64 loss: -0.11735790967941284
Batch 17/64 loss: -0.15099108219146729
Batch 18/64 loss: -0.145338237285614
Batch 19/64 loss: -0.16601067781448364
Batch 20/64 loss: -0.18249887228012085
Batch 21/64 loss: -0.1418207883834839
Batch 22/64 loss: -0.17750728130340576
Batch 23/64 loss: -0.14404934644699097
Batch 24/64 loss: -0.15455114841461182
Batch 25/64 loss: -0.12168043851852417
Batch 26/64 loss: -0.1625291109085083
Batch 27/64 loss: -0.1368856430053711
Batch 28/64 loss: -0.1564399003982544
Batch 29/64 loss: -0.1672871708869934
Batch 30/64 loss: -0.1601485013961792
Batch 31/64 loss: -0.1665869951248169
Batch 32/64 loss: -0.17864298820495605
Batch 33/64 loss: -0.1447911262512207
Batch 34/64 loss: -0.1472659707069397
Batch 35/64 loss: -0.17242473363876343
Batch 36/64 loss: -0.17686069011688232
Batch 37/64 loss: -0.156319260597229
Batch 38/64 loss: -0.11538344621658325
Batch 39/64 loss: -0.18316394090652466
Batch 40/64 loss: -0.17479419708251953
Batch 41/64 loss: -0.17463064193725586
Batch 42/64 loss: -0.13778001070022583
Batch 43/64 loss: -0.15439891815185547
Batch 44/64 loss: -0.18793904781341553
Batch 45/64 loss: -0.10504186153411865
Batch 46/64 loss: -0.17229902744293213
Batch 47/64 loss: -0.15524476766586304
Batch 48/64 loss: -0.1738530397415161
Batch 49/64 loss: -0.1554267406463623
Batch 50/64 loss: -0.19257748126983643
Batch 51/64 loss: -0.16793489456176758
Batch 52/64 loss: -0.1753368377685547
Batch 53/64 loss: -0.1689491868019104
Batch 54/64 loss: -0.18662863969802856
Batch 55/64 loss: -0.16182070970535278
Batch 56/64 loss: -0.17103904485702515
Batch 57/64 loss: -0.19714432954788208
Batch 58/64 loss: -0.14545202255249023
Batch 59/64 loss: -0.154660165309906
Batch 60/64 loss: -0.19542944431304932
Batch 61/64 loss: -0.14404672384262085
Batch 62/64 loss: -0.1882307529449463
Batch 63/64 loss: -0.1733921766281128
Batch 64/64 loss: -0.18380767107009888
Epoch 122  Train loss: -0.16455560268140307  Val loss: 0.014439504990462995
Epoch 123
-------------------------------
Batch 1/64 loss: -0.18499493598937988
Batch 2/64 loss: -0.18299925327301025
Batch 3/64 loss: -0.19220352172851562
Batch 4/64 loss: -0.16925281286239624
Batch 5/64 loss: -0.15253585577011108
Batch 6/64 loss: -0.17001068592071533
Batch 7/64 loss: -0.1625257134437561
Batch 8/64 loss: -0.18918699026107788
Batch 9/64 loss: -0.1610071063041687
Batch 10/64 loss: -0.17782598733901978
Batch 11/64 loss: -0.16386544704437256
Batch 12/64 loss: -0.17010903358459473
Batch 13/64 loss: -0.15845650434494019
Batch 14/64 loss: -0.15545105934143066
Batch 15/64 loss: -0.17176765203475952
Batch 16/64 loss: -0.162655770778656
Batch 17/64 loss: -0.18841755390167236
Batch 18/64 loss: -0.16455966234207153
Batch 19/64 loss: -0.1804567575454712
Batch 20/64 loss: -0.14916253089904785
Batch 21/64 loss: -0.16180908679962158
Batch 22/64 loss: -0.17229723930358887
Batch 23/64 loss: -0.10814076662063599
Batch 24/64 loss: -0.15508604049682617
Batch 25/64 loss: -0.1799699068069458
Batch 26/64 loss: -0.16686409711837769
Batch 27/64 loss: -0.15673404932022095
Batch 28/64 loss: -0.1794527769088745
Batch 29/64 loss: -0.18812322616577148
Batch 30/64 loss: -0.1659257411956787
Batch 31/64 loss: -0.15080994367599487
Batch 32/64 loss: -0.19111204147338867
Batch 33/64 loss: -0.1853882074356079
Batch 34/64 loss: -0.1711556315422058
Batch 35/64 loss: -0.1756678819656372
Batch 36/64 loss: -0.2025597095489502
Batch 37/64 loss: -0.14776217937469482
Batch 38/64 loss: -0.17129719257354736
Batch 39/64 loss: -0.1604636311531067
Batch 40/64 loss: -0.1599714159965515
Batch 41/64 loss: -0.1699293851852417
Batch 42/64 loss: -0.15782248973846436
Batch 43/64 loss: -0.14641636610031128
Batch 44/64 loss: -0.12266284227371216
Batch 45/64 loss: -0.1426706314086914
Batch 46/64 loss: -0.1706230640411377
Batch 47/64 loss: -0.15867221355438232
Batch 48/64 loss: -0.11989378929138184
Batch 49/64 loss: -0.1684563159942627
Batch 50/64 loss: -0.16589730978012085
Batch 51/64 loss: -0.1537182331085205
Batch 52/64 loss: -0.15982931852340698
Batch 53/64 loss: -0.17692571878433228
Batch 54/64 loss: -0.15644174814224243
Batch 55/64 loss: -0.15665340423583984
Batch 56/64 loss: -0.16921502351760864
Batch 57/64 loss: -0.1669343113899231
Batch 58/64 loss: -0.14318156242370605
Batch 59/64 loss: -0.19725364446640015
Batch 60/64 loss: -0.17000114917755127
Batch 61/64 loss: -0.18291014432907104
Batch 62/64 loss: -0.18011748790740967
Batch 63/64 loss: -0.175956130027771
Batch 64/64 loss: -0.14947688579559326
Epoch 123  Train loss: -0.1659972634969973  Val loss: 0.016944216903542326
Epoch 124
-------------------------------
Batch 1/64 loss: -0.183973491191864
Batch 2/64 loss: -0.1730024814605713
Batch 3/64 loss: -0.19750940799713135
Batch 4/64 loss: -0.17246180772781372
Batch 5/64 loss: -0.1778266429901123
Batch 6/64 loss: -0.17204183340072632
Batch 7/64 loss: -0.17918598651885986
Batch 8/64 loss: -0.15915381908416748
Batch 9/64 loss: -0.14059871435165405
Batch 10/64 loss: -0.14934998750686646
Batch 11/64 loss: -0.18578612804412842
Batch 12/64 loss: -0.1908503770828247
Batch 13/64 loss: -0.16620880365371704
Batch 14/64 loss: -0.18453526496887207
Batch 15/64 loss: -0.1757662296295166
Batch 16/64 loss: -0.17949587106704712
Batch 17/64 loss: -0.18939363956451416
Batch 18/64 loss: -0.14227360486984253
Batch 19/64 loss: -0.18422454595565796
Batch 20/64 loss: -0.14848661422729492
Batch 21/64 loss: -0.17526280879974365
Batch 22/64 loss: -0.1676287055015564
Batch 23/64 loss: -0.1663656234741211
Batch 24/64 loss: -0.1699601411819458
Batch 25/64 loss: -0.1775515079498291
Batch 26/64 loss: -0.1644544005393982
Batch 27/64 loss: -0.19361186027526855
Batch 28/64 loss: -0.16232025623321533
Batch 29/64 loss: -0.15788716077804565
Batch 30/64 loss: -0.1566387414932251
Batch 31/64 loss: -0.18341445922851562
Batch 32/64 loss: -0.19615203142166138
Batch 33/64 loss: -0.17458999156951904
Batch 34/64 loss: -0.17242401838302612
Batch 35/64 loss: -0.1797577142715454
Batch 36/64 loss: -0.1919703483581543
Batch 37/64 loss: -0.14885926246643066
Batch 38/64 loss: -0.16793394088745117
Batch 39/64 loss: -0.17002415657043457
Batch 40/64 loss: -0.1491687297821045
Batch 41/64 loss: -0.1842055320739746
Batch 42/64 loss: -0.1775159239768982
Batch 43/64 loss: -0.1884859800338745
Batch 44/64 loss: -0.18502849340438843
Batch 45/64 loss: -0.13504916429519653
Batch 46/64 loss: -0.17403137683868408
Batch 47/64 loss: -0.15004336833953857
Batch 48/64 loss: -0.17019802331924438
Batch 49/64 loss: -0.1392667293548584
Batch 50/64 loss: -0.1364402174949646
Batch 51/64 loss: -0.19148337841033936
Batch 52/64 loss: -0.1533462405204773
Batch 53/64 loss: -0.15737533569335938
Batch 54/64 loss: -0.15232360363006592
Batch 55/64 loss: -0.15505802631378174
Batch 56/64 loss: -0.15005981922149658
Batch 57/64 loss: -0.17401123046875
Batch 58/64 loss: -0.12551438808441162
Batch 59/64 loss: -0.16057205200195312
Batch 60/64 loss: -0.13646352291107178
Batch 61/64 loss: -0.17056787014007568
Batch 62/64 loss: -0.14568567276000977
Batch 63/64 loss: -0.16581356525421143
Batch 64/64 loss: -0.15676355361938477
Epoch 124  Train loss: -0.16743863984650256  Val loss: 0.01524580794921036
Epoch 125
-------------------------------
Batch 1/64 loss: -0.17487198114395142
Batch 2/64 loss: -0.17385882139205933
Batch 3/64 loss: -0.16895228624343872
Batch 4/64 loss: -0.1359657645225525
Batch 5/64 loss: -0.178391695022583
Batch 6/64 loss: -0.18328118324279785
Batch 7/64 loss: -0.17050576210021973
Batch 8/64 loss: -0.17003965377807617
Batch 9/64 loss: -0.17354494333267212
Batch 10/64 loss: -0.15953510999679565
Batch 11/64 loss: -0.19792819023132324
Batch 12/64 loss: -0.1611098051071167
Batch 13/64 loss: -0.18125426769256592
Batch 14/64 loss: -0.13827824592590332
Batch 15/64 loss: -0.16893577575683594
Batch 16/64 loss: -0.19711005687713623
Batch 17/64 loss: -0.17305046319961548
Batch 18/64 loss: -0.14369195699691772
Batch 19/64 loss: -0.19615602493286133
Batch 20/64 loss: -0.18334168195724487
Batch 21/64 loss: -0.16783159971237183
Batch 22/64 loss: -0.17913758754730225
Batch 23/64 loss: -0.16309762001037598
Batch 24/64 loss: -0.16448765993118286
Batch 25/64 loss: -0.15808647871017456
Batch 26/64 loss: -0.15975964069366455
Batch 27/64 loss: -0.16504377126693726
Batch 28/64 loss: -0.14301347732543945
Batch 29/64 loss: -0.16232478618621826
Batch 30/64 loss: -0.12701958417892456
Batch 31/64 loss: -0.178347647190094
Batch 32/64 loss: -0.15849977731704712
Batch 33/64 loss: -0.18124836683273315
Batch 34/64 loss: -0.17784321308135986
Batch 35/64 loss: -0.15061700344085693
Batch 36/64 loss: -0.1794087290763855
Batch 37/64 loss: -0.16472584009170532
Batch 38/64 loss: -0.16040164232254028
Batch 39/64 loss: -0.1547597050666809
Batch 40/64 loss: -0.1545148491859436
Batch 41/64 loss: -0.1566125750541687
Batch 42/64 loss: -0.1681588888168335
Batch 43/64 loss: -0.16719913482666016
Batch 44/64 loss: -0.17256689071655273
Batch 45/64 loss: -0.16020768880844116
Batch 46/64 loss: -0.18620431423187256
Batch 47/64 loss: -0.18305152654647827
Batch 48/64 loss: -0.14542484283447266
Batch 49/64 loss: -0.1874362826347351
Batch 50/64 loss: -0.13350898027420044
Batch 51/64 loss: -0.17539161443710327
Batch 52/64 loss: -0.172224223613739
Batch 53/64 loss: -0.18149280548095703
Batch 54/64 loss: -0.17956572771072388
Batch 55/64 loss: -0.15521681308746338
Batch 56/64 loss: -0.16268473863601685
Batch 57/64 loss: -0.15164077281951904
Batch 58/64 loss: -0.17907869815826416
Batch 59/64 loss: -0.17050564289093018
Batch 60/64 loss: -0.17771804332733154
Batch 61/64 loss: -0.18540441989898682
Batch 62/64 loss: -0.1815970540046692
Batch 63/64 loss: -0.16271936893463135
Batch 64/64 loss: -0.11431163549423218
Epoch 125  Train loss: -0.1672363517331142  Val loss: 0.020757413402046124
Epoch 126
-------------------------------
Batch 1/64 loss: -0.17416387796401978
Batch 2/64 loss: -0.1739562749862671
Batch 3/64 loss: -0.16763770580291748
Batch 4/64 loss: -0.18932992219924927
Batch 5/64 loss: -0.18177390098571777
Batch 6/64 loss: -0.18416166305541992
Batch 7/64 loss: -0.16352564096450806
Batch 8/64 loss: -0.16056954860687256
Batch 9/64 loss: -0.14950299263000488
Batch 10/64 loss: -0.17908644676208496
Batch 11/64 loss: -0.16906774044036865
Batch 12/64 loss: -0.17406731843948364
Batch 13/64 loss: -0.16604828834533691
Batch 14/64 loss: -0.13708853721618652
Batch 15/64 loss: -0.18043112754821777
Batch 16/64 loss: -0.18746930360794067
Batch 17/64 loss: -0.1959952712059021
Batch 18/64 loss: -0.17706823348999023
Batch 19/64 loss: -0.18316280841827393
Batch 20/64 loss: -0.17684036493301392
Batch 21/64 loss: -0.09387505054473877
Batch 22/64 loss: -0.1932774782180786
Batch 23/64 loss: -0.17451763153076172
Batch 24/64 loss: -0.17230093479156494
Batch 25/64 loss: -0.2044861912727356
Batch 26/64 loss: -0.12792450189590454
Batch 27/64 loss: -0.18687695264816284
Batch 28/64 loss: -0.1858360767364502
Batch 29/64 loss: -0.18262213468551636
Batch 30/64 loss: -0.17267870903015137
Batch 31/64 loss: -0.17014354467391968
Batch 32/64 loss: -0.13215506076812744
Batch 33/64 loss: -0.17320263385772705
Batch 34/64 loss: -0.16344833374023438
Batch 35/64 loss: -0.16921567916870117
Batch 36/64 loss: -0.1789553165435791
Batch 37/64 loss: -0.16707688570022583
Batch 38/64 loss: -0.1957281231880188
Batch 39/64 loss: -0.1360875964164734
Batch 40/64 loss: -0.1414581537246704
Batch 41/64 loss: -0.14681130647659302
Batch 42/64 loss: -0.14734166860580444
Batch 43/64 loss: -0.1541312336921692
Batch 44/64 loss: -0.17533183097839355
Batch 45/64 loss: -0.17758023738861084
Batch 46/64 loss: -0.1776413917541504
Batch 47/64 loss: -0.14197075366973877
Batch 48/64 loss: -0.1913052797317505
Batch 49/64 loss: -0.16520363092422485
Batch 50/64 loss: -0.15890955924987793
Batch 51/64 loss: -0.12872672080993652
Batch 52/64 loss: -0.1843193769454956
Batch 53/64 loss: -0.18014001846313477
Batch 54/64 loss: -0.17078322172164917
Batch 55/64 loss: -0.16903185844421387
Batch 56/64 loss: -0.18868255615234375
Batch 57/64 loss: -0.16386139392852783
Batch 58/64 loss: -0.1713266372680664
Batch 59/64 loss: -0.1548098921775818
Batch 60/64 loss: -0.16060328483581543
Batch 61/64 loss: -0.17021656036376953
Batch 62/64 loss: -0.19977408647537231
Batch 63/64 loss: -0.1686418652534485
Batch 64/64 loss: -0.16149383783340454
Epoch 126  Train loss: -0.168801234516443  Val loss: 0.01627425926247823
Epoch 127
-------------------------------
Batch 1/64 loss: -0.1709265112876892
Batch 2/64 loss: -0.19934165477752686
Batch 3/64 loss: -0.16491013765335083
Batch 4/64 loss: -0.200078547000885
Batch 5/64 loss: -0.15864324569702148
Batch 6/64 loss: -0.15354472398757935
Batch 7/64 loss: -0.16700923442840576
Batch 8/64 loss: -0.17649543285369873
Batch 9/64 loss: -0.19837361574172974
Batch 10/64 loss: -0.18263185024261475
Batch 11/64 loss: -0.1447962522506714
Batch 12/64 loss: -0.16603833436965942
Batch 13/64 loss: -0.19443362951278687
Batch 14/64 loss: -0.14994853734970093
Batch 15/64 loss: -0.1634312868118286
Batch 16/64 loss: -0.15857315063476562
Batch 17/64 loss: -0.17003345489501953
Batch 18/64 loss: -0.17215090990066528
Batch 19/64 loss: -0.16419047117233276
Batch 20/64 loss: -0.18416357040405273
Batch 21/64 loss: -0.14960592985153198
Batch 22/64 loss: -0.19583070278167725
Batch 23/64 loss: -0.1716914176940918
Batch 24/64 loss: -0.1980556845664978
Batch 25/64 loss: -0.1895509958267212
Batch 26/64 loss: -0.1719590425491333
Batch 27/64 loss: -0.141637921333313
Batch 28/64 loss: -0.12258797883987427
Batch 29/64 loss: -0.18384307622909546
Batch 30/64 loss: -0.17456936836242676
Batch 31/64 loss: -0.18486475944519043
Batch 32/64 loss: -0.18179190158843994
Batch 33/64 loss: -0.1215391755104065
Batch 34/64 loss: -0.13307523727416992
Batch 35/64 loss: -0.11952775716781616
Batch 36/64 loss: -0.17050564289093018
Batch 37/64 loss: -0.1802525520324707
Batch 38/64 loss: -0.16769540309906006
Batch 39/64 loss: -0.1805891990661621
Batch 40/64 loss: -0.18312937021255493
Batch 41/64 loss: -0.14949572086334229
Batch 42/64 loss: -0.1954401135444641
Batch 43/64 loss: -0.19467341899871826
Batch 44/64 loss: -0.1638321876525879
Batch 45/64 loss: -0.17204797267913818
Batch 46/64 loss: -0.17395180463790894
Batch 47/64 loss: -0.16775232553482056
Batch 48/64 loss: -0.1635739803314209
Batch 49/64 loss: -0.201901376247406
Batch 50/64 loss: -0.19401764869689941
Batch 51/64 loss: -0.15803462266921997
Batch 52/64 loss: -0.1396564245223999
Batch 53/64 loss: -0.1498919129371643
Batch 54/64 loss: -0.17573535442352295
Batch 55/64 loss: -0.17649829387664795
Batch 56/64 loss: -0.18708747625350952
Batch 57/64 loss: -0.16917896270751953
Batch 58/64 loss: -0.1763777732849121
Batch 59/64 loss: -0.19252783060073853
Batch 60/64 loss: -0.18940788507461548
Batch 61/64 loss: -0.15894341468811035
Batch 62/64 loss: -0.17489135265350342
Batch 63/64 loss: -0.1665644645690918
Batch 64/64 loss: -0.1520709991455078
Epoch 127  Train loss: -0.17047142328000536  Val loss: 0.015336490578667815
Epoch 128
-------------------------------
Batch 1/64 loss: -0.13766098022460938
Batch 2/64 loss: -0.1910998821258545
Batch 3/64 loss: -0.1580602526664734
Batch 4/64 loss: -0.19257718324661255
Batch 5/64 loss: -0.16402775049209595
Batch 6/64 loss: -0.17655372619628906
Batch 7/64 loss: -0.16764509677886963
Batch 8/64 loss: -0.16750335693359375
Batch 9/64 loss: -0.19292056560516357
Batch 10/64 loss: -0.19883698225021362
Batch 11/64 loss: -0.1707223653793335
Batch 12/64 loss: -0.21439945697784424
Batch 13/64 loss: -0.16287434101104736
Batch 14/64 loss: -0.13865941762924194
Batch 15/64 loss: -0.13824313879013062
Batch 16/64 loss: -0.20551437139511108
Batch 17/64 loss: -0.1785372495651245
Batch 18/64 loss: -0.17065554857254028
Batch 19/64 loss: -0.19456124305725098
Batch 20/64 loss: -0.17199116945266724
Batch 21/64 loss: -0.19987094402313232
Batch 22/64 loss: -0.1799336075782776
Batch 23/64 loss: -0.12699323892593384
Batch 24/64 loss: -0.11271131038665771
Batch 25/64 loss: -0.18100416660308838
Batch 26/64 loss: -0.16650176048278809
Batch 27/64 loss: -0.1832161545753479
Batch 28/64 loss: -0.18588989973068237
Batch 29/64 loss: -0.1686093807220459
Batch 30/64 loss: -0.2116527557373047
Batch 31/64 loss: -0.2054651379585266
Batch 32/64 loss: -0.1662842035293579
Batch 33/64 loss: -0.1830122470855713
Batch 34/64 loss: -0.1548769474029541
Batch 35/64 loss: -0.2076752781867981
Batch 36/64 loss: -0.182561993598938
Batch 37/64 loss: -0.16049504280090332
Batch 38/64 loss: -0.18389469385147095
Batch 39/64 loss: -0.1973159909248352
Batch 40/64 loss: -0.16697245836257935
Batch 41/64 loss: -0.16325825452804565
Batch 42/64 loss: -0.16106539964675903
Batch 43/64 loss: -0.16079753637313843
Batch 44/64 loss: -0.1612747311592102
Batch 45/64 loss: -0.18588793277740479
Batch 46/64 loss: -0.16311824321746826
Batch 47/64 loss: -0.20556020736694336
Batch 48/64 loss: -0.16704881191253662
Batch 49/64 loss: -0.18748575448989868
Batch 50/64 loss: -0.13087105751037598
Batch 51/64 loss: -0.1641790270805359
Batch 52/64 loss: -0.18271243572235107
Batch 53/64 loss: -0.18523412942886353
Batch 54/64 loss: -0.16711246967315674
Batch 55/64 loss: -0.17684262990951538
Batch 56/64 loss: -0.1815624237060547
Batch 57/64 loss: -0.18147319555282593
Batch 58/64 loss: -0.18677568435668945
Batch 59/64 loss: -0.19976484775543213
Batch 60/64 loss: -0.1972215175628662
Batch 61/64 loss: -0.15348511934280396
Batch 62/64 loss: -0.14192914962768555
Batch 63/64 loss: -0.16281551122665405
Batch 64/64 loss: -0.1467408537864685
Epoch 128  Train loss: -0.1740472627621071  Val loss: 0.015511020557167604
Epoch 129
-------------------------------
Batch 1/64 loss: -0.18401223421096802
Batch 2/64 loss: -0.16809523105621338
Batch 3/64 loss: -0.1789083480834961
Batch 4/64 loss: -0.17457032203674316
Batch 5/64 loss: -0.15752321481704712
Batch 6/64 loss: -0.15223026275634766
Batch 7/64 loss: -0.18211603164672852
Batch 8/64 loss: -0.18583208322525024
Batch 9/64 loss: -0.16573292016983032
Batch 10/64 loss: -0.1786777377128601
Batch 11/64 loss: -0.18176746368408203
Batch 12/64 loss: -0.19432145357131958
Batch 13/64 loss: -0.1879841685295105
Batch 14/64 loss: -0.16857916116714478
Batch 15/64 loss: -0.20514923334121704
Batch 16/64 loss: -0.1764392852783203
Batch 17/64 loss: -0.18725383281707764
Batch 18/64 loss: -0.1942288875579834
Batch 19/64 loss: -0.18844980001449585
Batch 20/64 loss: -0.20053619146347046
Batch 21/64 loss: -0.21807503700256348
Batch 22/64 loss: -0.15241563320159912
Batch 23/64 loss: -0.1799374222755432
Batch 24/64 loss: -0.1718379259109497
Batch 25/64 loss: -0.16416794061660767
Batch 26/64 loss: -0.2065873146057129
Batch 27/64 loss: -0.17134308815002441
Batch 28/64 loss: -0.1824798583984375
Batch 29/64 loss: -0.1737579107284546
Batch 30/64 loss: -0.201737642288208
Batch 31/64 loss: -0.1721094846725464
Batch 32/64 loss: -0.17656594514846802
Batch 33/64 loss: -0.20695239305496216
Batch 34/64 loss: -0.1751093864440918
Batch 35/64 loss: -0.18241417407989502
Batch 36/64 loss: -0.1608174443244934
Batch 37/64 loss: -0.16091275215148926
Batch 38/64 loss: -0.1718817949295044
Batch 39/64 loss: -0.2033790946006775
Batch 40/64 loss: -0.19690901041030884
Batch 41/64 loss: -0.14222383499145508
Batch 42/64 loss: -0.14169776439666748
Batch 43/64 loss: -0.1768677830696106
Batch 44/64 loss: -0.1812952756881714
Batch 45/64 loss: -0.16309618949890137
Batch 46/64 loss: -0.19142073392868042
Batch 47/64 loss: -0.1370558738708496
Batch 48/64 loss: -0.10600548982620239
Batch 49/64 loss: -0.17135071754455566
Batch 50/64 loss: -0.17633271217346191
Batch 51/64 loss: -0.18374353647232056
Batch 52/64 loss: -0.17787301540374756
Batch 53/64 loss: -0.1707286238670349
Batch 54/64 loss: -0.18190109729766846
Batch 55/64 loss: -0.1472928524017334
Batch 56/64 loss: -0.1701241135597229
Batch 57/64 loss: -0.14778590202331543
Batch 58/64 loss: -0.19322490692138672
Batch 59/64 loss: -0.1638762354850769
Batch 60/64 loss: -0.15939533710479736
Batch 61/64 loss: -0.1676090955734253
Batch 62/64 loss: -0.16551584005355835
Batch 63/64 loss: -0.1831754446029663
Batch 64/64 loss: -0.18755018711090088
Epoch 129  Train loss: -0.1754047707015393  Val loss: 0.013936954060780634
Epoch 130
-------------------------------
Batch 1/64 loss: -0.16575682163238525
Batch 2/64 loss: -0.16958683729171753
Batch 3/64 loss: -0.16087651252746582
Batch 4/64 loss: -0.19731783866882324
Batch 5/64 loss: -0.2006443738937378
Batch 6/64 loss: -0.1947629451751709
Batch 7/64 loss: -0.1690458059310913
Batch 8/64 loss: -0.18871533870697021
Batch 9/64 loss: -0.18670564889907837
Batch 10/64 loss: -0.18499279022216797
Batch 11/64 loss: -0.2111162543296814
Batch 12/64 loss: -0.1684330701828003
Batch 13/64 loss: -0.15862447023391724
Batch 14/64 loss: -0.17841064929962158
Batch 15/64 loss: -0.14903396368026733
Batch 16/64 loss: -0.19205933809280396
Batch 17/64 loss: -0.17836523056030273
Batch 18/64 loss: -0.15932470560073853
Batch 19/64 loss: -0.18385112285614014
Batch 20/64 loss: -0.1783442497253418
Batch 21/64 loss: -0.1979445219039917
Batch 22/64 loss: -0.20166146755218506
Batch 23/64 loss: -0.1881159543991089
Batch 24/64 loss: -0.16667425632476807
Batch 25/64 loss: -0.1680842638015747
Batch 26/64 loss: -0.14851367473602295
Batch 27/64 loss: -0.1528577208518982
Batch 28/64 loss: -0.17780500650405884
Batch 29/64 loss: -0.18255972862243652
Batch 30/64 loss: -0.15902268886566162
Batch 31/64 loss: -0.11823844909667969
Batch 32/64 loss: -0.1834501028060913
Batch 33/64 loss: -0.2027316689491272
Batch 34/64 loss: -0.1402745246887207
Batch 35/64 loss: -0.18219494819641113
Batch 36/64 loss: -0.18154841661453247
Batch 37/64 loss: -0.1629641056060791
Batch 38/64 loss: -0.17382365465164185
Batch 39/64 loss: -0.19369500875473022
Batch 40/64 loss: -0.17451417446136475
Batch 41/64 loss: -0.19142603874206543
Batch 42/64 loss: -0.1977909803390503
Batch 43/64 loss: -0.1971028447151184
Batch 44/64 loss: -0.16202610731124878
Batch 45/64 loss: -0.1618262529373169
Batch 46/64 loss: -0.17537802457809448
Batch 47/64 loss: -0.16527903079986572
Batch 48/64 loss: -0.1599709391593933
Batch 49/64 loss: -0.1917160153388977
Batch 50/64 loss: -0.18817448616027832
Batch 51/64 loss: -0.17429476976394653
Batch 52/64 loss: -0.12479734420776367
Batch 53/64 loss: -0.19369685649871826
Batch 54/64 loss: -0.1288132667541504
Batch 55/64 loss: -0.15082454681396484
Batch 56/64 loss: -0.18288898468017578
Batch 57/64 loss: -0.19366419315338135
Batch 58/64 loss: -0.17618519067764282
Batch 59/64 loss: -0.16655868291854858
Batch 60/64 loss: -0.1898273229598999
Batch 61/64 loss: -0.17888104915618896
Batch 62/64 loss: -0.13524454832077026
Batch 63/64 loss: -0.19121265411376953
Batch 64/64 loss: -0.13556808233261108
Epoch 130  Train loss: -0.17430429014505125  Val loss: 0.01701805743155201
Epoch 131
-------------------------------
Batch 1/64 loss: -0.15751999616622925
Batch 2/64 loss: -0.14648479223251343
Batch 3/64 loss: -0.14037251472473145
Batch 4/64 loss: -0.16783100366592407
Batch 5/64 loss: -0.172640860080719
Batch 6/64 loss: -0.16811925172805786
Batch 7/64 loss: -0.17332112789154053
Batch 8/64 loss: -0.15246832370758057
Batch 9/64 loss: -0.17404228448867798
Batch 10/64 loss: -0.18621069192886353
Batch 11/64 loss: -0.16452372074127197
Batch 12/64 loss: -0.19553744792938232
Batch 13/64 loss: -0.18748468160629272
Batch 14/64 loss: -0.1561673879623413
Batch 15/64 loss: -0.18129068613052368
Batch 16/64 loss: -0.15728330612182617
Batch 17/64 loss: -0.1857089400291443
Batch 18/64 loss: -0.18135684728622437
Batch 19/64 loss: -0.18279576301574707
Batch 20/64 loss: -0.14602839946746826
Batch 21/64 loss: -0.1900041699409485
Batch 22/64 loss: -0.18496298789978027
Batch 23/64 loss: -0.17891573905944824
Batch 24/64 loss: -0.2017151117324829
Batch 25/64 loss: -0.1840200424194336
Batch 26/64 loss: -0.19147473573684692
Batch 27/64 loss: -0.18335318565368652
Batch 28/64 loss: -0.15878278017044067
Batch 29/64 loss: -0.19649732112884521
Batch 30/64 loss: -0.15302890539169312
Batch 31/64 loss: -0.20167559385299683
Batch 32/64 loss: -0.1769288182258606
Batch 33/64 loss: -0.1934274435043335
Batch 34/64 loss: -0.18694788217544556
Batch 35/64 loss: -0.16119152307510376
Batch 36/64 loss: -0.18521082401275635
Batch 37/64 loss: -0.1769644021987915
Batch 38/64 loss: -0.19435065984725952
Batch 39/64 loss: -0.1792210340499878
Batch 40/64 loss: -0.1973496675491333
Batch 41/64 loss: -0.1768714189529419
Batch 42/64 loss: -0.16979271173477173
Batch 43/64 loss: -0.17281752824783325
Batch 44/64 loss: -0.13563036918640137
Batch 45/64 loss: -0.18297839164733887
Batch 46/64 loss: -0.189072847366333
Batch 47/64 loss: -0.19774824380874634
Batch 48/64 loss: -0.18295639753341675
Batch 49/64 loss: -0.1878027319908142
Batch 50/64 loss: -0.18311822414398193
Batch 51/64 loss: -0.1446601152420044
Batch 52/64 loss: -0.19281959533691406
Batch 53/64 loss: -0.19162476062774658
Batch 54/64 loss: -0.1817118525505066
Batch 55/64 loss: -0.17378836870193481
Batch 56/64 loss: -0.1641293168067932
Batch 57/64 loss: -0.20404070615768433
Batch 58/64 loss: -0.18198168277740479
Batch 59/64 loss: -0.16727495193481445
Batch 60/64 loss: -0.1870707869529724
Batch 61/64 loss: -0.20434445142745972
Batch 62/64 loss: -0.14144545793533325
Batch 63/64 loss: -0.20869213342666626
Batch 64/64 loss: -0.1726502776145935
Epoch 131  Train loss: -0.17733445658403285  Val loss: 0.014449532703845362
Epoch 132
-------------------------------
Batch 1/64 loss: -0.17623674869537354
Batch 2/64 loss: -0.1937909722328186
Batch 3/64 loss: -0.2072708010673523
Batch 4/64 loss: -0.16157948970794678
Batch 5/64 loss: -0.18497449159622192
Batch 6/64 loss: -0.21212148666381836
Batch 7/64 loss: -0.16597247123718262
Batch 8/64 loss: -0.16460102796554565
Batch 9/64 loss: -0.20830214023590088
Batch 10/64 loss: -0.19048726558685303
Batch 11/64 loss: -0.1506277322769165
Batch 12/64 loss: -0.1673753261566162
Batch 13/64 loss: -0.2030733823776245
Batch 14/64 loss: -0.18958216905593872
Batch 15/64 loss: -0.22108101844787598
Batch 16/64 loss: -0.19485503435134888
Batch 17/64 loss: -0.16795426607131958
Batch 18/64 loss: -0.20427542924880981
Batch 19/64 loss: -0.19504940509796143
Batch 20/64 loss: -0.15039068460464478
Batch 21/64 loss: -0.18546295166015625
Batch 22/64 loss: -0.17939990758895874
Batch 23/64 loss: -0.1541723608970642
Batch 24/64 loss: -0.17172563076019287
Batch 25/64 loss: -0.17702937126159668
Batch 26/64 loss: -0.1822635531425476
Batch 27/64 loss: -0.20185202360153198
Batch 28/64 loss: -0.2034536600112915
Batch 29/64 loss: -0.18513596057891846
Batch 30/64 loss: -0.17265713214874268
Batch 31/64 loss: -0.20808202028274536
Batch 32/64 loss: -0.1739901900291443
Batch 33/64 loss: -0.18637782335281372
Batch 34/64 loss: -0.2010023593902588
Batch 35/64 loss: -0.1721435785293579
Batch 36/64 loss: -0.18036013841629028
Batch 37/64 loss: -0.17427551746368408
Batch 38/64 loss: -0.17818284034729004
Batch 39/64 loss: -0.1520524024963379
Batch 40/64 loss: -0.19505375623703003
Batch 41/64 loss: -0.14746040105819702
Batch 42/64 loss: -0.18962061405181885
Batch 43/64 loss: -0.17310702800750732
Batch 44/64 loss: -0.17224210500717163
Batch 45/64 loss: -0.15013432502746582
Batch 46/64 loss: -0.1830427646636963
Batch 47/64 loss: -0.15972697734832764
Batch 48/64 loss: -0.19278192520141602
Batch 49/64 loss: -0.15854787826538086
Batch 50/64 loss: -0.17400449514389038
Batch 51/64 loss: -0.19450753927230835
Batch 52/64 loss: -0.18938273191452026
Batch 53/64 loss: -0.16680514812469482
Batch 54/64 loss: -0.19637316465377808
Batch 55/64 loss: -0.1923806071281433
Batch 56/64 loss: -0.15041178464889526
Batch 57/64 loss: -0.16420412063598633
Batch 58/64 loss: -0.17550992965698242
Batch 59/64 loss: -0.16905736923217773
Batch 60/64 loss: -0.17089688777923584
Batch 61/64 loss: -0.16829794645309448
Batch 62/64 loss: -0.18136531114578247
Batch 63/64 loss: -0.19535446166992188
Batch 64/64 loss: -0.1841091513633728
Epoch 132  Train loss: -0.1803540690272462  Val loss: 0.017332850452960562
Epoch 133
-------------------------------
Batch 1/64 loss: -0.188867449760437
Batch 2/64 loss: -0.17682260274887085
Batch 3/64 loss: -0.1999363899230957
Batch 4/64 loss: -0.17796462774276733
Batch 5/64 loss: -0.17748147249221802
Batch 6/64 loss: -0.18501633405685425
Batch 7/64 loss: -0.2184402346611023
Batch 8/64 loss: -0.2078753113746643
Batch 9/64 loss: -0.18890196084976196
Batch 10/64 loss: -0.18975096940994263
Batch 11/64 loss: -0.19845819473266602
Batch 12/64 loss: -0.22487974166870117
Batch 13/64 loss: -0.16160380840301514
Batch 14/64 loss: -0.193572998046875
Batch 15/64 loss: -0.19553548097610474
Batch 16/64 loss: -0.19487804174423218
Batch 17/64 loss: -0.17998671531677246
Batch 18/64 loss: -0.14658474922180176
Batch 19/64 loss: -0.17029130458831787
Batch 20/64 loss: -0.18187403678894043
Batch 21/64 loss: -0.19240570068359375
Batch 22/64 loss: -0.14529460668563843
Batch 23/64 loss: -0.18704408407211304
Batch 24/64 loss: -0.18780303001403809
Batch 25/64 loss: -0.1777973175048828
Batch 26/64 loss: -0.132767915725708
Batch 27/64 loss: -0.1896122694015503
Batch 28/64 loss: -0.1863054633140564
Batch 29/64 loss: -0.1790170669555664
Batch 30/64 loss: -0.1512817144393921
Batch 31/64 loss: -0.16429901123046875
Batch 32/64 loss: -0.1651214361190796
Batch 33/64 loss: -0.1502392292022705
Batch 34/64 loss: -0.1757429838180542
Batch 35/64 loss: -0.166032612323761
Batch 36/64 loss: -0.17216044664382935
Batch 37/64 loss: -0.1651836633682251
Batch 38/64 loss: -0.2009791135787964
Batch 39/64 loss: -0.1985173225402832
Batch 40/64 loss: -0.18293124437332153
Batch 41/64 loss: -0.16570764780044556
Batch 42/64 loss: -0.19261085987091064
Batch 43/64 loss: -0.16876649856567383
Batch 44/64 loss: -0.16868585348129272
Batch 45/64 loss: -0.1791750192642212
Batch 46/64 loss: -0.1887052059173584
Batch 47/64 loss: -0.1698024868965149
Batch 48/64 loss: -0.13649004697799683
Batch 49/64 loss: -0.18491113185882568
Batch 50/64 loss: -0.18909192085266113
Batch 51/64 loss: -0.15090668201446533
Batch 52/64 loss: -0.1620396375656128
Batch 53/64 loss: -0.16207391023635864
Batch 54/64 loss: -0.17392003536224365
Batch 55/64 loss: -0.16452628374099731
Batch 56/64 loss: -0.16558796167373657
Batch 57/64 loss: -0.1555565595626831
Batch 58/64 loss: -0.15042191743850708
Batch 59/64 loss: -0.13864171504974365
Batch 60/64 loss: -0.16962289810180664
Batch 61/64 loss: -0.17859524488449097
Batch 62/64 loss: -0.20257991552352905
Batch 63/64 loss: -0.19354623556137085
Batch 64/64 loss: -0.16180813312530518
Epoch 133  Train loss: -0.1766679282281913  Val loss: 0.018181255183269067
Epoch 134
-------------------------------
Batch 1/64 loss: -0.18103653192520142
Batch 2/64 loss: -0.21479380130767822
Batch 3/64 loss: -0.1566176414489746
Batch 4/64 loss: -0.1823243498802185
Batch 5/64 loss: -0.15989196300506592
Batch 6/64 loss: -0.16151177883148193
Batch 7/64 loss: -0.16945022344589233
Batch 8/64 loss: -0.18525290489196777
Batch 9/64 loss: -0.18373191356658936
Batch 10/64 loss: -0.20455312728881836
Batch 11/64 loss: -0.1998286247253418
Batch 12/64 loss: -0.15449756383895874
Batch 13/64 loss: -0.18545132875442505
Batch 14/64 loss: -0.1735965609550476
Batch 15/64 loss: -0.18134963512420654
Batch 16/64 loss: -0.16788232326507568
Batch 17/64 loss: -0.18713760375976562
Batch 18/64 loss: -0.17997223138809204
Batch 19/64 loss: -0.18093371391296387
Batch 20/64 loss: -0.19015979766845703
Batch 21/64 loss: -0.1829512119293213
Batch 22/64 loss: -0.19812941551208496
Batch 23/64 loss: -0.1918737292289734
Batch 24/64 loss: -0.15919679403305054
Batch 25/64 loss: -0.21108472347259521
Batch 26/64 loss: -0.19609332084655762
Batch 27/64 loss: -0.18104267120361328
Batch 28/64 loss: -0.18803179264068604
Batch 29/64 loss: -0.1951853632926941
Batch 30/64 loss: -0.18751460313796997
Batch 31/64 loss: -0.17952489852905273
Batch 32/64 loss: -0.1881396770477295
Batch 33/64 loss: -0.19581234455108643
Batch 34/64 loss: -0.16210871934890747
Batch 35/64 loss: -0.1542038917541504
Batch 36/64 loss: -0.16532617807388306
Batch 37/64 loss: -0.19042199850082397
Batch 38/64 loss: -0.19684100151062012
Batch 39/64 loss: -0.15040284395217896
Batch 40/64 loss: -0.20134639739990234
Batch 41/64 loss: -0.15633106231689453
Batch 42/64 loss: -0.1797173023223877
Batch 43/64 loss: -0.16058534383773804
Batch 44/64 loss: -0.2033374309539795
Batch 45/64 loss: -0.19743144512176514
Batch 46/64 loss: -0.18095123767852783
Batch 47/64 loss: -0.17576384544372559
Batch 48/64 loss: -0.16039860248565674
Batch 49/64 loss: -0.14870089292526245
Batch 50/64 loss: -0.19091618061065674
Batch 51/64 loss: -0.17798376083374023
Batch 52/64 loss: -0.1800159215927124
Batch 53/64 loss: -0.21016216278076172
Batch 54/64 loss: -0.18087232112884521
Batch 55/64 loss: -0.20600879192352295
Batch 56/64 loss: -0.1760009527206421
Batch 57/64 loss: -0.1406152844429016
Batch 58/64 loss: -0.19379550218582153
Batch 59/64 loss: -0.16129541397094727
Batch 60/64 loss: -0.154893696308136
Batch 61/64 loss: -0.15980327129364014
Batch 62/64 loss: -0.17446720600128174
Batch 63/64 loss: -0.20122134685516357
Batch 64/64 loss: -0.1651473045349121
Epoch 134  Train loss: -0.17992681802487842  Val loss: 0.013496488230334934
Epoch 135
-------------------------------
Batch 1/64 loss: -0.20707553625106812
Batch 2/64 loss: -0.2326989769935608
Batch 3/64 loss: -0.2001515030860901
Batch 4/64 loss: -0.17257577180862427
Batch 5/64 loss: -0.20254838466644287
Batch 6/64 loss: -0.18459737300872803
Batch 7/64 loss: -0.20300674438476562
Batch 8/64 loss: -0.19692498445510864
Batch 9/64 loss: -0.18535691499710083
Batch 10/64 loss: -0.1953181028366089
Batch 11/64 loss: -0.1987922191619873
Batch 12/64 loss: -0.20203548669815063
Batch 13/64 loss: -0.2083762288093567
Batch 14/64 loss: -0.16895437240600586
Batch 15/64 loss: -0.16936731338500977
Batch 16/64 loss: -0.19566285610198975
Batch 17/64 loss: -0.20206362009048462
Batch 18/64 loss: -0.16696226596832275
Batch 19/64 loss: -0.1671661138534546
Batch 20/64 loss: -0.16165047883987427
Batch 21/64 loss: -0.13048607110977173
Batch 22/64 loss: -0.1580677032470703
Batch 23/64 loss: -0.19746720790863037
Batch 24/64 loss: -0.16162598133087158
Batch 25/64 loss: -0.1984025239944458
Batch 26/64 loss: -0.2093769907951355
Batch 27/64 loss: -0.17097151279449463
Batch 28/64 loss: -0.1786852478981018
Batch 29/64 loss: -0.18064916133880615
Batch 30/64 loss: -0.19546711444854736
Batch 31/64 loss: -0.14828616380691528
Batch 32/64 loss: -0.17354190349578857
Batch 33/64 loss: -0.153103768825531
Batch 34/64 loss: -0.16401708126068115
Batch 35/64 loss: -0.18050140142440796
Batch 36/64 loss: -0.1928582787513733
Batch 37/64 loss: -0.1924741268157959
Batch 38/64 loss: -0.1920783519744873
Batch 39/64 loss: -0.16727304458618164
Batch 40/64 loss: -0.18269455432891846
Batch 41/64 loss: -0.17605900764465332
Batch 42/64 loss: -0.18137603998184204
Batch 43/64 loss: -0.15460044145584106
Batch 44/64 loss: -0.18474364280700684
Batch 45/64 loss: -0.1296941637992859
Batch 46/64 loss: -0.19208425283432007
Batch 47/64 loss: -0.16350603103637695
Batch 48/64 loss: -0.17410343885421753
Batch 49/64 loss: -0.18334448337554932
Batch 50/64 loss: -0.12771236896514893
Batch 51/64 loss: -0.17372405529022217
Batch 52/64 loss: -0.18819177150726318
Batch 53/64 loss: -0.20174115896224976
Batch 54/64 loss: -0.16560721397399902
Batch 55/64 loss: -0.162786602973938
Batch 56/64 loss: -0.18547159433364868
Batch 57/64 loss: -0.13368910551071167
Batch 58/64 loss: -0.17224156856536865
Batch 59/64 loss: -0.1574600338935852
Batch 60/64 loss: -0.20051956176757812
Batch 61/64 loss: -0.1914893388748169
Batch 62/64 loss: -0.19802749156951904
Batch 63/64 loss: -0.20299172401428223
Batch 64/64 loss: -0.19487690925598145
Epoch 135  Train loss: -0.18033939155877804  Val loss: 0.018197340244280102
Epoch 136
-------------------------------
Batch 1/64 loss: -0.17932438850402832
Batch 2/64 loss: -0.19891715049743652
Batch 3/64 loss: -0.20006155967712402
Batch 4/64 loss: -0.18483346700668335
Batch 5/64 loss: -0.20113039016723633
Batch 6/64 loss: -0.1732473373413086
Batch 7/64 loss: -0.17796945571899414
Batch 8/64 loss: -0.17980456352233887
Batch 9/64 loss: -0.17972689867019653
Batch 10/64 loss: -0.18937033414840698
Batch 11/64 loss: -0.16342192888259888
Batch 12/64 loss: -0.2019580602645874
Batch 13/64 loss: -0.2091144323348999
Batch 14/64 loss: -0.164933443069458
Batch 15/64 loss: -0.16263025999069214
Batch 16/64 loss: -0.17600160837173462
Batch 17/64 loss: -0.19993829727172852
Batch 18/64 loss: -0.08669233322143555
Batch 19/64 loss: -0.17976266145706177
Batch 20/64 loss: -0.21300923824310303
Batch 21/64 loss: -0.14791887998580933
Batch 22/64 loss: -0.20923763513565063
Batch 23/64 loss: -0.1967344880104065
Batch 24/64 loss: -0.17258745431900024
Batch 25/64 loss: -0.1779087781906128
Batch 26/64 loss: -0.17298537492752075
Batch 27/64 loss: -0.20283228158950806
Batch 28/64 loss: -0.18492388725280762
Batch 29/64 loss: -0.19892525672912598
Batch 30/64 loss: -0.1816626787185669
Batch 31/64 loss: -0.16945797204971313
Batch 32/64 loss: -0.1472962498664856
Batch 33/64 loss: -0.21400326490402222
Batch 34/64 loss: -0.18008220195770264
Batch 35/64 loss: -0.1745637059211731
Batch 36/64 loss: -0.15622246265411377
Batch 37/64 loss: -0.1804746389389038
Batch 38/64 loss: -0.19253617525100708
Batch 39/64 loss: -0.18254119157791138
Batch 40/64 loss: -0.1749069094657898
Batch 41/64 loss: -0.15862280130386353
Batch 42/64 loss: -0.17376720905303955
Batch 43/64 loss: -0.209183931350708
Batch 44/64 loss: -0.18199318647384644
Batch 45/64 loss: -0.1271762251853943
Batch 46/64 loss: -0.20086991786956787
Batch 47/64 loss: -0.16910237073898315
Batch 48/64 loss: -0.17727798223495483
Batch 49/64 loss: -0.16754239797592163
Batch 50/64 loss: -0.1899106502532959
Batch 51/64 loss: -0.19743025302886963
Batch 52/64 loss: -0.18397164344787598
Batch 53/64 loss: -0.1362714171409607
Batch 54/64 loss: -0.17524147033691406
Batch 55/64 loss: -0.20761024951934814
Batch 56/64 loss: -0.19361329078674316
Batch 57/64 loss: -0.16580486297607422
Batch 58/64 loss: -0.19649899005889893
Batch 59/64 loss: -0.20204156637191772
Batch 60/64 loss: -0.21633899211883545
Batch 61/64 loss: -0.18535274267196655
Batch 62/64 loss: -0.17962682247161865
Batch 63/64 loss: -0.21045172214508057
Batch 64/64 loss: -0.17667609453201294
Epoch 136  Train loss: -0.18161344411326388  Val loss: 0.015392009540112158
Epoch 137
-------------------------------
Batch 1/64 loss: -0.1624622344970703
Batch 2/64 loss: -0.2082863450050354
Batch 3/64 loss: -0.19199973344802856
Batch 4/64 loss: -0.16538745164871216
Batch 5/64 loss: -0.19883888959884644
Batch 6/64 loss: -0.16331017017364502
Batch 7/64 loss: -0.19049572944641113
Batch 8/64 loss: -0.20336955785751343
Batch 9/64 loss: -0.19323307275772095
Batch 10/64 loss: -0.17548006772994995
Batch 11/64 loss: -0.1934807300567627
Batch 12/64 loss: -0.18654859066009521
Batch 13/64 loss: -0.21198320388793945
Batch 14/64 loss: -0.1856403946876526
Batch 15/64 loss: -0.20049870014190674
Batch 16/64 loss: -0.16882753372192383
Batch 17/64 loss: -0.18323469161987305
Batch 18/64 loss: -0.17146426439285278
Batch 19/64 loss: -0.205552339553833
Batch 20/64 loss: -0.14868342876434326
Batch 21/64 loss: -0.18750381469726562
Batch 22/64 loss: -0.20397889614105225
Batch 23/64 loss: -0.16803312301635742
Batch 24/64 loss: -0.19484978914260864
Batch 25/64 loss: -0.20892119407653809
Batch 26/64 loss: -0.20792806148529053
Batch 27/64 loss: -0.1760692000389099
Batch 28/64 loss: -0.20346814393997192
Batch 29/64 loss: -0.2192959189414978
Batch 30/64 loss: -0.19073623418807983
Batch 31/64 loss: -0.21026742458343506
Batch 32/64 loss: -0.1672661304473877
Batch 33/64 loss: -0.1971052885055542
Batch 34/64 loss: -0.22308582067489624
Batch 35/64 loss: -0.20919370651245117
Batch 36/64 loss: -0.19826608896255493
Batch 37/64 loss: -0.20648252964019775
Batch 38/64 loss: -0.15121984481811523
Batch 39/64 loss: -0.2067718505859375
Batch 40/64 loss: -0.17885243892669678
Batch 41/64 loss: -0.16742265224456787
Batch 42/64 loss: -0.16072803735733032
Batch 43/64 loss: -0.17731690406799316
Batch 44/64 loss: -0.19709092378616333
Batch 45/64 loss: -0.1466665267944336
Batch 46/64 loss: -0.17425072193145752
Batch 47/64 loss: -0.20114994049072266
Batch 48/64 loss: -0.16042989492416382
Batch 49/64 loss: -0.19014453887939453
Batch 50/64 loss: -0.18633931875228882
Batch 51/64 loss: -0.1715756058692932
Batch 52/64 loss: -0.19223225116729736
Batch 53/64 loss: -0.186856210231781
Batch 54/64 loss: -0.20577311515808105
Batch 55/64 loss: -0.17692428827285767
Batch 56/64 loss: -0.16163420677185059
Batch 57/64 loss: -0.19465136528015137
Batch 58/64 loss: -0.16468602418899536
Batch 59/64 loss: -0.187178373336792
Batch 60/64 loss: -0.17198491096496582
Batch 61/64 loss: -0.1974831223487854
Batch 62/64 loss: -0.16170907020568848
Batch 63/64 loss: -0.20875799655914307
Batch 64/64 loss: -0.17552775144577026
Epoch 137  Train loss: -0.1865522265434265  Val loss: 0.01823417427613563
Epoch 138
-------------------------------
Batch 1/64 loss: -0.19993817806243896
Batch 2/64 loss: -0.19304484128952026
Batch 3/64 loss: -0.11006903648376465
Batch 4/64 loss: -0.17060929536819458
Batch 5/64 loss: -0.162445068359375
Batch 6/64 loss: -0.18771982192993164
Batch 7/64 loss: -0.2012682557106018
Batch 8/64 loss: -0.19849073886871338
Batch 9/64 loss: -0.2108306884765625
Batch 10/64 loss: -0.19148647785186768
Batch 11/64 loss: -0.17246133089065552
Batch 12/64 loss: -0.17812031507492065
Batch 13/64 loss: -0.19148331880569458
Batch 14/64 loss: -0.2301570177078247
Batch 15/64 loss: -0.19456839561462402
Batch 16/64 loss: -0.18457597494125366
Batch 17/64 loss: -0.1737629771232605
Batch 18/64 loss: -0.194769024848938
Batch 19/64 loss: -0.19542133808135986
Batch 20/64 loss: -0.20996052026748657
Batch 21/64 loss: -0.1985958218574524
Batch 22/64 loss: -0.16675102710723877
Batch 23/64 loss: -0.16868799924850464
Batch 24/64 loss: -0.20883697271347046
Batch 25/64 loss: -0.19257694482803345
Batch 26/64 loss: -0.2054685354232788
Batch 27/64 loss: -0.2003064751625061
Batch 28/64 loss: -0.21320128440856934
Batch 29/64 loss: -0.16775614023208618
Batch 30/64 loss: -0.19343990087509155
Batch 31/64 loss: -0.18638747930526733
Batch 32/64 loss: -0.17734259366989136
Batch 33/64 loss: -0.19230276346206665
Batch 34/64 loss: -0.1395893096923828
Batch 35/64 loss: -0.16849279403686523
Batch 36/64 loss: -0.192430317401886
Batch 37/64 loss: -0.17394113540649414
Batch 38/64 loss: -0.17336106300354004
Batch 39/64 loss: -0.18598198890686035
Batch 40/64 loss: -0.15729010105133057
Batch 41/64 loss: -0.19886064529418945
Batch 42/64 loss: -0.15904170274734497
Batch 43/64 loss: -0.18363428115844727
Batch 44/64 loss: -0.1870242953300476
Batch 45/64 loss: -0.1943732500076294
Batch 46/64 loss: -0.18451285362243652
Batch 47/64 loss: -0.20013803243637085
Batch 48/64 loss: -0.2075173258781433
Batch 49/64 loss: -0.20770078897476196
Batch 50/64 loss: -0.18084430694580078
Batch 51/64 loss: -0.1537414789199829
Batch 52/64 loss: -0.21415406465530396
Batch 53/64 loss: -0.18561184406280518
Batch 54/64 loss: -0.18630021810531616
Batch 55/64 loss: -0.1920483112335205
Batch 56/64 loss: -0.1773000955581665
Batch 57/64 loss: -0.19155490398406982
Batch 58/64 loss: -0.17778515815734863
Batch 59/64 loss: -0.1973036527633667
Batch 60/64 loss: -0.18413019180297852
Batch 61/64 loss: -0.19293808937072754
Batch 62/64 loss: -0.18256783485412598
Batch 63/64 loss: -0.20945262908935547
Batch 64/64 loss: -0.18357419967651367
Epoch 138  Train loss: -0.1866688606785793  Val loss: 0.01747688968566685
Epoch 139
-------------------------------
Batch 1/64 loss: -0.1818857192993164
Batch 2/64 loss: -0.17855000495910645
Batch 3/64 loss: -0.1656511425971985
Batch 4/64 loss: -0.1831289529800415
Batch 5/64 loss: -0.15666258335113525
Batch 6/64 loss: -0.17545074224472046
Batch 7/64 loss: -0.1850690245628357
Batch 8/64 loss: -0.1728125810623169
Batch 9/64 loss: -0.20012658834457397
Batch 10/64 loss: -0.20012658834457397
Batch 11/64 loss: -0.21420389413833618
Batch 12/64 loss: -0.22633475065231323
Batch 13/64 loss: -0.18031197786331177
Batch 14/64 loss: -0.17589306831359863
Batch 15/64 loss: -0.19277918338775635
Batch 16/64 loss: -0.21598803997039795
Batch 17/64 loss: -0.20742368698120117
Batch 18/64 loss: -0.18512952327728271
Batch 19/64 loss: -0.18076294660568237
Batch 20/64 loss: -0.19241935014724731
Batch 21/64 loss: -0.19352185726165771
Batch 22/64 loss: -0.184617280960083
Batch 23/64 loss: -0.17468976974487305
Batch 24/64 loss: -0.16696548461914062
Batch 25/64 loss: -0.198344886302948
Batch 26/64 loss: -0.19240951538085938
Batch 27/64 loss: -0.21535539627075195
Batch 28/64 loss: -0.1722758412361145
Batch 29/64 loss: -0.18491244316101074
Batch 30/64 loss: -0.20177394151687622
Batch 31/64 loss: -0.17947494983673096
Batch 32/64 loss: -0.17836564779281616
Batch 33/64 loss: -0.18523907661437988
Batch 34/64 loss: -0.15360373258590698
Batch 35/64 loss: -0.1926974058151245
Batch 36/64 loss: -0.17364585399627686
Batch 37/64 loss: -0.18486922979354858
Batch 38/64 loss: -0.21387076377868652
Batch 39/64 loss: -0.21483296155929565
Batch 40/64 loss: -0.1766543984413147
Batch 41/64 loss: -0.17732959985733032
Batch 42/64 loss: -0.19483709335327148
Batch 43/64 loss: -0.14632904529571533
Batch 44/64 loss: -0.1503608226776123
Batch 45/64 loss: -0.22633105516433716
Batch 46/64 loss: -0.16458433866500854
Batch 47/64 loss: -0.18996232748031616
Batch 48/64 loss: -0.19082355499267578
Batch 49/64 loss: -0.192821204662323
Batch 50/64 loss: -0.20602333545684814
Batch 51/64 loss: -0.21167129278182983
Batch 52/64 loss: -0.18995165824890137
Batch 53/64 loss: -0.21701884269714355
Batch 54/64 loss: -0.17907369136810303
Batch 55/64 loss: -0.22394698858261108
Batch 56/64 loss: -0.16612553596496582
Batch 57/64 loss: -0.19578802585601807
Batch 58/64 loss: -0.19402194023132324
Batch 59/64 loss: -0.15339785814285278
Batch 60/64 loss: -0.20220071077346802
Batch 61/64 loss: -0.22261208295822144
Batch 62/64 loss: -0.18425077199935913
Batch 63/64 loss: -0.18875110149383545
Batch 64/64 loss: -0.19778531789779663
Epoch 139  Train loss: -0.18863345337849038  Val loss: 0.01916793982187907
Epoch 140
-------------------------------
Batch 1/64 loss: -0.17626053094863892
Batch 2/64 loss: -0.18552839756011963
Batch 3/64 loss: -0.22048991918563843
Batch 4/64 loss: -0.21492892503738403
Batch 5/64 loss: -0.19357073307037354
Batch 6/64 loss: -0.1710553765296936
Batch 7/64 loss: -0.20037221908569336
Batch 8/64 loss: -0.19211244583129883
Batch 9/64 loss: -0.19790858030319214
Batch 10/64 loss: -0.19587171077728271
Batch 11/64 loss: -0.18102222681045532
Batch 12/64 loss: -0.19697457551956177
Batch 13/64 loss: -0.14628630876541138
Batch 14/64 loss: -0.20877563953399658
Batch 15/64 loss: -0.191836416721344
Batch 16/64 loss: -0.21920311450958252
Batch 17/64 loss: -0.17597460746765137
Batch 18/64 loss: -0.17296981811523438
Batch 19/64 loss: -0.19704973697662354
Batch 20/64 loss: -0.18011486530303955
Batch 21/64 loss: -0.19548243284225464
Batch 22/64 loss: -0.168590247631073
Batch 23/64 loss: -0.1905723214149475
Batch 24/64 loss: -0.1939328908920288
Batch 25/64 loss: -0.20967161655426025
Batch 26/64 loss: -0.14416706562042236
Batch 27/64 loss: -0.14175283908843994
Batch 28/64 loss: -0.2022935152053833
Batch 29/64 loss: -0.16659581661224365
Batch 30/64 loss: -0.16558879613876343
Batch 31/64 loss: -0.1565624475479126
Batch 32/64 loss: -0.18367117643356323
Batch 33/64 loss: -0.16714584827423096
Batch 34/64 loss: -0.1707620620727539
Batch 35/64 loss: -0.20293903350830078
Batch 36/64 loss: -0.16302496194839478
Batch 37/64 loss: -0.18766236305236816
Batch 38/64 loss: -0.19223207235336304
Batch 39/64 loss: -0.21340209245681763
Batch 40/64 loss: -0.20530200004577637
Batch 41/64 loss: -0.17558300495147705
Batch 42/64 loss: -0.16573727130889893
Batch 43/64 loss: -0.21839922666549683
Batch 44/64 loss: -0.20621258020401
Batch 45/64 loss: -0.2166493535041809
Batch 46/64 loss: -0.17056256532669067
Batch 47/64 loss: -0.15717464685440063
Batch 48/64 loss: -0.1994647979736328
Batch 49/64 loss: -0.2229793667793274
Batch 50/64 loss: -0.202683687210083
Batch 51/64 loss: -0.20998269319534302
Batch 52/64 loss: -0.17483270168304443
Batch 53/64 loss: -0.19301080703735352
Batch 54/64 loss: -0.2119535207748413
Batch 55/64 loss: -0.1937686800956726
Batch 56/64 loss: -0.15720856189727783
Batch 57/64 loss: -0.1991376280784607
Batch 58/64 loss: -0.12845975160598755
Batch 59/64 loss: -0.205999493598938
Batch 60/64 loss: -0.2115684151649475
Batch 61/64 loss: -0.2018943428993225
Batch 62/64 loss: -0.19383180141448975
Batch 63/64 loss: -0.18370622396469116
Batch 64/64 loss: -0.13717466592788696
Epoch 140  Train loss: -0.18734650775498035  Val loss: 0.016337822393043756
Epoch 141
-------------------------------
Batch 1/64 loss: -0.2120649218559265
Batch 2/64 loss: -0.21088284254074097
Batch 3/64 loss: -0.16471874713897705
Batch 4/64 loss: -0.2327325940132141
Batch 5/64 loss: -0.19130414724349976
Batch 6/64 loss: -0.18724465370178223
Batch 7/64 loss: -0.22048068046569824
Batch 8/64 loss: -0.1905803680419922
Batch 9/64 loss: -0.19005239009857178
Batch 10/64 loss: -0.1385287642478943
Batch 11/64 loss: -0.1902409791946411
Batch 12/64 loss: -0.19234317541122437
Batch 13/64 loss: -0.17709648609161377
Batch 14/64 loss: -0.1885107159614563
Batch 15/64 loss: -0.19521164894104004
Batch 16/64 loss: -0.19019293785095215
Batch 17/64 loss: -0.1983344554901123
Batch 18/64 loss: -0.2034924030303955
Batch 19/64 loss: -0.19026875495910645
Batch 20/64 loss: -0.16561371088027954
Batch 21/64 loss: -0.2032407522201538
Batch 22/64 loss: -0.17052000761032104
Batch 23/64 loss: -0.17251795530319214
Batch 24/64 loss: -0.14820986986160278
Batch 25/64 loss: -0.1383311152458191
Batch 26/64 loss: -0.1402747631072998
Batch 27/64 loss: -0.18082588911056519
Batch 28/64 loss: -0.18577265739440918
Batch 29/64 loss: -0.1932581663131714
Batch 30/64 loss: -0.17961502075195312
Batch 31/64 loss: -0.19249486923217773
Batch 32/64 loss: -0.17347240447998047
Batch 33/64 loss: -0.16768622398376465
Batch 34/64 loss: -0.1654430627822876
Batch 35/64 loss: -0.1900157332420349
Batch 36/64 loss: -0.15322595834732056
Batch 37/64 loss: -0.19572532176971436
Batch 38/64 loss: -0.18029481172561646
Batch 39/64 loss: -0.17903345823287964
Batch 40/64 loss: -0.1596454381942749
Batch 41/64 loss: -0.1841217279434204
Batch 42/64 loss: -0.15979045629501343
Batch 43/64 loss: -0.16851133108139038
Batch 44/64 loss: -0.2120775580406189
Batch 45/64 loss: -0.2027551531791687
Batch 46/64 loss: -0.21345341205596924
Batch 47/64 loss: -0.17501050233840942
Batch 48/64 loss: -0.1734594702720642
Batch 49/64 loss: -0.18753397464752197
Batch 50/64 loss: -0.19070369005203247
Batch 51/64 loss: -0.20657837390899658
Batch 52/64 loss: -0.1929416060447693
Batch 53/64 loss: -0.18873262405395508
Batch 54/64 loss: -0.20170700550079346
Batch 55/64 loss: -0.1852390170097351
Batch 56/64 loss: -0.1801500916481018
Batch 57/64 loss: -0.20089226961135864
Batch 58/64 loss: -0.1826413869857788
Batch 59/64 loss: -0.19734472036361694
Batch 60/64 loss: -0.22647088766098022
Batch 61/64 loss: -0.21187222003936768
Batch 62/64 loss: -0.19526654481887817
Batch 63/64 loss: -0.12996405363082886
Batch 64/64 loss: -0.19253087043762207
Epoch 141  Train loss: -0.18527239350711597  Val loss: 0.017570213968401513
Epoch 142
-------------------------------
Batch 1/64 loss: -0.19496333599090576
Batch 2/64 loss: -0.20532238483428955
Batch 3/64 loss: -0.20521891117095947
Batch 4/64 loss: -0.21182912588119507
Batch 5/64 loss: -0.18953156471252441
Batch 6/64 loss: -0.19401788711547852
Batch 7/64 loss: -0.20181775093078613
Batch 8/64 loss: -0.20661139488220215
Batch 9/64 loss: -0.17120224237442017
Batch 10/64 loss: -0.2143540382385254
Batch 11/64 loss: -0.20731782913208008
Batch 12/64 loss: -0.18415981531143188
Batch 13/64 loss: -0.13009542226791382
Batch 14/64 loss: -0.17869502305984497
Batch 15/64 loss: -0.19818341732025146
Batch 16/64 loss: -0.17755991220474243
Batch 17/64 loss: -0.19219881296157837
Batch 18/64 loss: -0.189433753490448
Batch 19/64 loss: -0.1957520842552185
Batch 20/64 loss: -0.20357763767242432
Batch 21/64 loss: -0.16096735000610352
Batch 22/64 loss: -0.1724386215209961
Batch 23/64 loss: -0.1534937024116516
Batch 24/64 loss: -0.17887663841247559
Batch 25/64 loss: -0.19186937808990479
Batch 26/64 loss: -0.2027772068977356
Batch 27/64 loss: -0.1850486397743225
Batch 28/64 loss: -0.18565243482589722
Batch 29/64 loss: -0.19502395391464233
Batch 30/64 loss: -0.16235065460205078
Batch 31/64 loss: -0.18646866083145142
Batch 32/64 loss: -0.20697486400604248
Batch 33/64 loss: -0.21196907758712769
Batch 34/64 loss: -0.1892130970954895
Batch 35/64 loss: -0.18381154537200928
Batch 36/64 loss: -0.159834623336792
Batch 37/64 loss: -0.21137768030166626
Batch 38/64 loss: -0.19627124071121216
Batch 39/64 loss: -0.19147157669067383
Batch 40/64 loss: -0.22447806596755981
Batch 41/64 loss: -0.20059847831726074
Batch 42/64 loss: -0.21304833889007568
Batch 43/64 loss: -0.14317429065704346
Batch 44/64 loss: -0.20215070247650146
Batch 45/64 loss: -0.2110946774482727
Batch 46/64 loss: -0.20585131645202637
Batch 47/64 loss: -0.20979762077331543
Batch 48/64 loss: -0.16290897130966187
Batch 49/64 loss: -0.20251160860061646
Batch 50/64 loss: -0.20896047353744507
Batch 51/64 loss: -0.16469472646713257
Batch 52/64 loss: -0.20754098892211914
Batch 53/64 loss: -0.2175920605659485
Batch 54/64 loss: -0.19395887851715088
Batch 55/64 loss: -0.17959558963775635
Batch 56/64 loss: -0.202589750289917
Batch 57/64 loss: -0.16126590967178345
Batch 58/64 loss: -0.20037841796875
Batch 59/64 loss: -0.17033225297927856
Batch 60/64 loss: -0.1891007423400879
Batch 61/64 loss: -0.20095229148864746
Batch 62/64 loss: -0.19344747066497803
Batch 63/64 loss: -0.208967387676239
Batch 64/64 loss: -0.21687525510787964
Epoch 142  Train loss: -0.1916138155787599  Val loss: 0.019887536252077502
Epoch 143
-------------------------------
Batch 1/64 loss: -0.19600635766983032
Batch 2/64 loss: -0.2082270383834839
Batch 3/64 loss: -0.22192370891571045
Batch 4/64 loss: -0.21361732482910156
Batch 5/64 loss: -0.19297587871551514
Batch 6/64 loss: -0.18749940395355225
Batch 7/64 loss: -0.19555604457855225
Batch 8/64 loss: -0.19196295738220215
Batch 9/64 loss: -0.1596677303314209
Batch 10/64 loss: -0.18981420993804932
Batch 11/64 loss: -0.15428590774536133
Batch 12/64 loss: -0.21176427602767944
Batch 13/64 loss: -0.20840013027191162
Batch 14/64 loss: -0.20150667428970337
Batch 15/64 loss: -0.197016179561615
Batch 16/64 loss: -0.21611785888671875
Batch 17/64 loss: -0.19911926984786987
Batch 18/64 loss: -0.21567559242248535
Batch 19/64 loss: -0.20287305116653442
Batch 20/64 loss: -0.23591989278793335
Batch 21/64 loss: -0.2071792483329773
Batch 22/64 loss: -0.23154622316360474
Batch 23/64 loss: -0.1864604949951172
Batch 24/64 loss: -0.22144067287445068
Batch 25/64 loss: -0.20842492580413818
Batch 26/64 loss: -0.16586613655090332
Batch 27/64 loss: -0.2137954831123352
Batch 28/64 loss: -0.18710511922836304
Batch 29/64 loss: -0.20011723041534424
Batch 30/64 loss: -0.20081764459609985
Batch 31/64 loss: -0.20460790395736694
Batch 32/64 loss: -0.17239677906036377
Batch 33/64 loss: -0.16025805473327637
Batch 34/64 loss: -0.22590434551239014
Batch 35/64 loss: -0.18131113052368164
Batch 36/64 loss: -0.15148717164993286
Batch 37/64 loss: -0.20505684614181519
Batch 38/64 loss: -0.19627755880355835
Batch 39/64 loss: -0.20749056339263916
Batch 40/64 loss: -0.18272137641906738
Batch 41/64 loss: -0.19749373197555542
Batch 42/64 loss: -0.16094475984573364
Batch 43/64 loss: -0.17018496990203857
Batch 44/64 loss: -0.16820085048675537
Batch 45/64 loss: -0.1855829954147339
Batch 46/64 loss: -0.16047435998916626
Batch 47/64 loss: -0.17122697830200195
Batch 48/64 loss: -0.2032870650291443
Batch 49/64 loss: -0.19524693489074707
Batch 50/64 loss: -0.18840265274047852
Batch 51/64 loss: -0.1790977120399475
Batch 52/64 loss: -0.18792903423309326
Batch 53/64 loss: -0.17745709419250488
Batch 54/64 loss: -0.1973283886909485
Batch 55/64 loss: -0.17074406147003174
Batch 56/64 loss: -0.19217514991760254
Batch 57/64 loss: -0.172371506690979
Batch 58/64 loss: -0.20096850395202637
Batch 59/64 loss: -0.19308292865753174
Batch 60/64 loss: -0.1103857159614563
Batch 61/64 loss: -0.20053935050964355
Batch 62/64 loss: -0.17570579051971436
Batch 63/64 loss: -0.1510372757911682
Batch 64/64 loss: -0.15774238109588623
Epoch 143  Train loss: -0.19040578814113843  Val loss: 0.018991200784637348
Epoch 144
-------------------------------
Batch 1/64 loss: -0.21257859468460083
Batch 2/64 loss: -0.16343837976455688
Batch 3/64 loss: -0.2164292335510254
Batch 4/64 loss: -0.22137635946273804
Batch 5/64 loss: -0.20322513580322266
Batch 6/64 loss: -0.21295416355133057
Batch 7/64 loss: -0.15746378898620605
Batch 8/64 loss: -0.2096099853515625
Batch 9/64 loss: -0.19537925720214844
Batch 10/64 loss: -0.18354171514511108
Batch 11/64 loss: -0.24288982152938843
Batch 12/64 loss: -0.23273086547851562
Batch 13/64 loss: -0.1827836036682129
Batch 14/64 loss: -0.17458248138427734
Batch 15/64 loss: -0.1822754144668579
Batch 16/64 loss: -0.16858822107315063
Batch 17/64 loss: -0.20166164636611938
Batch 18/64 loss: -0.17544883489608765
Batch 19/64 loss: -0.20245885848999023
Batch 20/64 loss: -0.2008817195892334
Batch 21/64 loss: -0.21313387155532837
Batch 22/64 loss: -0.20876574516296387
Batch 23/64 loss: -0.20075035095214844
Batch 24/64 loss: -0.2233561873435974
Batch 25/64 loss: -0.19188791513442993
Batch 26/64 loss: -0.17973804473876953
Batch 27/64 loss: -0.1643061637878418
Batch 28/64 loss: -0.13558733463287354
Batch 29/64 loss: -0.17310237884521484
Batch 30/64 loss: -0.1863599419593811
Batch 31/64 loss: -0.2092115879058838
Batch 32/64 loss: -0.16307008266448975
Batch 33/64 loss: -0.1828528642654419
Batch 34/64 loss: -0.15744102001190186
Batch 35/64 loss: -0.1751900315284729
Batch 36/64 loss: -0.1824379563331604
Batch 37/64 loss: -0.1763068437576294
Batch 38/64 loss: -0.18892645835876465
Batch 39/64 loss: -0.19633537530899048
Batch 40/64 loss: -0.19117450714111328
Batch 41/64 loss: -0.21416515111923218
Batch 42/64 loss: -0.19088584184646606
Batch 43/64 loss: -0.1754239797592163
Batch 44/64 loss: -0.2003975510597229
Batch 45/64 loss: -0.2057662010192871
Batch 46/64 loss: -0.19039207696914673
Batch 47/64 loss: -0.2077440619468689
Batch 48/64 loss: -0.18783152103424072
Batch 49/64 loss: -0.1908751130104065
Batch 50/64 loss: -0.16566765308380127
Batch 51/64 loss: -0.20041602849960327
Batch 52/64 loss: -0.18520450592041016
Batch 53/64 loss: -0.19998067617416382
Batch 54/64 loss: -0.1981486678123474
Batch 55/64 loss: -0.19187164306640625
Batch 56/64 loss: -0.21352320909500122
Batch 57/64 loss: -0.2089853286743164
Batch 58/64 loss: -0.1921015977859497
Batch 59/64 loss: -0.19698399305343628
Batch 60/64 loss: -0.19531244039535522
Batch 61/64 loss: -0.20068341493606567
Batch 62/64 loss: -0.18974947929382324
Batch 63/64 loss: -0.22075986862182617
Batch 64/64 loss: -0.20025134086608887
Epoch 144  Train loss: -0.19314966669269637  Val loss: 0.016765256108287274
Epoch 145
-------------------------------
Batch 1/64 loss: -0.21518218517303467
Batch 2/64 loss: -0.21556353569030762
Batch 3/64 loss: -0.19613397121429443
Batch 4/64 loss: -0.1854863166809082
Batch 5/64 loss: -0.19190722703933716
Batch 6/64 loss: -0.19941115379333496
Batch 7/64 loss: -0.1917034387588501
Batch 8/64 loss: -0.16899192333221436
Batch 9/64 loss: -0.17394447326660156
Batch 10/64 loss: -0.18310803174972534
Batch 11/64 loss: -0.21963834762573242
Batch 12/64 loss: -0.20181798934936523
Batch 13/64 loss: -0.19246238470077515
Batch 14/64 loss: -0.208088219165802
Batch 15/64 loss: -0.19734477996826172
Batch 16/64 loss: -0.22809624671936035
Batch 17/64 loss: -0.19926869869232178
Batch 18/64 loss: -0.1796703338623047
Batch 19/64 loss: -0.20147418975830078
Batch 20/64 loss: -0.20620983839035034
Batch 21/64 loss: -0.20117825269699097
Batch 22/64 loss: -0.22062641382217407
Batch 23/64 loss: -0.19967055320739746
Batch 24/64 loss: -0.16440701484680176
Batch 25/64 loss: -0.2346011996269226
Batch 26/64 loss: -0.17539751529693604
Batch 27/64 loss: -0.18699699640274048
Batch 28/64 loss: -0.19306224584579468
Batch 29/64 loss: -0.19343167543411255
Batch 30/64 loss: -0.18962842226028442
Batch 31/64 loss: -0.1996106505393982
Batch 32/64 loss: -0.20803117752075195
Batch 33/64 loss: -0.1453457474708557
Batch 34/64 loss: -0.1919587254524231
Batch 35/64 loss: -0.1793537735939026
Batch 36/64 loss: -0.21156471967697144
Batch 37/64 loss: -0.1906529664993286
Batch 38/64 loss: -0.2199000120162964
Batch 39/64 loss: -0.2112249732017517
Batch 40/64 loss: -0.21595478057861328
Batch 41/64 loss: -0.20159661769866943
Batch 42/64 loss: -0.22049182653427124
Batch 43/64 loss: -0.15086090564727783
Batch 44/64 loss: -0.2101449966430664
Batch 45/64 loss: -0.19691723585128784
Batch 46/64 loss: -0.1684092879295349
Batch 47/64 loss: -0.16248971223831177
Batch 48/64 loss: -0.21352672576904297
Batch 49/64 loss: -0.2210789918899536
Batch 50/64 loss: -0.17621910572052002
Batch 51/64 loss: -0.17552804946899414
Batch 52/64 loss: -0.1817176342010498
Batch 53/64 loss: -0.22151947021484375
Batch 54/64 loss: -0.20718878507614136
Batch 55/64 loss: -0.20417523384094238
Batch 56/64 loss: -0.20141857862472534
Batch 57/64 loss: -0.15828180313110352
Batch 58/64 loss: -0.19779759645462036
Batch 59/64 loss: -0.1671351194381714
Batch 60/64 loss: -0.1885089874267578
Batch 61/64 loss: -0.16634297370910645
Batch 62/64 loss: -0.19309747219085693
Batch 63/64 loss: -0.20554780960083008
Batch 64/64 loss: -0.19695401191711426
Epoch 145  Train loss: -0.19491469065348307  Val loss: 0.01712974649934015
Epoch 146
-------------------------------
Batch 1/64 loss: -0.1990906000137329
Batch 2/64 loss: -0.19512242078781128
Batch 3/64 loss: -0.1917097568511963
Batch 4/64 loss: -0.21071064472198486
Batch 5/64 loss: -0.16195279359817505
Batch 6/64 loss: -0.23324352502822876
Batch 7/64 loss: -0.22151660919189453
Batch 8/64 loss: -0.2170467972755432
Batch 9/64 loss: -0.22249853610992432
Batch 10/64 loss: -0.1787586212158203
Batch 11/64 loss: -0.16753888130187988
Batch 12/64 loss: -0.22086846828460693
Batch 13/64 loss: -0.16946721076965332
Batch 14/64 loss: -0.19485867023468018
Batch 15/64 loss: -0.18436747789382935
Batch 16/64 loss: -0.20873844623565674
Batch 17/64 loss: -0.21612977981567383
Batch 18/64 loss: -0.15457165241241455
Batch 19/64 loss: -0.19999384880065918
Batch 20/64 loss: -0.2043282389640808
Batch 21/64 loss: -0.19737398624420166
Batch 22/64 loss: -0.2135148048400879
Batch 23/64 loss: -0.1881939172744751
Batch 24/64 loss: -0.1123916506767273
Batch 25/64 loss: -0.190854012966156
Batch 26/64 loss: -0.20964139699935913
Batch 27/64 loss: -0.21432334184646606
Batch 28/64 loss: -0.2228206992149353
Batch 29/64 loss: -0.1728493571281433
Batch 30/64 loss: -0.19896584749221802
Batch 31/64 loss: -0.17776012420654297
Batch 32/64 loss: -0.19564706087112427
Batch 33/64 loss: -0.2121676206588745
Batch 34/64 loss: -0.2031385898590088
Batch 35/64 loss: -0.2148839235305786
Batch 36/64 loss: -0.20163774490356445
Batch 37/64 loss: -0.21960467100143433
Batch 38/64 loss: -0.2205858826637268
Batch 39/64 loss: -0.20906585454940796
Batch 40/64 loss: -0.2096092700958252
Batch 41/64 loss: -0.1667259931564331
Batch 42/64 loss: -0.15987294912338257
Batch 43/64 loss: -0.2270880937576294
Batch 44/64 loss: -0.19469380378723145
Batch 45/64 loss: -0.16301840543746948
Batch 46/64 loss: -0.1639711856842041
Batch 47/64 loss: -0.1869397759437561
Batch 48/64 loss: -0.1650846004486084
Batch 49/64 loss: -0.21776539087295532
Batch 50/64 loss: -0.15173476934432983
Batch 51/64 loss: -0.19294852018356323
Batch 52/64 loss: -0.20706897974014282
Batch 53/64 loss: -0.20394951105117798
Batch 54/64 loss: -0.21034425497055054
Batch 55/64 loss: -0.20255285501480103
Batch 56/64 loss: -0.23069405555725098
Batch 57/64 loss: -0.2141050100326538
Batch 58/64 loss: -0.19035017490386963
Batch 59/64 loss: -0.20675820112228394
Batch 60/64 loss: -0.21786439418792725
Batch 61/64 loss: -0.1916283369064331
Batch 62/64 loss: -0.18313580751419067
Batch 63/64 loss: -0.22941631078720093
Batch 64/64 loss: -0.2030048370361328
Epoch 146  Train loss: -0.1971373763738894  Val loss: 0.01851304454082476
Epoch 147
-------------------------------
Batch 1/64 loss: -0.22094511985778809
Batch 2/64 loss: -0.15513277053833008
Batch 3/64 loss: -0.20222866535186768
Batch 4/64 loss: -0.2101588249206543
Batch 5/64 loss: -0.21088969707489014
Batch 6/64 loss: -0.206143319606781
Batch 7/64 loss: -0.1271662712097168
Batch 8/64 loss: -0.2192617654800415
Batch 9/64 loss: -0.17179083824157715
Batch 10/64 loss: -0.21213018894195557
Batch 11/64 loss: -0.18890321254730225
Batch 12/64 loss: -0.20567578077316284
Batch 13/64 loss: -0.19184422492980957
Batch 14/64 loss: -0.22396379709243774
Batch 15/64 loss: -0.21896517276763916
Batch 16/64 loss: -0.23765981197357178
Batch 17/64 loss: -0.20527422428131104
Batch 18/64 loss: -0.21638178825378418
Batch 19/64 loss: -0.18883264064788818
Batch 20/64 loss: -0.19078081846237183
Batch 21/64 loss: -0.20326989889144897
Batch 22/64 loss: -0.18566089868545532
Batch 23/64 loss: -0.19503015279769897
Batch 24/64 loss: -0.20491254329681396
Batch 25/64 loss: -0.2056785225868225
Batch 26/64 loss: -0.17110222578048706
Batch 27/64 loss: -0.18134397268295288
Batch 28/64 loss: -0.20406651496887207
Batch 29/64 loss: -0.24098306894302368
Batch 30/64 loss: -0.2090209722518921
Batch 31/64 loss: -0.1954595446586609
Batch 32/64 loss: -0.18048107624053955
Batch 33/64 loss: -0.19949567317962646
Batch 34/64 loss: -0.19862878322601318
Batch 35/64 loss: -0.20954114198684692
Batch 36/64 loss: -0.20826733112335205
Batch 37/64 loss: -0.2074873447418213
Batch 38/64 loss: -0.20244306325912476
Batch 39/64 loss: -0.20050835609436035
Batch 40/64 loss: -0.21330153942108154
Batch 41/64 loss: -0.19808059930801392
Batch 42/64 loss: -0.21361398696899414
Batch 43/64 loss: -0.20330572128295898
Batch 44/64 loss: -0.19560521841049194
Batch 45/64 loss: -0.19494569301605225
Batch 46/64 loss: -0.20437681674957275
Batch 47/64 loss: -0.19705790281295776
Batch 48/64 loss: -0.19472932815551758
Batch 49/64 loss: -0.19220656156539917
Batch 50/64 loss: -0.19586604833602905
Batch 51/64 loss: -0.16077059507369995
Batch 52/64 loss: -0.20222526788711548
Batch 53/64 loss: -0.1854221224784851
Batch 54/64 loss: -0.21433228254318237
Batch 55/64 loss: -0.17127764225006104
Batch 56/64 loss: -0.19589555263519287
Batch 57/64 loss: -0.20929718017578125
Batch 58/64 loss: -0.20052802562713623
Batch 59/64 loss: -0.2023569941520691
Batch 60/64 loss: -0.19010722637176514
Batch 61/64 loss: -0.2012629508972168
Batch 62/64 loss: -0.14474350214004517
Batch 63/64 loss: -0.21606922149658203
Batch 64/64 loss: -0.16583311557769775
Epoch 147  Train loss: -0.1981060836829391  Val loss: 0.018766408728570053
Epoch 148
-------------------------------
Batch 1/64 loss: -0.22201627492904663
Batch 2/64 loss: -0.20634829998016357
Batch 3/64 loss: -0.20894229412078857
Batch 4/64 loss: -0.22669941186904907
Batch 5/64 loss: -0.15059804916381836
Batch 6/64 loss: -0.21267718076705933
Batch 7/64 loss: -0.18993467092514038
Batch 8/64 loss: -0.17329204082489014
Batch 9/64 loss: -0.20702701807022095
Batch 10/64 loss: -0.22312593460083008
Batch 11/64 loss: -0.21310055255889893
Batch 12/64 loss: -0.21926146745681763
Batch 13/64 loss: -0.2229762077331543
Batch 14/64 loss: -0.19298869371414185
Batch 15/64 loss: -0.19628435373306274
Batch 16/64 loss: -0.21492058038711548
Batch 17/64 loss: -0.18297773599624634
Batch 18/64 loss: -0.20147240161895752
Batch 19/64 loss: -0.20059919357299805
Batch 20/64 loss: -0.18922096490859985
Batch 21/64 loss: -0.20338338613510132
Batch 22/64 loss: -0.19458186626434326
Batch 23/64 loss: -0.19907128810882568
Batch 24/64 loss: -0.2163407802581787
Batch 25/64 loss: -0.20364195108413696
Batch 26/64 loss: -0.21773231029510498
Batch 27/64 loss: -0.15589016675949097
Batch 28/64 loss: -0.19311058521270752
Batch 29/64 loss: -0.22463107109069824
Batch 30/64 loss: -0.1944841742515564
Batch 31/64 loss: -0.23613262176513672
Batch 32/64 loss: -0.14695024490356445
Batch 33/64 loss: -0.22410398721694946
Batch 34/64 loss: -0.207463800907135
Batch 35/64 loss: -0.18464171886444092
Batch 36/64 loss: -0.20763087272644043
Batch 37/64 loss: -0.21726065874099731
Batch 38/64 loss: -0.18059051036834717
Batch 39/64 loss: -0.20912456512451172
Batch 40/64 loss: -0.18723511695861816
Batch 41/64 loss: -0.20284879207611084
Batch 42/64 loss: -0.20814889669418335
Batch 43/64 loss: -0.20424234867095947
Batch 44/64 loss: -0.18422019481658936
Batch 45/64 loss: -0.22145795822143555
Batch 46/64 loss: -0.20015376806259155
Batch 47/64 loss: -0.22738957405090332
Batch 48/64 loss: -0.18399900197982788
Batch 49/64 loss: -0.1972956657409668
Batch 50/64 loss: -0.1570236086845398
Batch 51/64 loss: -0.16499900817871094
Batch 52/64 loss: -0.18719881772994995
Batch 53/64 loss: -0.18008458614349365
Batch 54/64 loss: -0.17278575897216797
Batch 55/64 loss: -0.2129368782043457
Batch 56/64 loss: -0.20975565910339355
Batch 57/64 loss: -0.2216625213623047
Batch 58/64 loss: -0.2123260498046875
Batch 59/64 loss: -0.23245149850845337
Batch 60/64 loss: -0.212668776512146
Batch 61/64 loss: -0.22572708129882812
Batch 62/64 loss: -0.17079269886016846
Batch 63/64 loss: -0.1843653917312622
Batch 64/64 loss: -0.18464869260787964
Epoch 148  Train loss: -0.20033703621696022  Val loss: 0.017627205226019894
Epoch 149
-------------------------------
Batch 1/64 loss: -0.19742822647094727
Batch 2/64 loss: -0.19291967153549194
Batch 3/64 loss: -0.16706645488739014
Batch 4/64 loss: -0.1807636022567749
Batch 5/64 loss: -0.2163885235786438
Batch 6/64 loss: -0.19346439838409424
Batch 7/64 loss: -0.16597390174865723
Batch 8/64 loss: -0.1636219620704651
Batch 9/64 loss: -0.19057625532150269
Batch 10/64 loss: -0.20295584201812744
Batch 11/64 loss: -0.16641783714294434
Batch 12/64 loss: -0.2119455337524414
Batch 13/64 loss: -0.21546906232833862
Batch 14/64 loss: -0.20311623811721802
Batch 15/64 loss: -0.1991938352584839
Batch 16/64 loss: -0.20981109142303467
Batch 17/64 loss: -0.17909777164459229
Batch 18/64 loss: -0.22344475984573364
Batch 19/64 loss: -0.17043274641036987
Batch 20/64 loss: -0.17089438438415527
Batch 21/64 loss: -0.18664562702178955
Batch 22/64 loss: -0.21174836158752441
Batch 23/64 loss: -0.19503319263458252
Batch 24/64 loss: -0.2118687629699707
Batch 25/64 loss: -0.2228463888168335
Batch 26/64 loss: -0.22739964723587036
Batch 27/64 loss: -0.18663406372070312
Batch 28/64 loss: -0.2173703908920288
Batch 29/64 loss: -0.20479846000671387
Batch 30/64 loss: -0.20553839206695557
Batch 31/64 loss: -0.21317189931869507
Batch 32/64 loss: -0.22430366277694702
Batch 33/64 loss: -0.153325617313385
Batch 34/64 loss: -0.20944064855575562
Batch 35/64 loss: -0.18767213821411133
Batch 36/64 loss: -0.20981299877166748
Batch 37/64 loss: -0.19844919443130493
Batch 38/64 loss: -0.20471245050430298
Batch 39/64 loss: -0.20722240209579468
Batch 40/64 loss: -0.19860029220581055
Batch 41/64 loss: -0.20076662302017212
Batch 42/64 loss: -0.20759761333465576
Batch 43/64 loss: -0.19359499216079712
Batch 44/64 loss: -0.2081160545349121
Batch 45/64 loss: -0.22542154788970947
Batch 46/64 loss: -0.20759260654449463
Batch 47/64 loss: -0.20621544122695923
Batch 48/64 loss: -0.2067255973815918
Batch 49/64 loss: -0.1634286642074585
Batch 50/64 loss: -0.1966249942779541
Batch 51/64 loss: -0.21109771728515625
Batch 52/64 loss: -0.19073772430419922
Batch 53/64 loss: -0.18806535005569458
Batch 54/64 loss: -0.2227378487586975
Batch 55/64 loss: -0.20312893390655518
Batch 56/64 loss: -0.17963099479675293
Batch 57/64 loss: -0.19305360317230225
Batch 58/64 loss: -0.19374126195907593
Batch 59/64 loss: -0.20555448532104492
Batch 60/64 loss: -0.18616551160812378
Batch 61/64 loss: -0.20021069049835205
Batch 62/64 loss: -0.21382564306259155
Batch 63/64 loss: -0.2057226300239563
Batch 64/64 loss: -0.1864451766014099
Epoch 149  Train loss: -0.19838693492552814  Val loss: 0.016949472763284377
Epoch 150
-------------------------------
Batch 1/64 loss: -0.19855594635009766
Batch 2/64 loss: -0.18396741151809692
Batch 3/64 loss: -0.21633893251419067
Batch 4/64 loss: -0.19383633136749268
Batch 5/64 loss: -0.2228924036026001
Batch 6/64 loss: -0.20332872867584229
Batch 7/64 loss: -0.18435484170913696
Batch 8/64 loss: -0.21366935968399048
Batch 9/64 loss: -0.20383477210998535
Batch 10/64 loss: -0.19842898845672607
Batch 11/64 loss: -0.1767997145652771
Batch 12/64 loss: -0.21797055006027222
Batch 13/64 loss: -0.2264474630355835
Batch 14/64 loss: -0.23755240440368652
Batch 15/64 loss: -0.17254650592803955
Batch 16/64 loss: -0.2006610631942749
Batch 17/64 loss: -0.18408852815628052
Batch 18/64 loss: -0.19877171516418457
Batch 19/64 loss: -0.2118425965309143
Batch 20/64 loss: -0.2224552035331726
Batch 21/64 loss: -0.18252724409103394
Batch 22/64 loss: -0.20781981945037842
Batch 23/64 loss: -0.22373557090759277
Batch 24/64 loss: -0.22406095266342163
Batch 25/64 loss: -0.1888560652732849
Batch 26/64 loss: -0.21691685914993286
Batch 27/64 loss: -0.2203637957572937
Batch 28/64 loss: -0.2349676489830017
Batch 29/64 loss: -0.22298204898834229
Batch 30/64 loss: -0.2523128092288971
Batch 31/64 loss: -0.1949177384376526
Batch 32/64 loss: -0.18982547521591187
Batch 33/64 loss: -0.1811150312423706
Batch 34/64 loss: -0.15948855876922607
Batch 35/64 loss: -0.15473401546478271
Batch 36/64 loss: -0.18980032205581665
Batch 37/64 loss: -0.20949947834014893
Batch 38/64 loss: -0.20872896909713745
Batch 39/64 loss: -0.18993592262268066
Batch 40/64 loss: -0.21766537427902222
Batch 41/64 loss: -0.2032189965248108
Batch 42/64 loss: -0.195429265499115
Batch 43/64 loss: -0.21728992462158203
Batch 44/64 loss: -0.18358701467514038
Batch 45/64 loss: -0.2205529808998108
Batch 46/64 loss: -0.21739208698272705
Batch 47/64 loss: -0.20727282762527466
Batch 48/64 loss: -0.1789279580116272
Batch 49/64 loss: -0.21119368076324463
Batch 50/64 loss: -0.18256604671478271
Batch 51/64 loss: -0.21033644676208496
Batch 52/64 loss: -0.20144683122634888
Batch 53/64 loss: -0.19880753755569458
Batch 54/64 loss: -0.20685237646102905
Batch 55/64 loss: -0.21908187866210938
Batch 56/64 loss: -0.16904377937316895
Batch 57/64 loss: -0.2031446099281311
Batch 58/64 loss: -0.20169991254806519
Batch 59/64 loss: -0.2083948850631714
Batch 60/64 loss: -0.1835882067680359
Batch 61/64 loss: -0.21753525733947754
Batch 62/64 loss: -0.22905951738357544
Batch 63/64 loss: -0.20694893598556519
Batch 64/64 loss: -0.211090087890625
Epoch 150  Train loss: -0.20345546170776965  Val loss: 0.015295506752643389
Epoch 151
-------------------------------
Batch 1/64 loss: -0.18032097816467285
Batch 2/64 loss: -0.20331555604934692
Batch 3/64 loss: -0.21893799304962158
Batch 4/64 loss: -0.17192381620407104
Batch 5/64 loss: -0.2199193239212036
Batch 6/64 loss: -0.2349269986152649
Batch 7/64 loss: -0.2279561161994934
Batch 8/64 loss: -0.22467631101608276
Batch 9/64 loss: -0.19488167762756348
Batch 10/64 loss: -0.21145373582839966
Batch 11/64 loss: -0.2257658839225769
Batch 12/64 loss: -0.18660247325897217
Batch 13/64 loss: -0.211950421333313
Batch 14/64 loss: -0.1685490608215332
Batch 15/64 loss: -0.21860039234161377
Batch 16/64 loss: -0.22821903228759766
Batch 17/64 loss: -0.2378050684928894
Batch 18/64 loss: -0.192740797996521
Batch 19/64 loss: -0.2079300880432129
Batch 20/64 loss: -0.23635134100914001
Batch 21/64 loss: -0.22147780656814575
Batch 22/64 loss: -0.21237629652023315
Batch 23/64 loss: -0.19912129640579224
Batch 24/64 loss: -0.20699572563171387
Batch 25/64 loss: -0.19229573011398315
Batch 26/64 loss: -0.16194891929626465
Batch 27/64 loss: -0.20000773668289185
Batch 28/64 loss: -0.21087884902954102
Batch 29/64 loss: -0.1505136489868164
Batch 30/64 loss: -0.21147119998931885
Batch 31/64 loss: -0.18933796882629395
Batch 32/64 loss: -0.21531027555465698
Batch 33/64 loss: -0.21456563472747803
Batch 34/64 loss: -0.21446985006332397
Batch 35/64 loss: -0.20452308654785156
Batch 36/64 loss: -0.2155841588973999
Batch 37/64 loss: -0.21149283647537231
Batch 38/64 loss: -0.19180333614349365
Batch 39/64 loss: -0.18693459033966064
Batch 40/64 loss: -0.20471864938735962
Batch 41/64 loss: -0.20261245965957642
Batch 42/64 loss: -0.14471691846847534
Batch 43/64 loss: -0.2225760817527771
Batch 44/64 loss: -0.21606701612472534
Batch 45/64 loss: -0.21760344505310059
Batch 46/64 loss: -0.2120271921157837
Batch 47/64 loss: -0.20966506004333496
Batch 48/64 loss: -0.20737111568450928
Batch 49/64 loss: -0.16503310203552246
Batch 50/64 loss: -0.1869184970855713
Batch 51/64 loss: -0.19743883609771729
Batch 52/64 loss: -0.169899582862854
Batch 53/64 loss: -0.20445513725280762
Batch 54/64 loss: -0.20858067274093628
Batch 55/64 loss: -0.20998930931091309
Batch 56/64 loss: -0.19716554880142212
Batch 57/64 loss: -0.16160881519317627
Batch 58/64 loss: -0.17340368032455444
Batch 59/64 loss: -0.2125561237335205
Batch 60/64 loss: -0.17091935873031616
Batch 61/64 loss: -0.20756757259368896
Batch 62/64 loss: -0.1916041374206543
Batch 63/64 loss: -0.20200902223587036
Batch 64/64 loss: -0.23812484741210938
Epoch 151  Train loss: -0.20218097228629917  Val loss: 0.015475946398535135
Epoch 152
-------------------------------
Batch 1/64 loss: -0.2294614315032959
Batch 2/64 loss: -0.18573600053787231
Batch 3/64 loss: -0.21050769090652466
Batch 4/64 loss: -0.22910553216934204
Batch 5/64 loss: -0.2096651792526245
Batch 6/64 loss: -0.19194966554641724
Batch 7/64 loss: -0.2044583559036255
Batch 8/64 loss: -0.14804506301879883
Batch 9/64 loss: -0.2021808624267578
Batch 10/64 loss: -0.18039864301681519
Batch 11/64 loss: -0.20697689056396484
Batch 12/64 loss: -0.21456384658813477
Batch 13/64 loss: -0.1910337209701538
Batch 14/64 loss: -0.20800268650054932
Batch 15/64 loss: -0.20841169357299805
Batch 16/64 loss: -0.2267376184463501
Batch 17/64 loss: -0.21534419059753418
Batch 18/64 loss: -0.20506328344345093
Batch 19/64 loss: -0.199679434299469
Batch 20/64 loss: -0.21222782135009766
Batch 21/64 loss: -0.18568509817123413
Batch 22/64 loss: -0.2350684106349945
Batch 23/64 loss: -0.2004656195640564
Batch 24/64 loss: -0.22986966371536255
Batch 25/64 loss: -0.22027474641799927
Batch 26/64 loss: -0.22801852226257324
Batch 27/64 loss: -0.19922280311584473
Batch 28/64 loss: -0.2393312156200409
Batch 29/64 loss: -0.20797258615493774
Batch 30/64 loss: -0.2298344373703003
Batch 31/64 loss: -0.22695708274841309
Batch 32/64 loss: -0.1941385269165039
Batch 33/64 loss: -0.19076204299926758
Batch 34/64 loss: -0.23108839988708496
Batch 35/64 loss: -0.15812277793884277
Batch 36/64 loss: -0.16284418106079102
Batch 37/64 loss: -0.20445996522903442
Batch 38/64 loss: -0.19924288988113403
Batch 39/64 loss: -0.22211867570877075
Batch 40/64 loss: -0.20033931732177734
Batch 41/64 loss: -0.23080241680145264
Batch 42/64 loss: -0.20976567268371582
Batch 43/64 loss: -0.23657196760177612
Batch 44/64 loss: -0.2175147533416748
Batch 45/64 loss: -0.2308427095413208
Batch 46/64 loss: -0.1901007890701294
Batch 47/64 loss: -0.2014908790588379
Batch 48/64 loss: -0.1816927194595337
Batch 49/64 loss: -0.21775877475738525
Batch 50/64 loss: -0.1876387596130371
Batch 51/64 loss: -0.19414347410202026
Batch 52/64 loss: -0.20502954721450806
Batch 53/64 loss: -0.14179253578186035
Batch 54/64 loss: -0.2205202579498291
Batch 55/64 loss: -0.19153743982315063
Batch 56/64 loss: -0.18319785594940186
Batch 57/64 loss: -0.19303935766220093
Batch 58/64 loss: -0.14637458324432373
Batch 59/64 loss: -0.1857091188430786
Batch 60/64 loss: -0.1965232491493225
Batch 61/64 loss: -0.22520875930786133
Batch 62/64 loss: -0.21294379234313965
Batch 63/64 loss: -0.15728217363357544
Batch 64/64 loss: -0.1510811448097229
Epoch 152  Train loss: -0.2026064159823399  Val loss: 0.01791114372895755
Epoch 153
-------------------------------
Batch 1/64 loss: -0.21307528018951416
Batch 2/64 loss: -0.20683443546295166
Batch 3/64 loss: -0.21572870016098022
Batch 4/64 loss: -0.18408846855163574
Batch 5/64 loss: -0.16294699907302856
Batch 6/64 loss: -0.22610491514205933
Batch 7/64 loss: -0.2089030146598816
Batch 8/64 loss: -0.21418893337249756
Batch 9/64 loss: -0.22506636381149292
Batch 10/64 loss: -0.2056748867034912
Batch 11/64 loss: -0.18520551919937134
Batch 12/64 loss: -0.2427680492401123
Batch 13/64 loss: -0.18786031007766724
Batch 14/64 loss: -0.21204698085784912
Batch 15/64 loss: -0.22378474473953247
Batch 16/64 loss: -0.21335852146148682
Batch 17/64 loss: -0.24818825721740723
Batch 18/64 loss: -0.1997719407081604
Batch 19/64 loss: -0.19137579202651978
Batch 20/64 loss: -0.19191479682922363
Batch 21/64 loss: -0.18996447324752808
Batch 22/64 loss: -0.1985689401626587
Batch 23/64 loss: -0.2324845790863037
Batch 24/64 loss: -0.20933324098587036
Batch 25/64 loss: -0.2090356945991516
Batch 26/64 loss: -0.18504869937896729
Batch 27/64 loss: -0.21431082487106323
Batch 28/64 loss: -0.19818037748336792
Batch 29/64 loss: -0.17964917421340942
Batch 30/64 loss: -0.19861793518066406
Batch 31/64 loss: -0.2107764482498169
Batch 32/64 loss: -0.21146410703659058
Batch 33/64 loss: -0.18912816047668457
Batch 34/64 loss: -0.21205556392669678
Batch 35/64 loss: -0.1906198263168335
Batch 36/64 loss: -0.22052806615829468
Batch 37/64 loss: -0.21341407299041748
Batch 38/64 loss: -0.2091519832611084
Batch 39/64 loss: -0.18522268533706665
Batch 40/64 loss: -0.18408089876174927
Batch 41/64 loss: -0.20463407039642334
Batch 42/64 loss: -0.1886577010154724
Batch 43/64 loss: -0.1842523217201233
Batch 44/64 loss: -0.20330369472503662
Batch 45/64 loss: -0.194568932056427
Batch 46/64 loss: -0.23923683166503906
Batch 47/64 loss: -0.22938495874404907
Batch 48/64 loss: -0.20274502038955688
Batch 49/64 loss: -0.2165062427520752
Batch 50/64 loss: -0.1916925311088562
Batch 51/64 loss: -0.21546632051467896
Batch 52/64 loss: -0.23725995421409607
Batch 53/64 loss: -0.1799415946006775
Batch 54/64 loss: -0.20354855060577393
Batch 55/64 loss: -0.2301355004310608
Batch 56/64 loss: -0.23975396156311035
Batch 57/64 loss: -0.21418380737304688
Batch 58/64 loss: -0.17117738723754883
Batch 59/64 loss: -0.19419187307357788
Batch 60/64 loss: -0.20562052726745605
Batch 61/64 loss: -0.1852428913116455
Batch 62/64 loss: -0.1856086254119873
Batch 63/64 loss: -0.18995064496994019
Batch 64/64 loss: -0.1948280930519104
Epoch 153  Train loss: -0.2047640419473835  Val loss: 0.01851851342060312
Epoch 154
-------------------------------
Batch 1/64 loss: -0.24018967151641846
Batch 2/64 loss: -0.21854859590530396
Batch 3/64 loss: -0.176256000995636
Batch 4/64 loss: -0.21027088165283203
Batch 5/64 loss: -0.17466962337493896
Batch 6/64 loss: -0.2179355025291443
Batch 7/64 loss: -0.22543662786483765
Batch 8/64 loss: -0.17498403787612915
Batch 9/64 loss: -0.19729888439178467
Batch 10/64 loss: -0.22449052333831787
Batch 11/64 loss: -0.2088538408279419
Batch 12/64 loss: -0.19943100214004517
Batch 13/64 loss: -0.23847556114196777
Batch 14/64 loss: -0.1831476092338562
Batch 15/64 loss: -0.2205876111984253
Batch 16/64 loss: -0.21842384338378906
Batch 17/64 loss: -0.14095580577850342
Batch 18/64 loss: -0.2009354829788208
Batch 19/64 loss: -0.17662525177001953
Batch 20/64 loss: -0.20644545555114746
Batch 21/64 loss: -0.18828243017196655
Batch 22/64 loss: -0.20750463008880615
Batch 23/64 loss: -0.24038347601890564
Batch 24/64 loss: -0.2401113510131836
Batch 25/64 loss: -0.1924067735671997
Batch 26/64 loss: -0.19064533710479736
Batch 27/64 loss: -0.20019525289535522
Batch 28/64 loss: -0.20897024869918823
Batch 29/64 loss: -0.17685449123382568
Batch 30/64 loss: -0.23147976398468018
Batch 31/64 loss: -0.19376903772354126
Batch 32/64 loss: -0.19378763437271118
Batch 33/64 loss: -0.2154424786567688
Batch 34/64 loss: -0.2191230058670044
Batch 35/64 loss: -0.18044400215148926
Batch 36/64 loss: -0.1989685297012329
Batch 37/64 loss: -0.20692074298858643
Batch 38/64 loss: -0.21569424867630005
Batch 39/64 loss: -0.1731574535369873
Batch 40/64 loss: -0.20853513479232788
Batch 41/64 loss: -0.22011107206344604
Batch 42/64 loss: -0.23014020919799805
Batch 43/64 loss: -0.20740151405334473
Batch 44/64 loss: -0.20535659790039062
Batch 45/64 loss: -0.20304739475250244
Batch 46/64 loss: -0.17729413509368896
Batch 47/64 loss: -0.1919732689857483
Batch 48/64 loss: -0.22763556241989136
Batch 49/64 loss: -0.17443090677261353
Batch 50/64 loss: -0.20424145460128784
Batch 51/64 loss: -0.2182733416557312
Batch 52/64 loss: -0.23025482892990112
Batch 53/64 loss: -0.21624833345413208
Batch 54/64 loss: -0.19941812753677368
Batch 55/64 loss: -0.2043442726135254
Batch 56/64 loss: -0.21093106269836426
Batch 57/64 loss: -0.1962435245513916
Batch 58/64 loss: -0.21675455570220947
Batch 59/64 loss: -0.1968855857849121
Batch 60/64 loss: -0.223488450050354
Batch 61/64 loss: -0.21139520406723022
Batch 62/64 loss: -0.19785112142562866
Batch 63/64 loss: -0.18799257278442383
Batch 64/64 loss: -0.19136595726013184
Epoch 154  Train loss: -0.20442157492918125  Val loss: 0.014279629151845715
Epoch 155
-------------------------------
Batch 1/64 loss: -0.2083364725112915
Batch 2/64 loss: -0.21535372734069824
Batch 3/64 loss: -0.23240911960601807
Batch 4/64 loss: -0.22063225507736206
Batch 5/64 loss: -0.23469430208206177
Batch 6/64 loss: -0.19415396451950073
Batch 7/64 loss: -0.207866370677948
Batch 8/64 loss: -0.2156447172164917
Batch 9/64 loss: -0.16174542903900146
Batch 10/64 loss: -0.1916515827178955
Batch 11/64 loss: -0.2224102020263672
Batch 12/64 loss: -0.22676914930343628
Batch 13/64 loss: -0.22338241338729858
Batch 14/64 loss: -0.2188670039176941
Batch 15/64 loss: -0.21467387676239014
Batch 16/64 loss: -0.24458613991737366
Batch 17/64 loss: -0.19105327129364014
Batch 18/64 loss: -0.21507859230041504
Batch 19/64 loss: -0.22043871879577637
Batch 20/64 loss: -0.21754074096679688
Batch 21/64 loss: -0.19177985191345215
Batch 22/64 loss: -0.2204960584640503
Batch 23/64 loss: -0.20518440008163452
Batch 24/64 loss: -0.22519856691360474
Batch 25/64 loss: -0.16059541702270508
Batch 26/64 loss: -0.21605730056762695
Batch 27/64 loss: -0.19745713472366333
Batch 28/64 loss: -0.22901630401611328
Batch 29/64 loss: -0.204273521900177
Batch 30/64 loss: -0.21749281883239746
Batch 31/64 loss: -0.1842631697654724
Batch 32/64 loss: -0.21801352500915527
Batch 33/64 loss: -0.20821219682693481
Batch 34/64 loss: -0.22310113906860352
Batch 35/64 loss: -0.21914225816726685
Batch 36/64 loss: -0.19379198551177979
Batch 37/64 loss: -0.22099006175994873
Batch 38/64 loss: -0.20251226425170898
Batch 39/64 loss: -0.19977444410324097
Batch 40/64 loss: -0.2228875756263733
Batch 41/64 loss: -0.22967493534088135
Batch 42/64 loss: -0.24290531873703003
Batch 43/64 loss: -0.2414872646331787
Batch 44/64 loss: -0.22242653369903564
Batch 45/64 loss: -0.21750468015670776
Batch 46/64 loss: -0.17385655641555786
Batch 47/64 loss: -0.22921067476272583
Batch 48/64 loss: -0.21178758144378662
Batch 49/64 loss: -0.19917166233062744
Batch 50/64 loss: -0.21633446216583252
Batch 51/64 loss: -0.19147831201553345
Batch 52/64 loss: -0.2259211540222168
Batch 53/64 loss: -0.2071298360824585
Batch 54/64 loss: -0.16445422172546387
Batch 55/64 loss: -0.2168450951576233
Batch 56/64 loss: -0.18388307094573975
Batch 57/64 loss: -0.20115435123443604
Batch 58/64 loss: -0.19041132926940918
Batch 59/64 loss: -0.21548891067504883
Batch 60/64 loss: -0.18097907304763794
Batch 61/64 loss: -0.2338767647743225
Batch 62/64 loss: -0.19094568490982056
Batch 63/64 loss: -0.2070053219795227
Batch 64/64 loss: -0.19294190406799316
Epoch 155  Train loss: -0.20982223164801503  Val loss: 0.018653194928906627
Epoch 156
-------------------------------
Batch 1/64 loss: -0.17161458730697632
Batch 2/64 loss: -0.21680551767349243
Batch 3/64 loss: -0.22308975458145142
Batch 4/64 loss: -0.20037853717803955
Batch 5/64 loss: -0.2200617790222168
Batch 6/64 loss: -0.20934897661209106
Batch 7/64 loss: -0.23567229509353638
Batch 8/64 loss: -0.21732044219970703
Batch 9/64 loss: -0.1701040267944336
Batch 10/64 loss: -0.23393398523330688
Batch 11/64 loss: -0.2251276969909668
Batch 12/64 loss: -0.19927799701690674
Batch 13/64 loss: -0.21091312170028687
Batch 14/64 loss: -0.20061206817626953
Batch 15/64 loss: -0.1961139440536499
Batch 16/64 loss: -0.1972028613090515
Batch 17/64 loss: -0.19415557384490967
Batch 18/64 loss: -0.21242010593414307
Batch 19/64 loss: -0.2217639684677124
Batch 20/64 loss: -0.20253020524978638
Batch 21/64 loss: -0.2319047451019287
Batch 22/64 loss: -0.2372783124446869
Batch 23/64 loss: -0.2480866014957428
Batch 24/64 loss: -0.21885520219802856
Batch 25/64 loss: -0.18663549423217773
Batch 26/64 loss: -0.2312259078025818
Batch 27/64 loss: -0.1845024824142456
Batch 28/64 loss: -0.20081478357315063
Batch 29/64 loss: -0.1951587200164795
Batch 30/64 loss: -0.22247159481048584
Batch 31/64 loss: -0.17132484912872314
Batch 32/64 loss: -0.1601390242576599
Batch 33/64 loss: -0.22043132781982422
Batch 34/64 loss: -0.24247771501541138
Batch 35/64 loss: -0.19961047172546387
Batch 36/64 loss: -0.21306908130645752
Batch 37/64 loss: -0.2164609432220459
Batch 38/64 loss: -0.1974952220916748
Batch 39/64 loss: -0.2205774188041687
Batch 40/64 loss: -0.23359352350234985
Batch 41/64 loss: -0.19791853427886963
Batch 42/64 loss: -0.23363655805587769
Batch 43/64 loss: -0.22739577293395996
Batch 44/64 loss: -0.206634521484375
Batch 45/64 loss: -0.21309858560562134
Batch 46/64 loss: -0.2269255518913269
Batch 47/64 loss: -0.22311478853225708
Batch 48/64 loss: -0.23859065771102905
Batch 49/64 loss: -0.22751760482788086
Batch 50/64 loss: -0.19683486223220825
Batch 51/64 loss: -0.20328783988952637
Batch 52/64 loss: -0.2156020998954773
Batch 53/64 loss: -0.204481303691864
Batch 54/64 loss: -0.21047163009643555
Batch 55/64 loss: -0.21002310514450073
Batch 56/64 loss: -0.19273149967193604
Batch 57/64 loss: -0.1888962984085083
Batch 58/64 loss: -0.22365450859069824
Batch 59/64 loss: -0.19547373056411743
Batch 60/64 loss: -0.19873136281967163
Batch 61/64 loss: -0.206323504447937
Batch 62/64 loss: -0.1707713007926941
Batch 63/64 loss: -0.20538508892059326
Batch 64/64 loss: -0.2298644781112671
Epoch 156  Train loss: -0.20988956759957705  Val loss: 0.017009046888843027
Epoch 157
-------------------------------
Batch 1/64 loss: -0.20665514469146729
Batch 2/64 loss: -0.24205386638641357
Batch 3/64 loss: -0.20294702053070068
Batch 4/64 loss: -0.21982425451278687
Batch 5/64 loss: -0.2216523289680481
Batch 6/64 loss: -0.17660415172576904
Batch 7/64 loss: -0.20941227674484253
Batch 8/64 loss: -0.21703314781188965
Batch 9/64 loss: -0.18176305294036865
Batch 10/64 loss: -0.23634743690490723
Batch 11/64 loss: -0.20508164167404175
Batch 12/64 loss: -0.17298948764801025
Batch 13/64 loss: -0.16133534908294678
Batch 14/64 loss: -0.2144927978515625
Batch 15/64 loss: -0.22381258010864258
Batch 16/64 loss: -0.18170160055160522
Batch 17/64 loss: -0.21845006942749023
Batch 18/64 loss: -0.21287786960601807
Batch 19/64 loss: -0.19715142250061035
Batch 20/64 loss: -0.2197158932685852
Batch 21/64 loss: -0.18321001529693604
Batch 22/64 loss: -0.21481740474700928
Batch 23/64 loss: -0.19197672605514526
Batch 24/64 loss: -0.17869454622268677
Batch 25/64 loss: -0.2098383903503418
Batch 26/64 loss: -0.19253331422805786
Batch 27/64 loss: -0.21128731966018677
Batch 28/64 loss: -0.2106860876083374
Batch 29/64 loss: -0.17775100469589233
Batch 30/64 loss: -0.19823956489562988
Batch 31/64 loss: -0.20148754119873047
Batch 32/64 loss: -0.22185468673706055
Batch 33/64 loss: -0.2288188338279724
Batch 34/64 loss: -0.21626323461532593
Batch 35/64 loss: -0.20325905084609985
Batch 36/64 loss: -0.2010197639465332
Batch 37/64 loss: -0.22476321458816528
Batch 38/64 loss: -0.21421873569488525
Batch 39/64 loss: -0.24983811378479004
Batch 40/64 loss: -0.20339685678482056
Batch 41/64 loss: -0.21385395526885986
Batch 42/64 loss: -0.2069159746170044
Batch 43/64 loss: -0.18678361177444458
Batch 44/64 loss: -0.20961856842041016
Batch 45/64 loss: -0.20347940921783447
Batch 46/64 loss: -0.21046483516693115
Batch 47/64 loss: -0.21466362476348877
Batch 48/64 loss: -0.2329675555229187
Batch 49/64 loss: -0.22353768348693848
Batch 50/64 loss: -0.19822537899017334
Batch 51/64 loss: -0.240374356508255
Batch 52/64 loss: -0.2084592580795288
Batch 53/64 loss: -0.2331419587135315
Batch 54/64 loss: -0.20078223943710327
Batch 55/64 loss: -0.1895211935043335
Batch 56/64 loss: -0.1897432804107666
Batch 57/64 loss: -0.2001742720603943
Batch 58/64 loss: -0.19571340084075928
Batch 59/64 loss: -0.21378260850906372
Batch 60/64 loss: -0.2092939019203186
Batch 61/64 loss: -0.223166823387146
Batch 62/64 loss: -0.21584820747375488
Batch 63/64 loss: -0.22992929816246033
Batch 64/64 loss: -0.19179439544677734
Epoch 157  Train loss: -0.20784538025949514  Val loss: 0.016055811721434707
Epoch 158
-------------------------------
Batch 1/64 loss: -0.19422781467437744
Batch 2/64 loss: -0.23007720708847046
Batch 3/64 loss: -0.21099382638931274
Batch 4/64 loss: -0.23749279975891113
Batch 5/64 loss: -0.24759209156036377
Batch 6/64 loss: -0.22913765907287598
Batch 7/64 loss: -0.19501620531082153
Batch 8/64 loss: -0.21840208768844604
Batch 9/64 loss: -0.1902497410774231
Batch 10/64 loss: -0.21483182907104492
Batch 11/64 loss: -0.22130978107452393
Batch 12/64 loss: -0.23340418934822083
Batch 13/64 loss: -0.24030554294586182
Batch 14/64 loss: -0.20344847440719604
Batch 15/64 loss: -0.21140766143798828
Batch 16/64 loss: -0.2046932578086853
Batch 17/64 loss: -0.22262519598007202
Batch 18/64 loss: -0.23375052213668823
Batch 19/64 loss: -0.24127191305160522
Batch 20/64 loss: -0.20498406887054443
Batch 21/64 loss: -0.20922797918319702
Batch 22/64 loss: -0.19968461990356445
Batch 23/64 loss: -0.23774990439414978
Batch 24/64 loss: -0.2189772129058838
Batch 25/64 loss: -0.24176761507987976
Batch 26/64 loss: -0.22977811098098755
Batch 27/64 loss: -0.24418804049491882
Batch 28/64 loss: -0.22876930236816406
Batch 29/64 loss: -0.24183282256126404
Batch 30/64 loss: -0.2185625433921814
Batch 31/64 loss: -0.21120214462280273
Batch 32/64 loss: -0.24789953231811523
Batch 33/64 loss: -0.20497030019760132
Batch 34/64 loss: -0.2126217484474182
Batch 35/64 loss: -0.220653235912323
Batch 36/64 loss: -0.21195447444915771
Batch 37/64 loss: -0.20886093378067017
Batch 38/64 loss: -0.22624194622039795
Batch 39/64 loss: -0.22485947608947754
Batch 40/64 loss: -0.15499085187911987
Batch 41/64 loss: -0.2347690761089325
Batch 42/64 loss: -0.19714534282684326
Batch 43/64 loss: -0.18280094861984253
Batch 44/64 loss: -0.16755908727645874
Batch 45/64 loss: -0.16060572862625122
Batch 46/64 loss: -0.1895783543586731
Batch 47/64 loss: -0.22826647758483887
Batch 48/64 loss: -0.18638592958450317
Batch 49/64 loss: -0.1994696855545044
Batch 50/64 loss: -0.18129068613052368
Batch 51/64 loss: -0.2201189398765564
Batch 52/64 loss: -0.1832740306854248
Batch 53/64 loss: -0.2245681881904602
Batch 54/64 loss: -0.20216798782348633
Batch 55/64 loss: -0.2311019003391266
Batch 56/64 loss: -0.19818484783172607
Batch 57/64 loss: -0.20802998542785645
Batch 58/64 loss: -0.23004895448684692
Batch 59/64 loss: -0.20795458555221558
Batch 60/64 loss: -0.19865792989730835
Batch 61/64 loss: -0.21106958389282227
Batch 62/64 loss: -0.19796127080917358
Batch 63/64 loss: -0.2174243927001953
Batch 64/64 loss: -0.2189105749130249
Epoch 158  Train loss: -0.2133746119106517  Val loss: 0.018197578662859204
Epoch 159
-------------------------------
Batch 1/64 loss: -0.21293318271636963
Batch 2/64 loss: -0.19610214233398438
Batch 3/64 loss: -0.23208600282669067
Batch 4/64 loss: -0.21941262483596802
Batch 5/64 loss: -0.15396970510482788
Batch 6/64 loss: -0.20085161924362183
Batch 7/64 loss: -0.2391979992389679
Batch 8/64 loss: -0.1776278018951416
Batch 9/64 loss: -0.1876617670059204
Batch 10/64 loss: -0.2096942663192749
Batch 11/64 loss: -0.23545408248901367
Batch 12/64 loss: -0.2291812300682068
Batch 13/64 loss: -0.23134887218475342
Batch 14/64 loss: -0.2165994644165039
Batch 15/64 loss: -0.1769518256187439
Batch 16/64 loss: -0.21467214822769165
Batch 17/64 loss: -0.20341306924819946
Batch 18/64 loss: -0.1841866374015808
Batch 19/64 loss: -0.21558576822280884
Batch 20/64 loss: -0.19609510898590088
Batch 21/64 loss: -0.20214569568634033
Batch 22/64 loss: -0.22631967067718506
Batch 23/64 loss: -0.24137485027313232
Batch 24/64 loss: -0.2520342469215393
Batch 25/64 loss: -0.23183977603912354
Batch 26/64 loss: -0.18679934740066528
Batch 27/64 loss: -0.22094249725341797
Batch 28/64 loss: -0.2190861701965332
Batch 29/64 loss: -0.18060952425003052
Batch 30/64 loss: -0.18781572580337524
Batch 31/64 loss: -0.20872408151626587
Batch 32/64 loss: -0.1857309341430664
Batch 33/64 loss: -0.1960635781288147
Batch 34/64 loss: -0.2330731749534607
Batch 35/64 loss: -0.22754371166229248
Batch 36/64 loss: -0.2105565071105957
Batch 37/64 loss: -0.1948835253715515
Batch 38/64 loss: -0.2368147373199463
Batch 39/64 loss: -0.18372327089309692
Batch 40/64 loss: -0.19037318229675293
Batch 41/64 loss: -0.222628653049469
Batch 42/64 loss: -0.2137289047241211
Batch 43/64 loss: -0.2051418423652649
Batch 44/64 loss: -0.20204150676727295
Batch 45/64 loss: -0.22608798742294312
Batch 46/64 loss: -0.11596965789794922
Batch 47/64 loss: -0.22362780570983887
Batch 48/64 loss: -0.2159487009048462
Batch 49/64 loss: -0.2464025318622589
Batch 50/64 loss: -0.19889014959335327
Batch 51/64 loss: -0.21049320697784424
Batch 52/64 loss: -0.22420835494995117
Batch 53/64 loss: -0.2090359926223755
Batch 54/64 loss: -0.21913522481918335
Batch 55/64 loss: -0.22273826599121094
Batch 56/64 loss: -0.20871984958648682
Batch 57/64 loss: -0.24181890487670898
Batch 58/64 loss: -0.205718994140625
Batch 59/64 loss: -0.23209720849990845
Batch 60/64 loss: -0.17201495170593262
Batch 61/64 loss: -0.1996176838874817
Batch 62/64 loss: -0.22540366649627686
Batch 63/64 loss: -0.20117521286010742
Batch 64/64 loss: -0.21569585800170898
Epoch 159  Train loss: -0.20947288905873018  Val loss: 0.016366714464430138
Epoch 160
-------------------------------
Batch 1/64 loss: -0.23083412647247314
Batch 2/64 loss: -0.21205788850784302
Batch 3/64 loss: -0.2510994076728821
Batch 4/64 loss: -0.20098018646240234
Batch 5/64 loss: -0.210382878780365
Batch 6/64 loss: -0.22414636611938477
Batch 7/64 loss: -0.2364516258239746
Batch 8/64 loss: -0.2109239101409912
Batch 9/64 loss: -0.1877351999282837
Batch 10/64 loss: -0.2532879710197449
Batch 11/64 loss: -0.17503643035888672
Batch 12/64 loss: -0.21733403205871582
Batch 13/64 loss: -0.20052027702331543
Batch 14/64 loss: -0.2141890525817871
Batch 15/64 loss: -0.1973109245300293
Batch 16/64 loss: -0.26073500514030457
Batch 17/64 loss: -0.19328951835632324
Batch 18/64 loss: -0.2018607258796692
Batch 19/64 loss: -0.18094563484191895
Batch 20/64 loss: -0.22484207153320312
Batch 21/64 loss: -0.20793414115905762
Batch 22/64 loss: -0.24203383922576904
Batch 23/64 loss: -0.2246166467666626
Batch 24/64 loss: -0.23125141859054565
Batch 25/64 loss: -0.2094208002090454
Batch 26/64 loss: -0.20887988805770874
Batch 27/64 loss: -0.2374671995639801
Batch 28/64 loss: -0.21627330780029297
Batch 29/64 loss: -0.2224034070968628
Batch 30/64 loss: -0.237088143825531
Batch 31/64 loss: -0.20721304416656494
Batch 32/64 loss: -0.1843794584274292
Batch 33/64 loss: -0.21983373165130615
Batch 34/64 loss: -0.20091146230697632
Batch 35/64 loss: -0.19880014657974243
Batch 36/64 loss: -0.22363495826721191
Batch 37/64 loss: -0.23478740453720093
Batch 38/64 loss: -0.22763723134994507
Batch 39/64 loss: -0.20808571577072144
Batch 40/64 loss: -0.24804100394248962
Batch 41/64 loss: -0.24901455640792847
Batch 42/64 loss: -0.19012784957885742
Batch 43/64 loss: -0.21533560752868652
Batch 44/64 loss: -0.22012758255004883
Batch 45/64 loss: -0.2266642451286316
Batch 46/64 loss: -0.21430861949920654
Batch 47/64 loss: -0.23911666870117188
Batch 48/64 loss: -0.22940462827682495
Batch 49/64 loss: -0.20491647720336914
Batch 50/64 loss: -0.16801679134368896
Batch 51/64 loss: -0.22301316261291504
Batch 52/64 loss: -0.24065488576889038
Batch 53/64 loss: -0.23664379119873047
Batch 54/64 loss: -0.21240252256393433
Batch 55/64 loss: -0.19719332456588745
Batch 56/64 loss: -0.1775035262107849
Batch 57/64 loss: -0.2366543412208557
Batch 58/64 loss: -0.18303251266479492
Batch 59/64 loss: -0.22021257877349854
Batch 60/64 loss: -0.23214668035507202
Batch 61/64 loss: -0.21988052129745483
Batch 62/64 loss: -0.18837398290634155
Batch 63/64 loss: -0.21748775243759155
Batch 64/64 loss: -0.24027693271636963
Epoch 160  Train loss: -0.21642504299388213  Val loss: 0.017507672514702446
Epoch 161
-------------------------------
Batch 1/64 loss: -0.23614084720611572
Batch 2/64 loss: -0.21549969911575317
Batch 3/64 loss: -0.22568917274475098
Batch 4/64 loss: -0.22693002223968506
Batch 5/64 loss: -0.23946994543075562
Batch 6/64 loss: -0.23128628730773926
Batch 7/64 loss: -0.18322807550430298
Batch 8/64 loss: -0.19153660535812378
Batch 9/64 loss: -0.20496892929077148
Batch 10/64 loss: -0.23605260252952576
Batch 11/64 loss: -0.22407031059265137
Batch 12/64 loss: -0.24155694246292114
Batch 13/64 loss: -0.23081576824188232
Batch 14/64 loss: -0.21660387516021729
Batch 15/64 loss: -0.2307453751564026
Batch 16/64 loss: -0.2044128179550171
Batch 17/64 loss: -0.2349132001399994
Batch 18/64 loss: -0.22510260343551636
Batch 19/64 loss: -0.2128535509109497
Batch 20/64 loss: -0.19127565622329712
Batch 21/64 loss: -0.2039368748664856
Batch 22/64 loss: -0.2182660698890686
Batch 23/64 loss: -0.2135922908782959
Batch 24/64 loss: -0.1986185908317566
Batch 25/64 loss: -0.21487146615982056
Batch 26/64 loss: -0.22917628288269043
Batch 27/64 loss: -0.24932244420051575
Batch 28/64 loss: -0.22770822048187256
Batch 29/64 loss: -0.23337945342063904
Batch 30/64 loss: -0.21915268898010254
Batch 31/64 loss: -0.21309739351272583
Batch 32/64 loss: -0.2100832462310791
Batch 33/64 loss: -0.19934755563735962
Batch 34/64 loss: -0.22964060306549072
Batch 35/64 loss: -0.20022040605545044
Batch 36/64 loss: -0.2070673704147339
Batch 37/64 loss: -0.21724820137023926
Batch 38/64 loss: -0.24416306614875793
Batch 39/64 loss: -0.2087017297744751
Batch 40/64 loss: -0.1880631446838379
Batch 41/64 loss: -0.21364647150039673
Batch 42/64 loss: -0.22642850875854492
Batch 43/64 loss: -0.23300045728683472
Batch 44/64 loss: -0.20200449228286743
Batch 45/64 loss: -0.19584143161773682
Batch 46/64 loss: -0.21842104196548462
Batch 47/64 loss: -0.18280720710754395
Batch 48/64 loss: -0.20249390602111816
Batch 49/64 loss: -0.2392978072166443
Batch 50/64 loss: -0.191514253616333
Batch 51/64 loss: -0.22747528553009033
Batch 52/64 loss: -0.21962755918502808
Batch 53/64 loss: -0.17084741592407227
Batch 54/64 loss: -0.21010422706604004
Batch 55/64 loss: -0.21191227436065674
Batch 56/64 loss: -0.1866389513015747
Batch 57/64 loss: -0.19916480779647827
Batch 58/64 loss: -0.21958845853805542
Batch 59/64 loss: -0.17151618003845215
Batch 60/64 loss: -0.23852390050888062
Batch 61/64 loss: -0.18815094232559204
Batch 62/64 loss: -0.16530746221542358
Batch 63/64 loss: -0.23367202281951904
Batch 64/64 loss: -0.2164018154144287
Epoch 161  Train loss: -0.21394660098879945  Val loss: 0.015633073869030092
Epoch 162
-------------------------------
Batch 1/64 loss: -0.2257232666015625
Batch 2/64 loss: -0.21197426319122314
Batch 3/64 loss: -0.2199862003326416
Batch 4/64 loss: -0.22969233989715576
Batch 5/64 loss: -0.18644720315933228
Batch 6/64 loss: -0.2204464077949524
Batch 7/64 loss: -0.20062637329101562
Batch 8/64 loss: -0.20806199312210083
Batch 9/64 loss: -0.2144833207130432
Batch 10/64 loss: -0.22845101356506348
Batch 11/64 loss: -0.17623275518417358
Batch 12/64 loss: -0.21948665380477905
Batch 13/64 loss: -0.19487369060516357
Batch 14/64 loss: -0.1866297721862793
Batch 15/64 loss: -0.21400678157806396
Batch 16/64 loss: -0.22820401191711426
Batch 17/64 loss: -0.2083994746208191
Batch 18/64 loss: -0.19982236623764038
Batch 19/64 loss: -0.1801924705505371
Batch 20/64 loss: -0.23907002806663513
Batch 21/64 loss: -0.15226417779922485
Batch 22/64 loss: -0.23736822605133057
Batch 23/64 loss: -0.2247292399406433
Batch 24/64 loss: -0.23176145553588867
Batch 25/64 loss: -0.2189626693725586
Batch 26/64 loss: -0.24019324779510498
Batch 27/64 loss: -0.22238749265670776
Batch 28/64 loss: -0.2422136664390564
Batch 29/64 loss: -0.21716326475143433
Batch 30/64 loss: -0.19593781232833862
Batch 31/64 loss: -0.11890578269958496
Batch 32/64 loss: -0.23145681619644165
Batch 33/64 loss: -0.20911365747451782
Batch 34/64 loss: -0.2315366268157959
Batch 35/64 loss: -0.1960984468460083
Batch 36/64 loss: -0.21440893411636353
Batch 37/64 loss: -0.25209134817123413
Batch 38/64 loss: -0.15004444122314453
Batch 39/64 loss: -0.20597875118255615
Batch 40/64 loss: -0.1905301809310913
Batch 41/64 loss: -0.22527122497558594
Batch 42/64 loss: -0.19670456647872925
Batch 43/64 loss: -0.22500836849212646
Batch 44/64 loss: -0.21453917026519775
Batch 45/64 loss: -0.23627856373786926
Batch 46/64 loss: -0.19284194707870483
Batch 47/64 loss: -0.1820341944694519
Batch 48/64 loss: -0.23793327808380127
Batch 49/64 loss: -0.1488736867904663
Batch 50/64 loss: -0.23616936802864075
Batch 51/64 loss: -0.22102642059326172
Batch 52/64 loss: -0.24480152130126953
Batch 53/64 loss: -0.19928324222564697
Batch 54/64 loss: -0.22128194570541382
Batch 55/64 loss: -0.2484285831451416
Batch 56/64 loss: -0.21251678466796875
Batch 57/64 loss: -0.2282840609550476
Batch 58/64 loss: -0.24491965770721436
Batch 59/64 loss: -0.22347313165664673
Batch 60/64 loss: -0.22557568550109863
Batch 61/64 loss: -0.213553786277771
Batch 62/64 loss: -0.2210865020751953
Batch 63/64 loss: -0.24155282974243164
Batch 64/64 loss: -0.18093407154083252
Epoch 162  Train loss: -0.21259757958206477  Val loss: 0.016930774929597207
Epoch 163
-------------------------------
Batch 1/64 loss: -0.23481512069702148
Batch 2/64 loss: -0.23314791917800903
Batch 3/64 loss: -0.1905239224433899
Batch 4/64 loss: -0.22423195838928223
Batch 5/64 loss: -0.20864516496658325
Batch 6/64 loss: -0.22927260398864746
Batch 7/64 loss: -0.2320162057876587
Batch 8/64 loss: -0.24268680810928345
Batch 9/64 loss: -0.19445854425430298
Batch 10/64 loss: -0.19955462217330933
Batch 11/64 loss: -0.24516373872756958
Batch 12/64 loss: -0.2003483772277832
Batch 13/64 loss: -0.1993451714515686
Batch 14/64 loss: -0.2516774535179138
Batch 15/64 loss: -0.23478606343269348
Batch 16/64 loss: -0.23346441984176636
Batch 17/64 loss: -0.18483954668045044
Batch 18/64 loss: -0.20462274551391602
Batch 19/64 loss: -0.21846365928649902
Batch 20/64 loss: -0.2082275152206421
Batch 21/64 loss: -0.2099606990814209
Batch 22/64 loss: -0.22401416301727295
Batch 23/64 loss: -0.20201903581619263
Batch 24/64 loss: -0.2316383421421051
Batch 25/64 loss: -0.2177453637123108
Batch 26/64 loss: -0.23149633407592773
Batch 27/64 loss: -0.22581714391708374
Batch 28/64 loss: -0.23377129435539246
Batch 29/64 loss: -0.21292006969451904
Batch 30/64 loss: -0.21538269519805908
Batch 31/64 loss: -0.22250103950500488
Batch 32/64 loss: -0.21708619594573975
Batch 33/64 loss: -0.20025253295898438
Batch 34/64 loss: -0.19123780727386475
Batch 35/64 loss: -0.19712287187576294
Batch 36/64 loss: -0.2227417230606079
Batch 37/64 loss: -0.20662713050842285
Batch 38/64 loss: -0.1918371319770813
Batch 39/64 loss: -0.2219552993774414
Batch 40/64 loss: -0.2110845446586609
Batch 41/64 loss: -0.22040194272994995
Batch 42/64 loss: -0.21658217906951904
Batch 43/64 loss: -0.21474140882492065
Batch 44/64 loss: -0.2363007664680481
Batch 45/64 loss: -0.22188347578048706
Batch 46/64 loss: -0.21223193407058716
Batch 47/64 loss: -0.24089306592941284
Batch 48/64 loss: -0.23112517595291138
Batch 49/64 loss: -0.2023187279701233
Batch 50/64 loss: -0.17820274829864502
Batch 51/64 loss: -0.15135931968688965
Batch 52/64 loss: -0.20484626293182373
Batch 53/64 loss: -0.18830257654190063
Batch 54/64 loss: -0.22377276420593262
Batch 55/64 loss: -0.2165982723236084
Batch 56/64 loss: -0.20467638969421387
Batch 57/64 loss: -0.2213556170463562
Batch 58/64 loss: -0.19668376445770264
Batch 59/64 loss: -0.21075201034545898
Batch 60/64 loss: -0.22798341512680054
Batch 61/64 loss: -0.22107654809951782
Batch 62/64 loss: -0.2293250560760498
Batch 63/64 loss: -0.19633406400680542
Batch 64/64 loss: -0.1795727014541626
Epoch 163  Train loss: -0.214273411619897  Val loss: 0.02104221791336217
Epoch 164
-------------------------------
Batch 1/64 loss: -0.21426314115524292
Batch 2/64 loss: -0.16780108213424683
Batch 3/64 loss: -0.22094762325286865
Batch 4/64 loss: -0.20764446258544922
Batch 5/64 loss: -0.19784075021743774
Batch 6/64 loss: -0.23316723108291626
Batch 7/64 loss: -0.18528276681900024
Batch 8/64 loss: -0.2515258193016052
Batch 9/64 loss: -0.21715706586837769
Batch 10/64 loss: -0.22743666172027588
Batch 11/64 loss: -0.2254435420036316
Batch 12/64 loss: -0.1926679015159607
Batch 13/64 loss: -0.1979764699935913
Batch 14/64 loss: -0.22277390956878662
Batch 15/64 loss: -0.2012084722518921
Batch 16/64 loss: -0.22173649072647095
Batch 17/64 loss: -0.16475558280944824
Batch 18/64 loss: -0.21243226528167725
Batch 19/64 loss: -0.1800926923751831
Batch 20/64 loss: -0.22840583324432373
Batch 21/64 loss: -0.21027910709381104
Batch 22/64 loss: -0.21390748023986816
Batch 23/64 loss: -0.2309412956237793
Batch 24/64 loss: -0.23044461011886597
Batch 25/64 loss: -0.2224631905555725
Batch 26/64 loss: -0.22419404983520508
Batch 27/64 loss: -0.23551598191261292
Batch 28/64 loss: -0.24148091673851013
Batch 29/64 loss: -0.21271586418151855
Batch 30/64 loss: -0.21093016862869263
Batch 31/64 loss: -0.25414133071899414
Batch 32/64 loss: -0.21785807609558105
Batch 33/64 loss: -0.22761547565460205
Batch 34/64 loss: -0.21464753150939941
Batch 35/64 loss: -0.22874099016189575
Batch 36/64 loss: -0.23856526613235474
Batch 37/64 loss: -0.19365179538726807
Batch 38/64 loss: -0.21049338579177856
Batch 39/64 loss: -0.24983525276184082
Batch 40/64 loss: -0.21508467197418213
Batch 41/64 loss: -0.2187989354133606
Batch 42/64 loss: -0.22586220502853394
Batch 43/64 loss: -0.19424092769622803
Batch 44/64 loss: -0.24038982391357422
Batch 45/64 loss: -0.23728162050247192
Batch 46/64 loss: -0.1634621024131775
Batch 47/64 loss: -0.2418804168701172
Batch 48/64 loss: -0.20119589567184448
Batch 49/64 loss: -0.23099559545516968
Batch 50/64 loss: -0.20821213722229004
Batch 51/64 loss: -0.2239776849746704
Batch 52/64 loss: -0.2208465337753296
Batch 53/64 loss: -0.20639300346374512
Batch 54/64 loss: -0.19970083236694336
Batch 55/64 loss: -0.22818762063980103
Batch 56/64 loss: -0.19921517372131348
Batch 57/64 loss: -0.23727726936340332
Batch 58/64 loss: -0.22827553749084473
Batch 59/64 loss: -0.2058354616165161
Batch 60/64 loss: -0.24670538306236267
Batch 61/64 loss: -0.19902825355529785
Batch 62/64 loss: -0.1862921118736267
Batch 63/64 loss: -0.2138683795928955
Batch 64/64 loss: -0.2184125781059265
Epoch 164  Train loss: -0.21609095755745383  Val loss: 0.016397097880897653
Epoch 165
-------------------------------
Batch 1/64 loss: -0.23651927709579468
Batch 2/64 loss: -0.21877360343933105
Batch 3/64 loss: -0.171697735786438
Batch 4/64 loss: -0.24865299463272095
Batch 5/64 loss: -0.2424631416797638
Batch 6/64 loss: -0.24033606052398682
Batch 7/64 loss: -0.24231743812561035
Batch 8/64 loss: -0.21802771091461182
Batch 9/64 loss: -0.22932565212249756
Batch 10/64 loss: -0.2070239782333374
Batch 11/64 loss: -0.22751456499099731
Batch 12/64 loss: -0.2260839343070984
Batch 13/64 loss: -0.21894705295562744
Batch 14/64 loss: -0.24054664373397827
Batch 15/64 loss: -0.21649175882339478
Batch 16/64 loss: -0.2352534830570221
Batch 17/64 loss: -0.16109609603881836
Batch 18/64 loss: -0.232538104057312
Batch 19/64 loss: -0.22503751516342163
Batch 20/64 loss: -0.20916235446929932
Batch 21/64 loss: -0.23531872034072876
Batch 22/64 loss: -0.2194685935974121
Batch 23/64 loss: -0.23739832639694214
Batch 24/64 loss: -0.18237483501434326
Batch 25/64 loss: -0.22442221641540527
Batch 26/64 loss: -0.2303484082221985
Batch 27/64 loss: -0.21556740999221802
Batch 28/64 loss: -0.14564001560211182
Batch 29/64 loss: -0.22892063856124878
Batch 30/64 loss: -0.17758303880691528
Batch 31/64 loss: -0.19777661561965942
Batch 32/64 loss: -0.19302237033843994
Batch 33/64 loss: -0.2035531997680664
Batch 34/64 loss: -0.2530529797077179
Batch 35/64 loss: -0.20435655117034912
Batch 36/64 loss: -0.2269037365913391
Batch 37/64 loss: -0.2312508225440979
Batch 38/64 loss: -0.23091590404510498
Batch 39/64 loss: -0.21472513675689697
Batch 40/64 loss: -0.21152502298355103
Batch 41/64 loss: -0.21170878410339355
Batch 42/64 loss: -0.2155081033706665
Batch 43/64 loss: -0.19869333505630493
Batch 44/64 loss: -0.21575653553009033
Batch 45/64 loss: -0.19676166772842407
Batch 46/64 loss: -0.17093425989151
Batch 47/64 loss: -0.19411104917526245
Batch 48/64 loss: -0.183324933052063
Batch 49/64 loss: -0.21834003925323486
Batch 50/64 loss: -0.2085784673690796
Batch 51/64 loss: -0.22388768196105957
Batch 52/64 loss: -0.22829294204711914
Batch 53/64 loss: -0.2380799949169159
Batch 54/64 loss: -0.23566383123397827
Batch 55/64 loss: -0.21702873706817627
Batch 56/64 loss: -0.19402730464935303
Batch 57/64 loss: -0.20159310102462769
Batch 58/64 loss: -0.24937793612480164
Batch 59/64 loss: -0.1846088171005249
Batch 60/64 loss: -0.18484079837799072
Batch 61/64 loss: -0.22712993621826172
Batch 62/64 loss: -0.21329325437545776
Batch 63/64 loss: -0.21724414825439453
Batch 64/64 loss: -0.22943973541259766
Epoch 165  Train loss: -0.21510273101283053  Val loss: 0.018696577278609128
Epoch 166
-------------------------------
Batch 1/64 loss: -0.19661670923233032
Batch 2/64 loss: -0.21069973707199097
Batch 3/64 loss: -0.22239792346954346
Batch 4/64 loss: -0.20896518230438232
Batch 5/64 loss: -0.23891472816467285
Batch 6/64 loss: -0.2136523723602295
Batch 7/64 loss: -0.2165062427520752
Batch 8/64 loss: -0.22266608476638794
Batch 9/64 loss: -0.2096954584121704
Batch 10/64 loss: -0.20675534009933472
Batch 11/64 loss: -0.20392143726348877
Batch 12/64 loss: -0.2572859823703766
Batch 13/64 loss: -0.2203463315963745
Batch 14/64 loss: -0.22546309232711792
Batch 15/64 loss: -0.22540676593780518
Batch 16/64 loss: -0.2028118371963501
Batch 17/64 loss: -0.21681487560272217
Batch 18/64 loss: -0.24457618594169617
Batch 19/64 loss: -0.20747625827789307
Batch 20/64 loss: -0.19776266813278198
Batch 21/64 loss: -0.24243980646133423
Batch 22/64 loss: -0.19056731462478638
Batch 23/64 loss: -0.20551276206970215
Batch 24/64 loss: -0.2147800326347351
Batch 25/64 loss: -0.24040725827217102
Batch 26/64 loss: -0.2171926498413086
Batch 27/64 loss: -0.21674007177352905
Batch 28/64 loss: -0.2116081714630127
Batch 29/64 loss: -0.22006630897521973
Batch 30/64 loss: -0.19268208742141724
Batch 31/64 loss: -0.22489124536514282
Batch 32/64 loss: -0.2055848240852356
Batch 33/64 loss: -0.2367139458656311
Batch 34/64 loss: -0.23515862226486206
Batch 35/64 loss: -0.24249887466430664
Batch 36/64 loss: -0.18709462881088257
Batch 37/64 loss: -0.18169784545898438
Batch 38/64 loss: -0.19195818901062012
Batch 39/64 loss: -0.20378345251083374
Batch 40/64 loss: -0.23485863208770752
Batch 41/64 loss: -0.22944825887680054
Batch 42/64 loss: -0.1999836564064026
Batch 43/64 loss: -0.22677010297775269
Batch 44/64 loss: -0.2226189374923706
Batch 45/64 loss: -0.2139204740524292
Batch 46/64 loss: -0.23567870259284973
Batch 47/64 loss: -0.20997750759124756
Batch 48/64 loss: -0.2264617681503296
Batch 49/64 loss: -0.22765105962753296
Batch 50/64 loss: -0.22279977798461914
Batch 51/64 loss: -0.2087184190750122
Batch 52/64 loss: -0.2061339020729065
Batch 53/64 loss: -0.20989322662353516
Batch 54/64 loss: -0.22991406917572021
Batch 55/64 loss: -0.2104470133781433
Batch 56/64 loss: -0.21284151077270508
Batch 57/64 loss: -0.19603568315505981
Batch 58/64 loss: -0.2568630576133728
Batch 59/64 loss: -0.20813697576522827
Batch 60/64 loss: -0.2095191478729248
Batch 61/64 loss: -0.18264365196228027
Batch 62/64 loss: -0.2257826328277588
Batch 63/64 loss: -0.1802293062210083
Batch 64/64 loss: -0.20444172620773315
Epoch 166  Train loss: -0.21569838547239117  Val loss: 0.022925319540541608
Epoch 167
-------------------------------
Batch 1/64 loss: -0.22515743970870972
Batch 2/64 loss: -0.22803038358688354
Batch 3/64 loss: -0.23586279153823853
Batch 4/64 loss: -0.23462605476379395
Batch 5/64 loss: -0.24197989702224731
Batch 6/64 loss: -0.21989458799362183
Batch 7/64 loss: -0.22464478015899658
Batch 8/64 loss: -0.23435410857200623
Batch 9/64 loss: -0.23392993211746216
Batch 10/64 loss: -0.22263169288635254
Batch 11/64 loss: -0.20603609085083008
Batch 12/64 loss: -0.22365641593933105
Batch 13/64 loss: -0.23944434523582458
Batch 14/64 loss: -0.22381585836410522
Batch 15/64 loss: -0.2471534013748169
Batch 16/64 loss: -0.22203677892684937
Batch 17/64 loss: -0.23209983110427856
Batch 18/64 loss: -0.1786412000656128
Batch 19/64 loss: -0.22690486907958984
Batch 20/64 loss: -0.21822386980056763
Batch 21/64 loss: -0.1857990026473999
Batch 22/64 loss: -0.23569068312644958
Batch 23/64 loss: -0.22510230541229248
Batch 24/64 loss: -0.19283711910247803
Batch 25/64 loss: -0.24987345933914185
Batch 26/64 loss: -0.1866479516029358
Batch 27/64 loss: -0.2349785566329956
Batch 28/64 loss: -0.2167433500289917
Batch 29/64 loss: -0.20575904846191406
Batch 30/64 loss: -0.19444847106933594
Batch 31/64 loss: -0.21898698806762695
Batch 32/64 loss: -0.22177976369857788
Batch 33/64 loss: -0.1986626386642456
Batch 34/64 loss: -0.19659608602523804
Batch 35/64 loss: -0.2262122631072998
Batch 36/64 loss: -0.22165632247924805
Batch 37/64 loss: -0.20060986280441284
Batch 38/64 loss: -0.20313513278961182
Batch 39/64 loss: -0.1961498260498047
Batch 40/64 loss: -0.21719574928283691
Batch 41/64 loss: -0.2085748314857483
Batch 42/64 loss: -0.21348929405212402
Batch 43/64 loss: -0.2177431583404541
Batch 44/64 loss: -0.20983505249023438
Batch 45/64 loss: -0.2016025185585022
Batch 46/64 loss: -0.2342289686203003
Batch 47/64 loss: -0.20557057857513428
Batch 48/64 loss: -0.2017754316329956
Batch 49/64 loss: -0.22214776277542114
Batch 50/64 loss: -0.2399924099445343
Batch 51/64 loss: -0.2295580506324768
Batch 52/64 loss: -0.22708189487457275
Batch 53/64 loss: -0.21299773454666138
Batch 54/64 loss: -0.24284398555755615
Batch 55/64 loss: -0.2267504334449768
Batch 56/64 loss: -0.2072209119796753
Batch 57/64 loss: -0.2540270686149597
Batch 58/64 loss: -0.22533047199249268
Batch 59/64 loss: -0.20601582527160645
Batch 60/64 loss: -0.19691944122314453
Batch 61/64 loss: -0.22439825534820557
Batch 62/64 loss: -0.21163642406463623
Batch 63/64 loss: -0.17850786447525024
Batch 64/64 loss: -0.2187972068786621
Epoch 167  Train loss: -0.21820133620617435  Val loss: 0.018908488996250115
Epoch 168
-------------------------------
Batch 1/64 loss: -0.23984527587890625
Batch 2/64 loss: -0.2328394055366516
Batch 3/64 loss: -0.2451082468032837
Batch 4/64 loss: -0.22540265321731567
Batch 5/64 loss: -0.22402745485305786
Batch 6/64 loss: -0.22956585884094238
Batch 7/64 loss: -0.2248890995979309
Batch 8/64 loss: -0.19246894121170044
Batch 9/64 loss: -0.21241730451583862
Batch 10/64 loss: -0.2251434326171875
Batch 11/64 loss: -0.2310766577720642
Batch 12/64 loss: -0.19324928522109985
Batch 13/64 loss: -0.24238324165344238
Batch 14/64 loss: -0.2252785563468933
Batch 15/64 loss: -0.22492265701293945
Batch 16/64 loss: -0.17832326889038086
Batch 17/64 loss: -0.24035340547561646
Batch 18/64 loss: -0.2382732629776001
Batch 19/64 loss: -0.19469738006591797
Batch 20/64 loss: -0.22462308406829834
Batch 21/64 loss: -0.2478889524936676
Batch 22/64 loss: -0.2361539602279663
Batch 23/64 loss: -0.23309805989265442
Batch 24/64 loss: -0.19124209880828857
Batch 25/64 loss: -0.17641234397888184
Batch 26/64 loss: -0.21135056018829346
Batch 27/64 loss: -0.1739490032196045
Batch 28/64 loss: -0.21402841806411743
Batch 29/64 loss: -0.2085086703300476
Batch 30/64 loss: -0.22626465559005737
Batch 31/64 loss: -0.23342439532279968
Batch 32/64 loss: -0.19458454847335815
Batch 33/64 loss: -0.21197980642318726
Batch 34/64 loss: -0.1913241147994995
Batch 35/64 loss: -0.22228741645812988
Batch 36/64 loss: -0.21206241846084595
Batch 37/64 loss: -0.214011549949646
Batch 38/64 loss: -0.22383564710617065
Batch 39/64 loss: -0.18297266960144043
Batch 40/64 loss: -0.24172145128250122
Batch 41/64 loss: -0.24281692504882812
Batch 42/64 loss: -0.20925575494766235
Batch 43/64 loss: -0.22780239582061768
Batch 44/64 loss: -0.21543532609939575
Batch 45/64 loss: -0.24438276886940002
Batch 46/64 loss: -0.22070741653442383
Batch 47/64 loss: -0.18629127740859985
Batch 48/64 loss: -0.22835326194763184
Batch 49/64 loss: -0.21057194471359253
Batch 50/64 loss: -0.25344306230545044
Batch 51/64 loss: -0.22324103116989136
Batch 52/64 loss: -0.2469930350780487
Batch 53/64 loss: -0.23234206438064575
Batch 54/64 loss: -0.23143568634986877
Batch 55/64 loss: -0.22263026237487793
Batch 56/64 loss: -0.21353864669799805
Batch 57/64 loss: -0.23035097122192383
Batch 58/64 loss: -0.19778192043304443
Batch 59/64 loss: -0.25885921716690063
Batch 60/64 loss: -0.21783149242401123
Batch 61/64 loss: -0.20609498023986816
Batch 62/64 loss: -0.2036227583885193
Batch 63/64 loss: -0.22721070051193237
Batch 64/64 loss: -0.22166001796722412
Epoch 168  Train loss: -0.21975258238175335  Val loss: 0.02214594436265349
Epoch 169
-------------------------------
Batch 1/64 loss: -0.18647104501724243
Batch 2/64 loss: -0.1809956431388855
Batch 3/64 loss: -0.2248084545135498
Batch 4/64 loss: -0.24340733885765076
Batch 5/64 loss: -0.24283051490783691
Batch 6/64 loss: -0.19356554746627808
Batch 7/64 loss: -0.25619935989379883
Batch 8/64 loss: -0.22557014226913452
Batch 9/64 loss: -0.2098984718322754
Batch 10/64 loss: -0.22056496143341064
Batch 11/64 loss: -0.18740946054458618
Batch 12/64 loss: -0.2106924057006836
Batch 13/64 loss: -0.20065218210220337
Batch 14/64 loss: -0.24620670080184937
Batch 15/64 loss: -0.23776867985725403
Batch 16/64 loss: -0.19896787405014038
Batch 17/64 loss: -0.25590524077415466
Batch 18/64 loss: -0.2014153003692627
Batch 19/64 loss: -0.22842228412628174
Batch 20/64 loss: -0.22965437173843384
Batch 21/64 loss: -0.24798470735549927
Batch 22/64 loss: -0.21015864610671997
Batch 23/64 loss: -0.22811883687973022
Batch 24/64 loss: -0.21581566333770752
Batch 25/64 loss: -0.19261032342910767
Batch 26/64 loss: -0.23108845949172974
Batch 27/64 loss: -0.2413179874420166
Batch 28/64 loss: -0.2383287250995636
Batch 29/64 loss: -0.26148614287376404
Batch 30/64 loss: -0.2313985526561737
Batch 31/64 loss: -0.23856991529464722
Batch 32/64 loss: -0.22468256950378418
Batch 33/64 loss: -0.21884238719940186
Batch 34/64 loss: -0.214036226272583
Batch 35/64 loss: -0.2462191879749298
Batch 36/64 loss: -0.21834492683410645
Batch 37/64 loss: -0.189294695854187
Batch 38/64 loss: -0.15232348442077637
Batch 39/64 loss: -0.2395744025707245
Batch 40/64 loss: -0.1865520477294922
Batch 41/64 loss: -0.21591883897781372
Batch 42/64 loss: -0.20376503467559814
Batch 43/64 loss: -0.22196036577224731
Batch 44/64 loss: -0.25588077306747437
Batch 45/64 loss: -0.23170092701911926
Batch 46/64 loss: -0.20239758491516113
Batch 47/64 loss: -0.23877155780792236
Batch 48/64 loss: -0.23420679569244385
Batch 49/64 loss: -0.24824464321136475
Batch 50/64 loss: -0.238096684217453
Batch 51/64 loss: -0.22994768619537354
Batch 52/64 loss: -0.18523573875427246
Batch 53/64 loss: -0.19569486379623413
Batch 54/64 loss: -0.2136198878288269
Batch 55/64 loss: -0.2613964080810547
Batch 56/64 loss: -0.22966289520263672
Batch 57/64 loss: -0.23144462704658508
Batch 58/64 loss: -0.22009289264678955
Batch 59/64 loss: -0.21369093656539917
Batch 60/64 loss: -0.2239999771118164
Batch 61/64 loss: -0.24469560384750366
Batch 62/64 loss: -0.2174316644668579
Batch 63/64 loss: -0.22007155418395996
Batch 64/64 loss: -0.22374498844146729
Epoch 169  Train loss: -0.22202181255116182  Val loss: 0.017486566120816262
Epoch 170
-------------------------------
Batch 1/64 loss: -0.21779882907867432
Batch 2/64 loss: -0.22237372398376465
Batch 3/64 loss: -0.25117266178131104
Batch 4/64 loss: -0.23253899812698364
Batch 5/64 loss: -0.2372477650642395
Batch 6/64 loss: -0.24378159642219543
Batch 7/64 loss: -0.20285046100616455
Batch 8/64 loss: -0.2460881769657135
Batch 9/64 loss: -0.20519906282424927
Batch 10/64 loss: -0.2475847601890564
Batch 11/64 loss: -0.2490176558494568
Batch 12/64 loss: -0.2265714406967163
Batch 13/64 loss: -0.22283077239990234
Batch 14/64 loss: -0.20939642190933228
Batch 15/64 loss: -0.19034039974212646
Batch 16/64 loss: -0.2186458706855774
Batch 17/64 loss: -0.2233966588973999
Batch 18/64 loss: -0.22737199068069458
Batch 19/64 loss: -0.24045568704605103
Batch 20/64 loss: -0.24260973930358887
Batch 21/64 loss: -0.2192751169204712
Batch 22/64 loss: -0.20870649814605713
Batch 23/64 loss: -0.23641139268875122
Batch 24/64 loss: -0.20816022157669067
Batch 25/64 loss: -0.21476370096206665
Batch 26/64 loss: -0.2206171751022339
Batch 27/64 loss: -0.22917228937149048
Batch 28/64 loss: -0.25710785388946533
Batch 29/64 loss: -0.23588827252388
Batch 30/64 loss: -0.23995208740234375
Batch 31/64 loss: -0.2335941195487976
Batch 32/64 loss: -0.23205101490020752
Batch 33/64 loss: -0.21768933534622192
Batch 34/64 loss: -0.227830171585083
Batch 35/64 loss: -0.22994005680084229
Batch 36/64 loss: -0.23261767625808716
Batch 37/64 loss: -0.2187422513961792
Batch 38/64 loss: -0.2350938618183136
Batch 39/64 loss: -0.20215833187103271
Batch 40/64 loss: -0.2661482095718384
Batch 41/64 loss: -0.22856378555297852
Batch 42/64 loss: -0.21574831008911133
Batch 43/64 loss: -0.20418065786361694
Batch 44/64 loss: -0.2253934144973755
Batch 45/64 loss: -0.2152746319770813
Batch 46/64 loss: -0.23223799467086792
Batch 47/64 loss: -0.22793298959732056
Batch 48/64 loss: -0.20755290985107422
Batch 49/64 loss: -0.17848163843154907
Batch 50/64 loss: -0.2386690378189087
Batch 51/64 loss: -0.19740760326385498
Batch 52/64 loss: -0.19070690870285034
Batch 53/64 loss: -0.22613197565078735
Batch 54/64 loss: -0.22712641954421997
Batch 55/64 loss: -0.22852152585983276
Batch 56/64 loss: -0.197068452835083
Batch 57/64 loss: -0.24419406056404114
Batch 58/64 loss: -0.22077912092208862
Batch 59/64 loss: -0.24454912543296814
Batch 60/64 loss: -0.21634674072265625
Batch 61/64 loss: -0.22447413206100464
Batch 62/64 loss: -0.22026973962783813
Batch 63/64 loss: -0.20821058750152588
Batch 64/64 loss: -0.19600242376327515
Epoch 170  Train loss: -0.2241571120187348  Val loss: 0.025559794861836124
Epoch 171
-------------------------------
Batch 1/64 loss: -0.23972564935684204
Batch 2/64 loss: -0.2290440797805786
Batch 3/64 loss: -0.23904865980148315
Batch 4/64 loss: -0.22730660438537598
Batch 5/64 loss: -0.2461490035057068
Batch 6/64 loss: -0.26338765025138855
Batch 7/64 loss: -0.23152965307235718
Batch 8/64 loss: -0.2465418577194214
Batch 9/64 loss: -0.2761141359806061
Batch 10/64 loss: -0.23311233520507812
Batch 11/64 loss: -0.23295021057128906
Batch 12/64 loss: -0.2193405032157898
Batch 13/64 loss: -0.2520560026168823
Batch 14/64 loss: -0.21461910009384155
Batch 15/64 loss: -0.2373916506767273
Batch 16/64 loss: -0.22020971775054932
Batch 17/64 loss: -0.22287803888320923
Batch 18/64 loss: -0.18629950284957886
Batch 19/64 loss: -0.23715004324913025
Batch 20/64 loss: -0.21121543645858765
Batch 21/64 loss: -0.21846097707748413
Batch 22/64 loss: -0.21724742650985718
Batch 23/64 loss: -0.23073816299438477
Batch 24/64 loss: -0.2204042673110962
Batch 25/64 loss: -0.18163269758224487
Batch 26/64 loss: -0.21667379140853882
Batch 27/64 loss: -0.20555615425109863
Batch 28/64 loss: -0.23867005109786987
Batch 29/64 loss: -0.22274404764175415
Batch 30/64 loss: -0.2213836908340454
Batch 31/64 loss: -0.211109459400177
Batch 32/64 loss: -0.19911682605743408
Batch 33/64 loss: -0.22826546430587769
Batch 34/64 loss: -0.21530723571777344
Batch 35/64 loss: -0.21209579706192017
Batch 36/64 loss: -0.2334117293357849
Batch 37/64 loss: -0.2591713070869446
Batch 38/64 loss: -0.24969395995140076
Batch 39/64 loss: -0.2037830352783203
Batch 40/64 loss: -0.2359963059425354
Batch 41/64 loss: -0.197085440158844
Batch 42/64 loss: -0.2159366011619568
Batch 43/64 loss: -0.20121586322784424
Batch 44/64 loss: -0.23482397198677063
Batch 45/64 loss: -0.25436556339263916
Batch 46/64 loss: -0.2564878463745117
Batch 47/64 loss: -0.2204868197441101
Batch 48/64 loss: -0.22881096601486206
Batch 49/64 loss: -0.22100830078125
Batch 50/64 loss: -0.21671420335769653
Batch 51/64 loss: -0.21520984172821045
Batch 52/64 loss: -0.2174307107925415
Batch 53/64 loss: -0.20633846521377563
Batch 54/64 loss: -0.20661568641662598
Batch 55/64 loss: -0.24253353476524353
Batch 56/64 loss: -0.21959656476974487
Batch 57/64 loss: -0.2178698182106018
Batch 58/64 loss: -0.21686625480651855
Batch 59/64 loss: -0.20861274003982544
Batch 60/64 loss: -0.22356843948364258
Batch 61/64 loss: -0.21017956733703613
Batch 62/64 loss: -0.20729297399520874
Batch 63/64 loss: -0.22721761465072632
Batch 64/64 loss: -0.22253257036209106
Epoch 171  Train loss: -0.22463842256396424  Val loss: 0.017675416985737907
Epoch 172
-------------------------------
Batch 1/64 loss: -0.22483551502227783
Batch 2/64 loss: -0.25069260597229004
Batch 3/64 loss: -0.2324671745300293
Batch 4/64 loss: -0.22377163171768188
Batch 5/64 loss: -0.21376389265060425
Batch 6/64 loss: -0.234361469745636
Batch 7/64 loss: -0.23093730211257935
Batch 8/64 loss: -0.21799063682556152
Batch 9/64 loss: -0.19258379936218262
Batch 10/64 loss: -0.24241513013839722
Batch 11/64 loss: -0.2345256209373474
Batch 12/64 loss: -0.20636332035064697
Batch 13/64 loss: -0.23218587040901184
Batch 14/64 loss: -0.17837464809417725
Batch 15/64 loss: -0.2348918914794922
Batch 16/64 loss: -0.21232062578201294
Batch 17/64 loss: -0.23956972360610962
Batch 18/64 loss: -0.24517008662223816
Batch 19/64 loss: -0.2384537160396576
Batch 20/64 loss: -0.22280287742614746
Batch 21/64 loss: -0.24894797801971436
Batch 22/64 loss: -0.218786358833313
Batch 23/64 loss: -0.22969454526901245
Batch 24/64 loss: -0.16581523418426514
Batch 25/64 loss: -0.19465667009353638
Batch 26/64 loss: -0.2332327663898468
Batch 27/64 loss: -0.20694875717163086
Batch 28/64 loss: -0.21911942958831787
Batch 29/64 loss: -0.22065132856369019
Batch 30/64 loss: -0.2296028733253479
Batch 31/64 loss: -0.22296202182769775
Batch 32/64 loss: -0.22852349281311035
Batch 33/64 loss: -0.23048463463783264
Batch 34/64 loss: -0.19582360982894897
Batch 35/64 loss: -0.2358607053756714
Batch 36/64 loss: -0.24948304891586304
Batch 37/64 loss: -0.2031412124633789
Batch 38/64 loss: -0.2270163893699646
Batch 39/64 loss: -0.18896567821502686
Batch 40/64 loss: -0.21826034784317017
Batch 41/64 loss: -0.26593518257141113
Batch 42/64 loss: -0.23278504610061646
Batch 43/64 loss: -0.25413778424263
Batch 44/64 loss: -0.24674397706985474
Batch 45/64 loss: -0.23458224534988403
Batch 46/64 loss: -0.21233028173446655
Batch 47/64 loss: -0.2315281629562378
Batch 48/64 loss: -0.2059653401374817
Batch 49/64 loss: -0.2275611162185669
Batch 50/64 loss: -0.24227535724639893
Batch 51/64 loss: -0.2300207018852234
Batch 52/64 loss: -0.25509554147720337
Batch 53/64 loss: -0.22808164358139038
Batch 54/64 loss: -0.2396641969680786
Batch 55/64 loss: -0.20233392715454102
Batch 56/64 loss: -0.23395222425460815
Batch 57/64 loss: -0.23071742057800293
Batch 58/64 loss: -0.2348620891571045
Batch 59/64 loss: -0.21505022048950195
Batch 60/64 loss: -0.2103191614151001
Batch 61/64 loss: -0.24564820528030396
Batch 62/64 loss: -0.22975176572799683
Batch 63/64 loss: -0.23836451768875122
Batch 64/64 loss: -0.22135323286056519
Epoch 172  Train loss: -0.22572824557622273  Val loss: 0.018845570456121386
Epoch 173
-------------------------------
Batch 1/64 loss: -0.23307180404663086
Batch 2/64 loss: -0.21048450469970703
Batch 3/64 loss: -0.2328685224056244
Batch 4/64 loss: -0.2041454315185547
Batch 5/64 loss: -0.22809815406799316
Batch 6/64 loss: -0.26068562269210815
Batch 7/64 loss: -0.22945863008499146
Batch 8/64 loss: -0.24081972241401672
Batch 9/64 loss: -0.24963316321372986
Batch 10/64 loss: -0.19202709197998047
Batch 11/64 loss: -0.2478775978088379
Batch 12/64 loss: -0.23192840814590454
Batch 13/64 loss: -0.23102620244026184
Batch 14/64 loss: -0.23676061630249023
Batch 15/64 loss: -0.19331622123718262
Batch 16/64 loss: -0.2350674569606781
Batch 17/64 loss: -0.20544427633285522
Batch 18/64 loss: -0.18907558917999268
Batch 19/64 loss: -0.22843480110168457
Batch 20/64 loss: -0.2069370150566101
Batch 21/64 loss: -0.2352583408355713
Batch 22/64 loss: -0.24859601259231567
Batch 23/64 loss: -0.2444167137145996
Batch 24/64 loss: -0.2314985990524292
Batch 25/64 loss: -0.2432674765586853
Batch 26/64 loss: -0.22929298877716064
Batch 27/64 loss: -0.21805959939956665
Batch 28/64 loss: -0.23865550756454468
Batch 29/64 loss: -0.24205002188682556
Batch 30/64 loss: -0.26200103759765625
Batch 31/64 loss: -0.22748780250549316
Batch 32/64 loss: -0.1934826374053955
Batch 33/64 loss: -0.2595045566558838
Batch 34/64 loss: -0.2369759976863861
Batch 35/64 loss: -0.24203938245773315
Batch 36/64 loss: -0.2330528199672699
Batch 37/64 loss: -0.2524256110191345
Batch 38/64 loss: -0.2255169153213501
Batch 39/64 loss: -0.23696216940879822
Batch 40/64 loss: -0.22432565689086914
Batch 41/64 loss: -0.21412193775177002
Batch 42/64 loss: -0.22459226846694946
Batch 43/64 loss: -0.22885751724243164
Batch 44/64 loss: -0.25492244958877563
Batch 45/64 loss: -0.26429125666618347
Batch 46/64 loss: -0.221147358417511
Batch 47/64 loss: -0.24736329913139343
Batch 48/64 loss: -0.22179561853408813
Batch 49/64 loss: -0.19407367706298828
Batch 50/64 loss: -0.23137056827545166
Batch 51/64 loss: -0.21656841039657593
Batch 52/64 loss: -0.23071342706680298
Batch 53/64 loss: -0.22329163551330566
Batch 54/64 loss: -0.21943539381027222
Batch 55/64 loss: -0.19351279735565186
Batch 56/64 loss: -0.20578426122665405
Batch 57/64 loss: -0.24300968647003174
Batch 58/64 loss: -0.19383835792541504
Batch 59/64 loss: -0.19294726848602295
Batch 60/64 loss: -0.26037001609802246
Batch 61/64 loss: -0.19567209482192993
Batch 62/64 loss: -0.24870377779006958
Batch 63/64 loss: -0.25335144996643066
Batch 64/64 loss: -0.22904470562934875
Epoch 173  Train loss: -0.22844782331410576  Val loss: 0.017294719456807033
Epoch 174
-------------------------------
Batch 1/64 loss: -0.25537025928497314
Batch 2/64 loss: -0.22806602716445923
Batch 3/64 loss: -0.2789543867111206
Batch 4/64 loss: -0.23304325342178345
Batch 5/64 loss: -0.233261376619339
Batch 6/64 loss: -0.2546732425689697
Batch 7/64 loss: -0.22448372840881348
Batch 8/64 loss: -0.1906825304031372
Batch 9/64 loss: -0.25439175963401794
Batch 10/64 loss: -0.2544499635696411
Batch 11/64 loss: -0.22376924753189087
Batch 12/64 loss: -0.21231895685195923
Batch 13/64 loss: -0.2514849305152893
Batch 14/64 loss: -0.22350013256072998
Batch 15/64 loss: -0.24242550134658813
Batch 16/64 loss: -0.23442217707633972
Batch 17/64 loss: -0.23254024982452393
Batch 18/64 loss: -0.2587208151817322
Batch 19/64 loss: -0.23756954073905945
Batch 20/64 loss: -0.19939762353897095
Batch 21/64 loss: -0.18729186058044434
Batch 22/64 loss: -0.2235926389694214
Batch 23/64 loss: -0.21647393703460693
Batch 24/64 loss: -0.24059253931045532
Batch 25/64 loss: -0.2761722803115845
Batch 26/64 loss: -0.245334655046463
Batch 27/64 loss: -0.244813472032547
Batch 28/64 loss: -0.2529074549674988
Batch 29/64 loss: -0.19718068838119507
Batch 30/64 loss: -0.25359416007995605
Batch 31/64 loss: -0.22993826866149902
Batch 32/64 loss: -0.18788117170333862
Batch 33/64 loss: -0.21611982583999634
Batch 34/64 loss: -0.21456557512283325
Batch 35/64 loss: -0.2286442518234253
Batch 36/64 loss: -0.19723302125930786
Batch 37/64 loss: -0.19867533445358276
Batch 38/64 loss: -0.22725427150726318
Batch 39/64 loss: -0.27018874883651733
Batch 40/64 loss: -0.22089380025863647
Batch 41/64 loss: -0.19706547260284424
Batch 42/64 loss: -0.24117490649223328
Batch 43/64 loss: -0.23578310012817383
Batch 44/64 loss: -0.2148374319076538
Batch 45/64 loss: -0.2118040919303894
Batch 46/64 loss: -0.2269344925880432
Batch 47/64 loss: -0.24369767308235168
Batch 48/64 loss: -0.22002065181732178
Batch 49/64 loss: -0.248401939868927
Batch 50/64 loss: -0.19052892923355103
Batch 51/64 loss: -0.18636870384216309
Batch 52/64 loss: -0.23201453685760498
Batch 53/64 loss: -0.24631917476654053
Batch 54/64 loss: -0.23233121633529663
Batch 55/64 loss: -0.23122566938400269
Batch 56/64 loss: -0.22034531831741333
Batch 57/64 loss: -0.18170052766799927
Batch 58/64 loss: -0.24014702439308167
Batch 59/64 loss: -0.2547626793384552
Batch 60/64 loss: -0.2369307279586792
Batch 61/64 loss: -0.2570835053920746
Batch 62/64 loss: -0.20493322610855103
Batch 63/64 loss: -0.22719961404800415
Batch 64/64 loss: -0.1875227689743042
Epoch 174  Train loss: -0.22862943294001561  Val loss: 0.021914476586371353
Epoch 175
-------------------------------
Batch 1/64 loss: -0.22436195611953735
Batch 2/64 loss: -0.24979227781295776
Batch 3/64 loss: -0.23687323927879333
Batch 4/64 loss: -0.23909908533096313
Batch 5/64 loss: -0.2223305106163025
Batch 6/64 loss: -0.23963093757629395
Batch 7/64 loss: -0.26664987206459045
Batch 8/64 loss: -0.25890466570854187
Batch 9/64 loss: -0.206246018409729
Batch 10/64 loss: -0.22837579250335693
Batch 11/64 loss: -0.22175812721252441
Batch 12/64 loss: -0.22263717651367188
Batch 13/64 loss: -0.22812795639038086
Batch 14/64 loss: -0.23644614219665527
Batch 15/64 loss: -0.2248077392578125
Batch 16/64 loss: -0.21022284030914307
Batch 17/64 loss: -0.23595118522644043
Batch 18/64 loss: -0.21389007568359375
Batch 19/64 loss: -0.2422296404838562
Batch 20/64 loss: -0.2110888957977295
Batch 21/64 loss: -0.22826522588729858
Batch 22/64 loss: -0.22716760635375977
Batch 23/64 loss: -0.2556734085083008
Batch 24/64 loss: -0.20073115825653076
Batch 25/64 loss: -0.1865275502204895
Batch 26/64 loss: -0.21525853872299194
Batch 27/64 loss: -0.24533051252365112
Batch 28/64 loss: -0.23157578706741333
Batch 29/64 loss: -0.21794277429580688
Batch 30/64 loss: -0.24090582132339478
Batch 31/64 loss: -0.20265865325927734
Batch 32/64 loss: -0.2007594108581543
Batch 33/64 loss: -0.2621220648288727
Batch 34/64 loss: -0.21211540699005127
Batch 35/64 loss: -0.2097550630569458
Batch 36/64 loss: -0.2313162088394165
Batch 37/64 loss: -0.25503596663475037
Batch 38/64 loss: -0.2351643443107605
Batch 39/64 loss: -0.24463501572608948
Batch 40/64 loss: -0.2011154294013977
Batch 41/64 loss: -0.21488326787948608
Batch 42/64 loss: -0.1911298632621765
Batch 43/64 loss: -0.23749753832817078
Batch 44/64 loss: -0.22506791353225708
Batch 45/64 loss: -0.2091362476348877
Batch 46/64 loss: -0.24085742235183716
Batch 47/64 loss: -0.22440743446350098
Batch 48/64 loss: -0.1887146234512329
Batch 49/64 loss: -0.208856463432312
Batch 50/64 loss: -0.23254618048667908
Batch 51/64 loss: -0.24124228954315186
Batch 52/64 loss: -0.24850142002105713
Batch 53/64 loss: -0.19174081087112427
Batch 54/64 loss: -0.2283691167831421
Batch 55/64 loss: -0.23285937309265137
Batch 56/64 loss: -0.24228262901306152
Batch 57/64 loss: -0.22865521907806396
Batch 58/64 loss: -0.2387908697128296
Batch 59/64 loss: -0.2438642978668213
Batch 60/64 loss: -0.2071171998977661
Batch 61/64 loss: -0.23930451273918152
Batch 62/64 loss: -0.22524797916412354
Batch 63/64 loss: -0.24678611755371094
Batch 64/64 loss: -0.22223514318466187
Epoch 175  Train loss: -0.22710612124087765  Val loss: 0.01974310498057362
Epoch 176
-------------------------------
Batch 1/64 loss: -0.2439875304698944
Batch 2/64 loss: -0.2478243112564087
Batch 3/64 loss: -0.24405056238174438
Batch 4/64 loss: -0.21946591138839722
Batch 5/64 loss: -0.24889349937438965
Batch 6/64 loss: -0.2604033350944519
Batch 7/64 loss: -0.25874030590057373
Batch 8/64 loss: -0.2491176426410675
Batch 9/64 loss: -0.2583855092525482
Batch 10/64 loss: -0.23034417629241943
Batch 11/64 loss: -0.2206830382347107
Batch 12/64 loss: -0.2437124252319336
Batch 13/64 loss: -0.2268429398536682
Batch 14/64 loss: -0.1955488920211792
Batch 15/64 loss: -0.18816256523132324
Batch 16/64 loss: -0.24049600958824158
Batch 17/64 loss: -0.21556657552719116
Batch 18/64 loss: -0.20648294687271118
Batch 19/64 loss: -0.25773173570632935
Batch 20/64 loss: -0.20029282569885254
Batch 21/64 loss: -0.2591521441936493
Batch 22/64 loss: -0.2310428023338318
Batch 23/64 loss: -0.23515188694000244
Batch 24/64 loss: -0.23278528451919556
Batch 25/64 loss: -0.22561532258987427
Batch 26/64 loss: -0.20949453115463257
Batch 27/64 loss: -0.2306414246559143
Batch 28/64 loss: -0.2384326457977295
Batch 29/64 loss: -0.24315786361694336
Batch 30/64 loss: -0.22810906171798706
Batch 31/64 loss: -0.24181002378463745
Batch 32/64 loss: -0.23072978854179382
Batch 33/64 loss: -0.2132267951965332
Batch 34/64 loss: -0.22224581241607666
Batch 35/64 loss: -0.22576582431793213
Batch 36/64 loss: -0.22274017333984375
Batch 37/64 loss: -0.23808100819587708
Batch 38/64 loss: -0.2332368791103363
Batch 39/64 loss: -0.21577590703964233
Batch 40/64 loss: -0.20050948858261108
Batch 41/64 loss: -0.19108706712722778
Batch 42/64 loss: -0.25873127579689026
Batch 43/64 loss: -0.2527930438518524
Batch 44/64 loss: -0.2626795768737793
Batch 45/64 loss: -0.19725185632705688
Batch 46/64 loss: -0.21486997604370117
Batch 47/64 loss: -0.19849729537963867
Batch 48/64 loss: -0.18845033645629883
Batch 49/64 loss: -0.23585733771324158
Batch 50/64 loss: -0.23510679602622986
Batch 51/64 loss: -0.25954514741897583
Batch 52/64 loss: -0.21711832284927368
Batch 53/64 loss: -0.23123019933700562
Batch 54/64 loss: -0.21081280708312988
Batch 55/64 loss: -0.23932397365570068
Batch 56/64 loss: -0.2185211181640625
Batch 57/64 loss: -0.23446470499038696
Batch 58/64 loss: -0.2100570797920227
Batch 59/64 loss: -0.20564734935760498
Batch 60/64 loss: -0.25397205352783203
Batch 61/64 loss: -0.21313995122909546
Batch 62/64 loss: -0.21580785512924194
Batch 63/64 loss: -0.20994865894317627
Batch 64/64 loss: -0.2184765338897705
Epoch 176  Train loss: -0.22828562493417778  Val loss: 0.01911018763211175
Epoch 177
-------------------------------
Batch 1/64 loss: -0.21655666828155518
Batch 2/64 loss: -0.22470957040786743
Batch 3/64 loss: -0.26065120100975037
Batch 4/64 loss: -0.22351443767547607
Batch 5/64 loss: -0.22305428981781006
Batch 6/64 loss: -0.24862122535705566
Batch 7/64 loss: -0.23028934001922607
Batch 8/64 loss: -0.2413957715034485
Batch 9/64 loss: -0.24000069499015808
Batch 10/64 loss: -0.2520153224468231
Batch 11/64 loss: -0.1682753562927246
Batch 12/64 loss: -0.23360180854797363
Batch 13/64 loss: -0.24318912625312805
Batch 14/64 loss: -0.24304485321044922
Batch 15/64 loss: -0.25297069549560547
Batch 16/64 loss: -0.2616865932941437
Batch 17/64 loss: -0.24171856045722961
Batch 18/64 loss: -0.23692405223846436
Batch 19/64 loss: -0.199398934841156
Batch 20/64 loss: -0.236331045627594
Batch 21/64 loss: -0.24290606379508972
Batch 22/64 loss: -0.2112331986427307
Batch 23/64 loss: -0.25225019454956055
Batch 24/64 loss: -0.24098405241966248
Batch 25/64 loss: -0.23796787858009338
Batch 26/64 loss: -0.24188607931137085
Batch 27/64 loss: -0.21101313829421997
Batch 28/64 loss: -0.22701823711395264
Batch 29/64 loss: -0.23183730244636536
Batch 30/64 loss: -0.2587050795555115
Batch 31/64 loss: -0.22506511211395264
Batch 32/64 loss: -0.20732033252716064
Batch 33/64 loss: -0.25251591205596924
Batch 34/64 loss: -0.21275871992111206
Batch 35/64 loss: -0.2337442934513092
Batch 36/64 loss: -0.2103646993637085
Batch 37/64 loss: -0.23114114999771118
Batch 38/64 loss: -0.2400590181350708
Batch 39/64 loss: -0.19641536474227905
Batch 40/64 loss: -0.21630120277404785
Batch 41/64 loss: -0.24461835622787476
Batch 42/64 loss: -0.22079592943191528
Batch 43/64 loss: -0.2097063660621643
Batch 44/64 loss: -0.23219263553619385
Batch 45/64 loss: -0.22602075338363647
Batch 46/64 loss: -0.20758873224258423
Batch 47/64 loss: -0.22989612817764282
Batch 48/64 loss: -0.24347931146621704
Batch 49/64 loss: -0.22872769832611084
Batch 50/64 loss: -0.22919440269470215
Batch 51/64 loss: -0.22523105144500732
Batch 52/64 loss: -0.2328493893146515
Batch 53/64 loss: -0.21400630474090576
Batch 54/64 loss: -0.23231276869773865
Batch 55/64 loss: -0.2375493049621582
Batch 56/64 loss: -0.20960140228271484
Batch 57/64 loss: -0.22092211246490479
Batch 58/64 loss: -0.22912579774856567
Batch 59/64 loss: -0.1774977445602417
Batch 60/64 loss: -0.21036362648010254
Batch 61/64 loss: -0.2510676681995392
Batch 62/64 loss: -0.2334471046924591
Batch 63/64 loss: -0.24527981877326965
Batch 64/64 loss: -0.20916390419006348
Epoch 177  Train loss: -0.22914170844882142  Val loss: 0.017254060281511024
Epoch 178
-------------------------------
Batch 1/64 loss: -0.22222208976745605
Batch 2/64 loss: -0.2106495499610901
Batch 3/64 loss: -0.2403545379638672
Batch 4/64 loss: -0.2258819341659546
Batch 5/64 loss: -0.21547585725784302
Batch 6/64 loss: -0.24389061331748962
Batch 7/64 loss: -0.24559971690177917
Batch 8/64 loss: -0.2696729600429535
Batch 9/64 loss: -0.2317640483379364
Batch 10/64 loss: -0.2599726617336273
Batch 11/64 loss: -0.20707571506500244
Batch 12/64 loss: -0.22188901901245117
Batch 13/64 loss: -0.24817097187042236
Batch 14/64 loss: -0.22375530004501343
Batch 15/64 loss: -0.21345925331115723
Batch 16/64 loss: -0.24067366123199463
Batch 17/64 loss: -0.17473053932189941
Batch 18/64 loss: -0.24510544538497925
Batch 19/64 loss: -0.2521746754646301
Batch 20/64 loss: -0.2225663661956787
Batch 21/64 loss: -0.26146501302719116
Batch 22/64 loss: -0.24415373802185059
Batch 23/64 loss: -0.2294589877128601
Batch 24/64 loss: -0.2261871099472046
Batch 25/64 loss: -0.25472331047058105
Batch 26/64 loss: -0.23551762104034424
Batch 27/64 loss: -0.22102445363998413
Batch 28/64 loss: -0.25858595967292786
Batch 29/64 loss: -0.18787747621536255
Batch 30/64 loss: -0.24376612901687622
Batch 31/64 loss: -0.20822572708129883
Batch 32/64 loss: -0.1945641040802002
Batch 33/64 loss: -0.2632256746292114
Batch 34/64 loss: -0.24286743998527527
Batch 35/64 loss: -0.23248133063316345
Batch 36/64 loss: -0.22965258359909058
Batch 37/64 loss: -0.19895309209823608
Batch 38/64 loss: -0.21088409423828125
Batch 39/64 loss: -0.23620784282684326
Batch 40/64 loss: -0.2234957218170166
Batch 41/64 loss: -0.2475007176399231
Batch 42/64 loss: -0.24572205543518066
Batch 43/64 loss: -0.2296498715877533
Batch 44/64 loss: -0.23173749446868896
Batch 45/64 loss: -0.25254127383232117
Batch 46/64 loss: -0.21761173009872437
Batch 47/64 loss: -0.23732292652130127
Batch 48/64 loss: -0.1933695673942566
Batch 49/64 loss: -0.21610236167907715
Batch 50/64 loss: -0.2316245436668396
Batch 51/64 loss: -0.23863881826400757
Batch 52/64 loss: -0.22693544626235962
Batch 53/64 loss: -0.25170114636421204
Batch 54/64 loss: -0.20552819967269897
Batch 55/64 loss: -0.21792399883270264
Batch 56/64 loss: -0.2076992392539978
Batch 57/64 loss: -0.23092466592788696
Batch 58/64 loss: -0.21167480945587158
Batch 59/64 loss: -0.2247169017791748
Batch 60/64 loss: -0.24613118171691895
Batch 61/64 loss: -0.24733701348304749
Batch 62/64 loss: -0.2205643653869629
Batch 63/64 loss: -0.2397511899471283
Batch 64/64 loss: -0.2536684572696686
Epoch 178  Train loss: -0.23029589314086765  Val loss: 0.02005731752238323
Epoch 179
-------------------------------
Batch 1/64 loss: -0.25728756189346313
Batch 2/64 loss: -0.27089858055114746
Batch 3/64 loss: -0.2456808090209961
Batch 4/64 loss: -0.22609776258468628
Batch 5/64 loss: -0.20987635850906372
Batch 6/64 loss: -0.2675568461418152
Batch 7/64 loss: -0.24899986386299133
Batch 8/64 loss: -0.2280065417289734
Batch 9/64 loss: -0.20109307765960693
Batch 10/64 loss: -0.23621457815170288
Batch 11/64 loss: -0.23808073997497559
Batch 12/64 loss: -0.2585987150669098
Batch 13/64 loss: -0.26493871212005615
Batch 14/64 loss: -0.24820387363433838
Batch 15/64 loss: -0.2331373691558838
Batch 16/64 loss: -0.25159770250320435
Batch 17/64 loss: -0.23553121089935303
Batch 18/64 loss: -0.22578340768814087
Batch 19/64 loss: -0.24159109592437744
Batch 20/64 loss: -0.24517756700515747
Batch 21/64 loss: -0.22445142269134521
Batch 22/64 loss: -0.256795197725296
Batch 23/64 loss: -0.24237874150276184
Batch 24/64 loss: -0.23942816257476807
Batch 25/64 loss: -0.19379520416259766
Batch 26/64 loss: -0.2241995930671692
Batch 27/64 loss: -0.25208714604377747
Batch 28/64 loss: -0.24655571579933167
Batch 29/64 loss: -0.23806297779083252
Batch 30/64 loss: -0.2518830895423889
Batch 31/64 loss: -0.19984114170074463
Batch 32/64 loss: -0.21602028608322144
Batch 33/64 loss: -0.21768778562545776
Batch 34/64 loss: -0.2263582944869995
Batch 35/64 loss: -0.214255690574646
Batch 36/64 loss: -0.24021118879318237
Batch 37/64 loss: -0.23034241795539856
Batch 38/64 loss: -0.1999022364616394
Batch 39/64 loss: -0.24641942977905273
Batch 40/64 loss: -0.2330065369606018
Batch 41/64 loss: -0.2536022365093231
Batch 42/64 loss: -0.21681874990463257
Batch 43/64 loss: -0.20596402883529663
Batch 44/64 loss: -0.21197813749313354
Batch 45/64 loss: -0.26133352518081665
Batch 46/64 loss: -0.2333279848098755
Batch 47/64 loss: -0.24172908067703247
Batch 48/64 loss: -0.20442497730255127
Batch 49/64 loss: -0.20349299907684326
Batch 50/64 loss: -0.23549699783325195
Batch 51/64 loss: -0.23738634586334229
Batch 52/64 loss: -0.2420332431793213
Batch 53/64 loss: -0.23195955157279968
Batch 54/64 loss: -0.21136236190795898
Batch 55/64 loss: -0.24065902829170227
Batch 56/64 loss: -0.2104821801185608
Batch 57/64 loss: -0.20942604541778564
Batch 58/64 loss: -0.24786126613616943
Batch 59/64 loss: -0.23934495449066162
Batch 60/64 loss: -0.2415716052055359
Batch 61/64 loss: -0.22817271947860718
Batch 62/64 loss: -0.2218008041381836
Batch 63/64 loss: -0.24375468492507935
Batch 64/64 loss: -0.21803581714630127
Epoch 179  Train loss: -0.233185019680098  Val loss: 0.02107998063064523
Epoch 180
-------------------------------
Batch 1/64 loss: -0.2232065200805664
Batch 2/64 loss: -0.23040568828582764
Batch 3/64 loss: -0.25046804547309875
Batch 4/64 loss: -0.22101938724517822
Batch 5/64 loss: -0.22878730297088623
Batch 6/64 loss: -0.22704559564590454
Batch 7/64 loss: -0.23163315653800964
Batch 8/64 loss: -0.24684569239616394
Batch 9/64 loss: -0.24475929141044617
Batch 10/64 loss: -0.22250032424926758
Batch 11/64 loss: -0.2306649088859558
Batch 12/64 loss: -0.25439321994781494
Batch 13/64 loss: -0.27155032753944397
Batch 14/64 loss: -0.24160811305046082
Batch 15/64 loss: -0.20302188396453857
Batch 16/64 loss: -0.22658836841583252
Batch 17/64 loss: -0.24746519327163696
Batch 18/64 loss: -0.2474568486213684
Batch 19/64 loss: -0.24216079711914062
Batch 20/64 loss: -0.24416881799697876
Batch 21/64 loss: -0.21632826328277588
Batch 22/64 loss: -0.24570119380950928
Batch 23/64 loss: -0.20140159130096436
Batch 24/64 loss: -0.23468375205993652
Batch 25/64 loss: -0.2298608422279358
Batch 26/64 loss: -0.23567163944244385
Batch 27/64 loss: -0.22777479887008667
Batch 28/64 loss: -0.24048689007759094
Batch 29/64 loss: -0.23714670538902283
Batch 30/64 loss: -0.2565338611602783
Batch 31/64 loss: -0.23110923171043396
Batch 32/64 loss: -0.25281548500061035
Batch 33/64 loss: -0.26229849457740784
Batch 34/64 loss: -0.2262020707130432
Batch 35/64 loss: -0.24285459518432617
Batch 36/64 loss: -0.23170959949493408
Batch 37/64 loss: -0.21991360187530518
Batch 38/64 loss: -0.23149678111076355
Batch 39/64 loss: -0.2363533079624176
Batch 40/64 loss: -0.21674400568008423
Batch 41/64 loss: -0.2599782943725586
Batch 42/64 loss: -0.23092222213745117
Batch 43/64 loss: -0.2499198317527771
Batch 44/64 loss: -0.25621408224105835
Batch 45/64 loss: -0.19249296188354492
Batch 46/64 loss: -0.25779294967651367
Batch 47/64 loss: -0.20799380540847778
Batch 48/64 loss: -0.243547260761261
Batch 49/64 loss: -0.23532643914222717
Batch 50/64 loss: -0.21190685033798218
Batch 51/64 loss: -0.2377648949623108
Batch 52/64 loss: -0.26329535245895386
Batch 53/64 loss: -0.2439616322517395
Batch 54/64 loss: -0.24590373039245605
Batch 55/64 loss: -0.24947142601013184
Batch 56/64 loss: -0.23766744136810303
Batch 57/64 loss: -0.2150195837020874
Batch 58/64 loss: -0.22990161180496216
Batch 59/64 loss: -0.2502504289150238
Batch 60/64 loss: -0.2219783067703247
Batch 61/64 loss: -0.21399176120758057
Batch 62/64 loss: -0.2213684320449829
Batch 63/64 loss: -0.20737498998641968
Batch 64/64 loss: -0.21829664707183838
Epoch 180  Train loss: -0.23467612500284232  Val loss: 0.017343881613610126
Epoch 181
-------------------------------
Batch 1/64 loss: -0.26053252816200256
Batch 2/64 loss: -0.22968819737434387
Batch 3/64 loss: -0.20023155212402344
Batch 4/64 loss: -0.24245613813400269
Batch 5/64 loss: -0.1842343807220459
Batch 6/64 loss: -0.27600452303886414
Batch 7/64 loss: -0.23037439584732056
Batch 8/64 loss: -0.25296294689178467
Batch 9/64 loss: -0.1908387541770935
Batch 10/64 loss: -0.2377380132675171
Batch 11/64 loss: -0.24237948656082153
Batch 12/64 loss: -0.21958625316619873
Batch 13/64 loss: -0.2574610710144043
Batch 14/64 loss: -0.26217666268348694
Batch 15/64 loss: -0.22163033485412598
Batch 16/64 loss: -0.21005749702453613
Batch 17/64 loss: -0.25146234035491943
Batch 18/64 loss: -0.2543283998966217
Batch 19/64 loss: -0.23142790794372559
Batch 20/64 loss: -0.2251339554786682
Batch 21/64 loss: -0.22839096188545227
Batch 22/64 loss: -0.18474221229553223
Batch 23/64 loss: -0.2327142357826233
Batch 24/64 loss: -0.2420055866241455
Batch 25/64 loss: -0.2572746276855469
Batch 26/64 loss: -0.2147693634033203
Batch 27/64 loss: -0.24299436807632446
Batch 28/64 loss: -0.21067380905151367
Batch 29/64 loss: -0.22886312007904053
Batch 30/64 loss: -0.2603933811187744
Batch 31/64 loss: -0.2285846471786499
Batch 32/64 loss: -0.26314777135849
Batch 33/64 loss: -0.2381514310836792
Batch 34/64 loss: -0.2429960072040558
Batch 35/64 loss: -0.2094050645828247
Batch 36/64 loss: -0.21706295013427734
Batch 37/64 loss: -0.2482178807258606
Batch 38/64 loss: -0.2587316632270813
Batch 39/64 loss: -0.2789894938468933
Batch 40/64 loss: -0.21336454153060913
Batch 41/64 loss: -0.2381991147994995
Batch 42/64 loss: -0.23703354597091675
Batch 43/64 loss: -0.2447892129421234
Batch 44/64 loss: -0.24719282984733582
Batch 45/64 loss: -0.20034080743789673
Batch 46/64 loss: -0.2544488310813904
Batch 47/64 loss: -0.237315833568573
Batch 48/64 loss: -0.23442557454109192
Batch 49/64 loss: -0.2183307409286499
Batch 50/64 loss: -0.23572608828544617
Batch 51/64 loss: -0.24482738971710205
Batch 52/64 loss: -0.21668463945388794
Batch 53/64 loss: -0.23207855224609375
Batch 54/64 loss: -0.21766334772109985
Batch 55/64 loss: -0.2054506540298462
Batch 56/64 loss: -0.23826193809509277
Batch 57/64 loss: -0.25591927766799927
Batch 58/64 loss: -0.23250389099121094
Batch 59/64 loss: -0.24826568365097046
Batch 60/64 loss: -0.228179931640625
Batch 61/64 loss: -0.23424765467643738
Batch 62/64 loss: -0.2503105700016022
Batch 63/64 loss: -0.19892752170562744
Batch 64/64 loss: -0.26126182079315186
Epoch 181  Train loss: -0.23418428944606406  Val loss: 0.017417673597630766
Epoch 182
-------------------------------
Batch 1/64 loss: -0.22916913032531738
Batch 2/64 loss: -0.252642959356308
Batch 3/64 loss: -0.24326467514038086
Batch 4/64 loss: -0.21569228172302246
Batch 5/64 loss: -0.2421002984046936
Batch 6/64 loss: -0.24061155319213867
Batch 7/64 loss: -0.24573862552642822
Batch 8/64 loss: -0.247394859790802
Batch 9/64 loss: -0.20988309383392334
Batch 10/64 loss: -0.2273566722869873
Batch 11/64 loss: -0.25169599056243896
Batch 12/64 loss: -0.24404409527778625
Batch 13/64 loss: -0.22697460651397705
Batch 14/64 loss: -0.23602324724197388
Batch 15/64 loss: -0.2588951289653778
Batch 16/64 loss: -0.22801679372787476
Batch 17/64 loss: -0.2214532494544983
Batch 18/64 loss: -0.21164101362228394
Batch 19/64 loss: -0.226997971534729
Batch 20/64 loss: -0.2584589123725891
Batch 21/64 loss: -0.23347777128219604
Batch 22/64 loss: -0.2403673529624939
Batch 23/64 loss: -0.2394038736820221
Batch 24/64 loss: -0.24159720540046692
Batch 25/64 loss: -0.189855694770813
Batch 26/64 loss: -0.23292046785354614
Batch 27/64 loss: -0.2066975235939026
Batch 28/64 loss: -0.25334975123405457
Batch 29/64 loss: -0.25655701756477356
Batch 30/64 loss: -0.23295366764068604
Batch 31/64 loss: -0.20250314474105835
Batch 32/64 loss: -0.22230654954910278
Batch 33/64 loss: -0.27055594325065613
Batch 34/64 loss: -0.23775586485862732
Batch 35/64 loss: -0.23985671997070312
Batch 36/64 loss: -0.26217901706695557
Batch 37/64 loss: -0.21988099813461304
Batch 38/64 loss: -0.2430330216884613
Batch 39/64 loss: -0.2713867127895355
Batch 40/64 loss: -0.26836302876472473
Batch 41/64 loss: -0.24043861031532288
Batch 42/64 loss: -0.20796436071395874
Batch 43/64 loss: -0.205560564994812
Batch 44/64 loss: -0.2230650782585144
Batch 45/64 loss: -0.253929078578949
Batch 46/64 loss: -0.21529150009155273
Batch 47/64 loss: -0.20865249633789062
Batch 48/64 loss: -0.19530129432678223
Batch 49/64 loss: -0.20818167924880981
Batch 50/64 loss: -0.23637041449546814
Batch 51/64 loss: -0.24226349592208862
Batch 52/64 loss: -0.24386614561080933
Batch 53/64 loss: -0.23017394542694092
Batch 54/64 loss: -0.2315777838230133
Batch 55/64 loss: -0.2115824818611145
Batch 56/64 loss: -0.23442378640174866
Batch 57/64 loss: -0.20450544357299805
Batch 58/64 loss: -0.21922928094863892
Batch 59/64 loss: -0.2168726921081543
Batch 60/64 loss: -0.2649732232093811
Batch 61/64 loss: -0.23547953367233276
Batch 62/64 loss: -0.2461378574371338
Batch 63/64 loss: -0.2335537075996399
Batch 64/64 loss: -0.2550307512283325
Epoch 182  Train loss: -0.23347018045537613  Val loss: 0.01911437675305658
Epoch 183
-------------------------------
Batch 1/64 loss: -0.27546003460884094
Batch 2/64 loss: -0.24179601669311523
Batch 3/64 loss: -0.23250320553779602
Batch 4/64 loss: -0.2584192156791687
Batch 5/64 loss: -0.23503226041793823
Batch 6/64 loss: -0.22241759300231934
Batch 7/64 loss: -0.263624370098114
Batch 8/64 loss: -0.23785054683685303
Batch 9/64 loss: -0.24634668231010437
Batch 10/64 loss: -0.26128244400024414
Batch 11/64 loss: -0.23567146062850952
Batch 12/64 loss: -0.2025914192199707
Batch 13/64 loss: -0.2454291582107544
Batch 14/64 loss: -0.2194756269454956
Batch 15/64 loss: -0.22885984182357788
Batch 16/64 loss: -0.23279452323913574
Batch 17/64 loss: -0.2469174861907959
Batch 18/64 loss: -0.21582764387130737
Batch 19/64 loss: -0.24391993880271912
Batch 20/64 loss: -0.2538062632083893
Batch 21/64 loss: -0.24843689799308777
Batch 22/64 loss: -0.25844505429267883
Batch 23/64 loss: -0.243747740983963
Batch 24/64 loss: -0.21899831295013428
Batch 25/64 loss: -0.23827385902404785
Batch 26/64 loss: -0.23986613750457764
Batch 27/64 loss: -0.18894994258880615
Batch 28/64 loss: -0.22302085161209106
Batch 29/64 loss: -0.23427724838256836
Batch 30/64 loss: -0.25154978036880493
Batch 31/64 loss: -0.25954586267471313
Batch 32/64 loss: -0.2305299937725067
Batch 33/64 loss: -0.20725667476654053
Batch 34/64 loss: -0.23385095596313477
Batch 35/64 loss: -0.24778205156326294
Batch 36/64 loss: -0.24049901962280273
Batch 37/64 loss: -0.21030092239379883
Batch 38/64 loss: -0.2546287477016449
Batch 39/64 loss: -0.2415752410888672
Batch 40/64 loss: -0.26396986842155457
Batch 41/64 loss: -0.24102389812469482
Batch 42/64 loss: -0.2558767795562744
Batch 43/64 loss: -0.23483934998512268
Batch 44/64 loss: -0.24825316667556763
Batch 45/64 loss: -0.22116214036941528
Batch 46/64 loss: -0.23958319425582886
Batch 47/64 loss: -0.26589956879615784
Batch 48/64 loss: -0.22506964206695557
Batch 49/64 loss: -0.2048245668411255
Batch 50/64 loss: -0.24481311440467834
Batch 51/64 loss: -0.2361346185207367
Batch 52/64 loss: -0.25920921564102173
Batch 53/64 loss: -0.22609513998031616
Batch 54/64 loss: -0.25294065475463867
Batch 55/64 loss: -0.24776184558868408
Batch 56/64 loss: -0.22758054733276367
Batch 57/64 loss: -0.2048981785774231
Batch 58/64 loss: -0.24511688947677612
Batch 59/64 loss: -0.23250257968902588
Batch 60/64 loss: -0.21966516971588135
Batch 61/64 loss: -0.23882201313972473
Batch 62/64 loss: -0.24408560991287231
Batch 63/64 loss: -0.24515485763549805
Batch 64/64 loss: -0.18693572282791138
Epoch 183  Train loss: -0.23744384985344083  Val loss: 0.019103843117087977
Epoch 184
-------------------------------
Batch 1/64 loss: -0.24800032377243042
Batch 2/64 loss: -0.19535815715789795
Batch 3/64 loss: -0.22825634479522705
Batch 4/64 loss: -0.23353448510169983
Batch 5/64 loss: -0.21099424362182617
Batch 6/64 loss: -0.22734490036964417
Batch 7/64 loss: -0.23116740584373474
Batch 8/64 loss: -0.2511994242668152
Batch 9/64 loss: -0.2590690851211548
Batch 10/64 loss: -0.24486100673675537
Batch 11/64 loss: -0.22803455591201782
Batch 12/64 loss: -0.25173431634902954
Batch 13/64 loss: -0.26588600873947144
Batch 14/64 loss: -0.233531653881073
Batch 15/64 loss: -0.2588271200656891
Batch 16/64 loss: -0.2598147988319397
Batch 17/64 loss: -0.2537457048892975
Batch 18/64 loss: -0.2714715003967285
Batch 19/64 loss: -0.25040769577026367
Batch 20/64 loss: -0.21883714199066162
Batch 21/64 loss: -0.2327401041984558
Batch 22/64 loss: -0.21042901277542114
Batch 23/64 loss: -0.23866236209869385
Batch 24/64 loss: -0.2505587339401245
Batch 25/64 loss: -0.24960267543792725
Batch 26/64 loss: -0.2243807315826416
Batch 27/64 loss: -0.25242161750793457
Batch 28/64 loss: -0.20954090356826782
Batch 29/64 loss: -0.2372235357761383
Batch 30/64 loss: -0.23134583234786987
Batch 31/64 loss: -0.21542173624038696
Batch 32/64 loss: -0.23715245723724365
Batch 33/64 loss: -0.24858498573303223
Batch 34/64 loss: -0.24933284521102905
Batch 35/64 loss: -0.24196910858154297
Batch 36/64 loss: -0.2479531168937683
Batch 37/64 loss: -0.24071279168128967
Batch 38/64 loss: -0.18675440549850464
Batch 39/64 loss: -0.24884194135665894
Batch 40/64 loss: -0.21197277307510376
Batch 41/64 loss: -0.243763267993927
Batch 42/64 loss: -0.2632344961166382
Batch 43/64 loss: -0.21192944049835205
Batch 44/64 loss: -0.2231842279434204
Batch 45/64 loss: -0.2675936222076416
Batch 46/64 loss: -0.22497880458831787
Batch 47/64 loss: -0.22327810525894165
Batch 48/64 loss: -0.2561708688735962
Batch 49/64 loss: -0.2547876834869385
Batch 50/64 loss: -0.23496660590171814
Batch 51/64 loss: -0.22534239292144775
Batch 52/64 loss: -0.22858095169067383
Batch 53/64 loss: -0.238385409116745
Batch 54/64 loss: -0.23618775606155396
Batch 55/64 loss: -0.24750345945358276
Batch 56/64 loss: -0.2503780722618103
Batch 57/64 loss: -0.17368286848068237
Batch 58/64 loss: -0.2500458359718323
Batch 59/64 loss: -0.21660274267196655
Batch 60/64 loss: -0.2451532483100891
Batch 61/64 loss: -0.2301883101463318
Batch 62/64 loss: -0.2215876579284668
Batch 63/64 loss: -0.22222602367401123
Batch 64/64 loss: -0.2342204451560974
Epoch 184  Train loss: -0.2361269761534298  Val loss: 0.02342266766066404
Epoch 185
-------------------------------
Batch 1/64 loss: -0.2551456689834595
Batch 2/64 loss: -0.26662757992744446
Batch 3/64 loss: -0.17421185970306396
Batch 4/64 loss: -0.19888836145401
Batch 5/64 loss: -0.25337356328964233
Batch 6/64 loss: -0.23545625805854797
Batch 7/64 loss: -0.224531888961792
Batch 8/64 loss: -0.218622624874115
Batch 9/64 loss: -0.21595853567123413
Batch 10/64 loss: -0.24575558304786682
Batch 11/64 loss: -0.22116529941558838
Batch 12/64 loss: -0.2199324369430542
Batch 13/64 loss: -0.23102021217346191
Batch 14/64 loss: -0.22839534282684326
Batch 15/64 loss: -0.2337862253189087
Batch 16/64 loss: -0.2625906467437744
Batch 17/64 loss: -0.2528890371322632
Batch 18/64 loss: -0.27700287103652954
Batch 19/64 loss: -0.2315061390399933
Batch 20/64 loss: -0.2405940592288971
Batch 21/64 loss: -0.21786713600158691
Batch 22/64 loss: -0.2296343743801117
Batch 23/64 loss: -0.2424808144569397
Batch 24/64 loss: -0.2418426275253296
Batch 25/64 loss: -0.22801339626312256
Batch 26/64 loss: -0.22388958930969238
Batch 27/64 loss: -0.24054467678070068
Batch 28/64 loss: -0.258205384016037
Batch 29/64 loss: -0.22615092992782593
Batch 30/64 loss: -0.20849013328552246
Batch 31/64 loss: -0.26876774430274963
Batch 32/64 loss: -0.23530548810958862
Batch 33/64 loss: -0.22856125235557556
Batch 34/64 loss: -0.24880409240722656
Batch 35/64 loss: -0.21485477685928345
Batch 36/64 loss: -0.2307482361793518
Batch 37/64 loss: -0.22476071119308472
Batch 38/64 loss: -0.2620251178741455
Batch 39/64 loss: -0.24119389057159424
Batch 40/64 loss: -0.2070218324661255
Batch 41/64 loss: -0.22998061776161194
Batch 42/64 loss: -0.2589529752731323
Batch 43/64 loss: -0.23398259282112122
Batch 44/64 loss: -0.25912371277809143
Batch 45/64 loss: -0.1981133222579956
Batch 46/64 loss: -0.23746439814567566
Batch 47/64 loss: -0.2261197566986084
Batch 48/64 loss: -0.24054032564163208
Batch 49/64 loss: -0.25756192207336426
Batch 50/64 loss: -0.2752559781074524
Batch 51/64 loss: -0.24118024110794067
Batch 52/64 loss: -0.26498937606811523
Batch 53/64 loss: -0.23762735724449158
Batch 54/64 loss: -0.23271620273590088
Batch 55/64 loss: -0.26306772232055664
Batch 56/64 loss: -0.26221466064453125
Batch 57/64 loss: -0.2675313949584961
Batch 58/64 loss: -0.25933292508125305
Batch 59/64 loss: -0.2535291314125061
Batch 60/64 loss: -0.2201392650604248
Batch 61/64 loss: -0.240371972322464
Batch 62/64 loss: -0.24069178104400635
Batch 63/64 loss: -0.23429134488105774
Batch 64/64 loss: -0.25270408391952515
Epoch 185  Train loss: -0.2382889950976652  Val loss: 0.017948968303981926
Epoch 186
-------------------------------
Batch 1/64 loss: -0.26487457752227783
Batch 2/64 loss: -0.1763792634010315
Batch 3/64 loss: -0.2551305294036865
Batch 4/64 loss: -0.260814905166626
Batch 5/64 loss: -0.24107056856155396
Batch 6/64 loss: -0.24443423748016357
Batch 7/64 loss: -0.26307418942451477
Batch 8/64 loss: -0.24895769357681274
Batch 9/64 loss: -0.22057980298995972
Batch 10/64 loss: -0.2649434208869934
Batch 11/64 loss: -0.2782573699951172
Batch 12/64 loss: -0.22865420579910278
Batch 13/64 loss: -0.22678744792938232
Batch 14/64 loss: -0.2346908450126648
Batch 15/64 loss: -0.26531893014907837
Batch 16/64 loss: -0.2666730284690857
Batch 17/64 loss: -0.21930432319641113
Batch 18/64 loss: -0.2671777009963989
Batch 19/64 loss: -0.27459830045700073
Batch 20/64 loss: -0.25750917196273804
Batch 21/64 loss: -0.26050156354904175
Batch 22/64 loss: -0.24683797359466553
Batch 23/64 loss: -0.24520045518875122
Batch 24/64 loss: -0.24966198205947876
Batch 25/64 loss: -0.24438795447349548
Batch 26/64 loss: -0.26096048951148987
Batch 27/64 loss: -0.20624828338623047
Batch 28/64 loss: -0.22954294085502625
Batch 29/64 loss: -0.20503193140029907
Batch 30/64 loss: -0.2502985894680023
Batch 31/64 loss: -0.2180805802345276
Batch 32/64 loss: -0.23000243306159973
Batch 33/64 loss: -0.19606798887252808
Batch 34/64 loss: -0.26921382546424866
Batch 35/64 loss: -0.2541802227497101
Batch 36/64 loss: -0.21954065561294556
Batch 37/64 loss: -0.2086888551712036
Batch 38/64 loss: -0.21096694469451904
Batch 39/64 loss: -0.27122360467910767
Batch 40/64 loss: -0.2723078727722168
Batch 41/64 loss: -0.24458956718444824
Batch 42/64 loss: -0.2526323199272156
Batch 43/64 loss: -0.21656888723373413
Batch 44/64 loss: -0.26998263597488403
Batch 45/64 loss: -0.22970867156982422
Batch 46/64 loss: -0.2328549027442932
Batch 47/64 loss: -0.25129270553588867
Batch 48/64 loss: -0.2190701961517334
Batch 49/64 loss: -0.22200357913970947
Batch 50/64 loss: -0.2514670491218567
Batch 51/64 loss: -0.2057192325592041
Batch 52/64 loss: -0.2110351324081421
Batch 53/64 loss: -0.24449777603149414
Batch 54/64 loss: -0.23877131938934326
Batch 55/64 loss: -0.26295384764671326
Batch 56/64 loss: -0.21523070335388184
Batch 57/64 loss: -0.24034744501113892
Batch 58/64 loss: -0.22561591863632202
Batch 59/64 loss: -0.23165079951286316
Batch 60/64 loss: -0.25927597284317017
Batch 61/64 loss: -0.1817060112953186
Batch 62/64 loss: -0.25621122121810913
Batch 63/64 loss: -0.24911966919898987
Batch 64/64 loss: -0.22695589065551758
Epoch 186  Train loss: -0.23985408848407222  Val loss: 0.021496182249993393
Epoch 187
-------------------------------
Batch 1/64 loss: -0.23884928226470947
Batch 2/64 loss: -0.22229808568954468
Batch 3/64 loss: -0.23526641726493835
Batch 4/64 loss: -0.22611236572265625
Batch 5/64 loss: -0.25486987829208374
Batch 6/64 loss: -0.22729015350341797
Batch 7/64 loss: -0.26175108551979065
Batch 8/64 loss: -0.21835851669311523
Batch 9/64 loss: -0.2529110610485077
Batch 10/64 loss: -0.1934334635734558
Batch 11/64 loss: -0.22010517120361328
Batch 12/64 loss: -0.270024836063385
Batch 13/64 loss: -0.25444895029067993
Batch 14/64 loss: -0.24948954582214355
Batch 15/64 loss: -0.21753400564193726
Batch 16/64 loss: -0.17667531967163086
Batch 17/64 loss: -0.2521843910217285
Batch 18/64 loss: -0.23197638988494873
Batch 19/64 loss: -0.23130345344543457
Batch 20/64 loss: -0.22965288162231445
Batch 21/64 loss: -0.23289379477500916
Batch 22/64 loss: -0.23448902368545532
Batch 23/64 loss: -0.253134548664093
Batch 24/64 loss: -0.22354865074157715
Batch 25/64 loss: -0.2323366403579712
Batch 26/64 loss: -0.2147454023361206
Batch 27/64 loss: -0.2860190272331238
Batch 28/64 loss: -0.24071046710014343
Batch 29/64 loss: -0.23925566673278809
Batch 30/64 loss: -0.2310824990272522
Batch 31/64 loss: -0.2165508270263672
Batch 32/64 loss: -0.2676108181476593
Batch 33/64 loss: -0.2677614688873291
Batch 34/64 loss: -0.23313593864440918
Batch 35/64 loss: -0.1791900396347046
Batch 36/64 loss: -0.253291517496109
Batch 37/64 loss: -0.23273447155952454
Batch 38/64 loss: -0.2376810610294342
Batch 39/64 loss: -0.23198482394218445
Batch 40/64 loss: -0.2531927227973938
Batch 41/64 loss: -0.2553178071975708
Batch 42/64 loss: -0.2565962076187134
Batch 43/64 loss: -0.24495264887809753
Batch 44/64 loss: -0.23248237371444702
Batch 45/64 loss: -0.22766011953353882
Batch 46/64 loss: -0.22637253999710083
Batch 47/64 loss: -0.23643216490745544
Batch 48/64 loss: -0.20759737491607666
Batch 49/64 loss: -0.24512743949890137
Batch 50/64 loss: -0.25715476274490356
Batch 51/64 loss: -0.26052749156951904
Batch 52/64 loss: -0.2665216326713562
Batch 53/64 loss: -0.255509614944458
Batch 54/64 loss: -0.2723541259765625
Batch 55/64 loss: -0.2693072557449341
Batch 56/64 loss: -0.23580148816108704
Batch 57/64 loss: -0.2592281103134155
Batch 58/64 loss: -0.2409176230430603
Batch 59/64 loss: -0.23363232612609863
Batch 60/64 loss: -0.23452472686767578
Batch 61/64 loss: -0.26889652013778687
Batch 62/64 loss: -0.21731388568878174
Batch 63/64 loss: -0.2680506706237793
Batch 64/64 loss: -0.25549817085266113
Epoch 187  Train loss: -0.23983979926389806  Val loss: 0.01880693763392078
Epoch 188
-------------------------------
Batch 1/64 loss: -0.2584414482116699
Batch 2/64 loss: -0.2255033254623413
Batch 3/64 loss: -0.26831984519958496
Batch 4/64 loss: -0.25572043657302856
Batch 5/64 loss: -0.21297800540924072
Batch 6/64 loss: -0.21453380584716797
Batch 7/64 loss: -0.24518468976020813
Batch 8/64 loss: -0.26430708169937134
Batch 9/64 loss: -0.20421427488327026
Batch 10/64 loss: -0.2512100338935852
Batch 11/64 loss: -0.24124470353126526
Batch 12/64 loss: -0.2675555348396301
Batch 13/64 loss: -0.2516538202762604
Batch 14/64 loss: -0.24764996767044067
Batch 15/64 loss: -0.2823178172111511
Batch 16/64 loss: -0.22686642408370972
Batch 17/64 loss: -0.25183677673339844
Batch 18/64 loss: -0.21919268369674683
Batch 19/64 loss: -0.20598995685577393
Batch 20/64 loss: -0.2386993169784546
Batch 21/64 loss: -0.23468813300132751
Batch 22/64 loss: -0.25694626569747925
Batch 23/64 loss: -0.24975305795669556
Batch 24/64 loss: -0.2064812183380127
Batch 25/64 loss: -0.21441030502319336
Batch 26/64 loss: -0.2377665638923645
Batch 27/64 loss: -0.19757682085037231
Batch 28/64 loss: -0.23479852080345154
Batch 29/64 loss: -0.23131418228149414
Batch 30/64 loss: -0.21361172199249268
Batch 31/64 loss: -0.20755940675735474
Batch 32/64 loss: -0.2441146969795227
Batch 33/64 loss: -0.28041407465934753
Batch 34/64 loss: -0.22753220796585083
Batch 35/64 loss: -0.22873294353485107
Batch 36/64 loss: -0.24632912874221802
Batch 37/64 loss: -0.2405678927898407
Batch 38/64 loss: -0.24709495902061462
Batch 39/64 loss: -0.23238608241081238
Batch 40/64 loss: -0.24829339981079102
Batch 41/64 loss: -0.2512761652469635
Batch 42/64 loss: -0.2528623044490814
Batch 43/64 loss: -0.23888030648231506
Batch 44/64 loss: -0.25052422285079956
Batch 45/64 loss: -0.2323768436908722
Batch 46/64 loss: -0.2585420310497284
Batch 47/64 loss: -0.2654556334018707
Batch 48/64 loss: -0.26090091466903687
Batch 49/64 loss: -0.25867703557014465
Batch 50/64 loss: -0.26238229870796204
Batch 51/64 loss: -0.24926692247390747
Batch 52/64 loss: -0.21855807304382324
Batch 53/64 loss: -0.2557443380355835
Batch 54/64 loss: -0.2548539340496063
Batch 55/64 loss: -0.21267199516296387
Batch 56/64 loss: -0.21868741512298584
Batch 57/64 loss: -0.2367517054080963
Batch 58/64 loss: -0.21315085887908936
Batch 59/64 loss: -0.22675716876983643
Batch 60/64 loss: -0.23849904537200928
Batch 61/64 loss: -0.24651089310646057
Batch 62/64 loss: -0.27998340129852295
Batch 63/64 loss: -0.23819077014923096
Batch 64/64 loss: -0.19887638092041016
Epoch 188  Train loss: -0.23975612696479348  Val loss: 0.018975391420711765
Epoch 189
-------------------------------
Batch 1/64 loss: -0.2425295114517212
Batch 2/64 loss: -0.2744077146053314
Batch 3/64 loss: -0.2581157088279724
Batch 4/64 loss: -0.23264893889427185
Batch 5/64 loss: -0.2835566997528076
Batch 6/64 loss: -0.24311473965644836
Batch 7/64 loss: -0.2132449746131897
Batch 8/64 loss: -0.2688726484775543
Batch 9/64 loss: -0.25687500834465027
Batch 10/64 loss: -0.23616424202919006
Batch 11/64 loss: -0.26037973165512085
Batch 12/64 loss: -0.25250309705734253
Batch 13/64 loss: -0.27697300910949707
Batch 14/64 loss: -0.2722442150115967
Batch 15/64 loss: -0.2688361704349518
Batch 16/64 loss: -0.2603343427181244
Batch 17/64 loss: -0.22702598571777344
Batch 18/64 loss: -0.19325554370880127
Batch 19/64 loss: -0.28145119547843933
Batch 20/64 loss: -0.2543572187423706
Batch 21/64 loss: -0.1942879557609558
Batch 22/64 loss: -0.2476932406425476
Batch 23/64 loss: -0.2329784631729126
Batch 24/64 loss: -0.2760319709777832
Batch 25/64 loss: -0.2589433789253235
Batch 26/64 loss: -0.24971699714660645
Batch 27/64 loss: -0.24404549598693848
Batch 28/64 loss: -0.23819392919540405
Batch 29/64 loss: -0.2455132007598877
Batch 30/64 loss: -0.25524991750717163
Batch 31/64 loss: -0.25333166122436523
Batch 32/64 loss: -0.2668382525444031
Batch 33/64 loss: -0.24233269691467285
Batch 34/64 loss: -0.22446614503860474
Batch 35/64 loss: -0.23931288719177246
Batch 36/64 loss: -0.24464496970176697
Batch 37/64 loss: -0.2374211847782135
Batch 38/64 loss: -0.23457032442092896
Batch 39/64 loss: -0.23930159211158752
Batch 40/64 loss: -0.2226567268371582
Batch 41/64 loss: -0.2460012137889862
Batch 42/64 loss: -0.24382126331329346
Batch 43/64 loss: -0.24359911680221558
Batch 44/64 loss: -0.18042927980422974
Batch 45/64 loss: -0.21913039684295654
Batch 46/64 loss: -0.24734598398208618
Batch 47/64 loss: -0.2610979676246643
Batch 48/64 loss: -0.24096077680587769
Batch 49/64 loss: -0.2668498754501343
Batch 50/64 loss: -0.2386271357536316
Batch 51/64 loss: -0.22593897581100464
Batch 52/64 loss: -0.2598129212856293
Batch 53/64 loss: -0.22184991836547852
Batch 54/64 loss: -0.21408188343048096
Batch 55/64 loss: -0.2377898097038269
Batch 56/64 loss: -0.24050766229629517
Batch 57/64 loss: -0.22275614738464355
Batch 58/64 loss: -0.19630038738250732
Batch 59/64 loss: -0.2445278763771057
Batch 60/64 loss: -0.2542368769645691
Batch 61/64 loss: -0.24457001686096191
Batch 62/64 loss: -0.23370704054832458
Batch 63/64 loss: -0.25925350189208984
Batch 64/64 loss: -0.25772011280059814
Epoch 189  Train loss: -0.24377894588545257  Val loss: 0.021355434586502024
Epoch 190
-------------------------------
Batch 1/64 loss: -0.21443289518356323
Batch 2/64 loss: -0.2143329381942749
Batch 3/64 loss: -0.25232070684432983
Batch 4/64 loss: -0.2540016770362854
Batch 5/64 loss: -0.1923987865447998
Batch 6/64 loss: -0.20019614696502686
Batch 7/64 loss: -0.26077157258987427
Batch 8/64 loss: -0.2717781662940979
Batch 9/64 loss: -0.24640747904777527
Batch 10/64 loss: -0.26421019434928894
Batch 11/64 loss: -0.2583545446395874
Batch 12/64 loss: -0.27181100845336914
Batch 13/64 loss: -0.2561497986316681
Batch 14/64 loss: -0.21288621425628662
Batch 15/64 loss: -0.23531100153923035
Batch 16/64 loss: -0.2594558298587799
Batch 17/64 loss: -0.2453632950782776
Batch 18/64 loss: -0.23215597867965698
Batch 19/64 loss: -0.2515695095062256
Batch 20/64 loss: -0.23578289151191711
Batch 21/64 loss: -0.24119216203689575
Batch 22/64 loss: -0.22664877772331238
Batch 23/64 loss: -0.2642517685890198
Batch 24/64 loss: -0.2479311227798462
Batch 25/64 loss: -0.22484177350997925
Batch 26/64 loss: -0.20455455780029297
Batch 27/64 loss: -0.24565887451171875
Batch 28/64 loss: -0.24343925714492798
Batch 29/64 loss: -0.24416789412498474
Batch 30/64 loss: -0.26456645131111145
Batch 31/64 loss: -0.2285359799861908
Batch 32/64 loss: -0.2519977390766144
Batch 33/64 loss: -0.26791203022003174
Batch 34/64 loss: -0.2443145513534546
Batch 35/64 loss: -0.24725008010864258
Batch 36/64 loss: -0.2741135358810425
Batch 37/64 loss: -0.23779025673866272
Batch 38/64 loss: -0.24736446142196655
Batch 39/64 loss: -0.23879051208496094
Batch 40/64 loss: -0.24930226802825928
Batch 41/64 loss: -0.19088828563690186
Batch 42/64 loss: -0.23365378379821777
Batch 43/64 loss: -0.24835163354873657
Batch 44/64 loss: -0.21981215476989746
Batch 45/64 loss: -0.23851415514945984
Batch 46/64 loss: -0.25256964564323425
Batch 47/64 loss: -0.21485543251037598
Batch 48/64 loss: -0.2277359664440155
Batch 49/64 loss: -0.26987481117248535
Batch 50/64 loss: -0.259154736995697
Batch 51/64 loss: -0.2652095556259155
Batch 52/64 loss: -0.24848231673240662
Batch 53/64 loss: -0.26396721601486206
Batch 54/64 loss: -0.2487543523311615
Batch 55/64 loss: -0.23053377866744995
Batch 56/64 loss: -0.2429448962211609
Batch 57/64 loss: -0.25897204875946045
Batch 58/64 loss: -0.272377073764801
Batch 59/64 loss: -0.18833941221237183
Batch 60/64 loss: -0.25115731358528137
Batch 61/64 loss: -0.26760149002075195
Batch 62/64 loss: -0.23246285319328308
Batch 63/64 loss: -0.26667219400405884
Batch 64/64 loss: -0.27165937423706055
Epoch 190  Train loss: -0.24346579336652568  Val loss: 0.019077616254078972
Epoch 191
-------------------------------
Batch 1/64 loss: -0.2696716785430908
Batch 2/64 loss: -0.2539311647415161
Batch 3/64 loss: -0.24952572584152222
Batch 4/64 loss: -0.23788690567016602
Batch 5/64 loss: -0.1678248643875122
Batch 6/64 loss: -0.2529865503311157
Batch 7/64 loss: -0.2636403739452362
Batch 8/64 loss: -0.26534217596054077
Batch 9/64 loss: -0.24725639820098877
Batch 10/64 loss: -0.29599276185035706
Batch 11/64 loss: -0.258446604013443
Batch 12/64 loss: -0.20447182655334473
Batch 13/64 loss: -0.240026593208313
Batch 14/64 loss: -0.1886812448501587
Batch 15/64 loss: -0.23611295223236084
Batch 16/64 loss: -0.2855295240879059
Batch 17/64 loss: -0.256333589553833
Batch 18/64 loss: -0.23742273449897766
Batch 19/64 loss: -0.2748123109340668
Batch 20/64 loss: -0.23766791820526123
Batch 21/64 loss: -0.2506377696990967
Batch 22/64 loss: -0.21852785348892212
Batch 23/64 loss: -0.2279520034790039
Batch 24/64 loss: -0.27375686168670654
Batch 25/64 loss: -0.264880895614624
Batch 26/64 loss: -0.19467836618423462
Batch 27/64 loss: -0.2636353373527527
Batch 28/64 loss: -0.24950337409973145
Batch 29/64 loss: -0.24837595224380493
Batch 30/64 loss: -0.2233060598373413
Batch 31/64 loss: -0.2390989065170288
Batch 32/64 loss: -0.25974294543266296
Batch 33/64 loss: -0.26171234250068665
Batch 34/64 loss: -0.23528558015823364
Batch 35/64 loss: -0.25015580654144287
Batch 36/64 loss: -0.2675599753856659
Batch 37/64 loss: -0.22396808862686157
Batch 38/64 loss: -0.23305219411849976
Batch 39/64 loss: -0.22182095050811768
Batch 40/64 loss: -0.2390921711921692
Batch 41/64 loss: -0.2320948839187622
Batch 42/64 loss: -0.22904852032661438
Batch 43/64 loss: -0.2479095458984375
Batch 44/64 loss: -0.24956756830215454
Batch 45/64 loss: -0.28614509105682373
Batch 46/64 loss: -0.2639206051826477
Batch 47/64 loss: -0.2616698145866394
Batch 48/64 loss: -0.2569349706172943
Batch 49/64 loss: -0.26012134552001953
Batch 50/64 loss: -0.25818127393722534
Batch 51/64 loss: -0.26519012451171875
Batch 52/64 loss: -0.24473166465759277
Batch 53/64 loss: -0.28758442401885986
Batch 54/64 loss: -0.2520550489425659
Batch 55/64 loss: -0.24020594358444214
Batch 56/64 loss: -0.24878501892089844
Batch 57/64 loss: -0.2677912414073944
Batch 58/64 loss: -0.23343142867088318
Batch 59/64 loss: -0.257942259311676
Batch 60/64 loss: -0.22470861673355103
Batch 61/64 loss: -0.2162569761276245
Batch 62/64 loss: -0.23174139857292175
Batch 63/64 loss: -0.27245718240737915
Batch 64/64 loss: -0.2489410936832428
Epoch 191  Train loss: -0.24698804862358992  Val loss: 0.019794357806136927
Epoch 192
-------------------------------
Batch 1/64 loss: -0.2530617415904999
Batch 2/64 loss: -0.23082822561264038
Batch 3/64 loss: -0.22887969017028809
Batch 4/64 loss: -0.26312538981437683
Batch 5/64 loss: -0.2576862573623657
Batch 6/64 loss: -0.21390360593795776
Batch 7/64 loss: -0.2601264715194702
Batch 8/64 loss: -0.23693567514419556
Batch 9/64 loss: -0.2384769320487976
Batch 10/64 loss: -0.271236389875412
Batch 11/64 loss: -0.17742693424224854
Batch 12/64 loss: -0.2541046738624573
Batch 13/64 loss: -0.25706833600997925
Batch 14/64 loss: -0.2602100372314453
Batch 15/64 loss: -0.25329771637916565
Batch 16/64 loss: -0.24806523323059082
Batch 17/64 loss: -0.26918113231658936
Batch 18/64 loss: -0.2576017379760742
Batch 19/64 loss: -0.25927358865737915
Batch 20/64 loss: -0.23361021280288696
Batch 21/64 loss: -0.21698492765426636
Batch 22/64 loss: -0.23803210258483887
Batch 23/64 loss: -0.25585201382637024
Batch 24/64 loss: -0.26179879903793335
Batch 25/64 loss: -0.2590470314025879
Batch 26/64 loss: -0.2609163224697113
Batch 27/64 loss: -0.2663049101829529
Batch 28/64 loss: -0.24444586038589478
Batch 29/64 loss: -0.21401166915893555
Batch 30/64 loss: -0.21296477317810059
Batch 31/64 loss: -0.23050647974014282
Batch 32/64 loss: -0.2631489932537079
Batch 33/64 loss: -0.23241686820983887
Batch 34/64 loss: -0.23922038078308105
Batch 35/64 loss: -0.26182281970977783
Batch 36/64 loss: -0.2687737047672272
Batch 37/64 loss: -0.26668158173561096
Batch 38/64 loss: -0.2427971363067627
Batch 39/64 loss: -0.23432716727256775
Batch 40/64 loss: -0.22940421104431152
Batch 41/64 loss: -0.2329067587852478
Batch 42/64 loss: -0.1952841877937317
Batch 43/64 loss: -0.2703336179256439
Batch 44/64 loss: -0.2669667899608612
Batch 45/64 loss: -0.24064937233924866
Batch 46/64 loss: -0.24775183200836182
Batch 47/64 loss: -0.2586575448513031
Batch 48/64 loss: -0.25606516003608704
Batch 49/64 loss: -0.2346559762954712
Batch 50/64 loss: -0.2480672001838684
Batch 51/64 loss: -0.24617213010787964
Batch 52/64 loss: -0.26558470726013184
Batch 53/64 loss: -0.22126460075378418
Batch 54/64 loss: -0.25784218311309814
Batch 55/64 loss: -0.26994457840919495
Batch 56/64 loss: -0.26052039861679077
Batch 57/64 loss: -0.24638128280639648
Batch 58/64 loss: -0.24241337180137634
Batch 59/64 loss: -0.234266459941864
Batch 60/64 loss: -0.2006603479385376
Batch 61/64 loss: -0.2350597381591797
Batch 62/64 loss: -0.25282377004623413
Batch 63/64 loss: -0.2542558014392853
Batch 64/64 loss: -0.24935466051101685
Epoch 192  Train loss: -0.24547610259523578  Val loss: 0.02094667146296026
Epoch 193
-------------------------------
Batch 1/64 loss: -0.23842400312423706
Batch 2/64 loss: -0.270832359790802
Batch 3/64 loss: -0.2073347568511963
Batch 4/64 loss: -0.23540669679641724
Batch 5/64 loss: -0.22139328718185425
Batch 6/64 loss: -0.2531459927558899
Batch 7/64 loss: -0.23127302527427673
Batch 8/64 loss: -0.24216940999031067
Batch 9/64 loss: -0.27398359775543213
Batch 10/64 loss: -0.27858591079711914
Batch 11/64 loss: -0.2919159531593323
Batch 12/64 loss: -0.25150448083877563
Batch 13/64 loss: -0.2095179557800293
Batch 14/64 loss: -0.23822826147079468
Batch 15/64 loss: -0.25253182649612427
Batch 16/64 loss: -0.22861063480377197
Batch 17/64 loss: -0.2533389925956726
Batch 18/64 loss: -0.22728216648101807
Batch 19/64 loss: -0.2586875557899475
Batch 20/64 loss: -0.1830291748046875
Batch 21/64 loss: -0.2424236238002777
Batch 22/64 loss: -0.264191597700119
Batch 23/64 loss: -0.24929413199424744
Batch 24/64 loss: -0.2314273715019226
Batch 25/64 loss: -0.2328391671180725
Batch 26/64 loss: -0.23401570320129395
Batch 27/64 loss: -0.22716543078422546
Batch 28/64 loss: -0.21613264083862305
Batch 29/64 loss: -0.23711934685707092
Batch 30/64 loss: -0.26587164402008057
Batch 31/64 loss: -0.2580724358558655
Batch 32/64 loss: -0.2626459300518036
Batch 33/64 loss: -0.23925280570983887
Batch 34/64 loss: -0.23671197891235352
Batch 35/64 loss: -0.2630559802055359
Batch 36/64 loss: -0.2585366666316986
Batch 37/64 loss: -0.2166174054145813
Batch 38/64 loss: -0.27223244309425354
Batch 39/64 loss: -0.2149202823638916
Batch 40/64 loss: -0.23789596557617188
Batch 41/64 loss: -0.22971928119659424
Batch 42/64 loss: -0.25740140676498413
Batch 43/64 loss: -0.23118984699249268
Batch 44/64 loss: -0.2693895101547241
Batch 45/64 loss: -0.26028943061828613
Batch 46/64 loss: -0.24178367853164673
Batch 47/64 loss: -0.20494329929351807
Batch 48/64 loss: -0.25214883685112
Batch 49/64 loss: -0.2473435401916504
Batch 50/64 loss: -0.22214782238006592
Batch 51/64 loss: -0.23281347751617432
Batch 52/64 loss: -0.24389898777008057
Batch 53/64 loss: -0.25522300601005554
Batch 54/64 loss: -0.2506272792816162
Batch 55/64 loss: -0.25665175914764404
Batch 56/64 loss: -0.23402440547943115
Batch 57/64 loss: -0.2485208511352539
Batch 58/64 loss: -0.24840325117111206
Batch 59/64 loss: -0.2478829026222229
Batch 60/64 loss: -0.2520521283149719
Batch 61/64 loss: -0.2656806409358978
Batch 62/64 loss: -0.27300673723220825
Batch 63/64 loss: -0.2435961663722992
Batch 64/64 loss: -0.19704127311706543
Epoch 193  Train loss: -0.24351588697994456  Val loss: 0.020477867208395627
Epoch 194
-------------------------------
Batch 1/64 loss: -0.2322782278060913
Batch 2/64 loss: -0.25711894035339355
Batch 3/64 loss: -0.23389622569084167
Batch 4/64 loss: -0.234613835811615
Batch 5/64 loss: -0.22720080614089966
Batch 6/64 loss: -0.24879425764083862
Batch 7/64 loss: -0.24727630615234375
Batch 8/64 loss: -0.2723310589790344
Batch 9/64 loss: -0.2455000877380371
Batch 10/64 loss: -0.22954332828521729
Batch 11/64 loss: -0.2777792811393738
Batch 12/64 loss: -0.23774200677871704
Batch 13/64 loss: -0.23084980249404907
Batch 14/64 loss: -0.2570735812187195
Batch 15/64 loss: -0.2680567502975464
Batch 16/64 loss: -0.19989311695098877
Batch 17/64 loss: -0.2508401572704315
Batch 18/64 loss: -0.2199755311012268
Batch 19/64 loss: -0.24508145451545715
Batch 20/64 loss: -0.2698177099227905
Batch 21/64 loss: -0.23264819383621216
Batch 22/64 loss: -0.24318528175354004
Batch 23/64 loss: -0.23296970129013062
Batch 24/64 loss: -0.2762889266014099
Batch 25/64 loss: -0.26266413927078247
Batch 26/64 loss: -0.2755066156387329
Batch 27/64 loss: -0.20678424835205078
Batch 28/64 loss: -0.23025482892990112
Batch 29/64 loss: -0.25989294052124023
Batch 30/64 loss: -0.2616038918495178
Batch 31/64 loss: -0.2757558822631836
Batch 32/64 loss: -0.25601595640182495
Batch 33/64 loss: -0.26695239543914795
Batch 34/64 loss: -0.23876869678497314
Batch 35/64 loss: -0.2569337785243988
Batch 36/64 loss: -0.2468615472316742
Batch 37/64 loss: -0.23738908767700195
Batch 38/64 loss: -0.21914887428283691
Batch 39/64 loss: -0.250316321849823
Batch 40/64 loss: -0.25145941972732544
Batch 41/64 loss: -0.2211475968360901
Batch 42/64 loss: -0.235784113407135
Batch 43/64 loss: -0.23170506954193115
Batch 44/64 loss: -0.24975401163101196
Batch 45/64 loss: -0.235090434551239
Batch 46/64 loss: -0.2679785490036011
Batch 47/64 loss: -0.22041970491409302
Batch 48/64 loss: -0.21801215410232544
Batch 49/64 loss: -0.23244941234588623
Batch 50/64 loss: -0.2596815228462219
Batch 51/64 loss: -0.21153199672698975
Batch 52/64 loss: -0.27083951234817505
Batch 53/64 loss: -0.25619083642959595
Batch 54/64 loss: -0.2640385329723358
Batch 55/64 loss: -0.24818527698516846
Batch 56/64 loss: -0.2263643741607666
Batch 57/64 loss: -0.27574241161346436
Batch 58/64 loss: -0.26338398456573486
Batch 59/64 loss: -0.26825302839279175
Batch 60/64 loss: -0.22266989946365356
Batch 61/64 loss: -0.21073198318481445
Batch 62/64 loss: -0.2405674159526825
Batch 63/64 loss: -0.2223571538925171
Batch 64/64 loss: -0.19850444793701172
Epoch 194  Train loss: -0.2442167922562244  Val loss: 0.022587160064592396
Epoch 195
-------------------------------
Batch 1/64 loss: -0.2428654432296753
Batch 2/64 loss: -0.2801485061645508
Batch 3/64 loss: -0.21784073114395142
Batch 4/64 loss: -0.2577085793018341
Batch 5/64 loss: -0.2357887327671051
Batch 6/64 loss: -0.2920156717300415
Batch 7/64 loss: -0.21972912549972534
Batch 8/64 loss: -0.20250356197357178
Batch 9/64 loss: -0.24711185693740845
Batch 10/64 loss: -0.24783658981323242
Batch 11/64 loss: -0.18670278787612915
Batch 12/64 loss: -0.25958800315856934
Batch 13/64 loss: -0.25496822595596313
Batch 14/64 loss: -0.2622862458229065
Batch 15/64 loss: -0.250515878200531
Batch 16/64 loss: -0.2247600555419922
Batch 17/64 loss: -0.25979292392730713
Batch 18/64 loss: -0.27043092250823975
Batch 19/64 loss: -0.21910429000854492
Batch 20/64 loss: -0.2359713912010193
Batch 21/64 loss: -0.23709902167320251
Batch 22/64 loss: -0.23624536395072937
Batch 23/64 loss: -0.26008176803588867
Batch 24/64 loss: -0.24371793866157532
Batch 25/64 loss: -0.24904078245162964
Batch 26/64 loss: -0.23459738492965698
Batch 27/64 loss: -0.2457391917705536
Batch 28/64 loss: -0.2508144974708557
Batch 29/64 loss: -0.22186946868896484
Batch 30/64 loss: -0.26166602969169617
Batch 31/64 loss: -0.2327004075050354
Batch 32/64 loss: -0.25313854217529297
Batch 33/64 loss: -0.26109445095062256
Batch 34/64 loss: -0.259163498878479
Batch 35/64 loss: -0.2439677119255066
Batch 36/64 loss: -0.2453840970993042
Batch 37/64 loss: -0.23879706859588623
Batch 38/64 loss: -0.24927347898483276
Batch 39/64 loss: -0.22380036115646362
Batch 40/64 loss: -0.2899514436721802
Batch 41/64 loss: -0.2624795734882355
Batch 42/64 loss: -0.27284306287765503
Batch 43/64 loss: -0.2520562410354614
Batch 44/64 loss: -0.25454169511795044
Batch 45/64 loss: -0.18641263246536255
Batch 46/64 loss: -0.26545125246047974
Batch 47/64 loss: -0.26932644844055176
Batch 48/64 loss: -0.2683117985725403
Batch 49/64 loss: -0.21783417463302612
Batch 50/64 loss: -0.21378839015960693
Batch 51/64 loss: -0.20980066061019897
Batch 52/64 loss: -0.25458064675331116
Batch 53/64 loss: -0.21462535858154297
Batch 54/64 loss: -0.2519940733909607
Batch 55/64 loss: -0.2765379548072815
Batch 56/64 loss: -0.27595996856689453
Batch 57/64 loss: -0.22772693634033203
Batch 58/64 loss: -0.24489206075668335
Batch 59/64 loss: -0.26072096824645996
Batch 60/64 loss: -0.2551714777946472
Batch 61/64 loss: -0.2790834903717041
Batch 62/64 loss: -0.24894237518310547
Batch 63/64 loss: -0.2287745475769043
Batch 64/64 loss: -0.29721683263778687
Epoch 195  Train loss: -0.24659734029395908  Val loss: 0.019780380209696663
Epoch 196
-------------------------------
Batch 1/64 loss: -0.23452216386795044
Batch 2/64 loss: -0.28341057896614075
Batch 3/64 loss: -0.27678900957107544
Batch 4/64 loss: -0.24044126272201538
Batch 5/64 loss: -0.2724602222442627
Batch 6/64 loss: -0.2392038106918335
Batch 7/64 loss: -0.2787854075431824
Batch 8/64 loss: -0.2681542634963989
Batch 9/64 loss: -0.2605050802230835
Batch 10/64 loss: -0.26727616786956787
Batch 11/64 loss: -0.23623868823051453
Batch 12/64 loss: -0.27016836404800415
Batch 13/64 loss: -0.20441997051239014
Batch 14/64 loss: -0.2603546977043152
Batch 15/64 loss: -0.21322143077850342
Batch 16/64 loss: -0.19749915599822998
Batch 17/64 loss: -0.2780209183692932
Batch 18/64 loss: -0.26844385266304016
Batch 19/64 loss: -0.2197001576423645
Batch 20/64 loss: -0.2372233271598816
Batch 21/64 loss: -0.2543472647666931
Batch 22/64 loss: -0.23915520310401917
Batch 23/64 loss: -0.24194452166557312
Batch 24/64 loss: -0.22631901502609253
Batch 25/64 loss: -0.22829502820968628
Batch 26/64 loss: -0.2632661461830139
Batch 27/64 loss: -0.2633691430091858
Batch 28/64 loss: -0.2786247134208679
Batch 29/64 loss: -0.2530827522277832
Batch 30/64 loss: -0.2535756230354309
Batch 31/64 loss: -0.25777918100357056
Batch 32/64 loss: -0.27370601892471313
Batch 33/64 loss: -0.24177855253219604
Batch 34/64 loss: -0.2682268023490906
Batch 35/64 loss: -0.2555583119392395
Batch 36/64 loss: -0.21212249994277954
Batch 37/64 loss: -0.22378456592559814
Batch 38/64 loss: -0.2454671561717987
Batch 39/64 loss: -0.2088850736618042
Batch 40/64 loss: -0.2327057123184204
Batch 41/64 loss: -0.22228586673736572
Batch 42/64 loss: -0.2524316608905792
Batch 43/64 loss: -0.2137024998664856
Batch 44/64 loss: -0.2765023112297058
Batch 45/64 loss: -0.26256275177001953
Batch 46/64 loss: -0.2445652186870575
Batch 47/64 loss: -0.21612107753753662
Batch 48/64 loss: -0.2293238341808319
Batch 49/64 loss: -0.23851001262664795
Batch 50/64 loss: -0.2789151072502136
Batch 51/64 loss: -0.27349817752838135
Batch 52/64 loss: -0.25251591205596924
Batch 53/64 loss: -0.23659411072731018
Batch 54/64 loss: -0.251736044883728
Batch 55/64 loss: -0.23804745078086853
Batch 56/64 loss: -0.21701592206954956
Batch 57/64 loss: -0.2590291202068329
Batch 58/64 loss: -0.24783208966255188
Batch 59/64 loss: -0.23694178462028503
Batch 60/64 loss: -0.2513408958911896
Batch 61/64 loss: -0.22964519262313843
Batch 62/64 loss: -0.2726436257362366
Batch 63/64 loss: -0.25789669156074524
Batch 64/64 loss: -0.2622911334037781
Epoch 196  Train loss: -0.24761109842973597  Val loss: 0.02075484364303117
Epoch 197
-------------------------------
Batch 1/64 loss: -0.26387929916381836
Batch 2/64 loss: -0.25858888030052185
Batch 3/64 loss: -0.24840682744979858
Batch 4/64 loss: -0.2532074451446533
Batch 5/64 loss: -0.2770904302597046
Batch 6/64 loss: -0.25006982684135437
Batch 7/64 loss: -0.29168879985809326
Batch 8/64 loss: -0.24622109532356262
Batch 9/64 loss: -0.2562669813632965
Batch 10/64 loss: -0.26083385944366455
Batch 11/64 loss: -0.2365029752254486
Batch 12/64 loss: -0.1963704228401184
Batch 13/64 loss: -0.2693999707698822
Batch 14/64 loss: -0.23533469438552856
Batch 15/64 loss: -0.23551234602928162
Batch 16/64 loss: -0.24629318714141846
Batch 17/64 loss: -0.2346985936164856
Batch 18/64 loss: -0.2576763927936554
Batch 19/64 loss: -0.21807116270065308
Batch 20/64 loss: -0.2416556477546692
Batch 21/64 loss: -0.24906235933303833
Batch 22/64 loss: -0.23758018016815186
Batch 23/64 loss: -0.25706028938293457
Batch 24/64 loss: -0.19671237468719482
Batch 25/64 loss: -0.24893853068351746
Batch 26/64 loss: -0.2643736004829407
Batch 27/64 loss: -0.2643442451953888
Batch 28/64 loss: -0.23856040835380554
Batch 29/64 loss: -0.2436637580394745
Batch 30/64 loss: -0.2457450032234192
Batch 31/64 loss: -0.23456960916519165
Batch 32/64 loss: -0.27500399947166443
Batch 33/64 loss: -0.23814451694488525
Batch 34/64 loss: -0.253934770822525
Batch 35/64 loss: -0.23551630973815918
Batch 36/64 loss: -0.2547212243080139
Batch 37/64 loss: -0.252528578042984
Batch 38/64 loss: -0.3012145161628723
Batch 39/64 loss: -0.24285727739334106
Batch 40/64 loss: -0.19476020336151123
Batch 41/64 loss: -0.27395540475845337
Batch 42/64 loss: -0.2518763542175293
Batch 43/64 loss: -0.25356239080429077
Batch 44/64 loss: -0.25101009011268616
Batch 45/64 loss: -0.24137213826179504
Batch 46/64 loss: -0.23760637640953064
Batch 47/64 loss: -0.21691381931304932
Batch 48/64 loss: -0.21550500392913818
Batch 49/64 loss: -0.24636691808700562
Batch 50/64 loss: -0.26699942350387573
Batch 51/64 loss: -0.2431805431842804
Batch 52/64 loss: -0.27165278792381287
Batch 53/64 loss: -0.24760806560516357
Batch 54/64 loss: -0.2818504571914673
Batch 55/64 loss: -0.2666681408882141
Batch 56/64 loss: -0.24804791808128357
Batch 57/64 loss: -0.2707958221435547
Batch 58/64 loss: -0.25455349683761597
Batch 59/64 loss: -0.25643783807754517
Batch 60/64 loss: -0.21313655376434326
Batch 61/64 loss: -0.22799548506736755
Batch 62/64 loss: -0.2815515398979187
Batch 63/64 loss: -0.2570047378540039
Batch 64/64 loss: -0.2570921778678894
Epoch 197  Train loss: -0.2490279377675524  Val loss: 0.02293064033042934
Epoch 198
-------------------------------
Batch 1/64 loss: -0.25116047263145447
Batch 2/64 loss: -0.23945921659469604
Batch 3/64 loss: -0.2539312243461609
Batch 4/64 loss: -0.21905982494354248
Batch 5/64 loss: -0.2578800320625305
Batch 6/64 loss: -0.25346362590789795
Batch 7/64 loss: -0.2526286244392395
Batch 8/64 loss: -0.2528141736984253
Batch 9/64 loss: -0.22414517402648926
Batch 10/64 loss: -0.24063849449157715
Batch 11/64 loss: -0.26690399646759033
Batch 12/64 loss: -0.25405341386795044
Batch 13/64 loss: -0.26667821407318115
Batch 14/64 loss: -0.2168123722076416
Batch 15/64 loss: -0.2642238140106201
Batch 16/64 loss: -0.2614726424217224
Batch 17/64 loss: -0.259497731924057
Batch 18/64 loss: -0.23779016733169556
Batch 19/64 loss: -0.255807101726532
Batch 20/64 loss: -0.23583978414535522
Batch 21/64 loss: -0.26131317019462585
Batch 22/64 loss: -0.272768497467041
Batch 23/64 loss: -0.2684144973754883
Batch 24/64 loss: -0.2482064962387085
Batch 25/64 loss: -0.2536352574825287
Batch 26/64 loss: -0.23065757751464844
Batch 27/64 loss: -0.21139401197433472
Batch 28/64 loss: -0.25355350971221924
Batch 29/64 loss: -0.26179710030555725
Batch 30/64 loss: -0.2369515299797058
Batch 31/64 loss: -0.22939902544021606
Batch 32/64 loss: -0.2740135192871094
Batch 33/64 loss: -0.23528212308883667
Batch 34/64 loss: -0.2690426707267761
Batch 35/64 loss: -0.2403309941291809
Batch 36/64 loss: -0.25589150190353394
Batch 37/64 loss: -0.2709951400756836
Batch 38/64 loss: -0.22938579320907593
Batch 39/64 loss: -0.2260642647743225
Batch 40/64 loss: -0.2595731317996979
Batch 41/64 loss: -0.2769859731197357
Batch 42/64 loss: -0.22361350059509277
Batch 43/64 loss: -0.2668296694755554
Batch 44/64 loss: -0.2708492577075958
Batch 45/64 loss: -0.24081626534461975
Batch 46/64 loss: -0.23221051692962646
Batch 47/64 loss: -0.2348765730857849
Batch 48/64 loss: -0.24718892574310303
Batch 49/64 loss: -0.24853891134262085
Batch 50/64 loss: -0.22271698713302612
Batch 51/64 loss: -0.2564614415168762
Batch 52/64 loss: -0.2597060799598694
Batch 53/64 loss: -0.2220855951309204
Batch 54/64 loss: -0.24060720205307007
Batch 55/64 loss: -0.25589728355407715
Batch 56/64 loss: -0.2653923034667969
Batch 57/64 loss: -0.21109765768051147
Batch 58/64 loss: -0.21765977144241333
Batch 59/64 loss: -0.2606661319732666
Batch 60/64 loss: -0.2699785530567169
Batch 61/64 loss: -0.26414376497268677
Batch 62/64 loss: -0.22001415491104126
Batch 63/64 loss: -0.2052919864654541
Batch 64/64 loss: -0.23198813199996948
Epoch 198  Train loss: -0.24691058070051905  Val loss: 0.022033487808253756
Epoch 199
-------------------------------
Batch 1/64 loss: -0.25609689950942993
Batch 2/64 loss: -0.21548223495483398
Batch 3/64 loss: -0.2735859751701355
Batch 4/64 loss: -0.24683454632759094
Batch 5/64 loss: -0.24144858121871948
Batch 6/64 loss: -0.25758183002471924
Batch 7/64 loss: -0.25016382336616516
Batch 8/64 loss: -0.26834332942962646
Batch 9/64 loss: -0.2776775360107422
Batch 10/64 loss: -0.250816285610199
Batch 11/64 loss: -0.21782046556472778
Batch 12/64 loss: -0.2523346543312073
Batch 13/64 loss: -0.22189223766326904
Batch 14/64 loss: -0.23742911219596863
Batch 15/64 loss: -0.2467358112335205
Batch 16/64 loss: -0.19076895713806152
Batch 17/64 loss: -0.23240947723388672
Batch 18/64 loss: -0.2615993320941925
Batch 19/64 loss: -0.2656455636024475
Batch 20/64 loss: -0.2446834146976471
Batch 21/64 loss: -0.23240110278129578
Batch 22/64 loss: -0.26277294754981995
Batch 23/64 loss: -0.2682068645954132
Batch 24/64 loss: -0.25092124938964844
Batch 25/64 loss: -0.2637070119380951
Batch 26/64 loss: -0.22762161493301392
Batch 27/64 loss: -0.27766168117523193
Batch 28/64 loss: -0.2736314535140991
Batch 29/64 loss: -0.23246461153030396
Batch 30/64 loss: -0.2481377124786377
Batch 31/64 loss: -0.27992331981658936
Batch 32/64 loss: -0.27359825372695923
Batch 33/64 loss: -0.24299031496047974
Batch 34/64 loss: -0.24344992637634277
Batch 35/64 loss: -0.2591167688369751
Batch 36/64 loss: -0.26109766960144043
Batch 37/64 loss: -0.2808389663696289
Batch 38/64 loss: -0.18452000617980957
Batch 39/64 loss: -0.28023266792297363
Batch 40/64 loss: -0.256118506193161
Batch 41/64 loss: -0.24094471335411072
Batch 42/64 loss: -0.24709942936897278
Batch 43/64 loss: -0.23053646087646484
Batch 44/64 loss: -0.2469419240951538
Batch 45/64 loss: -0.23569542169570923
Batch 46/64 loss: -0.27697837352752686
Batch 47/64 loss: -0.2582729756832123
Batch 48/64 loss: -0.19785159826278687
Batch 49/64 loss: -0.2174798846244812
Batch 50/64 loss: -0.25701385736465454
Batch 51/64 loss: -0.2716665267944336
Batch 52/64 loss: -0.2369799017906189
Batch 53/64 loss: -0.26111429929733276
Batch 54/64 loss: -0.2353288233280182
Batch 55/64 loss: -0.2639998197555542
Batch 56/64 loss: -0.2702997922897339
Batch 57/64 loss: -0.27085280418395996
Batch 58/64 loss: -0.2667139172554016
Batch 59/64 loss: -0.2624579668045044
Batch 60/64 loss: -0.25664961338043213
Batch 61/64 loss: -0.25097185373306274
Batch 62/64 loss: -0.2315187156200409
Batch 63/64 loss: -0.27126461267471313
Batch 64/64 loss: -0.24399983882904053
Epoch 199  Train loss: -0.25020228834713204  Val loss: 0.021118607922517966
Epoch 200
-------------------------------
Batch 1/64 loss: -0.29944711923599243
Batch 2/64 loss: -0.26618486642837524
Batch 3/64 loss: -0.2911193370819092
Batch 4/64 loss: -0.2663683295249939
Batch 5/64 loss: -0.17819565534591675
Batch 6/64 loss: -0.26892077922821045
Batch 7/64 loss: -0.24885568022727966
Batch 8/64 loss: -0.26159757375717163
Batch 9/64 loss: -0.24382907152175903
Batch 10/64 loss: -0.2699592113494873
Batch 11/64 loss: -0.25084418058395386
Batch 12/64 loss: -0.23049280047416687
Batch 13/64 loss: -0.22371339797973633
Batch 14/64 loss: -0.26597556471824646
Batch 15/64 loss: -0.24493283033370972
Batch 16/64 loss: -0.2441757619380951
Batch 17/64 loss: -0.25402459502220154
Batch 18/64 loss: -0.25579890608787537
Batch 19/64 loss: -0.20396780967712402
Batch 20/64 loss: -0.2585342526435852
Batch 21/64 loss: -0.26131775975227356
Batch 22/64 loss: -0.23345312476158142
Batch 23/64 loss: -0.24493366479873657
Batch 24/64 loss: -0.26609006524086
Batch 25/64 loss: -0.2540487051010132
Batch 26/64 loss: -0.26328104734420776
Batch 27/64 loss: -0.23997896909713745
Batch 28/64 loss: -0.25977152585983276
Batch 29/64 loss: -0.255268394947052
Batch 30/64 loss: -0.24794507026672363
Batch 31/64 loss: -0.24159419536590576
Batch 32/64 loss: -0.24423474073410034
Batch 33/64 loss: -0.2421584129333496
Batch 34/64 loss: -0.22893956303596497
Batch 35/64 loss: -0.25482413172721863
Batch 36/64 loss: -0.2161273956298828
Batch 37/64 loss: -0.24368488788604736
Batch 38/64 loss: -0.2791573703289032
Batch 39/64 loss: -0.20249080657958984
Batch 40/64 loss: -0.2830623388290405
Batch 41/64 loss: -0.19445514678955078
Batch 42/64 loss: -0.2725880742073059
Batch 43/64 loss: -0.2976185083389282
Batch 44/64 loss: -0.2778051197528839
Batch 45/64 loss: -0.2351701259613037
Batch 46/64 loss: -0.2521909475326538
Batch 47/64 loss: -0.23834168910980225
Batch 48/64 loss: -0.22198963165283203
Batch 49/64 loss: -0.26549357175827026
Batch 50/64 loss: -0.23810407519340515
Batch 51/64 loss: -0.19848060607910156
Batch 52/64 loss: -0.24673277139663696
Batch 53/64 loss: -0.2756901979446411
Batch 54/64 loss: -0.21424198150634766
Batch 55/64 loss: -0.26606935262680054
Batch 56/64 loss: -0.24180817604064941
Batch 57/64 loss: -0.26545679569244385
Batch 58/64 loss: -0.24617469310760498
Batch 59/64 loss: -0.2653205990791321
Batch 60/64 loss: -0.2377992868423462
Batch 61/64 loss: -0.26822447776794434
Batch 62/64 loss: -0.24842572212219238
Batch 63/64 loss: -0.24914351105690002
Batch 64/64 loss: -0.2605760395526886
Epoch 200  Train loss: -0.2494440311310338  Val loss: 0.02148014982951056
Epoch 201
-------------------------------
Batch 1/64 loss: -0.26866209506988525
Batch 2/64 loss: -0.26824522018432617
Batch 3/64 loss: -0.24240225553512573
Batch 4/64 loss: -0.21856164932250977
Batch 5/64 loss: -0.23247447609901428
Batch 6/64 loss: -0.28336215019226074
Batch 7/64 loss: -0.24583953619003296
Batch 8/64 loss: -0.29364049434661865
Batch 9/64 loss: -0.22540369629859924
Batch 10/64 loss: -0.25629279017448425
Batch 11/64 loss: -0.23275965452194214
Batch 12/64 loss: -0.23539817333221436
Batch 13/64 loss: -0.22291266918182373
Batch 14/64 loss: -0.2597930133342743
Batch 15/64 loss: -0.25907063484191895
Batch 16/64 loss: -0.2762942910194397
Batch 17/64 loss: -0.2587733268737793
Batch 18/64 loss: -0.2700032591819763
Batch 19/64 loss: -0.28509747982025146
Batch 20/64 loss: -0.2807490825653076
Batch 21/64 loss: -0.23403006792068481
Batch 22/64 loss: -0.2703699767589569
Batch 23/64 loss: -0.2289067506790161
Batch 24/64 loss: -0.24252137541770935
Batch 25/64 loss: -0.2683188319206238
Batch 26/64 loss: -0.2628028392791748
Batch 27/64 loss: -0.23709064722061157
Batch 28/64 loss: -0.2699928879737854
Batch 29/64 loss: -0.1918066143989563
Batch 30/64 loss: -0.22881820797920227
Batch 31/64 loss: -0.24470657110214233
Batch 32/64 loss: -0.2525242269039154
Batch 33/64 loss: -0.23580333590507507
Batch 34/64 loss: -0.25692975521087646
Batch 35/64 loss: -0.2217566967010498
Batch 36/64 loss: -0.2231164574623108
Batch 37/64 loss: -0.23299074172973633
Batch 38/64 loss: -0.2237246036529541
Batch 39/64 loss: -0.2402183711528778
Batch 40/64 loss: -0.20417183637619019
Batch 41/64 loss: -0.25865286588668823
Batch 42/64 loss: -0.2650030255317688
Batch 43/64 loss: -0.22668346762657166
Batch 44/64 loss: -0.21245616674423218
Batch 45/64 loss: -0.2645590305328369
Batch 46/64 loss: -0.26995906233787537
Batch 47/64 loss: -0.25585120916366577
Batch 48/64 loss: -0.2657601237297058
Batch 49/64 loss: -0.26910537481307983
Batch 50/64 loss: -0.22440028190612793
Batch 51/64 loss: -0.25878897309303284
Batch 52/64 loss: -0.2718603014945984
Batch 53/64 loss: -0.24710294604301453
Batch 54/64 loss: -0.25356748700141907
Batch 55/64 loss: -0.26234740018844604
Batch 56/64 loss: -0.24658161401748657
Batch 57/64 loss: -0.2719286382198334
Batch 58/64 loss: -0.25858134031295776
Batch 59/64 loss: -0.2522633373737335
Batch 60/64 loss: -0.1961454153060913
Batch 61/64 loss: -0.2204410433769226
Batch 62/64 loss: -0.26413893699645996
Batch 63/64 loss: -0.22253626585006714
Batch 64/64 loss: -0.23625922203063965
Epoch 201  Train loss: -0.2478783602808036  Val loss: 0.021718639688393503
Epoch 202
-------------------------------
Batch 1/64 loss: -0.21627837419509888
Batch 2/64 loss: -0.2396513819694519
Batch 3/64 loss: -0.2544788718223572
Batch 4/64 loss: -0.22585299611091614
Batch 5/64 loss: -0.2538943588733673
Batch 6/64 loss: -0.2555049657821655
Batch 7/64 loss: -0.2177215814590454
Batch 8/64 loss: -0.23810964822769165
Batch 9/64 loss: -0.24207091331481934
Batch 10/64 loss: -0.21039754152297974
Batch 11/64 loss: -0.19314777851104736
Batch 12/64 loss: -0.2368701696395874
Batch 13/64 loss: -0.2565457820892334
Batch 14/64 loss: -0.26121973991394043
Batch 15/64 loss: -0.27285903692245483
Batch 16/64 loss: -0.28285765647888184
Batch 17/64 loss: -0.23860421776771545
Batch 18/64 loss: -0.2543225884437561
Batch 19/64 loss: -0.24125364422798157
Batch 20/64 loss: -0.24563416838645935
Batch 21/64 loss: -0.2376883625984192
Batch 22/64 loss: -0.2620176076889038
Batch 23/64 loss: -0.23001167178153992
Batch 24/64 loss: -0.21236836910247803
Batch 25/64 loss: -0.2667168974876404
Batch 26/64 loss: -0.26458653807640076
Batch 27/64 loss: -0.21366465091705322
Batch 28/64 loss: -0.23946160078048706
Batch 29/64 loss: -0.2710820436477661
Batch 30/64 loss: -0.22429931163787842
Batch 31/64 loss: -0.25331607460975647
Batch 32/64 loss: -0.26891860365867615
Batch 33/64 loss: -0.2908724248409271
Batch 34/64 loss: -0.2643761932849884
Batch 35/64 loss: -0.2737671136856079
Batch 36/64 loss: -0.2635219991207123
Batch 37/64 loss: -0.2588917315006256
Batch 38/64 loss: -0.27003830671310425
Batch 39/64 loss: -0.24947667121887207
Batch 40/64 loss: -0.24852067232131958
Batch 41/64 loss: -0.2318953275680542
Batch 42/64 loss: -0.23733291029930115
Batch 43/64 loss: -0.2819509506225586
Batch 44/64 loss: -0.21937108039855957
Batch 45/64 loss: -0.23493248224258423
Batch 46/64 loss: -0.2497645616531372
Batch 47/64 loss: -0.272599458694458
Batch 48/64 loss: -0.272030234336853
Batch 49/64 loss: -0.2767066955566406
Batch 50/64 loss: -0.19751834869384766
Batch 51/64 loss: -0.2617220878601074
Batch 52/64 loss: -0.23922979831695557
Batch 53/64 loss: -0.25029632449150085
Batch 54/64 loss: -0.2722770571708679
Batch 55/64 loss: -0.2540796101093292
Batch 56/64 loss: -0.25362473726272583
Batch 57/64 loss: -0.2717090845108032
Batch 58/64 loss: -0.23285847902297974
Batch 59/64 loss: -0.21870356798171997
Batch 60/64 loss: -0.2823103070259094
Batch 61/64 loss: -0.2594235837459564
Batch 62/64 loss: -0.277180016040802
Batch 63/64 loss: -0.2639857530593872
Batch 64/64 loss: -0.22764909267425537
Epoch 202  Train loss: -0.24914684763141706  Val loss: 0.02196023902532571
Epoch 203
-------------------------------
Batch 1/64 loss: -0.2531990706920624
Batch 2/64 loss: -0.2414298951625824
Batch 3/64 loss: -0.2481493353843689
Batch 4/64 loss: -0.24831759929656982
Batch 5/64 loss: -0.26615047454833984
Batch 6/64 loss: -0.1894536018371582
Batch 7/64 loss: -0.24426859617233276
Batch 8/64 loss: -0.2642395496368408
Batch 9/64 loss: -0.2761055827140808
Batch 10/64 loss: -0.22639834880828857
Batch 11/64 loss: -0.2618265450000763
Batch 12/64 loss: -0.22825688123703003
Batch 13/64 loss: -0.27183833718299866
Batch 14/64 loss: -0.2536022961139679
Batch 15/64 loss: -0.2485826015472412
Batch 16/64 loss: -0.23078668117523193
Batch 17/64 loss: -0.2801123261451721
Batch 18/64 loss: -0.23753565549850464
Batch 19/64 loss: -0.28314080834388733
Batch 20/64 loss: -0.2631418704986572
Batch 21/64 loss: -0.22281229496002197
Batch 22/64 loss: -0.268465518951416
Batch 23/64 loss: -0.22713488340377808
Batch 24/64 loss: -0.23846590518951416
Batch 25/64 loss: -0.24549341201782227
Batch 26/64 loss: -0.2587321698665619
Batch 27/64 loss: -0.242294043302536
Batch 28/64 loss: -0.252959281206131
Batch 29/64 loss: -0.24630984663963318
Batch 30/64 loss: -0.22471249103546143
Batch 31/64 loss: -0.19626981019973755
Batch 32/64 loss: -0.26081523299217224
Batch 33/64 loss: -0.26036158204078674
Batch 34/64 loss: -0.3027830719947815
Batch 35/64 loss: -0.24500536918640137
Batch 36/64 loss: -0.2724376320838928
Batch 37/64 loss: -0.272733211517334
Batch 38/64 loss: -0.2340318262577057
Batch 39/64 loss: -0.2278781533241272
Batch 40/64 loss: -0.27386903762817383
Batch 41/64 loss: -0.2812655568122864
Batch 42/64 loss: -0.22142016887664795
Batch 43/64 loss: -0.25634050369262695
Batch 44/64 loss: -0.23715627193450928
Batch 45/64 loss: -0.24592512845993042
Batch 46/64 loss: -0.28078392148017883
Batch 47/64 loss: -0.24177908897399902
Batch 48/64 loss: -0.24683767557144165
Batch 49/64 loss: -0.2223307490348816
Batch 50/64 loss: -0.24232450127601624
Batch 51/64 loss: -0.24405401945114136
Batch 52/64 loss: -0.22488200664520264
Batch 53/64 loss: -0.25176116824150085
Batch 54/64 loss: -0.23958030343055725
Batch 55/64 loss: -0.261771023273468
Batch 56/64 loss: -0.28638172149658203
Batch 57/64 loss: -0.2673131823539734
Batch 58/64 loss: -0.2663041949272156
Batch 59/64 loss: -0.20682328939437866
Batch 60/64 loss: -0.27150094509124756
Batch 61/64 loss: -0.2714046835899353
Batch 62/64 loss: -0.26886069774627686
Batch 63/64 loss: -0.2500866651535034
Batch 64/64 loss: -0.28861087560653687
Epoch 203  Train loss: -0.2508775914416594  Val loss: 0.01980088707507681
Epoch 204
-------------------------------
Batch 1/64 loss: -0.2657665014266968
Batch 2/64 loss: -0.23827236890792847
Batch 3/64 loss: -0.2585707902908325
Batch 4/64 loss: -0.2800328731536865
Batch 5/64 loss: -0.29213786125183105
Batch 6/64 loss: -0.26661670207977295
Batch 7/64 loss: -0.23665639758110046
Batch 8/64 loss: -0.29251620173454285
Batch 9/64 loss: -0.22420144081115723
Batch 10/64 loss: -0.2627156376838684
Batch 11/64 loss: -0.27382370829582214
Batch 12/64 loss: -0.2813622057437897
Batch 13/64 loss: -0.2638038694858551
Batch 14/64 loss: -0.25234749913215637
Batch 15/64 loss: -0.2545127868652344
Batch 16/64 loss: -0.240452378988266
Batch 17/64 loss: -0.2729363441467285
Batch 18/64 loss: -0.26844924688339233
Batch 19/64 loss: -0.2559337317943573
Batch 20/64 loss: -0.2628685534000397
Batch 21/64 loss: -0.24737203121185303
Batch 22/64 loss: -0.23992574214935303
Batch 23/64 loss: -0.22322136163711548
Batch 24/64 loss: -0.2489403486251831
Batch 25/64 loss: -0.2612534761428833
Batch 26/64 loss: -0.2877795398235321
Batch 27/64 loss: -0.23483586311340332
Batch 28/64 loss: -0.2398255467414856
Batch 29/64 loss: -0.277895987033844
Batch 30/64 loss: -0.22943350672721863
Batch 31/64 loss: -0.2694702744483948
Batch 32/64 loss: -0.2601691484451294
Batch 33/64 loss: -0.2844173312187195
Batch 34/64 loss: -0.2648998498916626
Batch 35/64 loss: -0.2555459141731262
Batch 36/64 loss: -0.2491394579410553
Batch 37/64 loss: -0.28036952018737793
Batch 38/64 loss: -0.19244247674942017
Batch 39/64 loss: -0.23961767554283142
Batch 40/64 loss: -0.24960541725158691
Batch 41/64 loss: -0.26360565423965454
Batch 42/64 loss: -0.2691456079483032
Batch 43/64 loss: -0.2789319157600403
Batch 44/64 loss: -0.26057755947113037
Batch 45/64 loss: -0.27059274911880493
Batch 46/64 loss: -0.253801167011261
Batch 47/64 loss: -0.26693207025527954
Batch 48/64 loss: -0.2096448540687561
Batch 49/64 loss: -0.2695581912994385
Batch 50/64 loss: -0.2886071801185608
Batch 51/64 loss: -0.2459455132484436
Batch 52/64 loss: -0.28369927406311035
Batch 53/64 loss: -0.24122488498687744
Batch 54/64 loss: -0.26535239815711975
Batch 55/64 loss: -0.23919859528541565
Batch 56/64 loss: -0.2773979902267456
Batch 57/64 loss: -0.26757168769836426
Batch 58/64 loss: -0.2627638280391693
Batch 59/64 loss: -0.17760252952575684
Batch 60/64 loss: -0.2588636875152588
Batch 61/64 loss: -0.24372410774230957
Batch 62/64 loss: -0.2849269211292267
Batch 63/64 loss: -0.24104973673820496
Batch 64/64 loss: -0.22248399257659912
Epoch 204  Train loss: -0.25674852810653986  Val loss: 0.023166931371918248
Epoch 205
-------------------------------
Batch 1/64 loss: -0.2739429473876953
Batch 2/64 loss: -0.2229473888874054
Batch 3/64 loss: -0.27482616901397705
Batch 4/64 loss: -0.2603456974029541
Batch 5/64 loss: -0.22420895099639893
Batch 6/64 loss: -0.23031818866729736
Batch 7/64 loss: -0.2807401120662689
Batch 8/64 loss: -0.2569119334220886
Batch 9/64 loss: -0.24638310074806213
Batch 10/64 loss: -0.23691442608833313
Batch 11/64 loss: -0.2857658267021179
Batch 12/64 loss: -0.26512977480888367
Batch 13/64 loss: -0.2490043044090271
Batch 14/64 loss: -0.28501445055007935
Batch 15/64 loss: -0.2610417902469635
Batch 16/64 loss: -0.2576512396335602
Batch 17/64 loss: -0.27458465099334717
Batch 18/64 loss: -0.2427319884300232
Batch 19/64 loss: -0.24897867441177368
Batch 20/64 loss: -0.260932058095932
Batch 21/64 loss: -0.23148325085639954
Batch 22/64 loss: -0.2490553855895996
Batch 23/64 loss: -0.25593435764312744
Batch 24/64 loss: -0.2550538182258606
Batch 25/64 loss: -0.2720637321472168
Batch 26/64 loss: -0.25230270624160767
Batch 27/64 loss: -0.2610068917274475
Batch 28/64 loss: -0.24144574999809265
Batch 29/64 loss: -0.2479781210422516
Batch 30/64 loss: -0.2548447549343109
Batch 31/64 loss: -0.2622140347957611
Batch 32/64 loss: -0.2815534770488739
Batch 33/64 loss: -0.22409486770629883
Batch 34/64 loss: -0.27032729983329773
Batch 35/64 loss: -0.2275581657886505
Batch 36/64 loss: -0.26395273208618164
Batch 37/64 loss: -0.25419938564300537
Batch 38/64 loss: -0.244022399187088
Batch 39/64 loss: -0.2553773820400238
Batch 40/64 loss: -0.2704145908355713
Batch 41/64 loss: -0.24095124006271362
Batch 42/64 loss: -0.2829362750053406
Batch 43/64 loss: -0.26841649413108826
Batch 44/64 loss: -0.2654346823692322
Batch 45/64 loss: -0.26419174671173096
Batch 46/64 loss: -0.24010449647903442
Batch 47/64 loss: -0.20640665292739868
Batch 48/64 loss: -0.25568071007728577
Batch 49/64 loss: -0.25378894805908203
Batch 50/64 loss: -0.25272637605667114
Batch 51/64 loss: -0.2269364595413208
Batch 52/64 loss: -0.24729764461517334
Batch 53/64 loss: -0.2380717694759369
Batch 54/64 loss: -0.25211167335510254
Batch 55/64 loss: -0.28542637825012207
Batch 56/64 loss: -0.2670646905899048
Batch 57/64 loss: -0.2446994185447693
Batch 58/64 loss: -0.2449105978012085
Batch 59/64 loss: -0.2397679090499878
Batch 60/64 loss: -0.2595444321632385
Batch 61/64 loss: -0.2704031467437744
Batch 62/64 loss: -0.2663382887840271
Batch 63/64 loss: -0.26390695571899414
Batch 64/64 loss: -0.2372487187385559
Epoch 205  Train loss: -0.25456173022588097  Val loss: 0.02271717930167811
Epoch 206
-------------------------------
Batch 1/64 loss: -0.2357902228832245
Batch 2/64 loss: -0.27361252903938293
Batch 3/64 loss: -0.2528534531593323
Batch 4/64 loss: -0.24257558584213257
Batch 5/64 loss: -0.2711133360862732
Batch 6/64 loss: -0.2624166011810303
Batch 7/64 loss: -0.2831401824951172
Batch 8/64 loss: -0.2662965655326843
Batch 9/64 loss: -0.26920002698898315
Batch 10/64 loss: -0.283555805683136
Batch 11/64 loss: -0.2674899995326996
Batch 12/64 loss: -0.26131144165992737
Batch 13/64 loss: -0.257435142993927
Batch 14/64 loss: -0.2646615505218506
Batch 15/64 loss: -0.24669793248176575
Batch 16/64 loss: -0.24601325392723083
Batch 17/64 loss: -0.25721901655197144
Batch 18/64 loss: -0.2611526846885681
Batch 19/64 loss: -0.2765544354915619
Batch 20/64 loss: -0.25735288858413696
Batch 21/64 loss: -0.27228543162345886
Batch 22/64 loss: -0.2653368413448334
Batch 23/64 loss: -0.27507925033569336
Batch 24/64 loss: -0.25560325384140015
Batch 25/64 loss: -0.27180996537208557
Batch 26/64 loss: -0.2613643407821655
Batch 27/64 loss: -0.2352239489555359
Batch 28/64 loss: -0.254623144865036
Batch 29/64 loss: -0.278545081615448
Batch 30/64 loss: -0.2638999819755554
Batch 31/64 loss: -0.2507978081703186
Batch 32/64 loss: -0.238067626953125
Batch 33/64 loss: -0.2485310435295105
Batch 34/64 loss: -0.28859615325927734
Batch 35/64 loss: -0.263888418674469
Batch 36/64 loss: -0.2684345543384552
Batch 37/64 loss: -0.2537417411804199
Batch 38/64 loss: -0.25908607244491577
Batch 39/64 loss: -0.2352583408355713
Batch 40/64 loss: -0.2900630831718445
Batch 41/64 loss: -0.24654817581176758
Batch 42/64 loss: -0.249306321144104
Batch 43/64 loss: -0.24560502171516418
Batch 44/64 loss: -0.2816121578216553
Batch 45/64 loss: -0.22879436612129211
Batch 46/64 loss: -0.25636476278305054
Batch 47/64 loss: -0.24618315696716309
Batch 48/64 loss: -0.24219053983688354
Batch 49/64 loss: -0.23311388492584229
Batch 50/64 loss: -0.21114403009414673
Batch 51/64 loss: -0.26830726861953735
Batch 52/64 loss: -0.2743723392486572
Batch 53/64 loss: -0.272156685590744
Batch 54/64 loss: -0.23736485838890076
Batch 55/64 loss: -0.2665591239929199
Batch 56/64 loss: -0.2600520849227905
Batch 57/64 loss: -0.23595106601715088
Batch 58/64 loss: -0.21486783027648926
Batch 59/64 loss: -0.24710559844970703
Batch 60/64 loss: -0.2519889175891876
Batch 61/64 loss: -0.22363388538360596
Batch 62/64 loss: -0.23160552978515625
Batch 63/64 loss: -0.28541117906570435
Batch 64/64 loss: -0.2100028395652771
Epoch 206  Train loss: -0.2562261906324648  Val loss: 0.021313337525960915
Epoch 207
-------------------------------
Batch 1/64 loss: -0.23318898677825928
Batch 2/64 loss: -0.2562941610813141
Batch 3/64 loss: -0.2762863039970398
Batch 4/64 loss: -0.24686038494110107
Batch 5/64 loss: -0.2355118691921234
Batch 6/64 loss: -0.2767804265022278
Batch 7/64 loss: -0.2850821018218994
Batch 8/64 loss: -0.2632296681404114
Batch 9/64 loss: -0.2553453743457794
Batch 10/64 loss: -0.2955680191516876
Batch 11/64 loss: -0.2656800150871277
Batch 12/64 loss: -0.2412741780281067
Batch 13/64 loss: -0.27626389265060425
Batch 14/64 loss: -0.273667573928833
Batch 15/64 loss: -0.29180219769477844
Batch 16/64 loss: -0.2746956944465637
Batch 17/64 loss: -0.2780115604400635
Batch 18/64 loss: -0.28406694531440735
Batch 19/64 loss: -0.2640726864337921
Batch 20/64 loss: -0.2454925775527954
Batch 21/64 loss: -0.2586318850517273
Batch 22/64 loss: -0.22496896982192993
Batch 23/64 loss: -0.2357025444507599
Batch 24/64 loss: -0.27177679538726807
Batch 25/64 loss: -0.24388092756271362
Batch 26/64 loss: -0.22006791830062866
Batch 27/64 loss: -0.2633422613143921
Batch 28/64 loss: -0.23236089944839478
Batch 29/64 loss: -0.24954771995544434
Batch 30/64 loss: -0.20927667617797852
Batch 31/64 loss: -0.2599759101867676
Batch 32/64 loss: -0.2170259952545166
Batch 33/64 loss: -0.28593048453330994
Batch 34/64 loss: -0.2708805501461029
Batch 35/64 loss: -0.2762281596660614
Batch 36/64 loss: -0.2349063754081726
Batch 37/64 loss: -0.26123982667922974
Batch 38/64 loss: -0.251914381980896
Batch 39/64 loss: -0.2256631851196289
Batch 40/64 loss: -0.28743308782577515
Batch 41/64 loss: -0.2593311369419098
Batch 42/64 loss: -0.2662460207939148
Batch 43/64 loss: -0.22731751203536987
Batch 44/64 loss: -0.28737765550613403
Batch 45/64 loss: -0.2261301875114441
Batch 46/64 loss: -0.2828892469406128
Batch 47/64 loss: -0.26207345724105835
Batch 48/64 loss: -0.2869560420513153
Batch 49/64 loss: -0.24600327014923096
Batch 50/64 loss: -0.28109413385391235
Batch 51/64 loss: -0.24576979875564575
Batch 52/64 loss: -0.279616117477417
Batch 53/64 loss: -0.2445683479309082
Batch 54/64 loss: -0.25169193744659424
Batch 55/64 loss: -0.2631106972694397
Batch 56/64 loss: -0.2690984010696411
Batch 57/64 loss: -0.23428785800933838
Batch 58/64 loss: -0.25982344150543213
Batch 59/64 loss: -0.23632684350013733
Batch 60/64 loss: -0.24052107334136963
Batch 61/64 loss: -0.23526573181152344
Batch 62/64 loss: -0.25596094131469727
Batch 63/64 loss: -0.27106189727783203
Batch 64/64 loss: -0.22228527069091797
Epoch 207  Train loss: -0.2569280764635871  Val loss: 0.02176248740494456
Epoch 208
-------------------------------
Batch 1/64 loss: -0.26464933156967163
Batch 2/64 loss: -0.23590707778930664
Batch 3/64 loss: -0.2714138627052307
Batch 4/64 loss: -0.2811206877231598
Batch 5/64 loss: -0.28257519006729126
Batch 6/64 loss: -0.2806548476219177
Batch 7/64 loss: -0.26378488540649414
Batch 8/64 loss: -0.26560157537460327
Batch 9/64 loss: -0.2451552152633667
Batch 10/64 loss: -0.2695399224758148
Batch 11/64 loss: -0.29425573348999023
Batch 12/64 loss: -0.2771601676940918
Batch 13/64 loss: -0.24127304553985596
Batch 14/64 loss: -0.26446807384490967
Batch 15/64 loss: -0.2529456615447998
Batch 16/64 loss: -0.270742803812027
Batch 17/64 loss: -0.26451319456100464
Batch 18/64 loss: -0.24839860200881958
Batch 19/64 loss: -0.2510853409767151
Batch 20/64 loss: -0.2648230195045471
Batch 21/64 loss: -0.22950071096420288
Batch 22/64 loss: -0.2909224033355713
Batch 23/64 loss: -0.25072580575942993
Batch 24/64 loss: -0.2482767105102539
Batch 25/64 loss: -0.2746451497077942
Batch 26/64 loss: -0.2071259617805481
Batch 27/64 loss: -0.23484939336776733
Batch 28/64 loss: -0.20885306596755981
Batch 29/64 loss: -0.2844598889350891
Batch 30/64 loss: -0.26193752884864807
Batch 31/64 loss: -0.25847649574279785
Batch 32/64 loss: -0.2612025737762451
Batch 33/64 loss: -0.2691038250923157
Batch 34/64 loss: -0.27192533016204834
Batch 35/64 loss: -0.2549661695957184
Batch 36/64 loss: -0.23828449845314026
Batch 37/64 loss: -0.20463991165161133
Batch 38/64 loss: -0.28684502840042114
Batch 39/64 loss: -0.25200068950653076
Batch 40/64 loss: -0.24534878134727478
Batch 41/64 loss: -0.20279276371002197
Batch 42/64 loss: -0.24523323774337769
Batch 43/64 loss: -0.261147677898407
Batch 44/64 loss: -0.21818649768829346
Batch 45/64 loss: -0.27483487129211426
Batch 46/64 loss: -0.260998010635376
Batch 47/64 loss: -0.2888054847717285
Batch 48/64 loss: -0.2905879020690918
Batch 49/64 loss: -0.22910648584365845
Batch 50/64 loss: -0.26859766244888306
Batch 51/64 loss: -0.25559142231941223
Batch 52/64 loss: -0.268821656703949
Batch 53/64 loss: -0.1988135576248169
Batch 54/64 loss: -0.26843875646591187
Batch 55/64 loss: -0.24490168690681458
Batch 56/64 loss: -0.25417494773864746
Batch 57/64 loss: -0.28756141662597656
Batch 58/64 loss: -0.2562217712402344
Batch 59/64 loss: -0.27020275592803955
Batch 60/64 loss: -0.26358744502067566
Batch 61/64 loss: -0.2668120861053467
Batch 62/64 loss: -0.23014026880264282
Batch 63/64 loss: -0.24738037586212158
Batch 64/64 loss: -0.24373245239257812
Epoch 208  Train loss: -0.2566258234136245  Val loss: 0.022200892061711996
Epoch 209
-------------------------------
Batch 1/64 loss: -0.2875460088253021
Batch 2/64 loss: -0.2740408778190613
Batch 3/64 loss: -0.2852109670639038
Batch 4/64 loss: -0.24563586711883545
Batch 5/64 loss: -0.2333948016166687
Batch 6/64 loss: -0.24022388458251953
Batch 7/64 loss: -0.27401208877563477
Batch 8/64 loss: -0.27175432443618774
Batch 9/64 loss: -0.24895977973937988
Batch 10/64 loss: -0.2589431405067444
Batch 11/64 loss: -0.19244492053985596
Batch 12/64 loss: -0.2466864287853241
Batch 13/64 loss: -0.27682238817214966
Batch 14/64 loss: -0.2737320363521576
Batch 15/64 loss: -0.2803889811038971
Batch 16/64 loss: -0.2629173696041107
Batch 17/64 loss: -0.2260611653327942
Batch 18/64 loss: -0.2376556098461151
Batch 19/64 loss: -0.25797581672668457
Batch 20/64 loss: -0.245308518409729
Batch 21/64 loss: -0.26749157905578613
Batch 22/64 loss: -0.267074316740036
Batch 23/64 loss: -0.2848089933395386
Batch 24/64 loss: -0.24847376346588135
Batch 25/64 loss: -0.26202359795570374
Batch 26/64 loss: -0.2812783122062683
Batch 27/64 loss: -0.26924657821655273
Batch 28/64 loss: -0.25308918952941895
Batch 29/64 loss: -0.25222623348236084
Batch 30/64 loss: -0.247918963432312
Batch 31/64 loss: -0.2303532361984253
Batch 32/64 loss: -0.24640995264053345
Batch 33/64 loss: -0.2543773651123047
Batch 34/64 loss: -0.25509876012802124
Batch 35/64 loss: -0.2854359745979309
Batch 36/64 loss: -0.27717435359954834
Batch 37/64 loss: -0.2741568982601166
Batch 38/64 loss: -0.22889560461044312
Batch 39/64 loss: -0.25978437066078186
Batch 40/64 loss: -0.26494914293289185
Batch 41/64 loss: -0.2596646547317505
Batch 42/64 loss: -0.23924344778060913
Batch 43/64 loss: -0.2401283085346222
Batch 44/64 loss: -0.2779269218444824
Batch 45/64 loss: -0.23154610395431519
Batch 46/64 loss: -0.2673460841178894
Batch 47/64 loss: -0.250542551279068
Batch 48/64 loss: -0.20049262046813965
Batch 49/64 loss: -0.21865522861480713
Batch 50/64 loss: -0.2636198401451111
Batch 51/64 loss: -0.27197545766830444
Batch 52/64 loss: -0.22535958886146545
Batch 53/64 loss: -0.2774888873100281
Batch 54/64 loss: -0.24424511194229126
Batch 55/64 loss: -0.22917205095291138
Batch 56/64 loss: -0.2916547954082489
Batch 57/64 loss: -0.2819675803184509
Batch 58/64 loss: -0.25586003065109253
Batch 59/64 loss: -0.27366888523101807
Batch 60/64 loss: -0.2842978239059448
Batch 61/64 loss: -0.23628365993499756
Batch 62/64 loss: -0.27021923661231995
Batch 63/64 loss: -0.24145501852035522
Batch 64/64 loss: -0.24561423063278198
Epoch 209  Train loss: -0.25642363487505443  Val loss: 0.01886792506548957
Epoch 210
-------------------------------
Batch 1/64 loss: -0.25151604413986206
Batch 2/64 loss: -0.2446194291114807
Batch 3/64 loss: -0.2825905978679657
Batch 4/64 loss: -0.239260733127594
Batch 5/64 loss: -0.2623298764228821
Batch 6/64 loss: -0.24348503351211548
Batch 7/64 loss: -0.2656746208667755
Batch 8/64 loss: -0.25834953784942627
Batch 9/64 loss: -0.2832486629486084
Batch 10/64 loss: -0.25597721338272095
Batch 11/64 loss: -0.2950345575809479
Batch 12/64 loss: -0.2740364670753479
Batch 13/64 loss: -0.2616550326347351
Batch 14/64 loss: -0.2504758834838867
Batch 15/64 loss: -0.24029728770256042
Batch 16/64 loss: -0.25892263650894165
Batch 17/64 loss: -0.2520374655723572
Batch 18/64 loss: -0.23517873883247375
Batch 19/64 loss: -0.2571380138397217
Batch 20/64 loss: -0.2495817244052887
Batch 21/64 loss: -0.2843610346317291
Batch 22/64 loss: -0.22468939423561096
Batch 23/64 loss: -0.2848324179649353
Batch 24/64 loss: -0.26970791816711426
Batch 25/64 loss: -0.28113222122192383
Batch 26/64 loss: -0.2649461627006531
Batch 27/64 loss: -0.2522127330303192
Batch 28/64 loss: -0.2528781592845917
Batch 29/64 loss: -0.2501615285873413
Batch 30/64 loss: -0.29544028639793396
Batch 31/64 loss: -0.22200626134872437
Batch 32/64 loss: -0.2774807810783386
Batch 33/64 loss: -0.2941853404045105
Batch 34/64 loss: -0.23866894841194153
Batch 35/64 loss: -0.26627081632614136
Batch 36/64 loss: -0.26149481534957886
Batch 37/64 loss: -0.26415756344795227
Batch 38/64 loss: -0.2645799517631531
Batch 39/64 loss: -0.26827067136764526
Batch 40/64 loss: -0.2793375849723816
Batch 41/64 loss: -0.23288220167160034
Batch 42/64 loss: -0.26475220918655396
Batch 43/64 loss: -0.24585425853729248
Batch 44/64 loss: -0.2557229697704315
Batch 45/64 loss: -0.26514920592308044
Batch 46/64 loss: -0.27752023935317993
Batch 47/64 loss: -0.2561725080013275
Batch 48/64 loss: -0.24572110176086426
Batch 49/64 loss: -0.29521599411964417
Batch 50/64 loss: -0.2943926751613617
Batch 51/64 loss: -0.2552995979785919
Batch 52/64 loss: -0.25231000781059265
Batch 53/64 loss: -0.2749345302581787
Batch 54/64 loss: -0.23229482769966125
Batch 55/64 loss: -0.2831194996833801
Batch 56/64 loss: -0.2830047905445099
Batch 57/64 loss: -0.2452256977558136
Batch 58/64 loss: -0.2625930607318878
Batch 59/64 loss: -0.2226141095161438
Batch 60/64 loss: -0.26731008291244507
Batch 61/64 loss: -0.2152063250541687
Batch 62/64 loss: -0.2763814628124237
Batch 63/64 loss: -0.29531046748161316
Batch 64/64 loss: -0.2322137951850891
Epoch 210  Train loss: -0.26088427164975336  Val loss: 0.02557589897175425
Epoch 211
-------------------------------
Batch 1/64 loss: -0.2583577632904053
Batch 2/64 loss: -0.26899558305740356
Batch 3/64 loss: -0.27204591035842896
Batch 4/64 loss: -0.28383669257164
Batch 5/64 loss: -0.26550525426864624
Batch 6/64 loss: -0.2702503502368927
Batch 7/64 loss: -0.25141605734825134
Batch 8/64 loss: -0.23803681135177612
Batch 9/64 loss: -0.23537394404411316
Batch 10/64 loss: -0.2732553482055664
Batch 11/64 loss: -0.27602165937423706
Batch 12/64 loss: -0.2563052177429199
Batch 13/64 loss: -0.29762959480285645
Batch 14/64 loss: -0.24444282054901123
Batch 15/64 loss: -0.28350526094436646
Batch 16/64 loss: -0.28096747398376465
Batch 17/64 loss: -0.2927917242050171
Batch 18/64 loss: -0.26317107677459717
Batch 19/64 loss: -0.23399916291236877
Batch 20/64 loss: -0.23813346028327942
Batch 21/64 loss: -0.2723468244075775
Batch 22/64 loss: -0.2724248766899109
Batch 23/64 loss: -0.2518255412578583
Batch 24/64 loss: -0.26716330647468567
Batch 25/64 loss: -0.28531646728515625
Batch 26/64 loss: -0.27248477935791016
Batch 27/64 loss: -0.24507591128349304
Batch 28/64 loss: -0.2706834673881531
Batch 29/64 loss: -0.2595447301864624
Batch 30/64 loss: -0.23322954773902893
Batch 31/64 loss: -0.22839510440826416
Batch 32/64 loss: -0.25873008370399475
Batch 33/64 loss: -0.27542591094970703
Batch 34/64 loss: -0.21988427639007568
Batch 35/64 loss: -0.2555673122406006
Batch 36/64 loss: -0.2539013624191284
Batch 37/64 loss: -0.2119331955909729
Batch 38/64 loss: -0.22474661469459534
Batch 39/64 loss: -0.2520736753940582
Batch 40/64 loss: -0.24864810705184937
Batch 41/64 loss: -0.20997118949890137
Batch 42/64 loss: -0.23977097868919373
Batch 43/64 loss: -0.2508840560913086
Batch 44/64 loss: -0.24913227558135986
Batch 45/64 loss: -0.24842607975006104
Batch 46/64 loss: -0.22974061965942383
Batch 47/64 loss: -0.2393946349620819
Batch 48/64 loss: -0.21605199575424194
Batch 49/64 loss: -0.2577081322669983
Batch 50/64 loss: -0.2021564245223999
Batch 51/64 loss: -0.23623573780059814
Batch 52/64 loss: -0.28539589047431946
Batch 53/64 loss: -0.2524973750114441
Batch 54/64 loss: -0.2513555586338043
Batch 55/64 loss: -0.2817063331604004
Batch 56/64 loss: -0.2326245903968811
Batch 57/64 loss: -0.25914353132247925
Batch 58/64 loss: -0.22092682123184204
Batch 59/64 loss: -0.2858899235725403
Batch 60/64 loss: -0.2525746524333954
Batch 61/64 loss: -0.2855994701385498
Batch 62/64 loss: -0.2810283303260803
Batch 63/64 loss: -0.2707005739212036
Batch 64/64 loss: -0.18523788452148438
Epoch 211  Train loss: -0.2544515427421121  Val loss: 0.02297933159005601
Epoch 212
-------------------------------
Batch 1/64 loss: -0.23632660508155823
Batch 2/64 loss: -0.2850835919380188
Batch 3/64 loss: -0.27091744542121887
Batch 4/64 loss: -0.259149432182312
Batch 5/64 loss: -0.29557889699935913
Batch 6/64 loss: -0.26380282640457153
Batch 7/64 loss: -0.25064027309417725
Batch 8/64 loss: -0.2530630826950073
Batch 9/64 loss: -0.25805333256721497
Batch 10/64 loss: -0.2691608667373657
Batch 11/64 loss: -0.2590119242668152
Batch 12/64 loss: -0.2710587680339813
Batch 13/64 loss: -0.25742894411087036
Batch 14/64 loss: -0.2970421612262726
Batch 15/64 loss: -0.25443851947784424
Batch 16/64 loss: -0.2694467008113861
Batch 17/64 loss: -0.2711693048477173
Batch 18/64 loss: -0.1966007947921753
Batch 19/64 loss: -0.2599654793739319
Batch 20/64 loss: -0.2582717537879944
Batch 21/64 loss: -0.23400861024856567
Batch 22/64 loss: -0.2782399356365204
Batch 23/64 loss: -0.2376912534236908
Batch 24/64 loss: -0.2627864480018616
Batch 25/64 loss: -0.2883085012435913
Batch 26/64 loss: -0.25571247935295105
Batch 27/64 loss: -0.2766161262989044
Batch 28/64 loss: -0.24016976356506348
Batch 29/64 loss: -0.2502545118331909
Batch 30/64 loss: -0.27615395188331604
Batch 31/64 loss: -0.28361397981643677
Batch 32/64 loss: -0.2637544274330139
Batch 33/64 loss: -0.22560322284698486
Batch 34/64 loss: -0.2716541886329651
Batch 35/64 loss: -0.2513180673122406
Batch 36/64 loss: -0.21734124422073364
Batch 37/64 loss: -0.24893933534622192
Batch 38/64 loss: -0.2811727821826935
Batch 39/64 loss: -0.2741875648498535
Batch 40/64 loss: -0.24714791774749756
Batch 41/64 loss: -0.24365615844726562
Batch 42/64 loss: -0.2739247977733612
Batch 43/64 loss: -0.23914751410484314
Batch 44/64 loss: -0.2441292107105255
Batch 45/64 loss: -0.2587798237800598
Batch 46/64 loss: -0.26162809133529663
Batch 47/64 loss: -0.2943874001502991
Batch 48/64 loss: -0.2625001072883606
Batch 49/64 loss: -0.22913706302642822
Batch 50/64 loss: -0.2771155536174774
Batch 51/64 loss: -0.2535015344619751
Batch 52/64 loss: -0.2522624135017395
Batch 53/64 loss: -0.2564169764518738
Batch 54/64 loss: -0.2587973475456238
Batch 55/64 loss: -0.23402738571166992
Batch 56/64 loss: -0.25631701946258545
Batch 57/64 loss: -0.250862181186676
Batch 58/64 loss: -0.2864281237125397
Batch 59/64 loss: -0.28980427980422974
Batch 60/64 loss: -0.23694872856140137
Batch 61/64 loss: -0.2677507996559143
Batch 62/64 loss: -0.25659534335136414
Batch 63/64 loss: -0.26439356803894043
Batch 64/64 loss: -0.27487748861312866
Epoch 212  Train loss: -0.2596949733939825  Val loss: 0.022963851178224963
Epoch 213
-------------------------------
Batch 1/64 loss: -0.26747453212738037
Batch 2/64 loss: -0.24602046608924866
Batch 3/64 loss: -0.24906614422798157
Batch 4/64 loss: -0.2740172743797302
Batch 5/64 loss: -0.2340162992477417
Batch 6/64 loss: -0.2746279537677765
Batch 7/64 loss: -0.26518410444259644
Batch 8/64 loss: -0.29736167192459106
Batch 9/64 loss: -0.2754587233066559
Batch 10/64 loss: -0.27441033720970154
Batch 11/64 loss: -0.22969913482666016
Batch 12/64 loss: -0.25731369853019714
Batch 13/64 loss: -0.2830970883369446
Batch 14/64 loss: -0.2768825590610504
Batch 15/64 loss: -0.24252718687057495
Batch 16/64 loss: -0.287544310092926
Batch 17/64 loss: -0.3003767728805542
Batch 18/64 loss: -0.2719460427761078
Batch 19/64 loss: -0.26166167855262756
Batch 20/64 loss: -0.2820568382740021
Batch 21/64 loss: -0.23812401294708252
Batch 22/64 loss: -0.2642750144004822
Batch 23/64 loss: -0.27610188722610474
Batch 24/64 loss: -0.2363957166671753
Batch 25/64 loss: -0.26157087087631226
Batch 26/64 loss: -0.269589364528656
Batch 27/64 loss: -0.25879645347595215
Batch 28/64 loss: -0.26859837770462036
Batch 29/64 loss: -0.25358909368515015
Batch 30/64 loss: -0.2609780430793762
Batch 31/64 loss: -0.23735061287879944
Batch 32/64 loss: -0.2853197157382965
Batch 33/64 loss: -0.2964187562465668
Batch 34/64 loss: -0.2193976640701294
Batch 35/64 loss: -0.2478368878364563
Batch 36/64 loss: -0.2647985517978668
Batch 37/64 loss: -0.2917778193950653
Batch 38/64 loss: -0.2806856036186218
Batch 39/64 loss: -0.27299490571022034
Batch 40/64 loss: -0.2336694598197937
Batch 41/64 loss: -0.2872018814086914
Batch 42/64 loss: -0.2552107870578766
Batch 43/64 loss: -0.2614988684654236
Batch 44/64 loss: -0.25974923372268677
Batch 45/64 loss: -0.24153608083724976
Batch 46/64 loss: -0.2562752366065979
Batch 47/64 loss: -0.2781984806060791
Batch 48/64 loss: -0.27468550205230713
Batch 49/64 loss: -0.2486225962638855
Batch 50/64 loss: -0.2714206874370575
Batch 51/64 loss: -0.23753058910369873
Batch 52/64 loss: -0.24729135632514954
Batch 53/64 loss: -0.28183913230895996
Batch 54/64 loss: -0.23209461569786072
Batch 55/64 loss: -0.2014002799987793
Batch 56/64 loss: -0.2525164484977722
Batch 57/64 loss: -0.24330806732177734
Batch 58/64 loss: -0.24589169025421143
Batch 59/64 loss: -0.26947668194770813
Batch 60/64 loss: -0.2501259744167328
Batch 61/64 loss: -0.2726479172706604
Batch 62/64 loss: -0.24736037850379944
Batch 63/64 loss: -0.25385499000549316
Batch 64/64 loss: -0.2563575506210327
Epoch 213  Train loss: -0.2608787022384943  Val loss: 0.021906452899945972
Epoch 214
-------------------------------
Batch 1/64 loss: -0.3004567623138428
Batch 2/64 loss: -0.2504740357398987
Batch 3/64 loss: -0.29015910625457764
Batch 4/64 loss: -0.24939322471618652
Batch 5/64 loss: -0.24749475717544556
Batch 6/64 loss: -0.28124111890792847
Batch 7/64 loss: -0.2946844696998596
Batch 8/64 loss: -0.2941776514053345
Batch 9/64 loss: -0.25359129905700684
Batch 10/64 loss: -0.22845491766929626
Batch 11/64 loss: -0.27353978157043457
Batch 12/64 loss: -0.2748035192489624
Batch 13/64 loss: -0.2637293338775635
Batch 14/64 loss: -0.26941752433776855
Batch 15/64 loss: -0.25241559743881226
Batch 16/64 loss: -0.27851274609565735
Batch 17/64 loss: -0.2987332344055176
Batch 18/64 loss: -0.24996274709701538
Batch 19/64 loss: -0.22775423526763916
Batch 20/64 loss: -0.26669448614120483
Batch 21/64 loss: -0.27139681577682495
Batch 22/64 loss: -0.26470279693603516
Batch 23/64 loss: -0.26874077320098877
Batch 24/64 loss: -0.26921021938323975
Batch 25/64 loss: -0.2535768151283264
Batch 26/64 loss: -0.26767194271087646
Batch 27/64 loss: -0.25519105792045593
Batch 28/64 loss: -0.280316561460495
Batch 29/64 loss: -0.2604045867919922
Batch 30/64 loss: -0.2716729938983917
Batch 31/64 loss: -0.24950385093688965
Batch 32/64 loss: -0.2789236307144165
Batch 33/64 loss: -0.24130576848983765
Batch 34/64 loss: -0.29727882146835327
Batch 35/64 loss: -0.24732738733291626
Batch 36/64 loss: -0.24103283882141113
Batch 37/64 loss: -0.2596437931060791
Batch 38/64 loss: -0.26312658190727234
Batch 39/64 loss: -0.2577745318412781
Batch 40/64 loss: -0.26975125074386597
Batch 41/64 loss: -0.26707321405410767
Batch 42/64 loss: -0.24657195806503296
Batch 43/64 loss: -0.2604689598083496
Batch 44/64 loss: -0.3062392473220825
Batch 45/64 loss: -0.26975756883621216
Batch 46/64 loss: -0.24513959884643555
Batch 47/64 loss: -0.22882309556007385
Batch 48/64 loss: -0.22158312797546387
Batch 49/64 loss: -0.2408682107925415
Batch 50/64 loss: -0.2634051442146301
Batch 51/64 loss: -0.28820106387138367
Batch 52/64 loss: -0.2446083426475525
Batch 53/64 loss: -0.2591654062271118
Batch 54/64 loss: -0.2143266797065735
Batch 55/64 loss: -0.2641022801399231
Batch 56/64 loss: -0.25787392258644104
Batch 57/64 loss: -0.23809587955474854
Batch 58/64 loss: -0.275776743888855
Batch 59/64 loss: -0.27546197175979614
Batch 60/64 loss: -0.30455291271209717
Batch 61/64 loss: -0.2535642385482788
Batch 62/64 loss: -0.223696768283844
Batch 63/64 loss: -0.2635144591331482
Batch 64/64 loss: -0.23127436637878418
Epoch 214  Train loss: -0.2619697276283713  Val loss: 0.02342979195191688
Epoch 215
-------------------------------
Batch 1/64 loss: -0.2954762578010559
Batch 2/64 loss: -0.2582016587257385
Batch 3/64 loss: -0.2652817666530609
Batch 4/64 loss: -0.29718929529190063
Batch 5/64 loss: -0.2788642644882202
Batch 6/64 loss: -0.2752244472503662
Batch 7/64 loss: -0.19581270217895508
Batch 8/64 loss: -0.24748313426971436
Batch 9/64 loss: -0.2889193892478943
Batch 10/64 loss: -0.2649937570095062
Batch 11/64 loss: -0.27560701966285706
Batch 12/64 loss: -0.2965075671672821
Batch 13/64 loss: -0.30849650502204895
Batch 14/64 loss: -0.29512307047843933
Batch 15/64 loss: -0.29958534240722656
Batch 16/64 loss: -0.27025121450424194
Batch 17/64 loss: -0.2754962146282196
Batch 18/64 loss: -0.2894909381866455
Batch 19/64 loss: -0.20418232679367065
Batch 20/64 loss: -0.2909561097621918
Batch 21/64 loss: -0.21077179908752441
Batch 22/64 loss: -0.2931011915206909
Batch 23/64 loss: -0.2835286855697632
Batch 24/64 loss: -0.2677913010120392
Batch 25/64 loss: -0.2771364152431488
Batch 26/64 loss: -0.2780272662639618
Batch 27/64 loss: -0.2666788697242737
Batch 28/64 loss: -0.2604895532131195
Batch 29/64 loss: -0.2722324728965759
Batch 30/64 loss: -0.27265816926956177
Batch 31/64 loss: -0.2652425765991211
Batch 32/64 loss: -0.22692030668258667
Batch 33/64 loss: -0.27437227964401245
Batch 34/64 loss: -0.24791884422302246
Batch 35/64 loss: -0.270818829536438
Batch 36/64 loss: -0.26474493741989136
Batch 37/64 loss: -0.27147895097732544
Batch 38/64 loss: -0.26121699810028076
Batch 39/64 loss: -0.29914021492004395
Batch 40/64 loss: -0.29224443435668945
Batch 41/64 loss: -0.2575346827507019
Batch 42/64 loss: -0.22922861576080322
Batch 43/64 loss: -0.2635292410850525
Batch 44/64 loss: -0.27125316858291626
Batch 45/64 loss: -0.28958970308303833
Batch 46/64 loss: -0.2927134335041046
Batch 47/64 loss: -0.25416815280914307
Batch 48/64 loss: -0.24591901898384094
Batch 49/64 loss: -0.23029595613479614
Batch 50/64 loss: -0.23073884844779968
Batch 51/64 loss: -0.23915624618530273
Batch 52/64 loss: -0.27267175912857056
Batch 53/64 loss: -0.26195284724235535
Batch 54/64 loss: -0.2309657335281372
Batch 55/64 loss: -0.2546648383140564
Batch 56/64 loss: -0.2582811117172241
Batch 57/64 loss: -0.28631675243377686
Batch 58/64 loss: -0.264823853969574
Batch 59/64 loss: -0.25663286447525024
Batch 60/64 loss: -0.24556055665016174
Batch 61/64 loss: -0.26264238357543945
Batch 62/64 loss: -0.2219257354736328
Batch 63/64 loss: -0.2759961485862732
Batch 64/64 loss: -0.26894962787628174
Epoch 215  Train loss: -0.2655361717822505  Val loss: 0.022007175122749356
Epoch 216
-------------------------------
Batch 1/64 loss: -0.2533056139945984
Batch 2/64 loss: -0.28467893600463867
Batch 3/64 loss: -0.2768987715244293
Batch 4/64 loss: -0.2457299530506134
Batch 5/64 loss: -0.23016977310180664
Batch 6/64 loss: -0.2683090567588806
Batch 7/64 loss: -0.2535805404186249
Batch 8/64 loss: -0.2817087173461914
Batch 9/64 loss: -0.2888489365577698
Batch 10/64 loss: -0.2670084834098816
Batch 11/64 loss: -0.28139084577560425
Batch 12/64 loss: -0.2295381724834442
Batch 13/64 loss: -0.26986467838287354
Batch 14/64 loss: -0.27542373538017273
Batch 15/64 loss: -0.2633807957172394
Batch 16/64 loss: -0.27344897389411926
Batch 17/64 loss: -0.28503093123435974
Batch 18/64 loss: -0.28572437167167664
Batch 19/64 loss: -0.24049943685531616
Batch 20/64 loss: -0.29611343145370483
Batch 21/64 loss: -0.29651156067848206
Batch 22/64 loss: -0.27596515417099
Batch 23/64 loss: -0.2469327747821808
Batch 24/64 loss: -0.27657562494277954
Batch 25/64 loss: -0.2804384231567383
Batch 26/64 loss: -0.2618491053581238
Batch 27/64 loss: -0.26510393619537354
Batch 28/64 loss: -0.23917853832244873
Batch 29/64 loss: -0.2893465757369995
Batch 30/64 loss: -0.27618104219436646
Batch 31/64 loss: -0.269378125667572
Batch 32/64 loss: -0.2286939024925232
Batch 33/64 loss: -0.25593987107276917
Batch 34/64 loss: -0.2646435797214508
Batch 35/64 loss: -0.2584514021873474
Batch 36/64 loss: -0.25058960914611816
Batch 37/64 loss: -0.2479270100593567
Batch 38/64 loss: -0.2703295946121216
Batch 39/64 loss: -0.2633765637874603
Batch 40/64 loss: -0.26840144395828247
Batch 41/64 loss: -0.2629571855068207
Batch 42/64 loss: -0.24139252305030823
Batch 43/64 loss: -0.2559189796447754
Batch 44/64 loss: -0.278384268283844
Batch 45/64 loss: -0.23884934186935425
Batch 46/64 loss: -0.2654041647911072
Batch 47/64 loss: -0.2828200161457062
Batch 48/64 loss: -0.26064613461494446
Batch 49/64 loss: -0.25361835956573486
Batch 50/64 loss: -0.21748924255371094
Batch 51/64 loss: -0.3000059425830841
Batch 52/64 loss: -0.29903027415275574
Batch 53/64 loss: -0.24839100241661072
Batch 54/64 loss: -0.23419731855392456
Batch 55/64 loss: -0.26752161979675293
Batch 56/64 loss: -0.22453027963638306
Batch 57/64 loss: -0.2564745843410492
Batch 58/64 loss: -0.24493971467018127
Batch 59/64 loss: -0.2736895680427551
Batch 60/64 loss: -0.25819021463394165
Batch 61/64 loss: -0.23264387249946594
Batch 62/64 loss: -0.27444368600845337
Batch 63/64 loss: -0.24629807472229004
Batch 64/64 loss: -0.2454630434513092
Epoch 216  Train loss: -0.2625631630420685  Val loss: 0.0241537249784699
Epoch 217
-------------------------------
Batch 1/64 loss: -0.2664211392402649
Batch 2/64 loss: -0.2530970275402069
Batch 3/64 loss: -0.2859842777252197
Batch 4/64 loss: -0.23916614055633545
Batch 5/64 loss: -0.274213045835495
Batch 6/64 loss: -0.28344276547431946
Batch 7/64 loss: -0.27386635541915894
Batch 8/64 loss: -0.20009517669677734
Batch 9/64 loss: -0.29924532771110535
Batch 10/64 loss: -0.2683716118335724
Batch 11/64 loss: -0.2801169753074646
Batch 12/64 loss: -0.24269139766693115
Batch 13/64 loss: -0.2758182883262634
Batch 14/64 loss: -0.23220086097717285
Batch 15/64 loss: -0.2690889835357666
Batch 16/64 loss: -0.2918604016304016
Batch 17/64 loss: -0.3090561032295227
Batch 18/64 loss: -0.2615004777908325
Batch 19/64 loss: -0.2850503921508789
Batch 20/64 loss: -0.21929192543029785
Batch 21/64 loss: -0.23856037855148315
Batch 22/64 loss: -0.27423664927482605
Batch 23/64 loss: -0.26398855447769165
Batch 24/64 loss: -0.28064918518066406
Batch 25/64 loss: -0.23912090063095093
Batch 26/64 loss: -0.2417374849319458
Batch 27/64 loss: -0.26471033692359924
Batch 28/64 loss: -0.2539043128490448
Batch 29/64 loss: -0.25448039174079895
Batch 30/64 loss: -0.24613440036773682
Batch 31/64 loss: -0.27927932143211365
Batch 32/64 loss: -0.2885615825653076
Batch 33/64 loss: -0.2624293267726898
Batch 34/64 loss: -0.2678754925727844
Batch 35/64 loss: -0.2497977614402771
Batch 36/64 loss: -0.2567438781261444
Batch 37/64 loss: -0.2548828721046448
Batch 38/64 loss: -0.2596638798713684
Batch 39/64 loss: -0.2442290186882019
Batch 40/64 loss: -0.27703118324279785
Batch 41/64 loss: -0.26552122831344604
Batch 42/64 loss: -0.25010138750076294
Batch 43/64 loss: -0.26098892092704773
Batch 44/64 loss: -0.2548924684524536
Batch 45/64 loss: -0.2598126530647278
Batch 46/64 loss: -0.2524242699146271
Batch 47/64 loss: -0.268303245306015
Batch 48/64 loss: -0.2695155143737793
Batch 49/64 loss: -0.24839621782302856
Batch 50/64 loss: -0.29191064834594727
Batch 51/64 loss: -0.28449782729148865
Batch 52/64 loss: -0.2796953320503235
Batch 53/64 loss: -0.2635173499584198
Batch 54/64 loss: -0.2886495590209961
Batch 55/64 loss: -0.24692976474761963
Batch 56/64 loss: -0.27101635932922363
Batch 57/64 loss: -0.24262475967407227
Batch 58/64 loss: -0.26770296692848206
Batch 59/64 loss: -0.28800809383392334
Batch 60/64 loss: -0.2813003659248352
Batch 61/64 loss: -0.2553940415382385
Batch 62/64 loss: -0.2688586115837097
Batch 63/64 loss: -0.2410449981689453
Batch 64/64 loss: -0.25622180104255676
Epoch 217  Train loss: -0.26355872189297397  Val loss: 0.02034689593560917
Epoch 218
-------------------------------
Batch 1/64 loss: -0.2535441517829895
Batch 2/64 loss: -0.302846759557724
Batch 3/64 loss: -0.26589322090148926
Batch 4/64 loss: -0.2828822731971741
Batch 5/64 loss: -0.29717302322387695
Batch 6/64 loss: -0.2654867470264435
Batch 7/64 loss: -0.2799326777458191
Batch 8/64 loss: -0.25896722078323364
Batch 9/64 loss: -0.2572093605995178
Batch 10/64 loss: -0.254184365272522
Batch 11/64 loss: -0.28702789545059204
Batch 12/64 loss: -0.24674353003501892
Batch 13/64 loss: -0.29450076818466187
Batch 14/64 loss: -0.24300360679626465
Batch 15/64 loss: -0.23900222778320312
Batch 16/64 loss: -0.28089216351509094
Batch 17/64 loss: -0.20825213193893433
Batch 18/64 loss: -0.230577290058136
Batch 19/64 loss: -0.25534719228744507
Batch 20/64 loss: -0.26747411489486694
Batch 21/64 loss: -0.279498815536499
Batch 22/64 loss: -0.2481904923915863
Batch 23/64 loss: -0.26489967107772827
Batch 24/64 loss: -0.271774560213089
Batch 25/64 loss: -0.2327418029308319
Batch 26/64 loss: -0.2516343593597412
Batch 27/64 loss: -0.2979515790939331
Batch 28/64 loss: -0.26761680841445923
Batch 29/64 loss: -0.2783253788948059
Batch 30/64 loss: -0.2596176266670227
Batch 31/64 loss: -0.2714453637599945
Batch 32/64 loss: -0.24679189920425415
Batch 33/64 loss: -0.2928507328033447
Batch 34/64 loss: -0.26476871967315674
Batch 35/64 loss: -0.2743958830833435
Batch 36/64 loss: -0.3071679174900055
Batch 37/64 loss: -0.27692729234695435
Batch 38/64 loss: -0.25649333000183105
Batch 39/64 loss: -0.2741386890411377
Batch 40/64 loss: -0.21380621194839478
Batch 41/64 loss: -0.19388729333877563
Batch 42/64 loss: -0.26130568981170654
Batch 43/64 loss: -0.20478427410125732
Batch 44/64 loss: -0.23925626277923584
Batch 45/64 loss: -0.24584218859672546
Batch 46/64 loss: -0.23329299688339233
Batch 47/64 loss: -0.26504239439964294
Batch 48/64 loss: -0.2873457372188568
Batch 49/64 loss: -0.273676335811615
Batch 50/64 loss: -0.2819558084011078
Batch 51/64 loss: -0.251353919506073
Batch 52/64 loss: -0.26129502058029175
Batch 53/64 loss: -0.276492714881897
Batch 54/64 loss: -0.24633198976516724
Batch 55/64 loss: -0.2684239149093628
Batch 56/64 loss: -0.28233152627944946
Batch 57/64 loss: -0.24699509143829346
Batch 58/64 loss: -0.23350560665130615
Batch 59/64 loss: -0.26669222116470337
Batch 60/64 loss: -0.2748194932937622
Batch 61/64 loss: -0.2735459804534912
Batch 62/64 loss: -0.23754465579986572
Batch 63/64 loss: -0.2397749125957489
Batch 64/64 loss: -0.24731656908988953
Epoch 218  Train loss: -0.26090919702660803  Val loss: 0.023111813871311566
Epoch 219
-------------------------------
Batch 1/64 loss: -0.26967135071754456
Batch 2/64 loss: -0.26938357949256897
Batch 3/64 loss: -0.2816641926765442
Batch 4/64 loss: -0.2774873375892639
Batch 5/64 loss: -0.23065894842147827
Batch 6/64 loss: -0.27942413091659546
Batch 7/64 loss: -0.28297579288482666
Batch 8/64 loss: -0.2661903500556946
Batch 9/64 loss: -0.28366202116012573
Batch 10/64 loss: -0.26943865418434143
Batch 11/64 loss: -0.2461923360824585
Batch 12/64 loss: -0.2828053832054138
Batch 13/64 loss: -0.249272882938385
Batch 14/64 loss: -0.2850053906440735
Batch 15/64 loss: -0.22774642705917358
Batch 16/64 loss: -0.24344560503959656
Batch 17/64 loss: -0.2695769667625427
Batch 18/64 loss: -0.268486350774765
Batch 19/64 loss: -0.26345133781433105
Batch 20/64 loss: -0.2618657946586609
Batch 21/64 loss: -0.26905280351638794
Batch 22/64 loss: -0.25974947214126587
Batch 23/64 loss: -0.25833284854888916
Batch 24/64 loss: -0.2552971839904785
Batch 25/64 loss: -0.2313450574874878
Batch 26/64 loss: -0.2438199520111084
Batch 27/64 loss: -0.2600216269493103
Batch 28/64 loss: -0.2749422788619995
Batch 29/64 loss: -0.2552288770675659
Batch 30/64 loss: -0.23165705800056458
Batch 31/64 loss: -0.26214927434921265
Batch 32/64 loss: -0.27344971895217896
Batch 33/64 loss: -0.2730860412120819
Batch 34/64 loss: -0.26528728008270264
Batch 35/64 loss: -0.25387054681777954
Batch 36/64 loss: -0.2701720595359802
Batch 37/64 loss: -0.2587759494781494
Batch 38/64 loss: -0.267999529838562
Batch 39/64 loss: -0.2754426598548889
Batch 40/64 loss: -0.2620830833911896
Batch 41/64 loss: -0.23085686564445496
Batch 42/64 loss: -0.28324076533317566
Batch 43/64 loss: -0.25173407793045044
Batch 44/64 loss: -0.25715699791908264
Batch 45/64 loss: -0.23389947414398193
Batch 46/64 loss: -0.21394586563110352
Batch 47/64 loss: -0.202153742313385
Batch 48/64 loss: -0.26545339822769165
Batch 49/64 loss: -0.2640179991722107
Batch 50/64 loss: -0.2563853859901428
Batch 51/64 loss: -0.2625977098941803
Batch 52/64 loss: -0.2462013065814972
Batch 53/64 loss: -0.26657581329345703
Batch 54/64 loss: -0.23273959755897522
Batch 55/64 loss: -0.2800184488296509
Batch 56/64 loss: -0.30120202898979187
Batch 57/64 loss: -0.2656548023223877
Batch 58/64 loss: -0.27059438824653625
Batch 59/64 loss: -0.27612560987472534
Batch 60/64 loss: -0.2660096287727356
Batch 61/64 loss: -0.23828956484794617
Batch 62/64 loss: -0.30399858951568604
Batch 63/64 loss: -0.29554247856140137
Batch 64/64 loss: -0.26130151748657227
Epoch 219  Train loss: -0.26149864804510975  Val loss: 0.02309006616421991
Epoch 220
-------------------------------
Batch 1/64 loss: -0.28160786628723145
Batch 2/64 loss: -0.2597072720527649
Batch 3/64 loss: -0.2747582793235779
Batch 4/64 loss: -0.2670971751213074
Batch 5/64 loss: -0.2560386657714844
Batch 6/64 loss: -0.2529016137123108
Batch 7/64 loss: -0.2849511504173279
Batch 8/64 loss: -0.30429890751838684
Batch 9/64 loss: -0.2554437518119812
Batch 10/64 loss: -0.28328070044517517
Batch 11/64 loss: -0.26350095868110657
Batch 12/64 loss: -0.27782827615737915
Batch 13/64 loss: -0.2395995855331421
Batch 14/64 loss: -0.25316715240478516
Batch 15/64 loss: -0.3134235441684723
Batch 16/64 loss: -0.24271684885025024
Batch 17/64 loss: -0.24646905064582825
Batch 18/64 loss: -0.2610457241535187
Batch 19/64 loss: -0.24716413021087646
Batch 20/64 loss: -0.249912291765213
Batch 21/64 loss: -0.26706284284591675
Batch 22/64 loss: -0.2535686790943146
Batch 23/64 loss: -0.2695663571357727
Batch 24/64 loss: -0.23692554235458374
Batch 25/64 loss: -0.24588271975517273
Batch 26/64 loss: -0.2882494628429413
Batch 27/64 loss: -0.29414355754852295
Batch 28/64 loss: -0.28950217366218567
Batch 29/64 loss: -0.29371118545532227
Batch 30/64 loss: -0.28296172618865967
Batch 31/64 loss: -0.24291908740997314
Batch 32/64 loss: -0.22886157035827637
Batch 33/64 loss: -0.284254252910614
Batch 34/64 loss: -0.24847334623336792
Batch 35/64 loss: -0.25845420360565186
Batch 36/64 loss: -0.2642248272895813
Batch 37/64 loss: -0.2653603255748749
Batch 38/64 loss: -0.2712141275405884
Batch 39/64 loss: -0.29196301102638245
Batch 40/64 loss: -0.27829939126968384
Batch 41/64 loss: -0.23291614651679993
Batch 42/64 loss: -0.29330843687057495
Batch 43/64 loss: -0.2889132499694824
Batch 44/64 loss: -0.22453835606575012
Batch 45/64 loss: -0.23982897400856018
Batch 46/64 loss: -0.25382840633392334
Batch 47/64 loss: -0.2753971517086029
Batch 48/64 loss: -0.27393996715545654
Batch 49/64 loss: -0.2531110346317291
Batch 50/64 loss: -0.27614137530326843
Batch 51/64 loss: -0.28896939754486084
Batch 52/64 loss: -0.21794795989990234
Batch 53/64 loss: -0.2626863718032837
Batch 54/64 loss: -0.2735315263271332
Batch 55/64 loss: -0.27423787117004395
Batch 56/64 loss: -0.27166885137557983
Batch 57/64 loss: -0.2872869372367859
Batch 58/64 loss: -0.2865802049636841
Batch 59/64 loss: -0.2621029317378998
Batch 60/64 loss: -0.26431727409362793
Batch 61/64 loss: -0.23612993955612183
Batch 62/64 loss: -0.27204081416130066
Batch 63/64 loss: -0.2958306670188904
Batch 64/64 loss: -0.2674446702003479
Epoch 220  Train loss: -0.2662956656194201  Val loss: 0.023145868811001072
Epoch 221
-------------------------------
Batch 1/64 loss: -0.29159337282180786
Batch 2/64 loss: -0.26960331201553345
Batch 3/64 loss: -0.2568126618862152
Batch 4/64 loss: -0.29335713386535645
Batch 5/64 loss: -0.2795216739177704
Batch 6/64 loss: -0.20225727558135986
Batch 7/64 loss: -0.3146592080593109
Batch 8/64 loss: -0.27416056394577026
Batch 9/64 loss: -0.25081077218055725
Batch 10/64 loss: -0.29494574666023254
Batch 11/64 loss: -0.22843772172927856
Batch 12/64 loss: -0.2832988500595093
Batch 13/64 loss: -0.29995524883270264
Batch 14/64 loss: -0.24585306644439697
Batch 15/64 loss: -0.2743222117424011
Batch 16/64 loss: -0.2766358256340027
Batch 17/64 loss: -0.2862289547920227
Batch 18/64 loss: -0.265376478433609
Batch 19/64 loss: -0.26029813289642334
Batch 20/64 loss: -0.29237860441207886
Batch 21/64 loss: -0.2579411268234253
Batch 22/64 loss: -0.27817732095718384
Batch 23/64 loss: -0.2853221893310547
Batch 24/64 loss: -0.30833181738853455
Batch 25/64 loss: -0.21197307109832764
Batch 26/64 loss: -0.2981065809726715
Batch 27/64 loss: -0.2722550630569458
Batch 28/64 loss: -0.2860845625400543
Batch 29/64 loss: -0.267951637506485
Batch 30/64 loss: -0.26002082228660583
Batch 31/64 loss: -0.28163301944732666
Batch 32/64 loss: -0.24813589453697205
Batch 33/64 loss: -0.24167504906654358
Batch 34/64 loss: -0.2683805525302887
Batch 35/64 loss: -0.2734927833080292
Batch 36/64 loss: -0.2612188458442688
Batch 37/64 loss: -0.2595992386341095
Batch 38/64 loss: -0.2535669207572937
Batch 39/64 loss: -0.2703165113925934
Batch 40/64 loss: -0.2652842104434967
Batch 41/64 loss: -0.2629978656768799
Batch 42/64 loss: -0.26077741384506226
Batch 43/64 loss: -0.27196502685546875
Batch 44/64 loss: -0.27879685163497925
Batch 45/64 loss: -0.1998143196105957
Batch 46/64 loss: -0.27053308486938477
Batch 47/64 loss: -0.2800261080265045
Batch 48/64 loss: -0.2961292862892151
Batch 49/64 loss: -0.25961488485336304
Batch 50/64 loss: -0.2565031051635742
Batch 51/64 loss: -0.22926467657089233
Batch 52/64 loss: -0.2614218592643738
Batch 53/64 loss: -0.28931695222854614
Batch 54/64 loss: -0.24758601188659668
Batch 55/64 loss: -0.21295928955078125
Batch 56/64 loss: -0.26687484979629517
Batch 57/64 loss: -0.2722533941268921
Batch 58/64 loss: -0.26035773754119873
Batch 59/64 loss: -0.2456909418106079
Batch 60/64 loss: -0.2999460697174072
Batch 61/64 loss: -0.2617959678173065
Batch 62/64 loss: -0.2798166275024414
Batch 63/64 loss: -0.2998232841491699
Batch 64/64 loss: -0.2746415436267853
Epoch 221  Train loss: -0.2676113066720028  Val loss: 0.023407226780435882
Epoch 222
-------------------------------
Batch 1/64 loss: -0.22984474897384644
Batch 2/64 loss: -0.3139339089393616
Batch 3/64 loss: -0.22953081130981445
Batch 4/64 loss: -0.28706422448158264
Batch 5/64 loss: -0.26347479224205017
Batch 6/64 loss: -0.250393271446228
Batch 7/64 loss: -0.26137691736221313
Batch 8/64 loss: -0.28402313590049744
Batch 9/64 loss: -0.20974838733673096
Batch 10/64 loss: -0.2728597819805145
Batch 11/64 loss: -0.25128695368766785
Batch 12/64 loss: -0.28911274671554565
Batch 13/64 loss: -0.26321983337402344
Batch 14/64 loss: -0.2924177646636963
Batch 15/64 loss: -0.27773380279541016
Batch 16/64 loss: -0.2624053359031677
Batch 17/64 loss: -0.2902246117591858
Batch 18/64 loss: -0.2437945008277893
Batch 19/64 loss: -0.25009557604789734
Batch 20/64 loss: -0.26584821939468384
Batch 21/64 loss: -0.2917332351207733
Batch 22/64 loss: -0.2706589698791504
Batch 23/64 loss: -0.2684885263442993
Batch 24/64 loss: -0.26126348972320557
Batch 25/64 loss: -0.26122570037841797
Batch 26/64 loss: -0.24935436248779297
Batch 27/64 loss: -0.2594478726387024
Batch 28/64 loss: -0.28445133566856384
Batch 29/64 loss: -0.27359554171562195
Batch 30/64 loss: -0.2541549503803253
Batch 31/64 loss: -0.27637505531311035
Batch 32/64 loss: -0.2885698080062866
Batch 33/64 loss: -0.28709906339645386
Batch 34/64 loss: -0.24151119589805603
Batch 35/64 loss: -0.2934373915195465
Batch 36/64 loss: -0.27445071935653687
Batch 37/64 loss: -0.2525905966758728
Batch 38/64 loss: -0.2787744402885437
Batch 39/64 loss: -0.2657468616962433
Batch 40/64 loss: -0.2853814959526062
Batch 41/64 loss: -0.2376149296760559
Batch 42/64 loss: -0.22791090607643127
Batch 43/64 loss: -0.26117080450057983
Batch 44/64 loss: -0.314528226852417
Batch 45/64 loss: -0.27797606587409973
Batch 46/64 loss: -0.30361661314964294
Batch 47/64 loss: -0.26888933777809143
Batch 48/64 loss: -0.27795934677124023
Batch 49/64 loss: -0.25327053666114807
Batch 50/64 loss: -0.24212870001792908
Batch 51/64 loss: -0.2825072407722473
Batch 52/64 loss: -0.2890620231628418
Batch 53/64 loss: -0.27053552865982056
Batch 54/64 loss: -0.2834787666797638
Batch 55/64 loss: -0.29767441749572754
Batch 56/64 loss: -0.2839796543121338
Batch 57/64 loss: -0.2749369740486145
Batch 58/64 loss: -0.28417766094207764
Batch 59/64 loss: -0.27513185143470764
Batch 60/64 loss: -0.2617967128753662
Batch 61/64 loss: -0.27187350392341614
Batch 62/64 loss: -0.2667247951030731
Batch 63/64 loss: -0.29698336124420166
Batch 64/64 loss: -0.26922449469566345
Epoch 222  Train loss: -0.2700007257508297  Val loss: 0.022845493763992468
Epoch 223
-------------------------------
Batch 1/64 loss: -0.2771705687046051
Batch 2/64 loss: -0.2810344696044922
Batch 3/64 loss: -0.30804795026779175
Batch 4/64 loss: -0.24857380986213684
Batch 5/64 loss: -0.2605479955673218
Batch 6/64 loss: -0.27549856901168823
Batch 7/64 loss: -0.285565584897995
Batch 8/64 loss: -0.27144789695739746
Batch 9/64 loss: -0.2889418601989746
Batch 10/64 loss: -0.2953646779060364
Batch 11/64 loss: -0.2675517201423645
Batch 12/64 loss: -0.2662767171859741
Batch 13/64 loss: -0.25731438398361206
Batch 14/64 loss: -0.2839427590370178
Batch 15/64 loss: -0.24464601278305054
Batch 16/64 loss: -0.30608469247817993
Batch 17/64 loss: -0.2716890871524811
Batch 18/64 loss: -0.26801854372024536
Batch 19/64 loss: -0.253146767616272
Batch 20/64 loss: -0.23159688711166382
Batch 21/64 loss: -0.28627896308898926
Batch 22/64 loss: -0.273629754781723
Batch 23/64 loss: -0.29034459590911865
Batch 24/64 loss: -0.31148436665534973
Batch 25/64 loss: -0.28865236043930054
Batch 26/64 loss: -0.25828611850738525
Batch 27/64 loss: -0.23641151189804077
Batch 28/64 loss: -0.2660776376724243
Batch 29/64 loss: -0.2924214005470276
Batch 30/64 loss: -0.26934051513671875
Batch 31/64 loss: -0.28908771276474
Batch 32/64 loss: -0.27808523178100586
Batch 33/64 loss: -0.25132372975349426
Batch 34/64 loss: -0.2638845443725586
Batch 35/64 loss: -0.24175477027893066
Batch 36/64 loss: -0.2685067057609558
Batch 37/64 loss: -0.2749677002429962
Batch 38/64 loss: -0.25626397132873535
Batch 39/64 loss: -0.28368425369262695
Batch 40/64 loss: -0.23041445016860962
Batch 41/64 loss: -0.2720470428466797
Batch 42/64 loss: -0.25483837723731995
Batch 43/64 loss: -0.25277411937713623
Batch 44/64 loss: -0.2525145411491394
Batch 45/64 loss: -0.26658135652542114
Batch 46/64 loss: -0.25307971239089966
Batch 47/64 loss: -0.26954081654548645
Batch 48/64 loss: -0.26861369609832764
Batch 49/64 loss: -0.2575263977050781
Batch 50/64 loss: -0.27511847019195557
Batch 51/64 loss: -0.27929314970970154
Batch 52/64 loss: -0.2531811594963074
Batch 53/64 loss: -0.2502198815345764
Batch 54/64 loss: -0.20551228523254395
Batch 55/64 loss: -0.26188862323760986
Batch 56/64 loss: -0.26081401109695435
Batch 57/64 loss: -0.282962441444397
Batch 58/64 loss: -0.274372935295105
Batch 59/64 loss: -0.2888602018356323
Batch 60/64 loss: -0.2736969590187073
Batch 61/64 loss: -0.27210310101509094
Batch 62/64 loss: -0.2898949384689331
Batch 63/64 loss: -0.25806915760040283
Batch 64/64 loss: -0.24302595853805542
Epoch 223  Train loss: -0.26837857400669773  Val loss: 0.022456035786068317
Epoch 224
-------------------------------
Batch 1/64 loss: -0.2853718400001526
Batch 2/64 loss: -0.2884821593761444
Batch 3/64 loss: -0.29820841550827026
Batch 4/64 loss: -0.2833743095397949
Batch 5/64 loss: -0.2799895107746124
Batch 6/64 loss: -0.25530633330345154
Batch 7/64 loss: -0.297907292842865
Batch 8/64 loss: -0.20295274257659912
Batch 9/64 loss: -0.2962263226509094
Batch 10/64 loss: -0.2697373628616333
Batch 11/64 loss: -0.2663379907608032
Batch 12/64 loss: -0.26767101883888245
Batch 13/64 loss: -0.2653215825557709
Batch 14/64 loss: -0.28031983971595764
Batch 15/64 loss: -0.25386276841163635
Batch 16/64 loss: -0.24781125783920288
Batch 17/64 loss: -0.25515419244766235
Batch 18/64 loss: -0.2862720787525177
Batch 19/64 loss: -0.28899678587913513
Batch 20/64 loss: -0.26769357919692993
Batch 21/64 loss: -0.2794423997402191
Batch 22/64 loss: -0.29086917638778687
Batch 23/64 loss: -0.2805674970149994
Batch 24/64 loss: -0.2908235192298889
Batch 25/64 loss: -0.2087101936340332
Batch 26/64 loss: -0.2723090648651123
Batch 27/64 loss: -0.2790369391441345
Batch 28/64 loss: -0.28246182203292847
Batch 29/64 loss: -0.2845400273799896
Batch 30/64 loss: -0.2830154597759247
Batch 31/64 loss: -0.27879250049591064
Batch 32/64 loss: -0.29173851013183594
Batch 33/64 loss: -0.2801916003227234
Batch 34/64 loss: -0.25953638553619385
Batch 35/64 loss: -0.26451894640922546
Batch 36/64 loss: -0.29524433612823486
Batch 37/64 loss: -0.26724421977996826
Batch 38/64 loss: -0.2559413015842438
Batch 39/64 loss: -0.2936563491821289
Batch 40/64 loss: -0.2754300832748413
Batch 41/64 loss: -0.24672001600265503
Batch 42/64 loss: -0.21781301498413086
Batch 43/64 loss: -0.2761857509613037
Batch 44/64 loss: -0.22606205940246582
Batch 45/64 loss: -0.28582656383514404
Batch 46/64 loss: -0.2806633710861206
Batch 47/64 loss: -0.28951969742774963
Batch 48/64 loss: -0.30761072039604187
Batch 49/64 loss: -0.24548742175102234
Batch 50/64 loss: -0.2673768401145935
Batch 51/64 loss: -0.27694863080978394
Batch 52/64 loss: -0.2932712435722351
Batch 53/64 loss: -0.26343056559562683
Batch 54/64 loss: -0.30340510606765747
Batch 55/64 loss: -0.24767938256263733
Batch 56/64 loss: -0.23353826999664307
Batch 57/64 loss: -0.2875942289829254
Batch 58/64 loss: -0.2306220531463623
Batch 59/64 loss: -0.2659428119659424
Batch 60/64 loss: -0.293473482131958
Batch 61/64 loss: -0.30567604303359985
Batch 62/64 loss: -0.2749018371105194
Batch 63/64 loss: -0.25198811292648315
Batch 64/64 loss: -0.254112184047699
Epoch 224  Train loss: -0.2715825737691393  Val loss: 0.021781959689359896
Epoch 225
-------------------------------
Batch 1/64 loss: -0.24513500928878784
Batch 2/64 loss: -0.2491481900215149
Batch 3/64 loss: -0.23322522640228271
Batch 4/64 loss: -0.31226393580436707
Batch 5/64 loss: -0.27725881338119507
Batch 6/64 loss: -0.2549542784690857
Batch 7/64 loss: -0.26664653420448303
Batch 8/64 loss: -0.25090742111206055
Batch 9/64 loss: -0.2710248529911041
Batch 10/64 loss: -0.2965411841869354
Batch 11/64 loss: -0.2816682457923889
Batch 12/64 loss: -0.2350943386554718
Batch 13/64 loss: -0.2114925980567932
Batch 14/64 loss: -0.2760236859321594
Batch 15/64 loss: -0.2780216336250305
Batch 16/64 loss: -0.2893778085708618
Batch 17/64 loss: -0.26095834374427795
Batch 18/64 loss: -0.2609873116016388
Batch 19/64 loss: -0.27560675144195557
Batch 20/64 loss: -0.3033473491668701
Batch 21/64 loss: -0.2215491533279419
Batch 22/64 loss: -0.29192930459976196
Batch 23/64 loss: -0.2709351181983948
Batch 24/64 loss: -0.2883235812187195
Batch 25/64 loss: -0.20716947317123413
Batch 26/64 loss: -0.2829801142215729
Batch 27/64 loss: -0.2952195107936859
Batch 28/64 loss: -0.2604064345359802
Batch 29/64 loss: -0.3093528747558594
Batch 30/64 loss: -0.2951323688030243
Batch 31/64 loss: -0.2845773696899414
Batch 32/64 loss: -0.2663894295692444
Batch 33/64 loss: -0.29424017667770386
Batch 34/64 loss: -0.29103124141693115
Batch 35/64 loss: -0.29137033224105835
Batch 36/64 loss: -0.2569895386695862
Batch 37/64 loss: -0.26882606744766235
Batch 38/64 loss: -0.24068742990493774
Batch 39/64 loss: -0.2670532763004303
Batch 40/64 loss: -0.23616403341293335
Batch 41/64 loss: -0.27446064352989197
Batch 42/64 loss: -0.26851844787597656
Batch 43/64 loss: -0.26929980516433716
Batch 44/64 loss: -0.2735103368759155
Batch 45/64 loss: -0.24148783087730408
Batch 46/64 loss: -0.2765865623950958
Batch 47/64 loss: -0.24663689732551575
Batch 48/64 loss: -0.2692720890045166
Batch 49/64 loss: -0.24169021844863892
Batch 50/64 loss: -0.2859923839569092
Batch 51/64 loss: -0.28879114985466003
Batch 52/64 loss: -0.28646206855773926
Batch 53/64 loss: -0.25475209951400757
Batch 54/64 loss: -0.23339754343032837
Batch 55/64 loss: -0.25925713777542114
Batch 56/64 loss: -0.2734242081642151
Batch 57/64 loss: -0.28354912996292114
Batch 58/64 loss: -0.28809183835983276
Batch 59/64 loss: -0.265953004360199
Batch 60/64 loss: -0.2921435832977295
Batch 61/64 loss: -0.28102409839630127
Batch 62/64 loss: -0.24092209339141846
Batch 63/64 loss: -0.2860766053199768
Batch 64/64 loss: -0.28653302788734436
Epoch 225  Train loss: -0.26896015510839577  Val loss: 0.024340641457600284
Epoch 226
-------------------------------
Batch 1/64 loss: -0.267310231924057
Batch 2/64 loss: -0.2675427794456482
Batch 3/64 loss: -0.24422746896743774
Batch 4/64 loss: -0.2964972257614136
Batch 5/64 loss: -0.29382723569869995
Batch 6/64 loss: -0.27128511667251587
Batch 7/64 loss: -0.2669655978679657
Batch 8/64 loss: -0.22164779901504517
Batch 9/64 loss: -0.2566837966442108
Batch 10/64 loss: -0.2930576205253601
Batch 11/64 loss: -0.2632151246070862
Batch 12/64 loss: -0.25825414061546326
Batch 13/64 loss: -0.28564149141311646
Batch 14/64 loss: -0.2722145915031433
Batch 15/64 loss: -0.2923664152622223
Batch 16/64 loss: -0.2954210638999939
Batch 17/64 loss: -0.23371362686157227
Batch 18/64 loss: -0.2660866975784302
Batch 19/64 loss: -0.2968016564846039
Batch 20/64 loss: -0.24728929996490479
Batch 21/64 loss: -0.3057161569595337
Batch 22/64 loss: -0.2573102116584778
Batch 23/64 loss: -0.2824452519416809
Batch 24/64 loss: -0.25006407499313354
Batch 25/64 loss: -0.2770286798477173
Batch 26/64 loss: -0.284750372171402
Batch 27/64 loss: -0.2974289059638977
Batch 28/64 loss: -0.3133918344974518
Batch 29/64 loss: -0.26481789350509644
Batch 30/64 loss: -0.27064570784568787
Batch 31/64 loss: -0.24872887134552002
Batch 32/64 loss: -0.2528928220272064
Batch 33/64 loss: -0.3039507269859314
Batch 34/64 loss: -0.26885515451431274
Batch 35/64 loss: -0.2872370481491089
Batch 36/64 loss: -0.26674818992614746
Batch 37/64 loss: -0.2785443067550659
Batch 38/64 loss: -0.2564290463924408
Batch 39/64 loss: -0.26776063442230225
Batch 40/64 loss: -0.25758397579193115
Batch 41/64 loss: -0.300772488117218
Batch 42/64 loss: -0.26933568716049194
Batch 43/64 loss: -0.28488510847091675
Batch 44/64 loss: -0.26285111904144287
Batch 45/64 loss: -0.2591349482536316
Batch 46/64 loss: -0.26600414514541626
Batch 47/64 loss: -0.22548913955688477
Batch 48/64 loss: -0.24518442153930664
Batch 49/64 loss: -0.2796446681022644
Batch 50/64 loss: -0.29860299825668335
Batch 51/64 loss: -0.25871339440345764
Batch 52/64 loss: -0.2816069722175598
Batch 53/64 loss: -0.23535355925559998
Batch 54/64 loss: -0.2943522334098816
Batch 55/64 loss: -0.26112762093544006
Batch 56/64 loss: -0.23252660036087036
Batch 57/64 loss: -0.2784947156906128
Batch 58/64 loss: -0.26323825120925903
Batch 59/64 loss: -0.2671504616737366
Batch 60/64 loss: -0.2744334638118744
Batch 61/64 loss: -0.2652548551559448
Batch 62/64 loss: -0.28348052501678467
Batch 63/64 loss: -0.27473801374435425
Batch 64/64 loss: -0.22931134700775146
Epoch 226  Train loss: -0.2700664744657629  Val loss: 0.022510423283396717
Epoch 227
-------------------------------
Batch 1/64 loss: -0.3009595572948456
Batch 2/64 loss: -0.25940561294555664
Batch 3/64 loss: -0.30071407556533813
Batch 4/64 loss: -0.286678284406662
Batch 5/64 loss: -0.24814307689666748
Batch 6/64 loss: -0.26484277844429016
Batch 7/64 loss: -0.2827998995780945
Batch 8/64 loss: -0.2803065776824951
Batch 9/64 loss: -0.23758089542388916
Batch 10/64 loss: -0.3096384108066559
Batch 11/64 loss: -0.24391111731529236
Batch 12/64 loss: -0.2782350778579712
Batch 13/64 loss: -0.27337872982025146
Batch 14/64 loss: -0.30284857749938965
Batch 15/64 loss: -0.29970163106918335
Batch 16/64 loss: -0.28018718957901
Batch 17/64 loss: -0.275961697101593
Batch 18/64 loss: -0.2977980077266693
Batch 19/64 loss: -0.3022403419017792
Batch 20/64 loss: -0.27135729789733887
Batch 21/64 loss: -0.23794078826904297
Batch 22/64 loss: -0.26986294984817505
Batch 23/64 loss: -0.29448068141937256
Batch 24/64 loss: -0.2550485134124756
Batch 25/64 loss: -0.27553117275238037
Batch 26/64 loss: -0.2226957082748413
Batch 27/64 loss: -0.2581620216369629
Batch 28/64 loss: -0.2888159453868866
Batch 29/64 loss: -0.2519755959510803
Batch 30/64 loss: -0.26445838809013367
Batch 31/64 loss: -0.29004326462745667
Batch 32/64 loss: -0.28253859281539917
Batch 33/64 loss: -0.2372167706489563
Batch 34/64 loss: -0.297159343957901
Batch 35/64 loss: -0.23985615372657776
Batch 36/64 loss: -0.2781234681606293
Batch 37/64 loss: -0.303153932094574
Batch 38/64 loss: -0.2511359751224518
Batch 39/64 loss: -0.2406572699546814
Batch 40/64 loss: -0.2554340660572052
Batch 41/64 loss: -0.27638548612594604
Batch 42/64 loss: -0.2849685549736023
Batch 43/64 loss: -0.29312875866889954
Batch 44/64 loss: -0.24981728196144104
Batch 45/64 loss: -0.2722083032131195
Batch 46/64 loss: -0.26265600323677063
Batch 47/64 loss: -0.2479712963104248
Batch 48/64 loss: -0.2663571238517761
Batch 49/64 loss: -0.250776469707489
Batch 50/64 loss: -0.25775372982025146
Batch 51/64 loss: -0.2508445978164673
Batch 52/64 loss: -0.28042513132095337
Batch 53/64 loss: -0.292580246925354
Batch 54/64 loss: -0.28407686948776245
Batch 55/64 loss: -0.17323285341262817
Batch 56/64 loss: -0.28801652789115906
Batch 57/64 loss: -0.28965383768081665
Batch 58/64 loss: -0.26703155040740967
Batch 59/64 loss: -0.2344282865524292
Batch 60/64 loss: -0.22456592321395874
Batch 61/64 loss: -0.27545371651649475
Batch 62/64 loss: -0.2812066674232483
Batch 63/64 loss: -0.26608550548553467
Batch 64/64 loss: -0.28545406460762024
Epoch 227  Train loss: -0.2694069758349774  Val loss: 0.02455943085483669
Epoch 228
-------------------------------
Batch 1/64 loss: -0.19350671768188477
Batch 2/64 loss: -0.27010321617126465
Batch 3/64 loss: -0.29535722732543945
Batch 4/64 loss: -0.23547732830047607
Batch 5/64 loss: -0.300894558429718
Batch 6/64 loss: -0.2893979549407959
Batch 7/64 loss: -0.21987485885620117
Batch 8/64 loss: -0.25519323348999023
Batch 9/64 loss: -0.3015398681163788
Batch 10/64 loss: -0.23402529954910278
Batch 11/64 loss: -0.2252918779850006
Batch 12/64 loss: -0.24643778800964355
Batch 13/64 loss: -0.24826985597610474
Batch 14/64 loss: -0.254457950592041
Batch 15/64 loss: -0.2698737382888794
Batch 16/64 loss: -0.2681000828742981
Batch 17/64 loss: -0.2576851546764374
Batch 18/64 loss: -0.2528510093688965
Batch 19/64 loss: -0.2914467453956604
Batch 20/64 loss: -0.3139721751213074
Batch 21/64 loss: -0.27699846029281616
Batch 22/64 loss: -0.25695160031318665
Batch 23/64 loss: -0.26058846712112427
Batch 24/64 loss: -0.26378774642944336
Batch 25/64 loss: -0.3079080581665039
Batch 26/64 loss: -0.29942867159843445
Batch 27/64 loss: -0.2998499870300293
Batch 28/64 loss: -0.2729186415672302
Batch 29/64 loss: -0.29688701033592224
Batch 30/64 loss: -0.2535434365272522
Batch 31/64 loss: -0.2642885446548462
Batch 32/64 loss: -0.28924721479415894
Batch 33/64 loss: -0.30875617265701294
Batch 34/64 loss: -0.27345001697540283
Batch 35/64 loss: -0.23823466897010803
Batch 36/64 loss: -0.2596164643764496
Batch 37/64 loss: -0.2621303200721741
Batch 38/64 loss: -0.2712666392326355
Batch 39/64 loss: -0.2869874835014343
Batch 40/64 loss: -0.2627792954444885
Batch 41/64 loss: -0.28871363401412964
Batch 42/64 loss: -0.26256510615348816
Batch 43/64 loss: -0.279099702835083
Batch 44/64 loss: -0.2869650721549988
Batch 45/64 loss: -0.2517838478088379
Batch 46/64 loss: -0.2688327133655548
Batch 47/64 loss: -0.2158207893371582
Batch 48/64 loss: -0.2507433593273163
Batch 49/64 loss: -0.28215593099594116
Batch 50/64 loss: -0.2573819160461426
Batch 51/64 loss: -0.2816557288169861
Batch 52/64 loss: -0.27731549739837646
Batch 53/64 loss: -0.2853240966796875
Batch 54/64 loss: -0.2732008695602417
Batch 55/64 loss: -0.29750359058380127
Batch 56/64 loss: -0.30033981800079346
Batch 57/64 loss: -0.2750224471092224
Batch 58/64 loss: -0.2886711359024048
Batch 59/64 loss: -0.2004704475402832
Batch 60/64 loss: -0.31313198804855347
Batch 61/64 loss: -0.2738453149795532
Batch 62/64 loss: -0.3030359745025635
Batch 63/64 loss: -0.2749151587486267
Batch 64/64 loss: -0.24393802881240845
Epoch 228  Train loss: -0.2698168345526153  Val loss: 0.02029209018163255
Epoch 229
-------------------------------
Batch 1/64 loss: -0.28914886713027954
Batch 2/64 loss: -0.32219594717025757
Batch 3/64 loss: -0.26750433444976807
Batch 4/64 loss: -0.2986416518688202
Batch 5/64 loss: -0.288088858127594
Batch 6/64 loss: -0.3011995553970337
Batch 7/64 loss: -0.24195510149002075
Batch 8/64 loss: -0.28476977348327637
Batch 9/64 loss: -0.2965344786643982
Batch 10/64 loss: -0.2907077968120575
Batch 11/64 loss: -0.2878810167312622
Batch 12/64 loss: -0.2988925576210022
Batch 13/64 loss: -0.29713737964630127
Batch 14/64 loss: -0.2931377589702606
Batch 15/64 loss: -0.29962557554244995
Batch 16/64 loss: -0.25761374831199646
Batch 17/64 loss: -0.2693415880203247
Batch 18/64 loss: -0.2631394863128662
Batch 19/64 loss: -0.2511972188949585
Batch 20/64 loss: -0.27473047375679016
Batch 21/64 loss: -0.27395206689834595
Batch 22/64 loss: -0.2602052092552185
Batch 23/64 loss: -0.20540034770965576
Batch 24/64 loss: -0.275343656539917
Batch 25/64 loss: -0.24279451370239258
Batch 26/64 loss: -0.24865618348121643
Batch 27/64 loss: -0.2740795612335205
Batch 28/64 loss: -0.29055705666542053
Batch 29/64 loss: -0.2539769411087036
Batch 30/64 loss: -0.25152045488357544
Batch 31/64 loss: -0.2944166958332062
Batch 32/64 loss: -0.24694406986236572
Batch 33/64 loss: -0.2916516661643982
Batch 34/64 loss: -0.2687223553657532
Batch 35/64 loss: -0.2688697874546051
Batch 36/64 loss: -0.27102476358413696
Batch 37/64 loss: -0.2837038040161133
Batch 38/64 loss: -0.30257898569107056
Batch 39/64 loss: -0.28212854266166687
Batch 40/64 loss: -0.2846040725708008
Batch 41/64 loss: -0.28413599729537964
Batch 42/64 loss: -0.29372674226760864
Batch 43/64 loss: -0.29276615381240845
Batch 44/64 loss: -0.28263962268829346
Batch 45/64 loss: -0.25629138946533203
Batch 46/64 loss: -0.27358126640319824
Batch 47/64 loss: -0.27214640378952026
Batch 48/64 loss: -0.26814818382263184
Batch 49/64 loss: -0.245228111743927
Batch 50/64 loss: -0.2769140601158142
Batch 51/64 loss: -0.28377604484558105
Batch 52/64 loss: -0.27429917454719543
Batch 53/64 loss: -0.31520694494247437
Batch 54/64 loss: -0.22055137157440186
Batch 55/64 loss: -0.2749881148338318
Batch 56/64 loss: -0.2977648973464966
Batch 57/64 loss: -0.2411196529865265
Batch 58/64 loss: -0.27471238374710083
Batch 59/64 loss: -0.2768263816833496
Batch 60/64 loss: -0.2567797899246216
Batch 61/64 loss: -0.2727242708206177
Batch 62/64 loss: -0.26104700565338135
Batch 63/64 loss: -0.2265772819519043
Batch 64/64 loss: -0.23443388938903809
Epoch 229  Train loss: -0.2736054990805832  Val loss: 0.023564865294191027
Epoch 230
-------------------------------
Batch 1/64 loss: -0.3003472685813904
Batch 2/64 loss: -0.31447064876556396
Batch 3/64 loss: -0.259681761264801
Batch 4/64 loss: -0.2843833565711975
Batch 5/64 loss: -0.2854226529598236
Batch 6/64 loss: -0.2990213632583618
Batch 7/64 loss: -0.1941840648651123
Batch 8/64 loss: -0.2611062228679657
Batch 9/64 loss: -0.2552337944507599
Batch 10/64 loss: -0.2745378613471985
Batch 11/64 loss: -0.3078903257846832
Batch 12/64 loss: -0.3058598041534424
Batch 13/64 loss: -0.2785677909851074
Batch 14/64 loss: -0.2685231864452362
Batch 15/64 loss: -0.3104243874549866
Batch 16/64 loss: -0.2392154335975647
Batch 17/64 loss: -0.29657572507858276
Batch 18/64 loss: -0.20392102003097534
Batch 19/64 loss: -0.24562668800354004
Batch 20/64 loss: -0.24290305376052856
Batch 21/64 loss: -0.2352333962917328
Batch 22/64 loss: -0.2542794346809387
Batch 23/64 loss: -0.2636202573776245
Batch 24/64 loss: -0.27921587228775024
Batch 25/64 loss: -0.2638791799545288
Batch 26/64 loss: -0.29295188188552856
Batch 27/64 loss: -0.23728841543197632
Batch 28/64 loss: -0.29839569330215454
Batch 29/64 loss: -0.28086528182029724
Batch 30/64 loss: -0.2799241244792938
Batch 31/64 loss: -0.29473114013671875
Batch 32/64 loss: -0.27035605907440186
Batch 33/64 loss: -0.26859211921691895
Batch 34/64 loss: -0.28926849365234375
Batch 35/64 loss: -0.27370452880859375
Batch 36/64 loss: -0.2973724901676178
Batch 37/64 loss: -0.2982727587223053
Batch 38/64 loss: -0.2736148238182068
Batch 39/64 loss: -0.2825617790222168
Batch 40/64 loss: -0.272132933139801
Batch 41/64 loss: -0.29265546798706055
Batch 42/64 loss: -0.29592230916023254
Batch 43/64 loss: -0.2537483274936676
Batch 44/64 loss: -0.25766968727111816
Batch 45/64 loss: -0.24131649732589722
Batch 46/64 loss: -0.2893918752670288
Batch 47/64 loss: -0.2766222655773163
Batch 48/64 loss: -0.22861412167549133
Batch 49/64 loss: -0.2713452875614166
Batch 50/64 loss: -0.2964639365673065
Batch 51/64 loss: -0.2644423544406891
Batch 52/64 loss: -0.2907184660434723
Batch 53/64 loss: -0.25472912192344666
Batch 54/64 loss: -0.26276353001594543
Batch 55/64 loss: -0.2706989049911499
Batch 56/64 loss: -0.260312020778656
Batch 57/64 loss: -0.28243041038513184
Batch 58/64 loss: -0.2720877528190613
Batch 59/64 loss: -0.2612290680408478
Batch 60/64 loss: -0.2537059783935547
Batch 61/64 loss: -0.29591965675354004
Batch 62/64 loss: -0.25241443514823914
Batch 63/64 loss: -0.25844377279281616
Batch 64/64 loss: -0.2540348172187805
Epoch 230  Train loss: -0.27150319113450894  Val loss: 0.023262709686436605
Epoch 231
-------------------------------
Batch 1/64 loss: -0.27546367049217224
Batch 2/64 loss: -0.3108742833137512
Batch 3/64 loss: -0.2838860750198364
Batch 4/64 loss: -0.3036080300807953
Batch 5/64 loss: -0.2905469536781311
Batch 6/64 loss: -0.32208725810050964
Batch 7/64 loss: -0.2735517621040344
Batch 8/64 loss: -0.3166462779045105
Batch 9/64 loss: -0.26995182037353516
Batch 10/64 loss: -0.2863297760486603
Batch 11/64 loss: -0.2443367838859558
Batch 12/64 loss: -0.29257264733314514
Batch 13/64 loss: -0.2856777608394623
Batch 14/64 loss: -0.26376447081565857
Batch 15/64 loss: -0.28412333130836487
Batch 16/64 loss: -0.24090299010276794
Batch 17/64 loss: -0.2756701707839966
Batch 18/64 loss: -0.305863618850708
Batch 19/64 loss: -0.2727542817592621
Batch 20/64 loss: -0.272085577249527
Batch 21/64 loss: -0.2884189486503601
Batch 22/64 loss: -0.27086108922958374
Batch 23/64 loss: -0.26214590668678284
Batch 24/64 loss: -0.28816044330596924
Batch 25/64 loss: -0.2875280976295471
Batch 26/64 loss: -0.24275165796279907
Batch 27/64 loss: -0.28477731347084045
Batch 28/64 loss: -0.25518059730529785
Batch 29/64 loss: -0.2457619607448578
Batch 30/64 loss: -0.27927690744400024
Batch 31/64 loss: -0.252936989068985
Batch 32/64 loss: -0.2728162109851837
Batch 33/64 loss: -0.274920791387558
Batch 34/64 loss: -0.22402682900428772
Batch 35/64 loss: -0.25114506483078003
Batch 36/64 loss: -0.265359103679657
Batch 37/64 loss: -0.26488471031188965
Batch 38/64 loss: -0.2861219644546509
Batch 39/64 loss: -0.25845441222190857
Batch 40/64 loss: -0.2998065948486328
Batch 41/64 loss: -0.30850374698638916
Batch 42/64 loss: -0.25950607657432556
Batch 43/64 loss: -0.26261353492736816
Batch 44/64 loss: -0.2753654420375824
Batch 45/64 loss: -0.23128098249435425
Batch 46/64 loss: -0.19734513759613037
Batch 47/64 loss: -0.2787508964538574
Batch 48/64 loss: -0.30815643072128296
Batch 49/64 loss: -0.27881914377212524
Batch 50/64 loss: -0.2654673457145691
Batch 51/64 loss: -0.21424847841262817
Batch 52/64 loss: -0.28491610288619995
Batch 53/64 loss: -0.2751275300979614
Batch 54/64 loss: -0.26970723271369934
Batch 55/64 loss: -0.30205851793289185
Batch 56/64 loss: -0.26914575695991516
Batch 57/64 loss: -0.2779529094696045
Batch 58/64 loss: -0.2597450017929077
Batch 59/64 loss: -0.26457515358924866
Batch 60/64 loss: -0.2764892876148224
Batch 61/64 loss: -0.2663203179836273
Batch 62/64 loss: -0.28419578075408936
Batch 63/64 loss: -0.2757037878036499
Batch 64/64 loss: -0.27462172508239746
Epoch 231  Train loss: -0.27322343564500995  Val loss: 0.024406307136889585
Epoch 232
-------------------------------
Batch 1/64 loss: -0.2688679099082947
Batch 2/64 loss: -0.2701019048690796
Batch 3/64 loss: -0.27134960889816284
Batch 4/64 loss: -0.2916306257247925
Batch 5/64 loss: -0.2856667637825012
Batch 6/64 loss: -0.26773515343666077
Batch 7/64 loss: -0.29702237248420715
Batch 8/64 loss: -0.2709606885910034
Batch 9/64 loss: -0.2669583559036255
Batch 10/64 loss: -0.2994202971458435
Batch 11/64 loss: -0.2390415370464325
Batch 12/64 loss: -0.223573237657547
Batch 13/64 loss: -0.26041948795318604
Batch 14/64 loss: -0.2894655168056488
Batch 15/64 loss: -0.2794787585735321
Batch 16/64 loss: -0.2627514600753784
Batch 17/64 loss: -0.2937283515930176
Batch 18/64 loss: -0.23740243911743164
Batch 19/64 loss: -0.26107358932495117
Batch 20/64 loss: -0.29464399814605713
Batch 21/64 loss: -0.26321202516555786
Batch 22/64 loss: -0.22781097888946533
Batch 23/64 loss: -0.269227534532547
Batch 24/64 loss: -0.28228044509887695
Batch 25/64 loss: -0.2862253785133362
Batch 26/64 loss: -0.26645541191101074
Batch 27/64 loss: -0.25110331177711487
Batch 28/64 loss: -0.2320261001586914
Batch 29/64 loss: -0.2596006989479065
Batch 30/64 loss: -0.2813515365123749
Batch 31/64 loss: -0.2930435538291931
Batch 32/64 loss: -0.25969910621643066
Batch 33/64 loss: -0.25293034315109253
Batch 34/64 loss: -0.27993547916412354
Batch 35/64 loss: -0.30902254581451416
Batch 36/64 loss: -0.2651226818561554
Batch 37/64 loss: -0.2629125416278839
Batch 38/64 loss: -0.2848539352416992
Batch 39/64 loss: -0.295778751373291
Batch 40/64 loss: -0.2822231352329254
Batch 41/64 loss: -0.2622775733470917
Batch 42/64 loss: -0.2744971513748169
Batch 43/64 loss: -0.2924139201641083
Batch 44/64 loss: -0.28502315282821655
Batch 45/64 loss: -0.29227930307388306
Batch 46/64 loss: -0.29400116205215454
Batch 47/64 loss: -0.29645830392837524
Batch 48/64 loss: -0.2953382432460785
Batch 49/64 loss: -0.26255863904953003
Batch 50/64 loss: -0.29852837324142456
Batch 51/64 loss: -0.26656749844551086
Batch 52/64 loss: -0.2701379954814911
Batch 53/64 loss: -0.2667132616043091
Batch 54/64 loss: -0.210191011428833
Batch 55/64 loss: -0.2624717056751251
Batch 56/64 loss: -0.29380911588668823
Batch 57/64 loss: -0.2878575026988983
Batch 58/64 loss: -0.256286084651947
Batch 59/64 loss: -0.24058851599693298
Batch 60/64 loss: -0.3148842751979828
Batch 61/64 loss: -0.2903329133987427
Batch 62/64 loss: -0.28821370005607605
Batch 63/64 loss: -0.2745538353919983
Batch 64/64 loss: -0.29575425386428833
Epoch 232  Train loss: -0.27347304283403884  Val loss: 0.021466797160119126
Epoch 233
-------------------------------
Batch 1/64 loss: -0.2912086248397827
Batch 2/64 loss: -0.28426188230514526
Batch 3/64 loss: -0.2929283380508423
Batch 4/64 loss: -0.2934742867946625
Batch 5/64 loss: -0.25773343443870544
Batch 6/64 loss: -0.29565703868865967
Batch 7/64 loss: -0.279902845621109
Batch 8/64 loss: -0.27849841117858887
Batch 9/64 loss: -0.2906004786491394
Batch 10/64 loss: -0.2523716390132904
Batch 11/64 loss: -0.2763657867908478
Batch 12/64 loss: -0.2679178714752197
Batch 13/64 loss: -0.2778790295124054
Batch 14/64 loss: -0.2771936058998108
Batch 15/64 loss: -0.2985902428627014
Batch 16/64 loss: -0.2779114842414856
Batch 17/64 loss: -0.2858586013317108
Batch 18/64 loss: -0.24653232097625732
Batch 19/64 loss: -0.24789917469024658
Batch 20/64 loss: -0.27764809131622314
Batch 21/64 loss: -0.286784827709198
Batch 22/64 loss: -0.2901814579963684
Batch 23/64 loss: -0.27236849069595337
Batch 24/64 loss: -0.2814811170101166
Batch 25/64 loss: -0.2487223744392395
Batch 26/64 loss: -0.2687853276729584
Batch 27/64 loss: -0.3245294690132141
Batch 28/64 loss: -0.271524578332901
Batch 29/64 loss: -0.27010464668273926
Batch 30/64 loss: -0.2870362401008606
Batch 31/64 loss: -0.27088600397109985
Batch 32/64 loss: -0.2947506010532379
Batch 33/64 loss: -0.283159077167511
Batch 34/64 loss: -0.28684961795806885
Batch 35/64 loss: -0.31143802404403687
Batch 36/64 loss: -0.2606682777404785
Batch 37/64 loss: -0.29032963514328003
Batch 38/64 loss: -0.27608972787857056
Batch 39/64 loss: -0.3086107075214386
Batch 40/64 loss: -0.2765701711177826
Batch 41/64 loss: -0.3006994426250458
Batch 42/64 loss: -0.2910623848438263
Batch 43/64 loss: -0.25818341970443726
Batch 44/64 loss: -0.24742072820663452
Batch 45/64 loss: -0.23782452940940857
Batch 46/64 loss: -0.253545880317688
Batch 47/64 loss: -0.2702428102493286
Batch 48/64 loss: -0.2927134335041046
Batch 49/64 loss: -0.2657570242881775
Batch 50/64 loss: -0.2776126563549042
Batch 51/64 loss: -0.28838929533958435
Batch 52/64 loss: -0.2712934911251068
Batch 53/64 loss: -0.2884518504142761
Batch 54/64 loss: -0.27843812108039856
Batch 55/64 loss: -0.2737157344818115
Batch 56/64 loss: -0.2766730487346649
Batch 57/64 loss: -0.27800413966178894
Batch 58/64 loss: -0.22846660017967224
Batch 59/64 loss: -0.29502803087234497
Batch 60/64 loss: -0.2651819586753845
Batch 61/64 loss: -0.26563185453414917
Batch 62/64 loss: -0.2954768240451813
Batch 63/64 loss: -0.2734845280647278
Batch 64/64 loss: -0.3058035671710968
Epoch 233  Train loss: -0.2778659454747742  Val loss: 0.021506923990151316
Epoch 234
-------------------------------
Batch 1/64 loss: -0.3157165050506592
Batch 2/64 loss: -0.30883586406707764
Batch 3/64 loss: -0.2945321798324585
Batch 4/64 loss: -0.26018205285072327
Batch 5/64 loss: -0.2550039291381836
Batch 6/64 loss: -0.25100284814834595
Batch 7/64 loss: -0.274960458278656
Batch 8/64 loss: -0.272430956363678
Batch 9/64 loss: -0.2728939652442932
Batch 10/64 loss: -0.2962370812892914
Batch 11/64 loss: -0.2603326439857483
Batch 12/64 loss: -0.24532687664031982
Batch 13/64 loss: -0.3007785379886627
Batch 14/64 loss: -0.24962231516838074
Batch 15/64 loss: -0.2799145579338074
Batch 16/64 loss: -0.2869844436645508
Batch 17/64 loss: -0.21543890237808228
Batch 18/64 loss: -0.2668873071670532
Batch 19/64 loss: -0.2847904562950134
Batch 20/64 loss: -0.2824506461620331
Batch 21/64 loss: -0.27644047141075134
Batch 22/64 loss: -0.27389657497406006
Batch 23/64 loss: -0.2548356354236603
Batch 24/64 loss: -0.28543341159820557
Batch 25/64 loss: -0.2271462082862854
Batch 26/64 loss: -0.2810017466545105
Batch 27/64 loss: -0.23776397109031677
Batch 28/64 loss: -0.30196094512939453
Batch 29/64 loss: -0.2888888716697693
Batch 30/64 loss: -0.2702794671058655
Batch 31/64 loss: -0.31748104095458984
Batch 32/64 loss: -0.23425966501235962
Batch 33/64 loss: -0.29116687178611755
Batch 34/64 loss: -0.2841523289680481
Batch 35/64 loss: -0.25069868564605713
Batch 36/64 loss: -0.28816404938697815
Batch 37/64 loss: -0.29746198654174805
Batch 38/64 loss: -0.25698474049568176
Batch 39/64 loss: -0.24367201328277588
Batch 40/64 loss: -0.2729393541812897
Batch 41/64 loss: -0.22945120930671692
Batch 42/64 loss: -0.27080968022346497
Batch 43/64 loss: -0.2830687165260315
Batch 44/64 loss: -0.23714366555213928
Batch 45/64 loss: -0.2996993660926819
Batch 46/64 loss: -0.2858700752258301
Batch 47/64 loss: -0.2921532690525055
Batch 48/64 loss: -0.24798396229743958
Batch 49/64 loss: -0.2573530375957489
Batch 50/64 loss: -0.30012089014053345
Batch 51/64 loss: -0.2954810857772827
Batch 52/64 loss: -0.24426400661468506
Batch 53/64 loss: -0.271953821182251
Batch 54/64 loss: -0.28532320261001587
Batch 55/64 loss: -0.2933083772659302
Batch 56/64 loss: -0.30113548040390015
Batch 57/64 loss: -0.27612555027008057
Batch 58/64 loss: -0.2576419711112976
Batch 59/64 loss: -0.29687535762786865
Batch 60/64 loss: -0.2827758193016052
Batch 61/64 loss: -0.261142760515213
Batch 62/64 loss: -0.28586289286613464
Batch 63/64 loss: -0.2651718556880951
Batch 64/64 loss: -0.28736916184425354
Epoch 234  Train loss: -0.2736500939902137  Val loss: 0.02394101464051971
Epoch 235
-------------------------------
Batch 1/64 loss: -0.2664545774459839
Batch 2/64 loss: -0.29398655891418457
Batch 3/64 loss: -0.2657378911972046
Batch 4/64 loss: -0.27803653478622437
Batch 5/64 loss: -0.24091261625289917
Batch 6/64 loss: -0.2686586380004883
Batch 7/64 loss: -0.2916008532047272
Batch 8/64 loss: -0.25824588537216187
Batch 9/64 loss: -0.2768440544605255
Batch 10/64 loss: -0.2644825577735901
Batch 11/64 loss: -0.20214080810546875
Batch 12/64 loss: -0.25643390417099
Batch 13/64 loss: -0.28169479966163635
Batch 14/64 loss: -0.2502850890159607
Batch 15/64 loss: -0.2964269816875458
Batch 16/64 loss: -0.2962043285369873
Batch 17/64 loss: -0.2906108796596527
Batch 18/64 loss: -0.3144741654396057
Batch 19/64 loss: -0.2779739797115326
Batch 20/64 loss: -0.26098132133483887
Batch 21/64 loss: -0.28934532403945923
Batch 22/64 loss: -0.3126220703125
Batch 23/64 loss: -0.2817380428314209
Batch 24/64 loss: -0.289702445268631
Batch 25/64 loss: -0.26163971424102783
Batch 26/64 loss: -0.27838999032974243
Batch 27/64 loss: -0.24201732873916626
Batch 28/64 loss: -0.27918803691864014
Batch 29/64 loss: -0.274067223072052
Batch 30/64 loss: -0.2795504331588745
Batch 31/64 loss: -0.24882188439369202
Batch 32/64 loss: -0.2584255337715149
Batch 33/64 loss: -0.29280662536621094
Batch 34/64 loss: -0.3099503815174103
Batch 35/64 loss: -0.23068934679031372
Batch 36/64 loss: -0.2762829661369324
Batch 37/64 loss: -0.29502809047698975
Batch 38/64 loss: -0.29699379205703735
Batch 39/64 loss: -0.2584753930568695
Batch 40/64 loss: -0.25971749424934387
Batch 41/64 loss: -0.2895129919052124
Batch 42/64 loss: -0.2693430185317993
Batch 43/64 loss: -0.29358506202697754
Batch 44/64 loss: -0.2973073720932007
Batch 45/64 loss: -0.2533576190471649
Batch 46/64 loss: -0.2958865165710449
Batch 47/64 loss: -0.25080645084381104
Batch 48/64 loss: -0.26217615604400635
Batch 49/64 loss: -0.29796451330184937
Batch 50/64 loss: -0.2698702812194824
Batch 51/64 loss: -0.2663344740867615
Batch 52/64 loss: -0.2860739827156067
Batch 53/64 loss: -0.2756050229072571
Batch 54/64 loss: -0.26172709465026855
Batch 55/64 loss: -0.2808144688606262
Batch 56/64 loss: -0.281858891248703
Batch 57/64 loss: -0.2712922692298889
Batch 58/64 loss: -0.30441367626190186
Batch 59/64 loss: -0.28248101472854614
Batch 60/64 loss: -0.2855154871940613
Batch 61/64 loss: -0.29257750511169434
Batch 62/64 loss: -0.25360244512557983
Batch 63/64 loss: -0.30741703510284424
Batch 64/64 loss: -0.2643773853778839
Epoch 235  Train loss: -0.27569325379296844  Val loss: 0.022994337008171474
Epoch 236
-------------------------------
Batch 1/64 loss: -0.310470849275589
Batch 2/64 loss: -0.2469019889831543
Batch 3/64 loss: -0.2844173312187195
Batch 4/64 loss: -0.2924633324146271
Batch 5/64 loss: -0.29563164710998535
Batch 6/64 loss: -0.2984095811843872
Batch 7/64 loss: -0.2757376730442047
Batch 8/64 loss: -0.25596940517425537
Batch 9/64 loss: -0.2879585027694702
Batch 10/64 loss: -0.317574679851532
Batch 11/64 loss: -0.25300174951553345
Batch 12/64 loss: -0.2905229330062866
Batch 13/64 loss: -0.3039846420288086
Batch 14/64 loss: -0.2345508635044098
Batch 15/64 loss: -0.2748967409133911
Batch 16/64 loss: -0.2914130985736847
Batch 17/64 loss: -0.24875041842460632
Batch 18/64 loss: -0.3188285529613495
Batch 19/64 loss: -0.31173133850097656
Batch 20/64 loss: -0.2978138327598572
Batch 21/64 loss: -0.2613799273967743
Batch 22/64 loss: -0.2782592177391052
Batch 23/64 loss: -0.23656177520751953
Batch 24/64 loss: -0.2824578583240509
Batch 25/64 loss: -0.2426832616329193
Batch 26/64 loss: -0.2790040969848633
Batch 27/64 loss: -0.26669517159461975
Batch 28/64 loss: -0.28278833627700806
Batch 29/64 loss: -0.2938019633293152
Batch 30/64 loss: -0.27042368054389954
Batch 31/64 loss: -0.2798580825328827
Batch 32/64 loss: -0.2939564883708954
Batch 33/64 loss: -0.2978040277957916
Batch 34/64 loss: -0.2885260283946991
Batch 35/64 loss: -0.2971017062664032
Batch 36/64 loss: -0.26388388872146606
Batch 37/64 loss: -0.2823121249675751
Batch 38/64 loss: -0.2371942698955536
Batch 39/64 loss: -0.2922511100769043
Batch 40/64 loss: -0.26906585693359375
Batch 41/64 loss: -0.23337560892105103
Batch 42/64 loss: -0.25788217782974243
Batch 43/64 loss: -0.25713658332824707
Batch 44/64 loss: -0.2465095818042755
Batch 45/64 loss: -0.2698732018470764
Batch 46/64 loss: -0.26375067234039307
Batch 47/64 loss: -0.2723197937011719
Batch 48/64 loss: -0.2800080180168152
Batch 49/64 loss: -0.2918367385864258
Batch 50/64 loss: -0.28212687373161316
Batch 51/64 loss: -0.29339808225631714
Batch 52/64 loss: -0.27244287729263306
Batch 53/64 loss: -0.3061179518699646
Batch 54/64 loss: -0.24698501825332642
Batch 55/64 loss: -0.2639208734035492
Batch 56/64 loss: -0.29191893339157104
Batch 57/64 loss: -0.2756536602973938
Batch 58/64 loss: -0.24153417348861694
Batch 59/64 loss: -0.23939383029937744
Batch 60/64 loss: -0.2707066535949707
Batch 61/64 loss: -0.23697495460510254
Batch 62/64 loss: -0.31702613830566406
Batch 63/64 loss: -0.27660682797431946
Batch 64/64 loss: -0.28251755237579346
Epoch 236  Train loss: -0.2758654968411315  Val loss: 0.023211381689379716
Epoch 237
-------------------------------
Batch 1/64 loss: -0.27963438630104065
Batch 2/64 loss: -0.3090924322605133
Batch 3/64 loss: -0.31234875321388245
Batch 4/64 loss: -0.27739423513412476
Batch 5/64 loss: -0.2350480556488037
Batch 6/64 loss: -0.30224236845970154
Batch 7/64 loss: -0.26748329401016235
Batch 8/64 loss: -0.29115790128707886
Batch 9/64 loss: -0.2552121579647064
Batch 10/64 loss: -0.28657108545303345
Batch 11/64 loss: -0.28345251083374023
Batch 12/64 loss: -0.26850852370262146
Batch 13/64 loss: -0.26106733083724976
Batch 14/64 loss: -0.2617250978946686
Batch 15/64 loss: -0.26321226358413696
Batch 16/64 loss: -0.2953813076019287
Batch 17/64 loss: -0.2818696200847626
Batch 18/64 loss: -0.25870752334594727
Batch 19/64 loss: -0.2659006118774414
Batch 20/64 loss: -0.2896970510482788
Batch 21/64 loss: -0.25750067830085754
Batch 22/64 loss: -0.28763216733932495
Batch 23/64 loss: -0.2833717167377472
Batch 24/64 loss: -0.3091568350791931
Batch 25/64 loss: -0.24802246689796448
Batch 26/64 loss: -0.2644709348678589
Batch 27/64 loss: -0.30779123306274414
Batch 28/64 loss: -0.253057062625885
Batch 29/64 loss: -0.2711457908153534
Batch 30/64 loss: -0.27244940400123596
Batch 31/64 loss: -0.2887369990348816
Batch 32/64 loss: -0.2566860318183899
Batch 33/64 loss: -0.2845427393913269
Batch 34/64 loss: -0.28975778818130493
Batch 35/64 loss: -0.2792469561100006
Batch 36/64 loss: -0.298403263092041
Batch 37/64 loss: -0.23519691824913025
Batch 38/64 loss: -0.2829821705818176
Batch 39/64 loss: -0.28025782108306885
Batch 40/64 loss: -0.2690235674381256
Batch 41/64 loss: -0.25559937953948975
Batch 42/64 loss: -0.24695295095443726
Batch 43/64 loss: -0.3072265684604645
Batch 44/64 loss: -0.30379995703697205
Batch 45/64 loss: -0.294202983379364
Batch 46/64 loss: -0.27697038650512695
Batch 47/64 loss: -0.29284828901290894
Batch 48/64 loss: -0.22262245416641235
Batch 49/64 loss: -0.2961246371269226
Batch 50/64 loss: -0.2857814133167267
Batch 51/64 loss: -0.2688480019569397
Batch 52/64 loss: -0.26589569449424744
Batch 53/64 loss: -0.2916257977485657
Batch 54/64 loss: -0.24315756559371948
Batch 55/64 loss: -0.2456725835800171
Batch 56/64 loss: -0.30754929780960083
Batch 57/64 loss: -0.27246755361557007
Batch 58/64 loss: -0.30184221267700195
Batch 59/64 loss: -0.24882715940475464
Batch 60/64 loss: -0.22989344596862793
Batch 61/64 loss: -0.2847685217857361
Batch 62/64 loss: -0.25112807750701904
Batch 63/64 loss: -0.2888050675392151
Batch 64/64 loss: -0.2736683487892151
Epoch 237  Train loss: -0.2753411814278247  Val loss: 0.02517515973946483
Epoch 238
-------------------------------
Batch 1/64 loss: -0.2559061348438263
Batch 2/64 loss: -0.3073902130126953
Batch 3/64 loss: -0.31709790229797363
Batch 4/64 loss: -0.30900609493255615
Batch 5/64 loss: -0.2780989706516266
Batch 6/64 loss: -0.26136231422424316
Batch 7/64 loss: -0.2804566025733948
Batch 8/64 loss: -0.2753554880619049
Batch 9/64 loss: -0.24823981523513794
Batch 10/64 loss: -0.30620890855789185
Batch 11/64 loss: -0.3075534701347351
Batch 12/64 loss: -0.26925110816955566
Batch 13/64 loss: -0.26971954107284546
Batch 14/64 loss: -0.3020111918449402
Batch 15/64 loss: -0.3054695725440979
Batch 16/64 loss: -0.2898123264312744
Batch 17/64 loss: -0.2534870505332947
Batch 18/64 loss: -0.28709372878074646
Batch 19/64 loss: -0.22079062461853027
Batch 20/64 loss: -0.2972753942012787
Batch 21/64 loss: -0.2860555946826935
Batch 22/64 loss: -0.26528987288475037
Batch 23/64 loss: -0.3072216212749481
Batch 24/64 loss: -0.2898806035518646
Batch 25/64 loss: -0.2407573163509369
Batch 26/64 loss: -0.2795376777648926
Batch 27/64 loss: -0.27263230085372925
Batch 28/64 loss: -0.30280059576034546
Batch 29/64 loss: -0.2949134409427643
Batch 30/64 loss: -0.3013000786304474
Batch 31/64 loss: -0.2575216591358185
Batch 32/64 loss: -0.2798425257205963
Batch 33/64 loss: -0.30998826026916504
Batch 34/64 loss: -0.29774683713912964
Batch 35/64 loss: -0.28324052691459656
Batch 36/64 loss: -0.2703167498111725
Batch 37/64 loss: -0.29110270738601685
Batch 38/64 loss: -0.28005295991897583
Batch 39/64 loss: -0.2952048182487488
Batch 40/64 loss: -0.2527996897697449
Batch 41/64 loss: -0.280894011259079
Batch 42/64 loss: -0.269915908575058
Batch 43/64 loss: -0.22852429747581482
Batch 44/64 loss: -0.2862176299095154
Batch 45/64 loss: -0.28363317251205444
Batch 46/64 loss: -0.283817857503891
Batch 47/64 loss: -0.2541775703430176
Batch 48/64 loss: -0.26570266485214233
Batch 49/64 loss: -0.24214553833007812
Batch 50/64 loss: -0.2900114357471466
Batch 51/64 loss: -0.2881019711494446
Batch 52/64 loss: -0.21934211254119873
Batch 53/64 loss: -0.2750707268714905
Batch 54/64 loss: -0.24553054571151733
Batch 55/64 loss: -0.2754404842853546
Batch 56/64 loss: -0.3051051199436188
Batch 57/64 loss: -0.28120356798171997
Batch 58/64 loss: -0.29231029748916626
Batch 59/64 loss: -0.2898666560649872
Batch 60/64 loss: -0.2952602505683899
Batch 61/64 loss: -0.29731112718582153
Batch 62/64 loss: -0.2547345459461212
Batch 63/64 loss: -0.25969159603118896
Batch 64/64 loss: -0.28795677423477173
Epoch 238  Train loss: -0.2788983366068672  Val loss: 0.025966744037837917
Epoch 239
-------------------------------
Batch 1/64 loss: -0.27722564339637756
Batch 2/64 loss: -0.25147223472595215
Batch 3/64 loss: -0.3028956949710846
Batch 4/64 loss: -0.2605642080307007
Batch 5/64 loss: -0.2803846001625061
Batch 6/64 loss: -0.2852466106414795
Batch 7/64 loss: -0.28760385513305664
Batch 8/64 loss: -0.29400426149368286
Batch 9/64 loss: -0.2953118085861206
Batch 10/64 loss: -0.29921090602874756
Batch 11/64 loss: -0.30179598927497864
Batch 12/64 loss: -0.24410822987556458
Batch 13/64 loss: -0.27770596742630005
Batch 14/64 loss: -0.317880779504776
Batch 15/64 loss: -0.27880972623825073
Batch 16/64 loss: -0.27559179067611694
Batch 17/64 loss: -0.2305651307106018
Batch 18/64 loss: -0.27751877903938293
Batch 19/64 loss: -0.3024873733520508
Batch 20/64 loss: -0.27810433506965637
Batch 21/64 loss: -0.23857003450393677
Batch 22/64 loss: -0.27942219376564026
Batch 23/64 loss: -0.3077811598777771
Batch 24/64 loss: -0.28412073850631714
Batch 25/64 loss: -0.27724704146385193
Batch 26/64 loss: -0.2710806727409363
Batch 27/64 loss: -0.28301459550857544
Batch 28/64 loss: -0.28218650817871094
Batch 29/64 loss: -0.24335888028144836
Batch 30/64 loss: -0.28955280780792236
Batch 31/64 loss: -0.24743574857711792
Batch 32/64 loss: -0.26250165700912476
Batch 33/64 loss: -0.28134918212890625
Batch 34/64 loss: -0.29196643829345703
Batch 35/64 loss: -0.26504409313201904
Batch 36/64 loss: -0.2738034129142761
Batch 37/64 loss: -0.2518130838871002
Batch 38/64 loss: -0.2900543212890625
Batch 39/64 loss: -0.2808208465576172
Batch 40/64 loss: -0.2662281095981598
Batch 41/64 loss: -0.2919943332672119
Batch 42/64 loss: -0.318981796503067
Batch 43/64 loss: -0.2663458585739136
Batch 44/64 loss: -0.25794699788093567
Batch 45/64 loss: -0.26411229372024536
Batch 46/64 loss: -0.2913014888763428
Batch 47/64 loss: -0.29710495471954346
Batch 48/64 loss: -0.2704647481441498
Batch 49/64 loss: -0.2780447006225586
Batch 50/64 loss: -0.2825736999511719
Batch 51/64 loss: -0.27392497658729553
Batch 52/64 loss: -0.22014504671096802
Batch 53/64 loss: -0.25999122858047485
Batch 54/64 loss: -0.2733526825904846
Batch 55/64 loss: -0.2751019597053528
Batch 56/64 loss: -0.30206960439682007
Batch 57/64 loss: -0.2907359302043915
Batch 58/64 loss: -0.2684754729270935
Batch 59/64 loss: -0.30435824394226074
Batch 60/64 loss: -0.28128862380981445
Batch 61/64 loss: -0.31297188997268677
Batch 62/64 loss: -0.2598569989204407
Batch 63/64 loss: -0.25530093908309937
Batch 64/64 loss: -0.27194440364837646
Epoch 239  Train loss: -0.2774312505535051  Val loss: 0.024871814291911435
Epoch 240
-------------------------------
Batch 1/64 loss: -0.31051480770111084
Batch 2/64 loss: -0.26700150966644287
Batch 3/64 loss: -0.3004363179206848
Batch 4/64 loss: -0.2506333589553833
Batch 5/64 loss: -0.24904462695121765
Batch 6/64 loss: -0.30677175521850586
Batch 7/64 loss: -0.2910219132900238
Batch 8/64 loss: -0.3016395568847656
Batch 9/64 loss: -0.2827742099761963
Batch 10/64 loss: -0.27931201457977295
Batch 11/64 loss: -0.29080963134765625
Batch 12/64 loss: -0.27907079458236694
Batch 13/64 loss: -0.29547104239463806
Batch 14/64 loss: -0.29817500710487366
Batch 15/64 loss: -0.28418952226638794
Batch 16/64 loss: -0.2931566834449768
Batch 17/64 loss: -0.29191818833351135
Batch 18/64 loss: -0.2780114412307739
Batch 19/64 loss: -0.26970377564430237
Batch 20/64 loss: -0.312648206949234
Batch 21/64 loss: -0.19988441467285156
Batch 22/64 loss: -0.2643316686153412
Batch 23/64 loss: -0.2795296907424927
Batch 24/64 loss: -0.21625983715057373
Batch 25/64 loss: -0.30862581729888916
Batch 26/64 loss: -0.262373149394989
Batch 27/64 loss: -0.3154681921005249
Batch 28/64 loss: -0.2754375636577606
Batch 29/64 loss: -0.25368231534957886
Batch 30/64 loss: -0.3001546263694763
Batch 31/64 loss: -0.31594666838645935
Batch 32/64 loss: -0.3133258819580078
Batch 33/64 loss: -0.27694404125213623
Batch 34/64 loss: -0.27613192796707153
Batch 35/64 loss: -0.25138574838638306
Batch 36/64 loss: -0.285761296749115
Batch 37/64 loss: -0.3031211197376251
Batch 38/64 loss: -0.28595221042633057
Batch 39/64 loss: -0.2928699553012848
Batch 40/64 loss: -0.2871025800704956
Batch 41/64 loss: -0.2719722390174866
Batch 42/64 loss: -0.31284505128860474
Batch 43/64 loss: -0.30275145173072815
Batch 44/64 loss: -0.28853583335876465
Batch 45/64 loss: -0.26879167556762695
Batch 46/64 loss: -0.2758875787258148
Batch 47/64 loss: -0.28607189655303955
Batch 48/64 loss: -0.2823401093482971
Batch 49/64 loss: -0.263231098651886
Batch 50/64 loss: -0.27738291025161743
Batch 51/64 loss: -0.2505301833152771
Batch 52/64 loss: -0.3132491707801819
Batch 53/64 loss: -0.2501567006111145
Batch 54/64 loss: -0.2641841471195221
Batch 55/64 loss: -0.30823320150375366
Batch 56/64 loss: -0.27575159072875977
Batch 57/64 loss: -0.2666441798210144
Batch 58/64 loss: -0.28375136852264404
Batch 59/64 loss: -0.27244722843170166
Batch 60/64 loss: -0.2603932023048401
Batch 61/64 loss: -0.29579266905784607
Batch 62/64 loss: -0.297783762216568
Batch 63/64 loss: -0.2767466604709625
Batch 64/64 loss: -0.2710701525211334
Epoch 240  Train loss: -0.2814959936282214  Val loss: 0.024175456709058834
Epoch 241
-------------------------------
Batch 1/64 loss: -0.25898250937461853
Batch 2/64 loss: -0.26443612575531006
Batch 3/64 loss: -0.31972116231918335
Batch 4/64 loss: -0.28778237104415894
Batch 5/64 loss: -0.30115076899528503
Batch 6/64 loss: -0.22408556938171387
Batch 7/64 loss: -0.25089290738105774
Batch 8/64 loss: -0.2538778781890869
Batch 9/64 loss: -0.3063927888870239
Batch 10/64 loss: -0.24514687061309814
Batch 11/64 loss: -0.28762680292129517
Batch 12/64 loss: -0.2974543571472168
Batch 13/64 loss: -0.3160087466239929
Batch 14/64 loss: -0.30664658546447754
Batch 15/64 loss: -0.29172056913375854
Batch 16/64 loss: -0.2904834747314453
Batch 17/64 loss: -0.2489423155784607
Batch 18/64 loss: -0.29729485511779785
Batch 19/64 loss: -0.2707539200782776
Batch 20/64 loss: -0.2749655842781067
Batch 21/64 loss: -0.27861732244491577
Batch 22/64 loss: -0.23925164341926575
Batch 23/64 loss: -0.2887105941772461
Batch 24/64 loss: -0.2785865068435669
Batch 25/64 loss: -0.2856740355491638
Batch 26/64 loss: -0.28729379177093506
Batch 27/64 loss: -0.3057997226715088
Batch 28/64 loss: -0.29780685901641846
Batch 29/64 loss: -0.30311769247055054
Batch 30/64 loss: -0.23951810598373413
Batch 31/64 loss: -0.2921384572982788
Batch 32/64 loss: -0.2727969288825989
Batch 33/64 loss: -0.2464239001274109
Batch 34/64 loss: -0.2546139359474182
Batch 35/64 loss: -0.3025924265384674
Batch 36/64 loss: -0.2576645016670227
Batch 37/64 loss: -0.2974124550819397
Batch 38/64 loss: -0.30533555150032043
Batch 39/64 loss: -0.29402732849121094
Batch 40/64 loss: -0.3033601939678192
Batch 41/64 loss: -0.29376548528671265
Batch 42/64 loss: -0.24259909987449646
Batch 43/64 loss: -0.2776361107826233
Batch 44/64 loss: -0.25351032614707947
Batch 45/64 loss: -0.31096452474594116
Batch 46/64 loss: -0.2584236264228821
Batch 47/64 loss: -0.29626691341400146
Batch 48/64 loss: -0.2955626845359802
Batch 49/64 loss: -0.26443496346473694
Batch 50/64 loss: -0.3152170181274414
Batch 51/64 loss: -0.2869396209716797
Batch 52/64 loss: -0.3171154260635376
Batch 53/64 loss: -0.2843952178955078
Batch 54/64 loss: -0.2791783809661865
Batch 55/64 loss: -0.3261544108390808
Batch 56/64 loss: -0.25468242168426514
Batch 57/64 loss: -0.31046155095100403
Batch 58/64 loss: -0.29218170046806335
Batch 59/64 loss: -0.27392908930778503
Batch 60/64 loss: -0.28484851121902466
Batch 61/64 loss: -0.2751249074935913
Batch 62/64 loss: -0.29097387194633484
Batch 63/64 loss: -0.2940724194049835
Batch 64/64 loss: -0.29524990916252136
Epoch 241  Train loss: -0.2827448444039214  Val loss: 0.02354367819848339
Epoch 242
-------------------------------
Batch 1/64 loss: -0.2719120383262634
Batch 2/64 loss: -0.24780797958374023
Batch 3/64 loss: -0.27710461616516113
Batch 4/64 loss: -0.2816375494003296
Batch 5/64 loss: -0.2776815891265869
Batch 6/64 loss: -0.3003165125846863
Batch 7/64 loss: -0.30636468529701233
Batch 8/64 loss: -0.2839687168598175
Batch 9/64 loss: -0.2960379719734192
Batch 10/64 loss: -0.2981117069721222
Batch 11/64 loss: -0.2941729724407196
Batch 12/64 loss: -0.2798681855201721
Batch 13/64 loss: -0.3149796426296234
Batch 14/64 loss: -0.28384751081466675
Batch 15/64 loss: -0.28347527980804443
Batch 16/64 loss: -0.2780367434024811
Batch 17/64 loss: -0.2980842888355255
Batch 18/64 loss: -0.28827744722366333
Batch 19/64 loss: -0.30048930644989014
Batch 20/64 loss: -0.28346383571624756
Batch 21/64 loss: -0.2956838607788086
Batch 22/64 loss: -0.273621529340744
Batch 23/64 loss: -0.28305310010910034
Batch 24/64 loss: -0.2545938491821289
Batch 25/64 loss: -0.2437777817249298
Batch 26/64 loss: -0.3034130930900574
Batch 27/64 loss: -0.2912771701812744
Batch 28/64 loss: -0.27212899923324585
Batch 29/64 loss: -0.287784218788147
Batch 30/64 loss: -0.26679545640945435
Batch 31/64 loss: -0.25690585374832153
Batch 32/64 loss: -0.2761681079864502
Batch 33/64 loss: -0.3019823431968689
Batch 34/64 loss: -0.2843780517578125
Batch 35/64 loss: -0.2916666269302368
Batch 36/64 loss: -0.2344129979610443
Batch 37/64 loss: -0.2585729956626892
Batch 38/64 loss: -0.2757793962955475
Batch 39/64 loss: -0.3060203194618225
Batch 40/64 loss: -0.2760239839553833
Batch 41/64 loss: -0.29925620555877686
Batch 42/64 loss: -0.2520925998687744
Batch 43/64 loss: -0.2826043963432312
Batch 44/64 loss: -0.3102668523788452
Batch 45/64 loss: -0.2843683362007141
Batch 46/64 loss: -0.27113544940948486
Batch 47/64 loss: -0.2518713176250458
Batch 48/64 loss: -0.29977405071258545
Batch 49/64 loss: -0.273703008890152
Batch 50/64 loss: -0.2885083258152008
Batch 51/64 loss: -0.27470260858535767
Batch 52/64 loss: -0.29791271686553955
Batch 53/64 loss: -0.31084245443344116
Batch 54/64 loss: -0.31666600704193115
Batch 55/64 loss: -0.27275723218917847
Batch 56/64 loss: -0.27916398644447327
Batch 57/64 loss: -0.32142356038093567
Batch 58/64 loss: -0.2695409059524536
Batch 59/64 loss: -0.29108959436416626
Batch 60/64 loss: -0.2267221212387085
Batch 61/64 loss: -0.27692800760269165
Batch 62/64 loss: -0.324861079454422
Batch 63/64 loss: -0.24591481685638428
Batch 64/64 loss: -0.286895751953125
Epoch 242  Train loss: -0.28261891393100513  Val loss: 0.02210489888371471
Epoch 243
-------------------------------
Batch 1/64 loss: -0.32125020027160645
Batch 2/64 loss: -0.2971374988555908
Batch 3/64 loss: -0.28343939781188965
Batch 4/64 loss: -0.27268078923225403
Batch 5/64 loss: -0.22689682245254517
Batch 6/64 loss: -0.28235673904418945
Batch 7/64 loss: -0.3109608590602875
Batch 8/64 loss: -0.28059786558151245
Batch 9/64 loss: -0.28176265954971313
Batch 10/64 loss: -0.2942814230918884
Batch 11/64 loss: -0.3116967976093292
Batch 12/64 loss: -0.25998440384864807
Batch 13/64 loss: -0.3056323528289795
Batch 14/64 loss: -0.24543628096580505
Batch 15/64 loss: -0.32156261801719666
Batch 16/64 loss: -0.26919496059417725
Batch 17/64 loss: -0.28941187262535095
Batch 18/64 loss: -0.23037829995155334
Batch 19/64 loss: -0.3220413327217102
Batch 20/64 loss: -0.28673309087753296
Batch 21/64 loss: -0.2889508605003357
Batch 22/64 loss: -0.3087702989578247
Batch 23/64 loss: -0.2815658450126648
Batch 24/64 loss: -0.2817245423793793
Batch 25/64 loss: -0.27334606647491455
Batch 26/64 loss: -0.2900340259075165
Batch 27/64 loss: -0.28587543964385986
Batch 28/64 loss: -0.28619831800460815
Batch 29/64 loss: -0.2813425660133362
Batch 30/64 loss: -0.3061342239379883
Batch 31/64 loss: -0.28436559438705444
Batch 32/64 loss: -0.31088167428970337
Batch 33/64 loss: -0.276314377784729
Batch 34/64 loss: -0.30697768926620483
Batch 35/64 loss: -0.31468963623046875
Batch 36/64 loss: -0.2565056085586548
Batch 37/64 loss: -0.2657564580440521
Batch 38/64 loss: -0.3100379407405853
Batch 39/64 loss: -0.2684516906738281
Batch 40/64 loss: -0.27768945693969727
Batch 41/64 loss: -0.2672497630119324
Batch 42/64 loss: -0.27971506118774414
Batch 43/64 loss: -0.2783605754375458
Batch 44/64 loss: -0.27777230739593506
Batch 45/64 loss: -0.332297146320343
Batch 46/64 loss: -0.30466389656066895
Batch 47/64 loss: -0.27499669790267944
Batch 48/64 loss: -0.30209165811538696
Batch 49/64 loss: -0.3025350570678711
Batch 50/64 loss: -0.2700749337673187
Batch 51/64 loss: -0.2708665430545807
Batch 52/64 loss: -0.22336521744728088
Batch 53/64 loss: -0.28817394375801086
Batch 54/64 loss: -0.31409895420074463
Batch 55/64 loss: -0.307271271944046
Batch 56/64 loss: -0.23925834894180298
Batch 57/64 loss: -0.28931140899658203
Batch 58/64 loss: -0.30171704292297363
Batch 59/64 loss: -0.2651529312133789
Batch 60/64 loss: -0.3080520033836365
Batch 61/64 loss: -0.24502506852149963
Batch 62/64 loss: -0.2927795350551605
Batch 63/64 loss: -0.27747052907943726
Batch 64/64 loss: -0.27446436882019043
Epoch 243  Train loss: -0.28497563530417047  Val loss: 0.024028729122528916
Epoch 244
-------------------------------
Batch 1/64 loss: -0.29340189695358276
Batch 2/64 loss: -0.3047410547733307
Batch 3/64 loss: -0.27788421511650085
Batch 4/64 loss: -0.3002263903617859
Batch 5/64 loss: -0.30096155405044556
Batch 6/64 loss: -0.3029904365539551
Batch 7/64 loss: -0.25226959586143494
Batch 8/64 loss: -0.2935258746147156
Batch 9/64 loss: -0.23856067657470703
Batch 10/64 loss: -0.3165546655654907
Batch 11/64 loss: -0.27451127767562866
Batch 12/64 loss: -0.28058797121047974
Batch 13/64 loss: -0.289473295211792
Batch 14/64 loss: -0.30995917320251465
Batch 15/64 loss: -0.29834896326065063
Batch 16/64 loss: -0.3122042715549469
Batch 17/64 loss: -0.2749997675418854
Batch 18/64 loss: -0.29114747047424316
Batch 19/64 loss: -0.29555898904800415
Batch 20/64 loss: -0.3052593469619751
Batch 21/64 loss: -0.2840719223022461
Batch 22/64 loss: -0.23482725024223328
Batch 23/64 loss: -0.27428925037384033
Batch 24/64 loss: -0.28189617395401
Batch 25/64 loss: -0.2738496661186218
Batch 26/64 loss: -0.30070868134498596
Batch 27/64 loss: -0.31424787640571594
Batch 28/64 loss: -0.29557228088378906
Batch 29/64 loss: -0.29908567667007446
Batch 30/64 loss: -0.2931438088417053
Batch 31/64 loss: -0.29070621728897095
Batch 32/64 loss: -0.2842707633972168
Batch 33/64 loss: -0.24511367082595825
Batch 34/64 loss: -0.26708292961120605
Batch 35/64 loss: -0.2972494959831238
Batch 36/64 loss: -0.2820437550544739
Batch 37/64 loss: -0.26740962266921997
Batch 38/64 loss: -0.30746084451675415
Batch 39/64 loss: -0.30120888352394104
Batch 40/64 loss: -0.26511770486831665
Batch 41/64 loss: -0.26188045740127563
Batch 42/64 loss: -0.31900322437286377
Batch 43/64 loss: -0.3290170431137085
Batch 44/64 loss: -0.2632429301738739
Batch 45/64 loss: -0.2848394513130188
Batch 46/64 loss: -0.24760258197784424
Batch 47/64 loss: -0.31550467014312744
Batch 48/64 loss: -0.2683706283569336
Batch 49/64 loss: -0.2715853452682495
Batch 50/64 loss: -0.2766348421573639
Batch 51/64 loss: -0.31015127897262573
Batch 52/64 loss: -0.2855483591556549
Batch 53/64 loss: -0.2889614701271057
Batch 54/64 loss: -0.33185189962387085
Batch 55/64 loss: -0.2606106996536255
Batch 56/64 loss: -0.2680501937866211
Batch 57/64 loss: -0.2558819651603699
Batch 58/64 loss: -0.28069108724594116
Batch 59/64 loss: -0.2868375778198242
Batch 60/64 loss: -0.28397876024246216
Batch 61/64 loss: -0.2988535165786743
Batch 62/64 loss: -0.23826751112937927
Batch 63/64 loss: -0.26202261447906494
Batch 64/64 loss: -0.2872910499572754
Epoch 244  Train loss: -0.2851353687398574  Val loss: 0.02450323965131622
Epoch 245
-------------------------------
Batch 1/64 loss: -0.30105531215667725
Batch 2/64 loss: -0.2712092399597168
Batch 3/64 loss: -0.2855410575866699
Batch 4/64 loss: -0.30887916684150696
Batch 5/64 loss: -0.2917553782463074
Batch 6/64 loss: -0.24778592586517334
Batch 7/64 loss: -0.3033725619316101
Batch 8/64 loss: -0.27432820200920105
Batch 9/64 loss: -0.30787748098373413
Batch 10/64 loss: -0.24615946412086487
Batch 11/64 loss: -0.2998259365558624
Batch 12/64 loss: -0.2639034390449524
Batch 13/64 loss: -0.2895033657550812
Batch 14/64 loss: -0.26401665806770325
Batch 15/64 loss: -0.2609795928001404
Batch 16/64 loss: -0.28751394152641296
Batch 17/64 loss: -0.3149034380912781
Batch 18/64 loss: -0.3063811659812927
Batch 19/64 loss: -0.3044831156730652
Batch 20/64 loss: -0.29635995626449585
Batch 21/64 loss: -0.2721787393093109
Batch 22/64 loss: -0.2895815968513489
Batch 23/64 loss: -0.2792137861251831
Batch 24/64 loss: -0.29111385345458984
Batch 25/64 loss: -0.3165013790130615
Batch 26/64 loss: -0.2702564597129822
Batch 27/64 loss: -0.2902756929397583
Batch 28/64 loss: -0.25564098358154297
Batch 29/64 loss: -0.29047298431396484
Batch 30/64 loss: -0.29973581433296204
Batch 31/64 loss: -0.2829449772834778
Batch 32/64 loss: -0.28228169679641724
Batch 33/64 loss: -0.2864004373550415
Batch 34/64 loss: -0.2585911750793457
Batch 35/64 loss: -0.30842316150665283
Batch 36/64 loss: -0.3168163001537323
Batch 37/64 loss: -0.2724578380584717
Batch 38/64 loss: -0.27915239334106445
Batch 39/64 loss: -0.31359177827835083
Batch 40/64 loss: -0.28098973631858826
Batch 41/64 loss: -0.27708253264427185
Batch 42/64 loss: -0.30057573318481445
Batch 43/64 loss: -0.272278368473053
Batch 44/64 loss: -0.26007145643234253
Batch 45/64 loss: -0.25382325053215027
Batch 46/64 loss: -0.2922725975513458
Batch 47/64 loss: -0.24961066246032715
Batch 48/64 loss: -0.23453524708747864
Batch 49/64 loss: -0.283963143825531
Batch 50/64 loss: -0.29821860790252686
Batch 51/64 loss: -0.28292274475097656
Batch 52/64 loss: -0.27351242303848267
Batch 53/64 loss: -0.2819265127182007
Batch 54/64 loss: -0.2956297993659973
Batch 55/64 loss: -0.3125852346420288
Batch 56/64 loss: -0.2508474588394165
Batch 57/64 loss: -0.2563941478729248
Batch 58/64 loss: -0.2814762592315674
Batch 59/64 loss: -0.29768621921539307
Batch 60/64 loss: -0.2762386202812195
Batch 61/64 loss: -0.2976829409599304
Batch 62/64 loss: -0.26267266273498535
Batch 63/64 loss: -0.27863091230392456
Batch 64/64 loss: -0.17080485820770264
Epoch 245  Train loss: -0.28174425667407466  Val loss: 0.02682028439446413
Epoch 246
-------------------------------
Batch 1/64 loss: -0.2976365089416504
Batch 2/64 loss: -0.3014463782310486
Batch 3/64 loss: -0.2599496841430664
Batch 4/64 loss: -0.30993029475212097
Batch 5/64 loss: -0.28213152289390564
Batch 6/64 loss: -0.2593741714954376
Batch 7/64 loss: -0.29013797640800476
Batch 8/64 loss: -0.3035625219345093
Batch 9/64 loss: -0.29700762033462524
Batch 10/64 loss: -0.2963406443595886
Batch 11/64 loss: -0.2581585943698883
Batch 12/64 loss: -0.2891313433647156
Batch 13/64 loss: -0.3044441044330597
Batch 14/64 loss: -0.2847325801849365
Batch 15/64 loss: -0.3035750985145569
Batch 16/64 loss: -0.2949821949005127
Batch 17/64 loss: -0.2629653811454773
Batch 18/64 loss: -0.28787675499916077
Batch 19/64 loss: -0.2309156060218811
Batch 20/64 loss: -0.2775309085845947
Batch 21/64 loss: -0.30521589517593384
Batch 22/64 loss: -0.25053220987319946
Batch 23/64 loss: -0.27707022428512573
Batch 24/64 loss: -0.31862491369247437
Batch 25/64 loss: -0.2604925334453583
Batch 26/64 loss: -0.2738131582736969
Batch 27/64 loss: -0.27653831243515015
Batch 28/64 loss: -0.2774191200733185
Batch 29/64 loss: -0.28884899616241455
Batch 30/64 loss: -0.2849745452404022
Batch 31/64 loss: -0.30803143978118896
Batch 32/64 loss: -0.27396631240844727
Batch 33/64 loss: -0.2907332479953766
Batch 34/64 loss: -0.3139776885509491
Batch 35/64 loss: -0.28552842140197754
Batch 36/64 loss: -0.30414000153541565
Batch 37/64 loss: -0.3234391212463379
Batch 38/64 loss: -0.25909462571144104
Batch 39/64 loss: -0.3166385889053345
Batch 40/64 loss: -0.2855677008628845
Batch 41/64 loss: -0.29636073112487793
Batch 42/64 loss: -0.25880739092826843
Batch 43/64 loss: -0.2733813524246216
Batch 44/64 loss: -0.31361478567123413
Batch 45/64 loss: -0.3023717999458313
Batch 46/64 loss: -0.29290348291397095
Batch 47/64 loss: -0.26602327823638916
Batch 48/64 loss: -0.28978222608566284
Batch 49/64 loss: -0.2283341884613037
Batch 50/64 loss: -0.2620110511779785
Batch 51/64 loss: -0.21277880668640137
Batch 52/64 loss: -0.2836421728134155
Batch 53/64 loss: -0.2969168424606323
Batch 54/64 loss: -0.26133638620376587
Batch 55/64 loss: -0.275124728679657
Batch 56/64 loss: -0.2652398347854614
Batch 57/64 loss: -0.29273998737335205
Batch 58/64 loss: -0.2757025361061096
Batch 59/64 loss: -0.32379260659217834
Batch 60/64 loss: -0.26482707262039185
Batch 61/64 loss: -0.2821575999259949
Batch 62/64 loss: -0.248060941696167
Batch 63/64 loss: -0.30485475063323975
Batch 64/64 loss: -0.24984997510910034
Epoch 246  Train loss: -0.2827392781482023  Val loss: 0.02381016380598455
Epoch 247
-------------------------------
Batch 1/64 loss: -0.31821972131729126
Batch 2/64 loss: -0.31178873777389526
Batch 3/64 loss: -0.26466119289398193
Batch 4/64 loss: -0.31785741448402405
Batch 5/64 loss: -0.3245968520641327
Batch 6/64 loss: -0.3175084888935089
Batch 7/64 loss: -0.2888103127479553
Batch 8/64 loss: -0.2876327335834503
Batch 9/64 loss: -0.2969581186771393
Batch 10/64 loss: -0.306054949760437
Batch 11/64 loss: -0.2577757239341736
Batch 12/64 loss: -0.2659915089607239
Batch 13/64 loss: -0.26949477195739746
Batch 14/64 loss: -0.28197598457336426
Batch 15/64 loss: -0.29435956478118896
Batch 16/64 loss: -0.28313255310058594
Batch 17/64 loss: -0.29967066645622253
Batch 18/64 loss: -0.2839437425136566
Batch 19/64 loss: -0.3021067976951599
Batch 20/64 loss: -0.26923295855522156
Batch 21/64 loss: -0.2804439067840576
Batch 22/64 loss: -0.3039925694465637
Batch 23/64 loss: -0.29939255118370056
Batch 24/64 loss: -0.23092463612556458
Batch 25/64 loss: -0.31450343132019043
Batch 26/64 loss: -0.27924326062202454
Batch 27/64 loss: -0.31992650032043457
Batch 28/64 loss: -0.3061702847480774
Batch 29/64 loss: -0.286538302898407
Batch 30/64 loss: -0.3083433508872986
Batch 31/64 loss: -0.2914785146713257
Batch 32/64 loss: -0.3050740957260132
Batch 33/64 loss: -0.2466360330581665
Batch 34/64 loss: -0.3017171323299408
Batch 35/64 loss: -0.3312346935272217
Batch 36/64 loss: -0.25111573934555054
Batch 37/64 loss: -0.24595963954925537
Batch 38/64 loss: -0.31111788749694824
Batch 39/64 loss: -0.27523520588874817
Batch 40/64 loss: -0.2573426365852356
Batch 41/64 loss: -0.258415162563324
Batch 42/64 loss: -0.30673089623451233
Batch 43/64 loss: -0.28011101484298706
Batch 44/64 loss: -0.29980260133743286
Batch 45/64 loss: -0.25708720088005066
Batch 46/64 loss: -0.19610637426376343
Batch 47/64 loss: -0.2762363851070404
Batch 48/64 loss: -0.2821003496646881
Batch 49/64 loss: -0.2983575463294983
Batch 50/64 loss: -0.25203800201416016
Batch 51/64 loss: -0.30133235454559326
Batch 52/64 loss: -0.20072609186172485
Batch 53/64 loss: -0.26702237129211426
Batch 54/64 loss: -0.2956835627555847
Batch 55/64 loss: -0.287464439868927
Batch 56/64 loss: -0.29413163661956787
Batch 57/64 loss: -0.3063960671424866
Batch 58/64 loss: -0.23511415719985962
Batch 59/64 loss: -0.27703195810317993
Batch 60/64 loss: -0.280617356300354
Batch 61/64 loss: -0.29891663789749146
Batch 62/64 loss: -0.26244622468948364
Batch 63/64 loss: -0.2853158116340637
Batch 64/64 loss: -0.283006489276886
Epoch 247  Train loss: -0.28391532921323587  Val loss: 0.0260148793971006
Epoch 248
-------------------------------
Batch 1/64 loss: -0.3017651438713074
Batch 2/64 loss: -0.29562491178512573
Batch 3/64 loss: -0.2908817231655121
Batch 4/64 loss: -0.2876720428466797
Batch 5/64 loss: -0.3151177167892456
Batch 6/64 loss: -0.30780741572380066
Batch 7/64 loss: -0.29097992181777954
Batch 8/64 loss: -0.264384388923645
Batch 9/64 loss: -0.29554468393325806
Batch 10/64 loss: -0.2939540445804596
Batch 11/64 loss: -0.2820594906806946
Batch 12/64 loss: -0.2918801009654999
Batch 13/64 loss: -0.2896616458892822
Batch 14/64 loss: -0.3158751130104065
Batch 15/64 loss: -0.2656514644622803
Batch 16/64 loss: -0.29849275946617126
Batch 17/64 loss: -0.2838468551635742
Batch 18/64 loss: -0.271189421415329
Batch 19/64 loss: -0.2925860583782196
Batch 20/64 loss: -0.28114211559295654
Batch 21/64 loss: -0.2965007722377777
Batch 22/64 loss: -0.30431801080703735
Batch 23/64 loss: -0.3192203640937805
Batch 24/64 loss: -0.2813777029514313
Batch 25/64 loss: -0.27651429176330566
Batch 26/64 loss: -0.25153210759162903
Batch 27/64 loss: -0.2999782860279083
Batch 28/64 loss: -0.26696258783340454
Batch 29/64 loss: -0.29552286863327026
Batch 30/64 loss: -0.2780229151248932
Batch 31/64 loss: -0.3029458522796631
Batch 32/64 loss: -0.2710365653038025
Batch 33/64 loss: -0.2778393626213074
Batch 34/64 loss: -0.3025965988636017
Batch 35/64 loss: -0.2913477420806885
Batch 36/64 loss: -0.25241199135780334
Batch 37/64 loss: -0.23690128326416016
Batch 38/64 loss: -0.29824018478393555
Batch 39/64 loss: -0.2701330780982971
Batch 40/64 loss: -0.2826572060585022
Batch 41/64 loss: -0.3004275858402252
Batch 42/64 loss: -0.2773818075656891
Batch 43/64 loss: -0.3041872978210449
Batch 44/64 loss: -0.2905113101005554
Batch 45/64 loss: -0.31866195797920227
Batch 46/64 loss: -0.292138934135437
Batch 47/64 loss: -0.28928983211517334
Batch 48/64 loss: -0.278789222240448
Batch 49/64 loss: -0.30150482058525085
Batch 50/64 loss: -0.31432223320007324
Batch 51/64 loss: -0.28051525354385376
Batch 52/64 loss: -0.2934587895870209
Batch 53/64 loss: -0.29403823614120483
Batch 54/64 loss: -0.2646111249923706
Batch 55/64 loss: -0.2965875566005707
Batch 56/64 loss: -0.2788628935813904
Batch 57/64 loss: -0.2809651494026184
Batch 58/64 loss: -0.2534589469432831
Batch 59/64 loss: -0.2516914904117584
Batch 60/64 loss: -0.27551892399787903
Batch 61/64 loss: -0.2949460446834564
Batch 62/64 loss: -0.27184051275253296
Batch 63/64 loss: -0.2864321172237396
Batch 64/64 loss: -0.27090054750442505
Epoch 248  Train loss: -0.28651755674212587  Val loss: 0.02718942456229036
Epoch 249
-------------------------------
Batch 1/64 loss: -0.28050172328948975
Batch 2/64 loss: -0.30005204677581787
Batch 3/64 loss: -0.25974512100219727
Batch 4/64 loss: -0.2924635112285614
Batch 5/64 loss: -0.2709442973136902
Batch 6/64 loss: -0.3081655502319336
Batch 7/64 loss: -0.29294896125793457
Batch 8/64 loss: -0.27609479427337646
Batch 9/64 loss: -0.3056242763996124
Batch 10/64 loss: -0.26777586340904236
Batch 11/64 loss: -0.2746260166168213
Batch 12/64 loss: -0.25719451904296875
Batch 13/64 loss: -0.24350309371948242
Batch 14/64 loss: -0.3017689883708954
Batch 15/64 loss: -0.3026387095451355
Batch 16/64 loss: -0.25653591752052307
Batch 17/64 loss: -0.31215566396713257
Batch 18/64 loss: -0.2588202953338623
Batch 19/64 loss: -0.2956388294696808
Batch 20/64 loss: -0.28882062435150146
Batch 21/64 loss: -0.3043156564235687
Batch 22/64 loss: -0.2892194986343384
Batch 23/64 loss: -0.2683475911617279
Batch 24/64 loss: -0.30342721939086914
Batch 25/64 loss: -0.2986031174659729
Batch 26/64 loss: -0.22824043035507202
Batch 27/64 loss: -0.2771872878074646
Batch 28/64 loss: -0.27172884345054626
Batch 29/64 loss: -0.32542431354522705
Batch 30/64 loss: -0.24813491106033325
Batch 31/64 loss: -0.2899065613746643
Batch 32/64 loss: -0.2344178855419159
Batch 33/64 loss: -0.3013039231300354
Batch 34/64 loss: -0.3036344647407532
Batch 35/64 loss: -0.26524457335472107
Batch 36/64 loss: -0.27479204535484314
Batch 37/64 loss: -0.3020983934402466
Batch 38/64 loss: -0.29790425300598145
Batch 39/64 loss: -0.2744518220424652
Batch 40/64 loss: -0.2887999415397644
Batch 41/64 loss: -0.28067851066589355
Batch 42/64 loss: -0.25041136145591736
Batch 43/64 loss: -0.2510106563568115
Batch 44/64 loss: -0.29860711097717285
Batch 45/64 loss: -0.2656306326389313
Batch 46/64 loss: -0.256232887506485
Batch 47/64 loss: -0.27705898880958557
Batch 48/64 loss: -0.2951103448867798
Batch 49/64 loss: -0.2591854929924011
Batch 50/64 loss: -0.3190145492553711
Batch 51/64 loss: -0.2665746212005615
Batch 52/64 loss: -0.2903978228569031
Batch 53/64 loss: -0.299586683511734
Batch 54/64 loss: -0.2931753098964691
Batch 55/64 loss: -0.2834393382072449
Batch 56/64 loss: -0.2654845118522644
Batch 57/64 loss: -0.2911917567253113
Batch 58/64 loss: -0.2849884629249573
Batch 59/64 loss: -0.2796364724636078
Batch 60/64 loss: -0.2844153046607971
Batch 61/64 loss: -0.2684013247489929
Batch 62/64 loss: -0.2989165782928467
Batch 63/64 loss: -0.29841893911361694
Batch 64/64 loss: -0.2500302195549011
Epoch 249  Train loss: -0.2813849703938353  Val loss: 0.0266017379219999
Epoch 250
-------------------------------
Batch 1/64 loss: -0.2796871066093445
Batch 2/64 loss: -0.2904888391494751
Batch 3/64 loss: -0.27243995666503906
Batch 4/64 loss: -0.2779914438724518
Batch 5/64 loss: -0.2989344596862793
Batch 6/64 loss: -0.248582661151886
Batch 7/64 loss: -0.3208656311035156
Batch 8/64 loss: -0.27088093757629395
Batch 9/64 loss: -0.2889835834503174
Batch 10/64 loss: -0.3170763850212097
Batch 11/64 loss: -0.25221121311187744
Batch 12/64 loss: -0.29752951860427856
Batch 13/64 loss: -0.2070552110671997
Batch 14/64 loss: -0.3143366873264313
Batch 15/64 loss: -0.3060134947299957
Batch 16/64 loss: -0.2936592102050781
Batch 17/64 loss: -0.3012824058532715
Batch 18/64 loss: -0.2956974506378174
Batch 19/64 loss: -0.3003718852996826
Batch 20/64 loss: -0.2988852858543396
Batch 21/64 loss: -0.29566746950149536
Batch 22/64 loss: -0.23579692840576172
Batch 23/64 loss: -0.3079680800437927
Batch 24/64 loss: -0.30676940083503723
Batch 25/64 loss: -0.2505558729171753
Batch 26/64 loss: -0.3138657808303833
Batch 27/64 loss: -0.2999722361564636
Batch 28/64 loss: -0.3133370876312256
Batch 29/64 loss: -0.2844184935092926
Batch 30/64 loss: -0.2553219795227051
Batch 31/64 loss: -0.26848268508911133
Batch 32/64 loss: -0.2558129131793976
Batch 33/64 loss: -0.29449447989463806
Batch 34/64 loss: -0.2860955595970154
Batch 35/64 loss: -0.30736759305000305
Batch 36/64 loss: -0.2988327145576477
Batch 37/64 loss: -0.3010045289993286
Batch 38/64 loss: -0.2885786294937134
Batch 39/64 loss: -0.2951478958129883
Batch 40/64 loss: -0.2894091010093689
Batch 41/64 loss: -0.2784082889556885
Batch 42/64 loss: -0.27507540583610535
Batch 43/64 loss: -0.285457968711853
Batch 44/64 loss: -0.26212865114212036
Batch 45/64 loss: -0.2870250344276428
Batch 46/64 loss: -0.2900584638118744
Batch 47/64 loss: -0.29972338676452637
Batch 48/64 loss: -0.2827951908111572
Batch 49/64 loss: -0.2559080123901367
Batch 50/64 loss: -0.2582327723503113
Batch 51/64 loss: -0.2944157123565674
Batch 52/64 loss: -0.29571622610092163
Batch 53/64 loss: -0.2856798768043518
Batch 54/64 loss: -0.2592766284942627
Batch 55/64 loss: -0.2815602719783783
Batch 56/64 loss: -0.20762181282043457
Batch 57/64 loss: -0.2536126375198364
Batch 58/64 loss: -0.26893875002861023
Batch 59/64 loss: -0.30677932500839233
Batch 60/64 loss: -0.2908194661140442
Batch 61/64 loss: -0.3071846663951874
Batch 62/64 loss: -0.27914363145828247
Batch 63/64 loss: -0.2549353837966919
Batch 64/64 loss: -0.2957325875759125
Epoch 250  Train loss: -0.28335952630230027  Val loss: 0.02628348045742389
Epoch 251
-------------------------------
Batch 1/64 loss: -0.296488493680954
Batch 2/64 loss: -0.2974129915237427
Batch 3/64 loss: -0.29758623242378235
Batch 4/64 loss: -0.30911147594451904
Batch 5/64 loss: -0.3097112476825714
Batch 6/64 loss: -0.302631676197052
Batch 7/64 loss: -0.322148859500885
Batch 8/64 loss: -0.28522610664367676
Batch 9/64 loss: -0.29801201820373535
Batch 10/64 loss: -0.28032976388931274
Batch 11/64 loss: -0.2663273215293884
Batch 12/64 loss: -0.28788134455680847
Batch 13/64 loss: -0.3107041120529175
Batch 14/64 loss: -0.29420265555381775
Batch 15/64 loss: -0.25702276825904846
Batch 16/64 loss: -0.27440589666366577
Batch 17/64 loss: -0.2868543267250061
Batch 18/64 loss: -0.3093106150627136
Batch 19/64 loss: -0.3111117482185364
Batch 20/64 loss: -0.2994772493839264
Batch 21/64 loss: -0.2667851150035858
Batch 22/64 loss: -0.25021418929100037
Batch 23/64 loss: -0.28577208518981934
Batch 24/64 loss: -0.2734391689300537
Batch 25/64 loss: -0.2652660608291626
Batch 26/64 loss: -0.2602510452270508
Batch 27/64 loss: -0.2793468236923218
Batch 28/64 loss: -0.30131736397743225
Batch 29/64 loss: -0.30013027787208557
Batch 30/64 loss: -0.27837491035461426
Batch 31/64 loss: -0.30329346656799316
Batch 32/64 loss: -0.32751208543777466
Batch 33/64 loss: -0.30675897002220154
Batch 34/64 loss: -0.2530430555343628
Batch 35/64 loss: -0.2958613634109497
Batch 36/64 loss: -0.292183518409729
Batch 37/64 loss: -0.2832198739051819
Batch 38/64 loss: -0.23335537314414978
Batch 39/64 loss: -0.27065303921699524
Batch 40/64 loss: -0.2840440273284912
Batch 41/64 loss: -0.29560327529907227
Batch 42/64 loss: -0.30355286598205566
Batch 43/64 loss: -0.26344501972198486
Batch 44/64 loss: -0.2765280604362488
Batch 45/64 loss: -0.2810792326927185
Batch 46/64 loss: -0.30958834290504456
Batch 47/64 loss: -0.30206483602523804
Batch 48/64 loss: -0.2689433693885803
Batch 49/64 loss: -0.2858773469924927
Batch 50/64 loss: -0.2632724642753601
Batch 51/64 loss: -0.2787342071533203
Batch 52/64 loss: -0.3095271587371826
Batch 53/64 loss: -0.281159907579422
Batch 54/64 loss: -0.302263081073761
Batch 55/64 loss: -0.2702057659626007
Batch 56/64 loss: -0.28495344519615173
Batch 57/64 loss: -0.28105679154396057
Batch 58/64 loss: -0.2977055311203003
Batch 59/64 loss: -0.27333250641822815
Batch 60/64 loss: -0.27853062748908997
Batch 61/64 loss: -0.2692047357559204
Batch 62/64 loss: -0.3325536251068115
Batch 63/64 loss: -0.2598840594291687
Batch 64/64 loss: -0.30874204635620117
Epoch 251  Train loss: -0.2871749256171432  Val loss: 0.023541250999031198
Epoch 252
-------------------------------
Batch 1/64 loss: -0.31347742676734924
Batch 2/64 loss: -0.3138998746871948
Batch 3/64 loss: -0.2856495678424835
Batch 4/64 loss: -0.31359943747520447
Batch 5/64 loss: -0.2870810925960541
Batch 6/64 loss: -0.3054925799369812
Batch 7/64 loss: -0.2162717580795288
Batch 8/64 loss: -0.26802265644073486
Batch 9/64 loss: -0.31719720363616943
Batch 10/64 loss: -0.3055322766304016
Batch 11/64 loss: -0.26468756794929504
Batch 12/64 loss: -0.30264437198638916
Batch 13/64 loss: -0.28585946559906006
Batch 14/64 loss: -0.32741579413414
Batch 15/64 loss: -0.2947275936603546
Batch 16/64 loss: -0.2698093354701996
Batch 17/64 loss: -0.2958492934703827
Batch 18/64 loss: -0.28952929377555847
Batch 19/64 loss: -0.25728142261505127
Batch 20/64 loss: -0.30693042278289795
Batch 21/64 loss: -0.28492578864097595
Batch 22/64 loss: -0.26043224334716797
Batch 23/64 loss: -0.24891778826713562
Batch 24/64 loss: -0.29842785000801086
Batch 25/64 loss: -0.2887512743473053
Batch 26/64 loss: -0.3029438853263855
Batch 27/64 loss: -0.28575271368026733
Batch 28/64 loss: -0.30971136689186096
Batch 29/64 loss: -0.3092266917228699
Batch 30/64 loss: -0.19926011562347412
Batch 31/64 loss: -0.3177211284637451
Batch 32/64 loss: -0.235956072807312
Batch 33/64 loss: -0.2723374366760254
Batch 34/64 loss: -0.2805883288383484
Batch 35/64 loss: -0.28801870346069336
Batch 36/64 loss: -0.33292847871780396
Batch 37/64 loss: -0.27007532119750977
Batch 38/64 loss: -0.30280962586402893
Batch 39/64 loss: -0.27541303634643555
Batch 40/64 loss: -0.286864697933197
Batch 41/64 loss: -0.26578763127326965
Batch 42/64 loss: -0.2873310148715973
Batch 43/64 loss: -0.24965041875839233
Batch 44/64 loss: -0.28116440773010254
Batch 45/64 loss: -0.28070589900016785
Batch 46/64 loss: -0.28392165899276733
Batch 47/64 loss: -0.29294660687446594
Batch 48/64 loss: -0.3281480371952057
Batch 49/64 loss: -0.2873857021331787
Batch 50/64 loss: -0.3306346535682678
Batch 51/64 loss: -0.3050919771194458
Batch 52/64 loss: -0.28482937812805176
Batch 53/64 loss: -0.28320014476776123
Batch 54/64 loss: -0.32338041067123413
Batch 55/64 loss: -0.2767728567123413
Batch 56/64 loss: -0.2753452658653259
Batch 57/64 loss: -0.2940216660499573
Batch 58/64 loss: -0.27704864740371704
Batch 59/64 loss: -0.2666570544242859
Batch 60/64 loss: -0.32045474648475647
Batch 61/64 loss: -0.29784417152404785
Batch 62/64 loss: -0.27800774574279785
Batch 63/64 loss: -0.2763894200325012
Batch 64/64 loss: -0.30365389585494995
Epoch 252  Train loss: -0.2877879360142876  Val loss: 0.02458566324817356
Epoch 253
-------------------------------
Batch 1/64 loss: -0.2904493808746338
Batch 2/64 loss: -0.308351993560791
Batch 3/64 loss: -0.29885435104370117
Batch 4/64 loss: -0.3138728141784668
Batch 5/64 loss: -0.3189586400985718
Batch 6/64 loss: -0.31254905462265015
Batch 7/64 loss: -0.3359004855155945
Batch 8/64 loss: -0.3054457902908325
Batch 9/64 loss: -0.30257174372673035
Batch 10/64 loss: -0.31359779834747314
Batch 11/64 loss: -0.3034052848815918
Batch 12/64 loss: -0.28874748945236206
Batch 13/64 loss: -0.3024160861968994
Batch 14/64 loss: -0.29889512062072754
Batch 15/64 loss: -0.30042093992233276
Batch 16/64 loss: -0.2993195652961731
Batch 17/64 loss: -0.3286239504814148
Batch 18/64 loss: -0.2858890891075134
Batch 19/64 loss: -0.30058133602142334
Batch 20/64 loss: -0.25933918356895447
Batch 21/64 loss: -0.2913239300251007
Batch 22/64 loss: -0.30833035707473755
Batch 23/64 loss: -0.32129114866256714
Batch 24/64 loss: -0.3084573745727539
Batch 25/64 loss: -0.2716452479362488
Batch 26/64 loss: -0.29223954677581787
Batch 27/64 loss: -0.24735930562019348
Batch 28/64 loss: -0.31155115365982056
Batch 29/64 loss: -0.30563467741012573
Batch 30/64 loss: -0.28919121623039246
Batch 31/64 loss: -0.29000747203826904
Batch 32/64 loss: -0.30064499378204346
Batch 33/64 loss: -0.30710524320602417
Batch 34/64 loss: -0.3057452440261841
Batch 35/64 loss: -0.29786545038223267
Batch 36/64 loss: -0.2813536524772644
Batch 37/64 loss: -0.287885844707489
Batch 38/64 loss: -0.2752867341041565
Batch 39/64 loss: -0.30015891790390015
Batch 40/64 loss: -0.2872166633605957
Batch 41/64 loss: -0.2930612564086914
Batch 42/64 loss: -0.2818591594696045
Batch 43/64 loss: -0.2956116497516632
Batch 44/64 loss: -0.3256615400314331
Batch 45/64 loss: -0.24827364087104797
Batch 46/64 loss: -0.28418099880218506
Batch 47/64 loss: -0.26753464341163635
Batch 48/64 loss: -0.2892138361930847
Batch 49/64 loss: -0.2178369164466858
Batch 50/64 loss: -0.29876652359962463
Batch 51/64 loss: -0.2875477373600006
Batch 52/64 loss: -0.2896729111671448
Batch 53/64 loss: -0.25864726305007935
Batch 54/64 loss: -0.2929834723472595
Batch 55/64 loss: -0.2752217650413513
Batch 56/64 loss: -0.29609566926956177
Batch 57/64 loss: -0.3033318519592285
Batch 58/64 loss: -0.25841376185417175
Batch 59/64 loss: -0.27326908707618713
Batch 60/64 loss: -0.2576272487640381
Batch 61/64 loss: -0.29691603779792786
Batch 62/64 loss: -0.2606925070285797
Batch 63/64 loss: -0.25610700249671936
Batch 64/64 loss: -0.25655078887939453
Epoch 253  Train loss: -0.29097135440976013  Val loss: 0.024986671623085783
Epoch 254
-------------------------------
Batch 1/64 loss: -0.285977840423584
Batch 2/64 loss: -0.29436880350112915
Batch 3/64 loss: -0.2752588987350464
Batch 4/64 loss: -0.28230997920036316
Batch 5/64 loss: -0.2832055687904358
Batch 6/64 loss: -0.29939910769462585
Batch 7/64 loss: -0.26195597648620605
Batch 8/64 loss: -0.25753486156463623
Batch 9/64 loss: -0.296090692281723
Batch 10/64 loss: -0.270132452249527
Batch 11/64 loss: -0.2969738245010376
Batch 12/64 loss: -0.2783016562461853
Batch 13/64 loss: -0.3253876566886902
Batch 14/64 loss: -0.3070087730884552
Batch 15/64 loss: -0.3315993547439575
Batch 16/64 loss: -0.27243220806121826
Batch 17/64 loss: -0.25466904044151306
Batch 18/64 loss: -0.3234686255455017
Batch 19/64 loss: -0.29024964570999146
Batch 20/64 loss: -0.297152042388916
Batch 21/64 loss: -0.2806587219238281
Batch 22/64 loss: -0.31056222319602966
Batch 23/64 loss: -0.2624908685684204
Batch 24/64 loss: -0.30394256114959717
Batch 25/64 loss: -0.29991090297698975
Batch 26/64 loss: -0.3261735439300537
Batch 27/64 loss: -0.3144238591194153
Batch 28/64 loss: -0.2876623272895813
Batch 29/64 loss: -0.2854820489883423
Batch 30/64 loss: -0.291782021522522
Batch 31/64 loss: -0.2838851809501648
Batch 32/64 loss: -0.26778194308280945
Batch 33/64 loss: -0.2755342125892639
Batch 34/64 loss: -0.29962652921676636
Batch 35/64 loss: -0.28033629059791565
Batch 36/64 loss: -0.29558777809143066
Batch 37/64 loss: -0.3066123127937317
Batch 38/64 loss: -0.31541672348976135
Batch 39/64 loss: -0.28646212816238403
Batch 40/64 loss: -0.2833068072795868
Batch 41/64 loss: -0.2957606911659241
Batch 42/64 loss: -0.2934429943561554
Batch 43/64 loss: -0.26874321699142456
Batch 44/64 loss: -0.3015074133872986
Batch 45/64 loss: -0.29397687315940857
Batch 46/64 loss: -0.2948152422904968
Batch 47/64 loss: -0.3178679347038269
Batch 48/64 loss: -0.29866236448287964
Batch 49/64 loss: -0.2977369427680969
Batch 50/64 loss: -0.29348060488700867
Batch 51/64 loss: -0.2818033695220947
Batch 52/64 loss: -0.2808847427368164
Batch 53/64 loss: -0.2641574442386627
Batch 54/64 loss: -0.25011515617370605
Batch 55/64 loss: -0.2867105007171631
Batch 56/64 loss: -0.2616790533065796
Batch 57/64 loss: -0.27934542298316956
Batch 58/64 loss: -0.2597236931324005
Batch 59/64 loss: -0.30168965458869934
Batch 60/64 loss: -0.2825336456298828
Batch 61/64 loss: -0.2887752056121826
Batch 62/64 loss: -0.27365750074386597
Batch 63/64 loss: -0.2775253653526306
Batch 64/64 loss: -0.26459231972694397
Epoch 254  Train loss: -0.28837890613312817  Val loss: 0.02676916573055831
Epoch 255
-------------------------------
Batch 1/64 loss: -0.27628064155578613
Batch 2/64 loss: -0.2925470769405365
Batch 3/64 loss: -0.2941291332244873
Batch 4/64 loss: -0.32990697026252747
Batch 5/64 loss: -0.28600937128067017
Batch 6/64 loss: -0.3157871961593628
Batch 7/64 loss: -0.25216421484947205
Batch 8/64 loss: -0.31772947311401367
Batch 9/64 loss: -0.2996545433998108
Batch 10/64 loss: -0.3431780934333801
Batch 11/64 loss: -0.3236035704612732
Batch 12/64 loss: -0.2959529161453247
Batch 13/64 loss: -0.2839661240577698
Batch 14/64 loss: -0.30933886766433716
Batch 15/64 loss: -0.2970893383026123
Batch 16/64 loss: -0.3166593909263611
Batch 17/64 loss: -0.2959405183792114
Batch 18/64 loss: -0.29476040601730347
Batch 19/64 loss: -0.2897128760814667
Batch 20/64 loss: -0.28560972213745117
Batch 21/64 loss: -0.2776721715927124
Batch 22/64 loss: -0.24946153163909912
Batch 23/64 loss: -0.2675723433494568
Batch 24/64 loss: -0.28651225566864014
Batch 25/64 loss: -0.23058953881263733
Batch 26/64 loss: -0.27766528725624084
Batch 27/64 loss: -0.27500593662261963
Batch 28/64 loss: -0.27532750368118286
Batch 29/64 loss: -0.23267149925231934
Batch 30/64 loss: -0.30127960443496704
Batch 31/64 loss: -0.29256734251976013
Batch 32/64 loss: -0.28990641236305237
Batch 33/64 loss: -0.29780226945877075
Batch 34/64 loss: -0.29044708609580994
Batch 35/64 loss: -0.2993587255477905
Batch 36/64 loss: -0.3008713126182556
Batch 37/64 loss: -0.2711024284362793
Batch 38/64 loss: -0.30831873416900635
Batch 39/64 loss: -0.30830663442611694
Batch 40/64 loss: -0.27534088492393494
Batch 41/64 loss: -0.2820228338241577
Batch 42/64 loss: -0.2615011930465698
Batch 43/64 loss: -0.3108890950679779
Batch 44/64 loss: -0.26700735092163086
Batch 45/64 loss: -0.30709773302078247
Batch 46/64 loss: -0.2855665981769562
Batch 47/64 loss: -0.2798957824707031
Batch 48/64 loss: -0.23405668139457703
Batch 49/64 loss: -0.2720220386981964
Batch 50/64 loss: -0.29844796657562256
Batch 51/64 loss: -0.250267893075943
Batch 52/64 loss: -0.29490262269973755
Batch 53/64 loss: -0.31056898832321167
Batch 54/64 loss: -0.3124573230743408
Batch 55/64 loss: -0.2927837669849396
Batch 56/64 loss: -0.27084779739379883
Batch 57/64 loss: -0.2768322229385376
Batch 58/64 loss: -0.27229559421539307
Batch 59/64 loss: -0.26654553413391113
Batch 60/64 loss: -0.2867569029331207
Batch 61/64 loss: -0.2960996627807617
Batch 62/64 loss: -0.3126385807991028
Batch 63/64 loss: -0.28983938694000244
Batch 64/64 loss: -0.3058270812034607
Epoch 255  Train loss: -0.28813354992399026  Val loss: 0.02225165883290399
Epoch 256
-------------------------------
Batch 1/64 loss: -0.27152150869369507
Batch 2/64 loss: -0.30502963066101074
Batch 3/64 loss: -0.2475672960281372
Batch 4/64 loss: -0.336736798286438
Batch 5/64 loss: -0.272704541683197
Batch 6/64 loss: -0.30426961183547974
Batch 7/64 loss: -0.2838744819164276
Batch 8/64 loss: -0.2975028157234192
Batch 9/64 loss: -0.28998279571533203
Batch 10/64 loss: -0.31447258591651917
Batch 11/64 loss: -0.29964831471443176
Batch 12/64 loss: -0.29689866304397583
Batch 13/64 loss: -0.2867045998573303
Batch 14/64 loss: -0.2812500596046448
Batch 15/64 loss: -0.2993979752063751
Batch 16/64 loss: -0.27738240361213684
Batch 17/64 loss: -0.2913264036178589
Batch 18/64 loss: -0.31901538372039795
Batch 19/64 loss: -0.3241763114929199
Batch 20/64 loss: -0.2899763584136963
Batch 21/64 loss: -0.30489999055862427
Batch 22/64 loss: -0.28161701560020447
Batch 23/64 loss: -0.30341827869415283
Batch 24/64 loss: -0.2984705865383148
Batch 25/64 loss: -0.27610599994659424
Batch 26/64 loss: -0.26800537109375
Batch 27/64 loss: -0.2705741822719574
Batch 28/64 loss: -0.26010680198669434
Batch 29/64 loss: -0.3047358989715576
Batch 30/64 loss: -0.2908293604850769
Batch 31/64 loss: -0.28370150923728943
Batch 32/64 loss: -0.309519499540329
Batch 33/64 loss: -0.2989388704299927
Batch 34/64 loss: -0.29657602310180664
Batch 35/64 loss: -0.2864179313182831
Batch 36/64 loss: -0.28716713190078735
Batch 37/64 loss: -0.2908264398574829
Batch 38/64 loss: -0.2742505371570587
Batch 39/64 loss: -0.2937363386154175
Batch 40/64 loss: -0.26141291856765747
Batch 41/64 loss: -0.29148221015930176
Batch 42/64 loss: -0.26286864280700684
Batch 43/64 loss: -0.28556323051452637
Batch 44/64 loss: -0.30454105138778687
Batch 45/64 loss: -0.28161871433258057
Batch 46/64 loss: -0.2594383656978607
Batch 47/64 loss: -0.25479960441589355
Batch 48/64 loss: -0.2919294834136963
Batch 49/64 loss: -0.28757959604263306
Batch 50/64 loss: -0.2687120735645294
Batch 51/64 loss: -0.28095078468322754
Batch 52/64 loss: -0.31566259264945984
Batch 53/64 loss: -0.2949067950248718
Batch 54/64 loss: -0.29713332653045654
Batch 55/64 loss: -0.2780175507068634
Batch 56/64 loss: -0.2888597548007965
Batch 57/64 loss: -0.293133407831192
Batch 58/64 loss: -0.32785260677337646
Batch 59/64 loss: -0.2964077591896057
Batch 60/64 loss: -0.3098212480545044
Batch 61/64 loss: -0.3047092854976654
Batch 62/64 loss: -0.24904459714889526
Batch 63/64 loss: -0.2695803940296173
Batch 64/64 loss: -0.31327933073043823
Epoch 256  Train loss: -0.28957367527718636  Val loss: 0.024913901111104645
Epoch 257
-------------------------------
Batch 1/64 loss: -0.2903949022293091
Batch 2/64 loss: -0.2986127734184265
Batch 3/64 loss: -0.3119555115699768
Batch 4/64 loss: -0.2726438641548157
Batch 5/64 loss: -0.3120880126953125
Batch 6/64 loss: -0.2701331377029419
Batch 7/64 loss: -0.28212156891822815
Batch 8/64 loss: -0.27721473574638367
Batch 9/64 loss: -0.3124426305294037
Batch 10/64 loss: -0.24776607751846313
Batch 11/64 loss: -0.32288631796836853
Batch 12/64 loss: -0.29410988092422485
Batch 13/64 loss: -0.20407575368881226
Batch 14/64 loss: -0.31079721450805664
Batch 15/64 loss: -0.28234097361564636
Batch 16/64 loss: -0.2958935797214508
Batch 17/64 loss: -0.29904356598854065
Batch 18/64 loss: -0.2794778347015381
Batch 19/64 loss: -0.2661377787590027
Batch 20/64 loss: -0.2882786989212036
Batch 21/64 loss: -0.28796008229255676
Batch 22/64 loss: -0.23730283975601196
Batch 23/64 loss: -0.25082141160964966
Batch 24/64 loss: -0.31035545468330383
Batch 25/64 loss: -0.28028586506843567
Batch 26/64 loss: -0.2902560234069824
Batch 27/64 loss: -0.27764004468917847
Batch 28/64 loss: -0.2777444124221802
Batch 29/64 loss: -0.32286691665649414
Batch 30/64 loss: -0.3126545250415802
Batch 31/64 loss: -0.2880229353904724
Batch 32/64 loss: -0.29688766598701477
Batch 33/64 loss: -0.2979382276535034
Batch 34/64 loss: -0.29903697967529297
Batch 35/64 loss: -0.24494975805282593
Batch 36/64 loss: -0.27519524097442627
Batch 37/64 loss: -0.3071227967739105
Batch 38/64 loss: -0.26463598012924194
Batch 39/64 loss: -0.2806445360183716
Batch 40/64 loss: -0.29142165184020996
Batch 41/64 loss: -0.3104979395866394
Batch 42/64 loss: -0.3321486711502075
Batch 43/64 loss: -0.31162989139556885
Batch 44/64 loss: -0.27696043252944946
Batch 45/64 loss: -0.26988470554351807
Batch 46/64 loss: -0.2778555452823639
Batch 47/64 loss: -0.2893132269382477
Batch 48/64 loss: -0.32297030091285706
Batch 49/64 loss: -0.2988067865371704
Batch 50/64 loss: -0.31003203988075256
Batch 51/64 loss: -0.21234917640686035
Batch 52/64 loss: -0.2969662547111511
Batch 53/64 loss: -0.3028362989425659
Batch 54/64 loss: -0.31209006905555725
Batch 55/64 loss: -0.3097935914993286
Batch 56/64 loss: -0.3134637475013733
Batch 57/64 loss: -0.2865687906742096
Batch 58/64 loss: -0.30045318603515625
Batch 59/64 loss: -0.2595871388912201
Batch 60/64 loss: -0.2597845494747162
Batch 61/64 loss: -0.3046928644180298
Batch 62/64 loss: -0.2889578342437744
Batch 63/64 loss: -0.2677648365497589
Batch 64/64 loss: -0.30851149559020996
Epoch 257  Train loss: -0.2879835240981158  Val loss: 0.026796812454040107
Epoch 258
-------------------------------
Batch 1/64 loss: -0.30635523796081543
Batch 2/64 loss: -0.3206041753292084
Batch 3/64 loss: -0.31505629420280457
Batch 4/64 loss: -0.2575322985649109
Batch 5/64 loss: -0.29666078090667725
Batch 6/64 loss: -0.28862833976745605
Batch 7/64 loss: -0.2790904641151428
Batch 8/64 loss: -0.2640863060951233
Batch 9/64 loss: -0.27042824029922485
Batch 10/64 loss: -0.3048020303249359
Batch 11/64 loss: -0.246067076921463
Batch 12/64 loss: -0.2889244258403778
Batch 13/64 loss: -0.2793636918067932
Batch 14/64 loss: -0.23326197266578674
Batch 15/64 loss: -0.30107641220092773
Batch 16/64 loss: -0.2563493251800537
Batch 17/64 loss: -0.29892104864120483
Batch 18/64 loss: -0.31700342893600464
Batch 19/64 loss: -0.29072991013526917
Batch 20/64 loss: -0.30247488617897034
Batch 21/64 loss: -0.2793799042701721
Batch 22/64 loss: -0.29160836338996887
Batch 23/64 loss: -0.31655997037887573
Batch 24/64 loss: -0.2795434594154358
Batch 25/64 loss: -0.26407796144485474
Batch 26/64 loss: -0.30217409133911133
Batch 27/64 loss: -0.32544857263565063
Batch 28/64 loss: -0.28807294368743896
Batch 29/64 loss: -0.3057814836502075
Batch 30/64 loss: -0.2822977304458618
Batch 31/64 loss: -0.3049103021621704
Batch 32/64 loss: -0.291387677192688
Batch 33/64 loss: -0.23583191633224487
Batch 34/64 loss: -0.295574426651001
Batch 35/64 loss: -0.2834809124469757
Batch 36/64 loss: -0.299613356590271
Batch 37/64 loss: -0.27176982164382935
Batch 38/64 loss: -0.280628502368927
Batch 39/64 loss: -0.29677754640579224
Batch 40/64 loss: -0.308552622795105
Batch 41/64 loss: -0.2722909450531006
Batch 42/64 loss: -0.2592233419418335
Batch 43/64 loss: -0.28697460889816284
Batch 44/64 loss: -0.292410671710968
Batch 45/64 loss: -0.3058469593524933
Batch 46/64 loss: -0.29282182455062866
Batch 47/64 loss: -0.2669444978237152
Batch 48/64 loss: -0.2802804112434387
Batch 49/64 loss: -0.31736496090888977
Batch 50/64 loss: -0.29816973209381104
Batch 51/64 loss: -0.33513620495796204
Batch 52/64 loss: -0.30274462699890137
Batch 53/64 loss: -0.2638954818248749
Batch 54/64 loss: -0.3036228120326996
Batch 55/64 loss: -0.30213338136672974
Batch 56/64 loss: -0.2830551862716675
Batch 57/64 loss: -0.3126775026321411
Batch 58/64 loss: -0.289786696434021
Batch 59/64 loss: -0.2815757393836975
Batch 60/64 loss: -0.30872172117233276
Batch 61/64 loss: -0.2866666316986084
Batch 62/64 loss: -0.303286075592041
Batch 63/64 loss: -0.30654576420783997
Batch 64/64 loss: -0.29552924633026123
Epoch 258  Train loss: -0.2901131078308704  Val loss: 0.028248913099675654
Epoch 259
-------------------------------
Batch 1/64 loss: -0.32038241624832153
Batch 2/64 loss: -0.322507381439209
Batch 3/64 loss: -0.27352261543273926
Batch 4/64 loss: -0.3184836208820343
Batch 5/64 loss: -0.2884938716888428
Batch 6/64 loss: -0.29587751626968384
Batch 7/64 loss: -0.275877982378006
Batch 8/64 loss: -0.26545706391334534
Batch 9/64 loss: -0.3058599829673767
Batch 10/64 loss: -0.26818960905075073
Batch 11/64 loss: -0.3211454749107361
Batch 12/64 loss: -0.2976411283016205
Batch 13/64 loss: -0.2982975244522095
Batch 14/64 loss: -0.3035570979118347
Batch 15/64 loss: -0.3031277656555176
Batch 16/64 loss: -0.3027019798755646
Batch 17/64 loss: -0.28898316621780396
Batch 18/64 loss: -0.31565946340560913
Batch 19/64 loss: -0.32084619998931885
Batch 20/64 loss: -0.2857750952243805
Batch 21/64 loss: -0.3226008713245392
Batch 22/64 loss: -0.30626577138900757
Batch 23/64 loss: -0.31847041845321655
Batch 24/64 loss: -0.2867323160171509
Batch 25/64 loss: -0.3015871047973633
Batch 26/64 loss: -0.2885372042655945
Batch 27/64 loss: -0.2613162696361542
Batch 28/64 loss: -0.32904455065727234
Batch 29/64 loss: -0.2748727798461914
Batch 30/64 loss: -0.25714096426963806
Batch 31/64 loss: -0.2799186706542969
Batch 32/64 loss: -0.30806297063827515
Batch 33/64 loss: -0.27739089727401733
Batch 34/64 loss: -0.26369553804397583
Batch 35/64 loss: -0.2519991397857666
Batch 36/64 loss: -0.2847962975502014
Batch 37/64 loss: -0.2874799966812134
Batch 38/64 loss: -0.3084266483783722
Batch 39/64 loss: -0.3012644052505493
Batch 40/64 loss: -0.2845122218132019
Batch 41/64 loss: -0.27966219186782837
Batch 42/64 loss: -0.29711419343948364
Batch 43/64 loss: -0.3240598440170288
Batch 44/64 loss: -0.290140837430954
Batch 45/64 loss: -0.264773964881897
Batch 46/64 loss: -0.314309298992157
Batch 47/64 loss: -0.27829113602638245
Batch 48/64 loss: -0.24985146522521973
Batch 49/64 loss: -0.2952118217945099
Batch 50/64 loss: -0.2893860936164856
Batch 51/64 loss: -0.2780396342277527
Batch 52/64 loss: -0.3086380660533905
Batch 53/64 loss: -0.3176613450050354
Batch 54/64 loss: -0.2878963053226471
Batch 55/64 loss: -0.27347415685653687
Batch 56/64 loss: -0.29415425658226013
Batch 57/64 loss: -0.3049842119216919
Batch 58/64 loss: -0.3046244978904724
Batch 59/64 loss: -0.3215346932411194
Batch 60/64 loss: -0.3185644745826721
Batch 61/64 loss: -0.2464945912361145
Batch 62/64 loss: -0.30908578634262085
Batch 63/64 loss: -0.2697167992591858
Batch 64/64 loss: -0.24397897720336914
Epoch 259  Train loss: -0.29281813163383336  Val loss: 0.028364793131851248
Epoch 260
-------------------------------
Batch 1/64 loss: -0.23021629452705383
Batch 2/64 loss: -0.25734657049179077
Batch 3/64 loss: -0.3079608976840973
Batch 4/64 loss: -0.268815815448761
Batch 5/64 loss: -0.26355093717575073
Batch 6/64 loss: -0.31024813652038574
Batch 7/64 loss: -0.32767924666404724
Batch 8/64 loss: -0.324023574590683
Batch 9/64 loss: -0.3024404048919678
Batch 10/64 loss: -0.3129276931285858
Batch 11/64 loss: -0.2719741463661194
Batch 12/64 loss: -0.27736544609069824
Batch 13/64 loss: -0.3244644105434418
Batch 14/64 loss: -0.30974388122558594
Batch 15/64 loss: -0.3201502561569214
Batch 16/64 loss: -0.315663605928421
Batch 17/64 loss: -0.2916174530982971
Batch 18/64 loss: -0.31865841150283813
Batch 19/64 loss: -0.30834585428237915
Batch 20/64 loss: -0.28056538105010986
Batch 21/64 loss: -0.296177476644516
Batch 22/64 loss: -0.2140178680419922
Batch 23/64 loss: -0.2750750184059143
Batch 24/64 loss: -0.28493836522102356
Batch 25/64 loss: -0.309051513671875
Batch 26/64 loss: -0.29507291316986084
Batch 27/64 loss: -0.28956279158592224
Batch 28/64 loss: -0.28042253851890564
Batch 29/64 loss: -0.3124585747718811
Batch 30/64 loss: -0.2610064148902893
Batch 31/64 loss: -0.2851014733314514
Batch 32/64 loss: -0.31375259160995483
Batch 33/64 loss: -0.3152811527252197
Batch 34/64 loss: -0.2670045495033264
Batch 35/64 loss: -0.317329466342926
Batch 36/64 loss: -0.2849993407726288
Batch 37/64 loss: -0.273348867893219
Batch 38/64 loss: -0.3044878840446472
Batch 39/64 loss: -0.26685741543769836
Batch 40/64 loss: -0.3011936545372009
Batch 41/64 loss: -0.30410686135292053
Batch 42/64 loss: -0.31193214654922485
Batch 43/64 loss: -0.27507948875427246
Batch 44/64 loss: -0.2887955605983734
Batch 45/64 loss: -0.28747040033340454
Batch 46/64 loss: -0.3147525191307068
Batch 47/64 loss: -0.2971639633178711
Batch 48/64 loss: -0.3013646602630615
Batch 49/64 loss: -0.302476167678833
Batch 50/64 loss: -0.3154590129852295
Batch 51/64 loss: -0.2885145843029022
Batch 52/64 loss: -0.3111279606819153
Batch 53/64 loss: -0.2668722867965698
Batch 54/64 loss: -0.3019481301307678
Batch 55/64 loss: -0.2964475154876709
Batch 56/64 loss: -0.2579099237918854
Batch 57/64 loss: -0.29553261399269104
Batch 58/64 loss: -0.3161371350288391
Batch 59/64 loss: -0.3047523498535156
Batch 60/64 loss: -0.29804229736328125
Batch 61/64 loss: -0.27870190143585205
Batch 62/64 loss: -0.3044382333755493
Batch 63/64 loss: -0.32070329785346985
Batch 64/64 loss: -0.3070013225078583
Epoch 260  Train loss: -0.2939745614341661  Val loss: 0.024489550656059764
Epoch 261
-------------------------------
Batch 1/64 loss: -0.29167523980140686
Batch 2/64 loss: -0.31290340423583984
Batch 3/64 loss: -0.3291313648223877
Batch 4/64 loss: -0.3397057354450226
Batch 5/64 loss: -0.32563480734825134
Batch 6/64 loss: -0.330685019493103
Batch 7/64 loss: -0.22991380095481873
Batch 8/64 loss: -0.3156546354293823
Batch 9/64 loss: -0.29916614294052124
Batch 10/64 loss: -0.30382177233695984
Batch 11/64 loss: -0.2855648994445801
Batch 12/64 loss: -0.27821025252342224
Batch 13/64 loss: -0.28597474098205566
Batch 14/64 loss: -0.2860158085823059
Batch 15/64 loss: -0.2985590100288391
Batch 16/64 loss: -0.2317482829093933
Batch 17/64 loss: -0.31586718559265137
Batch 18/64 loss: -0.2837189733982086
Batch 19/64 loss: -0.3082561194896698
Batch 20/64 loss: -0.29449668526649475
Batch 21/64 loss: -0.3412458002567291
Batch 22/64 loss: -0.3013802766799927
Batch 23/64 loss: -0.2899915874004364
Batch 24/64 loss: -0.2773952782154083
Batch 25/64 loss: -0.31712496280670166
Batch 26/64 loss: -0.28610464930534363
Batch 27/64 loss: -0.2350425124168396
Batch 28/64 loss: -0.2991991639137268
Batch 29/64 loss: -0.24979937076568604
Batch 30/64 loss: -0.3143496811389923
Batch 31/64 loss: -0.2929144501686096
Batch 32/64 loss: -0.30491581559181213
Batch 33/64 loss: -0.2948615550994873
Batch 34/64 loss: -0.28136321902275085
Batch 35/64 loss: -0.30044686794281006
Batch 36/64 loss: -0.3006856441497803
Batch 37/64 loss: -0.3164435625076294
Batch 38/64 loss: -0.3141365349292755
Batch 39/64 loss: -0.3049679398536682
Batch 40/64 loss: -0.24495148658752441
Batch 41/64 loss: -0.28302276134490967
Batch 42/64 loss: -0.2895321249961853
Batch 43/64 loss: -0.29643911123275757
Batch 44/64 loss: -0.2865983247756958
Batch 45/64 loss: -0.27199575304985046
Batch 46/64 loss: -0.29101675748825073
Batch 47/64 loss: -0.32075220346450806
Batch 48/64 loss: -0.2842360734939575
Batch 49/64 loss: -0.3006708025932312
Batch 50/64 loss: -0.268210768699646
Batch 51/64 loss: -0.28766316175460815
Batch 52/64 loss: -0.2556290626525879
Batch 53/64 loss: -0.31964755058288574
Batch 54/64 loss: -0.30572181940078735
Batch 55/64 loss: -0.3076498508453369
Batch 56/64 loss: -0.31499171257019043
Batch 57/64 loss: -0.30837953090667725
Batch 58/64 loss: -0.3009609580039978
Batch 59/64 loss: -0.28918763995170593
Batch 60/64 loss: -0.26616519689559937
Batch 61/64 loss: -0.26841363310813904
Batch 62/64 loss: -0.2581077814102173
Batch 63/64 loss: -0.2855358123779297
Batch 64/64 loss: -0.29010456800460815
Epoch 261  Train loss: -0.2932098989393197  Val loss: 0.024696473813138876
Epoch 262
-------------------------------
Batch 1/64 loss: -0.3325362503528595
Batch 2/64 loss: -0.32142290472984314
Batch 3/64 loss: -0.3020826280117035
Batch 4/64 loss: -0.3447800278663635
Batch 5/64 loss: -0.31903406977653503
Batch 6/64 loss: -0.2999499440193176
Batch 7/64 loss: -0.3386951684951782
Batch 8/64 loss: -0.3309429883956909
Batch 9/64 loss: -0.3055935502052307
Batch 10/64 loss: -0.28356432914733887
Batch 11/64 loss: -0.2980903089046478
Batch 12/64 loss: -0.32260027527809143
Batch 13/64 loss: -0.3255627155303955
Batch 14/64 loss: -0.28193244338035583
Batch 15/64 loss: -0.28093066811561584
Batch 16/64 loss: -0.2565861642360687
Batch 17/64 loss: -0.29671189188957214
Batch 18/64 loss: -0.3006438612937927
Batch 19/64 loss: -0.30200085043907166
Batch 20/64 loss: -0.26724424958229065
Batch 21/64 loss: -0.31288355588912964
Batch 22/64 loss: -0.30697739124298096
Batch 23/64 loss: -0.3160995841026306
Batch 24/64 loss: -0.3068433701992035
Batch 25/64 loss: -0.29068806767463684
Batch 26/64 loss: -0.28161877393722534
Batch 27/64 loss: -0.2816469967365265
Batch 28/64 loss: -0.3072702884674072
Batch 29/64 loss: -0.26957154273986816
Batch 30/64 loss: -0.3103150725364685
Batch 31/64 loss: -0.28864097595214844
Batch 32/64 loss: -0.29347288608551025
Batch 33/64 loss: -0.2375611662864685
Batch 34/64 loss: -0.2630062401294708
Batch 35/64 loss: -0.2729261815547943
Batch 36/64 loss: -0.2947092056274414
Batch 37/64 loss: -0.2984445095062256
Batch 38/64 loss: -0.28209561109542847
Batch 39/64 loss: -0.23685717582702637
Batch 40/64 loss: -0.2839747667312622
Batch 41/64 loss: -0.3116028904914856
Batch 42/64 loss: -0.2874763607978821
Batch 43/64 loss: -0.2928736209869385
Batch 44/64 loss: -0.28917497396469116
Batch 45/64 loss: -0.3064296543598175
Batch 46/64 loss: -0.24508529901504517
Batch 47/64 loss: -0.28745079040527344
Batch 48/64 loss: -0.287469744682312
Batch 49/64 loss: -0.3086514472961426
Batch 50/64 loss: -0.294514924287796
Batch 51/64 loss: -0.2852294147014618
Batch 52/64 loss: -0.2716212272644043
Batch 53/64 loss: -0.27744966745376587
Batch 54/64 loss: -0.2981050908565521
Batch 55/64 loss: -0.31967949867248535
Batch 56/64 loss: -0.2808668315410614
Batch 57/64 loss: -0.30640870332717896
Batch 58/64 loss: -0.3120526075363159
Batch 59/64 loss: -0.31807321310043335
Batch 60/64 loss: -0.28014615178108215
Batch 61/64 loss: -0.32119041681289673
Batch 62/64 loss: -0.29944533109664917
Batch 63/64 loss: -0.28942015767097473
Batch 64/64 loss: -0.30014458298683167
Epoch 262  Train loss: -0.2955613350166994  Val loss: 0.024559912403014927
Epoch 263
-------------------------------
Batch 1/64 loss: -0.32232433557510376
Batch 2/64 loss: -0.30556851625442505
Batch 3/64 loss: -0.27290433645248413
Batch 4/64 loss: -0.28844594955444336
Batch 5/64 loss: -0.31294360756874084
Batch 6/64 loss: -0.28915470838546753
Batch 7/64 loss: -0.25852322578430176
Batch 8/64 loss: -0.30404117703437805
Batch 9/64 loss: -0.3143162727355957
Batch 10/64 loss: -0.23813703656196594
Batch 11/64 loss: -0.3278222680091858
Batch 12/64 loss: -0.31772467494010925
Batch 13/64 loss: -0.2816285192966461
Batch 14/64 loss: -0.2936515510082245
Batch 15/64 loss: -0.27409887313842773
Batch 16/64 loss: -0.31127285957336426
Batch 17/64 loss: -0.27956920862197876
Batch 18/64 loss: -0.30468878149986267
Batch 19/64 loss: -0.31299304962158203
Batch 20/64 loss: -0.2757819890975952
Batch 21/64 loss: -0.2538544535636902
Batch 22/64 loss: -0.31119126081466675
Batch 23/64 loss: -0.32577216625213623
Batch 24/64 loss: -0.2884601354598999
Batch 25/64 loss: -0.2967544496059418
Batch 26/64 loss: -0.30098602175712585
Batch 27/64 loss: -0.260782927274704
Batch 28/64 loss: -0.3220604956150055
Batch 29/64 loss: -0.28777581453323364
Batch 30/64 loss: -0.320163756608963
Batch 31/64 loss: -0.3266758322715759
Batch 32/64 loss: -0.25284263491630554
Batch 33/64 loss: -0.2862958312034607
Batch 34/64 loss: -0.27585819363594055
Batch 35/64 loss: -0.2585551142692566
Batch 36/64 loss: -0.3113771677017212
Batch 37/64 loss: -0.26062244176864624
Batch 38/64 loss: -0.305342435836792
Batch 39/64 loss: -0.2957603931427002
Batch 40/64 loss: -0.2577018737792969
Batch 41/64 loss: -0.2912449836730957
Batch 42/64 loss: -0.2915116548538208
Batch 43/64 loss: -0.3209124207496643
Batch 44/64 loss: -0.31331372261047363
Batch 45/64 loss: -0.30303215980529785
Batch 46/64 loss: -0.2647111415863037
Batch 47/64 loss: -0.2987208366394043
Batch 48/64 loss: -0.3238677382469177
Batch 49/64 loss: -0.30008721351623535
Batch 50/64 loss: -0.291439950466156
Batch 51/64 loss: -0.2781054973602295
Batch 52/64 loss: -0.3033163845539093
Batch 53/64 loss: -0.2263590395450592
Batch 54/64 loss: -0.28820207715034485
Batch 55/64 loss: -0.2831183075904846
Batch 56/64 loss: -0.2978878617286682
Batch 57/64 loss: -0.29889601469039917
Batch 58/64 loss: -0.26942139863967896
Batch 59/64 loss: -0.2896842062473297
Batch 60/64 loss: -0.30563318729400635
Batch 61/64 loss: -0.3205986022949219
Batch 62/64 loss: -0.32797738909721375
Batch 63/64 loss: -0.29281070828437805
Batch 64/64 loss: -0.3149607181549072
Epoch 263  Train loss: -0.2933568215837666  Val loss: 0.024595120518477922
Epoch 264
-------------------------------
Batch 1/64 loss: -0.2848929762840271
Batch 2/64 loss: -0.30928122997283936
Batch 3/64 loss: -0.30492088198661804
Batch 4/64 loss: -0.3357003927230835
Batch 5/64 loss: -0.3067238926887512
Batch 6/64 loss: -0.32633423805236816
Batch 7/64 loss: -0.3022977411746979
Batch 8/64 loss: -0.32202082872390747
Batch 9/64 loss: -0.2911069393157959
Batch 10/64 loss: -0.27441132068634033
Batch 11/64 loss: -0.3047308027744293
Batch 12/64 loss: -0.2755270004272461
Batch 13/64 loss: -0.291513055562973
Batch 14/64 loss: -0.29187095165252686
Batch 15/64 loss: -0.2993637025356293
Batch 16/64 loss: -0.3185010552406311
Batch 17/64 loss: -0.2932407259941101
Batch 18/64 loss: -0.30484408140182495
Batch 19/64 loss: -0.2959873080253601
Batch 20/64 loss: -0.30135661363601685
Batch 21/64 loss: -0.29752564430236816
Batch 22/64 loss: -0.3232404291629791
Batch 23/64 loss: -0.34207433462142944
Batch 24/64 loss: -0.28827399015426636
Batch 25/64 loss: -0.31737738847732544
Batch 26/64 loss: -0.2631465792655945
Batch 27/64 loss: -0.30936866998672485
Batch 28/64 loss: -0.297728955745697
Batch 29/64 loss: -0.27228274941444397
Batch 30/64 loss: -0.2950442433357239
Batch 31/64 loss: -0.28340965509414673
Batch 32/64 loss: -0.2794803977012634
Batch 33/64 loss: -0.29531657695770264
Batch 34/64 loss: -0.30109792947769165
Batch 35/64 loss: -0.322509765625
Batch 36/64 loss: -0.28900599479675293
Batch 37/64 loss: -0.3047868311405182
Batch 38/64 loss: -0.2780091166496277
Batch 39/64 loss: -0.28684574365615845
Batch 40/64 loss: -0.24951454997062683
Batch 41/64 loss: -0.2978147864341736
Batch 42/64 loss: -0.31226256489753723
Batch 43/64 loss: -0.2969885766506195
Batch 44/64 loss: -0.2835521101951599
Batch 45/64 loss: -0.26616308093070984
Batch 46/64 loss: -0.3115524649620056
Batch 47/64 loss: -0.2704513669013977
Batch 48/64 loss: -0.2574556767940521
Batch 49/64 loss: -0.2850395739078522
Batch 50/64 loss: -0.27447739243507385
Batch 51/64 loss: -0.2689925730228424
Batch 52/64 loss: -0.28092262148857117
Batch 53/64 loss: -0.2933947443962097
Batch 54/64 loss: -0.2804691791534424
Batch 55/64 loss: -0.28851696848869324
Batch 56/64 loss: -0.26530176401138306
Batch 57/64 loss: -0.24882256984710693
Batch 58/64 loss: -0.31267663836479187
Batch 59/64 loss: -0.293851375579834
Batch 60/64 loss: -0.312823623418808
Batch 61/64 loss: -0.26986953616142273
Batch 62/64 loss: -0.28488829731941223
Batch 63/64 loss: -0.29507553577423096
Batch 64/64 loss: -0.31224948167800903
Epoch 264  Train loss: -0.29358769281237734  Val loss: 0.025059152100094406
Epoch 265
-------------------------------
Batch 1/64 loss: -0.30769410729408264
Batch 2/64 loss: -0.316750705242157
Batch 3/64 loss: -0.2865566313266754
Batch 4/64 loss: -0.31421613693237305
Batch 5/64 loss: -0.29553478956222534
Batch 6/64 loss: -0.30920952558517456
Batch 7/64 loss: -0.3049745261669159
Batch 8/64 loss: -0.3069050908088684
Batch 9/64 loss: -0.2847703993320465
Batch 10/64 loss: -0.3200531601905823
Batch 11/64 loss: -0.3227713704109192
Batch 12/64 loss: -0.22518247365951538
Batch 13/64 loss: -0.27931851148605347
Batch 14/64 loss: -0.2757962942123413
Batch 15/64 loss: -0.33214282989501953
Batch 16/64 loss: -0.32797324657440186
Batch 17/64 loss: -0.2997341752052307
Batch 18/64 loss: -0.3228731155395508
Batch 19/64 loss: -0.28606516122817993
Batch 20/64 loss: -0.3205704092979431
Batch 21/64 loss: -0.29297778010368347
Batch 22/64 loss: -0.3012603521347046
Batch 23/64 loss: -0.32465115189552307
Batch 24/64 loss: -0.304983913898468
Batch 25/64 loss: -0.3108150064945221
Batch 26/64 loss: -0.29485222697257996
Batch 27/64 loss: -0.2928498089313507
Batch 28/64 loss: -0.29582715034484863
Batch 29/64 loss: -0.30292272567749023
Batch 30/64 loss: -0.2864938974380493
Batch 31/64 loss: -0.2982737720012665
Batch 32/64 loss: -0.213118314743042
Batch 33/64 loss: -0.32108592987060547
Batch 34/64 loss: -0.3180602192878723
Batch 35/64 loss: -0.2962149977684021
Batch 36/64 loss: -0.3077309727668762
Batch 37/64 loss: -0.28432998061180115
Batch 38/64 loss: -0.29537153244018555
Batch 39/64 loss: -0.3174308240413666
Batch 40/64 loss: -0.24918213486671448
Batch 41/64 loss: -0.26017633080482483
Batch 42/64 loss: -0.3019135594367981
Batch 43/64 loss: -0.294156551361084
Batch 44/64 loss: -0.291671484708786
Batch 45/64 loss: -0.29866600036621094
Batch 46/64 loss: -0.29619842767715454
Batch 47/64 loss: -0.2969600558280945
Batch 48/64 loss: -0.2997220754623413
Batch 49/64 loss: -0.2930457592010498
Batch 50/64 loss: -0.29598134756088257
Batch 51/64 loss: -0.32068943977355957
Batch 52/64 loss: -0.2963193655014038
Batch 53/64 loss: -0.29102563858032227
Batch 54/64 loss: -0.3037967085838318
Batch 55/64 loss: -0.26293689012527466
Batch 56/64 loss: -0.3079129457473755
Batch 57/64 loss: -0.2859986424446106
Batch 58/64 loss: -0.28560346364974976
Batch 59/64 loss: -0.25932982563972473
Batch 60/64 loss: -0.2608679533004761
Batch 61/64 loss: -0.28985702991485596
Batch 62/64 loss: -0.3148610591888428
Batch 63/64 loss: -0.29993224143981934
Batch 64/64 loss: -0.27707546949386597
Epoch 265  Train loss: -0.295889486284817  Val loss: 0.02544133011827764
Epoch 266
-------------------------------
Batch 1/64 loss: -0.28389447927474976
Batch 2/64 loss: -0.28740960359573364
Batch 3/64 loss: -0.30991607904434204
Batch 4/64 loss: -0.2738547623157501
Batch 5/64 loss: -0.29080355167388916
Batch 6/64 loss: -0.3085787296295166
Batch 7/64 loss: -0.31689831614494324
Batch 8/64 loss: -0.2594209313392639
Batch 9/64 loss: -0.3069485127925873
Batch 10/64 loss: -0.31154125928878784
Batch 11/64 loss: -0.3076457381248474
Batch 12/64 loss: -0.32468071579933167
Batch 13/64 loss: -0.3122154474258423
Batch 14/64 loss: -0.3057401180267334
Batch 15/64 loss: -0.2972118854522705
Batch 16/64 loss: -0.32699117064476013
Batch 17/64 loss: -0.2656902074813843
Batch 18/64 loss: -0.32111483812332153
Batch 19/64 loss: -0.3316178321838379
Batch 20/64 loss: -0.320163756608963
Batch 21/64 loss: -0.278524786233902
Batch 22/64 loss: -0.3056598901748657
Batch 23/64 loss: -0.3489266037940979
Batch 24/64 loss: -0.3097347617149353
Batch 25/64 loss: -0.26513540744781494
Batch 26/64 loss: -0.3253365755081177
Batch 27/64 loss: -0.3179267346858978
Batch 28/64 loss: -0.31400108337402344
Batch 29/64 loss: -0.26057320833206177
Batch 30/64 loss: -0.28887617588043213
Batch 31/64 loss: -0.2977171838283539
Batch 32/64 loss: -0.28488844633102417
Batch 33/64 loss: -0.3204823136329651
Batch 34/64 loss: -0.2762531042098999
Batch 35/64 loss: -0.29760637879371643
Batch 36/64 loss: -0.3254570960998535
Batch 37/64 loss: -0.3190118074417114
Batch 38/64 loss: -0.32322579622268677
Batch 39/64 loss: -0.18630516529083252
Batch 40/64 loss: -0.2846664786338806
Batch 41/64 loss: -0.31353479623794556
Batch 42/64 loss: -0.23370736837387085
Batch 43/64 loss: -0.24485868215560913
Batch 44/64 loss: -0.2772890627384186
Batch 45/64 loss: -0.29270708560943604
Batch 46/64 loss: -0.2273770570755005
Batch 47/64 loss: -0.3292878568172455
Batch 48/64 loss: -0.2643257975578308
Batch 49/64 loss: -0.2931225895881653
Batch 50/64 loss: -0.30426153540611267
Batch 51/64 loss: -0.29348239302635193
Batch 52/64 loss: -0.32623833417892456
Batch 53/64 loss: -0.25359123945236206
Batch 54/64 loss: -0.2876521050930023
Batch 55/64 loss: -0.28949207067489624
Batch 56/64 loss: -0.30423349142074585
Batch 57/64 loss: -0.3279372453689575
Batch 58/64 loss: -0.29615089297294617
Batch 59/64 loss: -0.3033706545829773
Batch 60/64 loss: -0.3204424977302551
Batch 61/64 loss: -0.2623252272605896
Batch 62/64 loss: -0.27967289090156555
Batch 63/64 loss: -0.2867029309272766
Batch 64/64 loss: -0.2287607192993164
Epoch 266  Train loss: -0.2945251965055279  Val loss: 0.027826761462024806
Epoch 267
-------------------------------
Batch 1/64 loss: -0.3051738440990448
Batch 2/64 loss: -0.28438273072242737
Batch 3/64 loss: -0.31344908475875854
Batch 4/64 loss: -0.3377453684806824
Batch 5/64 loss: -0.28466078639030457
Batch 6/64 loss: -0.259207546710968
Batch 7/64 loss: -0.30841755867004395
Batch 8/64 loss: -0.2629566788673401
Batch 9/64 loss: -0.309378981590271
Batch 10/64 loss: -0.31061533093452454
Batch 11/64 loss: -0.289189875125885
Batch 12/64 loss: -0.26600950956344604
Batch 13/64 loss: -0.29410651326179504
Batch 14/64 loss: -0.3067709803581238
Batch 15/64 loss: -0.33205118775367737
Batch 16/64 loss: -0.2739935517311096
Batch 17/64 loss: -0.3210103213787079
Batch 18/64 loss: -0.2855544984340668
Batch 19/64 loss: -0.31830495595932007
Batch 20/64 loss: -0.23278161883354187
Batch 21/64 loss: -0.31700560450553894
Batch 22/64 loss: -0.2864878177642822
Batch 23/64 loss: -0.29601627588272095
Batch 24/64 loss: -0.31702524423599243
Batch 25/64 loss: -0.2784874439239502
Batch 26/64 loss: -0.2904830873012543
Batch 27/64 loss: -0.2760600447654724
Batch 28/64 loss: -0.3211473822593689
Batch 29/64 loss: -0.312636137008667
Batch 30/64 loss: -0.297505646944046
Batch 31/64 loss: -0.3157440423965454
Batch 32/64 loss: -0.31557121872901917
Batch 33/64 loss: -0.2892007827758789
Batch 34/64 loss: -0.30654293298721313
Batch 35/64 loss: -0.29696571826934814
Batch 36/64 loss: -0.2999155819416046
Batch 37/64 loss: -0.3106285631656647
Batch 38/64 loss: -0.2966381907463074
Batch 39/64 loss: -0.29088735580444336
Batch 40/64 loss: -0.30270013213157654
Batch 41/64 loss: -0.2849549651145935
Batch 42/64 loss: -0.3188558518886566
Batch 43/64 loss: -0.32957619428634644
Batch 44/64 loss: -0.3071731925010681
Batch 45/64 loss: -0.31851816177368164
Batch 46/64 loss: -0.3363972008228302
Batch 47/64 loss: -0.2954372465610504
Batch 48/64 loss: -0.294172465801239
Batch 49/64 loss: -0.2491363286972046
Batch 50/64 loss: -0.31040316820144653
Batch 51/64 loss: -0.2217821478843689
Batch 52/64 loss: -0.2690874934196472
Batch 53/64 loss: -0.3136052191257477
Batch 54/64 loss: -0.261918842792511
Batch 55/64 loss: -0.29379934072494507
Batch 56/64 loss: -0.3188844621181488
Batch 57/64 loss: -0.32708221673965454
Batch 58/64 loss: -0.293874591588974
Batch 59/64 loss: -0.31034886837005615
Batch 60/64 loss: -0.3007562458515167
Batch 61/64 loss: -0.26789841055870056
Batch 62/64 loss: -0.2975059747695923
Batch 63/64 loss: -0.2741186022758484
Batch 64/64 loss: -0.2820678949356079
Epoch 267  Train loss: -0.29678820768992104  Val loss: 0.027322235385986538
Epoch 268
-------------------------------
Batch 1/64 loss: -0.30207616090774536
Batch 2/64 loss: -0.29838401079177856
Batch 3/64 loss: -0.2688092887401581
Batch 4/64 loss: -0.2952032685279846
Batch 5/64 loss: -0.3155680298805237
Batch 6/64 loss: -0.27899840474128723
Batch 7/64 loss: -0.3215305507183075
Batch 8/64 loss: -0.28493988513946533
Batch 9/64 loss: -0.28646427392959595
Batch 10/64 loss: -0.2873167395591736
Batch 11/64 loss: -0.2924402952194214
Batch 12/64 loss: -0.3125174343585968
Batch 13/64 loss: -0.27663931250572205
Batch 14/64 loss: -0.25234368443489075
Batch 15/64 loss: -0.3030507266521454
Batch 16/64 loss: -0.32006025314331055
Batch 17/64 loss: -0.2940574288368225
Batch 18/64 loss: -0.3174651265144348
Batch 19/64 loss: -0.2919467091560364
Batch 20/64 loss: -0.3031250238418579
Batch 21/64 loss: -0.25035274028778076
Batch 22/64 loss: -0.29194509983062744
Batch 23/64 loss: -0.30053818225860596
Batch 24/64 loss: -0.31911733746528625
Batch 25/64 loss: -0.31394335627555847
Batch 26/64 loss: -0.31355565786361694
Batch 27/64 loss: -0.2670426070690155
Batch 28/64 loss: -0.2834002375602722
Batch 29/64 loss: -0.31538888812065125
Batch 30/64 loss: -0.3119022250175476
Batch 31/64 loss: -0.30234575271606445
Batch 32/64 loss: -0.3301355838775635
Batch 33/64 loss: -0.28208127617836
Batch 34/64 loss: -0.2646825909614563
Batch 35/64 loss: -0.29504114389419556
Batch 36/64 loss: -0.28521472215652466
Batch 37/64 loss: -0.2823640704154968
Batch 38/64 loss: -0.3010178208351135
Batch 39/64 loss: -0.32545527815818787
Batch 40/64 loss: -0.3051729202270508
Batch 41/64 loss: -0.2819429636001587
Batch 42/64 loss: -0.3001847267150879
Batch 43/64 loss: -0.2927323579788208
Batch 44/64 loss: -0.3108508288860321
Batch 45/64 loss: -0.3276398181915283
Batch 46/64 loss: -0.29502201080322266
Batch 47/64 loss: -0.24146097898483276
Batch 48/64 loss: -0.29178136587142944
Batch 49/64 loss: -0.31877487897872925
Batch 50/64 loss: -0.3146359622478485
Batch 51/64 loss: -0.26310068368911743
Batch 52/64 loss: -0.31112563610076904
Batch 53/64 loss: -0.28684449195861816
Batch 54/64 loss: -0.2963835895061493
Batch 55/64 loss: -0.31670641899108887
Batch 56/64 loss: -0.2753710150718689
Batch 57/64 loss: -0.3233218789100647
Batch 58/64 loss: -0.3052341938018799
Batch 59/64 loss: -0.31014296412467957
Batch 60/64 loss: -0.32789433002471924
Batch 61/64 loss: -0.3107905685901642
Batch 62/64 loss: -0.3104834258556366
Batch 63/64 loss: -0.2556995153427124
Batch 64/64 loss: -0.30548909306526184
Epoch 268  Train loss: -0.2971117729065465  Val loss: 0.025320003327635145
Epoch 269
-------------------------------
Batch 1/64 loss: -0.3087182641029358
Batch 2/64 loss: -0.2580326199531555
Batch 3/64 loss: -0.3271229863166809
Batch 4/64 loss: -0.2916506826877594
Batch 5/64 loss: -0.33543407917022705
Batch 6/64 loss: -0.30270493030548096
Batch 7/64 loss: -0.3181082308292389
Batch 8/64 loss: -0.28642576932907104
Batch 9/64 loss: -0.30413371324539185
Batch 10/64 loss: -0.3007984161376953
Batch 11/64 loss: -0.2916538119316101
Batch 12/64 loss: -0.2946421504020691
Batch 13/64 loss: -0.2881127595901489
Batch 14/64 loss: -0.335845410823822
Batch 15/64 loss: -0.3201908469200134
Batch 16/64 loss: -0.30587121844291687
Batch 17/64 loss: -0.298056423664093
Batch 18/64 loss: -0.2973300814628601
Batch 19/64 loss: -0.2947280704975128
Batch 20/64 loss: -0.2897990942001343
Batch 21/64 loss: -0.305950403213501
Batch 22/64 loss: -0.3358558714389801
Batch 23/64 loss: -0.3121330142021179
Batch 24/64 loss: -0.30202606320381165
Batch 25/64 loss: -0.30851244926452637
Batch 26/64 loss: -0.28898561000823975
Batch 27/64 loss: -0.28786665201187134
Batch 28/64 loss: -0.2829674184322357
Batch 29/64 loss: -0.23741492629051208
Batch 30/64 loss: -0.28604763746261597
Batch 31/64 loss: -0.2800755202770233
Batch 32/64 loss: -0.29658085107803345
Batch 33/64 loss: -0.33114469051361084
Batch 34/64 loss: -0.2850467562675476
Batch 35/64 loss: -0.30513936281204224
Batch 36/64 loss: -0.2566679120063782
Batch 37/64 loss: -0.3007230758666992
Batch 38/64 loss: -0.28160929679870605
Batch 39/64 loss: -0.27694275975227356
Batch 40/64 loss: -0.28600871562957764
Batch 41/64 loss: -0.2796630859375
Batch 42/64 loss: -0.31586119532585144
Batch 43/64 loss: -0.2740958333015442
Batch 44/64 loss: -0.2992960810661316
Batch 45/64 loss: -0.25699740648269653
Batch 46/64 loss: -0.3091461956501007
Batch 47/64 loss: -0.3061921000480652
Batch 48/64 loss: -0.245996356010437
Batch 49/64 loss: -0.34016722440719604
Batch 50/64 loss: -0.319137305021286
Batch 51/64 loss: -0.30375996232032776
Batch 52/64 loss: -0.28240686655044556
Batch 53/64 loss: -0.2911943793296814
Batch 54/64 loss: -0.2819744348526001
Batch 55/64 loss: -0.31263816356658936
Batch 56/64 loss: -0.321718692779541
Batch 57/64 loss: -0.2586985230445862
Batch 58/64 loss: -0.30512914061546326
Batch 59/64 loss: -0.2925603985786438
Batch 60/64 loss: -0.24932855367660522
Batch 61/64 loss: -0.2980026602745056
Batch 62/64 loss: -0.30032190680503845
Batch 63/64 loss: -0.2961825132369995
Batch 64/64 loss: -0.2953188717365265
Epoch 269  Train loss: -0.29582771261533103  Val loss: 0.02698803746823183
Epoch 270
-------------------------------
Batch 1/64 loss: -0.33985409140586853
Batch 2/64 loss: -0.2880594730377197
Batch 3/64 loss: -0.28976213932037354
Batch 4/64 loss: -0.27132469415664673
Batch 5/64 loss: -0.29908859729766846
Batch 6/64 loss: -0.30841630697250366
Batch 7/64 loss: -0.29650312662124634
Batch 8/64 loss: -0.2960313856601715
Batch 9/64 loss: -0.30752718448638916
Batch 10/64 loss: -0.27870073914527893
Batch 11/64 loss: -0.2751884460449219
Batch 12/64 loss: -0.29079920053482056
Batch 13/64 loss: -0.2691503167152405
Batch 14/64 loss: -0.3060454726219177
Batch 15/64 loss: -0.2825649678707123
Batch 16/64 loss: -0.296743243932724
Batch 17/64 loss: -0.2944478392601013
Batch 18/64 loss: -0.31480056047439575
Batch 19/64 loss: -0.3176935911178589
Batch 20/64 loss: -0.29020068049430847
Batch 21/64 loss: -0.2614113688468933
Batch 22/64 loss: -0.3164467215538025
Batch 23/64 loss: -0.3194035589694977
Batch 24/64 loss: -0.3034805655479431
Batch 25/64 loss: -0.30014681816101074
Batch 26/64 loss: -0.3099793791770935
Batch 27/64 loss: -0.3230557143688202
Batch 28/64 loss: -0.3078078031539917
Batch 29/64 loss: -0.3103678822517395
Batch 30/64 loss: -0.3055294156074524
Batch 31/64 loss: -0.30966711044311523
Batch 32/64 loss: -0.3183777630329132
Batch 33/64 loss: -0.31689685583114624
Batch 34/64 loss: -0.3225722312927246
Batch 35/64 loss: -0.29528501629829407
Batch 36/64 loss: -0.2816692590713501
Batch 37/64 loss: -0.32644012570381165
Batch 38/64 loss: -0.3170129060745239
Batch 39/64 loss: -0.33676010370254517
Batch 40/64 loss: -0.28700336813926697
Batch 41/64 loss: -0.34369486570358276
Batch 42/64 loss: -0.289286732673645
Batch 43/64 loss: -0.2930139899253845
Batch 44/64 loss: -0.33362525701522827
Batch 45/64 loss: -0.30095359683036804
Batch 46/64 loss: -0.321556955575943
Batch 47/64 loss: -0.2896192967891693
Batch 48/64 loss: -0.31824496388435364
Batch 49/64 loss: -0.299072265625
Batch 50/64 loss: -0.30311113595962524
Batch 51/64 loss: -0.3008132576942444
Batch 52/64 loss: -0.27931612730026245
Batch 53/64 loss: -0.253201425075531
Batch 54/64 loss: -0.26648473739624023
Batch 55/64 loss: -0.31718239188194275
Batch 56/64 loss: -0.29203110933303833
Batch 57/64 loss: -0.309088796377182
Batch 58/64 loss: -0.30076876282691956
Batch 59/64 loss: -0.298645943403244
Batch 60/64 loss: -0.2970306873321533
Batch 61/64 loss: -0.3158453702926636
Batch 62/64 loss: -0.27329111099243164
Batch 63/64 loss: -0.2932065427303314
Batch 64/64 loss: -0.2656596004962921
Epoch 270  Train loss: -0.30071444780218837  Val loss: 0.028906331029544582
Epoch 271
-------------------------------
Batch 1/64 loss: -0.31566566228866577
Batch 2/64 loss: -0.31093651056289673
Batch 3/64 loss: -0.2733994722366333
Batch 4/64 loss: -0.28582650423049927
Batch 5/64 loss: -0.32016491889953613
Batch 6/64 loss: -0.2839963436126709
Batch 7/64 loss: -0.3277340531349182
Batch 8/64 loss: -0.3175157904624939
Batch 9/64 loss: -0.33701443672180176
Batch 10/64 loss: -0.31679487228393555
Batch 11/64 loss: -0.2619616389274597
Batch 12/64 loss: -0.29470095038414
Batch 13/64 loss: -0.31176161766052246
Batch 14/64 loss: -0.3045828342437744
Batch 15/64 loss: -0.2919926643371582
Batch 16/64 loss: -0.2943466901779175
Batch 17/64 loss: -0.305407851934433
Batch 18/64 loss: -0.30650919675827026
Batch 19/64 loss: -0.289018839597702
Batch 20/64 loss: -0.31683623790740967
Batch 21/64 loss: -0.28763341903686523
Batch 22/64 loss: -0.3227827847003937
Batch 23/64 loss: -0.2619696855545044
Batch 24/64 loss: -0.2576633095741272
Batch 25/64 loss: -0.30578529834747314
Batch 26/64 loss: -0.300703227519989
Batch 27/64 loss: -0.30438390374183655
Batch 28/64 loss: -0.2796313166618347
Batch 29/64 loss: -0.3198554515838623
Batch 30/64 loss: -0.26676836609840393
Batch 31/64 loss: -0.2904788553714752
Batch 32/64 loss: -0.3284817337989807
Batch 33/64 loss: -0.2967049777507782
Batch 34/64 loss: -0.293424516916275
Batch 35/64 loss: -0.29475724697113037
Batch 36/64 loss: -0.28761178255081177
Batch 37/64 loss: -0.2928328812122345
Batch 38/64 loss: -0.3007934093475342
Batch 39/64 loss: -0.32493478059768677
Batch 40/64 loss: -0.30077409744262695
Batch 41/64 loss: -0.3050447106361389
Batch 42/64 loss: -0.30298370122909546
Batch 43/64 loss: -0.33492350578308105
Batch 44/64 loss: -0.2913421392440796
Batch 45/64 loss: -0.25622880458831787
Batch 46/64 loss: -0.30735358595848083
Batch 47/64 loss: -0.2913888394832611
Batch 48/64 loss: -0.31346455216407776
Batch 49/64 loss: -0.25132012367248535
Batch 50/64 loss: -0.2769421339035034
Batch 51/64 loss: -0.289557546377182
Batch 52/64 loss: -0.31146305799484253
Batch 53/64 loss: -0.2949831187725067
Batch 54/64 loss: -0.30294954776763916
Batch 55/64 loss: -0.3050737977027893
Batch 56/64 loss: -0.3020067811012268
Batch 57/64 loss: -0.2894550561904907
Batch 58/64 loss: -0.3194080591201782
Batch 59/64 loss: -0.27554962038993835
Batch 60/64 loss: -0.27470332384109497
Batch 61/64 loss: -0.2891642451286316
Batch 62/64 loss: -0.31477653980255127
Batch 63/64 loss: -0.2795909345149994
Batch 64/64 loss: -0.3239157199859619
Epoch 271  Train loss: -0.2981450768078075  Val loss: 0.02911513285948239
Epoch 272
-------------------------------
Batch 1/64 loss: -0.279338002204895
Batch 2/64 loss: -0.30922672152519226
Batch 3/64 loss: -0.3176433742046356
Batch 4/64 loss: -0.3077561855316162
Batch 5/64 loss: -0.30618953704833984
Batch 6/64 loss: -0.3454829156398773
Batch 7/64 loss: -0.25936710834503174
Batch 8/64 loss: -0.3146490156650543
Batch 9/64 loss: -0.2934953570365906
Batch 10/64 loss: -0.25896191596984863
Batch 11/64 loss: -0.2572394907474518
Batch 12/64 loss: -0.32743334770202637
Batch 13/64 loss: -0.32457423210144043
Batch 14/64 loss: -0.3276849389076233
Batch 15/64 loss: -0.27806922793388367
Batch 16/64 loss: -0.30594995617866516
Batch 17/64 loss: -0.30174720287323
Batch 18/64 loss: -0.30006980895996094
Batch 19/64 loss: -0.3220532536506653
Batch 20/64 loss: -0.3081943988800049
Batch 21/64 loss: -0.29853230714797974
Batch 22/64 loss: -0.31450313329696655
Batch 23/64 loss: -0.3128150403499603
Batch 24/64 loss: -0.27344268560409546
Batch 25/64 loss: -0.2981933057308197
Batch 26/64 loss: -0.2853814959526062
Batch 27/64 loss: -0.3122521638870239
Batch 28/64 loss: -0.32206153869628906
Batch 29/64 loss: -0.31691884994506836
Batch 30/64 loss: -0.3113093376159668
Batch 31/64 loss: -0.3053244352340698
Batch 32/64 loss: -0.298488050699234
Batch 33/64 loss: -0.2653028964996338
Batch 34/64 loss: -0.2961636185646057
Batch 35/64 loss: -0.3194059133529663
Batch 36/64 loss: -0.2961058020591736
Batch 37/64 loss: -0.33148086071014404
Batch 38/64 loss: -0.28309744596481323
Batch 39/64 loss: -0.31262749433517456
Batch 40/64 loss: -0.2899198532104492
Batch 41/64 loss: -0.3406207859516144
Batch 42/64 loss: -0.30020415782928467
Batch 43/64 loss: -0.31775689125061035
Batch 44/64 loss: -0.30716800689697266
Batch 45/64 loss: -0.31045499444007874
Batch 46/64 loss: -0.28500163555145264
Batch 47/64 loss: -0.24887046217918396
Batch 48/64 loss: -0.2744365334510803
Batch 49/64 loss: -0.28663092851638794
Batch 50/64 loss: -0.3246680498123169
Batch 51/64 loss: -0.29625624418258667
Batch 52/64 loss: -0.2847142517566681
Batch 53/64 loss: -0.30138224363327026
Batch 54/64 loss: -0.3164656162261963
Batch 55/64 loss: -0.3193764090538025
Batch 56/64 loss: -0.2850140333175659
Batch 57/64 loss: -0.2446017563343048
Batch 58/64 loss: -0.32289689779281616
Batch 59/64 loss: -0.2816980481147766
Batch 60/64 loss: -0.31898033618927
Batch 61/64 loss: -0.3234216570854187
Batch 62/64 loss: -0.27292555570602417
Batch 63/64 loss: -0.2722383141517639
Batch 64/64 loss: -0.29209575057029724
Epoch 272  Train loss: -0.3002871818402234  Val loss: 0.02752459315499899
Epoch 273
-------------------------------
Batch 1/64 loss: -0.32231786847114563
Batch 2/64 loss: -0.28949448466300964
Batch 3/64 loss: -0.28278934955596924
Batch 4/64 loss: -0.2842177152633667
Batch 5/64 loss: -0.2966676950454712
Batch 6/64 loss: -0.2774161100387573
Batch 7/64 loss: -0.3325812518596649
Batch 8/64 loss: -0.3266754150390625
Batch 9/64 loss: -0.31389784812927246
Batch 10/64 loss: -0.29349446296691895
Batch 11/64 loss: -0.26807093620300293
Batch 12/64 loss: -0.27820706367492676
Batch 13/64 loss: -0.3204064667224884
Batch 14/64 loss: -0.32634320855140686
Batch 15/64 loss: -0.3175193965435028
Batch 16/64 loss: -0.2996581494808197
Batch 17/64 loss: -0.2785084843635559
Batch 18/64 loss: -0.2852536737918854
Batch 19/64 loss: -0.28453320264816284
Batch 20/64 loss: -0.3242993950843811
Batch 21/64 loss: -0.3268756866455078
Batch 22/64 loss: -0.2986261546611786
Batch 23/64 loss: -0.29388895630836487
Batch 24/64 loss: -0.3090091943740845
Batch 25/64 loss: -0.29019448161125183
Batch 26/64 loss: -0.3200209140777588
Batch 27/64 loss: -0.28231579065322876
Batch 28/64 loss: -0.31308823823928833
Batch 29/64 loss: -0.2896847724914551
Batch 30/64 loss: -0.2952955365180969
Batch 31/64 loss: -0.32828599214553833
Batch 32/64 loss: -0.2915732264518738
Batch 33/64 loss: -0.3304324746131897
Batch 34/64 loss: -0.3179551959037781
Batch 35/64 loss: -0.31765541434288025
Batch 36/64 loss: -0.3098390996456146
Batch 37/64 loss: -0.29754817485809326
Batch 38/64 loss: -0.2800337076187134
Batch 39/64 loss: -0.29599830508232117
Batch 40/64 loss: -0.31270530819892883
Batch 41/64 loss: -0.32116758823394775
Batch 42/64 loss: -0.3049168586730957
Batch 43/64 loss: -0.32758939266204834
Batch 44/64 loss: -0.3063077926635742
Batch 45/64 loss: -0.2663539946079254
Batch 46/64 loss: -0.2844885587692261
Batch 47/64 loss: -0.29733121395111084
Batch 48/64 loss: -0.30745983123779297
Batch 49/64 loss: -0.29665303230285645
Batch 50/64 loss: -0.3329797387123108
Batch 51/64 loss: -0.32399338483810425
Batch 52/64 loss: -0.3025881350040436
Batch 53/64 loss: -0.25559550523757935
Batch 54/64 loss: -0.3191014230251312
Batch 55/64 loss: -0.31286725401878357
Batch 56/64 loss: -0.3084658980369568
Batch 57/64 loss: -0.2845180332660675
Batch 58/64 loss: -0.3115772008895874
Batch 59/64 loss: -0.32320636510849
Batch 60/64 loss: -0.3008342385292053
Batch 61/64 loss: -0.2732089161872864
Batch 62/64 loss: -0.29073137044906616
Batch 63/64 loss: -0.2988261878490448
Batch 64/64 loss: -0.3012118935585022
Epoch 273  Train loss: -0.3024321511680005  Val loss: 0.025875728359746768
Epoch 274
-------------------------------
Batch 1/64 loss: -0.28076356649398804
Batch 2/64 loss: -0.3038340210914612
Batch 3/64 loss: -0.26547738909721375
Batch 4/64 loss: -0.3009890615940094
Batch 5/64 loss: -0.3209083676338196
Batch 6/64 loss: -0.2965131402015686
Batch 7/64 loss: -0.29062318801879883
Batch 8/64 loss: -0.2972514033317566
Batch 9/64 loss: -0.3123897314071655
Batch 10/64 loss: -0.298543781042099
Batch 11/64 loss: -0.30517974495887756
Batch 12/64 loss: -0.2847287654876709
Batch 13/64 loss: -0.31521177291870117
Batch 14/64 loss: -0.2967762351036072
Batch 15/64 loss: -0.2823619842529297
Batch 16/64 loss: -0.29453790187835693
Batch 17/64 loss: -0.3170301616191864
Batch 18/64 loss: -0.32560062408447266
Batch 19/64 loss: -0.32593417167663574
Batch 20/64 loss: -0.32003194093704224
Batch 21/64 loss: -0.27876612544059753
Batch 22/64 loss: -0.2844851016998291
Batch 23/64 loss: -0.26114365458488464
Batch 24/64 loss: -0.305318146944046
Batch 25/64 loss: -0.3357195258140564
Batch 26/64 loss: -0.2985732853412628
Batch 27/64 loss: -0.3244580030441284
Batch 28/64 loss: -0.3234419822692871
Batch 29/64 loss: -0.3170625567436218
Batch 30/64 loss: -0.27175453305244446
Batch 31/64 loss: -0.3196534514427185
Batch 32/64 loss: -0.3234815001487732
Batch 33/64 loss: -0.28868940472602844
Batch 34/64 loss: -0.29343414306640625
Batch 35/64 loss: -0.3213828206062317
Batch 36/64 loss: -0.31978532671928406
Batch 37/64 loss: -0.22919651865959167
Batch 38/64 loss: -0.32802724838256836
Batch 39/64 loss: -0.28364187479019165
Batch 40/64 loss: -0.3087049722671509
Batch 41/64 loss: -0.3238881230354309
Batch 42/64 loss: -0.25576767325401306
Batch 43/64 loss: -0.3171755075454712
Batch 44/64 loss: -0.3180083632469177
Batch 45/64 loss: -0.30532240867614746
Batch 46/64 loss: -0.3094157576560974
Batch 47/64 loss: -0.3095667362213135
Batch 48/64 loss: -0.3024248480796814
Batch 49/64 loss: -0.2784770131111145
Batch 50/64 loss: -0.296356737613678
Batch 51/64 loss: -0.3013097047805786
Batch 52/64 loss: -0.2832190692424774
Batch 53/64 loss: -0.2823753356933594
Batch 54/64 loss: -0.29538166522979736
Batch 55/64 loss: -0.2949855625629425
Batch 56/64 loss: -0.29485997557640076
Batch 57/64 loss: -0.32097142934799194
Batch 58/64 loss: -0.30326777696609497
Batch 59/64 loss: -0.23659861087799072
Batch 60/64 loss: -0.28624218702316284
Batch 61/64 loss: -0.29692137241363525
Batch 62/64 loss: -0.3110930919647217
Batch 63/64 loss: -0.28505292534828186
Batch 64/64 loss: -0.29787519574165344
Epoch 274  Train loss: -0.29934894746425106  Val loss: 0.028327394187245582
Epoch 275
-------------------------------
Batch 1/64 loss: -0.32808881998062134
Batch 2/64 loss: -0.3084888458251953
Batch 3/64 loss: -0.2778969705104828
Batch 4/64 loss: -0.2445579469203949
Batch 5/64 loss: -0.29926496744155884
Batch 6/64 loss: -0.29606133699417114
Batch 7/64 loss: -0.2884867191314697
Batch 8/64 loss: -0.2913103699684143
Batch 9/64 loss: -0.2882477045059204
Batch 10/64 loss: -0.28072047233581543
Batch 11/64 loss: -0.2691532373428345
Batch 12/64 loss: -0.32766395807266235
Batch 13/64 loss: -0.26579809188842773
Batch 14/64 loss: -0.3088383674621582
Batch 15/64 loss: -0.3123670220375061
Batch 16/64 loss: -0.28020814061164856
Batch 17/64 loss: -0.26815471053123474
Batch 18/64 loss: -0.29137080907821655
Batch 19/64 loss: -0.30443400144577026
Batch 20/64 loss: -0.3130718469619751
Batch 21/64 loss: -0.3322775661945343
Batch 22/64 loss: -0.30932313203811646
Batch 23/64 loss: -0.2888699471950531
Batch 24/64 loss: -0.31151995062828064
Batch 25/64 loss: -0.32502591609954834
Batch 26/64 loss: -0.2962942123413086
Batch 27/64 loss: -0.2686101794242859
Batch 28/64 loss: -0.2848612070083618
Batch 29/64 loss: -0.3092769384384155
Batch 30/64 loss: -0.3271583020687103
Batch 31/64 loss: -0.3197155296802521
Batch 32/64 loss: -0.29848891496658325
Batch 33/64 loss: -0.31714296340942383
Batch 34/64 loss: -0.26408928632736206
Batch 35/64 loss: -0.3228817880153656
Batch 36/64 loss: -0.3360353112220764
Batch 37/64 loss: -0.2896113693714142
Batch 38/64 loss: -0.26695865392684937
Batch 39/64 loss: -0.3170229196548462
Batch 40/64 loss: -0.31242820620536804
Batch 41/64 loss: -0.31649869680404663
Batch 42/64 loss: -0.2860262989997864
Batch 43/64 loss: -0.31949037313461304
Batch 44/64 loss: -0.31156155467033386
Batch 45/64 loss: -0.29714861512184143
Batch 46/64 loss: -0.31086966395378113
Batch 47/64 loss: -0.32028722763061523
Batch 48/64 loss: -0.29727959632873535
Batch 49/64 loss: -0.29207825660705566
Batch 50/64 loss: -0.2937568128108978
Batch 51/64 loss: -0.3091096878051758
Batch 52/64 loss: -0.30576080083847046
Batch 53/64 loss: -0.28570061922073364
Batch 54/64 loss: -0.2959597706794739
Batch 55/64 loss: -0.26866334676742554
Batch 56/64 loss: -0.2625509202480316
Batch 57/64 loss: -0.27010786533355713
Batch 58/64 loss: -0.3231063187122345
Batch 59/64 loss: -0.3209937810897827
Batch 60/64 loss: -0.28363287448883057
Batch 61/64 loss: -0.27320051193237305
Batch 62/64 loss: -0.3194834291934967
Batch 63/64 loss: -0.30598706007003784
Batch 64/64 loss: -0.31671619415283203
Epoch 275  Train loss: -0.298801064491272  Val loss: 0.026777402027366087
Epoch 276
-------------------------------
Batch 1/64 loss: -0.29869329929351807
Batch 2/64 loss: -0.2659767270088196
Batch 3/64 loss: -0.3045194745063782
Batch 4/64 loss: -0.2701757550239563
Batch 5/64 loss: -0.327132910490036
Batch 6/64 loss: -0.309267520904541
Batch 7/64 loss: -0.272774338722229
Batch 8/64 loss: -0.295777827501297
Batch 9/64 loss: -0.3293875455856323
Batch 10/64 loss: -0.30859291553497314
Batch 11/64 loss: -0.3321481943130493
Batch 12/64 loss: -0.32446402311325073
Batch 13/64 loss: -0.3043479919433594
Batch 14/64 loss: -0.32334083318710327
Batch 15/64 loss: -0.3068912923336029
Batch 16/64 loss: -0.32336100935935974
Batch 17/64 loss: -0.31507962942123413
Batch 18/64 loss: -0.2684568464756012
Batch 19/64 loss: -0.33937913179397583
Batch 20/64 loss: -0.3008679449558258
Batch 21/64 loss: -0.2651686370372772
Batch 22/64 loss: -0.30392545461654663
Batch 23/64 loss: -0.2869179844856262
Batch 24/64 loss: -0.2882121801376343
Batch 25/64 loss: -0.2855784595012665
Batch 26/64 loss: -0.33376240730285645
Batch 27/64 loss: -0.3151249885559082
Batch 28/64 loss: -0.29388391971588135
Batch 29/64 loss: -0.31367963552474976
Batch 30/64 loss: -0.3347667157649994
Batch 31/64 loss: -0.310928612947464
Batch 32/64 loss: -0.3367730975151062
Batch 33/64 loss: -0.3202972412109375
Batch 34/64 loss: -0.2821846306324005
Batch 35/64 loss: -0.32385945320129395
Batch 36/64 loss: -0.31359270215034485
Batch 37/64 loss: -0.2989390790462494
Batch 38/64 loss: -0.3258413076400757
Batch 39/64 loss: -0.3201335370540619
Batch 40/64 loss: -0.2804446220397949
Batch 41/64 loss: -0.28591492772102356
Batch 42/64 loss: -0.31720370054244995
Batch 43/64 loss: -0.3297900855541229
Batch 44/64 loss: -0.28700751066207886
Batch 45/64 loss: -0.315204381942749
Batch 46/64 loss: -0.31820806860923767
Batch 47/64 loss: -0.28890329599380493
Batch 48/64 loss: -0.29740458726882935
Batch 49/64 loss: -0.2960837185382843
Batch 50/64 loss: -0.30749747157096863
Batch 51/64 loss: -0.29384535551071167
Batch 52/64 loss: -0.30206790566444397
Batch 53/64 loss: -0.30290311574935913
Batch 54/64 loss: -0.27168703079223633
Batch 55/64 loss: -0.3375425338745117
Batch 56/64 loss: -0.27562618255615234
Batch 57/64 loss: -0.2783471941947937
Batch 58/64 loss: -0.23069632053375244
Batch 59/64 loss: -0.2816391587257385
Batch 60/64 loss: -0.260884553194046
Batch 61/64 loss: -0.3138529658317566
Batch 62/64 loss: -0.2696993947029114
Batch 63/64 loss: -0.24434813857078552
Batch 64/64 loss: -0.28001952171325684
Epoch 276  Train loss: -0.3007219155629476  Val loss: 0.028421899911873938
Epoch 277
-------------------------------
Batch 1/64 loss: -0.271625280380249
Batch 2/64 loss: -0.28419339656829834
Batch 3/64 loss: -0.329256534576416
Batch 4/64 loss: -0.29901301860809326
Batch 5/64 loss: -0.2832263708114624
Batch 6/64 loss: -0.2989058494567871
Batch 7/64 loss: -0.31304046511650085
Batch 8/64 loss: -0.32253384590148926
Batch 9/64 loss: -0.30217447876930237
Batch 10/64 loss: -0.324776828289032
Batch 11/64 loss: -0.30185315012931824
Batch 12/64 loss: -0.3276279866695404
Batch 13/64 loss: -0.2635279893875122
Batch 14/64 loss: -0.3240855932235718
Batch 15/64 loss: -0.29868626594543457
Batch 16/64 loss: -0.3149670362472534
Batch 17/64 loss: -0.3125647306442261
Batch 18/64 loss: -0.2935691177845001
Batch 19/64 loss: -0.2978073060512543
Batch 20/64 loss: -0.29707956314086914
Batch 21/64 loss: -0.291728138923645
Batch 22/64 loss: -0.2959684133529663
Batch 23/64 loss: -0.30695784091949463
Batch 24/64 loss: -0.31353336572647095
Batch 25/64 loss: -0.2751436233520508
Batch 26/64 loss: -0.31067490577697754
Batch 27/64 loss: -0.31986096501350403
Batch 28/64 loss: -0.28737369179725647
Batch 29/64 loss: -0.31505686044692993
Batch 30/64 loss: -0.3277592658996582
Batch 31/64 loss: -0.28263235092163086
Batch 32/64 loss: -0.3169853985309601
Batch 33/64 loss: -0.298281729221344
Batch 34/64 loss: -0.32190370559692383
Batch 35/64 loss: -0.2892611622810364
Batch 36/64 loss: -0.2941628694534302
Batch 37/64 loss: -0.33782756328582764
Batch 38/64 loss: -0.3228137195110321
Batch 39/64 loss: -0.3230816423892975
Batch 40/64 loss: -0.31160640716552734
Batch 41/64 loss: -0.2572900056838989
Batch 42/64 loss: -0.3007752299308777
Batch 43/64 loss: -0.31450799107551575
Batch 44/64 loss: -0.31213802099227905
Batch 45/64 loss: -0.2892303466796875
Batch 46/64 loss: -0.29050523042678833
Batch 47/64 loss: -0.31454378366470337
Batch 48/64 loss: -0.31318604946136475
Batch 49/64 loss: -0.2728572189807892
Batch 50/64 loss: -0.32440805435180664
Batch 51/64 loss: -0.3467405438423157
Batch 52/64 loss: -0.3045753240585327
Batch 53/64 loss: -0.3168445825576782
Batch 54/64 loss: -0.2818160057067871
Batch 55/64 loss: -0.3049106001853943
Batch 56/64 loss: -0.2930283546447754
Batch 57/64 loss: -0.3053615093231201
Batch 58/64 loss: -0.30582159757614136
Batch 59/64 loss: -0.3062295913696289
Batch 60/64 loss: -0.3274204134941101
Batch 61/64 loss: -0.2805187702178955
Batch 62/64 loss: -0.31242635846138
Batch 63/64 loss: -0.25739821791648865
Batch 64/64 loss: -0.25548088550567627
Epoch 277  Train loss: -0.3031729081097771  Val loss: 0.02824617702117081
Epoch 278
-------------------------------
Batch 1/64 loss: -0.30333614349365234
Batch 2/64 loss: -0.2602323889732361
Batch 3/64 loss: -0.30939170718193054
Batch 4/64 loss: -0.32788097858428955
Batch 5/64 loss: -0.26563385128974915
Batch 6/64 loss: -0.30294638872146606
Batch 7/64 loss: -0.2639079689979553
Batch 8/64 loss: -0.3121902048587799
Batch 9/64 loss: -0.31140345335006714
Batch 10/64 loss: -0.3272400498390198
Batch 11/64 loss: -0.3036654591560364
Batch 12/64 loss: -0.2933264970779419
Batch 13/64 loss: -0.2980886399745941
Batch 14/64 loss: -0.3219853639602661
Batch 15/64 loss: -0.3247417211532593
Batch 16/64 loss: -0.3233106732368469
Batch 17/64 loss: -0.3109719455242157
Batch 18/64 loss: -0.337502658367157
Batch 19/64 loss: -0.3235439658164978
Batch 20/64 loss: -0.32412242889404297
Batch 21/64 loss: -0.2826346755027771
Batch 22/64 loss: -0.28104522824287415
Batch 23/64 loss: -0.30025237798690796
Batch 24/64 loss: -0.29717278480529785
Batch 25/64 loss: -0.3376155197620392
Batch 26/64 loss: -0.28957444429397583
Batch 27/64 loss: -0.2860350012779236
Batch 28/64 loss: -0.2739320993423462
Batch 29/64 loss: -0.28247708082199097
Batch 30/64 loss: -0.3304983377456665
Batch 31/64 loss: -0.26905596256256104
Batch 32/64 loss: -0.3023204803466797
Batch 33/64 loss: -0.3104129433631897
Batch 34/64 loss: -0.2834823727607727
Batch 35/64 loss: -0.30615976452827454
Batch 36/64 loss: -0.3060620427131653
Batch 37/64 loss: -0.290679931640625
Batch 38/64 loss: -0.3164842128753662
Batch 39/64 loss: -0.2891610860824585
Batch 40/64 loss: -0.3167155385017395
Batch 41/64 loss: -0.30858832597732544
Batch 42/64 loss: -0.23842376470565796
Batch 43/64 loss: -0.30709633231163025
Batch 44/64 loss: -0.3006454110145569
Batch 45/64 loss: -0.2558518946170807
Batch 46/64 loss: -0.3077194094657898
Batch 47/64 loss: -0.28615373373031616
Batch 48/64 loss: -0.3034612238407135
Batch 49/64 loss: -0.338685005903244
Batch 50/64 loss: -0.2698165476322174
Batch 51/64 loss: -0.3203580975532532
Batch 52/64 loss: -0.3018835783004761
Batch 53/64 loss: -0.3013212978839874
Batch 54/64 loss: -0.2683417499065399
Batch 55/64 loss: -0.32575803995132446
Batch 56/64 loss: -0.301883339881897
Batch 57/64 loss: -0.29885151982307434
Batch 58/64 loss: -0.29787057638168335
Batch 59/64 loss: -0.28222453594207764
Batch 60/64 loss: -0.2784017324447632
Batch 61/64 loss: -0.339735209941864
Batch 62/64 loss: -0.29487210512161255
Batch 63/64 loss: -0.2877960801124573
Batch 64/64 loss: -0.2991027235984802
Epoch 278  Train loss: -0.30019232829411824  Val loss: 0.027229759496511872
Epoch 279
-------------------------------
Batch 1/64 loss: -0.23535370826721191
Batch 2/64 loss: -0.3079371154308319
Batch 3/64 loss: -0.3064325153827667
Batch 4/64 loss: -0.32559895515441895
Batch 5/64 loss: -0.31342068314552307
Batch 6/64 loss: -0.2763269543647766
Batch 7/64 loss: -0.289003849029541
Batch 8/64 loss: -0.2975153923034668
Batch 9/64 loss: -0.28907114267349243
Batch 10/64 loss: -0.32083308696746826
Batch 11/64 loss: -0.2840401530265808
Batch 12/64 loss: -0.31070876121520996
Batch 13/64 loss: -0.3028998374938965
Batch 14/64 loss: -0.32870805263519287
Batch 15/64 loss: -0.33063480257987976
Batch 16/64 loss: -0.2927643060684204
Batch 17/64 loss: -0.31034690141677856
Batch 18/64 loss: -0.3092362880706787
Batch 19/64 loss: -0.2879096269607544
Batch 20/64 loss: -0.2863292396068573
Batch 21/64 loss: -0.32671284675598145
Batch 22/64 loss: -0.3215806484222412
Batch 23/64 loss: -0.3228892683982849
Batch 24/64 loss: -0.2603810727596283
Batch 25/64 loss: -0.33024516701698303
Batch 26/64 loss: -0.2769126892089844
Batch 27/64 loss: -0.32539188861846924
Batch 28/64 loss: -0.34572017192840576
Batch 29/64 loss: -0.29448580741882324
Batch 30/64 loss: -0.296924889087677
Batch 31/64 loss: -0.2937343120574951
Batch 32/64 loss: -0.30900099873542786
Batch 33/64 loss: -0.2853281795978546
Batch 34/64 loss: -0.29506340622901917
Batch 35/64 loss: -0.31102731823921204
Batch 36/64 loss: -0.28082409501075745
Batch 37/64 loss: -0.28503039479255676
Batch 38/64 loss: -0.29149001836776733
Batch 39/64 loss: -0.28487035632133484
Batch 40/64 loss: -0.31032827496528625
Batch 41/64 loss: -0.318069189786911
Batch 42/64 loss: -0.316154420375824
Batch 43/64 loss: -0.30954688787460327
Batch 44/64 loss: -0.2876020669937134
Batch 45/64 loss: -0.2772004008293152
Batch 46/64 loss: -0.29393208026885986
Batch 47/64 loss: -0.29294973611831665
Batch 48/64 loss: -0.2955302596092224
Batch 49/64 loss: -0.2476082146167755
Batch 50/64 loss: -0.2792896330356598
Batch 51/64 loss: -0.28419169783592224
Batch 52/64 loss: -0.2681461572647095
Batch 53/64 loss: -0.31156328320503235
Batch 54/64 loss: -0.2958219647407532
Batch 55/64 loss: -0.31595873832702637
Batch 56/64 loss: -0.29200443625450134
Batch 57/64 loss: -0.33324384689331055
Batch 58/64 loss: -0.2866128087043762
Batch 59/64 loss: -0.3128730356693268
Batch 60/64 loss: -0.32883399724960327
Batch 61/64 loss: -0.285898894071579
Batch 62/64 loss: -0.27512675523757935
Batch 63/64 loss: -0.3037305474281311
Batch 64/64 loss: -0.28494393825531006
Epoch 279  Train loss: -0.2992723165773878  Val loss: 0.027272683033828474
Epoch 280
-------------------------------
Batch 1/64 loss: -0.3053608536720276
Batch 2/64 loss: -0.3001359701156616
Batch 3/64 loss: -0.3186340630054474
Batch 4/64 loss: -0.3191157579421997
Batch 5/64 loss: -0.31086939573287964
Batch 6/64 loss: -0.3365001082420349
Batch 7/64 loss: -0.3119249939918518
Batch 8/64 loss: -0.31714773178100586
Batch 9/64 loss: -0.3026445806026459
Batch 10/64 loss: -0.33616533875465393
Batch 11/64 loss: -0.31039658188819885
Batch 12/64 loss: -0.2980014383792877
Batch 13/64 loss: -0.3081805109977722
Batch 14/64 loss: -0.3053833246231079
Batch 15/64 loss: -0.31948626041412354
Batch 16/64 loss: -0.24744939804077148
Batch 17/64 loss: -0.30317503213882446
Batch 18/64 loss: -0.30023062229156494
Batch 19/64 loss: -0.3024848401546478
Batch 20/64 loss: -0.27997010946273804
Batch 21/64 loss: -0.27328455448150635
Batch 22/64 loss: -0.3090580701828003
Batch 23/64 loss: -0.3248019516468048
Batch 24/64 loss: -0.29915642738342285
Batch 25/64 loss: -0.27720484137535095
Batch 26/64 loss: -0.2908340394496918
Batch 27/64 loss: -0.306133896112442
Batch 28/64 loss: -0.275387167930603
Batch 29/64 loss: -0.2915514409542084
Batch 30/64 loss: -0.28115782141685486
Batch 31/64 loss: -0.3353153169155121
Batch 32/64 loss: -0.3084030747413635
Batch 33/64 loss: -0.24945354461669922
Batch 34/64 loss: -0.27099019289016724
Batch 35/64 loss: -0.30413082242012024
Batch 36/64 loss: -0.2724771499633789
Batch 37/64 loss: -0.32900771498680115
Batch 38/64 loss: -0.27078428864479065
Batch 39/64 loss: -0.3207155466079712
Batch 40/64 loss: -0.3207131326198578
Batch 41/64 loss: -0.3105119466781616
Batch 42/64 loss: -0.29670393466949463
Batch 43/64 loss: -0.2662930488586426
Batch 44/64 loss: -0.3217657506465912
Batch 45/64 loss: -0.29215502738952637
Batch 46/64 loss: -0.27337735891342163
Batch 47/64 loss: -0.2964297831058502
Batch 48/64 loss: -0.3014339804649353
Batch 49/64 loss: -0.27240562438964844
Batch 50/64 loss: -0.30110228061676025
Batch 51/64 loss: -0.3026982843875885
Batch 52/64 loss: -0.31723982095718384
Batch 53/64 loss: -0.2992846965789795
Batch 54/64 loss: -0.3395685851573944
Batch 55/64 loss: -0.26806992292404175
Batch 56/64 loss: -0.2725679576396942
Batch 57/64 loss: -0.31487879157066345
Batch 58/64 loss: -0.2815488576889038
Batch 59/64 loss: -0.3288037180900574
Batch 60/64 loss: -0.32361748814582825
Batch 61/64 loss: -0.2416108250617981
Batch 62/64 loss: -0.30983179807662964
Batch 63/64 loss: -0.3101641535758972
Batch 64/64 loss: -0.3140885829925537
Epoch 280  Train loss: -0.2999442820455514  Val loss: 0.025491546724260468
Epoch 281
-------------------------------
Batch 1/64 loss: -0.29026806354522705
Batch 2/64 loss: -0.31122881174087524
Batch 3/64 loss: -0.27311956882476807
Batch 4/64 loss: -0.313418984413147
Batch 5/64 loss: -0.2960229218006134
Batch 6/64 loss: -0.2565559148788452
Batch 7/64 loss: -0.31130754947662354
Batch 8/64 loss: -0.31529325246810913
Batch 9/64 loss: -0.3134037256240845
Batch 10/64 loss: -0.2753889858722687
Batch 11/64 loss: -0.3124181628227234
Batch 12/64 loss: -0.3074239492416382
Batch 13/64 loss: -0.31404954195022583
Batch 14/64 loss: -0.29793596267700195
Batch 15/64 loss: -0.27721232175827026
Batch 16/64 loss: -0.32724207639694214
Batch 17/64 loss: -0.3290081322193146
Batch 18/64 loss: -0.3279989957809448
Batch 19/64 loss: -0.32419925928115845
Batch 20/64 loss: -0.29762083292007446
Batch 21/64 loss: -0.3111565113067627
Batch 22/64 loss: -0.30751270055770874
Batch 23/64 loss: -0.29835593700408936
Batch 24/64 loss: -0.2967630624771118
Batch 25/64 loss: -0.3355267345905304
Batch 26/64 loss: -0.30358999967575073
Batch 27/64 loss: -0.3314353823661804
Batch 28/64 loss: -0.310138463973999
Batch 29/64 loss: -0.32637277245521545
Batch 30/64 loss: -0.30989718437194824
Batch 31/64 loss: -0.29649674892425537
Batch 32/64 loss: -0.31415867805480957
Batch 33/64 loss: -0.2761259973049164
Batch 34/64 loss: -0.30469995737075806
Batch 35/64 loss: -0.27650150656700134
Batch 36/64 loss: -0.32490071654319763
Batch 37/64 loss: -0.3155297040939331
Batch 38/64 loss: -0.2823755145072937
Batch 39/64 loss: -0.3105970025062561
Batch 40/64 loss: -0.30567899346351624
Batch 41/64 loss: -0.31040823459625244
Batch 42/64 loss: -0.2877844572067261
Batch 43/64 loss: -0.3273743689060211
Batch 44/64 loss: -0.28835874795913696
Batch 45/64 loss: -0.3281043469905853
Batch 46/64 loss: -0.200475811958313
Batch 47/64 loss: -0.3108617663383484
Batch 48/64 loss: -0.28291618824005127
Batch 49/64 loss: -0.326187402009964
Batch 50/64 loss: -0.2632966637611389
Batch 51/64 loss: -0.3099600672721863
Batch 52/64 loss: -0.3378177881240845
Batch 53/64 loss: -0.2597605586051941
Batch 54/64 loss: -0.3099846839904785
Batch 55/64 loss: -0.2672821879386902
Batch 56/64 loss: -0.2932364344596863
Batch 57/64 loss: -0.2949792742729187
Batch 58/64 loss: -0.253520667552948
Batch 59/64 loss: -0.2622947692871094
Batch 60/64 loss: -0.32928740978240967
Batch 61/64 loss: -0.2989596128463745
Batch 62/64 loss: -0.2849937081336975
Batch 63/64 loss: -0.3174149692058563
Batch 64/64 loss: -0.29986098408699036
Epoch 281  Train loss: -0.30084841473429813  Val loss: 0.02704227708049656
Epoch 282
-------------------------------
Batch 1/64 loss: -0.2998582422733307
Batch 2/64 loss: -0.29258015751838684
Batch 3/64 loss: -0.2862433195114136
Batch 4/64 loss: -0.32587385177612305
Batch 5/64 loss: -0.30620521306991577
Batch 6/64 loss: -0.3112752139568329
Batch 7/64 loss: -0.2975412905216217
Batch 8/64 loss: -0.29694509506225586
Batch 9/64 loss: -0.30622753500938416
Batch 10/64 loss: -0.2836347818374634
Batch 11/64 loss: -0.31017476320266724
Batch 12/64 loss: -0.30306094884872437
Batch 13/64 loss: -0.24594902992248535
Batch 14/64 loss: -0.3297612965106964
Batch 15/64 loss: -0.25466784834861755
Batch 16/64 loss: -0.3201030492782593
Batch 17/64 loss: -0.3171623945236206
Batch 18/64 loss: -0.27464690804481506
Batch 19/64 loss: -0.3086611032485962
Batch 20/64 loss: -0.3224085569381714
Batch 21/64 loss: -0.24490469694137573
Batch 22/64 loss: -0.29406917095184326
Batch 23/64 loss: -0.30531615018844604
Batch 24/64 loss: -0.2844471335411072
Batch 25/64 loss: -0.31147024035453796
Batch 26/64 loss: -0.32211005687713623
Batch 27/64 loss: -0.32165050506591797
Batch 28/64 loss: -0.28604888916015625
Batch 29/64 loss: -0.329556405544281
Batch 30/64 loss: -0.2763063907623291
Batch 31/64 loss: -0.2848324775695801
Batch 32/64 loss: -0.30100560188293457
Batch 33/64 loss: -0.3060903549194336
Batch 34/64 loss: -0.3271853029727936
Batch 35/64 loss: -0.33909034729003906
Batch 36/64 loss: -0.30860626697540283
Batch 37/64 loss: -0.32472655177116394
Batch 38/64 loss: -0.27278637886047363
Batch 39/64 loss: -0.3421955108642578
Batch 40/64 loss: -0.2927404046058655
Batch 41/64 loss: -0.30488961935043335
Batch 42/64 loss: -0.2819454073905945
Batch 43/64 loss: -0.3079844117164612
Batch 44/64 loss: -0.3050512671470642
Batch 45/64 loss: -0.3249799311161041
Batch 46/64 loss: -0.2749326825141907
Batch 47/64 loss: -0.2633892297744751
Batch 48/64 loss: -0.31719037890434265
Batch 49/64 loss: -0.27349725365638733
Batch 50/64 loss: -0.2825031876564026
Batch 51/64 loss: -0.3196362853050232
Batch 52/64 loss: -0.3098900318145752
Batch 53/64 loss: -0.26765531301498413
Batch 54/64 loss: -0.28203320503234863
Batch 55/64 loss: -0.2942209541797638
Batch 56/64 loss: -0.314761757850647
Batch 57/64 loss: -0.3205512762069702
Batch 58/64 loss: -0.29409560561180115
Batch 59/64 loss: -0.32017308473587036
Batch 60/64 loss: -0.31531596183776855
Batch 61/64 loss: -0.279352605342865
Batch 62/64 loss: -0.2966446876525879
Batch 63/64 loss: -0.29265347123146057
Batch 64/64 loss: -0.30702468752861023
Epoch 282  Train loss: -0.30026251860693387  Val loss: 0.02589726448059082
Epoch 283
-------------------------------
Batch 1/64 loss: -0.2969113886356354
Batch 2/64 loss: -0.3307715654373169
Batch 3/64 loss: -0.304588258266449
Batch 4/64 loss: -0.29483455419540405
Batch 5/64 loss: -0.29757896065711975
Batch 6/64 loss: -0.300173282623291
Batch 7/64 loss: -0.3034933805465698
Batch 8/64 loss: -0.2977602183818817
Batch 9/64 loss: -0.32898056507110596
Batch 10/64 loss: -0.30226004123687744
Batch 11/64 loss: -0.31032848358154297
Batch 12/64 loss: -0.306108295917511
Batch 13/64 loss: -0.32970353960990906
Batch 14/64 loss: -0.306446373462677
Batch 15/64 loss: -0.2752494812011719
Batch 16/64 loss: -0.3127579092979431
Batch 17/64 loss: -0.3223879635334015
Batch 18/64 loss: -0.3258814811706543
Batch 19/64 loss: -0.3372328579425812
Batch 20/64 loss: -0.26318663358688354
Batch 21/64 loss: -0.2916855812072754
Batch 22/64 loss: -0.28464680910110474
Batch 23/64 loss: -0.33527809381484985
Batch 24/64 loss: -0.29670995473861694
Batch 25/64 loss: -0.3055427074432373
Batch 26/64 loss: -0.29912900924682617
Batch 27/64 loss: -0.31259775161743164
Batch 28/64 loss: -0.3274514973163605
Batch 29/64 loss: -0.30035674571990967
Batch 30/64 loss: -0.31879907846450806
Batch 31/64 loss: -0.29514896869659424
Batch 32/64 loss: -0.3153716027736664
Batch 33/64 loss: -0.293676495552063
Batch 34/64 loss: -0.3055970072746277
Batch 35/64 loss: -0.23631525039672852
Batch 36/64 loss: -0.2838018238544464
Batch 37/64 loss: -0.31171783804893494
Batch 38/64 loss: -0.31472840905189514
Batch 39/64 loss: -0.31523463129997253
Batch 40/64 loss: -0.2821137011051178
Batch 41/64 loss: -0.29286932945251465
Batch 42/64 loss: -0.2655026316642761
Batch 43/64 loss: -0.3223472833633423
Batch 44/64 loss: -0.2984190285205841
Batch 45/64 loss: -0.3017524182796478
Batch 46/64 loss: -0.295082151889801
Batch 47/64 loss: -0.33962035179138184
Batch 48/64 loss: -0.32280391454696655
Batch 49/64 loss: -0.3217407464981079
Batch 50/64 loss: -0.3269646167755127
Batch 51/64 loss: -0.3092968463897705
Batch 52/64 loss: -0.32205668091773987
Batch 53/64 loss: -0.22569122910499573
Batch 54/64 loss: -0.2886226177215576
Batch 55/64 loss: -0.3003590404987335
Batch 56/64 loss: -0.2746323347091675
Batch 57/64 loss: -0.2703007161617279
Batch 58/64 loss: -0.31534111499786377
Batch 59/64 loss: -0.29571643471717834
Batch 60/64 loss: -0.30558478832244873
Batch 61/64 loss: -0.31093496084213257
Batch 62/64 loss: -0.30010730028152466
Batch 63/64 loss: -0.2344750165939331
Batch 64/64 loss: -0.24896323680877686
Epoch 283  Train loss: -0.3011683478074915  Val loss: 0.02904375926735475
Epoch 284
-------------------------------
Batch 1/64 loss: -0.33379384875297546
Batch 2/64 loss: -0.30633777379989624
Batch 3/64 loss: -0.24402648210525513
Batch 4/64 loss: -0.23252028226852417
Batch 5/64 loss: -0.3359717130661011
Batch 6/64 loss: -0.30811721086502075
Batch 7/64 loss: -0.3079121708869934
Batch 8/64 loss: -0.2931358218193054
Batch 9/64 loss: -0.274609237909317
Batch 10/64 loss: -0.3286970257759094
Batch 11/64 loss: -0.31064358353614807
Batch 12/64 loss: -0.30128222703933716
Batch 13/64 loss: -0.27398547530174255
Batch 14/64 loss: -0.3224700689315796
Batch 15/64 loss: -0.3267318904399872
Batch 16/64 loss: -0.3232954740524292
Batch 17/64 loss: -0.25949403643608093
Batch 18/64 loss: -0.2860926389694214
Batch 19/64 loss: -0.26829755306243896
Batch 20/64 loss: -0.31177443265914917
Batch 21/64 loss: -0.3004433214664459
Batch 22/64 loss: -0.25664597749710083
Batch 23/64 loss: -0.2854847013950348
Batch 24/64 loss: -0.3174673318862915
Batch 25/64 loss: -0.294760525226593
Batch 26/64 loss: -0.3131833076477051
Batch 27/64 loss: -0.2842252254486084
Batch 28/64 loss: -0.32204973697662354
Batch 29/64 loss: -0.31846120953559875
Batch 30/64 loss: -0.30292364954948425
Batch 31/64 loss: -0.3009163737297058
Batch 32/64 loss: -0.26631098985671997
Batch 33/64 loss: -0.29675984382629395
Batch 34/64 loss: -0.3044131398200989
Batch 35/64 loss: -0.3512691855430603
Batch 36/64 loss: -0.31696629524230957
Batch 37/64 loss: -0.289289265871048
Batch 38/64 loss: -0.2818233370780945
Batch 39/64 loss: -0.31887486577033997
Batch 40/64 loss: -0.27418893575668335
Batch 41/64 loss: -0.30466288328170776
Batch 42/64 loss: -0.3170129954814911
Batch 43/64 loss: -0.3104819059371948
Batch 44/64 loss: -0.2827901840209961
Batch 45/64 loss: -0.32053065299987793
Batch 46/64 loss: -0.3216150999069214
Batch 47/64 loss: -0.32438522577285767
Batch 48/64 loss: -0.3265911340713501
Batch 49/64 loss: -0.3342490792274475
Batch 50/64 loss: -0.3038286566734314
Batch 51/64 loss: -0.30342692136764526
Batch 52/64 loss: -0.2953490614891052
Batch 53/64 loss: -0.28747493028640747
Batch 54/64 loss: -0.3380277454853058
Batch 55/64 loss: -0.33229655027389526
Batch 56/64 loss: -0.34753477573394775
Batch 57/64 loss: -0.2940157651901245
Batch 58/64 loss: -0.3122479319572449
Batch 59/64 loss: -0.3333069384098053
Batch 60/64 loss: -0.2691674828529358
Batch 61/64 loss: -0.2969774007797241
Batch 62/64 loss: -0.3204605281352997
Batch 63/64 loss: -0.3068467974662781
Batch 64/64 loss: -0.3156914710998535
Epoch 284  Train loss: -0.3037756143831739  Val loss: 0.02870893335014684
Epoch 285
-------------------------------
Batch 1/64 loss: -0.3168891370296478
Batch 2/64 loss: -0.35087910294532776
Batch 3/64 loss: -0.2546617090702057
Batch 4/64 loss: -0.33962786197662354
Batch 5/64 loss: -0.3381449580192566
Batch 6/64 loss: -0.27060267329216003
Batch 7/64 loss: -0.3419209420681
Batch 8/64 loss: -0.33610114455223083
Batch 9/64 loss: -0.31787607073783875
Batch 10/64 loss: -0.3328067362308502
Batch 11/64 loss: -0.3366675078868866
Batch 12/64 loss: -0.30242419242858887
Batch 13/64 loss: -0.28939956426620483
Batch 14/64 loss: -0.3362627923488617
Batch 15/64 loss: -0.3240790367126465
Batch 16/64 loss: -0.2813790440559387
Batch 17/64 loss: -0.3256582021713257
Batch 18/64 loss: -0.2809368371963501
Batch 19/64 loss: -0.30771711468696594
Batch 20/64 loss: -0.3087986409664154
Batch 21/64 loss: -0.3251913785934448
Batch 22/64 loss: -0.3184877932071686
Batch 23/64 loss: -0.3158578872680664
Batch 24/64 loss: -0.31673842668533325
Batch 25/64 loss: -0.3381901979446411
Batch 26/64 loss: -0.2744882106781006
Batch 27/64 loss: -0.33222484588623047
Batch 28/64 loss: -0.3327304720878601
Batch 29/64 loss: -0.3142926096916199
Batch 30/64 loss: -0.3163857161998749
Batch 31/64 loss: -0.2660191059112549
Batch 32/64 loss: -0.29047006368637085
Batch 33/64 loss: -0.3335312604904175
Batch 34/64 loss: -0.3338465690612793
Batch 35/64 loss: -0.2926645576953888
Batch 36/64 loss: -0.3329697847366333
Batch 37/64 loss: -0.3302197754383087
Batch 38/64 loss: -0.3028149902820587
Batch 39/64 loss: -0.31522855162620544
Batch 40/64 loss: -0.27551597356796265
Batch 41/64 loss: -0.3088077902793884
Batch 42/64 loss: -0.29280608892440796
Batch 43/64 loss: -0.2926296591758728
Batch 44/64 loss: -0.30790603160858154
Batch 45/64 loss: -0.3098973035812378
Batch 46/64 loss: -0.2723442316055298
Batch 47/64 loss: -0.2892829179763794
Batch 48/64 loss: -0.2938537299633026
Batch 49/64 loss: -0.3054927885532379
Batch 50/64 loss: -0.23034831881523132
Batch 51/64 loss: -0.33798378705978394
Batch 52/64 loss: -0.2946450412273407
Batch 53/64 loss: -0.318744421005249
Batch 54/64 loss: -0.2660522162914276
Batch 55/64 loss: -0.3163544535636902
Batch 56/64 loss: -0.28456777334213257
Batch 57/64 loss: -0.29817795753479004
Batch 58/64 loss: -0.2970152497291565
Batch 59/64 loss: -0.269662082195282
Batch 60/64 loss: -0.3154565095901489
Batch 61/64 loss: -0.2908557653427124
Batch 62/64 loss: -0.28206706047058105
Batch 63/64 loss: -0.2544981837272644
Batch 64/64 loss: -0.2871856689453125
Epoch 285  Train loss: -0.3058123929827821  Val loss: 0.026593742092040806
Epoch 286
-------------------------------
Batch 1/64 loss: -0.29235005378723145
Batch 2/64 loss: -0.31410497426986694
Batch 3/64 loss: -0.31834590435028076
Batch 4/64 loss: -0.32289743423461914
Batch 5/64 loss: -0.3106642961502075
Batch 6/64 loss: -0.2930369973182678
Batch 7/64 loss: -0.3210831880569458
Batch 8/64 loss: -0.2879689335823059
Batch 9/64 loss: -0.31005343794822693
Batch 10/64 loss: -0.31760233640670776
Batch 11/64 loss: -0.30060720443725586
Batch 12/64 loss: -0.3162635564804077
Batch 13/64 loss: -0.2772802412509918
Batch 14/64 loss: -0.32688480615615845
Batch 15/64 loss: -0.33563706278800964
Batch 16/64 loss: -0.3163958787918091
Batch 17/64 loss: -0.32379767298698425
Batch 18/64 loss: -0.3402837812900543
Batch 19/64 loss: -0.2826845049858093
Batch 20/64 loss: -0.3048487603664398
Batch 21/64 loss: -0.3189944326877594
Batch 22/64 loss: -0.310416579246521
Batch 23/64 loss: -0.2964473068714142
Batch 24/64 loss: -0.32969826459884644
Batch 25/64 loss: -0.3237816393375397
Batch 26/64 loss: -0.2768435478210449
Batch 27/64 loss: -0.2807396352291107
Batch 28/64 loss: -0.30851098895072937
Batch 29/64 loss: -0.29412367939949036
Batch 30/64 loss: -0.285204142332077
Batch 31/64 loss: -0.3059631586074829
Batch 32/64 loss: -0.2491675615310669
Batch 33/64 loss: -0.30108240246772766
Batch 34/64 loss: -0.3182438015937805
Batch 35/64 loss: -0.3372175097465515
Batch 36/64 loss: -0.330835223197937
Batch 37/64 loss: -0.2752803564071655
Batch 38/64 loss: -0.30724290013313293
Batch 39/64 loss: -0.2692701518535614
Batch 40/64 loss: -0.3047243356704712
Batch 41/64 loss: -0.3113115429878235
Batch 42/64 loss: -0.3060697317123413
Batch 43/64 loss: -0.3360499143600464
Batch 44/64 loss: -0.310418039560318
Batch 45/64 loss: -0.2779634892940521
Batch 46/64 loss: -0.31859809160232544
Batch 47/64 loss: -0.30031687021255493
Batch 48/64 loss: -0.28520286083221436
Batch 49/64 loss: -0.3150082230567932
Batch 50/64 loss: -0.274372935295105
Batch 51/64 loss: -0.27142325043678284
Batch 52/64 loss: -0.28601521253585815
Batch 53/64 loss: -0.31042250990867615
Batch 54/64 loss: -0.3065531253814697
Batch 55/64 loss: -0.27986618876457214
Batch 56/64 loss: -0.3101675510406494
Batch 57/64 loss: -0.3080921173095703
Batch 58/64 loss: -0.25051143765449524
Batch 59/64 loss: -0.3157755136489868
Batch 60/64 loss: -0.30558037757873535
Batch 61/64 loss: -0.2928343713283539
Batch 62/64 loss: -0.30910390615463257
Batch 63/64 loss: -0.2899743318557739
Batch 64/64 loss: -0.20369595289230347
Epoch 286  Train loss: -0.30213305412554275  Val loss: 0.028588511894658667
Epoch 287
-------------------------------
Batch 1/64 loss: -0.282457560300827
Batch 2/64 loss: -0.28177106380462646
Batch 3/64 loss: -0.32324501872062683
Batch 4/64 loss: -0.30020034313201904
Batch 5/64 loss: -0.3186689615249634
Batch 6/64 loss: -0.29374992847442627
Batch 7/64 loss: -0.3197970688343048
Batch 8/64 loss: -0.3021497130393982
Batch 9/64 loss: -0.28471651673316956
Batch 10/64 loss: -0.2830260396003723
Batch 11/64 loss: -0.30420660972595215
Batch 12/64 loss: -0.2922086715698242
Batch 13/64 loss: -0.3272927403450012
Batch 14/64 loss: -0.3062439262866974
Batch 15/64 loss: -0.28340622782707214
Batch 16/64 loss: -0.3167431354522705
Batch 17/64 loss: -0.28603774309158325
Batch 18/64 loss: -0.30855682492256165
Batch 19/64 loss: -0.3231640160083771
Batch 20/64 loss: -0.3211614489555359
Batch 21/64 loss: -0.3000277876853943
Batch 22/64 loss: -0.2781216502189636
Batch 23/64 loss: -0.3205065429210663
Batch 24/64 loss: -0.2593429386615753
Batch 25/64 loss: -0.3269249200820923
Batch 26/64 loss: -0.3030235469341278
Batch 27/64 loss: -0.29439881443977356
Batch 28/64 loss: -0.3195343613624573
Batch 29/64 loss: -0.34353938698768616
Batch 30/64 loss: -0.3144129514694214
Batch 31/64 loss: -0.3050514757633209
Batch 32/64 loss: -0.27118468284606934
Batch 33/64 loss: -0.3002117872238159
Batch 34/64 loss: -0.3205196261405945
Batch 35/64 loss: -0.3384735584259033
Batch 36/64 loss: -0.31930339336395264
Batch 37/64 loss: -0.3495391607284546
Batch 38/64 loss: -0.34224069118499756
Batch 39/64 loss: -0.30966490507125854
Batch 40/64 loss: -0.3154960870742798
Batch 41/64 loss: -0.29771724343299866
Batch 42/64 loss: -0.3098675608634949
Batch 43/64 loss: -0.32251179218292236
Batch 44/64 loss: -0.33604195713996887
Batch 45/64 loss: -0.3091104030609131
Batch 46/64 loss: -0.26408109068870544
Batch 47/64 loss: -0.3079042434692383
Batch 48/64 loss: -0.31895875930786133
Batch 49/64 loss: -0.31088289618492126
Batch 50/64 loss: -0.27721405029296875
Batch 51/64 loss: -0.3049217164516449
Batch 52/64 loss: -0.3476020395755768
Batch 53/64 loss: -0.3189607262611389
Batch 54/64 loss: -0.30721569061279297
Batch 55/64 loss: -0.34152621030807495
Batch 56/64 loss: -0.3406994342803955
Batch 57/64 loss: -0.298136830329895
Batch 58/64 loss: -0.28602349758148193
Batch 59/64 loss: -0.2950420379638672
Batch 60/64 loss: -0.3009531497955322
Batch 61/64 loss: -0.32213157415390015
Batch 62/64 loss: -0.307729035615921
Batch 63/64 loss: -0.29972535371780396
Batch 64/64 loss: -0.33126068115234375
Epoch 287  Train loss: -0.30845058244817397  Val loss: 0.026638440864602316
Epoch 288
-------------------------------
Batch 1/64 loss: -0.306682825088501
Batch 2/64 loss: -0.3058646619319916
Batch 3/64 loss: -0.3235369026660919
Batch 4/64 loss: -0.28789424896240234
Batch 5/64 loss: -0.2899182438850403
Batch 6/64 loss: -0.2798023819923401
Batch 7/64 loss: -0.26619139313697815
Batch 8/64 loss: -0.33257633447647095
Batch 9/64 loss: -0.33215370774269104
Batch 10/64 loss: -0.30707430839538574
Batch 11/64 loss: -0.3508591949939728
Batch 12/64 loss: -0.30372166633605957
Batch 13/64 loss: -0.353634774684906
Batch 14/64 loss: -0.30898892879486084
Batch 15/64 loss: -0.2680931091308594
Batch 16/64 loss: -0.29508620500564575
Batch 17/64 loss: -0.30061304569244385
Batch 18/64 loss: -0.32134002447128296
Batch 19/64 loss: -0.29747897386550903
Batch 20/64 loss: -0.2886582016944885
Batch 21/64 loss: -0.3201412558555603
Batch 22/64 loss: -0.3175560235977173
Batch 23/64 loss: -0.3348109722137451
Batch 24/64 loss: -0.30695900321006775
Batch 25/64 loss: -0.29095882177352905
Batch 26/64 loss: -0.2984457015991211
Batch 27/64 loss: -0.311355859041214
Batch 28/64 loss: -0.3148602843284607
Batch 29/64 loss: -0.3261757791042328
Batch 30/64 loss: -0.2970949113368988
Batch 31/64 loss: -0.3075997233390808
Batch 32/64 loss: -0.31078243255615234
Batch 33/64 loss: -0.30191150307655334
Batch 34/64 loss: -0.26965197920799255
Batch 35/64 loss: -0.2846320867538452
Batch 36/64 loss: -0.258667916059494
Batch 37/64 loss: -0.2900104522705078
Batch 38/64 loss: -0.26063573360443115
Batch 39/64 loss: -0.3005239963531494
Batch 40/64 loss: -0.32340365648269653
Batch 41/64 loss: -0.2964731454849243
Batch 42/64 loss: -0.29664507508277893
Batch 43/64 loss: -0.3094353675842285
Batch 44/64 loss: -0.29405999183654785
Batch 45/64 loss: -0.32803618907928467
Batch 46/64 loss: -0.31607550382614136
Batch 47/64 loss: -0.2738771438598633
Batch 48/64 loss: -0.3056032955646515
Batch 49/64 loss: -0.25533825159072876
Batch 50/64 loss: -0.3270200788974762
Batch 51/64 loss: -0.2774813771247864
Batch 52/64 loss: -0.31803712248802185
Batch 53/64 loss: -0.3152509331703186
Batch 54/64 loss: -0.30501800775527954
Batch 55/64 loss: -0.3122492730617523
Batch 56/64 loss: -0.31897956132888794
Batch 57/64 loss: -0.2843109667301178
Batch 58/64 loss: -0.2948523163795471
Batch 59/64 loss: -0.3129192590713501
Batch 60/64 loss: -0.26466524600982666
Batch 61/64 loss: -0.30191314220428467
Batch 62/64 loss: -0.3361400365829468
Batch 63/64 loss: -0.3156784176826477
Batch 64/64 loss: -0.30353420972824097
Epoch 288  Train loss: -0.3032793346573325  Val loss: 0.02668333360829304
Epoch 289
-------------------------------
Batch 1/64 loss: -0.29667574167251587
Batch 2/64 loss: -0.3196321427822113
Batch 3/64 loss: -0.27697184681892395
Batch 4/64 loss: -0.3149878978729248
Batch 5/64 loss: -0.32304638624191284
Batch 6/64 loss: -0.3090435266494751
Batch 7/64 loss: -0.29565665125846863
Batch 8/64 loss: -0.2879694998264313
Batch 9/64 loss: -0.2834794819355011
Batch 10/64 loss: -0.3250609040260315
Batch 11/64 loss: -0.3039515018463135
Batch 12/64 loss: -0.3103439211845398
Batch 13/64 loss: -0.31046897172927856
Batch 14/64 loss: -0.30892235040664673
Batch 15/64 loss: -0.30321750044822693
Batch 16/64 loss: -0.3206104040145874
Batch 17/64 loss: -0.31028836965560913
Batch 18/64 loss: -0.30945760011672974
Batch 19/64 loss: -0.3390612304210663
Batch 20/64 loss: -0.26729267835617065
Batch 21/64 loss: -0.3191809356212616
Batch 22/64 loss: -0.30872079730033875
Batch 23/64 loss: -0.3270121216773987
Batch 24/64 loss: -0.31374144554138184
Batch 25/64 loss: -0.3064127564430237
Batch 26/64 loss: -0.300190806388855
Batch 27/64 loss: -0.291838139295578
Batch 28/64 loss: -0.31644976139068604
Batch 29/64 loss: -0.31469449400901794
Batch 30/64 loss: -0.2999430000782013
Batch 31/64 loss: -0.3385089635848999
Batch 32/64 loss: -0.32148775458335876
Batch 33/64 loss: -0.3197636604309082
Batch 34/64 loss: -0.3174944519996643
Batch 35/64 loss: -0.3456670641899109
Batch 36/64 loss: -0.3077831566333771
Batch 37/64 loss: -0.28337985277175903
Batch 38/64 loss: -0.2691560983657837
Batch 39/64 loss: -0.3388141095638275
Batch 40/64 loss: -0.3314049243927002
Batch 41/64 loss: -0.32150259613990784
Batch 42/64 loss: -0.29537081718444824
Batch 43/64 loss: -0.3074631094932556
Batch 44/64 loss: -0.2868097424507141
Batch 45/64 loss: -0.28771770000457764
Batch 46/64 loss: -0.33376699686050415
Batch 47/64 loss: -0.34022802114486694
Batch 48/64 loss: -0.3193461298942566
Batch 49/64 loss: -0.29141950607299805
Batch 50/64 loss: -0.314809650182724
Batch 51/64 loss: -0.25191718339920044
Batch 52/64 loss: -0.3199704885482788
Batch 53/64 loss: -0.3107960522174835
Batch 54/64 loss: -0.29274895787239075
Batch 55/64 loss: -0.29827719926834106
Batch 56/64 loss: -0.33531782031059265
Batch 57/64 loss: -0.31750476360321045
Batch 58/64 loss: -0.2785220146179199
Batch 59/64 loss: -0.33827438950538635
Batch 60/64 loss: -0.3267480134963989
Batch 61/64 loss: -0.2853941321372986
Batch 62/64 loss: -0.32657885551452637
Batch 63/64 loss: -0.3221534192562103
Batch 64/64 loss: -0.29491743445396423
Epoch 289  Train loss: -0.3092017025339837  Val loss: 0.027699240704172665
Epoch 290
-------------------------------
Batch 1/64 loss: -0.33745458722114563
Batch 2/64 loss: -0.3031407296657562
Batch 3/64 loss: -0.26008865237236023
Batch 4/64 loss: -0.35061484575271606
Batch 5/64 loss: -0.2806101441383362
Batch 6/64 loss: -0.2989647388458252
Batch 7/64 loss: -0.3483436703681946
Batch 8/64 loss: -0.3319658041000366
Batch 9/64 loss: -0.3304075598716736
Batch 10/64 loss: -0.3220944404602051
Batch 11/64 loss: -0.3182299733161926
Batch 12/64 loss: -0.3172229528427124
Batch 13/64 loss: -0.33033573627471924
Batch 14/64 loss: -0.2903691530227661
Batch 15/64 loss: -0.3189716339111328
Batch 16/64 loss: -0.23204326629638672
Batch 17/64 loss: -0.3195817172527313
Batch 18/64 loss: -0.2884448766708374
Batch 19/64 loss: -0.32340800762176514
Batch 20/64 loss: -0.302979052066803
Batch 21/64 loss: -0.3384784460067749
Batch 22/64 loss: -0.3149368166923523
Batch 23/64 loss: -0.32155507802963257
Batch 24/64 loss: -0.25417858362197876
Batch 25/64 loss: -0.30566656589508057
Batch 26/64 loss: -0.27784404158592224
Batch 27/64 loss: -0.3278694152832031
Batch 28/64 loss: -0.28111010789871216
Batch 29/64 loss: -0.33003661036491394
Batch 30/64 loss: -0.2991790771484375
Batch 31/64 loss: -0.3116037845611572
Batch 32/64 loss: -0.30034804344177246
Batch 33/64 loss: -0.2658196687698364
Batch 34/64 loss: -0.31097739934921265
Batch 35/64 loss: -0.2989294230937958
Batch 36/64 loss: -0.3146434426307678
Batch 37/64 loss: -0.34055182337760925
Batch 38/64 loss: -0.30672693252563477
Batch 39/64 loss: -0.3001388609409332
Batch 40/64 loss: -0.32227659225463867
Batch 41/64 loss: -0.3060629963874817
Batch 42/64 loss: -0.2918993830680847
Batch 43/64 loss: -0.3218947947025299
Batch 44/64 loss: -0.29954099655151367
Batch 45/64 loss: -0.30632758140563965
Batch 46/64 loss: -0.3251262307167053
Batch 47/64 loss: -0.30728238821029663
Batch 48/64 loss: -0.30356454849243164
Batch 49/64 loss: -0.294952392578125
Batch 50/64 loss: -0.3027868866920471
Batch 51/64 loss: -0.2962735891342163
Batch 52/64 loss: -0.3387582302093506
Batch 53/64 loss: -0.3039773106575012
Batch 54/64 loss: -0.30532971024513245
Batch 55/64 loss: -0.34117990732192993
Batch 56/64 loss: -0.32493531703948975
Batch 57/64 loss: -0.26150184869766235
Batch 58/64 loss: -0.3116374909877777
Batch 59/64 loss: -0.29507505893707275
Batch 60/64 loss: -0.30490219593048096
Batch 61/64 loss: -0.3280773460865021
Batch 62/64 loss: -0.3417038023471832
Batch 63/64 loss: -0.3107337951660156
Batch 64/64 loss: -0.31669753789901733
Epoch 290  Train loss: -0.3088500268319074  Val loss: 0.025633401887113695
Epoch 291
-------------------------------
Batch 1/64 loss: -0.3200715482234955
Batch 2/64 loss: -0.3213445544242859
Batch 3/64 loss: -0.32410386204719543
Batch 4/64 loss: -0.30328369140625
Batch 5/64 loss: -0.346029132604599
Batch 6/64 loss: -0.3399902582168579
Batch 7/64 loss: -0.30296286940574646
Batch 8/64 loss: -0.3129844665527344
Batch 9/64 loss: -0.3502148985862732
Batch 10/64 loss: -0.2844003438949585
Batch 11/64 loss: -0.2989963889122009
Batch 12/64 loss: -0.3236188292503357
Batch 13/64 loss: -0.3214963674545288
Batch 14/64 loss: -0.3199722170829773
Batch 15/64 loss: -0.3242295980453491
Batch 16/64 loss: -0.31005537509918213
Batch 17/64 loss: -0.2986674904823303
Batch 18/64 loss: -0.3315950632095337
Batch 19/64 loss: -0.3272843062877655
Batch 20/64 loss: -0.3325919210910797
Batch 21/64 loss: -0.33952510356903076
Batch 22/64 loss: -0.3153250217437744
Batch 23/64 loss: -0.29745227098464966
Batch 24/64 loss: -0.3180857300758362
Batch 25/64 loss: -0.2786611318588257
Batch 26/64 loss: -0.2845671474933624
Batch 27/64 loss: -0.2930341362953186
Batch 28/64 loss: -0.3168759346008301
Batch 29/64 loss: -0.29172930121421814
Batch 30/64 loss: -0.3121791481971741
Batch 31/64 loss: -0.2848849296569824
Batch 32/64 loss: -0.30129507184028625
Batch 33/64 loss: -0.29582321643829346
Batch 34/64 loss: -0.3211026191711426
Batch 35/64 loss: -0.31823602318763733
Batch 36/64 loss: -0.2940046489238739
Batch 37/64 loss: -0.3139066994190216
Batch 38/64 loss: -0.30495473742485046
Batch 39/64 loss: -0.3065723776817322
Batch 40/64 loss: -0.29594045877456665
Batch 41/64 loss: -0.31152772903442383
Batch 42/64 loss: -0.32555776834487915
Batch 43/64 loss: -0.30625420808792114
Batch 44/64 loss: -0.34039777517318726
Batch 45/64 loss: -0.30345088243484497
Batch 46/64 loss: -0.30703824758529663
Batch 47/64 loss: -0.31486058235168457
Batch 48/64 loss: -0.2788633704185486
Batch 49/64 loss: -0.3013831377029419
Batch 50/64 loss: -0.2901160717010498
Batch 51/64 loss: -0.30621111392974854
Batch 52/64 loss: -0.2789902091026306
Batch 53/64 loss: -0.30678412318229675
Batch 54/64 loss: -0.32386261224746704
Batch 55/64 loss: -0.321980357170105
Batch 56/64 loss: -0.33483946323394775
Batch 57/64 loss: -0.32382461428642273
Batch 58/64 loss: -0.32590344548225403
Batch 59/64 loss: -0.2956588864326477
Batch 60/64 loss: -0.3370482921600342
Batch 61/64 loss: -0.3099784851074219
Batch 62/64 loss: -0.30905836820602417
Batch 63/64 loss: -0.29183483123779297
Batch 64/64 loss: -0.2666230797767639
Epoch 291  Train loss: -0.3109559337298075  Val loss: 0.02787095943267403
Epoch 292
-------------------------------
Batch 1/64 loss: -0.29639407992362976
Batch 2/64 loss: -0.3101077675819397
Batch 3/64 loss: -0.3235621452331543
Batch 4/64 loss: -0.31141042709350586
Batch 5/64 loss: -0.3260596990585327
Batch 6/64 loss: -0.3178789019584656
Batch 7/64 loss: -0.29166775941848755
Batch 8/64 loss: -0.28761523962020874
Batch 9/64 loss: -0.31854933500289917
Batch 10/64 loss: -0.3190552890300751
Batch 11/64 loss: -0.31412482261657715
Batch 12/64 loss: -0.3152589499950409
Batch 13/64 loss: -0.32513266801834106
Batch 14/64 loss: -0.3067317008972168
Batch 15/64 loss: -0.30971091985702515
Batch 16/64 loss: -0.31994348764419556
Batch 17/64 loss: -0.33721011877059937
Batch 18/64 loss: -0.3073464632034302
Batch 19/64 loss: -0.3240947127342224
Batch 20/64 loss: -0.34491997957229614
Batch 21/64 loss: -0.3256043493747711
Batch 22/64 loss: -0.3012796938419342
Batch 23/64 loss: -0.33457687497138977
Batch 24/64 loss: -0.31442874670028687
Batch 25/64 loss: -0.32779210805892944
Batch 26/64 loss: -0.30046921968460083
Batch 27/64 loss: -0.31312114000320435
Batch 28/64 loss: -0.33194172382354736
Batch 29/64 loss: -0.3487321138381958
Batch 30/64 loss: -0.24771466851234436
Batch 31/64 loss: -0.2571975588798523
Batch 32/64 loss: -0.3019343614578247
Batch 33/64 loss: -0.272166907787323
Batch 34/64 loss: -0.34506702423095703
Batch 35/64 loss: -0.3158525824546814
Batch 36/64 loss: -0.3434322476387024
Batch 37/64 loss: -0.3195529878139496
Batch 38/64 loss: -0.27059993147850037
Batch 39/64 loss: -0.32454413175582886
Batch 40/64 loss: -0.28728151321411133
Batch 41/64 loss: -0.30407246947288513
Batch 42/64 loss: -0.32439714670181274
Batch 43/64 loss: -0.3174467086791992
Batch 44/64 loss: -0.3283776342868805
Batch 45/64 loss: -0.2973693907260895
Batch 46/64 loss: -0.3024753928184509
Batch 47/64 loss: -0.3246455490589142
Batch 48/64 loss: -0.3256193995475769
Batch 49/64 loss: -0.30346226692199707
Batch 50/64 loss: -0.35186606645584106
Batch 51/64 loss: -0.3176185190677643
Batch 52/64 loss: -0.33231741189956665
Batch 53/64 loss: -0.30361273884773254
Batch 54/64 loss: -0.334030419588089
Batch 55/64 loss: -0.2739992141723633
Batch 56/64 loss: -0.32314401865005493
Batch 57/64 loss: -0.3326786160469055
Batch 58/64 loss: -0.26067042350769043
Batch 59/64 loss: -0.30964627861976624
Batch 60/64 loss: -0.2848946452140808
Batch 61/64 loss: -0.3053952157497406
Batch 62/64 loss: -0.32541799545288086
Batch 63/64 loss: -0.31914085149765015
Batch 64/64 loss: -0.3092185854911804
Epoch 292  Train loss: -0.3125376731741662  Val loss: 0.027451045119885317
Epoch 293
-------------------------------
Batch 1/64 loss: -0.31149157881736755
Batch 2/64 loss: -0.2769061326980591
Batch 3/64 loss: -0.30918073654174805
Batch 4/64 loss: -0.310940682888031
Batch 5/64 loss: -0.31841254234313965
Batch 6/64 loss: -0.32456034421920776
Batch 7/64 loss: -0.31618642807006836
Batch 8/64 loss: -0.3210563659667969
Batch 9/64 loss: -0.3042234182357788
Batch 10/64 loss: -0.30181047320365906
Batch 11/64 loss: -0.32131054997444153
Batch 12/64 loss: -0.32532310485839844
Batch 13/64 loss: -0.3015217185020447
Batch 14/64 loss: -0.3030458986759186
Batch 15/64 loss: -0.34287115931510925
Batch 16/64 loss: -0.327312171459198
Batch 17/64 loss: -0.3234650492668152
Batch 18/64 loss: -0.31168675422668457
Batch 19/64 loss: -0.2816937565803528
Batch 20/64 loss: -0.3296671509742737
Batch 21/64 loss: -0.2894960641860962
Batch 22/64 loss: -0.2729763388633728
Batch 23/64 loss: -0.29665398597717285
Batch 24/64 loss: -0.29355379939079285
Batch 25/64 loss: -0.3088010549545288
Batch 26/64 loss: -0.3370932936668396
Batch 27/64 loss: -0.29888075590133667
Batch 28/64 loss: -0.32587936520576477
Batch 29/64 loss: -0.2969706654548645
Batch 30/64 loss: -0.3231273889541626
Batch 31/64 loss: -0.3366600573062897
Batch 32/64 loss: -0.32606643438339233
Batch 33/64 loss: -0.31003671884536743
Batch 34/64 loss: -0.32845547795295715
Batch 35/64 loss: -0.28151142597198486
Batch 36/64 loss: -0.3121861219406128
Batch 37/64 loss: -0.3315102458000183
Batch 38/64 loss: -0.3060888946056366
Batch 39/64 loss: -0.2971876859664917
Batch 40/64 loss: -0.3032454550266266
Batch 41/64 loss: -0.29661980271339417
Batch 42/64 loss: -0.30113351345062256
Batch 43/64 loss: -0.30999159812927246
Batch 44/64 loss: -0.2806154787540436
Batch 45/64 loss: -0.3126683533191681
Batch 46/64 loss: -0.334557443857193
Batch 47/64 loss: -0.30916309356689453
Batch 48/64 loss: -0.3044522702693939
Batch 49/64 loss: -0.2929976284503937
Batch 50/64 loss: -0.3290541470050812
Batch 51/64 loss: -0.27672314643859863
Batch 52/64 loss: -0.28194624185562134
Batch 53/64 loss: -0.31792405247688293
Batch 54/64 loss: -0.3038991689682007
Batch 55/64 loss: -0.27501511573791504
Batch 56/64 loss: -0.2853422462940216
Batch 57/64 loss: -0.3166080713272095
Batch 58/64 loss: -0.3203113079071045
Batch 59/64 loss: -0.294575959444046
Batch 60/64 loss: -0.31858915090560913
Batch 61/64 loss: -0.28812679648399353
Batch 62/64 loss: -0.28259870409965515
Batch 63/64 loss: -0.30399757623672485
Batch 64/64 loss: -0.28965145349502563
Epoch 293  Train loss: -0.3073442620389602  Val loss: 0.02920072103284069
Epoch 294
-------------------------------
Batch 1/64 loss: -0.3193090260028839
Batch 2/64 loss: -0.31780481338500977
Batch 3/64 loss: -0.32126495242118835
Batch 4/64 loss: -0.30133575201034546
Batch 5/64 loss: -0.28378307819366455
Batch 6/64 loss: -0.3123694658279419
Batch 7/64 loss: -0.2949824035167694
Batch 8/64 loss: -0.3098398447036743
Batch 9/64 loss: -0.24137216806411743
Batch 10/64 loss: -0.3413667678833008
Batch 11/64 loss: -0.31480228900909424
Batch 12/64 loss: -0.2994682490825653
Batch 13/64 loss: -0.3311716616153717
Batch 14/64 loss: -0.2570551633834839
Batch 15/64 loss: -0.32158172130584717
Batch 16/64 loss: -0.3223689794540405
Batch 17/64 loss: -0.3102441132068634
Batch 18/64 loss: -0.32852455973625183
Batch 19/64 loss: -0.34675687551498413
Batch 20/64 loss: -0.29813456535339355
Batch 21/64 loss: -0.2788581848144531
Batch 22/64 loss: -0.30648019909858704
Batch 23/64 loss: -0.308982789516449
Batch 24/64 loss: -0.3037188947200775
Batch 25/64 loss: -0.307214617729187
Batch 26/64 loss: -0.2967025339603424
Batch 27/64 loss: -0.264513224363327
Batch 28/64 loss: -0.26718276739120483
Batch 29/64 loss: -0.3194029927253723
Batch 30/64 loss: -0.32349371910095215
Batch 31/64 loss: -0.31421715021133423
Batch 32/64 loss: -0.3025785982608795
Batch 33/64 loss: -0.31538712978363037
Batch 34/64 loss: -0.3033052682876587
Batch 35/64 loss: -0.30440080165863037
Batch 36/64 loss: -0.3158681392669678
Batch 37/64 loss: -0.29756805300712585
Batch 38/64 loss: -0.30927032232284546
Batch 39/64 loss: -0.3399030864238739
Batch 40/64 loss: -0.29680609703063965
Batch 41/64 loss: -0.32814472913742065
Batch 42/64 loss: -0.30241695046424866
Batch 43/64 loss: -0.29040300846099854
Batch 44/64 loss: -0.3269031345844269
Batch 45/64 loss: -0.3055860102176666
Batch 46/64 loss: -0.3075058162212372
Batch 47/64 loss: -0.28287017345428467
Batch 48/64 loss: -0.30301374197006226
Batch 49/64 loss: -0.3107384443283081
Batch 50/64 loss: -0.29994943737983704
Batch 51/64 loss: -0.31803908944129944
Batch 52/64 loss: -0.32316553592681885
Batch 53/64 loss: -0.2611161768436432
Batch 54/64 loss: -0.3114843964576721
Batch 55/64 loss: -0.2906828224658966
Batch 56/64 loss: -0.33474647998809814
Batch 57/64 loss: -0.34665223956108093
Batch 58/64 loss: -0.31861355900764465
Batch 59/64 loss: -0.29553869366645813
Batch 60/64 loss: -0.3414344787597656
Batch 61/64 loss: -0.34428220987319946
Batch 62/64 loss: -0.29922303557395935
Batch 63/64 loss: -0.3106895685195923
Batch 64/64 loss: -0.317695677280426
Epoch 294  Train loss: -0.30809196093503166  Val loss: 0.030153237462453415
Epoch 295
-------------------------------
Batch 1/64 loss: -0.3177185654640198
Batch 2/64 loss: -0.3326836824417114
Batch 3/64 loss: -0.2914022207260132
Batch 4/64 loss: -0.32981055974960327
Batch 5/64 loss: -0.33519038558006287
Batch 6/64 loss: -0.3399527072906494
Batch 7/64 loss: -0.3198202848434448
Batch 8/64 loss: -0.3104422688484192
Batch 9/64 loss: -0.3470967710018158
Batch 10/64 loss: -0.3232758641242981
Batch 11/64 loss: -0.2716287076473236
Batch 12/64 loss: -0.3236502408981323
Batch 13/64 loss: -0.3290756344795227
Batch 14/64 loss: -0.33418455719947815
Batch 15/64 loss: -0.34673047065734863
Batch 16/64 loss: -0.2914416790008545
Batch 17/64 loss: -0.31104400753974915
Batch 18/64 loss: -0.3456728458404541
Batch 19/64 loss: -0.3270043134689331
Batch 20/64 loss: -0.3375011086463928
Batch 21/64 loss: -0.32719630002975464
Batch 22/64 loss: -0.29458656907081604
Batch 23/64 loss: -0.3360303044319153
Batch 24/64 loss: -0.33962616324424744
Batch 25/64 loss: -0.31218376755714417
Batch 26/64 loss: -0.32533353567123413
Batch 27/64 loss: -0.33553799986839294
Batch 28/64 loss: -0.33954474329948425
Batch 29/64 loss: -0.30580219626426697
Batch 30/64 loss: -0.33850541710853577
Batch 31/64 loss: -0.33928465843200684
Batch 32/64 loss: -0.2715441584587097
Batch 33/64 loss: -0.2847638726234436
Batch 34/64 loss: -0.3102458417415619
Batch 35/64 loss: -0.33179014921188354
Batch 36/64 loss: -0.31677165627479553
Batch 37/64 loss: -0.3273066282272339
Batch 38/64 loss: -0.33379843831062317
Batch 39/64 loss: -0.2924320101737976
Batch 40/64 loss: -0.27900126576423645
Batch 41/64 loss: -0.29014790058135986
Batch 42/64 loss: -0.2741008996963501
Batch 43/64 loss: -0.32676491141319275
Batch 44/64 loss: -0.3307725787162781
Batch 45/64 loss: -0.3035552501678467
Batch 46/64 loss: -0.3126527667045593
Batch 47/64 loss: -0.2930416464805603
Batch 48/64 loss: -0.3146209716796875
Batch 49/64 loss: -0.28463008999824524
Batch 50/64 loss: -0.2985929846763611
Batch 51/64 loss: -0.29139411449432373
Batch 52/64 loss: -0.2810392379760742
Batch 53/64 loss: -0.31374189257621765
Batch 54/64 loss: -0.3135385513305664
Batch 55/64 loss: -0.3034581243991852
Batch 56/64 loss: -0.2921050786972046
Batch 57/64 loss: -0.325521320104599
Batch 58/64 loss: -0.3126276731491089
Batch 59/64 loss: -0.27282851934432983
Batch 60/64 loss: -0.28867197036743164
Batch 61/64 loss: -0.3221667408943176
Batch 62/64 loss: -0.3130772113800049
Batch 63/64 loss: -0.3049277365207672
Batch 64/64 loss: -0.33802473545074463
Epoch 295  Train loss: -0.31410367488861085  Val loss: 0.0270961797524154
Epoch 296
-------------------------------
Batch 1/64 loss: -0.33031564950942993
Batch 2/64 loss: -0.28201693296432495
Batch 3/64 loss: -0.31330716609954834
Batch 4/64 loss: -0.35167431831359863
Batch 5/64 loss: -0.2902296781539917
Batch 6/64 loss: -0.3508673310279846
Batch 7/64 loss: -0.31054818630218506
Batch 8/64 loss: -0.30913957953453064
Batch 9/64 loss: -0.32681047916412354
Batch 10/64 loss: -0.28667181730270386
Batch 11/64 loss: -0.31242677569389343
Batch 12/64 loss: -0.32391414046287537
Batch 13/64 loss: -0.2997090220451355
Batch 14/64 loss: -0.3474484086036682
Batch 15/64 loss: -0.31863391399383545
Batch 16/64 loss: -0.33236050605773926
Batch 17/64 loss: -0.3227226138114929
Batch 18/64 loss: -0.333067387342453
Batch 19/64 loss: -0.33712321519851685
Batch 20/64 loss: -0.31253379583358765
Batch 21/64 loss: -0.3424638509750366
Batch 22/64 loss: -0.3007729947566986
Batch 23/64 loss: -0.26008984446525574
Batch 24/64 loss: -0.3207532465457916
Batch 25/64 loss: -0.29623234272003174
Batch 26/64 loss: -0.3110533058643341
Batch 27/64 loss: -0.31463027000427246
Batch 28/64 loss: -0.306404173374176
Batch 29/64 loss: -0.2794395685195923
Batch 30/64 loss: -0.32000264525413513
Batch 31/64 loss: -0.24756181240081787
Batch 32/64 loss: -0.2832132577896118
Batch 33/64 loss: -0.2914084792137146
Batch 34/64 loss: -0.3230821490287781
Batch 35/64 loss: -0.3188828229904175
Batch 36/64 loss: -0.30037516355514526
Batch 37/64 loss: -0.2361256182193756
Batch 38/64 loss: -0.32071444392204285
Batch 39/64 loss: -0.3322993516921997
Batch 40/64 loss: -0.304290771484375
Batch 41/64 loss: -0.32751986384391785
Batch 42/64 loss: -0.2571166753768921
Batch 43/64 loss: -0.3276381492614746
Batch 44/64 loss: -0.33637169003486633
Batch 45/64 loss: -0.3160083293914795
Batch 46/64 loss: -0.30513617396354675
Batch 47/64 loss: -0.2716672420501709
Batch 48/64 loss: -0.3060356378555298
Batch 49/64 loss: -0.33632099628448486
Batch 50/64 loss: -0.32224440574645996
Batch 51/64 loss: -0.29996681213378906
Batch 52/64 loss: -0.33356839418411255
Batch 53/64 loss: -0.30386653542518616
Batch 54/64 loss: -0.3105902075767517
Batch 55/64 loss: -0.3027353286743164
Batch 56/64 loss: -0.31439438462257385
Batch 57/64 loss: -0.2852068543434143
Batch 58/64 loss: -0.2800005376338959
Batch 59/64 loss: -0.25841230154037476
Batch 60/64 loss: -0.3129357099533081
Batch 61/64 loss: -0.3103938698768616
Batch 62/64 loss: -0.3317403197288513
Batch 63/64 loss: -0.2768605053424835
Batch 64/64 loss: -0.2859421372413635
Epoch 296  Train loss: -0.308117718556348  Val loss: 0.027571487262896245
Epoch 297
-------------------------------
Batch 1/64 loss: -0.3306885361671448
Batch 2/64 loss: -0.3127819895744324
Batch 3/64 loss: -0.28789347410202026
Batch 4/64 loss: -0.2987598180770874
Batch 5/64 loss: -0.31761395931243896
Batch 6/64 loss: -0.29365110397338867
Batch 7/64 loss: -0.3258735239505768
Batch 8/64 loss: -0.3075207769870758
Batch 9/64 loss: -0.31262606382369995
Batch 10/64 loss: -0.32893243432044983
Batch 11/64 loss: -0.3136303424835205
Batch 12/64 loss: -0.29007458686828613
Batch 13/64 loss: -0.2923499345779419
Batch 14/64 loss: -0.3553212881088257
Batch 15/64 loss: -0.30792033672332764
Batch 16/64 loss: -0.26888078451156616
Batch 17/64 loss: -0.3351479768753052
Batch 18/64 loss: -0.3236323893070221
Batch 19/64 loss: -0.31114867329597473
Batch 20/64 loss: -0.29129987955093384
Batch 21/64 loss: -0.3242883086204529
Batch 22/64 loss: -0.28390032052993774
Batch 23/64 loss: -0.2949199080467224
Batch 24/64 loss: -0.2955184578895569
Batch 25/64 loss: -0.3097558617591858
Batch 26/64 loss: -0.3585858643054962
Batch 27/64 loss: -0.3290717601776123
Batch 28/64 loss: -0.2737886607646942
Batch 29/64 loss: -0.31564584374427795
Batch 30/64 loss: -0.277990460395813
Batch 31/64 loss: -0.2616162598133087
Batch 32/64 loss: -0.26637303829193115
Batch 33/64 loss: -0.301689475774765
Batch 34/64 loss: -0.3138990104198456
Batch 35/64 loss: -0.33902329206466675
Batch 36/64 loss: -0.3199734091758728
Batch 37/64 loss: -0.29634004831314087
Batch 38/64 loss: -0.30156663060188293
Batch 39/64 loss: -0.3030523657798767
Batch 40/64 loss: -0.30384862422943115
Batch 41/64 loss: -0.32109683752059937
Batch 42/64 loss: -0.34292057156562805
Batch 43/64 loss: -0.30939561128616333
Batch 44/64 loss: -0.2765543460845947
Batch 45/64 loss: -0.27433741092681885
Batch 46/64 loss: -0.315494179725647
Batch 47/64 loss: -0.3021473288536072
Batch 48/64 loss: -0.31315121054649353
Batch 49/64 loss: -0.2913854122161865
Batch 50/64 loss: -0.3114548921585083
Batch 51/64 loss: -0.2877570390701294
Batch 52/64 loss: -0.32756394147872925
Batch 53/64 loss: -0.3197307586669922
Batch 54/64 loss: -0.2942304015159607
Batch 55/64 loss: -0.3155766725540161
Batch 56/64 loss: -0.3215601444244385
Batch 57/64 loss: -0.3152846693992615
Batch 58/64 loss: -0.31736743450164795
Batch 59/64 loss: -0.3171246647834778
Batch 60/64 loss: -0.32991302013397217
Batch 61/64 loss: -0.2739582657814026
Batch 62/64 loss: -0.3347820043563843
Batch 63/64 loss: -0.32062917947769165
Batch 64/64 loss: -0.3355039954185486
Epoch 297  Train loss: -0.308417873990302  Val loss: 0.026703559246259865
Epoch 298
-------------------------------
Batch 1/64 loss: -0.2751690745353699
Batch 2/64 loss: -0.3349931240081787
Batch 3/64 loss: -0.3305498957633972
Batch 4/64 loss: -0.2951256036758423
Batch 5/64 loss: -0.32550346851348877
Batch 6/64 loss: -0.30043190717697144
Batch 7/64 loss: -0.31182125210762024
Batch 8/64 loss: -0.30505436658859253
Batch 9/64 loss: -0.27745598554611206
Batch 10/64 loss: -0.30185768008232117
Batch 11/64 loss: -0.34726670384407043
Batch 12/64 loss: -0.34671837091445923
Batch 13/64 loss: -0.33828312158584595
Batch 14/64 loss: -0.31029272079467773
Batch 15/64 loss: -0.33262577652931213
Batch 16/64 loss: -0.3060840964317322
Batch 17/64 loss: -0.325009822845459
Batch 18/64 loss: -0.32441604137420654
Batch 19/64 loss: -0.3052424192428589
Batch 20/64 loss: -0.3059081435203552
Batch 21/64 loss: -0.3109903931617737
Batch 22/64 loss: -0.31477659940719604
Batch 23/64 loss: -0.33368492126464844
Batch 24/64 loss: -0.31963270902633667
Batch 25/64 loss: -0.3527337908744812
Batch 26/64 loss: -0.3276693820953369
Batch 27/64 loss: -0.3258431851863861
Batch 28/64 loss: -0.31527721881866455
Batch 29/64 loss: -0.33067357540130615
Batch 30/64 loss: -0.3429473042488098
Batch 31/64 loss: -0.30812638998031616
Batch 32/64 loss: -0.31333014369010925
Batch 33/64 loss: -0.3052319586277008
Batch 34/64 loss: -0.34059712290763855
Batch 35/64 loss: -0.3180602490901947
Batch 36/64 loss: -0.3138188123703003
Batch 37/64 loss: -0.2780126631259918
Batch 38/64 loss: -0.292574942111969
Batch 39/64 loss: -0.30658549070358276
Batch 40/64 loss: -0.3147439956665039
Batch 41/64 loss: -0.3307153880596161
Batch 42/64 loss: -0.3121183514595032
Batch 43/64 loss: -0.3007475733757019
Batch 44/64 loss: -0.3090250790119171
Batch 45/64 loss: -0.3253207802772522
Batch 46/64 loss: -0.31659168004989624
Batch 47/64 loss: -0.2777915596961975
Batch 48/64 loss: -0.28430867195129395
Batch 49/64 loss: -0.28490692377090454
Batch 50/64 loss: -0.3349238634109497
Batch 51/64 loss: -0.2942873239517212
Batch 52/64 loss: -0.29892539978027344
Batch 53/64 loss: -0.32639655470848083
Batch 54/64 loss: -0.33174043893814087
Batch 55/64 loss: -0.3257598280906677
Batch 56/64 loss: -0.30841219425201416
Batch 57/64 loss: -0.307491660118103
Batch 58/64 loss: -0.3246670961380005
Batch 59/64 loss: -0.2863467335700989
Batch 60/64 loss: -0.2852438986301422
Batch 61/64 loss: -0.32356181740760803
Batch 62/64 loss: -0.2818957269191742
Batch 63/64 loss: -0.31388595700263977
Batch 64/64 loss: -0.2267407476902008
Epoch 298  Train loss: -0.31247436053612654  Val loss: 0.025659713753310266
Epoch 299
-------------------------------
Batch 1/64 loss: -0.30436962842941284
Batch 2/64 loss: -0.32416078448295593
Batch 3/64 loss: -0.30850541591644287
Batch 4/64 loss: -0.3233288824558258
Batch 5/64 loss: -0.3220461308956146
Batch 6/64 loss: -0.329085111618042
Batch 7/64 loss: -0.33353883028030396
Batch 8/64 loss: -0.3115749657154083
Batch 9/64 loss: -0.33214884996414185
Batch 10/64 loss: -0.3150898814201355
Batch 11/64 loss: -0.33481156826019287
Batch 12/64 loss: -0.317222535610199
Batch 13/64 loss: -0.2925456762313843
Batch 14/64 loss: -0.2911364436149597
Batch 15/64 loss: -0.33431899547576904
Batch 16/64 loss: -0.2930760383605957
Batch 17/64 loss: -0.3104552626609802
Batch 18/64 loss: -0.31064945459365845
Batch 19/64 loss: -0.2974594831466675
Batch 20/64 loss: -0.2857348620891571
Batch 21/64 loss: -0.2731875479221344
Batch 22/64 loss: -0.28788095712661743
Batch 23/64 loss: -0.30548495054244995
Batch 24/64 loss: -0.30177074670791626
Batch 25/64 loss: -0.3254554271697998
Batch 26/64 loss: -0.27197200059890747
Batch 27/64 loss: -0.25520098209381104
Batch 28/64 loss: -0.34524083137512207
Batch 29/64 loss: -0.31780993938446045
Batch 30/64 loss: -0.3366812467575073
Batch 31/64 loss: -0.3143492341041565
Batch 32/64 loss: -0.3068115711212158
Batch 33/64 loss: -0.3094606101512909
Batch 34/64 loss: -0.3304003179073334
Batch 35/64 loss: -0.31861820816993713
Batch 36/64 loss: -0.26521071791648865
Batch 37/64 loss: -0.30601346492767334
Batch 38/64 loss: -0.2959008514881134
Batch 39/64 loss: -0.3296433985233307
Batch 40/64 loss: -0.33685874938964844
Batch 41/64 loss: -0.3106387257575989
Batch 42/64 loss: -0.2889691889286041
Batch 43/64 loss: -0.2917095422744751
Batch 44/64 loss: -0.34443774819374084
Batch 45/64 loss: -0.3298337459564209
Batch 46/64 loss: -0.33038216829299927
Batch 47/64 loss: -0.28888237476348877
Batch 48/64 loss: -0.3163315951824188
Batch 49/64 loss: -0.33009129762649536
Batch 50/64 loss: -0.3408874571323395
Batch 51/64 loss: -0.3236767053604126
Batch 52/64 loss: -0.3081192076206207
Batch 53/64 loss: -0.28236454725265503
Batch 54/64 loss: -0.3157135844230652
Batch 55/64 loss: -0.34911370277404785
Batch 56/64 loss: -0.3280547559261322
Batch 57/64 loss: -0.32612136006355286
Batch 58/64 loss: -0.3151046931743622
Batch 59/64 loss: -0.33807373046875
Batch 60/64 loss: -0.3140084743499756
Batch 61/64 loss: -0.31909823417663574
Batch 62/64 loss: -0.3274056911468506
Batch 63/64 loss: -0.3288537263870239
Batch 64/64 loss: -0.319136381149292
Epoch 299  Train loss: -0.3136068250618729  Val loss: 0.027078742423827705
Epoch 300
-------------------------------
Batch 1/64 loss: -0.3364981710910797
Batch 2/64 loss: -0.325640469789505
Batch 3/64 loss: -0.29329708218574524
Batch 4/64 loss: -0.29349106550216675
Batch 5/64 loss: -0.30501842498779297
Batch 6/64 loss: -0.3082641363143921
Batch 7/64 loss: -0.3423301577568054
Batch 8/64 loss: -0.31150153279304504
Batch 9/64 loss: -0.3422947824001312
Batch 10/64 loss: -0.30284976959228516
Batch 11/64 loss: -0.2973557114601135
Batch 12/64 loss: -0.28747233748435974
Batch 13/64 loss: -0.3241269290447235
Batch 14/64 loss: -0.3302302956581116
Batch 15/64 loss: -0.3222116231918335
Batch 16/64 loss: -0.34171539545059204
Batch 17/64 loss: -0.2927790880203247
Batch 18/64 loss: -0.3027309775352478
Batch 19/64 loss: -0.30434292554855347
Batch 20/64 loss: -0.31010574102401733
Batch 21/64 loss: -0.34352394938468933
Batch 22/64 loss: -0.3341667056083679
Batch 23/64 loss: -0.3171457350254059
Batch 24/64 loss: -0.27988091111183167
Batch 25/64 loss: -0.3064517080783844
Batch 26/64 loss: -0.2722426950931549
Batch 27/64 loss: -0.34231409430503845
Batch 28/64 loss: -0.34122806787490845
Batch 29/64 loss: -0.33859309554100037
Batch 30/64 loss: -0.2861992120742798
Batch 31/64 loss: -0.3143317997455597
Batch 32/64 loss: -0.3362002372741699
Batch 33/64 loss: -0.29721853137016296
Batch 34/64 loss: -0.315539687871933
Batch 35/64 loss: -0.3221471905708313
Batch 36/64 loss: -0.3166119158267975
Batch 37/64 loss: -0.2956553101539612
Batch 38/64 loss: -0.27588945627212524
Batch 39/64 loss: -0.311608225107193
Batch 40/64 loss: -0.3008105754852295
Batch 41/64 loss: -0.3403014540672302
Batch 42/64 loss: -0.33162975311279297
Batch 43/64 loss: -0.317677766084671
Batch 44/64 loss: -0.339632511138916
Batch 45/64 loss: -0.307780385017395
Batch 46/64 loss: -0.3211846947669983
Batch 47/64 loss: -0.3464719355106354
Batch 48/64 loss: -0.28968346118927
Batch 49/64 loss: -0.29733800888061523
Batch 50/64 loss: -0.3014317750930786
Batch 51/64 loss: -0.30659937858581543
Batch 52/64 loss: -0.28434044122695923
Batch 53/64 loss: -0.3056712746620178
Batch 54/64 loss: -0.3028402030467987
Batch 55/64 loss: -0.2934458553791046
Batch 56/64 loss: -0.31374239921569824
Batch 57/64 loss: -0.29744571447372437
Batch 58/64 loss: -0.32296621799468994
Batch 59/64 loss: -0.29835742712020874
Batch 60/64 loss: -0.3079960346221924
Batch 61/64 loss: -0.3175601661205292
Batch 62/64 loss: -0.31654566526412964
Batch 63/64 loss: -0.28272128105163574
Batch 64/64 loss: -0.282787024974823
Epoch 300  Train loss: -0.31180344773273844  Val loss: 0.029067309861330642
Epoch 301
-------------------------------
Batch 1/64 loss: -0.335644006729126
Batch 2/64 loss: -0.3113560676574707
Batch 3/64 loss: -0.3163048326969147
Batch 4/64 loss: -0.31528234481811523
Batch 5/64 loss: -0.3162572383880615
Batch 6/64 loss: -0.3047070801258087
Batch 7/64 loss: -0.3478686213493347
Batch 8/64 loss: -0.3032710552215576
Batch 9/64 loss: -0.29329514503479004
Batch 10/64 loss: -0.32020407915115356
Batch 11/64 loss: -0.27317410707473755
Batch 12/64 loss: -0.3329472541809082
Batch 13/64 loss: -0.32808929681777954
Batch 14/64 loss: -0.30958181619644165
Batch 15/64 loss: -0.3115345239639282
Batch 16/64 loss: -0.28915077447891235
Batch 17/64 loss: -0.3143383860588074
Batch 18/64 loss: -0.3151136338710785
Batch 19/64 loss: -0.30608057975769043
Batch 20/64 loss: -0.3363392949104309
Batch 21/64 loss: -0.25830650329589844
Batch 22/64 loss: -0.31836727261543274
Batch 23/64 loss: -0.3213997483253479
Batch 24/64 loss: -0.336961030960083
Batch 25/64 loss: -0.3119184970855713
Batch 26/64 loss: -0.3275148272514343
Batch 27/64 loss: -0.3127732574939728
Batch 28/64 loss: -0.3117319643497467
Batch 29/64 loss: -0.2901266813278198
Batch 30/64 loss: -0.30094319581985474
Batch 31/64 loss: -0.31766796112060547
Batch 32/64 loss: -0.3308151960372925
Batch 33/64 loss: -0.29287803173065186
Batch 34/64 loss: -0.30572769045829773
Batch 35/64 loss: -0.24494391679763794
Batch 36/64 loss: -0.31266480684280396
Batch 37/64 loss: -0.2944510579109192
Batch 38/64 loss: -0.2986999750137329
Batch 39/64 loss: -0.3284153640270233
Batch 40/64 loss: -0.339411199092865
Batch 41/64 loss: -0.31897175312042236
Batch 42/64 loss: -0.31384652853012085
Batch 43/64 loss: -0.341540664434433
Batch 44/64 loss: -0.2987263798713684
Batch 45/64 loss: -0.2979939877986908
Batch 46/64 loss: -0.3308353126049042
Batch 47/64 loss: -0.3140929341316223
Batch 48/64 loss: -0.2977330684661865
Batch 49/64 loss: -0.3207145929336548
Batch 50/64 loss: -0.29356980323791504
Batch 51/64 loss: -0.3130559027194977
Batch 52/64 loss: -0.2946128249168396
Batch 53/64 loss: -0.3067186176776886
Batch 54/64 loss: -0.26464027166366577
Batch 55/64 loss: -0.29112276434898376
Batch 56/64 loss: -0.2840571701526642
Batch 57/64 loss: -0.3222470283508301
Batch 58/64 loss: -0.3004305362701416
Batch 59/64 loss: -0.25687068700790405
Batch 60/64 loss: -0.32908549904823303
Batch 61/64 loss: -0.3196925222873688
Batch 62/64 loss: -0.31740331649780273
Batch 63/64 loss: -0.34869104623794556
Batch 64/64 loss: -0.26749420166015625
Epoch 301  Train loss: -0.30923187732696533  Val loss: 0.02913314226976375
Epoch 302
-------------------------------
Batch 1/64 loss: -0.31467288732528687
Batch 2/64 loss: -0.32095637917518616
Batch 3/64 loss: -0.3403012752532959
Batch 4/64 loss: -0.3004187345504761
Batch 5/64 loss: -0.3332883417606354
Batch 6/64 loss: -0.3066709637641907
Batch 7/64 loss: -0.3051701784133911
Batch 8/64 loss: -0.3066816031932831
Batch 9/64 loss: -0.3314453363418579
Batch 10/64 loss: -0.33966922760009766
Batch 11/64 loss: -0.29601454734802246
Batch 12/64 loss: -0.3034633994102478
Batch 13/64 loss: -0.3448706269264221
Batch 14/64 loss: -0.29328444600105286
Batch 15/64 loss: -0.3413558006286621
Batch 16/64 loss: -0.30221307277679443
Batch 17/64 loss: -0.3040844202041626
Batch 18/64 loss: -0.3175683617591858
Batch 19/64 loss: -0.31871387362480164
Batch 20/64 loss: -0.31554657220840454
Batch 21/64 loss: -0.26879626512527466
Batch 22/64 loss: -0.34069645404815674
Batch 23/64 loss: -0.3388241231441498
Batch 24/64 loss: -0.33356016874313354
Batch 25/64 loss: -0.31055498123168945
Batch 26/64 loss: -0.3250933885574341
Batch 27/64 loss: -0.3187938928604126
Batch 28/64 loss: -0.32960760593414307
Batch 29/64 loss: -0.31050506234169006
Batch 30/64 loss: -0.3295048475265503
Batch 31/64 loss: -0.28924131393432617
Batch 32/64 loss: -0.29839810729026794
Batch 33/64 loss: -0.2817729413509369
Batch 34/64 loss: -0.30773377418518066
Batch 35/64 loss: -0.29879409074783325
Batch 36/64 loss: -0.2579827606678009
Batch 37/64 loss: -0.3262177109718323
Batch 38/64 loss: -0.2799159288406372
Batch 39/64 loss: -0.3348419666290283
Batch 40/64 loss: -0.32804009318351746
Batch 41/64 loss: -0.2650804817676544
Batch 42/64 loss: -0.26669004559516907
Batch 43/64 loss: -0.33073607087135315
Batch 44/64 loss: -0.31639039516448975
Batch 45/64 loss: -0.31336426734924316
Batch 46/64 loss: -0.3235311210155487
Batch 47/64 loss: -0.27035629749298096
Batch 48/64 loss: -0.333963006734848
Batch 49/64 loss: -0.2807905375957489
Batch 50/64 loss: -0.28458958864212036
Batch 51/64 loss: -0.31933796405792236
Batch 52/64 loss: -0.29824507236480713
Batch 53/64 loss: -0.31219393014907837
Batch 54/64 loss: -0.35133111476898193
Batch 55/64 loss: -0.2927399277687073
Batch 56/64 loss: -0.3396432399749756
Batch 57/64 loss: -0.34418487548828125
Batch 58/64 loss: -0.3116474151611328
Batch 59/64 loss: -0.303382009267807
Batch 60/64 loss: -0.27504122257232666
Batch 61/64 loss: -0.3142857551574707
Batch 62/64 loss: -0.31890586018562317
Batch 63/64 loss: -0.2948181629180908
Batch 64/64 loss: -0.2914135456085205
Epoch 302  Train loss: -0.31098155367608166  Val loss: 0.028035983187226495
Epoch 303
-------------------------------
Batch 1/64 loss: -0.3286569118499756
Batch 2/64 loss: -0.2727288007736206
Batch 3/64 loss: -0.32000285387039185
Batch 4/64 loss: -0.28157228231430054
Batch 5/64 loss: -0.3104565739631653
Batch 6/64 loss: -0.27177998423576355
Batch 7/64 loss: -0.28303366899490356
Batch 8/64 loss: -0.33978453278541565
Batch 9/64 loss: -0.30892133712768555
Batch 10/64 loss: -0.33404773473739624
Batch 11/64 loss: -0.3348100185394287
Batch 12/64 loss: -0.3300244212150574
Batch 13/64 loss: -0.3078312277793884
Batch 14/64 loss: -0.34630149602890015
Batch 15/64 loss: -0.31296753883361816
Batch 16/64 loss: -0.32581257820129395
Batch 17/64 loss: -0.33523499965667725
Batch 18/64 loss: -0.296409010887146
Batch 19/64 loss: -0.3274043798446655
Batch 20/64 loss: -0.31247764825820923
Batch 21/64 loss: -0.3046734929084778
Batch 22/64 loss: -0.33521759510040283
Batch 23/64 loss: -0.33848536014556885
Batch 24/64 loss: -0.3136328458786011
Batch 25/64 loss: -0.3163244426250458
Batch 26/64 loss: -0.32705748081207275
Batch 27/64 loss: -0.2769187092781067
Batch 28/64 loss: -0.31488102674484253
Batch 29/64 loss: -0.33135053515434265
Batch 30/64 loss: -0.3406434655189514
Batch 31/64 loss: -0.2944716215133667
Batch 32/64 loss: -0.3239242732524872
Batch 33/64 loss: -0.31252652406692505
Batch 34/64 loss: -0.2988811731338501
Batch 35/64 loss: -0.255296915769577
Batch 36/64 loss: -0.33147740364074707
Batch 37/64 loss: -0.29945629835128784
Batch 38/64 loss: -0.2720082700252533
Batch 39/64 loss: -0.3420640230178833
Batch 40/64 loss: -0.3441653847694397
Batch 41/64 loss: -0.3036116361618042
Batch 42/64 loss: -0.3401377201080322
Batch 43/64 loss: -0.30319923162460327
Batch 44/64 loss: -0.32951927185058594
Batch 45/64 loss: -0.3264140784740448
Batch 46/64 loss: -0.3255721926689148
Batch 47/64 loss: -0.3392302691936493
Batch 48/64 loss: -0.25201016664505005
Batch 49/64 loss: -0.3128003478050232
Batch 50/64 loss: -0.3065069913864136
Batch 51/64 loss: -0.33311691880226135
Batch 52/64 loss: -0.3308512568473816
Batch 53/64 loss: -0.32882899045944214
Batch 54/64 loss: -0.2496425211429596
Batch 55/64 loss: -0.34734416007995605
Batch 56/64 loss: -0.2721157670021057
Batch 57/64 loss: -0.34547483921051025
Batch 58/64 loss: -0.304340660572052
Batch 59/64 loss: -0.2664417326450348
Batch 60/64 loss: -0.32001927495002747
Batch 61/64 loss: -0.27732133865356445
Batch 62/64 loss: -0.3469485640525818
Batch 63/64 loss: -0.3245308995246887
Batch 64/64 loss: -0.32989293336868286
Epoch 303  Train loss: -0.31352334700378715  Val loss: 0.027104513956509094
Epoch 304
-------------------------------
Batch 1/64 loss: -0.35184526443481445
Batch 2/64 loss: -0.32854220271110535
Batch 3/64 loss: -0.3472641408443451
Batch 4/64 loss: -0.3408316671848297
Batch 5/64 loss: -0.31900346279144287
Batch 6/64 loss: -0.29238206148147583
Batch 7/64 loss: -0.3230193257331848
Batch 8/64 loss: -0.32336103916168213
Batch 9/64 loss: -0.3274952173233032
Batch 10/64 loss: -0.34584841132164
Batch 11/64 loss: -0.28963911533355713
Batch 12/64 loss: -0.3050200939178467
Batch 13/64 loss: -0.32885658740997314
Batch 14/64 loss: -0.29509568214416504
Batch 15/64 loss: -0.32301580905914307
Batch 16/64 loss: -0.3327684700489044
Batch 17/64 loss: -0.3472651243209839
Batch 18/64 loss: -0.3225497007369995
Batch 19/64 loss: -0.352566123008728
Batch 20/64 loss: -0.31679636240005493
Batch 21/64 loss: -0.3086082339286804
Batch 22/64 loss: -0.31984272599220276
Batch 23/64 loss: -0.29451441764831543
Batch 24/64 loss: -0.3056710958480835
Batch 25/64 loss: -0.33018726110458374
Batch 26/64 loss: -0.31186622381210327
Batch 27/64 loss: -0.3282267451286316
Batch 28/64 loss: -0.3041122853755951
Batch 29/64 loss: -0.3054679334163666
Batch 30/64 loss: -0.3165826201438904
Batch 31/64 loss: -0.33378082513809204
Batch 32/64 loss: -0.33375096321105957
Batch 33/64 loss: -0.28633037209510803
Batch 34/64 loss: -0.2966460585594177
Batch 35/64 loss: -0.3048771619796753
Batch 36/64 loss: -0.3245430588722229
Batch 37/64 loss: -0.32702744007110596
Batch 38/64 loss: -0.3338105380535126
Batch 39/64 loss: -0.3075903654098511
Batch 40/64 loss: -0.34456372261047363
Batch 41/64 loss: -0.3143307566642761
Batch 42/64 loss: -0.32987624406814575
Batch 43/64 loss: -0.33005625009536743
Batch 44/64 loss: -0.3161604404449463
Batch 45/64 loss: -0.26667049527168274
Batch 46/64 loss: -0.30164942145347595
Batch 47/64 loss: -0.26726970076560974
Batch 48/64 loss: -0.33322641253471375
Batch 49/64 loss: -0.28913217782974243
Batch 50/64 loss: -0.3251610994338989
Batch 51/64 loss: -0.30241858959198
Batch 52/64 loss: -0.28482961654663086
Batch 53/64 loss: -0.3104960322380066
Batch 54/64 loss: -0.2903119623661041
Batch 55/64 loss: -0.29124563932418823
Batch 56/64 loss: -0.2714288830757141
Batch 57/64 loss: -0.3272501826286316
Batch 58/64 loss: -0.3428403437137604
Batch 59/64 loss: -0.32860028743743896
Batch 60/64 loss: -0.33349764347076416
Batch 61/64 loss: -0.2930794954299927
Batch 62/64 loss: -0.31630194187164307
Batch 63/64 loss: -0.3334193825721741
Batch 64/64 loss: -0.3340930938720703
Epoch 304  Train loss: -0.3165645290823544  Val loss: 0.029321184068201334
Epoch 305
-------------------------------
Batch 1/64 loss: -0.32325124740600586
Batch 2/64 loss: -0.3448096215724945
Batch 3/64 loss: -0.31342053413391113
Batch 4/64 loss: -0.33084696531295776
Batch 5/64 loss: -0.3290177881717682
Batch 6/64 loss: -0.3421841263771057
Batch 7/64 loss: -0.335779070854187
Batch 8/64 loss: -0.33817338943481445
Batch 9/64 loss: -0.3026673197746277
Batch 10/64 loss: -0.35079509019851685
Batch 11/64 loss: -0.3262678384780884
Batch 12/64 loss: -0.3200341463088989
Batch 13/64 loss: -0.3440052270889282
Batch 14/64 loss: -0.3126187026500702
Batch 15/64 loss: -0.3529840111732483
Batch 16/64 loss: -0.3465070128440857
Batch 17/64 loss: -0.33485257625579834
Batch 18/64 loss: -0.2957359552383423
Batch 19/64 loss: -0.2912721037864685
Batch 20/64 loss: -0.3082338571548462
Batch 21/64 loss: -0.3031936585903168
Batch 22/64 loss: -0.2936347723007202
Batch 23/64 loss: -0.3007785975933075
Batch 24/64 loss: -0.31254318356513977
Batch 25/64 loss: -0.33892035484313965
Batch 26/64 loss: -0.3545532822608948
Batch 27/64 loss: -0.30308204889297485
Batch 28/64 loss: -0.3140820860862732
Batch 29/64 loss: -0.34198248386383057
Batch 30/64 loss: -0.2909533977508545
Batch 31/64 loss: -0.32462120056152344
Batch 32/64 loss: -0.33055055141448975
Batch 33/64 loss: -0.29553550481796265
Batch 34/64 loss: -0.28495699167251587
Batch 35/64 loss: -0.30636224150657654
Batch 36/64 loss: -0.3076273202896118
Batch 37/64 loss: -0.31590932607650757
Batch 38/64 loss: -0.24962300062179565
Batch 39/64 loss: -0.30794721841812134
Batch 40/64 loss: -0.29581791162490845
Batch 41/64 loss: -0.32214033603668213
Batch 42/64 loss: -0.3458069860935211
Batch 43/64 loss: -0.27449947595596313
Batch 44/64 loss: -0.33405429124832153
Batch 45/64 loss: -0.3209591805934906
Batch 46/64 loss: -0.30581235885620117
Batch 47/64 loss: -0.29625204205513
Batch 48/64 loss: -0.2814602851867676
Batch 49/64 loss: -0.32563215494155884
Batch 50/64 loss: -0.32124289870262146
Batch 51/64 loss: -0.2937955856323242
Batch 52/64 loss: -0.32518333196640015
Batch 53/64 loss: -0.30934128165245056
Batch 54/64 loss: -0.3292345404624939
Batch 55/64 loss: -0.3189873695373535
Batch 56/64 loss: -0.3300473093986511
Batch 57/64 loss: -0.3236643075942993
Batch 58/64 loss: -0.30833905935287476
Batch 59/64 loss: -0.32257530093193054
Batch 60/64 loss: -0.34526199102401733
Batch 61/64 loss: -0.29435214400291443
Batch 62/64 loss: -0.325871080160141
Batch 63/64 loss: -0.2951729893684387
Batch 64/64 loss: -0.30184751749038696
Epoch 305  Train loss: -0.31674044950335634  Val loss: 0.03006008236678605
Epoch 306
-------------------------------
Batch 1/64 loss: -0.2994203269481659
Batch 2/64 loss: -0.2863481640815735
Batch 3/64 loss: -0.314738005399704
Batch 4/64 loss: -0.32861635088920593
Batch 5/64 loss: -0.35243749618530273
Batch 6/64 loss: -0.3524683117866516
Batch 7/64 loss: -0.3251552879810333
Batch 8/64 loss: -0.3188354969024658
Batch 9/64 loss: -0.2917189598083496
Batch 10/64 loss: -0.32597100734710693
Batch 11/64 loss: -0.3315754234790802
Batch 12/64 loss: -0.3032245635986328
Batch 13/64 loss: -0.29485344886779785
Batch 14/64 loss: -0.2584089934825897
Batch 15/64 loss: -0.3355334401130676
Batch 16/64 loss: -0.3258305788040161
Batch 17/64 loss: -0.31599295139312744
Batch 18/64 loss: -0.30560725927352905
Batch 19/64 loss: -0.33004605770111084
Batch 20/64 loss: -0.3257756233215332
Batch 21/64 loss: -0.31630396842956543
Batch 22/64 loss: -0.3057752251625061
Batch 23/64 loss: -0.2682263255119324
Batch 24/64 loss: -0.340995192527771
Batch 25/64 loss: -0.33616581559181213
Batch 26/64 loss: -0.33075758814811707
Batch 27/64 loss: -0.26327288150787354
Batch 28/64 loss: -0.3237592577934265
Batch 29/64 loss: -0.3244866132736206
Batch 30/64 loss: -0.34049034118652344
Batch 31/64 loss: -0.34490156173706055
Batch 32/64 loss: -0.3235227167606354
Batch 33/64 loss: -0.33139118552207947
Batch 34/64 loss: -0.3199940621852875
Batch 35/64 loss: -0.3006509840488434
Batch 36/64 loss: -0.3210180103778839
Batch 37/64 loss: -0.33104220032691956
Batch 38/64 loss: -0.3249824643135071
Batch 39/64 loss: -0.3265528380870819
Batch 40/64 loss: -0.29971179366111755
Batch 41/64 loss: -0.32296836376190186
Batch 42/64 loss: -0.307611346244812
Batch 43/64 loss: -0.3172292411327362
Batch 44/64 loss: -0.30054008960723877
Batch 45/64 loss: -0.24223002791404724
Batch 46/64 loss: -0.2895162105560303
Batch 47/64 loss: -0.30089062452316284
Batch 48/64 loss: -0.304186075925827
Batch 49/64 loss: -0.2758799195289612
Batch 50/64 loss: -0.2897852957248688
Batch 51/64 loss: -0.3051148056983948
Batch 52/64 loss: -0.30701249837875366
Batch 53/64 loss: -0.3159266710281372
Batch 54/64 loss: -0.3181055784225464
Batch 55/64 loss: -0.3023470640182495
Batch 56/64 loss: -0.3168557286262512
Batch 57/64 loss: -0.3002098798751831
Batch 58/64 loss: -0.3168971538543701
Batch 59/64 loss: -0.3481534719467163
Batch 60/64 loss: -0.3450464904308319
Batch 61/64 loss: -0.3244163393974304
Batch 62/64 loss: -0.31758588552474976
Batch 63/64 loss: -0.29452288150787354
Batch 64/64 loss: -0.296186625957489
Epoch 306  Train loss: -0.3134389080253302  Val loss: 0.025875377900821648
Epoch 307
-------------------------------
Batch 1/64 loss: -0.3401148319244385
Batch 2/64 loss: -0.32939696311950684
Batch 3/64 loss: -0.34226465225219727
Batch 4/64 loss: -0.3274453282356262
Batch 5/64 loss: -0.3337779641151428
Batch 6/64 loss: -0.29061412811279297
Batch 7/64 loss: -0.32414790987968445
Batch 8/64 loss: -0.3208189606666565
Batch 9/64 loss: -0.329250693321228
Batch 10/64 loss: -0.3179479241371155
Batch 11/64 loss: -0.34928402304649353
Batch 12/64 loss: -0.32962945103645325
Batch 13/64 loss: -0.34661632776260376
Batch 14/64 loss: -0.31997525691986084
Batch 15/64 loss: -0.32020390033721924
Batch 16/64 loss: -0.31064367294311523
Batch 17/64 loss: -0.30344530940055847
Batch 18/64 loss: -0.3560764789581299
Batch 19/64 loss: -0.3152126669883728
Batch 20/64 loss: -0.3068819046020508
Batch 21/64 loss: -0.2888980209827423
Batch 22/64 loss: -0.2504206597805023
Batch 23/64 loss: -0.32557040452957153
Batch 24/64 loss: -0.33433035016059875
Batch 25/64 loss: -0.31342607736587524
Batch 26/64 loss: -0.3312099874019623
Batch 27/64 loss: -0.325988233089447
Batch 28/64 loss: -0.3538692891597748
Batch 29/64 loss: -0.3286997675895691
Batch 30/64 loss: -0.31511715054512024
Batch 31/64 loss: -0.29953664541244507
Batch 32/64 loss: -0.3378664553165436
Batch 33/64 loss: -0.3276216983795166
Batch 34/64 loss: -0.2794795632362366
Batch 35/64 loss: -0.24745094776153564
Batch 36/64 loss: -0.2996094226837158
Batch 37/64 loss: -0.3316226303577423
Batch 38/64 loss: -0.3008182942867279
Batch 39/64 loss: -0.29484593868255615
Batch 40/64 loss: -0.32048073410987854
Batch 41/64 loss: -0.3126348555088043
Batch 42/64 loss: -0.31880486011505127
Batch 43/64 loss: -0.3405296206474304
Batch 44/64 loss: -0.3461305499076843
Batch 45/64 loss: -0.27321141958236694
Batch 46/64 loss: -0.33252978324890137
Batch 47/64 loss: -0.3018442988395691
Batch 48/64 loss: -0.29530981183052063
Batch 49/64 loss: -0.3279538154602051
Batch 50/64 loss: -0.3327081799507141
Batch 51/64 loss: -0.2548993229866028
Batch 52/64 loss: -0.320259690284729
Batch 53/64 loss: -0.3179531693458557
Batch 54/64 loss: -0.3056710362434387
Batch 55/64 loss: -0.2921188175678253
Batch 56/64 loss: -0.3092647194862366
Batch 57/64 loss: -0.31592193245887756
Batch 58/64 loss: -0.3412906527519226
Batch 59/64 loss: -0.33655595779418945
Batch 60/64 loss: -0.31196653842926025
Batch 61/64 loss: -0.3028821647167206
Batch 62/64 loss: -0.31752318143844604
Batch 63/64 loss: -0.3172440826892853
Batch 64/64 loss: -0.29900407791137695
Epoch 307  Train loss: -0.3163932884440703  Val loss: 0.027743373539849247
Epoch 308
-------------------------------
Batch 1/64 loss: -0.35588738322257996
Batch 2/64 loss: -0.2875150442123413
Batch 3/64 loss: -0.34673434495925903
Batch 4/64 loss: -0.3047192692756653
Batch 5/64 loss: -0.3052695393562317
Batch 6/64 loss: -0.31650274991989136
Batch 7/64 loss: -0.3025859296321869
Batch 8/64 loss: -0.3112795948982239
Batch 9/64 loss: -0.32665133476257324
Batch 10/64 loss: -0.3366091549396515
Batch 11/64 loss: -0.3198138177394867
Batch 12/64 loss: -0.3265460133552551
Batch 13/64 loss: -0.32920974493026733
Batch 14/64 loss: -0.2833060026168823
Batch 15/64 loss: -0.29109424352645874
Batch 16/64 loss: -0.26680928468704224
Batch 17/64 loss: -0.32605987787246704
Batch 18/64 loss: -0.29512614011764526
Batch 19/64 loss: -0.3405473530292511
Batch 20/64 loss: -0.34541988372802734
Batch 21/64 loss: -0.35037750005722046
Batch 22/64 loss: -0.29323768615722656
Batch 23/64 loss: -0.33992624282836914
Batch 24/64 loss: -0.29182469844818115
Batch 25/64 loss: -0.36175429821014404
Batch 26/64 loss: -0.28862208127975464
Batch 27/64 loss: -0.3007825016975403
Batch 28/64 loss: -0.3200203776359558
Batch 29/64 loss: -0.3078575134277344
Batch 30/64 loss: -0.31063729524612427
Batch 31/64 loss: -0.3316952586174011
Batch 32/64 loss: -0.3263471722602844
Batch 33/64 loss: -0.31782013177871704
Batch 34/64 loss: -0.27074098587036133
Batch 35/64 loss: -0.31135764718055725
Batch 36/64 loss: -0.3433632552623749
Batch 37/64 loss: -0.2942172586917877
Batch 38/64 loss: -0.3520512580871582
Batch 39/64 loss: -0.3340076804161072
Batch 40/64 loss: -0.30867278575897217
Batch 41/64 loss: -0.33311858773231506
Batch 42/64 loss: -0.25996869802474976
Batch 43/64 loss: -0.2919772267341614
Batch 44/64 loss: -0.31230953335762024
Batch 45/64 loss: -0.33674171566963196
Batch 46/64 loss: -0.2928956151008606
Batch 47/64 loss: -0.2756563425064087
Batch 48/64 loss: -0.3338213264942169
Batch 49/64 loss: -0.3263004422187805
Batch 50/64 loss: -0.3256736099720001
Batch 51/64 loss: -0.322920024394989
Batch 52/64 loss: -0.3229350447654724
Batch 53/64 loss: -0.3320493698120117
Batch 54/64 loss: -0.2540903389453888
Batch 55/64 loss: -0.2956228256225586
Batch 56/64 loss: -0.31826627254486084
Batch 57/64 loss: -0.29471009969711304
Batch 58/64 loss: -0.3521214425563812
Batch 59/64 loss: -0.3181725740432739
Batch 60/64 loss: -0.3510020673274994
Batch 61/64 loss: -0.28786563873291016
Batch 62/64 loss: -0.33393406867980957
Batch 63/64 loss: -0.3067106604576111
Batch 64/64 loss: -0.24653157591819763
Epoch 308  Train loss: -0.3143021575376099  Val loss: 0.030300499032862818
Epoch 309
-------------------------------
Batch 1/64 loss: -0.3401435613632202
Batch 2/64 loss: -0.333061158657074
Batch 3/64 loss: -0.29620635509490967
Batch 4/64 loss: -0.27721333503723145
Batch 5/64 loss: -0.3276439905166626
Batch 6/64 loss: -0.2386467456817627
Batch 7/64 loss: -0.3312239944934845
Batch 8/64 loss: -0.2962685227394104
Batch 9/64 loss: -0.33832672238349915
Batch 10/64 loss: -0.32030153274536133
Batch 11/64 loss: -0.318403035402298
Batch 12/64 loss: -0.31532955169677734
Batch 13/64 loss: -0.3239811658859253
Batch 14/64 loss: -0.3166806697845459
Batch 15/64 loss: -0.30584716796875
Batch 16/64 loss: -0.3114071488380432
Batch 17/64 loss: -0.3115362823009491
Batch 18/64 loss: -0.3226154148578644
Batch 19/64 loss: -0.3423345685005188
Batch 20/64 loss: -0.34332752227783203
Batch 21/64 loss: -0.2884342074394226
Batch 22/64 loss: -0.3300611078739166
Batch 23/64 loss: -0.33967846632003784
Batch 24/64 loss: -0.32994940876960754
Batch 25/64 loss: -0.3178975582122803
Batch 26/64 loss: -0.3397379219532013
Batch 27/64 loss: -0.3190615773200989
Batch 28/64 loss: -0.3143351972103119
Batch 29/64 loss: -0.3129177391529083
Batch 30/64 loss: -0.35205525159835815
Batch 31/64 loss: -0.2879030108451843
Batch 32/64 loss: -0.3306431770324707
Batch 33/64 loss: -0.2970157861709595
Batch 34/64 loss: -0.31097421050071716
Batch 35/64 loss: -0.3365960121154785
Batch 36/64 loss: -0.33390024304389954
Batch 37/64 loss: -0.3018796443939209
Batch 38/64 loss: -0.33142513036727905
Batch 39/64 loss: -0.32018911838531494
Batch 40/64 loss: -0.30582195520401
Batch 41/64 loss: -0.3055349290370941
Batch 42/64 loss: -0.28303655982017517
Batch 43/64 loss: -0.32917746901512146
Batch 44/64 loss: -0.3286740779876709
Batch 45/64 loss: -0.30386680364608765
Batch 46/64 loss: -0.3137131333351135
Batch 47/64 loss: -0.3131978213787079
Batch 48/64 loss: -0.29146575927734375
Batch 49/64 loss: -0.3167390823364258
Batch 50/64 loss: -0.3194361627101898
Batch 51/64 loss: -0.30196109414100647
Batch 52/64 loss: -0.27893975377082825
Batch 53/64 loss: -0.3133138418197632
Batch 54/64 loss: -0.3351587653160095
Batch 55/64 loss: -0.29350942373275757
Batch 56/64 loss: -0.3144723176956177
Batch 57/64 loss: -0.3100017309188843
Batch 58/64 loss: -0.2783375084400177
Batch 59/64 loss: -0.2270483672618866
Batch 60/64 loss: -0.26459288597106934
Batch 61/64 loss: -0.3114336431026459
Batch 62/64 loss: -0.3259067237377167
Batch 63/64 loss: -0.34708768129348755
Batch 64/64 loss: -0.2918749451637268
Epoch 309  Train loss: -0.31272920650594377  Val loss: 0.028046048383942174
Epoch 310
-------------------------------
Batch 1/64 loss: -0.3047477602958679
Batch 2/64 loss: -0.28609195351600647
Batch 3/64 loss: -0.30291324853897095
Batch 4/64 loss: -0.34054458141326904
Batch 5/64 loss: -0.3162686824798584
Batch 6/64 loss: -0.32827067375183105
Batch 7/64 loss: -0.2697468400001526
Batch 8/64 loss: -0.3520001173019409
Batch 9/64 loss: -0.3075321316719055
Batch 10/64 loss: -0.36011600494384766
Batch 11/64 loss: -0.31278282403945923
Batch 12/64 loss: -0.33124804496765137
Batch 13/64 loss: -0.3230966329574585
Batch 14/64 loss: -0.3200179934501648
Batch 15/64 loss: -0.29850104451179504
Batch 16/64 loss: -0.32729315757751465
Batch 17/64 loss: -0.33950909972190857
Batch 18/64 loss: -0.3332577645778656
Batch 19/64 loss: -0.300235778093338
Batch 20/64 loss: -0.3376644551753998
Batch 21/64 loss: -0.31966888904571533
Batch 22/64 loss: -0.33306458592414856
Batch 23/64 loss: -0.3397316038608551
Batch 24/64 loss: -0.3353334069252014
Batch 25/64 loss: -0.33306315541267395
Batch 26/64 loss: -0.34423696994781494
Batch 27/64 loss: -0.2765474319458008
Batch 28/64 loss: -0.3403307795524597
Batch 29/64 loss: -0.3033025860786438
Batch 30/64 loss: -0.3541421890258789
Batch 31/64 loss: -0.3347959518432617
Batch 32/64 loss: -0.26971957087516785
Batch 33/64 loss: -0.342273473739624
Batch 34/64 loss: -0.2663573622703552
Batch 35/64 loss: -0.3186071515083313
Batch 36/64 loss: -0.3285812437534332
Batch 37/64 loss: -0.3196938633918762
Batch 38/64 loss: -0.29167211055755615
Batch 39/64 loss: -0.3268457353115082
Batch 40/64 loss: -0.2913558781147003
Batch 41/64 loss: -0.3335450291633606
Batch 42/64 loss: -0.3108375370502472
Batch 43/64 loss: -0.3302055597305298
Batch 44/64 loss: -0.3269256353378296
Batch 45/64 loss: -0.31742018461227417
Batch 46/64 loss: -0.3466488718986511
Batch 47/64 loss: -0.31934165954589844
Batch 48/64 loss: -0.29871058464050293
Batch 49/64 loss: -0.2797181010246277
Batch 50/64 loss: -0.34362006187438965
Batch 51/64 loss: -0.33966225385665894
Batch 52/64 loss: -0.3440074026584625
Batch 53/64 loss: -0.26386308670043945
Batch 54/64 loss: -0.3227440118789673
Batch 55/64 loss: -0.29677969217300415
Batch 56/64 loss: -0.2985231876373291
Batch 57/64 loss: -0.33027884364128113
Batch 58/64 loss: -0.3305821716785431
Batch 59/64 loss: -0.33562493324279785
Batch 60/64 loss: -0.291492223739624
Batch 61/64 loss: -0.32601791620254517
Batch 62/64 loss: -0.2834857702255249
Batch 63/64 loss: -0.29449552297592163
Batch 64/64 loss: -0.35321393609046936
Epoch 310  Train loss: -0.318283975241231  Val loss: 0.028921169513689282
Epoch 311
-------------------------------
Batch 1/64 loss: -0.32331383228302
Batch 2/64 loss: -0.33574533462524414
Batch 3/64 loss: -0.33013400435447693
Batch 4/64 loss: -0.2972955107688904
Batch 5/64 loss: -0.33821481466293335
Batch 6/64 loss: -0.30988699197769165
Batch 7/64 loss: -0.3153189420700073
Batch 8/64 loss: -0.30622899532318115
Batch 9/64 loss: -0.3322843313217163
Batch 10/64 loss: -0.314780592918396
Batch 11/64 loss: -0.3003775477409363
Batch 12/64 loss: -0.33810341358184814
Batch 13/64 loss: -0.3449481129646301
Batch 14/64 loss: -0.33595991134643555
Batch 15/64 loss: -0.31996214389801025
Batch 16/64 loss: -0.34129631519317627
Batch 17/64 loss: -0.3105754256248474
Batch 18/64 loss: -0.3230731189250946
Batch 19/64 loss: -0.33275341987609863
Batch 20/64 loss: -0.3173672556877136
Batch 21/64 loss: -0.35333991050720215
Batch 22/64 loss: -0.259860098361969
Batch 23/64 loss: -0.337830126285553
Batch 24/64 loss: -0.33788764476776123
Batch 25/64 loss: -0.30826735496520996
Batch 26/64 loss: -0.30441439151763916
Batch 27/64 loss: -0.3480595052242279
Batch 28/64 loss: -0.276725172996521
Batch 29/64 loss: -0.31767717003822327
Batch 30/64 loss: -0.34559017419815063
Batch 31/64 loss: -0.34078943729400635
Batch 32/64 loss: -0.28194373846054077
Batch 33/64 loss: -0.2704380750656128
Batch 34/64 loss: -0.3245355486869812
Batch 35/64 loss: -0.3590150475502014
Batch 36/64 loss: -0.3383824825286865
Batch 37/64 loss: -0.32510730624198914
Batch 38/64 loss: -0.2945009469985962
Batch 39/64 loss: -0.27254578471183777
Batch 40/64 loss: -0.2935292422771454
Batch 41/64 loss: -0.3061900734901428
Batch 42/64 loss: -0.34405088424682617
Batch 43/64 loss: -0.28107643127441406
Batch 44/64 loss: -0.25951242446899414
Batch 45/64 loss: -0.31808966398239136
Batch 46/64 loss: -0.3098773956298828
Batch 47/64 loss: -0.3379628360271454
Batch 48/64 loss: -0.32232600450515747
Batch 49/64 loss: -0.27365541458129883
Batch 50/64 loss: -0.30078989267349243
Batch 51/64 loss: -0.3272969722747803
Batch 52/64 loss: -0.3272651433944702
Batch 53/64 loss: -0.3372514843940735
Batch 54/64 loss: -0.30258846282958984
Batch 55/64 loss: -0.28198257088661194
Batch 56/64 loss: -0.31129270792007446
Batch 57/64 loss: -0.32663795351982117
Batch 58/64 loss: -0.2808017134666443
Batch 59/64 loss: -0.31208401918411255
Batch 60/64 loss: -0.33716538548469543
Batch 61/64 loss: -0.3055318295955658
Batch 62/64 loss: -0.336841881275177
Batch 63/64 loss: -0.32578328251838684
Batch 64/64 loss: -0.2582232356071472
Epoch 311  Train loss: -0.31557303573571  Val loss: 0.03244742105916603
Epoch 312
-------------------------------
Batch 1/64 loss: -0.3376421332359314
Batch 2/64 loss: -0.33287978172302246
Batch 3/64 loss: -0.34558796882629395
Batch 4/64 loss: -0.2849087119102478
Batch 5/64 loss: -0.34799253940582275
Batch 6/64 loss: -0.29739850759506226
Batch 7/64 loss: -0.336765319108963
Batch 8/64 loss: -0.29553818702697754
Batch 9/64 loss: -0.3302840292453766
Batch 10/64 loss: -0.2995920181274414
Batch 11/64 loss: -0.3041260540485382
Batch 12/64 loss: -0.2829914689064026
Batch 13/64 loss: -0.331228107213974
Batch 14/64 loss: -0.3139723241329193
Batch 15/64 loss: -0.3330896198749542
Batch 16/64 loss: -0.3010576069355011
Batch 17/64 loss: -0.3111116886138916
Batch 18/64 loss: -0.3296930193901062
Batch 19/64 loss: -0.31699278950691223
Batch 20/64 loss: -0.3274794816970825
Batch 21/64 loss: -0.3128860592842102
Batch 22/64 loss: -0.31553715467453003
Batch 23/64 loss: -0.3145850598812103
Batch 24/64 loss: -0.30876898765563965
Batch 25/64 loss: -0.32882511615753174
Batch 26/64 loss: -0.31335964798927307
Batch 27/64 loss: -0.3159598410129547
Batch 28/64 loss: -0.3086940050125122
Batch 29/64 loss: -0.33325961232185364
Batch 30/64 loss: -0.2980431914329529
Batch 31/64 loss: -0.32796525955200195
Batch 32/64 loss: -0.2893877327442169
Batch 33/64 loss: -0.34107840061187744
Batch 34/64 loss: -0.3333011567592621
Batch 35/64 loss: -0.33138197660446167
Batch 36/64 loss: -0.28460919857025146
Batch 37/64 loss: -0.31350618600845337
Batch 38/64 loss: -0.3387879729270935
Batch 39/64 loss: -0.33106791973114014
Batch 40/64 loss: -0.31954628229141235
Batch 41/64 loss: -0.28799670934677124
Batch 42/64 loss: -0.31187447905540466
Batch 43/64 loss: -0.34060484170913696
Batch 44/64 loss: -0.3464003801345825
Batch 45/64 loss: -0.30587077140808105
Batch 46/64 loss: -0.32062482833862305
Batch 47/64 loss: -0.3016446828842163
Batch 48/64 loss: -0.24499323964118958
Batch 49/64 loss: -0.31057995557785034
Batch 50/64 loss: -0.3259294033050537
Batch 51/64 loss: -0.3126610815525055
Batch 52/64 loss: -0.3427879214286804
Batch 53/64 loss: -0.309043824672699
Batch 54/64 loss: -0.33276310563087463
Batch 55/64 loss: -0.2965417504310608
Batch 56/64 loss: -0.3122268617153168
Batch 57/64 loss: -0.3275640308856964
Batch 58/64 loss: -0.3077555298805237
Batch 59/64 loss: -0.30836907029151917
Batch 60/64 loss: -0.3161897659301758
Batch 61/64 loss: -0.33601972460746765
Batch 62/64 loss: -0.3399285674095154
Batch 63/64 loss: -0.29136061668395996
Batch 64/64 loss: -0.2897337079048157
Epoch 312  Train loss: -0.31635942809722  Val loss: 0.029882161477996723
Epoch 313
-------------------------------
Batch 1/64 loss: -0.3213551938533783
Batch 2/64 loss: -0.31832772493362427
Batch 3/64 loss: -0.27605241537094116
Batch 4/64 loss: -0.32948023080825806
Batch 5/64 loss: -0.31310632824897766
Batch 6/64 loss: -0.3367639183998108
Batch 7/64 loss: -0.32880181074142456
Batch 8/64 loss: -0.29588207602500916
Batch 9/64 loss: -0.354634165763855
Batch 10/64 loss: -0.35839563608169556
Batch 11/64 loss: -0.32355427742004395
Batch 12/64 loss: -0.3681323528289795
Batch 13/64 loss: -0.2938694953918457
Batch 14/64 loss: -0.3237972557544708
Batch 15/64 loss: -0.3450636565685272
Batch 16/64 loss: -0.31288182735443115
Batch 17/64 loss: -0.29513105750083923
Batch 18/64 loss: -0.32749849557876587
Batch 19/64 loss: -0.28845643997192383
Batch 20/64 loss: -0.28521230816841125
Batch 21/64 loss: -0.3453493118286133
Batch 22/64 loss: -0.35530173778533936
Batch 23/64 loss: -0.31706860661506653
Batch 24/64 loss: -0.33383363485336304
Batch 25/64 loss: -0.3471031188964844
Batch 26/64 loss: -0.31201595067977905
Batch 27/64 loss: -0.351429283618927
Batch 28/64 loss: -0.3483876585960388
Batch 29/64 loss: -0.3255945146083832
Batch 30/64 loss: -0.33431944251060486
Batch 31/64 loss: -0.32753700017929077
Batch 32/64 loss: -0.31708985567092896
Batch 33/64 loss: -0.3421323299407959
Batch 34/64 loss: -0.3120168447494507
Batch 35/64 loss: -0.32875752449035645
Batch 36/64 loss: -0.30265602469444275
Batch 37/64 loss: -0.3015638291835785
Batch 38/64 loss: -0.3117113709449768
Batch 39/64 loss: -0.3166739344596863
Batch 40/64 loss: -0.31257715821266174
Batch 41/64 loss: -0.32318153977394104
Batch 42/64 loss: -0.31987103819847107
Batch 43/64 loss: -0.32754993438720703
Batch 44/64 loss: -0.30873483419418335
Batch 45/64 loss: -0.3120921850204468
Batch 46/64 loss: -0.3302214443683624
Batch 47/64 loss: -0.3340648412704468
Batch 48/64 loss: -0.291856050491333
Batch 49/64 loss: -0.2907426357269287
Batch 50/64 loss: -0.32912135124206543
Batch 51/64 loss: -0.2743183970451355
Batch 52/64 loss: -0.34041067957878113
Batch 53/64 loss: -0.29234033823013306
Batch 54/64 loss: -0.3111618757247925
Batch 55/64 loss: -0.30596670508384705
Batch 56/64 loss: -0.3317324221134186
Batch 57/64 loss: -0.2998800277709961
Batch 58/64 loss: -0.3230839967727661
Batch 59/64 loss: -0.3374200761318207
Batch 60/64 loss: -0.326651930809021
Batch 61/64 loss: -0.3035354018211365
Batch 62/64 loss: -0.27384328842163086
Batch 63/64 loss: -0.3355039358139038
Batch 64/64 loss: -0.2844025194644928
Epoch 313  Train loss: -0.31962513911957835  Val loss: 0.030389065799844225
Epoch 314
-------------------------------
Batch 1/64 loss: -0.27488434314727783
Batch 2/64 loss: -0.30176812410354614
Batch 3/64 loss: -0.3353644013404846
Batch 4/64 loss: -0.31742024421691895
Batch 5/64 loss: -0.3288941979408264
Batch 6/64 loss: -0.3053925633430481
Batch 7/64 loss: -0.3363221287727356
Batch 8/64 loss: -0.31816112995147705
Batch 9/64 loss: -0.27796244621276855
Batch 10/64 loss: -0.3246219754219055
Batch 11/64 loss: -0.349986732006073
Batch 12/64 loss: -0.32811039686203003
Batch 13/64 loss: -0.3293495178222656
Batch 14/64 loss: -0.32592451572418213
Batch 15/64 loss: -0.3132970631122589
Batch 16/64 loss: -0.30680352449417114
Batch 17/64 loss: -0.3234020173549652
Batch 18/64 loss: -0.3386216163635254
Batch 19/64 loss: -0.3262555003166199
Batch 20/64 loss: -0.3264836072921753
Batch 21/64 loss: -0.2832813262939453
Batch 22/64 loss: -0.33816951513290405
Batch 23/64 loss: -0.3271219730377197
Batch 24/64 loss: -0.2748783826828003
Batch 25/64 loss: -0.32148051261901855
Batch 26/64 loss: -0.3244333863258362
Batch 27/64 loss: -0.34640979766845703
Batch 28/64 loss: -0.34476935863494873
Batch 29/64 loss: -0.31996917724609375
Batch 30/64 loss: -0.3465157151222229
Batch 31/64 loss: -0.36518949270248413
Batch 32/64 loss: -0.33277109265327454
Batch 33/64 loss: -0.2960509657859802
Batch 34/64 loss: -0.3020293414592743
Batch 35/64 loss: -0.29964587092399597
Batch 36/64 loss: -0.31628701090812683
Batch 37/64 loss: -0.3095160126686096
Batch 38/64 loss: -0.3183045983314514
Batch 39/64 loss: -0.2728458046913147
Batch 40/64 loss: -0.3481518030166626
Batch 41/64 loss: -0.2931396961212158
Batch 42/64 loss: -0.3423537015914917
Batch 43/64 loss: -0.31986647844314575
Batch 44/64 loss: -0.31744420528411865
Batch 45/64 loss: -0.31186822056770325
Batch 46/64 loss: -0.2940340042114258
Batch 47/64 loss: -0.32323169708251953
Batch 48/64 loss: -0.3071296215057373
Batch 49/64 loss: -0.3096732497215271
Batch 50/64 loss: -0.29896920919418335
Batch 51/64 loss: -0.3000020384788513
Batch 52/64 loss: -0.29345935583114624
Batch 53/64 loss: -0.33133578300476074
Batch 54/64 loss: -0.2998473048210144
Batch 55/64 loss: -0.3558194041252136
Batch 56/64 loss: -0.3240925669670105
Batch 57/64 loss: -0.3172670304775238
Batch 58/64 loss: -0.3012942373752594
Batch 59/64 loss: -0.3212658762931824
Batch 60/64 loss: -0.3240458369255066
Batch 61/64 loss: -0.33395200967788696
Batch 62/64 loss: -0.30874526500701904
Batch 63/64 loss: -0.32062745094299316
Batch 64/64 loss: -0.3145373463630676
Epoch 314  Train loss: -0.31783865783728804  Val loss: 0.030846813904870415
Epoch 315
-------------------------------
Batch 1/64 loss: -0.304256409406662
Batch 2/64 loss: -0.3399405777454376
Batch 3/64 loss: -0.3244224488735199
Batch 4/64 loss: -0.30997467041015625
Batch 5/64 loss: -0.31225401163101196
Batch 6/64 loss: -0.2821394205093384
Batch 7/64 loss: -0.3528730273246765
Batch 8/64 loss: -0.31522682309150696
Batch 9/64 loss: -0.2543404698371887
Batch 10/64 loss: -0.35166990756988525
Batch 11/64 loss: -0.3195239305496216
Batch 12/64 loss: -0.29845526814460754
Batch 13/64 loss: -0.3137204647064209
Batch 14/64 loss: -0.27604931592941284
Batch 15/64 loss: -0.3052576780319214
Batch 16/64 loss: -0.3423072397708893
Batch 17/64 loss: -0.30406177043914795
Batch 18/64 loss: -0.31458503007888794
Batch 19/64 loss: -0.3445580005645752
Batch 20/64 loss: -0.3215508460998535
Batch 21/64 loss: -0.28168219327926636
Batch 22/64 loss: -0.31994765996932983
Batch 23/64 loss: -0.3060583472251892
Batch 24/64 loss: -0.3086772561073303
Batch 25/64 loss: -0.3161209523677826
Batch 26/64 loss: -0.31489723920822144
Batch 27/64 loss: -0.31801217794418335
Batch 28/64 loss: -0.35433632135391235
Batch 29/64 loss: -0.3203279376029968
Batch 30/64 loss: -0.3430599570274353
Batch 31/64 loss: -0.29150885343551636
Batch 32/64 loss: -0.33786171674728394
Batch 33/64 loss: -0.3177363872528076
Batch 34/64 loss: -0.2958102822303772
Batch 35/64 loss: -0.30315864086151123
Batch 36/64 loss: -0.3087824583053589
Batch 37/64 loss: -0.33560559153556824
Batch 38/64 loss: -0.33912500739097595
Batch 39/64 loss: -0.2936058044433594
Batch 40/64 loss: -0.312061071395874
Batch 41/64 loss: -0.3217058777809143
Batch 42/64 loss: -0.33888256549835205
Batch 43/64 loss: -0.29526397585868835
Batch 44/64 loss: -0.316012442111969
Batch 45/64 loss: -0.3376852869987488
Batch 46/64 loss: -0.2945384979248047
Batch 47/64 loss: -0.35813993215560913
Batch 48/64 loss: -0.31773126125335693
Batch 49/64 loss: -0.331631600856781
Batch 50/64 loss: -0.29242080450057983
Batch 51/64 loss: -0.3353043794631958
Batch 52/64 loss: -0.28479981422424316
Batch 53/64 loss: -0.3284252882003784
Batch 54/64 loss: -0.3212762475013733
Batch 55/64 loss: -0.31105273962020874
Batch 56/64 loss: -0.30830442905426025
Batch 57/64 loss: -0.34950345754623413
Batch 58/64 loss: -0.314382404088974
Batch 59/64 loss: -0.33468249440193176
Batch 60/64 loss: -0.2996993660926819
Batch 61/64 loss: -0.3247085213661194
Batch 62/64 loss: -0.32706761360168457
Batch 63/64 loss: -0.31779634952545166
Batch 64/64 loss: -0.3449498414993286
Epoch 315  Train loss: -0.31725915132784377  Val loss: 0.02746025536887834
Epoch 316
-------------------------------
Batch 1/64 loss: -0.33673396706581116
Batch 2/64 loss: -0.33920779824256897
Batch 3/64 loss: -0.33989715576171875
Batch 4/64 loss: -0.3714449405670166
Batch 5/64 loss: -0.3226446211338043
Batch 6/64 loss: -0.3134925663471222
Batch 7/64 loss: -0.3309497833251953
Batch 8/64 loss: -0.33187663555145264
Batch 9/64 loss: -0.34709179401397705
Batch 10/64 loss: -0.3220405876636505
Batch 11/64 loss: -0.3200572729110718
Batch 12/64 loss: -0.3267289996147156
Batch 13/64 loss: -0.2929270565509796
Batch 14/64 loss: -0.3345227539539337
Batch 15/64 loss: -0.3204631209373474
Batch 16/64 loss: -0.31473737955093384
Batch 17/64 loss: -0.34048721194267273
Batch 18/64 loss: -0.3485034108161926
Batch 19/64 loss: -0.30718469619750977
Batch 20/64 loss: -0.3185494542121887
Batch 21/64 loss: -0.30526003241539
Batch 22/64 loss: -0.3220013976097107
Batch 23/64 loss: -0.31188011169433594
Batch 24/64 loss: -0.2978706955909729
Batch 25/64 loss: -0.2989085018634796
Batch 26/64 loss: -0.28469640016555786
Batch 27/64 loss: -0.3324735164642334
Batch 28/64 loss: -0.3250282406806946
Batch 29/64 loss: -0.32345977425575256
Batch 30/64 loss: -0.332435667514801
Batch 31/64 loss: -0.33999699354171753
Batch 32/64 loss: -0.32071200013160706
Batch 33/64 loss: -0.3204401135444641
Batch 34/64 loss: -0.33931124210357666
Batch 35/64 loss: -0.2906348705291748
Batch 36/64 loss: -0.3199867010116577
Batch 37/64 loss: -0.3260810673236847
Batch 38/64 loss: -0.3278442621231079
Batch 39/64 loss: -0.3480844497680664
Batch 40/64 loss: -0.29713183641433716
Batch 41/64 loss: -0.3366329073905945
Batch 42/64 loss: -0.3345525860786438
Batch 43/64 loss: -0.3127437233924866
Batch 44/64 loss: -0.32632797956466675
Batch 45/64 loss: -0.35068029165267944
Batch 46/64 loss: -0.3557807207107544
Batch 47/64 loss: -0.307041734457016
Batch 48/64 loss: -0.3053758144378662
Batch 49/64 loss: -0.2785847783088684
Batch 50/64 loss: -0.31732434034347534
Batch 51/64 loss: -0.3161163032054901
Batch 52/64 loss: -0.30701708793640137
Batch 53/64 loss: -0.3266788423061371
Batch 54/64 loss: -0.30042368173599243
Batch 55/64 loss: -0.3184025287628174
Batch 56/64 loss: -0.29498806595802307
Batch 57/64 loss: -0.29474347829818726
Batch 58/64 loss: -0.3084080219268799
Batch 59/64 loss: -0.2694249749183655
Batch 60/64 loss: -0.3323354125022888
Batch 61/64 loss: -0.29757848381996155
Batch 62/64 loss: -0.32804399728775024
Batch 63/64 loss: -0.29775604605674744
Batch 64/64 loss: -0.3486408293247223
Epoch 316  Train loss: -0.32034857261414623  Val loss: 0.030922334218762584
Epoch 317
-------------------------------
Batch 1/64 loss: -0.3074299693107605
Batch 2/64 loss: -0.2824925184249878
Batch 3/64 loss: -0.33230215311050415
Batch 4/64 loss: -0.3480224907398224
Batch 5/64 loss: -0.32954180240631104
Batch 6/64 loss: -0.3316742777824402
Batch 7/64 loss: -0.35286933183670044
Batch 8/64 loss: -0.33547890186309814
Batch 9/64 loss: -0.32707643508911133
Batch 10/64 loss: -0.34674105048179626
Batch 11/64 loss: -0.31572189927101135
Batch 12/64 loss: -0.2906206548213959
Batch 13/64 loss: -0.2934625744819641
Batch 14/64 loss: -0.34522679448127747
Batch 15/64 loss: -0.3386121988296509
Batch 16/64 loss: -0.3165132701396942
Batch 17/64 loss: -0.3430625796318054
Batch 18/64 loss: -0.35799720883369446
Batch 19/64 loss: -0.3331075608730316
Batch 20/64 loss: -0.33505702018737793
Batch 21/64 loss: -0.32641708850860596
Batch 22/64 loss: -0.2805924415588379
Batch 23/64 loss: -0.25381284952163696
Batch 24/64 loss: -0.3095778822898865
Batch 25/64 loss: -0.3341473340988159
Batch 26/64 loss: -0.2813414931297302
Batch 27/64 loss: -0.3194199204444885
Batch 28/64 loss: -0.3058917820453644
Batch 29/64 loss: -0.3515651226043701
Batch 30/64 loss: -0.33500608801841736
Batch 31/64 loss: -0.32786816358566284
Batch 32/64 loss: -0.3093865215778351
Batch 33/64 loss: -0.32265007495880127
Batch 34/64 loss: -0.3333684206008911
Batch 35/64 loss: -0.34010058641433716
Batch 36/64 loss: -0.31464824080467224
Batch 37/64 loss: -0.32595813274383545
Batch 38/64 loss: -0.33227258920669556
Batch 39/64 loss: -0.26772457361221313
Batch 40/64 loss: -0.22672498226165771
Batch 41/64 loss: -0.33244264125823975
Batch 42/64 loss: -0.29662182927131653
Batch 43/64 loss: -0.2581445574760437
Batch 44/64 loss: -0.30802491307258606
Batch 45/64 loss: -0.3033730983734131
Batch 46/64 loss: -0.32090893387794495
Batch 47/64 loss: -0.3095901310443878
Batch 48/64 loss: -0.30793076753616333
Batch 49/64 loss: -0.327910453081131
Batch 50/64 loss: -0.299049437046051
Batch 51/64 loss: -0.32892006635665894
Batch 52/64 loss: -0.30847370624542236
Batch 53/64 loss: -0.3099713921546936
Batch 54/64 loss: -0.306196391582489
Batch 55/64 loss: -0.30227649211883545
Batch 56/64 loss: -0.3300689458847046
Batch 57/64 loss: -0.31769007444381714
Batch 58/64 loss: -0.31487834453582764
Batch 59/64 loss: -0.3396790623664856
Batch 60/64 loss: -0.28817906975746155
Batch 61/64 loss: -0.3247748613357544
Batch 62/64 loss: -0.33934682607650757
Batch 63/64 loss: -0.2825111150741577
Batch 64/64 loss: -0.3140372037887573
Epoch 317  Train loss: -0.3161407998963898  Val loss: 0.030632445697522246
Epoch 318
-------------------------------
Batch 1/64 loss: -0.3453950881958008
Batch 2/64 loss: -0.3537386357784271
Batch 3/64 loss: -0.33809345960617065
Batch 4/64 loss: -0.3312110900878906
Batch 5/64 loss: -0.33805400133132935
Batch 6/64 loss: -0.3025062084197998
Batch 7/64 loss: -0.3321889042854309
Batch 8/64 loss: -0.3011266589164734
Batch 9/64 loss: -0.31763771176338196
Batch 10/64 loss: -0.30914419889450073
Batch 11/64 loss: -0.3148021996021271
Batch 12/64 loss: -0.3240918517112732
Batch 13/64 loss: -0.29618188738822937
Batch 14/64 loss: -0.3273034691810608
Batch 15/64 loss: -0.3189418911933899
Batch 16/64 loss: -0.3525335192680359
Batch 17/64 loss: -0.29006248712539673
Batch 18/64 loss: -0.3635922074317932
Batch 19/64 loss: -0.29297375679016113
Batch 20/64 loss: -0.34132179617881775
Batch 21/64 loss: -0.31749483942985535
Batch 22/64 loss: -0.33124563097953796
Batch 23/64 loss: -0.32697197794914246
Batch 24/64 loss: -0.33745890855789185
Batch 25/64 loss: -0.3014410138130188
Batch 26/64 loss: -0.32342618703842163
Batch 27/64 loss: -0.32145628333091736
Batch 28/64 loss: -0.325695276260376
Batch 29/64 loss: -0.3110958933830261
Batch 30/64 loss: -0.3396342694759369
Batch 31/64 loss: -0.3313777446746826
Batch 32/64 loss: -0.29630690813064575
Batch 33/64 loss: -0.32286888360977173
Batch 34/64 loss: -0.3288719356060028
Batch 35/64 loss: -0.33966004848480225
Batch 36/64 loss: -0.28556954860687256
Batch 37/64 loss: -0.30403637886047363
Batch 38/64 loss: -0.29424262046813965
Batch 39/64 loss: -0.3104201555252075
Batch 40/64 loss: -0.3363747298717499
Batch 41/64 loss: -0.3006948232650757
Batch 42/64 loss: -0.30925533175468445
Batch 43/64 loss: -0.3093397617340088
Batch 44/64 loss: -0.3124690055847168
Batch 45/64 loss: -0.29692453145980835
Batch 46/64 loss: -0.35384559631347656
Batch 47/64 loss: -0.32359999418258667
Batch 48/64 loss: -0.33958232402801514
Batch 49/64 loss: -0.3373486399650574
Batch 50/64 loss: -0.3236609697341919
Batch 51/64 loss: -0.29282957315444946
Batch 52/64 loss: -0.3389825224876404
Batch 53/64 loss: -0.33166056871414185
Batch 54/64 loss: -0.3107762336730957
Batch 55/64 loss: -0.3198357820510864
Batch 56/64 loss: -0.3379550576210022
Batch 57/64 loss: -0.30130523443222046
Batch 58/64 loss: -0.32956305146217346
Batch 59/64 loss: -0.30818504095077515
Batch 60/64 loss: -0.3028632402420044
Batch 61/64 loss: -0.3136909008026123
Batch 62/64 loss: -0.33426687121391296
Batch 63/64 loss: -0.32432347536087036
Batch 64/64 loss: -0.3430543541908264
Epoch 318  Train loss: -0.3213615616162618  Val loss: 0.02888413331762622
Epoch 319
-------------------------------
Batch 1/64 loss: -0.3273686468601227
Batch 2/64 loss: -0.2957732677459717
Batch 3/64 loss: -0.32614588737487793
Batch 4/64 loss: -0.311132550239563
Batch 5/64 loss: -0.3618742823600769
Batch 6/64 loss: -0.29432791471481323
Batch 7/64 loss: -0.32258668541908264
Batch 8/64 loss: -0.3486544191837311
Batch 9/64 loss: -0.2930753231048584
Batch 10/64 loss: -0.33810150623321533
Batch 11/64 loss: -0.3106529712677002
Batch 12/64 loss: -0.33591926097869873
Batch 13/64 loss: -0.33355003595352173
Batch 14/64 loss: -0.31059032678604126
Batch 15/64 loss: -0.34018611907958984
Batch 16/64 loss: -0.3043968081474304
Batch 17/64 loss: -0.31001704931259155
Batch 18/64 loss: -0.32553836703300476
Batch 19/64 loss: -0.3448299169540405
Batch 20/64 loss: -0.25182271003723145
Batch 21/64 loss: -0.32138386368751526
Batch 22/64 loss: -0.28414928913116455
Batch 23/64 loss: -0.33742260932922363
Batch 24/64 loss: -0.32495546340942383
Batch 25/64 loss: -0.3195664882659912
Batch 26/64 loss: -0.31887051463127136
Batch 27/64 loss: -0.294150710105896
Batch 28/64 loss: -0.32986098527908325
Batch 29/64 loss: -0.3176056742668152
Batch 30/64 loss: -0.3312232792377472
Batch 31/64 loss: -0.30752214789390564
Batch 32/64 loss: -0.3523321747779846
Batch 33/64 loss: -0.330299973487854
Batch 34/64 loss: -0.3529439866542816
Batch 35/64 loss: -0.3091318607330322
Batch 36/64 loss: -0.32708877325057983
Batch 37/64 loss: -0.33717402815818787
Batch 38/64 loss: -0.3048521876335144
Batch 39/64 loss: -0.3176421523094177
Batch 40/64 loss: -0.35181137919425964
Batch 41/64 loss: -0.3193272054195404
Batch 42/64 loss: -0.27698880434036255
Batch 43/64 loss: -0.3390987515449524
Batch 44/64 loss: -0.3038041591644287
Batch 45/64 loss: -0.2933582663536072
Batch 46/64 loss: -0.32299110293388367
Batch 47/64 loss: -0.31408342719078064
Batch 48/64 loss: -0.2857733964920044
Batch 49/64 loss: -0.3224859833717346
Batch 50/64 loss: -0.30524563789367676
Batch 51/64 loss: -0.32761144638061523
Batch 52/64 loss: -0.30627042055130005
Batch 53/64 loss: -0.29396766424179077
Batch 54/64 loss: -0.3152739405632019
Batch 55/64 loss: -0.35016748309135437
Batch 56/64 loss: -0.34494855999946594
Batch 57/64 loss: -0.30214715003967285
Batch 58/64 loss: -0.2874870300292969
Batch 59/64 loss: -0.31537848711013794
Batch 60/64 loss: -0.31737783551216125
Batch 61/64 loss: -0.31840914487838745
Batch 62/64 loss: -0.33887094259262085
Batch 63/64 loss: -0.3383416533470154
Batch 64/64 loss: -0.3602398931980133
Epoch 319  Train loss: -0.31946854906923633  Val loss: 0.02828493253471925
Epoch 320
-------------------------------
Batch 1/64 loss: -0.3440902531147003
Batch 2/64 loss: -0.3435271978378296
Batch 3/64 loss: -0.3420616090297699
Batch 4/64 loss: -0.33508890867233276
Batch 5/64 loss: -0.31210070848464966
Batch 6/64 loss: -0.3389556407928467
Batch 7/64 loss: -0.3338069021701813
Batch 8/64 loss: -0.3325931429862976
Batch 9/64 loss: -0.37319812178611755
Batch 10/64 loss: -0.3079889118671417
Batch 11/64 loss: -0.34107834100723267
Batch 12/64 loss: -0.33552706241607666
Batch 13/64 loss: -0.31317415833473206
Batch 14/64 loss: -0.32765525579452515
Batch 15/64 loss: -0.2912391722202301
Batch 16/64 loss: -0.3042893409729004
Batch 17/64 loss: -0.34411025047302246
Batch 18/64 loss: -0.321704238653183
Batch 19/64 loss: -0.2673940062522888
Batch 20/64 loss: -0.31854337453842163
Batch 21/64 loss: -0.328251451253891
Batch 22/64 loss: -0.31755614280700684
Batch 23/64 loss: -0.3425045609474182
Batch 24/64 loss: -0.30472591519355774
Batch 25/64 loss: -0.30973076820373535
Batch 26/64 loss: -0.3078828454017639
Batch 27/64 loss: -0.3609129786491394
Batch 28/64 loss: -0.3304606080055237
Batch 29/64 loss: -0.28892725706100464
Batch 30/64 loss: -0.2992458939552307
Batch 31/64 loss: -0.3258986473083496
Batch 32/64 loss: -0.3425936996936798
Batch 33/64 loss: -0.3409581482410431
Batch 34/64 loss: -0.28261598944664
Batch 35/64 loss: -0.29359856247901917
Batch 36/64 loss: -0.33619382977485657
Batch 37/64 loss: -0.3336208462715149
Batch 38/64 loss: -0.29565784335136414
Batch 39/64 loss: -0.31258291006088257
Batch 40/64 loss: -0.3299611210823059
Batch 41/64 loss: -0.31444716453552246
Batch 42/64 loss: -0.29969125986099243
Batch 43/64 loss: -0.32673370838165283
Batch 44/64 loss: -0.34217122197151184
Batch 45/64 loss: -0.33117926120758057
Batch 46/64 loss: -0.337488055229187
Batch 47/64 loss: -0.3610793948173523
Batch 48/64 loss: -0.33305907249450684
Batch 49/64 loss: -0.306077778339386
Batch 50/64 loss: -0.2896387577056885
Batch 51/64 loss: -0.35173124074935913
Batch 52/64 loss: -0.28344619274139404
Batch 53/64 loss: -0.2880116105079651
Batch 54/64 loss: -0.3324728012084961
Batch 55/64 loss: -0.3460845351219177
Batch 56/64 loss: -0.2619646191596985
Batch 57/64 loss: -0.3318261504173279
Batch 58/64 loss: -0.2894146144390106
Batch 59/64 loss: -0.3385455012321472
Batch 60/64 loss: -0.3099655508995056
Batch 61/64 loss: -0.3162368834018707
Batch 62/64 loss: -0.283612996339798
Batch 63/64 loss: -0.3435601592063904
Batch 64/64 loss: -0.34388548135757446
Epoch 320  Train loss: -0.3213860275698643  Val loss: 0.02801765281310196
Epoch 321
-------------------------------
Batch 1/64 loss: -0.3318330943584442
Batch 2/64 loss: -0.3615051507949829
Batch 3/64 loss: -0.35038191080093384
Batch 4/64 loss: -0.365561306476593
Batch 5/64 loss: -0.34460365772247314
Batch 6/64 loss: -0.3341236412525177
Batch 7/64 loss: -0.34654539823532104
Batch 8/64 loss: -0.3114592432975769
Batch 9/64 loss: -0.3273918032646179
Batch 10/64 loss: -0.34168052673339844
Batch 11/64 loss: -0.32700949907302856
Batch 12/64 loss: -0.35268181562423706
Batch 13/64 loss: -0.3248038589954376
Batch 14/64 loss: -0.2853546738624573
Batch 15/64 loss: -0.33032625913619995
Batch 16/64 loss: -0.3040018081665039
Batch 17/64 loss: -0.31296306848526
Batch 18/64 loss: -0.3292722702026367
Batch 19/64 loss: -0.3253270089626312
Batch 20/64 loss: -0.3267970085144043
Batch 21/64 loss: -0.3274935483932495
Batch 22/64 loss: -0.3342360258102417
Batch 23/64 loss: -0.30868709087371826
Batch 24/64 loss: -0.3349416255950928
Batch 25/64 loss: -0.33534181118011475
Batch 26/64 loss: -0.27565157413482666
Batch 27/64 loss: -0.31087374687194824
Batch 28/64 loss: -0.295059472322464
Batch 29/64 loss: -0.30090558528900146
Batch 30/64 loss: -0.2781233489513397
Batch 31/64 loss: -0.31219184398651123
Batch 32/64 loss: -0.32368203997612
Batch 33/64 loss: -0.2739828824996948
Batch 34/64 loss: -0.3325514793395996
Batch 35/64 loss: -0.28417181968688965
Batch 36/64 loss: -0.34001970291137695
Batch 37/64 loss: -0.3350982964038849
Batch 38/64 loss: -0.3406611382961273
Batch 39/64 loss: -0.3221972584724426
Batch 40/64 loss: -0.3322233557701111
Batch 41/64 loss: -0.3183618187904358
Batch 42/64 loss: -0.31904637813568115
Batch 43/64 loss: -0.3219106197357178
Batch 44/64 loss: -0.32065045833587646
Batch 45/64 loss: -0.35711249709129333
Batch 46/64 loss: -0.2942562699317932
Batch 47/64 loss: -0.3238416314125061
Batch 48/64 loss: -0.3322003483772278
Batch 49/64 loss: -0.35177111625671387
Batch 50/64 loss: -0.29624152183532715
Batch 51/64 loss: -0.3134480118751526
Batch 52/64 loss: -0.29401785135269165
Batch 53/64 loss: -0.31718629598617554
Batch 54/64 loss: -0.30430203676223755
Batch 55/64 loss: -0.3277261555194855
Batch 56/64 loss: -0.31596678495407104
Batch 57/64 loss: -0.3103499710559845
Batch 58/64 loss: -0.30228912830352783
Batch 59/64 loss: -0.330912709236145
Batch 60/64 loss: -0.33635568618774414
Batch 61/64 loss: -0.31900572776794434
Batch 62/64 loss: -0.31786829233169556
Batch 63/64 loss: -0.3136965036392212
Batch 64/64 loss: -0.3134457468986511
Epoch 321  Train loss: -0.32165205922781254  Val loss: 0.029002617314918756
Epoch 322
-------------------------------
Batch 1/64 loss: -0.31678080558776855
Batch 2/64 loss: -0.29906541109085083
Batch 3/64 loss: -0.35060715675354004
Batch 4/64 loss: -0.3239123821258545
Batch 5/64 loss: -0.34234556555747986
Batch 6/64 loss: -0.3089170753955841
Batch 7/64 loss: -0.27655887603759766
Batch 8/64 loss: -0.3373633623123169
Batch 9/64 loss: -0.3523028492927551
Batch 10/64 loss: -0.2805188298225403
Batch 11/64 loss: -0.3273877501487732
Batch 12/64 loss: -0.31985390186309814
Batch 13/64 loss: -0.3363673985004425
Batch 14/64 loss: -0.3281747102737427
Batch 15/64 loss: -0.3716161847114563
Batch 16/64 loss: -0.32025936245918274
Batch 17/64 loss: -0.32701122760772705
Batch 18/64 loss: -0.3083261251449585
Batch 19/64 loss: -0.29441988468170166
Batch 20/64 loss: -0.3310273289680481
Batch 21/64 loss: -0.3377964198589325
Batch 22/64 loss: -0.2784130871295929
Batch 23/64 loss: -0.3492743670940399
Batch 24/64 loss: -0.34098657965660095
Batch 25/64 loss: -0.3515487313270569
Batch 26/64 loss: -0.3346030116081238
Batch 27/64 loss: -0.3189309537410736
Batch 28/64 loss: -0.3144676089286804
Batch 29/64 loss: -0.3263013958930969
Batch 30/64 loss: -0.37660905718803406
Batch 31/64 loss: -0.305155485868454
Batch 32/64 loss: -0.2525264620780945
Batch 33/64 loss: -0.3286181092262268
Batch 34/64 loss: -0.2921839952468872
Batch 35/64 loss: -0.3334987163543701
Batch 36/64 loss: -0.2958025336265564
Batch 37/64 loss: -0.26314422488212585
Batch 38/64 loss: -0.324577271938324
Batch 39/64 loss: -0.3346472382545471
Batch 40/64 loss: -0.3397926092147827
Batch 41/64 loss: -0.2767733037471771
Batch 42/64 loss: -0.33912765979766846
Batch 43/64 loss: -0.3511192798614502
Batch 44/64 loss: -0.3352338671684265
Batch 45/64 loss: -0.315091997385025
Batch 46/64 loss: -0.33030202984809875
Batch 47/64 loss: -0.30920493602752686
Batch 48/64 loss: -0.2811196744441986
Batch 49/64 loss: -0.3087053894996643
Batch 50/64 loss: -0.34743615984916687
Batch 51/64 loss: -0.3043476343154907
Batch 52/64 loss: -0.30711501836776733
Batch 53/64 loss: -0.2741984724998474
Batch 54/64 loss: -0.32152098417282104
Batch 55/64 loss: -0.3189566135406494
Batch 56/64 loss: -0.3168830871582031
Batch 57/64 loss: -0.28394606709480286
Batch 58/64 loss: -0.314924031496048
Batch 59/64 loss: -0.31427130103111267
Batch 60/64 loss: -0.3260771632194519
Batch 61/64 loss: -0.32778218388557434
Batch 62/64 loss: -0.27181845903396606
Batch 63/64 loss: -0.328935831785202
Batch 64/64 loss: -0.32074347138404846
Epoch 322  Train loss: -0.3183865536661709  Val loss: 0.028557088571725432
Epoch 323
-------------------------------
Batch 1/64 loss: -0.3114708662033081
Batch 2/64 loss: -0.3213883638381958
Batch 3/64 loss: -0.34238293766975403
Batch 4/64 loss: -0.3135954737663269
Batch 5/64 loss: -0.2994873821735382
Batch 6/64 loss: -0.340435266494751
Batch 7/64 loss: -0.3411628007888794
Batch 8/64 loss: -0.3339942693710327
Batch 9/64 loss: -0.28503966331481934
Batch 10/64 loss: -0.34149909019470215
Batch 11/64 loss: -0.3437424302101135
Batch 12/64 loss: -0.35265135765075684
Batch 13/64 loss: -0.33387842774391174
Batch 14/64 loss: -0.3199233114719391
Batch 15/64 loss: -0.26580384373664856
Batch 16/64 loss: -0.27323219180107117
Batch 17/64 loss: -0.31794947385787964
Batch 18/64 loss: -0.3242322504520416
Batch 19/64 loss: -0.3584609031677246
Batch 20/64 loss: -0.3320145606994629
Batch 21/64 loss: -0.313651442527771
Batch 22/64 loss: -0.312852680683136
Batch 23/64 loss: -0.3138493299484253
Batch 24/64 loss: -0.317648708820343
Batch 25/64 loss: -0.34240031242370605
Batch 26/64 loss: -0.33974093198776245
Batch 27/64 loss: -0.3426470160484314
Batch 28/64 loss: -0.3147815465927124
Batch 29/64 loss: -0.3034731149673462
Batch 30/64 loss: -0.26612627506256104
Batch 31/64 loss: -0.306296706199646
Batch 32/64 loss: -0.32851696014404297
Batch 33/64 loss: -0.3354260325431824
Batch 34/64 loss: -0.32728785276412964
Batch 35/64 loss: -0.35406601428985596
Batch 36/64 loss: -0.3010798394680023
Batch 37/64 loss: -0.34384599328041077
Batch 38/64 loss: -0.29170721769332886
Batch 39/64 loss: -0.32875484228134155
Batch 40/64 loss: -0.3143363893032074
Batch 41/64 loss: -0.31310349702835083
Batch 42/64 loss: -0.33974671363830566
Batch 43/64 loss: -0.33123549818992615
Batch 44/64 loss: -0.3448694348335266
Batch 45/64 loss: -0.3531397581100464
Batch 46/64 loss: -0.3403097987174988
Batch 47/64 loss: -0.31322911381721497
Batch 48/64 loss: -0.298224538564682
Batch 49/64 loss: -0.33753734827041626
Batch 50/64 loss: -0.2873770296573639
Batch 51/64 loss: -0.3558817505836487
Batch 52/64 loss: -0.33630096912384033
Batch 53/64 loss: -0.32982274889945984
Batch 54/64 loss: -0.3140561282634735
Batch 55/64 loss: -0.30494654178619385
Batch 56/64 loss: -0.3392125368118286
Batch 57/64 loss: -0.3597659468650818
Batch 58/64 loss: -0.32663199305534363
Batch 59/64 loss: -0.33803337812423706
Batch 60/64 loss: -0.32156550884246826
Batch 61/64 loss: -0.29817938804626465
Batch 62/64 loss: -0.32141873240470886
Batch 63/64 loss: -0.32386863231658936
Batch 64/64 loss: -0.322468101978302
Epoch 323  Train loss: -0.3234688962207121  Val loss: 0.030547159234273064
Epoch 324
-------------------------------
Batch 1/64 loss: -0.2965202331542969
Batch 2/64 loss: -0.32646629214286804
Batch 3/64 loss: -0.3512650728225708
Batch 4/64 loss: -0.32721930742263794
Batch 5/64 loss: -0.33434802293777466
Batch 6/64 loss: -0.3500322699546814
Batch 7/64 loss: -0.31204459071159363
Batch 8/64 loss: -0.32157081365585327
Batch 9/64 loss: -0.3378673791885376
Batch 10/64 loss: -0.3525524139404297
Batch 11/64 loss: -0.3316197991371155
Batch 12/64 loss: -0.3400535583496094
Batch 13/64 loss: -0.28693535923957825
Batch 14/64 loss: -0.2606319785118103
Batch 15/64 loss: -0.32786983251571655
Batch 16/64 loss: -0.3323286473751068
Batch 17/64 loss: -0.3601110577583313
Batch 18/64 loss: -0.31743234395980835
Batch 19/64 loss: -0.34564292430877686
Batch 20/64 loss: -0.3360961675643921
Batch 21/64 loss: -0.306628942489624
Batch 22/64 loss: -0.3268588185310364
Batch 23/64 loss: -0.33844369649887085
Batch 24/64 loss: -0.3562292456626892
Batch 25/64 loss: -0.3303752541542053
Batch 26/64 loss: -0.31647080183029175
Batch 27/64 loss: -0.3308558464050293
Batch 28/64 loss: -0.29536718130111694
Batch 29/64 loss: -0.3228524625301361
Batch 30/64 loss: -0.32349538803100586
Batch 31/64 loss: -0.2617518901824951
Batch 32/64 loss: -0.2817336320877075
Batch 33/64 loss: -0.34526458382606506
Batch 34/64 loss: -0.3019396662712097
Batch 35/64 loss: -0.34182411432266235
Batch 36/64 loss: -0.33788731694221497
Batch 37/64 loss: -0.33405521512031555
Batch 38/64 loss: -0.34039098024368286
Batch 39/64 loss: -0.3397662341594696
Batch 40/64 loss: -0.31824398040771484
Batch 41/64 loss: -0.30565834045410156
Batch 42/64 loss: -0.3250138759613037
Batch 43/64 loss: -0.3513585925102234
Batch 44/64 loss: -0.32939499616622925
Batch 45/64 loss: -0.3185729384422302
Batch 46/64 loss: -0.30482497811317444
Batch 47/64 loss: -0.32051143050193787
Batch 48/64 loss: -0.27842214703559875
Batch 49/64 loss: -0.3241143226623535
Batch 50/64 loss: -0.31683284044265747
Batch 51/64 loss: -0.34447181224823
Batch 52/64 loss: -0.3285673260688782
Batch 53/64 loss: -0.33220043778419495
Batch 54/64 loss: -0.3143911361694336
Batch 55/64 loss: -0.32406681776046753
Batch 56/64 loss: -0.3188983201980591
Batch 57/64 loss: -0.33278876543045044
Batch 58/64 loss: -0.3302496075630188
Batch 59/64 loss: -0.3364494740962982
Batch 60/64 loss: -0.3251740038394928
Batch 61/64 loss: -0.3495887517929077
Batch 62/64 loss: -0.3195476830005646
Batch 63/64 loss: -0.3426779508590698
Batch 64/64 loss: -0.34760522842407227
Epoch 324  Train loss: -0.3252317456638112  Val loss: 0.028474011576872103
Epoch 325
-------------------------------
Batch 1/64 loss: -0.31858009099960327
Batch 2/64 loss: -0.3493923544883728
Batch 3/64 loss: -0.33794283866882324
Batch 4/64 loss: -0.3380051255226135
Batch 5/64 loss: -0.3289046585559845
Batch 6/64 loss: -0.3337722420692444
Batch 7/64 loss: -0.32048922777175903
Batch 8/64 loss: -0.3163481056690216
Batch 9/64 loss: -0.34113025665283203
Batch 10/64 loss: -0.33016014099121094
Batch 11/64 loss: -0.3482568562030792
Batch 12/64 loss: -0.3605347275733948
Batch 13/64 loss: -0.3230123519897461
Batch 14/64 loss: -0.3236725628376007
Batch 15/64 loss: -0.3077159523963928
Batch 16/64 loss: -0.341375470161438
Batch 17/64 loss: -0.3216504156589508
Batch 18/64 loss: -0.3139050602912903
Batch 19/64 loss: -0.27608829736709595
Batch 20/64 loss: -0.3390304744243622
Batch 21/64 loss: -0.3170223534107208
Batch 22/64 loss: -0.3452126681804657
Batch 23/64 loss: -0.31709611415863037
Batch 24/64 loss: -0.31043097376823425
Batch 25/64 loss: -0.30308997631073
Batch 26/64 loss: -0.31601154804229736
Batch 27/64 loss: -0.3195539116859436
Batch 28/64 loss: -0.30531805753707886
Batch 29/64 loss: -0.34280988574028015
Batch 30/64 loss: -0.3457973897457123
Batch 31/64 loss: -0.33119040727615356
Batch 32/64 loss: -0.31285297870635986
Batch 33/64 loss: -0.29874667525291443
Batch 34/64 loss: -0.3357124328613281
Batch 35/64 loss: -0.335286021232605
Batch 36/64 loss: -0.3411630392074585
Batch 37/64 loss: -0.29994136095046997
Batch 38/64 loss: -0.34526342153549194
Batch 39/64 loss: -0.31406229734420776
Batch 40/64 loss: -0.3137967884540558
Batch 41/64 loss: -0.33260005712509155
Batch 42/64 loss: -0.3371545672416687
Batch 43/64 loss: -0.31453806161880493
Batch 44/64 loss: -0.33263957500457764
Batch 45/64 loss: -0.26256582140922546
Batch 46/64 loss: -0.3362012505531311
Batch 47/64 loss: -0.33902835845947266
Batch 48/64 loss: -0.31897228956222534
Batch 49/64 loss: -0.3468371331691742
Batch 50/64 loss: -0.36530131101608276
Batch 51/64 loss: -0.34159183502197266
Batch 52/64 loss: -0.3321188688278198
Batch 53/64 loss: -0.2967640161514282
Batch 54/64 loss: -0.3153761029243469
Batch 55/64 loss: -0.3323218822479248
Batch 56/64 loss: -0.31656280159950256
Batch 57/64 loss: -0.3126083314418793
Batch 58/64 loss: -0.30707934498786926
Batch 59/64 loss: -0.3222259283065796
Batch 60/64 loss: -0.3074337840080261
Batch 61/64 loss: -0.3331797122955322
Batch 62/64 loss: -0.28939566016197205
Batch 63/64 loss: -0.3023056983947754
Batch 64/64 loss: -0.2945418357849121
Epoch 325  Train loss: -0.3237024985107721  Val loss: 0.03001261761098383
Epoch 326
-------------------------------
Batch 1/64 loss: -0.2722824215888977
Batch 2/64 loss: -0.30737775564193726
Batch 3/64 loss: -0.3165284991264343
Batch 4/64 loss: -0.3173567056655884
Batch 5/64 loss: -0.2802541255950928
Batch 6/64 loss: -0.29396069049835205
Batch 7/64 loss: -0.31315016746520996
Batch 8/64 loss: -0.3191547989845276
Batch 9/64 loss: -0.3293878138065338
Batch 10/64 loss: -0.3276301622390747
Batch 11/64 loss: -0.3307853937149048
Batch 12/64 loss: -0.3563767075538635
Batch 13/64 loss: -0.3500899076461792
Batch 14/64 loss: -0.29976293444633484
Batch 15/64 loss: -0.2896798551082611
Batch 16/64 loss: -0.33487799763679504
Batch 17/64 loss: -0.3623424172401428
Batch 18/64 loss: -0.3490537106990814
Batch 19/64 loss: -0.32916751503944397
Batch 20/64 loss: -0.35558217763900757
Batch 21/64 loss: -0.3365684151649475
Batch 22/64 loss: -0.3579636216163635
Batch 23/64 loss: -0.3295287489891052
Batch 24/64 loss: -0.3727632761001587
Batch 25/64 loss: -0.30142027139663696
Batch 26/64 loss: -0.31925296783447266
Batch 27/64 loss: -0.3394465446472168
Batch 28/64 loss: -0.31256020069122314
Batch 29/64 loss: -0.3045778274536133
Batch 30/64 loss: -0.30450859665870667
Batch 31/64 loss: -0.3277413845062256
Batch 32/64 loss: -0.3157259225845337
Batch 33/64 loss: -0.3237321674823761
Batch 34/64 loss: -0.329140305519104
Batch 35/64 loss: -0.3591340184211731
Batch 36/64 loss: -0.3368583619594574
Batch 37/64 loss: -0.3167305588722229
Batch 38/64 loss: -0.3444916009902954
Batch 39/64 loss: -0.32164278626441956
Batch 40/64 loss: -0.3499634265899658
Batch 41/64 loss: -0.31137847900390625
Batch 42/64 loss: -0.3241879940032959
Batch 43/64 loss: -0.3257865607738495
Batch 44/64 loss: -0.34889519214630127
Batch 45/64 loss: -0.32012999057769775
Batch 46/64 loss: -0.3040972948074341
Batch 47/64 loss: -0.3349706530570984
Batch 48/64 loss: -0.2646666169166565
Batch 49/64 loss: -0.3434842824935913
Batch 50/64 loss: -0.2734695076942444
Batch 51/64 loss: -0.32095202803611755
Batch 52/64 loss: -0.3613562285900116
Batch 53/64 loss: -0.3085620403289795
Batch 54/64 loss: -0.3236212730407715
Batch 55/64 loss: -0.35089311003685
Batch 56/64 loss: -0.2960384786128998
Batch 57/64 loss: -0.35210031270980835
Batch 58/64 loss: -0.3047083914279938
Batch 59/64 loss: -0.3357716202735901
Batch 60/64 loss: -0.34676259756088257
Batch 61/64 loss: -0.3022167682647705
Batch 62/64 loss: -0.311330646276474
Batch 63/64 loss: -0.3102760910987854
Batch 64/64 loss: -0.29932525753974915
Epoch 326  Train loss: -0.32374435861905415  Val loss: 0.028427343597936465
Epoch 327
-------------------------------
Batch 1/64 loss: -0.35004574060440063
Batch 2/64 loss: -0.3331202268600464
Batch 3/64 loss: -0.3287929594516754
Batch 4/64 loss: -0.3475642800331116
Batch 5/64 loss: -0.31783097982406616
Batch 6/64 loss: -0.3186545968055725
Batch 7/64 loss: -0.3047634959220886
Batch 8/64 loss: -0.32951486110687256
Batch 9/64 loss: -0.3214030861854553
Batch 10/64 loss: -0.31137120723724365
Batch 11/64 loss: -0.31586381793022156
Batch 12/64 loss: -0.3314985930919647
Batch 13/64 loss: -0.2940225601196289
Batch 14/64 loss: -0.31011641025543213
Batch 15/64 loss: -0.35004743933677673
Batch 16/64 loss: -0.34818288683891296
Batch 17/64 loss: -0.315975546836853
Batch 18/64 loss: -0.343251496553421
Batch 19/64 loss: -0.2855141758918762
Batch 20/64 loss: -0.34628963470458984
Batch 21/64 loss: -0.34859979152679443
Batch 22/64 loss: -0.32764968276023865
Batch 23/64 loss: -0.2659261226654053
Batch 24/64 loss: -0.32092511653900146
Batch 25/64 loss: -0.30255746841430664
Batch 26/64 loss: -0.3256989121437073
Batch 27/64 loss: -0.32723021507263184
Batch 28/64 loss: -0.3383730947971344
Batch 29/64 loss: -0.34055930376052856
Batch 30/64 loss: -0.33115941286087036
Batch 31/64 loss: -0.2955732047557831
Batch 32/64 loss: -0.31015393137931824
Batch 33/64 loss: -0.293478786945343
Batch 34/64 loss: -0.34868156909942627
Batch 35/64 loss: -0.3267934024333954
Batch 36/64 loss: -0.287359356880188
Batch 37/64 loss: -0.3307717740535736
Batch 38/64 loss: -0.31702059507369995
Batch 39/64 loss: -0.3346860408782959
Batch 40/64 loss: -0.32916808128356934
Batch 41/64 loss: -0.3409349322319031
Batch 42/64 loss: -0.32668185234069824
Batch 43/64 loss: -0.360490083694458
Batch 44/64 loss: -0.30935314297676086
Batch 45/64 loss: -0.3088217079639435
Batch 46/64 loss: -0.3308710753917694
Batch 47/64 loss: -0.3283316493034363
Batch 48/64 loss: -0.3266880512237549
Batch 49/64 loss: -0.3427925407886505
Batch 50/64 loss: -0.3182567358016968
Batch 51/64 loss: -0.337185800075531
Batch 52/64 loss: -0.3058341145515442
Batch 53/64 loss: -0.33459290862083435
Batch 54/64 loss: -0.35807114839553833
Batch 55/64 loss: -0.32004258036613464
Batch 56/64 loss: -0.3429621458053589
Batch 57/64 loss: -0.31244248151779175
Batch 58/64 loss: -0.32660865783691406
Batch 59/64 loss: -0.33171388506889343
Batch 60/64 loss: -0.29895320534706116
Batch 61/64 loss: -0.3246883451938629
Batch 62/64 loss: -0.3305336833000183
Batch 63/64 loss: -0.35619354248046875
Batch 64/64 loss: -0.26341933012008667
Epoch 327  Train loss: -0.3243419392436158  Val loss: 0.030207299694572528
Epoch 328
-------------------------------
Batch 1/64 loss: -0.342901349067688
Batch 2/64 loss: -0.3690469264984131
Batch 3/64 loss: -0.33684012293815613
Batch 4/64 loss: -0.3114197254180908
Batch 5/64 loss: -0.3247147500514984
Batch 6/64 loss: -0.3286644220352173
Batch 7/64 loss: -0.3392307162284851
Batch 8/64 loss: -0.3175380825996399
Batch 9/64 loss: -0.35177916288375854
Batch 10/64 loss: -0.36796310544013977
Batch 11/64 loss: -0.32621562480926514
Batch 12/64 loss: -0.3618541955947876
Batch 13/64 loss: -0.31941327452659607
Batch 14/64 loss: -0.35798782110214233
Batch 15/64 loss: -0.33797189593315125
Batch 16/64 loss: -0.3109932541847229
Batch 17/64 loss: -0.3486880660057068
Batch 18/64 loss: -0.23770666122436523
Batch 19/64 loss: -0.28863856196403503
Batch 20/64 loss: -0.35094720125198364
Batch 21/64 loss: -0.35338273644447327
Batch 22/64 loss: -0.28022629022598267
Batch 23/64 loss: -0.3273124098777771
Batch 24/64 loss: -0.3306938409805298
Batch 25/64 loss: -0.3324390649795532
Batch 26/64 loss: -0.3548595607280731
Batch 27/64 loss: -0.31361082196235657
Batch 28/64 loss: -0.2930644154548645
Batch 29/64 loss: -0.34901630878448486
Batch 30/64 loss: -0.305868536233902
Batch 31/64 loss: -0.3370845317840576
Batch 32/64 loss: -0.3452908992767334
Batch 33/64 loss: -0.34011560678482056
Batch 34/64 loss: -0.3554850220680237
Batch 35/64 loss: -0.31924986839294434
Batch 36/64 loss: -0.2721707820892334
Batch 37/64 loss: -0.32912677526474
Batch 38/64 loss: -0.2976374924182892
Batch 39/64 loss: -0.33124566078186035
Batch 40/64 loss: -0.3366420567035675
Batch 41/64 loss: -0.31523606181144714
Batch 42/64 loss: -0.34154659509658813
Batch 43/64 loss: -0.35356783866882324
Batch 44/64 loss: -0.33332452178001404
Batch 45/64 loss: -0.3464431166648865
Batch 46/64 loss: -0.3081415295600891
Batch 47/64 loss: -0.3280622959136963
Batch 48/64 loss: -0.334369421005249
Batch 49/64 loss: -0.2779657542705536
Batch 50/64 loss: -0.32805630564689636
Batch 51/64 loss: -0.3029380142688751
Batch 52/64 loss: -0.33011484146118164
Batch 53/64 loss: -0.33717823028564453
Batch 54/64 loss: -0.3034812808036804
Batch 55/64 loss: -0.31371068954467773
Batch 56/64 loss: -0.3175697326660156
Batch 57/64 loss: -0.3326703608036041
Batch 58/64 loss: -0.33866453170776367
Batch 59/64 loss: -0.30494675040245056
Batch 60/64 loss: -0.3526020050048828
Batch 61/64 loss: -0.29365813732147217
Batch 62/64 loss: -0.3254082500934601
Batch 63/64 loss: -0.2994234263896942
Batch 64/64 loss: -0.3130282759666443
Epoch 328  Train loss: -0.3261002117512273  Val loss: 0.030696590331821507
Epoch 329
-------------------------------
Batch 1/64 loss: -0.3498992323875427
Batch 2/64 loss: -0.3581649661064148
Batch 3/64 loss: -0.3254784345626831
Batch 4/64 loss: -0.3258257210254669
Batch 5/64 loss: -0.33553963899612427
Batch 6/64 loss: -0.33714884519577026
Batch 7/64 loss: -0.34035950899124146
Batch 8/64 loss: -0.33558040857315063
Batch 9/64 loss: -0.27993708848953247
Batch 10/64 loss: -0.352990984916687
Batch 11/64 loss: -0.31381848454475403
Batch 12/64 loss: -0.3047686219215393
Batch 13/64 loss: -0.3198896646499634
Batch 14/64 loss: -0.2937575578689575
Batch 15/64 loss: -0.3304297924041748
Batch 16/64 loss: -0.3478931188583374
Batch 17/64 loss: -0.34094879031181335
Batch 18/64 loss: -0.2838938236236572
Batch 19/64 loss: -0.30706948041915894
Batch 20/64 loss: -0.3114519715309143
Batch 21/64 loss: -0.3292820453643799
Batch 22/64 loss: -0.31001776456832886
Batch 23/64 loss: -0.3191147446632385
Batch 24/64 loss: -0.2959100008010864
Batch 25/64 loss: -0.33466243743896484
Batch 26/64 loss: -0.33628907799720764
Batch 27/64 loss: -0.2992934584617615
Batch 28/64 loss: -0.3163042962551117
Batch 29/64 loss: -0.29313573241233826
Batch 30/64 loss: -0.31659847497940063
Batch 31/64 loss: -0.3401590585708618
Batch 32/64 loss: -0.3241966962814331
Batch 33/64 loss: -0.28235456347465515
Batch 34/64 loss: -0.3597829043865204
Batch 35/64 loss: -0.3086667060852051
Batch 36/64 loss: -0.2784903049468994
Batch 37/64 loss: -0.286995530128479
Batch 38/64 loss: -0.2986191511154175
Batch 39/64 loss: -0.3408214747905731
Batch 40/64 loss: -0.3169826865196228
Batch 41/64 loss: -0.33398038148880005
Batch 42/64 loss: -0.3417186141014099
Batch 43/64 loss: -0.34191009402275085
Batch 44/64 loss: -0.33365878462791443
Batch 45/64 loss: -0.31414222717285156
Batch 46/64 loss: -0.3630736172199249
Batch 47/64 loss: -0.3257092535495758
Batch 48/64 loss: -0.3024177849292755
Batch 49/64 loss: -0.3422514796257019
Batch 50/64 loss: -0.3439033329486847
Batch 51/64 loss: -0.3551488518714905
Batch 52/64 loss: -0.33825957775115967
Batch 53/64 loss: -0.3232133984565735
Batch 54/64 loss: -0.32690179347991943
Batch 55/64 loss: -0.33625683188438416
Batch 56/64 loss: -0.30516207218170166
Batch 57/64 loss: -0.36987587809562683
Batch 58/64 loss: -0.29840800166130066
Batch 59/64 loss: -0.3505480885505676
Batch 60/64 loss: -0.3369199335575104
Batch 61/64 loss: -0.3490354120731354
Batch 62/64 loss: -0.32085657119750977
Batch 63/64 loss: -0.32214778661727905
Batch 64/64 loss: -0.27310457825660706
Epoch 329  Train loss: -0.32412315992748036  Val loss: 0.02927190085866607
Epoch 330
-------------------------------
Batch 1/64 loss: -0.33277076482772827
Batch 2/64 loss: -0.32668402791023254
Batch 3/64 loss: -0.31536465883255005
Batch 4/64 loss: -0.2992713451385498
Batch 5/64 loss: -0.33049166202545166
Batch 6/64 loss: -0.3537386357784271
Batch 7/64 loss: -0.3398616313934326
Batch 8/64 loss: -0.3286517560482025
Batch 9/64 loss: -0.33191853761672974
Batch 10/64 loss: -0.322671502828598
Batch 11/64 loss: -0.3259141743183136
Batch 12/64 loss: -0.3286246955394745
Batch 13/64 loss: -0.35464268922805786
Batch 14/64 loss: -0.34041035175323486
Batch 15/64 loss: -0.3137042820453644
Batch 16/64 loss: -0.32816535234451294
Batch 17/64 loss: -0.3442891240119934
Batch 18/64 loss: -0.3125957250595093
Batch 19/64 loss: -0.3019387125968933
Batch 20/64 loss: -0.3327837586402893
Batch 21/64 loss: -0.3020729124546051
Batch 22/64 loss: -0.34400030970573425
Batch 23/64 loss: -0.3220888376235962
Batch 24/64 loss: -0.344413697719574
Batch 25/64 loss: -0.35639578104019165
Batch 26/64 loss: -0.3096488118171692
Batch 27/64 loss: -0.3347693383693695
Batch 28/64 loss: -0.2892257273197174
Batch 29/64 loss: -0.31270694732666016
Batch 30/64 loss: -0.3313518166542053
Batch 31/64 loss: -0.3465467095375061
Batch 32/64 loss: -0.32332345843315125
Batch 33/64 loss: -0.320740669965744
Batch 34/64 loss: -0.3113657236099243
Batch 35/64 loss: -0.3261348009109497
Batch 36/64 loss: -0.32156121730804443
Batch 37/64 loss: -0.3260863423347473
Batch 38/64 loss: -0.3005921542644501
Batch 39/64 loss: -0.31403830647468567
Batch 40/64 loss: -0.3346141576766968
Batch 41/64 loss: -0.3232204020023346
Batch 42/64 loss: -0.29251837730407715
Batch 43/64 loss: -0.3315304219722748
Batch 44/64 loss: -0.31368133425712585
Batch 45/64 loss: -0.3443809151649475
Batch 46/64 loss: -0.28012630343437195
Batch 47/64 loss: -0.3359695076942444
Batch 48/64 loss: -0.3011719584465027
Batch 49/64 loss: -0.3335677981376648
Batch 50/64 loss: -0.33482515811920166
Batch 51/64 loss: -0.3501622676849365
Batch 52/64 loss: -0.32619303464889526
Batch 53/64 loss: -0.32254642248153687
Batch 54/64 loss: -0.34649795293807983
Batch 55/64 loss: -0.3127533197402954
Batch 56/64 loss: -0.3632855713367462
Batch 57/64 loss: -0.32722705602645874
Batch 58/64 loss: -0.34754979610443115
Batch 59/64 loss: -0.33393967151641846
Batch 60/64 loss: -0.3041878342628479
Batch 61/64 loss: -0.3364276885986328
Batch 62/64 loss: -0.32280853390693665
Batch 63/64 loss: -0.3331628441810608
Batch 64/64 loss: -0.31809213757514954
Epoch 330  Train loss: -0.3261564608882455  Val loss: 0.031177962358874555
Epoch 331
-------------------------------
Batch 1/64 loss: -0.31983548402786255
Batch 2/64 loss: -0.3277110755443573
Batch 3/64 loss: -0.34552663564682007
Batch 4/64 loss: -0.34779709577560425
Batch 5/64 loss: -0.3086131513118744
Batch 6/64 loss: -0.34182924032211304
Batch 7/64 loss: -0.3023627698421478
Batch 8/64 loss: -0.31595951318740845
Batch 9/64 loss: -0.33521515130996704
Batch 10/64 loss: -0.34466367959976196
Batch 11/64 loss: -0.34534475207328796
Batch 12/64 loss: -0.33137112855911255
Batch 13/64 loss: -0.31547507643699646
Batch 14/64 loss: -0.2989419102668762
Batch 15/64 loss: -0.33967408537864685
Batch 16/64 loss: -0.34562957286834717
Batch 17/64 loss: -0.3063669204711914
Batch 18/64 loss: -0.3464636206626892
Batch 19/64 loss: -0.29793494939804077
Batch 20/64 loss: -0.35713905096054077
Batch 21/64 loss: -0.3452299237251282
Batch 22/64 loss: -0.33772364258766174
Batch 23/64 loss: -0.27651989459991455
Batch 24/64 loss: -0.33524835109710693
Batch 25/64 loss: -0.3073338270187378
Batch 26/64 loss: -0.3355887234210968
Batch 27/64 loss: -0.32377469539642334
Batch 28/64 loss: -0.3385007977485657
Batch 29/64 loss: -0.32916778326034546
Batch 30/64 loss: -0.3209383487701416
Batch 31/64 loss: -0.36313396692276
Batch 32/64 loss: -0.3061532974243164
Batch 33/64 loss: -0.35173091292381287
Batch 34/64 loss: -0.29668712615966797
Batch 35/64 loss: -0.3524514436721802
Batch 36/64 loss: -0.32201486825942993
Batch 37/64 loss: -0.2806262969970703
Batch 38/64 loss: -0.33046695590019226
Batch 39/64 loss: -0.3365136981010437
Batch 40/64 loss: -0.3377436697483063
Batch 41/64 loss: -0.35736456513404846
Batch 42/64 loss: -0.2810860872268677
Batch 43/64 loss: -0.3508700132369995
Batch 44/64 loss: -0.3471265435218811
Batch 45/64 loss: -0.31485140323638916
Batch 46/64 loss: -0.35037466883659363
Batch 47/64 loss: -0.29241350293159485
Batch 48/64 loss: -0.30329209566116333
Batch 49/64 loss: -0.32686614990234375
Batch 50/64 loss: -0.33924952149391174
Batch 51/64 loss: -0.30033233761787415
Batch 52/64 loss: -0.3351137638092041
Batch 53/64 loss: -0.32793235778808594
Batch 54/64 loss: -0.3182772994041443
Batch 55/64 loss: -0.353409081697464
Batch 56/64 loss: -0.34089159965515137
Batch 57/64 loss: -0.30242007970809937
Batch 58/64 loss: -0.36621057987213135
Batch 59/64 loss: -0.3066147565841675
Batch 60/64 loss: -0.3468315005302429
Batch 61/64 loss: -0.3031483590602875
Batch 62/64 loss: -0.30320388078689575
Batch 63/64 loss: -0.33160021901130676
Batch 64/64 loss: -0.3249271810054779
Epoch 331  Train loss: -0.32697378572295693  Val loss: 0.02996036010919158
Epoch 332
-------------------------------
Batch 1/64 loss: -0.3675457239151001
Batch 2/64 loss: -0.3380821645259857
Batch 3/64 loss: -0.3551308512687683
Batch 4/64 loss: -0.29806822538375854
Batch 5/64 loss: -0.32819968461990356
Batch 6/64 loss: -0.3626779317855835
Batch 7/64 loss: -0.3244298994541168
Batch 8/64 loss: -0.33749181032180786
Batch 9/64 loss: -0.3315886855125427
Batch 10/64 loss: -0.29416966438293457
Batch 11/64 loss: -0.30631765723228455
Batch 12/64 loss: -0.3576008677482605
Batch 13/64 loss: -0.33297619223594666
Batch 14/64 loss: -0.3206965923309326
Batch 15/64 loss: -0.2810208797454834
Batch 16/64 loss: -0.3555155396461487
Batch 17/64 loss: -0.34282588958740234
Batch 18/64 loss: -0.29165810346603394
Batch 19/64 loss: -0.2269282042980194
Batch 20/64 loss: -0.3717721700668335
Batch 21/64 loss: -0.24697697162628174
Batch 22/64 loss: -0.3362172842025757
Batch 23/64 loss: -0.33763620257377625
Batch 24/64 loss: -0.33456113934516907
Batch 25/64 loss: -0.29256367683410645
Batch 26/64 loss: -0.31483471393585205
Batch 27/64 loss: -0.3299838900566101
Batch 28/64 loss: -0.3085423707962036
Batch 29/64 loss: -0.2809911370277405
Batch 30/64 loss: -0.3626886010169983
Batch 31/64 loss: -0.29148489236831665
Batch 32/64 loss: -0.3258146047592163
Batch 33/64 loss: -0.2956244945526123
Batch 34/64 loss: -0.34953808784484863
Batch 35/64 loss: -0.34191441535949707
Batch 36/64 loss: -0.3508800268173218
Batch 37/64 loss: -0.29803910851478577
Batch 38/64 loss: -0.3212910592556
Batch 39/64 loss: -0.30926257371902466
Batch 40/64 loss: -0.3380383551120758
Batch 41/64 loss: -0.3077995181083679
Batch 42/64 loss: -0.29519832134246826
Batch 43/64 loss: -0.3224632740020752
Batch 44/64 loss: -0.3318873643875122
Batch 45/64 loss: -0.3198213577270508
Batch 46/64 loss: -0.35505640506744385
Batch 47/64 loss: -0.3029318153858185
Batch 48/64 loss: -0.24313825368881226
Batch 49/64 loss: -0.33784931898117065
Batch 50/64 loss: -0.31756579875946045
Batch 51/64 loss: -0.3191286325454712
Batch 52/64 loss: -0.3375789225101471
Batch 53/64 loss: -0.30991947650909424
Batch 54/64 loss: -0.29876577854156494
Batch 55/64 loss: -0.3524351119995117
Batch 56/64 loss: -0.332145631313324
Batch 57/64 loss: -0.33306190371513367
Batch 58/64 loss: -0.34002965688705444
Batch 59/64 loss: -0.28053075075149536
Batch 60/64 loss: -0.33116984367370605
Batch 61/64 loss: -0.3319396376609802
Batch 62/64 loss: -0.35585811734199524
Batch 63/64 loss: -0.3035210967063904
Batch 64/64 loss: -0.2882632613182068
Epoch 332  Train loss: -0.3210599808131947  Val loss: 0.02812355429036511
Epoch 333
-------------------------------
Batch 1/64 loss: -0.26149845123291016
Batch 2/64 loss: -0.35056519508361816
Batch 3/64 loss: -0.3320719599723816
Batch 4/64 loss: -0.3608859181404114
Batch 5/64 loss: -0.3347374200820923
Batch 6/64 loss: -0.34982287883758545
Batch 7/64 loss: -0.34777799248695374
Batch 8/64 loss: -0.35119420289993286
Batch 9/64 loss: -0.3496376872062683
Batch 10/64 loss: -0.3081252872943878
Batch 11/64 loss: -0.33534517884254456
Batch 12/64 loss: -0.34397077560424805
Batch 13/64 loss: -0.33430588245391846
Batch 14/64 loss: -0.32412195205688477
Batch 15/64 loss: -0.2966225743293762
Batch 16/64 loss: -0.344616562128067
Batch 17/64 loss: -0.32304561138153076
Batch 18/64 loss: -0.26984795928001404
Batch 19/64 loss: -0.3167741894721985
Batch 20/64 loss: -0.2854495644569397
Batch 21/64 loss: -0.35441091656684875
Batch 22/64 loss: -0.3479064702987671
Batch 23/64 loss: -0.3192136287689209
Batch 24/64 loss: -0.33823293447494507
Batch 25/64 loss: -0.2925969958305359
Batch 26/64 loss: -0.3557789623737335
Batch 27/64 loss: -0.32392609119415283
Batch 28/64 loss: -0.35502809286117554
Batch 29/64 loss: -0.31328779458999634
Batch 30/64 loss: -0.32431161403656006
Batch 31/64 loss: -0.34480464458465576
Batch 32/64 loss: -0.322905957698822
Batch 33/64 loss: -0.31073176860809326
Batch 34/64 loss: -0.30845949053764343
Batch 35/64 loss: -0.34319519996643066
Batch 36/64 loss: -0.34178033471107483
Batch 37/64 loss: -0.31459349393844604
Batch 38/64 loss: -0.32702821493148804
Batch 39/64 loss: -0.2547430396080017
Batch 40/64 loss: -0.33888429403305054
Batch 41/64 loss: -0.30997371673583984
Batch 42/64 loss: -0.3111732006072998
Batch 43/64 loss: -0.32079559564590454
Batch 44/64 loss: -0.30276066064834595
Batch 45/64 loss: -0.30871033668518066
Batch 46/64 loss: -0.33427637815475464
Batch 47/64 loss: -0.34004586935043335
Batch 48/64 loss: -0.2684691846370697
Batch 49/64 loss: -0.33440646529197693
Batch 50/64 loss: -0.3630344867706299
Batch 51/64 loss: -0.32210996747016907
Batch 52/64 loss: -0.30833709239959717
Batch 53/64 loss: -0.3398320972919464
Batch 54/64 loss: -0.3517869710922241
Batch 55/64 loss: -0.3650825619697571
Batch 56/64 loss: -0.3194534480571747
Batch 57/64 loss: -0.33897095918655396
Batch 58/64 loss: -0.3521122336387634
Batch 59/64 loss: -0.3501501679420471
Batch 60/64 loss: -0.3216797411441803
Batch 61/64 loss: -0.3080870807170868
Batch 62/64 loss: -0.3089551329612732
Batch 63/64 loss: -0.30587148666381836
Batch 64/64 loss: -0.33423280715942383
Epoch 333  Train loss: -0.3261017509535247  Val loss: 0.03240150110828098
Epoch 334
-------------------------------
Batch 1/64 loss: -0.3278075158596039
Batch 2/64 loss: -0.29096847772598267
Batch 3/64 loss: -0.33129674196243286
Batch 4/64 loss: -0.3165183663368225
Batch 5/64 loss: -0.3451785445213318
Batch 6/64 loss: -0.36437296867370605
Batch 7/64 loss: -0.32480695843696594
Batch 8/64 loss: -0.27360567450523376
Batch 9/64 loss: -0.30923038721084595
Batch 10/64 loss: -0.3319874405860901
Batch 11/64 loss: -0.3270350396633148
Batch 12/64 loss: -0.32439571619033813
Batch 13/64 loss: -0.30604326725006104
Batch 14/64 loss: -0.3472352623939514
Batch 15/64 loss: -0.34208953380584717
Batch 16/64 loss: -0.31021714210510254
Batch 17/64 loss: -0.27176955342292786
Batch 18/64 loss: -0.31467849016189575
Batch 19/64 loss: -0.365439236164093
Batch 20/64 loss: -0.3600463271141052
Batch 21/64 loss: -0.29489243030548096
Batch 22/64 loss: -0.34902042150497437
Batch 23/64 loss: -0.3402487635612488
Batch 24/64 loss: -0.2954000234603882
Batch 25/64 loss: -0.2702001929283142
Batch 26/64 loss: -0.33547818660736084
Batch 27/64 loss: -0.31461888551712036
Batch 28/64 loss: -0.3000545799732208
Batch 29/64 loss: -0.36436957120895386
Batch 30/64 loss: -0.3199438154697418
Batch 31/64 loss: -0.2982524633407593
Batch 32/64 loss: -0.2781665325164795
Batch 33/64 loss: -0.3369387984275818
Batch 34/64 loss: -0.3395644426345825
Batch 35/64 loss: -0.3512960374355316
Batch 36/64 loss: -0.3299368917942047
Batch 37/64 loss: -0.33713769912719727
Batch 38/64 loss: -0.3172813951969147
Batch 39/64 loss: -0.3400592803955078
Batch 40/64 loss: -0.33263325691223145
Batch 41/64 loss: -0.3599458932876587
Batch 42/64 loss: -0.3000164031982422
Batch 43/64 loss: -0.33326101303100586
Batch 44/64 loss: -0.3529438078403473
Batch 45/64 loss: -0.3344866931438446
Batch 46/64 loss: -0.34870296716690063
Batch 47/64 loss: -0.3076949417591095
Batch 48/64 loss: -0.3458842933177948
Batch 49/64 loss: -0.32711613178253174
Batch 50/64 loss: -0.32679644227027893
Batch 51/64 loss: -0.33958715200424194
Batch 52/64 loss: -0.3442021608352661
Batch 53/64 loss: -0.3471972942352295
Batch 54/64 loss: -0.32734057307243347
Batch 55/64 loss: -0.3390132188796997
Batch 56/64 loss: -0.31082406640052795
Batch 57/64 loss: -0.3357836604118347
Batch 58/64 loss: -0.3553515076637268
Batch 59/64 loss: -0.34070825576782227
Batch 60/64 loss: -0.3129504919052124
Batch 61/64 loss: -0.3097309172153473
Batch 62/64 loss: -0.3323914408683777
Batch 63/64 loss: -0.2944701313972473
Batch 64/64 loss: -0.2747059166431427
Epoch 334  Train loss: -0.32568855227208604  Val loss: 0.031230898042724713
Epoch 335
-------------------------------
Batch 1/64 loss: -0.3557630479335785
Batch 2/64 loss: -0.3341304063796997
Batch 3/64 loss: -0.32958829402923584
Batch 4/64 loss: -0.3124275803565979
Batch 5/64 loss: -0.34329336881637573
Batch 6/64 loss: -0.30961835384368896
Batch 7/64 loss: -0.3286203145980835
Batch 8/64 loss: -0.33282703161239624
Batch 9/64 loss: -0.34817588329315186
Batch 10/64 loss: -0.3068504333496094
Batch 11/64 loss: -0.30895406007766724
Batch 12/64 loss: -0.33367717266082764
Batch 13/64 loss: -0.3128504753112793
Batch 14/64 loss: -0.3474448323249817
Batch 15/64 loss: -0.30836403369903564
Batch 16/64 loss: -0.3172728419303894
Batch 17/64 loss: -0.3100711703300476
Batch 18/64 loss: -0.312356173992157
Batch 19/64 loss: -0.28328660130500793
Batch 20/64 loss: -0.34013089537620544
Batch 21/64 loss: -0.2657138705253601
Batch 22/64 loss: -0.3314639925956726
Batch 23/64 loss: -0.3236834406852722
Batch 24/64 loss: -0.32254621386528015
Batch 25/64 loss: -0.3696882724761963
Batch 26/64 loss: -0.295340895652771
Batch 27/64 loss: -0.3003079891204834
Batch 28/64 loss: -0.34507042169570923
Batch 29/64 loss: -0.3467574119567871
Batch 30/64 loss: -0.3660554587841034
Batch 31/64 loss: -0.325905442237854
Batch 32/64 loss: -0.3333492875099182
Batch 33/64 loss: -0.3550994396209717
Batch 34/64 loss: -0.2877131402492523
Batch 35/64 loss: -0.33218714594841003
Batch 36/64 loss: -0.33505356311798096
Batch 37/64 loss: -0.3416120409965515
Batch 38/64 loss: -0.33588114380836487
Batch 39/64 loss: -0.33014270663261414
Batch 40/64 loss: -0.3283342123031616
Batch 41/64 loss: -0.3236839771270752
Batch 42/64 loss: -0.32330775260925293
Batch 43/64 loss: -0.35409533977508545
Batch 44/64 loss: -0.3250139653682709
Batch 45/64 loss: -0.3245779275894165
Batch 46/64 loss: -0.2988767921924591
Batch 47/64 loss: -0.32600992918014526
Batch 48/64 loss: -0.33859771490097046
Batch 49/64 loss: -0.2759022116661072
Batch 50/64 loss: -0.3461560308933258
Batch 51/64 loss: -0.31935620307922363
Batch 52/64 loss: -0.32847970724105835
Batch 53/64 loss: -0.3452514410018921
Batch 54/64 loss: -0.3182607591152191
Batch 55/64 loss: -0.3547338843345642
Batch 56/64 loss: -0.3482634425163269
Batch 57/64 loss: -0.32475316524505615
Batch 58/64 loss: -0.355448454618454
Batch 59/64 loss: -0.33919382095336914
Batch 60/64 loss: -0.3337094187736511
Batch 61/64 loss: -0.3256138265132904
Batch 62/64 loss: -0.33839151263237
Batch 63/64 loss: -0.34160488843917847
Batch 64/64 loss: -0.3205394148826599
Epoch 335  Train loss: -0.3278007182420469  Val loss: 0.03038398178991993
Epoch 336
-------------------------------
Batch 1/64 loss: -0.3127952814102173
Batch 2/64 loss: -0.33738577365875244
Batch 3/64 loss: -0.3200584053993225
Batch 4/64 loss: -0.37085187435150146
Batch 5/64 loss: -0.322504460811615
Batch 6/64 loss: -0.3161136209964752
Batch 7/64 loss: -0.3002054691314697
Batch 8/64 loss: -0.3584676682949066
Batch 9/64 loss: -0.3418542742729187
Batch 10/64 loss: -0.33350786566734314
Batch 11/64 loss: -0.28023040294647217
Batch 12/64 loss: -0.32528603076934814
Batch 13/64 loss: -0.351052463054657
Batch 14/64 loss: -0.3563029170036316
Batch 15/64 loss: -0.34644100069999695
Batch 16/64 loss: -0.30782121419906616
Batch 17/64 loss: -0.3453058898448944
Batch 18/64 loss: -0.33321356773376465
Batch 19/64 loss: -0.31283748149871826
Batch 20/64 loss: -0.3434308171272278
Batch 21/64 loss: -0.33655303716659546
Batch 22/64 loss: -0.3267204165458679
Batch 23/64 loss: -0.2972062826156616
Batch 24/64 loss: -0.34682828187942505
Batch 25/64 loss: -0.3360975980758667
Batch 26/64 loss: -0.34622275829315186
Batch 27/64 loss: -0.31543537974357605
Batch 28/64 loss: -0.3460363447666168
Batch 29/64 loss: -0.31507837772369385
Batch 30/64 loss: -0.31878989934921265
Batch 31/64 loss: -0.2837265431880951
Batch 32/64 loss: -0.32148611545562744
Batch 33/64 loss: -0.3163386583328247
Batch 34/64 loss: -0.3361620008945465
Batch 35/64 loss: -0.3322853446006775
Batch 36/64 loss: -0.3714055120944977
Batch 37/64 loss: -0.3316799998283386
Batch 38/64 loss: -0.32600685954093933
Batch 39/64 loss: -0.3167386054992676
Batch 40/64 loss: -0.32576343417167664
Batch 41/64 loss: -0.34934836626052856
Batch 42/64 loss: -0.32532331347465515
Batch 43/64 loss: -0.3456874489784241
Batch 44/64 loss: -0.30670997500419617
Batch 45/64 loss: -0.3057703971862793
Batch 46/64 loss: -0.2884962856769562
Batch 47/64 loss: -0.3529321253299713
Batch 48/64 loss: -0.35580042004585266
Batch 49/64 loss: -0.33750730752944946
Batch 50/64 loss: -0.35919174551963806
Batch 51/64 loss: -0.36457347869873047
Batch 52/64 loss: -0.31925109028816223
Batch 53/64 loss: -0.3279433250427246
Batch 54/64 loss: -0.3300848603248596
Batch 55/64 loss: -0.36179664731025696
Batch 56/64 loss: -0.33377766609191895
Batch 57/64 loss: -0.312186598777771
Batch 58/64 loss: -0.32626375555992126
Batch 59/64 loss: -0.31970685720443726
Batch 60/64 loss: -0.3296281099319458
Batch 61/64 loss: -0.3528674244880676
Batch 62/64 loss: -0.34347450733184814
Batch 63/64 loss: -0.3504210412502289
Batch 64/64 loss: -0.35056930780410767
Epoch 336  Train loss: -0.3313552573615429  Val loss: 0.02985522960059831
Epoch 337
-------------------------------
Batch 1/64 loss: -0.25477278232574463
Batch 2/64 loss: -0.32914263010025024
Batch 3/64 loss: -0.3332902193069458
Batch 4/64 loss: -0.3547353148460388
Batch 5/64 loss: -0.35541635751724243
Batch 6/64 loss: -0.33070313930511475
Batch 7/64 loss: -0.32830190658569336
Batch 8/64 loss: -0.3203536868095398
Batch 9/64 loss: -0.2730705440044403
Batch 10/64 loss: -0.33780282735824585
Batch 11/64 loss: -0.33432769775390625
Batch 12/64 loss: -0.3471475839614868
Batch 13/64 loss: -0.35364872217178345
Batch 14/64 loss: -0.3351726830005646
Batch 15/64 loss: -0.3295605778694153
Batch 16/64 loss: -0.3378726840019226
Batch 17/64 loss: -0.31106406450271606
Batch 18/64 loss: -0.34684211015701294
Batch 19/64 loss: -0.31224143505096436
Batch 20/64 loss: -0.28883248567581177
Batch 21/64 loss: -0.33938080072402954
Batch 22/64 loss: -0.3345212936401367
Batch 23/64 loss: -0.36541998386383057
Batch 24/64 loss: -0.3652827739715576
Batch 25/64 loss: -0.3256528079509735
Batch 26/64 loss: -0.27553635835647583
Batch 27/64 loss: -0.35479164123535156
Batch 28/64 loss: -0.299747496843338
Batch 29/64 loss: -0.3181239366531372
Batch 30/64 loss: -0.3431750237941742
Batch 31/64 loss: -0.3077731430530548
Batch 32/64 loss: -0.3324156403541565
Batch 33/64 loss: -0.3028597831726074
Batch 34/64 loss: -0.34262099862098694
Batch 35/64 loss: -0.30486759543418884
Batch 36/64 loss: -0.3128303289413452
Batch 37/64 loss: -0.2901045083999634
Batch 38/64 loss: -0.32704102993011475
Batch 39/64 loss: -0.3261749744415283
Batch 40/64 loss: -0.30760979652404785
Batch 41/64 loss: -0.32582253217697144
Batch 42/64 loss: -0.2891055941581726
Batch 43/64 loss: -0.3036253750324249
Batch 44/64 loss: -0.3365183472633362
Batch 45/64 loss: -0.3373745381832123
Batch 46/64 loss: -0.3248465061187744
Batch 47/64 loss: -0.3538980185985565
Batch 48/64 loss: -0.35697609186172485
Batch 49/64 loss: -0.332963764667511
Batch 50/64 loss: -0.2941723167896271
Batch 51/64 loss: -0.3450288772583008
Batch 52/64 loss: -0.3421761691570282
Batch 53/64 loss: -0.35981428623199463
Batch 54/64 loss: -0.3106963634490967
Batch 55/64 loss: -0.3531349301338196
Batch 56/64 loss: -0.30336225032806396
Batch 57/64 loss: -0.3114723265171051
Batch 58/64 loss: -0.3463882803916931
Batch 59/64 loss: -0.3487296402454376
Batch 60/64 loss: -0.30781418085098267
Batch 61/64 loss: -0.3400474190711975
Batch 62/64 loss: -0.3152334690093994
Batch 63/64 loss: -0.3312755227088928
Batch 64/64 loss: -0.35259658098220825
Epoch 337  Train loss: -0.3266690761435266  Val loss: 0.03018094891125394
Epoch 338
-------------------------------
Batch 1/64 loss: -0.341107040643692
Batch 2/64 loss: -0.3492472171783447
Batch 3/64 loss: -0.316672146320343
Batch 4/64 loss: -0.34458625316619873
Batch 5/64 loss: -0.30589181184768677
Batch 6/64 loss: -0.3819287419319153
Batch 7/64 loss: -0.3574216365814209
Batch 8/64 loss: -0.33672475814819336
Batch 9/64 loss: -0.30837416648864746
Batch 10/64 loss: -0.30969780683517456
Batch 11/64 loss: -0.34137070178985596
Batch 12/64 loss: -0.3040810227394104
Batch 13/64 loss: -0.32366636395454407
Batch 14/64 loss: -0.31469762325286865
Batch 15/64 loss: -0.3258776366710663
Batch 16/64 loss: -0.3462383449077606
Batch 17/64 loss: -0.34966781735420227
Batch 18/64 loss: -0.3260640501976013
Batch 19/64 loss: -0.3337087631225586
Batch 20/64 loss: -0.33006423711776733
Batch 21/64 loss: -0.35144633054733276
Batch 22/64 loss: -0.3119644522666931
Batch 23/64 loss: -0.2860901355743408
Batch 24/64 loss: -0.3168313503265381
Batch 25/64 loss: -0.334877073764801
Batch 26/64 loss: -0.34377390146255493
Batch 27/64 loss: -0.3612433969974518
Batch 28/64 loss: -0.288146436214447
Batch 29/64 loss: -0.34146061539649963
Batch 30/64 loss: -0.35714346170425415
Batch 31/64 loss: -0.31316450238227844
Batch 32/64 loss: -0.20621979236602783
Batch 33/64 loss: -0.3427896201610565
Batch 34/64 loss: -0.33025282621383667
Batch 35/64 loss: -0.3533506393432617
Batch 36/64 loss: -0.2796652317047119
Batch 37/64 loss: -0.34648841619491577
Batch 38/64 loss: -0.3309883773326874
Batch 39/64 loss: -0.34683382511138916
Batch 40/64 loss: -0.32237517833709717
Batch 41/64 loss: -0.3481598496437073
Batch 42/64 loss: -0.35024023056030273
Batch 43/64 loss: -0.33603435754776
Batch 44/64 loss: -0.3387601673603058
Batch 45/64 loss: -0.34631797671318054
Batch 46/64 loss: -0.3127555549144745
Batch 47/64 loss: -0.36437907814979553
Batch 48/64 loss: -0.33138665556907654
Batch 49/64 loss: -0.3420892357826233
Batch 50/64 loss: -0.34203875064849854
Batch 51/64 loss: -0.3120601177215576
Batch 52/64 loss: -0.3197164237499237
Batch 53/64 loss: -0.3380544185638428
Batch 54/64 loss: -0.33105039596557617
Batch 55/64 loss: -0.3342605531215668
Batch 56/64 loss: -0.37082478404045105
Batch 57/64 loss: -0.34850186109542847
Batch 58/64 loss: -0.3408220708370209
Batch 59/64 loss: -0.3315299153327942
Batch 60/64 loss: -0.33324337005615234
Batch 61/64 loss: -0.2768394351005554
Batch 62/64 loss: -0.29038023948669434
Batch 63/64 loss: -0.27653563022613525
Batch 64/64 loss: -0.3274569809436798
Epoch 338  Train loss: -0.32900027469092724  Val loss: 0.03354412337758697
Epoch 339
-------------------------------
Batch 1/64 loss: -0.32743269205093384
Batch 2/64 loss: -0.33340713381767273
Batch 3/64 loss: -0.30612605810165405
Batch 4/64 loss: -0.33676838874816895
Batch 5/64 loss: -0.3057391047477722
Batch 6/64 loss: -0.32310518622398376
Batch 7/64 loss: -0.3374869227409363
Batch 8/64 loss: -0.36584219336509705
Batch 9/64 loss: -0.3261474370956421
Batch 10/64 loss: -0.2926402688026428
Batch 11/64 loss: -0.2931918799877167
Batch 12/64 loss: -0.3695826530456543
Batch 13/64 loss: -0.3442591428756714
Batch 14/64 loss: -0.3223980665206909
Batch 15/64 loss: -0.33435407280921936
Batch 16/64 loss: -0.3511210083961487
Batch 17/64 loss: -0.3304680287837982
Batch 18/64 loss: -0.30217868089675903
Batch 19/64 loss: -0.350894033908844
Batch 20/64 loss: -0.3233235478401184
Batch 21/64 loss: -0.3480373024940491
Batch 22/64 loss: -0.3161010146141052
Batch 23/64 loss: -0.32152748107910156
Batch 24/64 loss: -0.3369160592556
Batch 25/64 loss: -0.2888090908527374
Batch 26/64 loss: -0.3413933515548706
Batch 27/64 loss: -0.32224947214126587
Batch 28/64 loss: -0.3197544813156128
Batch 29/64 loss: -0.32129961252212524
Batch 30/64 loss: -0.3226085305213928
Batch 31/64 loss: -0.2822627127170563
Batch 32/64 loss: -0.34445562958717346
Batch 33/64 loss: -0.32030338048934937
Batch 34/64 loss: -0.33286774158477783
Batch 35/64 loss: -0.34588420391082764
Batch 36/64 loss: -0.2954632639884949
Batch 37/64 loss: -0.30125513672828674
Batch 38/64 loss: -0.3583027720451355
Batch 39/64 loss: -0.2796891927719116
Batch 40/64 loss: -0.3495844006538391
Batch 41/64 loss: -0.30008840560913086
Batch 42/64 loss: -0.3292282819747925
Batch 43/64 loss: -0.31612417101860046
Batch 44/64 loss: -0.3274575471878052
Batch 45/64 loss: -0.3277382254600525
Batch 46/64 loss: -0.31909772753715515
Batch 47/64 loss: -0.32637596130371094
Batch 48/64 loss: -0.3068762719631195
Batch 49/64 loss: -0.3359813094139099
Batch 50/64 loss: -0.33716827630996704
Batch 51/64 loss: -0.32906192541122437
Batch 52/64 loss: -0.331004798412323
Batch 53/64 loss: -0.31659460067749023
Batch 54/64 loss: -0.3343566656112671
Batch 55/64 loss: -0.34157347679138184
Batch 56/64 loss: -0.3038930296897888
Batch 57/64 loss: -0.31194543838500977
Batch 58/64 loss: -0.34944334626197815
Batch 59/64 loss: -0.344681978225708
Batch 60/64 loss: -0.32343506813049316
Batch 61/64 loss: -0.3103044927120209
Batch 62/64 loss: -0.2997179627418518
Batch 63/64 loss: -0.3408252000808716
Batch 64/64 loss: -0.3480292856693268
Epoch 339  Train loss: -0.3254780777529174  Val loss: 0.029908430945012988
Epoch 340
-------------------------------
Batch 1/64 loss: -0.23746156692504883
Batch 2/64 loss: -0.3265872001647949
Batch 3/64 loss: -0.33802324533462524
Batch 4/64 loss: -0.335235595703125
Batch 5/64 loss: -0.31429168581962585
Batch 6/64 loss: -0.2899448573589325
Batch 7/64 loss: -0.30077487230300903
Batch 8/64 loss: -0.35277819633483887
Batch 9/64 loss: -0.36231380701065063
Batch 10/64 loss: -0.3423963785171509
Batch 11/64 loss: -0.36633527278900146
Batch 12/64 loss: -0.3027552366256714
Batch 13/64 loss: -0.334852397441864
Batch 14/64 loss: -0.314067006111145
Batch 15/64 loss: -0.3280208110809326
Batch 16/64 loss: -0.32236552238464355
Batch 17/64 loss: -0.29703789949417114
Batch 18/64 loss: -0.3555300831794739
Batch 19/64 loss: -0.3584592342376709
Batch 20/64 loss: -0.3590729832649231
Batch 21/64 loss: -0.32728657126426697
Batch 22/64 loss: -0.32568126916885376
Batch 23/64 loss: -0.330665647983551
Batch 24/64 loss: -0.35481154918670654
Batch 25/64 loss: -0.3467637300491333
Batch 26/64 loss: -0.2795257270336151
Batch 27/64 loss: -0.3039085268974304
Batch 28/64 loss: -0.2838374972343445
Batch 29/64 loss: -0.3474534749984741
Batch 30/64 loss: -0.3249565362930298
Batch 31/64 loss: -0.34958600997924805
Batch 32/64 loss: -0.34487128257751465
Batch 33/64 loss: -0.3286900818347931
Batch 34/64 loss: -0.32078370451927185
Batch 35/64 loss: -0.3491235375404358
Batch 36/64 loss: -0.3307194113731384
Batch 37/64 loss: -0.36588722467422485
Batch 38/64 loss: -0.3415924608707428
Batch 39/64 loss: -0.3600156009197235
Batch 40/64 loss: -0.32759037613868713
Batch 41/64 loss: -0.28573766350746155
Batch 42/64 loss: -0.34866613149642944
Batch 43/64 loss: -0.33153292536735535
Batch 44/64 loss: -0.31924575567245483
Batch 45/64 loss: -0.33778250217437744
Batch 46/64 loss: -0.26214760541915894
Batch 47/64 loss: -0.3103962242603302
Batch 48/64 loss: -0.3164457380771637
Batch 49/64 loss: -0.31341081857681274
Batch 50/64 loss: -0.34422770142555237
Batch 51/64 loss: -0.3517070412635803
Batch 52/64 loss: -0.3200870752334595
Batch 53/64 loss: -0.34001976251602173
Batch 54/64 loss: -0.2929017245769501
Batch 55/64 loss: -0.3328266143798828
Batch 56/64 loss: -0.3601512610912323
Batch 57/64 loss: -0.3492683172225952
Batch 58/64 loss: -0.3605616092681885
Batch 59/64 loss: -0.3449666500091553
Batch 60/64 loss: -0.35377776622772217
Batch 61/64 loss: -0.34743183851242065
Batch 62/64 loss: -0.32571136951446533
Batch 63/64 loss: -0.3203984797000885
Batch 64/64 loss: -0.31192126870155334
Epoch 340  Train loss: -0.32918270744529426  Val loss: 0.030488317365089234
Epoch 341
-------------------------------
Batch 1/64 loss: -0.3622000217437744
Batch 2/64 loss: -0.32756945490837097
Batch 3/64 loss: -0.30737581849098206
Batch 4/64 loss: -0.3439749479293823
Batch 5/64 loss: -0.3332296907901764
Batch 6/64 loss: -0.29684415459632874
Batch 7/64 loss: -0.306107759475708
Batch 8/64 loss: -0.35628458857536316
Batch 9/64 loss: -0.33446750044822693
Batch 10/64 loss: -0.37135374546051025
Batch 11/64 loss: -0.3583587408065796
Batch 12/64 loss: -0.29490917921066284
Batch 13/64 loss: -0.35817936062812805
Batch 14/64 loss: -0.30173856019973755
Batch 15/64 loss: -0.3086932897567749
Batch 16/64 loss: -0.2970830202102661
Batch 17/64 loss: -0.35727250576019287
Batch 18/64 loss: -0.2953062355518341
Batch 19/64 loss: -0.3241977393627167
Batch 20/64 loss: -0.3229951858520508
Batch 21/64 loss: -0.3328915238380432
Batch 22/64 loss: -0.34608015418052673
Batch 23/64 loss: -0.36969274282455444
Batch 24/64 loss: -0.2966408133506775
Batch 25/64 loss: -0.3179457485675812
Batch 26/64 loss: -0.31272876262664795
Batch 27/64 loss: -0.2910001873970032
Batch 28/64 loss: -0.3404092490673065
Batch 29/64 loss: -0.34911635518074036
Batch 30/64 loss: -0.33805716037750244
Batch 31/64 loss: -0.33161690831184387
Batch 32/64 loss: -0.32695722579956055
Batch 33/64 loss: -0.3567608892917633
Batch 34/64 loss: -0.29764336347579956
Batch 35/64 loss: -0.3097851276397705
Batch 36/64 loss: -0.2645840644836426
Batch 37/64 loss: -0.32797864079475403
Batch 38/64 loss: -0.3319814205169678
Batch 39/64 loss: -0.32697421312332153
Batch 40/64 loss: -0.3280782401561737
Batch 41/64 loss: -0.2717238664627075
Batch 42/64 loss: -0.3299963176250458
Batch 43/64 loss: -0.28982770442962646
Batch 44/64 loss: -0.3177071213722229
Batch 45/64 loss: -0.3308612108230591
Batch 46/64 loss: -0.33232590556144714
Batch 47/64 loss: -0.3225274085998535
Batch 48/64 loss: -0.3337137699127197
Batch 49/64 loss: -0.3132040798664093
Batch 50/64 loss: -0.3394278883934021
Batch 51/64 loss: -0.3577806353569031
Batch 52/64 loss: -0.3427758812904358
Batch 53/64 loss: -0.34440067410469055
Batch 54/64 loss: -0.3039800226688385
Batch 55/64 loss: -0.31990891695022583
Batch 56/64 loss: -0.3240642547607422
Batch 57/64 loss: -0.31112706661224365
Batch 58/64 loss: -0.333698034286499
Batch 59/64 loss: -0.3452277183532715
Batch 60/64 loss: -0.3268144130706787
Batch 61/64 loss: -0.3182855248451233
Batch 62/64 loss: -0.34675467014312744
Batch 63/64 loss: -0.3142645061016083
Batch 64/64 loss: -0.33062073588371277
Epoch 341  Train loss: -0.32585768524338216  Val loss: 0.032099919835316765
Epoch 342
-------------------------------
Batch 1/64 loss: -0.3224283456802368
Batch 2/64 loss: -0.34538552165031433
Batch 3/64 loss: -0.32901448011398315
Batch 4/64 loss: -0.322372704744339
Batch 5/64 loss: -0.3576314151287079
Batch 6/64 loss: -0.34245121479034424
Batch 7/64 loss: -0.29564160108566284
Batch 8/64 loss: -0.28966304659843445
Batch 9/64 loss: -0.3149722218513489
Batch 10/64 loss: -0.3372042179107666
Batch 11/64 loss: -0.3450510501861572
Batch 12/64 loss: -0.3157420754432678
Batch 13/64 loss: -0.3394273817539215
Batch 14/64 loss: -0.34003275632858276
Batch 15/64 loss: -0.3481234014034271
Batch 16/64 loss: -0.23331218957901
Batch 17/64 loss: -0.321266233921051
Batch 18/64 loss: -0.3025531470775604
Batch 19/64 loss: -0.3265710473060608
Batch 20/64 loss: -0.339471697807312
Batch 21/64 loss: -0.3448862135410309
Batch 22/64 loss: -0.32789167761802673
Batch 23/64 loss: -0.3295198082923889
Batch 24/64 loss: -0.30941537022590637
Batch 25/64 loss: -0.3430372476577759
Batch 26/64 loss: -0.31119686365127563
Batch 27/64 loss: -0.35474908351898193
Batch 28/64 loss: -0.3220425844192505
Batch 29/64 loss: -0.36442121863365173
Batch 30/64 loss: -0.3372173011302948
Batch 31/64 loss: -0.31536608934402466
Batch 32/64 loss: -0.3400299549102783
Batch 33/64 loss: -0.3539116382598877
Batch 34/64 loss: -0.358695924282074
Batch 35/64 loss: -0.3420233726501465
Batch 36/64 loss: -0.3283107280731201
Batch 37/64 loss: -0.3362009525299072
Batch 38/64 loss: -0.2944238781929016
Batch 39/64 loss: -0.29363304376602173
Batch 40/64 loss: -0.3529677391052246
Batch 41/64 loss: -0.3305930495262146
Batch 42/64 loss: -0.3475426137447357
Batch 43/64 loss: -0.3563610911369324
Batch 44/64 loss: -0.2943943440914154
Batch 45/64 loss: -0.3204514682292938
Batch 46/64 loss: -0.31102225184440613
Batch 47/64 loss: -0.32174938917160034
Batch 48/64 loss: -0.3076997995376587
Batch 49/64 loss: -0.3324187397956848
Batch 50/64 loss: -0.35535165667533875
Batch 51/64 loss: -0.27458593249320984
Batch 52/64 loss: -0.35117384791374207
Batch 53/64 loss: -0.3065531253814697
Batch 54/64 loss: -0.34654805064201355
Batch 55/64 loss: -0.3281710743904114
Batch 56/64 loss: -0.3445061445236206
Batch 57/64 loss: -0.3405363857746124
Batch 58/64 loss: -0.28491508960723877
Batch 59/64 loss: -0.37638792395591736
Batch 60/64 loss: -0.3222315311431885
Batch 61/64 loss: -0.3470906615257263
Batch 62/64 loss: -0.30949756503105164
Batch 63/64 loss: -0.32110753655433655
Batch 64/64 loss: -0.30835670232772827
Epoch 342  Train loss: -0.32769275669958076  Val loss: 0.03004461793145773
Epoch 343
-------------------------------
Batch 1/64 loss: -0.3729069232940674
Batch 2/64 loss: -0.3336445093154907
Batch 3/64 loss: -0.364571750164032
Batch 4/64 loss: -0.3782799541950226
Batch 5/64 loss: -0.32251104712486267
Batch 6/64 loss: -0.3458791673183441
Batch 7/64 loss: -0.33137422800064087
Batch 8/64 loss: -0.3255685865879059
Batch 9/64 loss: -0.26305103302001953
Batch 10/64 loss: -0.3520573377609253
Batch 11/64 loss: -0.28029727935791016
Batch 12/64 loss: -0.34765756130218506
Batch 13/64 loss: -0.305255651473999
Batch 14/64 loss: -0.3121577501296997
Batch 15/64 loss: -0.3495153784751892
Batch 16/64 loss: -0.3344898223876953
Batch 17/64 loss: -0.3430432677268982
Batch 18/64 loss: -0.30655986070632935
Batch 19/64 loss: -0.35890936851501465
Batch 20/64 loss: -0.32852452993392944
Batch 21/64 loss: -0.3532888889312744
Batch 22/64 loss: -0.3275749683380127
Batch 23/64 loss: -0.34452587366104126
Batch 24/64 loss: -0.3115606904029846
Batch 25/64 loss: -0.3048646152019501
Batch 26/64 loss: -0.3493255376815796
Batch 27/64 loss: -0.33519691228866577
Batch 28/64 loss: -0.3345811367034912
Batch 29/64 loss: -0.36740902066230774
Batch 30/64 loss: -0.34421491622924805
Batch 31/64 loss: -0.36120688915252686
Batch 32/64 loss: -0.3303283452987671
Batch 33/64 loss: -0.32248231768608093
Batch 34/64 loss: -0.33705902099609375
Batch 35/64 loss: -0.3396736979484558
Batch 36/64 loss: -0.3145678639411926
Batch 37/64 loss: -0.30290231108665466
Batch 38/64 loss: -0.2721637189388275
Batch 39/64 loss: -0.30456024408340454
Batch 40/64 loss: -0.333479642868042
Batch 41/64 loss: -0.3466987609863281
Batch 42/64 loss: -0.3443921208381653
Batch 43/64 loss: -0.320601224899292
Batch 44/64 loss: -0.35064858198165894
Batch 45/64 loss: -0.32342103123664856
Batch 46/64 loss: -0.31474965810775757
Batch 47/64 loss: -0.33500322699546814
Batch 48/64 loss: -0.2775687575340271
Batch 49/64 loss: -0.312385618686676
Batch 50/64 loss: -0.31455662846565247
Batch 51/64 loss: -0.3306903541088104
Batch 52/64 loss: -0.35086673498153687
Batch 53/64 loss: -0.3041995167732239
Batch 54/64 loss: -0.34302353858947754
Batch 55/64 loss: -0.31352698802948
Batch 56/64 loss: -0.3503125309944153
Batch 57/64 loss: -0.3641020655632019
Batch 58/64 loss: -0.3381595015525818
Batch 59/64 loss: -0.34620851278305054
Batch 60/64 loss: -0.3213348388671875
Batch 61/64 loss: -0.35754379630088806
Batch 62/64 loss: -0.33643877506256104
Batch 63/64 loss: -0.3376517593860626
Batch 64/64 loss: -0.2939593195915222
Epoch 343  Train loss: -0.33100824519699695  Val loss: 0.030723303249201823
Epoch 344
-------------------------------
Batch 1/64 loss: -0.3528572916984558
Batch 2/64 loss: -0.34200066328048706
Batch 3/64 loss: -0.3505741059780121
Batch 4/64 loss: -0.35098186135292053
Batch 5/64 loss: -0.3734660744667053
Batch 6/64 loss: -0.3444666862487793
Batch 7/64 loss: -0.3190014362335205
Batch 8/64 loss: -0.3590899705886841
Batch 9/64 loss: -0.3190470337867737
Batch 10/64 loss: -0.32973670959472656
Batch 11/64 loss: -0.299782931804657
Batch 12/64 loss: -0.3598017692565918
Batch 13/64 loss: -0.3403266668319702
Batch 14/64 loss: -0.35173481702804565
Batch 15/64 loss: -0.35577550530433655
Batch 16/64 loss: -0.36165469884872437
Batch 17/64 loss: -0.3413451910018921
Batch 18/64 loss: -0.34626370668411255
Batch 19/64 loss: -0.35998672246932983
Batch 20/64 loss: -0.3576606512069702
Batch 21/64 loss: -0.31090247631073
Batch 22/64 loss: -0.3023855686187744
Batch 23/64 loss: -0.361341267824173
Batch 24/64 loss: -0.31735241413116455
Batch 25/64 loss: -0.3180810213088989
Batch 26/64 loss: -0.33264029026031494
Batch 27/64 loss: -0.3735402226448059
Batch 28/64 loss: -0.35961174964904785
Batch 29/64 loss: -0.3444724678993225
Batch 30/64 loss: -0.3471405506134033
Batch 31/64 loss: -0.34477126598358154
Batch 32/64 loss: -0.333379864692688
Batch 33/64 loss: -0.33324021100997925
Batch 34/64 loss: -0.31623244285583496
Batch 35/64 loss: -0.37117311358451843
Batch 36/64 loss: -0.318805456161499
Batch 37/64 loss: -0.3257989287376404
Batch 38/64 loss: -0.3253212869167328
Batch 39/64 loss: -0.31343281269073486
Batch 40/64 loss: -0.35906991362571716
Batch 41/64 loss: -0.30397871136665344
Batch 42/64 loss: -0.28651610016822815
Batch 43/64 loss: -0.2657111883163452
Batch 44/64 loss: -0.36446231603622437
Batch 45/64 loss: -0.3317604660987854
Batch 46/64 loss: -0.3273722529411316
Batch 47/64 loss: -0.3298930823802948
Batch 48/64 loss: -0.3300066590309143
Batch 49/64 loss: -0.30176717042922974
Batch 50/64 loss: -0.2768309712409973
Batch 51/64 loss: -0.3475741744041443
Batch 52/64 loss: -0.32419949769973755
Batch 53/64 loss: -0.3060901165008545
Batch 54/64 loss: -0.34774479269981384
Batch 55/64 loss: -0.3362717628479004
Batch 56/64 loss: -0.3099241852760315
Batch 57/64 loss: -0.26948457956314087
Batch 58/64 loss: -0.3464174270629883
Batch 59/64 loss: -0.32871347665786743
Batch 60/64 loss: -0.34001457691192627
Batch 61/64 loss: -0.33942049741744995
Batch 62/64 loss: -0.33075615763664246
Batch 63/64 loss: -0.3204808831214905
Batch 64/64 loss: -0.3613063097000122
Epoch 344  Train loss: -0.3334998995650048  Val loss: 0.02833848228978947
Epoch 345
-------------------------------
Batch 1/64 loss: -0.3405332565307617
Batch 2/64 loss: -0.3443584442138672
Batch 3/64 loss: -0.3490360975265503
Batch 4/64 loss: -0.3439086079597473
Batch 5/64 loss: -0.36591771245002747
Batch 6/64 loss: -0.3604350984096527
Batch 7/64 loss: -0.31214210391044617
Batch 8/64 loss: -0.36369043588638306
Batch 9/64 loss: -0.3549254834651947
Batch 10/64 loss: -0.32307085394859314
Batch 11/64 loss: -0.304964542388916
Batch 12/64 loss: -0.3592364192008972
Batch 13/64 loss: -0.3144276738166809
Batch 14/64 loss: -0.30526426434516907
Batch 15/64 loss: -0.3299707770347595
Batch 16/64 loss: -0.3323054909706116
Batch 17/64 loss: -0.34855127334594727
Batch 18/64 loss: -0.3627587556838989
Batch 19/64 loss: -0.3373713791370392
Batch 20/64 loss: -0.3319677710533142
Batch 21/64 loss: -0.3404819071292877
Batch 22/64 loss: -0.32910239696502686
Batch 23/64 loss: -0.31748610734939575
Batch 24/64 loss: -0.32907816767692566
Batch 25/64 loss: -0.3117663264274597
Batch 26/64 loss: -0.32929059863090515
Batch 27/64 loss: -0.30413180589675903
Batch 28/64 loss: -0.3499484062194824
Batch 29/64 loss: -0.30722394585609436
Batch 30/64 loss: -0.3072471618652344
Batch 31/64 loss: -0.337854266166687
Batch 32/64 loss: -0.33212655782699585
Batch 33/64 loss: -0.3425014615058899
Batch 34/64 loss: -0.32461464405059814
Batch 35/64 loss: -0.3202034831047058
Batch 36/64 loss: -0.3459392189979553
Batch 37/64 loss: -0.3090953230857849
Batch 38/64 loss: -0.3152102828025818
Batch 39/64 loss: -0.3096199035644531
Batch 40/64 loss: -0.3289533853530884
Batch 41/64 loss: -0.35708075761795044
Batch 42/64 loss: -0.3473440408706665
Batch 43/64 loss: -0.31461337208747864
Batch 44/64 loss: -0.3107824921607971
Batch 45/64 loss: -0.32243427634239197
Batch 46/64 loss: -0.32199323177337646
Batch 47/64 loss: -0.3530426323413849
Batch 48/64 loss: -0.34149816632270813
Batch 49/64 loss: -0.30979594588279724
Batch 50/64 loss: -0.27072322368621826
Batch 51/64 loss: -0.34808507561683655
Batch 52/64 loss: -0.32288774847984314
Batch 53/64 loss: -0.33840206265449524
Batch 54/64 loss: -0.32555949687957764
Batch 55/64 loss: -0.3037492036819458
Batch 56/64 loss: -0.3215102553367615
Batch 57/64 loss: -0.3134133219718933
Batch 58/64 loss: -0.34956884384155273
Batch 59/64 loss: -0.3475187420845032
Batch 60/64 loss: -0.35304397344589233
Batch 61/64 loss: -0.3533744215965271
Batch 62/64 loss: -0.3071383535861969
Batch 63/64 loss: -0.34568727016448975
Batch 64/64 loss: -0.33346185088157654
Epoch 345  Train loss: -0.3310753739347645  Val loss: 0.029567566933910463
Epoch 346
-------------------------------
Batch 1/64 loss: -0.3571367859840393
Batch 2/64 loss: -0.3522713780403137
Batch 3/64 loss: -0.33533766865730286
Batch 4/64 loss: -0.3521836996078491
Batch 5/64 loss: -0.372475802898407
Batch 6/64 loss: -0.34696096181869507
Batch 7/64 loss: -0.34956279397010803
Batch 8/64 loss: -0.3235030174255371
Batch 9/64 loss: -0.3623012900352478
Batch 10/64 loss: -0.32688820362091064
Batch 11/64 loss: -0.3405469059944153
Batch 12/64 loss: -0.35560858249664307
Batch 13/64 loss: -0.3545268177986145
Batch 14/64 loss: -0.31060659885406494
Batch 15/64 loss: -0.3491460382938385
Batch 16/64 loss: -0.32495319843292236
Batch 17/64 loss: -0.35095831751823425
Batch 18/64 loss: -0.3204888105392456
Batch 19/64 loss: -0.3390212655067444
Batch 20/64 loss: -0.2859933376312256
Batch 21/64 loss: -0.3242514133453369
Batch 22/64 loss: -0.3332633376121521
Batch 23/64 loss: -0.3476882576942444
Batch 24/64 loss: -0.3525201678276062
Batch 25/64 loss: -0.35655301809310913
Batch 26/64 loss: -0.3037811517715454
Batch 27/64 loss: -0.3206755220890045
Batch 28/64 loss: -0.31467878818511963
Batch 29/64 loss: -0.3194712996482849
Batch 30/64 loss: -0.3147110641002655
Batch 31/64 loss: -0.34068357944488525
Batch 32/64 loss: -0.3137483596801758
Batch 33/64 loss: -0.2965317964553833
Batch 34/64 loss: -0.33235883712768555
Batch 35/64 loss: -0.34100788831710815
Batch 36/64 loss: -0.3506389260292053
Batch 37/64 loss: -0.32891881465911865
Batch 38/64 loss: -0.2968296706676483
Batch 39/64 loss: -0.30352506041526794
Batch 40/64 loss: -0.3506581783294678
Batch 41/64 loss: -0.36678266525268555
Batch 42/64 loss: -0.2961283028125763
Batch 43/64 loss: -0.31252121925354004
Batch 44/64 loss: -0.3255940079689026
Batch 45/64 loss: -0.32014864683151245
Batch 46/64 loss: -0.3013770878314972
Batch 47/64 loss: -0.34145140647888184
Batch 48/64 loss: -0.3194463551044464
Batch 49/64 loss: -0.3395279347896576
Batch 50/64 loss: -0.2979595363140106
Batch 51/64 loss: -0.30493849515914917
Batch 52/64 loss: -0.33025848865509033
Batch 53/64 loss: -0.3079187273979187
Batch 54/64 loss: -0.3284258544445038
Batch 55/64 loss: -0.3713018298149109
Batch 56/64 loss: -0.37700581550598145
Batch 57/64 loss: -0.3469632863998413
Batch 58/64 loss: -0.3515039086341858
Batch 59/64 loss: -0.3286113739013672
Batch 60/64 loss: -0.3542265295982361
Batch 61/64 loss: -0.33678895235061646
Batch 62/64 loss: -0.320870041847229
Batch 63/64 loss: -0.3294250965118408
Batch 64/64 loss: -0.27136266231536865
Epoch 346  Train loss: -0.33201041829352285  Val loss: 0.032430693865641694
Epoch 347
-------------------------------
Batch 1/64 loss: -0.3366405963897705
Batch 2/64 loss: -0.32563167810440063
Batch 3/64 loss: -0.35392260551452637
Batch 4/64 loss: -0.32740890979766846
Batch 5/64 loss: -0.34009554982185364
Batch 6/64 loss: -0.3184448480606079
Batch 7/64 loss: -0.3272116780281067
Batch 8/64 loss: -0.34937888383865356
Batch 9/64 loss: -0.27251023054122925
Batch 10/64 loss: -0.32278284430503845
Batch 11/64 loss: -0.37069791555404663
Batch 12/64 loss: -0.33499088883399963
Batch 13/64 loss: -0.3308175802230835
Batch 14/64 loss: -0.3281327486038208
Batch 15/64 loss: -0.35239291191101074
Batch 16/64 loss: -0.3402799367904663
Batch 17/64 loss: -0.35255730152130127
Batch 18/64 loss: -0.3694051504135132
Batch 19/64 loss: -0.3480108380317688
Batch 20/64 loss: -0.31975987553596497
Batch 21/64 loss: -0.3251712918281555
Batch 22/64 loss: -0.3500184714794159
Batch 23/64 loss: -0.32207629084587097
Batch 24/64 loss: -0.31993943452835083
Batch 25/64 loss: -0.2865595817565918
Batch 26/64 loss: -0.3174291253089905
Batch 27/64 loss: -0.324287086725235
Batch 28/64 loss: -0.3452867567539215
Batch 29/64 loss: -0.3335874080657959
Batch 30/64 loss: -0.29098695516586304
Batch 31/64 loss: -0.3461044728755951
Batch 32/64 loss: -0.3367553949356079
Batch 33/64 loss: -0.3254131078720093
Batch 34/64 loss: -0.3416704535484314
Batch 35/64 loss: -0.3242289423942566
Batch 36/64 loss: -0.344061940908432
Batch 37/64 loss: -0.3429837226867676
Batch 38/64 loss: -0.36670413613319397
Batch 39/64 loss: -0.33727535605430603
Batch 40/64 loss: -0.32193249464035034
Batch 41/64 loss: -0.31893956661224365
Batch 42/64 loss: -0.34087932109832764
Batch 43/64 loss: -0.3124784231185913
Batch 44/64 loss: -0.33428120613098145
Batch 45/64 loss: -0.32445424795150757
Batch 46/64 loss: -0.2855810225009918
Batch 47/64 loss: -0.3500092327594757
Batch 48/64 loss: -0.32504764199256897
Batch 49/64 loss: -0.3505329489707947
Batch 50/64 loss: -0.32832440733909607
Batch 51/64 loss: -0.32790762186050415
Batch 52/64 loss: -0.3473506271839142
Batch 53/64 loss: -0.30026480555534363
Batch 54/64 loss: -0.34354305267333984
Batch 55/64 loss: -0.33180710673332214
Batch 56/64 loss: -0.29191553592681885
Batch 57/64 loss: -0.3417366147041321
Batch 58/64 loss: -0.32332831621170044
Batch 59/64 loss: -0.31910228729248047
Batch 60/64 loss: -0.33085939288139343
Batch 61/64 loss: -0.32587742805480957
Batch 62/64 loss: -0.34761255979537964
Batch 63/64 loss: -0.3414149582386017
Batch 64/64 loss: -0.28370219469070435
Epoch 347  Train loss: -0.33065992734011485  Val loss: 0.030741052119592622
Epoch 348
-------------------------------
Batch 1/64 loss: -0.3339747190475464
Batch 2/64 loss: -0.34146982431411743
Batch 3/64 loss: -0.35104894638061523
Batch 4/64 loss: -0.34640300273895264
Batch 5/64 loss: -0.31995728611946106
Batch 6/64 loss: -0.32518553733825684
Batch 7/64 loss: -0.2902073860168457
Batch 8/64 loss: -0.3458366096019745
Batch 9/64 loss: -0.332891583442688
Batch 10/64 loss: -0.3347049951553345
Batch 11/64 loss: -0.3594270944595337
Batch 12/64 loss: -0.31246131658554077
Batch 13/64 loss: -0.3224112391471863
Batch 14/64 loss: -0.3447743356227875
Batch 15/64 loss: -0.325081467628479
Batch 16/64 loss: -0.3581889271736145
Batch 17/64 loss: -0.34850606322288513
Batch 18/64 loss: -0.34194913506507874
Batch 19/64 loss: -0.296938419342041
Batch 20/64 loss: -0.33188551664352417
Batch 21/64 loss: -0.346876323223114
Batch 22/64 loss: -0.30083703994750977
Batch 23/64 loss: -0.35549575090408325
Batch 24/64 loss: -0.3032325804233551
Batch 25/64 loss: -0.3634905517101288
Batch 26/64 loss: -0.31580182909965515
Batch 27/64 loss: -0.3594008982181549
Batch 28/64 loss: -0.32446837425231934
Batch 29/64 loss: -0.3579733371734619
Batch 30/64 loss: -0.33464574813842773
Batch 31/64 loss: -0.3617551326751709
Batch 32/64 loss: -0.34322425723075867
Batch 33/64 loss: -0.31825512647628784
Batch 34/64 loss: -0.37056872248649597
Batch 35/64 loss: -0.2935898005962372
Batch 36/64 loss: -0.3447290360927582
Batch 37/64 loss: -0.3452228903770447
Batch 38/64 loss: -0.3530246317386627
Batch 39/64 loss: -0.32556068897247314
Batch 40/64 loss: -0.315205842256546
Batch 41/64 loss: -0.3309062719345093
Batch 42/64 loss: -0.34972083568573
Batch 43/64 loss: -0.3475889563560486
Batch 44/64 loss: -0.32440388202667236
Batch 45/64 loss: -0.32935115694999695
Batch 46/64 loss: -0.3204724192619324
Batch 47/64 loss: -0.3236786723136902
Batch 48/64 loss: -0.29457128047943115
Batch 49/64 loss: -0.3308127820491791
Batch 50/64 loss: -0.3493937849998474
Batch 51/64 loss: -0.3076166808605194
Batch 52/64 loss: -0.34107041358947754
Batch 53/64 loss: -0.3248908519744873
Batch 54/64 loss: -0.32875099778175354
Batch 55/64 loss: -0.31314191222190857
Batch 56/64 loss: -0.3347366750240326
Batch 57/64 loss: -0.33238959312438965
Batch 58/64 loss: -0.33845055103302
Batch 59/64 loss: -0.33439984917640686
Batch 60/64 loss: -0.35189270973205566
Batch 61/64 loss: -0.3381875455379486
Batch 62/64 loss: -0.31384679675102234
Batch 63/64 loss: -0.2925519645214081
Batch 64/64 loss: -0.33196893334388733
Epoch 348  Train loss: -0.3325249451048234  Val loss: 0.029503390346605752
Epoch 349
-------------------------------
Batch 1/64 loss: -0.3494056761264801
Batch 2/64 loss: -0.3652302920818329
Batch 3/64 loss: -0.36402347683906555
Batch 4/64 loss: -0.3669593632221222
Batch 5/64 loss: -0.34660786390304565
Batch 6/64 loss: -0.3047367334365845
Batch 7/64 loss: -0.3855878710746765
Batch 8/64 loss: -0.2939908802509308
Batch 9/64 loss: -0.3226425349712372
Batch 10/64 loss: -0.3574265241622925
Batch 11/64 loss: -0.3464776873588562
Batch 12/64 loss: -0.35193249583244324
Batch 13/64 loss: -0.36908239126205444
Batch 14/64 loss: -0.3312324285507202
Batch 15/64 loss: -0.3047499656677246
Batch 16/64 loss: -0.3149612545967102
Batch 17/64 loss: -0.3614341616630554
Batch 18/64 loss: -0.3527713418006897
Batch 19/64 loss: -0.33147668838500977
Batch 20/64 loss: -0.36153072118759155
Batch 21/64 loss: -0.339063823223114
Batch 22/64 loss: -0.3235378861427307
Batch 23/64 loss: -0.3155075013637543
Batch 24/64 loss: -0.3668330907821655
Batch 25/64 loss: -0.3244890868663788
Batch 26/64 loss: -0.2924567461013794
Batch 27/64 loss: -0.3157607614994049
Batch 28/64 loss: -0.3444421887397766
Batch 29/64 loss: -0.3379400372505188
Batch 30/64 loss: -0.3398846983909607
Batch 31/64 loss: -0.3431186079978943
Batch 32/64 loss: -0.3219453692436218
Batch 33/64 loss: -0.3253650665283203
Batch 34/64 loss: -0.3397979438304901
Batch 35/64 loss: -0.3056275248527527
Batch 36/64 loss: -0.3244940936565399
Batch 37/64 loss: -0.341671347618103
Batch 38/64 loss: -0.3327195346355438
Batch 39/64 loss: -0.3463473618030548
Batch 40/64 loss: -0.3251049220561981
Batch 41/64 loss: -0.34215277433395386
Batch 42/64 loss: -0.34826210141181946
Batch 43/64 loss: -0.32555583119392395
Batch 44/64 loss: -0.3193059265613556
Batch 45/64 loss: -0.2883530259132385
Batch 46/64 loss: -0.32755208015441895
Batch 47/64 loss: -0.30354005098342896
Batch 48/64 loss: -0.30128633975982666
Batch 49/64 loss: -0.34440064430236816
Batch 50/64 loss: -0.3041355311870575
Batch 51/64 loss: -0.31657466292381287
Batch 52/64 loss: -0.3332276940345764
Batch 53/64 loss: -0.34271788597106934
Batch 54/64 loss: -0.3451550602912903
Batch 55/64 loss: -0.31551259756088257
Batch 56/64 loss: -0.33099839091300964
Batch 57/64 loss: -0.3548320531845093
Batch 58/64 loss: -0.32604730129241943
Batch 59/64 loss: -0.3197481334209442
Batch 60/64 loss: -0.34232819080352783
Batch 61/64 loss: -0.280182808637619
Batch 62/64 loss: -0.34045296907424927
Batch 63/64 loss: -0.3109564781188965
Batch 64/64 loss: -0.2963883876800537
Epoch 349  Train loss: -0.33214020027833824  Val loss: 0.03217616544146718
Epoch 350
-------------------------------
Batch 1/64 loss: -0.30402639508247375
Batch 2/64 loss: -0.3461736738681793
Batch 3/64 loss: -0.31654131412506104
Batch 4/64 loss: -0.33121630549430847
Batch 5/64 loss: -0.3070029020309448
Batch 6/64 loss: -0.3316701650619507
Batch 7/64 loss: -0.3474528193473816
Batch 8/64 loss: -0.3525959849357605
Batch 9/64 loss: -0.3193964660167694
Batch 10/64 loss: -0.3209584653377533
Batch 11/64 loss: -0.32951003313064575
Batch 12/64 loss: -0.3597029149532318
Batch 13/64 loss: -0.34275227785110474
Batch 14/64 loss: -0.3098362982273102
Batch 15/64 loss: -0.3225961923599243
Batch 16/64 loss: -0.3118933141231537
Batch 17/64 loss: -0.3378671407699585
Batch 18/64 loss: -0.3453640937805176
Batch 19/64 loss: -0.34255558252334595
Batch 20/64 loss: -0.3195880055427551
Batch 21/64 loss: -0.32208311557769775
Batch 22/64 loss: -0.3520015776157379
Batch 23/64 loss: -0.3368759751319885
Batch 24/64 loss: -0.34112706780433655
Batch 25/64 loss: -0.33534127473831177
Batch 26/64 loss: -0.34814852476119995
Batch 27/64 loss: -0.36484038829803467
Batch 28/64 loss: -0.30983296036720276
Batch 29/64 loss: -0.34672772884368896
Batch 30/64 loss: -0.3242930769920349
Batch 31/64 loss: -0.3553069233894348
Batch 32/64 loss: -0.3422294855117798
Batch 33/64 loss: -0.33054789900779724
Batch 34/64 loss: -0.3580932021141052
Batch 35/64 loss: -0.29039353132247925
Batch 36/64 loss: -0.33069610595703125
Batch 37/64 loss: -0.3167116641998291
Batch 38/64 loss: -0.3130127191543579
Batch 39/64 loss: -0.33985742926597595
Batch 40/64 loss: -0.34234708547592163
Batch 41/64 loss: -0.34713393449783325
Batch 42/64 loss: -0.3230692446231842
Batch 43/64 loss: -0.2868608832359314
Batch 44/64 loss: -0.2827529311180115
Batch 45/64 loss: -0.2975847125053406
Batch 46/64 loss: -0.30695873498916626
Batch 47/64 loss: -0.3332620859146118
Batch 48/64 loss: -0.3119228184223175
Batch 49/64 loss: -0.36178845167160034
Batch 50/64 loss: -0.3321418762207031
Batch 51/64 loss: -0.34071868658065796
Batch 52/64 loss: -0.37386369705200195
Batch 53/64 loss: -0.3334132134914398
Batch 54/64 loss: -0.3537091016769409
Batch 55/64 loss: -0.3857146203517914
Batch 56/64 loss: -0.33171719312667847
Batch 57/64 loss: -0.3425673842430115
Batch 58/64 loss: -0.35301417112350464
Batch 59/64 loss: -0.24423512816429138
Batch 60/64 loss: -0.32881680130958557
Batch 61/64 loss: -0.3368227481842041
Batch 62/64 loss: -0.3585794270038605
Batch 63/64 loss: -0.3224893808364868
Batch 64/64 loss: -0.2988964319229126
Epoch 350  Train loss: -0.33117611969218536  Val loss: 0.03183706355668425
Epoch 351
-------------------------------
Batch 1/64 loss: -0.3479178845882416
Batch 2/64 loss: -0.3139227330684662
Batch 3/64 loss: -0.3628580868244171
Batch 4/64 loss: -0.24700114130973816
Batch 5/64 loss: -0.34976086020469666
Batch 6/64 loss: -0.32568520307540894
Batch 7/64 loss: -0.33594706654548645
Batch 8/64 loss: -0.34793752431869507
Batch 9/64 loss: -0.3337057828903198
Batch 10/64 loss: -0.31807276606559753
Batch 11/64 loss: -0.35389137268066406
Batch 12/64 loss: -0.30374497175216675
Batch 13/64 loss: -0.33415883779525757
Batch 14/64 loss: -0.3471195101737976
Batch 15/64 loss: -0.3297920227050781
Batch 16/64 loss: -0.3490777611732483
Batch 17/64 loss: -0.35606926679611206
Batch 18/64 loss: -0.32713067531585693
Batch 19/64 loss: -0.3427158296108246
Batch 20/64 loss: -0.34868893027305603
Batch 21/64 loss: -0.34469133615493774
Batch 22/64 loss: -0.3426528871059418
Batch 23/64 loss: -0.30884766578674316
Batch 24/64 loss: -0.34934842586517334
Batch 25/64 loss: -0.32227280735969543
Batch 26/64 loss: -0.33623144030570984
Batch 27/64 loss: -0.3528788983821869
Batch 28/64 loss: -0.27575916051864624
Batch 29/64 loss: -0.3392828106880188
Batch 30/64 loss: -0.34848129749298096
Batch 31/64 loss: -0.36151498556137085
Batch 32/64 loss: -0.34820422530174255
Batch 33/64 loss: -0.36790889501571655
Batch 34/64 loss: -0.3376467823982239
Batch 35/64 loss: -0.2951894700527191
Batch 36/64 loss: -0.3436676561832428
Batch 37/64 loss: -0.34278303384780884
Batch 38/64 loss: -0.33366212248802185
Batch 39/64 loss: -0.3180837035179138
Batch 40/64 loss: -0.2506714463233948
Batch 41/64 loss: -0.32840338349342346
Batch 42/64 loss: -0.3193527162075043
Batch 43/64 loss: -0.32425636053085327
Batch 44/64 loss: -0.31846341490745544
Batch 45/64 loss: -0.3058738708496094
Batch 46/64 loss: -0.34838414192199707
Batch 47/64 loss: -0.3266921639442444
Batch 48/64 loss: -0.3113146424293518
Batch 49/64 loss: -0.3041684627532959
Batch 50/64 loss: -0.3019554018974304
Batch 51/64 loss: -0.33405229449272156
Batch 52/64 loss: -0.34908533096313477
Batch 53/64 loss: -0.3119971752166748
Batch 54/64 loss: -0.3187483549118042
Batch 55/64 loss: -0.3124207556247711
Batch 56/64 loss: -0.3390434682369232
Batch 57/64 loss: -0.3249228596687317
Batch 58/64 loss: -0.341464102268219
Batch 59/64 loss: -0.29283416271209717
Batch 60/64 loss: -0.3315523862838745
Batch 61/64 loss: -0.34152090549468994
Batch 62/64 loss: -0.33842307329177856
Batch 63/64 loss: -0.358494371175766
Batch 64/64 loss: -0.35787123441696167
Epoch 351  Train loss: -0.33014595298206106  Val loss: 0.03244440723530615
Epoch 352
-------------------------------
Batch 1/64 loss: -0.3799341917037964
Batch 2/64 loss: -0.344644695520401
Batch 3/64 loss: -0.3688557744026184
Batch 4/64 loss: -0.35403960943222046
Batch 5/64 loss: -0.375261127948761
Batch 6/64 loss: -0.2917163670063019
Batch 7/64 loss: -0.3226360082626343
Batch 8/64 loss: -0.3560231029987335
Batch 9/64 loss: -0.34236761927604675
Batch 10/64 loss: -0.34209930896759033
Batch 11/64 loss: -0.35285308957099915
Batch 12/64 loss: -0.3323880434036255
Batch 13/64 loss: -0.33118778467178345
Batch 14/64 loss: -0.36124011874198914
Batch 15/64 loss: -0.3379840850830078
Batch 16/64 loss: -0.31345033645629883
Batch 17/64 loss: -0.35915157198905945
Batch 18/64 loss: -0.34920406341552734
Batch 19/64 loss: -0.32849663496017456
Batch 20/64 loss: -0.3355328142642975
Batch 21/64 loss: -0.3198053240776062
Batch 22/64 loss: -0.3442903757095337
Batch 23/64 loss: -0.36307716369628906
Batch 24/64 loss: -0.35819098353385925
Batch 25/64 loss: -0.3276994824409485
Batch 26/64 loss: -0.30696314573287964
Batch 27/64 loss: -0.3566156029701233
Batch 28/64 loss: -0.3252534568309784
Batch 29/64 loss: -0.3206965923309326
Batch 30/64 loss: -0.3390148878097534
Batch 31/64 loss: -0.3748127818107605
Batch 32/64 loss: -0.3131089210510254
Batch 33/64 loss: -0.33124619722366333
Batch 34/64 loss: -0.33729812502861023
Batch 35/64 loss: -0.34840020537376404
Batch 36/64 loss: -0.3065549433231354
Batch 37/64 loss: -0.36686813831329346
Batch 38/64 loss: -0.3465263843536377
Batch 39/64 loss: -0.2424626350402832
Batch 40/64 loss: -0.35601043701171875
Batch 41/64 loss: -0.3443692624568939
Batch 42/64 loss: -0.3225507438182831
Batch 43/64 loss: -0.31847745180130005
Batch 44/64 loss: -0.33929944038391113
Batch 45/64 loss: -0.3576742708683014
Batch 46/64 loss: -0.3368874192237854
Batch 47/64 loss: -0.34460294246673584
Batch 48/64 loss: -0.36364251375198364
Batch 49/64 loss: -0.2835841178894043
Batch 50/64 loss: -0.3128697872161865
Batch 51/64 loss: -0.3526751399040222
Batch 52/64 loss: -0.34846532344818115
Batch 53/64 loss: -0.2924709618091583
Batch 54/64 loss: -0.3203534483909607
Batch 55/64 loss: -0.34210023283958435
Batch 56/64 loss: -0.30727332830429077
Batch 57/64 loss: -0.3458802103996277
Batch 58/64 loss: -0.3303232491016388
Batch 59/64 loss: -0.33405452966690063
Batch 60/64 loss: -0.3351809084415436
Batch 61/64 loss: -0.33100953698158264
Batch 62/64 loss: -0.35269832611083984
Batch 63/64 loss: -0.34659749269485474
Batch 64/64 loss: -0.31428515911102295
Epoch 352  Train loss: -0.3366700649261475  Val loss: 0.03138107040903412
Epoch 353
-------------------------------
Batch 1/64 loss: -0.3534862697124481
Batch 2/64 loss: -0.34648334980010986
Batch 3/64 loss: -0.3327621817588806
Batch 4/64 loss: -0.3492901921272278
Batch 5/64 loss: -0.3831997215747833
Batch 6/64 loss: -0.31593331694602966
Batch 7/64 loss: -0.3496429920196533
Batch 8/64 loss: -0.3195107579231262
Batch 9/64 loss: -0.32445788383483887
Batch 10/64 loss: -0.3033718168735504
Batch 11/64 loss: -0.3573206067085266
Batch 12/64 loss: -0.3142063021659851
Batch 13/64 loss: -0.29902517795562744
Batch 14/64 loss: -0.311049222946167
Batch 15/64 loss: -0.35676878690719604
Batch 16/64 loss: -0.34266626834869385
Batch 17/64 loss: -0.3023456633090973
Batch 18/64 loss: -0.3506457805633545
Batch 19/64 loss: -0.3167715072631836
Batch 20/64 loss: -0.31250569224357605
Batch 21/64 loss: -0.31241315603256226
Batch 22/64 loss: -0.3374137878417969
Batch 23/64 loss: -0.3285290002822876
Batch 24/64 loss: -0.3560158610343933
Batch 25/64 loss: -0.3218235373497009
Batch 26/64 loss: -0.29652267694473267
Batch 27/64 loss: -0.3192453384399414
Batch 28/64 loss: -0.29233694076538086
Batch 29/64 loss: -0.3238047957420349
Batch 30/64 loss: -0.3301481008529663
Batch 31/64 loss: -0.3369501829147339
Batch 32/64 loss: -0.2921898663043976
Batch 33/64 loss: -0.36246705055236816
Batch 34/64 loss: -0.3468175530433655
Batch 35/64 loss: -0.312399685382843
Batch 36/64 loss: -0.3324154019355774
Batch 37/64 loss: -0.33885595202445984
Batch 38/64 loss: -0.30881449580192566
Batch 39/64 loss: -0.354289174079895
Batch 40/64 loss: -0.2951747179031372
Batch 41/64 loss: -0.2751513421535492
Batch 42/64 loss: -0.34936392307281494
Batch 43/64 loss: -0.32297128438949585
Batch 44/64 loss: -0.29060521721839905
Batch 45/64 loss: -0.35098597407341003
Batch 46/64 loss: -0.35368093848228455
Batch 47/64 loss: -0.35170939564704895
Batch 48/64 loss: -0.359706312417984
Batch 49/64 loss: -0.3126119375228882
Batch 50/64 loss: -0.34909337759017944
Batch 51/64 loss: -0.3496488034725189
Batch 52/64 loss: -0.2957456111907959
Batch 53/64 loss: -0.3238578140735626
Batch 54/64 loss: -0.3215368688106537
Batch 55/64 loss: -0.3639415204524994
Batch 56/64 loss: -0.34038475155830383
Batch 57/64 loss: -0.33165645599365234
Batch 58/64 loss: -0.2955998182296753
Batch 59/64 loss: -0.3294675946235657
Batch 60/64 loss: -0.3268335461616516
Batch 61/64 loss: -0.3422119915485382
Batch 62/64 loss: -0.3235582709312439
Batch 63/64 loss: -0.34640294313430786
Batch 64/64 loss: -0.26859214901924133
Epoch 353  Train loss: -0.3285998520897884  Val loss: 0.032918953403984146
Epoch 354
-------------------------------
Batch 1/64 loss: -0.3822701871395111
Batch 2/64 loss: -0.3317423462867737
Batch 3/64 loss: -0.3375473916530609
Batch 4/64 loss: -0.3077526092529297
Batch 5/64 loss: -0.3315221667289734
Batch 6/64 loss: -0.35563337802886963
Batch 7/64 loss: -0.34246528148651123
Batch 8/64 loss: -0.3248893618583679
Batch 9/64 loss: -0.3293282389640808
Batch 10/64 loss: -0.3588196635246277
Batch 11/64 loss: -0.34713882207870483
Batch 12/64 loss: -0.3596567213535309
Batch 13/64 loss: -0.351198673248291
Batch 14/64 loss: -0.3626246750354767
Batch 15/64 loss: -0.2966289520263672
Batch 16/64 loss: -0.32646411657333374
Batch 17/64 loss: -0.3391457498073578
Batch 18/64 loss: -0.33948081731796265
Batch 19/64 loss: -0.34748393297195435
Batch 20/64 loss: -0.3654925525188446
Batch 21/64 loss: -0.33745312690734863
Batch 22/64 loss: -0.35422685742378235
Batch 23/64 loss: -0.35059046745300293
Batch 24/64 loss: -0.36642134189605713
Batch 25/64 loss: -0.35884809494018555
Batch 26/64 loss: -0.31684714555740356
Batch 27/64 loss: -0.35187214612960815
Batch 28/64 loss: -0.3709559440612793
Batch 29/64 loss: -0.3631806969642639
Batch 30/64 loss: -0.36082327365875244
Batch 31/64 loss: -0.32378607988357544
Batch 32/64 loss: -0.34690362215042114
Batch 33/64 loss: -0.3073216378688812
Batch 34/64 loss: -0.31633856892585754
Batch 35/64 loss: -0.3254624605178833
Batch 36/64 loss: -0.30551591515541077
Batch 37/64 loss: -0.3455296754837036
Batch 38/64 loss: -0.33015432953834534
Batch 39/64 loss: -0.33081334829330444
Batch 40/64 loss: -0.3201148509979248
Batch 41/64 loss: -0.3128827214241028
Batch 42/64 loss: -0.3176036775112152
Batch 43/64 loss: -0.3424786329269409
Batch 44/64 loss: -0.3272000849246979
Batch 45/64 loss: -0.302375465631485
Batch 46/64 loss: -0.3321698307991028
Batch 47/64 loss: -0.317996084690094
Batch 48/64 loss: -0.31903892755508423
Batch 49/64 loss: -0.3480198383331299
Batch 50/64 loss: -0.3226315677165985
Batch 51/64 loss: -0.3303462862968445
Batch 52/64 loss: -0.2996672987937927
Batch 53/64 loss: -0.3317793309688568
Batch 54/64 loss: -0.30425310134887695
Batch 55/64 loss: -0.3405907154083252
Batch 56/64 loss: -0.330533504486084
Batch 57/64 loss: -0.3429744243621826
Batch 58/64 loss: -0.3456980288028717
Batch 59/64 loss: -0.35506707429885864
Batch 60/64 loss: -0.3132031559944153
Batch 61/64 loss: -0.34850358963012695
Batch 62/64 loss: -0.3158378601074219
Batch 63/64 loss: -0.2892445921897888
Batch 64/64 loss: -0.3135802149772644
Epoch 354  Train loss: -0.33483492042504104  Val loss: 0.03392184251772169
Epoch 355
-------------------------------
Batch 1/64 loss: -0.37794435024261475
Batch 2/64 loss: -0.32813265919685364
Batch 3/64 loss: -0.3519424796104431
Batch 4/64 loss: -0.3325348496437073
Batch 5/64 loss: -0.3647439181804657
Batch 6/64 loss: -0.3657715916633606
Batch 7/64 loss: -0.37036025524139404
Batch 8/64 loss: -0.3464207947254181
Batch 9/64 loss: -0.34451308846473694
Batch 10/64 loss: -0.36251914501190186
Batch 11/64 loss: -0.3651381731033325
Batch 12/64 loss: -0.3502117395401001
Batch 13/64 loss: -0.3336816430091858
Batch 14/64 loss: -0.3349542021751404
Batch 15/64 loss: -0.3497689962387085
Batch 16/64 loss: -0.30303657054901123
Batch 17/64 loss: -0.2868843674659729
Batch 18/64 loss: -0.2838551700115204
Batch 19/64 loss: -0.33855897188186646
Batch 20/64 loss: -0.3508361577987671
Batch 21/64 loss: -0.34457454085350037
Batch 22/64 loss: -0.32566606998443604
Batch 23/64 loss: -0.3487464487552643
Batch 24/64 loss: -0.34767067432403564
Batch 25/64 loss: -0.3539506196975708
Batch 26/64 loss: -0.3589075207710266
Batch 27/64 loss: -0.36058706045150757
Batch 28/64 loss: -0.3421868085861206
Batch 29/64 loss: -0.3154737055301666
Batch 30/64 loss: -0.34856081008911133
Batch 31/64 loss: -0.32823336124420166
Batch 32/64 loss: -0.35353419184684753
Batch 33/64 loss: -0.3264855146408081
Batch 34/64 loss: -0.34308916330337524
Batch 35/64 loss: -0.33859676122665405
Batch 36/64 loss: -0.35280850529670715
Batch 37/64 loss: -0.32465726137161255
Batch 38/64 loss: -0.2968883514404297
Batch 39/64 loss: -0.3129929304122925
Batch 40/64 loss: -0.33247441053390503
Batch 41/64 loss: -0.3500877916812897
Batch 42/64 loss: -0.34318649768829346
Batch 43/64 loss: -0.3262290954589844
Batch 44/64 loss: -0.34473681449890137
Batch 45/64 loss: -0.3355504870414734
Batch 46/64 loss: -0.3550921678543091
Batch 47/64 loss: -0.3344859778881073
Batch 48/64 loss: -0.3520539700984955
Batch 49/64 loss: -0.35995107889175415
Batch 50/64 loss: -0.2996941804885864
Batch 51/64 loss: -0.33020204305648804
Batch 52/64 loss: -0.2567659616470337
Batch 53/64 loss: -0.3396579325199127
Batch 54/64 loss: -0.3041492998600006
Batch 55/64 loss: -0.34128934144973755
Batch 56/64 loss: -0.3122181296348572
Batch 57/64 loss: -0.330363005399704
Batch 58/64 loss: -0.35127580165863037
Batch 59/64 loss: -0.350117027759552
Batch 60/64 loss: -0.3543440103530884
Batch 61/64 loss: -0.3398384153842926
Batch 62/64 loss: -0.30643123388290405
Batch 63/64 loss: -0.36789557337760925
Batch 64/64 loss: -0.35161057114601135
Epoch 355  Train loss: -0.3379955701968249  Val loss: 0.029008380531035747
Epoch 356
-------------------------------
Batch 1/64 loss: -0.344321608543396
Batch 2/64 loss: -0.35750770568847656
Batch 3/64 loss: -0.35551929473876953
Batch 4/64 loss: -0.3181350827217102
Batch 5/64 loss: -0.30807700753211975
Batch 6/64 loss: -0.33923178911209106
Batch 7/64 loss: -0.35453692078590393
Batch 8/64 loss: -0.36188969016075134
Batch 9/64 loss: -0.36402449011802673
Batch 10/64 loss: -0.3252473473548889
Batch 11/64 loss: -0.35198724269866943
Batch 12/64 loss: -0.32403308153152466
Batch 13/64 loss: -0.3505341410636902
Batch 14/64 loss: -0.336838960647583
Batch 15/64 loss: -0.3167295455932617
Batch 16/64 loss: -0.3151189684867859
Batch 17/64 loss: -0.3544062674045563
Batch 18/64 loss: -0.32840675115585327
Batch 19/64 loss: -0.320795476436615
Batch 20/64 loss: -0.33946484327316284
Batch 21/64 loss: -0.3706214427947998
Batch 22/64 loss: -0.35143110156059265
Batch 23/64 loss: -0.32026755809783936
Batch 24/64 loss: -0.3160362243652344
Batch 25/64 loss: -0.30675894021987915
Batch 26/64 loss: -0.3515031933784485
Batch 27/64 loss: -0.3085692822933197
Batch 28/64 loss: -0.3377082943916321
Batch 29/64 loss: -0.3515901267528534
Batch 30/64 loss: -0.3524682819843292
Batch 31/64 loss: -0.32494497299194336
Batch 32/64 loss: -0.3302519917488098
Batch 33/64 loss: -0.3249024450778961
Batch 34/64 loss: -0.3723413646221161
Batch 35/64 loss: -0.35719823837280273
Batch 36/64 loss: -0.3123337924480438
Batch 37/64 loss: -0.3199227452278137
Batch 38/64 loss: -0.35066354274749756
Batch 39/64 loss: -0.31855449080467224
Batch 40/64 loss: -0.35617226362228394
Batch 41/64 loss: -0.33624178171157837
Batch 42/64 loss: -0.3082178831100464
Batch 43/64 loss: -0.36606496572494507
Batch 44/64 loss: -0.3385651111602783
Batch 45/64 loss: -0.33635929226875305
Batch 46/64 loss: -0.3641049861907959
Batch 47/64 loss: -0.35313987731933594
Batch 48/64 loss: -0.35258546471595764
Batch 49/64 loss: -0.3614056706428528
Batch 50/64 loss: -0.24044427275657654
Batch 51/64 loss: -0.331049382686615
Batch 52/64 loss: -0.33703187108039856
Batch 53/64 loss: -0.3513258099555969
Batch 54/64 loss: -0.3604559302330017
Batch 55/64 loss: -0.3347699046134949
Batch 56/64 loss: -0.3424191474914551
Batch 57/64 loss: -0.35351091623306274
Batch 58/64 loss: -0.33217892050743103
Batch 59/64 loss: -0.36129072308540344
Batch 60/64 loss: -0.3137696385383606
Batch 61/64 loss: -0.35527700185775757
Batch 62/64 loss: -0.3287314474582672
Batch 63/64 loss: -0.33276766538619995
Batch 64/64 loss: -0.3308991491794586
Epoch 356  Train loss: -0.3378969181986416  Val loss: 0.03256109571948494
Epoch 357
-------------------------------
Batch 1/64 loss: -0.3711230456829071
Batch 2/64 loss: -0.351129412651062
Batch 3/64 loss: -0.33543944358825684
Batch 4/64 loss: -0.3620021641254425
Batch 5/64 loss: -0.3210689425468445
Batch 6/64 loss: -0.3443293571472168
Batch 7/64 loss: -0.3243071436882019
Batch 8/64 loss: -0.3107593059539795
Batch 9/64 loss: -0.3635532259941101
Batch 10/64 loss: -0.3164595365524292
Batch 11/64 loss: -0.37044280767440796
Batch 12/64 loss: -0.3256998658180237
Batch 13/64 loss: -0.33843183517456055
Batch 14/64 loss: -0.34500014781951904
Batch 15/64 loss: -0.30344533920288086
Batch 16/64 loss: -0.3403893709182739
Batch 17/64 loss: -0.3066979646682739
Batch 18/64 loss: -0.2585212290287018
Batch 19/64 loss: -0.33596089482307434
Batch 20/64 loss: -0.32756805419921875
Batch 21/64 loss: -0.353361576795578
Batch 22/64 loss: -0.3443269729614258
Batch 23/64 loss: -0.35728591680526733
Batch 24/64 loss: -0.36227452754974365
Batch 25/64 loss: -0.33526307344436646
Batch 26/64 loss: -0.34065574407577515
Batch 27/64 loss: -0.3593819737434387
Batch 28/64 loss: -0.34011900424957275
Batch 29/64 loss: -0.34151729941368103
Batch 30/64 loss: -0.3545607924461365
Batch 31/64 loss: -0.35564011335372925
Batch 32/64 loss: -0.313122421503067
Batch 33/64 loss: -0.3011338412761688
Batch 34/64 loss: -0.33005255460739136
Batch 35/64 loss: -0.35341739654541016
Batch 36/64 loss: -0.3331846296787262
Batch 37/64 loss: -0.33202412724494934
Batch 38/64 loss: -0.32446175813674927
Batch 39/64 loss: -0.36307990550994873
Batch 40/64 loss: -0.3572145998477936
Batch 41/64 loss: -0.35147392749786377
Batch 42/64 loss: -0.33321613073349
Batch 43/64 loss: -0.36823222041130066
Batch 44/64 loss: -0.31305643916130066
Batch 45/64 loss: -0.36749041080474854
Batch 46/64 loss: -0.33038580417633057
Batch 47/64 loss: -0.36193007230758667
Batch 48/64 loss: -0.3492957353591919
Batch 49/64 loss: -0.3302207291126251
Batch 50/64 loss: -0.3452874720096588
Batch 51/64 loss: -0.3364328145980835
Batch 52/64 loss: -0.3395303189754486
Batch 53/64 loss: -0.36208200454711914
Batch 54/64 loss: -0.3668990135192871
Batch 55/64 loss: -0.3391473591327667
Batch 56/64 loss: -0.3259681463241577
Batch 57/64 loss: -0.3361603021621704
Batch 58/64 loss: -0.33436864614486694
Batch 59/64 loss: -0.33726441860198975
Batch 60/64 loss: -0.34205150604248047
Batch 61/64 loss: -0.3430357575416565
Batch 62/64 loss: -0.327780157327652
Batch 63/64 loss: -0.2954670190811157
Batch 64/64 loss: -0.3031430244445801
Epoch 357  Train loss: -0.3383300547506295  Val loss: 0.03233884597562023
Epoch 358
-------------------------------
Batch 1/64 loss: -0.3119675815105438
Batch 2/64 loss: -0.26811137795448303
Batch 3/64 loss: -0.3497903347015381
Batch 4/64 loss: -0.3538234531879425
Batch 5/64 loss: -0.33335357904434204
Batch 6/64 loss: -0.34919148683547974
Batch 7/64 loss: -0.30093619227409363
Batch 8/64 loss: -0.33887410163879395
Batch 9/64 loss: -0.35796451568603516
Batch 10/64 loss: -0.32351166009902954
Batch 11/64 loss: -0.35599464178085327
Batch 12/64 loss: -0.3321525454521179
Batch 13/64 loss: -0.35399553179740906
Batch 14/64 loss: -0.37683600187301636
Batch 15/64 loss: -0.3403967022895813
Batch 16/64 loss: -0.34530648589134216
Batch 17/64 loss: -0.30073678493499756
Batch 18/64 loss: -0.3310275673866272
Batch 19/64 loss: -0.3612867593765259
Batch 20/64 loss: -0.31260353326797485
Batch 21/64 loss: -0.3604702949523926
Batch 22/64 loss: -0.30056118965148926
Batch 23/64 loss: -0.34532690048217773
Batch 24/64 loss: -0.3233433663845062
Batch 25/64 loss: -0.36520421504974365
Batch 26/64 loss: -0.3156737685203552
Batch 27/64 loss: -0.3255884647369385
Batch 28/64 loss: -0.31686660647392273
Batch 29/64 loss: -0.35563206672668457
Batch 30/64 loss: -0.3406318426132202
Batch 31/64 loss: -0.2671297788619995
Batch 32/64 loss: -0.2894550859928131
Batch 33/64 loss: -0.3060852885246277
Batch 34/64 loss: -0.37235790491104126
Batch 35/64 loss: -0.31954067945480347
Batch 36/64 loss: -0.3313790261745453
Batch 37/64 loss: -0.2915324866771698
Batch 38/64 loss: -0.32882076501846313
Batch 39/64 loss: -0.3344661593437195
Batch 40/64 loss: -0.3293657898902893
Batch 41/64 loss: -0.3548915684223175
Batch 42/64 loss: -0.3175492584705353
Batch 43/64 loss: -0.289689302444458
Batch 44/64 loss: -0.34862321615219116
Batch 45/64 loss: -0.36119478940963745
Batch 46/64 loss: -0.361110657453537
Batch 47/64 loss: -0.35816025733947754
Batch 48/64 loss: -0.3431780934333801
Batch 49/64 loss: -0.3346118927001953
Batch 50/64 loss: -0.346709668636322
Batch 51/64 loss: -0.3459416329860687
Batch 52/64 loss: -0.28383830189704895
Batch 53/64 loss: -0.33733993768692017
Batch 54/64 loss: -0.3240829110145569
Batch 55/64 loss: -0.33307281136512756
Batch 56/64 loss: -0.35436826944351196
Batch 57/64 loss: -0.31292611360549927
Batch 58/64 loss: -0.3665319085121155
Batch 59/64 loss: -0.3505361080169678
Batch 60/64 loss: -0.35306406021118164
Batch 61/64 loss: -0.35458114743232727
Batch 62/64 loss: -0.3482181429862976
Batch 63/64 loss: -0.32903093099594116
Batch 64/64 loss: -0.3123570680618286
Epoch 358  Train loss: -0.3335029223385979  Val loss: 0.03279014653766278
Epoch 359
-------------------------------
Batch 1/64 loss: -0.3661404848098755
Batch 2/64 loss: -0.3444821238517761
Batch 3/64 loss: -0.3445184826850891
Batch 4/64 loss: -0.3539317846298218
Batch 5/64 loss: -0.34602636098861694
Batch 6/64 loss: -0.376849889755249
Batch 7/64 loss: -0.338908851146698
Batch 8/64 loss: -0.3904648721218109
Batch 9/64 loss: -0.3687075674533844
Batch 10/64 loss: -0.32568275928497314
Batch 11/64 loss: -0.3383328318595886
Batch 12/64 loss: -0.3701428174972534
Batch 13/64 loss: -0.3378070890903473
Batch 14/64 loss: -0.33239883184432983
Batch 15/64 loss: -0.3161851465702057
Batch 16/64 loss: -0.3808184862136841
Batch 17/64 loss: -0.33348867297172546
Batch 18/64 loss: -0.3663884997367859
Batch 19/64 loss: -0.32810693979263306
Batch 20/64 loss: -0.35383594036102295
Batch 21/64 loss: -0.2932470440864563
Batch 22/64 loss: -0.30425208806991577
Batch 23/64 loss: -0.3093950152397156
Batch 24/64 loss: -0.36247748136520386
Batch 25/64 loss: -0.33265817165374756
Batch 26/64 loss: -0.35345953702926636
Batch 27/64 loss: -0.2942788302898407
Batch 28/64 loss: -0.30399441719055176
Batch 29/64 loss: -0.3149944543838501
Batch 30/64 loss: -0.3347201645374298
Batch 31/64 loss: -0.3445994257926941
Batch 32/64 loss: -0.316061794757843
Batch 33/64 loss: -0.31630027294158936
Batch 34/64 loss: -0.356941282749176
Batch 35/64 loss: -0.3218173682689667
Batch 36/64 loss: -0.3123587965965271
Batch 37/64 loss: -0.3691059350967407
Batch 38/64 loss: -0.3242330253124237
Batch 39/64 loss: -0.3375394940376282
Batch 40/64 loss: -0.3258722424507141
Batch 41/64 loss: -0.283005028963089
Batch 42/64 loss: -0.35203418135643005
Batch 43/64 loss: -0.3544260859489441
Batch 44/64 loss: -0.3544026017189026
Batch 45/64 loss: -0.31389468908309937
Batch 46/64 loss: -0.33855950832366943
Batch 47/64 loss: -0.3708239793777466
Batch 48/64 loss: -0.3391296863555908
Batch 49/64 loss: -0.2934712767601013
Batch 50/64 loss: -0.3226044774055481
Batch 51/64 loss: -0.3067895770072937
Batch 52/64 loss: -0.3190279006958008
Batch 53/64 loss: -0.3493311107158661
Batch 54/64 loss: -0.33751848340034485
Batch 55/64 loss: -0.3593413829803467
Batch 56/64 loss: -0.3582790493965149
Batch 57/64 loss: -0.3592691719532013
Batch 58/64 loss: -0.34272557497024536
Batch 59/64 loss: -0.342171311378479
Batch 60/64 loss: -0.28373122215270996
Batch 61/64 loss: -0.30310264229774475
Batch 62/64 loss: -0.33673661947250366
Batch 63/64 loss: -0.3286101222038269
Batch 64/64 loss: -0.3635627031326294
Epoch 359  Train loss: -0.3366773801691392  Val loss: 0.03140281115200921
Epoch 360
-------------------------------
Batch 1/64 loss: -0.35740044713020325
Batch 2/64 loss: -0.36644938588142395
Batch 3/64 loss: -0.3303532004356384
Batch 4/64 loss: -0.3587666451931
Batch 5/64 loss: -0.3463784456253052
Batch 6/64 loss: -0.3481054902076721
Batch 7/64 loss: -0.3593176007270813
Batch 8/64 loss: -0.35092684626579285
Batch 9/64 loss: -0.3369467258453369
Batch 10/64 loss: -0.317436546087265
Batch 11/64 loss: -0.2910284996032715
Batch 12/64 loss: -0.34823036193847656
Batch 13/64 loss: -0.36071404814720154
Batch 14/64 loss: -0.3418460786342621
Batch 15/64 loss: -0.3459796905517578
Batch 16/64 loss: -0.3272349536418915
Batch 17/64 loss: -0.34671923518180847
Batch 18/64 loss: -0.30675816535949707
Batch 19/64 loss: -0.3140673041343689
Batch 20/64 loss: -0.3533405065536499
Batch 21/64 loss: -0.2691161632537842
Batch 22/64 loss: -0.2776601314544678
Batch 23/64 loss: -0.3264181613922119
Batch 24/64 loss: -0.3345317840576172
Batch 25/64 loss: -0.34902405738830566
Batch 26/64 loss: -0.32284781336784363
Batch 27/64 loss: -0.32658976316452026
Batch 28/64 loss: -0.32629722356796265
Batch 29/64 loss: -0.3517582416534424
Batch 30/64 loss: -0.2979022264480591
Batch 31/64 loss: -0.32653287053108215
Batch 32/64 loss: -0.35094958543777466
Batch 33/64 loss: -0.3334615230560303
Batch 34/64 loss: -0.3704646825790405
Batch 35/64 loss: -0.3226357698440552
Batch 36/64 loss: -0.33654701709747314
Batch 37/64 loss: -0.29696890711784363
Batch 38/64 loss: -0.35708969831466675
Batch 39/64 loss: -0.32941165566444397
Batch 40/64 loss: -0.3286915123462677
Batch 41/64 loss: -0.3206638693809509
Batch 42/64 loss: -0.34770041704177856
Batch 43/64 loss: -0.31359827518463135
Batch 44/64 loss: -0.3683174252510071
Batch 45/64 loss: -0.3124828338623047
Batch 46/64 loss: -0.3397599160671234
Batch 47/64 loss: -0.34466278553009033
Batch 48/64 loss: -0.3422408401966095
Batch 49/64 loss: -0.32965314388275146
Batch 50/64 loss: -0.3296511471271515
Batch 51/64 loss: -0.3269166946411133
Batch 52/64 loss: -0.3331303298473358
Batch 53/64 loss: -0.3170377314090729
Batch 54/64 loss: -0.33704474568367004
Batch 55/64 loss: -0.37516480684280396
Batch 56/64 loss: -0.3602299094200134
Batch 57/64 loss: -0.34930071234703064
Batch 58/64 loss: -0.35721659660339355
Batch 59/64 loss: -0.3378068208694458
Batch 60/64 loss: -0.3469431698322296
Batch 61/64 loss: -0.28530874848365784
Batch 62/64 loss: -0.363637238740921
Batch 63/64 loss: -0.31973177194595337
Batch 64/64 loss: -0.3324543833732605
Epoch 360  Train loss: -0.33490885776631973  Val loss: 0.03325249997201245
Epoch 361
-------------------------------
Batch 1/64 loss: -0.33421897888183594
Batch 2/64 loss: -0.33873897790908813
Batch 3/64 loss: -0.3550555109977722
Batch 4/64 loss: -0.3554140627384186
Batch 5/64 loss: -0.3665071725845337
Batch 6/64 loss: -0.331402063369751
Batch 7/64 loss: -0.3285582661628723
Batch 8/64 loss: -0.340431809425354
Batch 9/64 loss: -0.3466092348098755
Batch 10/64 loss: -0.3111138939857483
Batch 11/64 loss: -0.33943021297454834
Batch 12/64 loss: -0.3427887558937073
Batch 13/64 loss: -0.3031865656375885
Batch 14/64 loss: -0.3572641611099243
Batch 15/64 loss: -0.3214978873729706
Batch 16/64 loss: -0.34932154417037964
Batch 17/64 loss: -0.32648730278015137
Batch 18/64 loss: -0.3059118390083313
Batch 19/64 loss: -0.3619470000267029
Batch 20/64 loss: -0.35848695039749146
Batch 21/64 loss: -0.3486706018447876
Batch 22/64 loss: -0.3509441018104553
Batch 23/64 loss: -0.3047446012496948
Batch 24/64 loss: -0.3270406723022461
Batch 25/64 loss: -0.3175153136253357
Batch 26/64 loss: -0.34225454926490784
Batch 27/64 loss: -0.3334805965423584
Batch 28/64 loss: -0.33113783597946167
Batch 29/64 loss: -0.32223132252693176
Batch 30/64 loss: -0.34010928869247437
Batch 31/64 loss: -0.3524419665336609
Batch 32/64 loss: -0.37341243028640747
Batch 33/64 loss: -0.3466169834136963
Batch 34/64 loss: -0.3422628939151764
Batch 35/64 loss: -0.35119107365608215
Batch 36/64 loss: -0.32911279797554016
Batch 37/64 loss: -0.3630145788192749
Batch 38/64 loss: -0.34774309396743774
Batch 39/64 loss: -0.3510738015174866
Batch 40/64 loss: -0.32527583837509155
Batch 41/64 loss: -0.34848517179489136
Batch 42/64 loss: -0.3138955235481262
Batch 43/64 loss: -0.33470797538757324
Batch 44/64 loss: -0.3297058939933777
Batch 45/64 loss: -0.35344189405441284
Batch 46/64 loss: -0.31690382957458496
Batch 47/64 loss: -0.3230058550834656
Batch 48/64 loss: -0.31029999256134033
Batch 49/64 loss: -0.2778356075286865
Batch 50/64 loss: -0.32404524087905884
Batch 51/64 loss: -0.3314163088798523
Batch 52/64 loss: -0.33685415983200073
Batch 53/64 loss: -0.35426077246665955
Batch 54/64 loss: -0.3244639039039612
Batch 55/64 loss: -0.340861976146698
Batch 56/64 loss: -0.3699663281440735
Batch 57/64 loss: -0.3327677845954895
Batch 58/64 loss: -0.30214646458625793
Batch 59/64 loss: -0.2731984555721283
Batch 60/64 loss: -0.3536657691001892
Batch 61/64 loss: -0.33293455839157104
Batch 62/64 loss: -0.35441264510154724
Batch 63/64 loss: -0.31579989194869995
Batch 64/64 loss: -0.32699766755104065
Epoch 361  Train loss: -0.33529365939252515  Val loss: 0.029881774559873075
Epoch 362
-------------------------------
Batch 1/64 loss: -0.33364763855934143
Batch 2/64 loss: -0.3085220456123352
Batch 3/64 loss: -0.36395323276519775
Batch 4/64 loss: -0.3290781080722809
Batch 5/64 loss: -0.3494901657104492
Batch 6/64 loss: -0.2878207564353943
Batch 7/64 loss: -0.34702542424201965
Batch 8/64 loss: -0.33858805894851685
Batch 9/64 loss: -0.30745863914489746
Batch 10/64 loss: -0.3726857900619507
Batch 11/64 loss: -0.3286401927471161
Batch 12/64 loss: -0.31319767236709595
Batch 13/64 loss: -0.3207746148109436
Batch 14/64 loss: -0.3752402663230896
Batch 15/64 loss: -0.3466908931732178
Batch 16/64 loss: -0.35388100147247314
Batch 17/64 loss: -0.3511543869972229
Batch 18/64 loss: -0.3346584141254425
Batch 19/64 loss: -0.3229922652244568
Batch 20/64 loss: -0.36966240406036377
Batch 21/64 loss: -0.36736512184143066
Batch 22/64 loss: -0.3576732873916626
Batch 23/64 loss: -0.32865110039711
Batch 24/64 loss: -0.29793256521224976
Batch 25/64 loss: -0.3464656472206116
Batch 26/64 loss: -0.32619398832321167
Batch 27/64 loss: -0.3223746120929718
Batch 28/64 loss: -0.3530330955982208
Batch 29/64 loss: -0.3738609552383423
Batch 30/64 loss: -0.3228421211242676
Batch 31/64 loss: -0.3433499336242676
Batch 32/64 loss: -0.3271775245666504
Batch 33/64 loss: -0.3632745146751404
Batch 34/64 loss: -0.36500638723373413
Batch 35/64 loss: -0.3387787938117981
Batch 36/64 loss: -0.3667595386505127
Batch 37/64 loss: -0.3369781970977783
Batch 38/64 loss: -0.2858518064022064
Batch 39/64 loss: -0.358695387840271
Batch 40/64 loss: -0.3040883541107178
Batch 41/64 loss: -0.34478211402893066
Batch 42/64 loss: -0.34013429284095764
Batch 43/64 loss: -0.341537743806839
Batch 44/64 loss: -0.3126824200153351
Batch 45/64 loss: -0.3388688564300537
Batch 46/64 loss: -0.3482229709625244
Batch 47/64 loss: -0.3568926751613617
Batch 48/64 loss: -0.3381890654563904
Batch 49/64 loss: -0.345845490694046
Batch 50/64 loss: -0.3083678185939789
Batch 51/64 loss: -0.29676079750061035
Batch 52/64 loss: -0.35095351934432983
Batch 53/64 loss: -0.33707019686698914
Batch 54/64 loss: -0.34801989793777466
Batch 55/64 loss: -0.34338894486427307
Batch 56/64 loss: -0.3113122284412384
Batch 57/64 loss: -0.34694722294807434
Batch 58/64 loss: -0.3076270520687103
Batch 59/64 loss: -0.33605581521987915
Batch 60/64 loss: -0.337274968624115
Batch 61/64 loss: -0.3289504647254944
Batch 62/64 loss: -0.34591078758239746
Batch 63/64 loss: -0.3702155351638794
Batch 64/64 loss: -0.3333202004432678
Epoch 362  Train loss: -0.3376865244379231  Val loss: 0.031151864946503
Epoch 363
-------------------------------
Batch 1/64 loss: -0.35136687755584717
Batch 2/64 loss: -0.3451915979385376
Batch 3/64 loss: -0.34020036458969116
Batch 4/64 loss: -0.3489331603050232
Batch 5/64 loss: -0.37331485748291016
Batch 6/64 loss: -0.3666095733642578
Batch 7/64 loss: -0.3102046251296997
Batch 8/64 loss: -0.3376852869987488
Batch 9/64 loss: -0.3512086272239685
Batch 10/64 loss: -0.3619807958602905
Batch 11/64 loss: -0.2883232533931732
Batch 12/64 loss: -0.3072645664215088
Batch 13/64 loss: -0.36890265345573425
Batch 14/64 loss: -0.3229074478149414
Batch 15/64 loss: -0.3855559229850769
Batch 16/64 loss: -0.31653717160224915
Batch 17/64 loss: -0.3548990488052368
Batch 18/64 loss: -0.3554767966270447
Batch 19/64 loss: -0.3098434805870056
Batch 20/64 loss: -0.35533443093299866
Batch 21/64 loss: -0.3383778929710388
Batch 22/64 loss: -0.36935892701148987
Batch 23/64 loss: -0.34571367502212524
Batch 24/64 loss: -0.34968847036361694
Batch 25/64 loss: -0.34287863969802856
Batch 26/64 loss: -0.36523205041885376
Batch 27/64 loss: -0.3013521432876587
Batch 28/64 loss: -0.3175188899040222
Batch 29/64 loss: -0.3424571454524994
Batch 30/64 loss: -0.3422595262527466
Batch 31/64 loss: -0.31387990713119507
Batch 32/64 loss: -0.3599410057067871
Batch 33/64 loss: -0.3267623484134674
Batch 34/64 loss: -0.3279750943183899
Batch 35/64 loss: -0.36542463302612305
Batch 36/64 loss: -0.33017802238464355
Batch 37/64 loss: -0.2944072484970093
Batch 38/64 loss: -0.37438786029815674
Batch 39/64 loss: -0.3712707459926605
Batch 40/64 loss: -0.33879393339157104
Batch 41/64 loss: -0.34071141481399536
Batch 42/64 loss: -0.30620038509368896
Batch 43/64 loss: -0.3343355059623718
Batch 44/64 loss: -0.3579176068305969
Batch 45/64 loss: -0.28382399678230286
Batch 46/64 loss: -0.32738208770751953
Batch 47/64 loss: -0.32981643080711365
Batch 48/64 loss: -0.3368808329105377
Batch 49/64 loss: -0.33355337381362915
Batch 50/64 loss: -0.3339270353317261
Batch 51/64 loss: -0.3778442144393921
Batch 52/64 loss: -0.3458509147167206
Batch 53/64 loss: -0.31646478176116943
Batch 54/64 loss: -0.33026185631752014
Batch 55/64 loss: -0.3062037229537964
Batch 56/64 loss: -0.3765098452568054
Batch 57/64 loss: -0.35476988554000854
Batch 58/64 loss: -0.34191155433654785
Batch 59/64 loss: -0.3265962600708008
Batch 60/64 loss: -0.34057947993278503
Batch 61/64 loss: -0.3327908217906952
Batch 62/64 loss: -0.3458224833011627
Batch 63/64 loss: -0.3455924391746521
Batch 64/64 loss: -0.3352836072444916
Epoch 363  Train loss: -0.33955777778345  Val loss: 0.0332135713796845
Epoch 364
-------------------------------
Batch 1/64 loss: -0.34631258249282837
Batch 2/64 loss: -0.3472129702568054
Batch 3/64 loss: -0.32659727334976196
Batch 4/64 loss: -0.329351007938385
Batch 5/64 loss: -0.29766252636909485
Batch 6/64 loss: -0.3554927706718445
Batch 7/64 loss: -0.3670176863670349
Batch 8/64 loss: -0.30828791856765747
Batch 9/64 loss: -0.3738843500614166
Batch 10/64 loss: -0.3557529151439667
Batch 11/64 loss: -0.3375103771686554
Batch 12/64 loss: -0.33684349060058594
Batch 13/64 loss: -0.3233034610748291
Batch 14/64 loss: -0.35326194763183594
Batch 15/64 loss: -0.35825401544570923
Batch 16/64 loss: -0.3560939133167267
Batch 17/64 loss: -0.35887813568115234
Batch 18/64 loss: -0.32381653785705566
Batch 19/64 loss: -0.35176053643226624
Batch 20/64 loss: -0.3437778353691101
Batch 21/64 loss: -0.3763321340084076
Batch 22/64 loss: -0.34801846742630005
Batch 23/64 loss: -0.3489970564842224
Batch 24/64 loss: -0.3542136549949646
Batch 25/64 loss: -0.32974085211753845
Batch 26/64 loss: -0.3602474331855774
Batch 27/64 loss: -0.3228834569454193
Batch 28/64 loss: -0.35870617628097534
Batch 29/64 loss: -0.35028472542762756
Batch 30/64 loss: -0.3187858462333679
Batch 31/64 loss: -0.3044852018356323
Batch 32/64 loss: -0.33188730478286743
Batch 33/64 loss: -0.34519481658935547
Batch 34/64 loss: -0.3447195589542389
Batch 35/64 loss: -0.3177582025527954
Batch 36/64 loss: -0.3621835708618164
Batch 37/64 loss: -0.3352733850479126
Batch 38/64 loss: -0.33895623683929443
Batch 39/64 loss: -0.30541184544563293
Batch 40/64 loss: -0.3265032172203064
Batch 41/64 loss: -0.35117584466934204
Batch 42/64 loss: -0.36220455169677734
Batch 43/64 loss: -0.30464091897010803
Batch 44/64 loss: -0.3791048526763916
Batch 45/64 loss: -0.31599223613739014
Batch 46/64 loss: -0.285861611366272
Batch 47/64 loss: -0.3456982374191284
Batch 48/64 loss: -0.3405524492263794
Batch 49/64 loss: -0.3500237464904785
Batch 50/64 loss: -0.3450094759464264
Batch 51/64 loss: -0.3279274106025696
Batch 52/64 loss: -0.3549361228942871
Batch 53/64 loss: -0.3401349186897278
Batch 54/64 loss: -0.36317378282546997
Batch 55/64 loss: -0.3277801275253296
Batch 56/64 loss: -0.37539711594581604
Batch 57/64 loss: -0.30939799547195435
Batch 58/64 loss: -0.336636483669281
Batch 59/64 loss: -0.28427839279174805
Batch 60/64 loss: -0.3018747568130493
Batch 61/64 loss: -0.30626752972602844
Batch 62/64 loss: -0.32403841614723206
Batch 63/64 loss: -0.3170628547668457
Batch 64/64 loss: -0.2917695939540863
Epoch 364  Train loss: -0.3372494497719933  Val loss: 0.03447930653070666
Epoch 365
-------------------------------
Batch 1/64 loss: -0.34137850999832153
Batch 2/64 loss: -0.3477025330066681
Batch 3/64 loss: -0.32243719696998596
Batch 4/64 loss: -0.3037458062171936
Batch 5/64 loss: -0.3056918680667877
Batch 6/64 loss: -0.33906984329223633
Batch 7/64 loss: -0.3241518437862396
Batch 8/64 loss: -0.3329891860485077
Batch 9/64 loss: -0.3218013644218445
Batch 10/64 loss: -0.38017380237579346
Batch 11/64 loss: -0.35515034198760986
Batch 12/64 loss: -0.2963802218437195
Batch 13/64 loss: -0.3677649199962616
Batch 14/64 loss: -0.2934947609901428
Batch 15/64 loss: -0.345353364944458
Batch 16/64 loss: -0.3149647116661072
Batch 17/64 loss: -0.3629045784473419
Batch 18/64 loss: -0.35126304626464844
Batch 19/64 loss: -0.3324012756347656
Batch 20/64 loss: -0.3148946166038513
Batch 21/64 loss: -0.33221516013145447
Batch 22/64 loss: -0.35055840015411377
Batch 23/64 loss: -0.34685248136520386
Batch 24/64 loss: -0.36760973930358887
Batch 25/64 loss: -0.34953778982162476
Batch 26/64 loss: -0.3512611985206604
Batch 27/64 loss: -0.30956774950027466
Batch 28/64 loss: -0.31810131669044495
Batch 29/64 loss: -0.3670104146003723
Batch 30/64 loss: -0.3108609616756439
Batch 31/64 loss: -0.34713390469551086
Batch 32/64 loss: -0.31547167897224426
Batch 33/64 loss: -0.3555784225463867
Batch 34/64 loss: -0.35508453845977783
Batch 35/64 loss: -0.34260571002960205
Batch 36/64 loss: -0.3243896961212158
Batch 37/64 loss: -0.29687097668647766
Batch 38/64 loss: -0.34218645095825195
Batch 39/64 loss: -0.34414470195770264
Batch 40/64 loss: -0.34595200419425964
Batch 41/64 loss: -0.37410876154899597
Batch 42/64 loss: -0.3365146517753601
Batch 43/64 loss: -0.32074248790740967
Batch 44/64 loss: -0.35111257433891296
Batch 45/64 loss: -0.33666694164276123
Batch 46/64 loss: -0.35867393016815186
Batch 47/64 loss: -0.2576024532318115
Batch 48/64 loss: -0.2994532883167267
Batch 49/64 loss: -0.33587998151779175
Batch 50/64 loss: -0.34642189741134644
Batch 51/64 loss: -0.3594557046890259
Batch 52/64 loss: -0.3250768780708313
Batch 53/64 loss: -0.3318036198616028
Batch 54/64 loss: -0.34709981083869934
Batch 55/64 loss: -0.33021390438079834
Batch 56/64 loss: -0.3431558609008789
Batch 57/64 loss: -0.3219450116157532
Batch 58/64 loss: -0.34392881393432617
Batch 59/64 loss: -0.3437405824661255
Batch 60/64 loss: -0.36389297246932983
Batch 61/64 loss: -0.3435581922531128
Batch 62/64 loss: -0.35411718487739563
Batch 63/64 loss: -0.3250468373298645
Batch 64/64 loss: -0.31354498863220215
Epoch 365  Train loss: -0.3358757360308778  Val loss: 0.03388651109642999
Epoch 366
-------------------------------
Batch 1/64 loss: -0.35028600692749023
Batch 2/64 loss: -0.3343857526779175
Batch 3/64 loss: -0.3633572459220886
Batch 4/64 loss: -0.28969162702560425
Batch 5/64 loss: -0.3422248065471649
Batch 6/64 loss: -0.30665165185928345
Batch 7/64 loss: -0.3165964186191559
Batch 8/64 loss: -0.3276892304420471
Batch 9/64 loss: -0.33588430285453796
Batch 10/64 loss: -0.34444865584373474
Batch 11/64 loss: -0.33517974615097046
Batch 12/64 loss: -0.3743191659450531
Batch 13/64 loss: -0.31730473041534424
Batch 14/64 loss: -0.33823639154434204
Batch 15/64 loss: -0.3348189890384674
Batch 16/64 loss: -0.33247578144073486
Batch 17/64 loss: -0.34260717034339905
Batch 18/64 loss: -0.353877991437912
Batch 19/64 loss: -0.3299654722213745
Batch 20/64 loss: -0.3779700994491577
Batch 21/64 loss: -0.325514554977417
Batch 22/64 loss: -0.33025917410850525
Batch 23/64 loss: -0.32995909452438354
Batch 24/64 loss: -0.34034469723701477
Batch 25/64 loss: -0.34595853090286255
Batch 26/64 loss: -0.36830809712409973
Batch 27/64 loss: -0.34700119495391846
Batch 28/64 loss: -0.35146838426589966
Batch 29/64 loss: -0.3475553095340729
Batch 30/64 loss: -0.33762532472610474
Batch 31/64 loss: -0.3463226556777954
Batch 32/64 loss: -0.36319655179977417
Batch 33/64 loss: -0.28728264570236206
Batch 34/64 loss: -0.36570096015930176
Batch 35/64 loss: -0.3545387387275696
Batch 36/64 loss: -0.3323855400085449
Batch 37/64 loss: -0.34846025705337524
Batch 38/64 loss: -0.33528733253479004
Batch 39/64 loss: -0.34464144706726074
Batch 40/64 loss: -0.3314700722694397
Batch 41/64 loss: -0.31676220893859863
Batch 42/64 loss: -0.36670467257499695
Batch 43/64 loss: -0.3683178424835205
Batch 44/64 loss: -0.30408626794815063
Batch 45/64 loss: -0.3198035955429077
Batch 46/64 loss: -0.3416241407394409
Batch 47/64 loss: -0.3389858901500702
Batch 48/64 loss: -0.35570764541625977
Batch 49/64 loss: -0.3191646337509155
Batch 50/64 loss: -0.32890182733535767
Batch 51/64 loss: -0.3653073012828827
Batch 52/64 loss: -0.347679078578949
Batch 53/64 loss: -0.34294912219047546
Batch 54/64 loss: -0.3481523394584656
Batch 55/64 loss: -0.3342386484146118
Batch 56/64 loss: -0.31068894267082214
Batch 57/64 loss: -0.33002448081970215
Batch 58/64 loss: -0.3021147847175598
Batch 59/64 loss: -0.34131988883018494
Batch 60/64 loss: -0.29692649841308594
Batch 61/64 loss: -0.341102659702301
Batch 62/64 loss: -0.27799564599990845
Batch 63/64 loss: -0.3345069885253906
Batch 64/64 loss: -0.2844773530960083
Epoch 366  Train loss: -0.33612039089202883  Val loss: 0.033156613304033314
Epoch 367
-------------------------------
Batch 1/64 loss: -0.3637039363384247
Batch 2/64 loss: -0.3631201386451721
Batch 3/64 loss: -0.3421948552131653
Batch 4/64 loss: -0.3512360751628876
Batch 5/64 loss: -0.3167085647583008
Batch 6/64 loss: -0.3519018292427063
Batch 7/64 loss: -0.35977205634117126
Batch 8/64 loss: -0.3242231011390686
Batch 9/64 loss: -0.37296342849731445
Batch 10/64 loss: -0.32917556166648865
Batch 11/64 loss: -0.3753141760826111
Batch 12/64 loss: -0.3288719654083252
Batch 13/64 loss: -0.3691818118095398
Batch 14/64 loss: -0.343832790851593
Batch 15/64 loss: -0.3520832359790802
Batch 16/64 loss: -0.3382084369659424
Batch 17/64 loss: -0.3624824285507202
Batch 18/64 loss: -0.31644415855407715
Batch 19/64 loss: -0.35152217745780945
Batch 20/64 loss: -0.36098402738571167
Batch 21/64 loss: -0.3213117718696594
Batch 22/64 loss: -0.31947219371795654
Batch 23/64 loss: -0.3309636116027832
Batch 24/64 loss: -0.3635033369064331
Batch 25/64 loss: -0.3209272623062134
Batch 26/64 loss: -0.3338421881198883
Batch 27/64 loss: -0.3133096992969513
Batch 28/64 loss: -0.344753623008728
Batch 29/64 loss: -0.29023173451423645
Batch 30/64 loss: -0.33970406651496887
Batch 31/64 loss: -0.34745675325393677
Batch 32/64 loss: -0.33550357818603516
Batch 33/64 loss: -0.3713602125644684
Batch 34/64 loss: -0.3523937463760376
Batch 35/64 loss: -0.3562488555908203
Batch 36/64 loss: -0.30318164825439453
Batch 37/64 loss: -0.36691543459892273
Batch 38/64 loss: -0.29136401414871216
Batch 39/64 loss: -0.3475448787212372
Batch 40/64 loss: -0.37910735607147217
Batch 41/64 loss: -0.37290480732917786
Batch 42/64 loss: -0.34250617027282715
Batch 43/64 loss: -0.331887423992157
Batch 44/64 loss: -0.3529796600341797
Batch 45/64 loss: -0.3194194436073303
Batch 46/64 loss: -0.29940900206565857
Batch 47/64 loss: -0.3606206774711609
Batch 48/64 loss: -0.35806071758270264
Batch 49/64 loss: -0.34383130073547363
Batch 50/64 loss: -0.2926648259162903
Batch 51/64 loss: -0.33355480432510376
Batch 52/64 loss: -0.31725984811782837
Batch 53/64 loss: -0.3654860258102417
Batch 54/64 loss: -0.33934804797172546
Batch 55/64 loss: -0.31548771262168884
Batch 56/64 loss: -0.35553497076034546
Batch 57/64 loss: -0.3288642168045044
Batch 58/64 loss: -0.35199812054634094
Batch 59/64 loss: -0.3468383550643921
Batch 60/64 loss: -0.318013995885849
Batch 61/64 loss: -0.33883726596832275
Batch 62/64 loss: -0.3109220862388611
Batch 63/64 loss: -0.2978574335575104
Batch 64/64 loss: -0.3583970069885254
Epoch 367  Train loss: -0.3398604766995299  Val loss: 0.03327911703037642
Epoch 368
-------------------------------
Batch 1/64 loss: -0.3342367708683014
Batch 2/64 loss: -0.3544916808605194
Batch 3/64 loss: -0.3495906591415405
Batch 4/64 loss: -0.32155776023864746
Batch 5/64 loss: -0.3601146340370178
Batch 6/64 loss: -0.3443300127983093
Batch 7/64 loss: -0.36405372619628906
Batch 8/64 loss: -0.3523727357387543
Batch 9/64 loss: -0.345095157623291
Batch 10/64 loss: -0.370169997215271
Batch 11/64 loss: -0.32763826847076416
Batch 12/64 loss: -0.3226518929004669
Batch 13/64 loss: -0.3269158601760864
Batch 14/64 loss: -0.3397406339645386
Batch 15/64 loss: -0.3167004883289337
Batch 16/64 loss: -0.36586007475852966
Batch 17/64 loss: -0.3396818935871124
Batch 18/64 loss: -0.3227325677871704
Batch 19/64 loss: -0.3360501825809479
Batch 20/64 loss: -0.3099331557750702
Batch 21/64 loss: -0.3521336615085602
Batch 22/64 loss: -0.32350221276283264
Batch 23/64 loss: -0.3430468440055847
Batch 24/64 loss: -0.29862162470817566
Batch 25/64 loss: -0.33241263031959534
Batch 26/64 loss: -0.3635573387145996
Batch 27/64 loss: -0.30823495984077454
Batch 28/64 loss: -0.3655407428741455
Batch 29/64 loss: -0.3353988528251648
Batch 30/64 loss: -0.33655762672424316
Batch 31/64 loss: -0.36610859632492065
Batch 32/64 loss: -0.32729899883270264
Batch 33/64 loss: -0.33880722522735596
Batch 34/64 loss: -0.3224896788597107
Batch 35/64 loss: -0.3607051372528076
Batch 36/64 loss: -0.3018958568572998
Batch 37/64 loss: -0.3542032241821289
Batch 38/64 loss: -0.3835042715072632
Batch 39/64 loss: -0.32042235136032104
Batch 40/64 loss: -0.35170578956604004
Batch 41/64 loss: -0.3247670829296112
Batch 42/64 loss: -0.3054748773574829
Batch 43/64 loss: -0.31530240178108215
Batch 44/64 loss: -0.3245728015899658
Batch 45/64 loss: -0.3340846300125122
Batch 46/64 loss: -0.28930193185806274
Batch 47/64 loss: -0.3137601912021637
Batch 48/64 loss: -0.3582935631275177
Batch 49/64 loss: -0.3626783490180969
Batch 50/64 loss: -0.34111636877059937
Batch 51/64 loss: -0.32815423607826233
Batch 52/64 loss: -0.32943403720855713
Batch 53/64 loss: -0.294286847114563
Batch 54/64 loss: -0.3499079942703247
Batch 55/64 loss: -0.36117494106292725
Batch 56/64 loss: -0.37451398372650146
Batch 57/64 loss: -0.33657944202423096
Batch 58/64 loss: -0.293718159198761
Batch 59/64 loss: -0.3393242061138153
Batch 60/64 loss: -0.3571983575820923
Batch 61/64 loss: -0.34934210777282715
Batch 62/64 loss: -0.33280736207962036
Batch 63/64 loss: -0.33824706077575684
Batch 64/64 loss: -0.3073892891407013
Epoch 368  Train loss: -0.33685723414608076  Val loss: 0.032026919917142675
Epoch 369
-------------------------------
Batch 1/64 loss: -0.35199058055877686
Batch 2/64 loss: -0.34587833285331726
Batch 3/64 loss: -0.3571287989616394
Batch 4/64 loss: -0.353973388671875
Batch 5/64 loss: -0.3594760596752167
Batch 6/64 loss: -0.34541648626327515
Batch 7/64 loss: -0.3219373822212219
Batch 8/64 loss: -0.35029423236846924
Batch 9/64 loss: -0.339136004447937
Batch 10/64 loss: -0.3296237587928772
Batch 11/64 loss: -0.36079177260398865
Batch 12/64 loss: -0.3283618688583374
Batch 13/64 loss: -0.3426710367202759
Batch 14/64 loss: -0.3035199046134949
Batch 15/64 loss: -0.3419448733329773
Batch 16/64 loss: -0.3358617126941681
Batch 17/64 loss: -0.3369024097919464
Batch 18/64 loss: -0.3366624712944031
Batch 19/64 loss: -0.3295280933380127
Batch 20/64 loss: -0.3523644804954529
Batch 21/64 loss: -0.3327139914035797
Batch 22/64 loss: -0.31927454471588135
Batch 23/64 loss: -0.34123891592025757
Batch 24/64 loss: -0.3487972915172577
Batch 25/64 loss: -0.3545840382575989
Batch 26/64 loss: -0.3646068572998047
Batch 27/64 loss: -0.3472159802913666
Batch 28/64 loss: -0.31274357438087463
Batch 29/64 loss: -0.3311220407485962
Batch 30/64 loss: -0.3204169273376465
Batch 31/64 loss: -0.3333558738231659
Batch 32/64 loss: -0.3539053201675415
Batch 33/64 loss: -0.3364538550376892
Batch 34/64 loss: -0.3501550555229187
Batch 35/64 loss: -0.3338361084461212
Batch 36/64 loss: -0.3346877992153168
Batch 37/64 loss: -0.3394576609134674
Batch 38/64 loss: -0.33258700370788574
Batch 39/64 loss: -0.35717523097991943
Batch 40/64 loss: -0.36518359184265137
Batch 41/64 loss: -0.3295498490333557
Batch 42/64 loss: -0.3216582238674164
Batch 43/64 loss: -0.28749358654022217
Batch 44/64 loss: -0.36520063877105713
Batch 45/64 loss: -0.37117063999176025
Batch 46/64 loss: -0.35943901538848877
Batch 47/64 loss: -0.3292080760002136
Batch 48/64 loss: -0.3134024739265442
Batch 49/64 loss: -0.35752737522125244
Batch 50/64 loss: -0.3525853455066681
Batch 51/64 loss: -0.37467634677886963
Batch 52/64 loss: -0.3344244360923767
Batch 53/64 loss: -0.31720250844955444
Batch 54/64 loss: -0.3615587055683136
Batch 55/64 loss: -0.26111188530921936
Batch 56/64 loss: -0.32346829771995544
Batch 57/64 loss: -0.307681679725647
Batch 58/64 loss: -0.3678751587867737
Batch 59/64 loss: -0.3258923292160034
Batch 60/64 loss: -0.32165759801864624
Batch 61/64 loss: -0.34304672479629517
Batch 62/64 loss: -0.3651396930217743
Batch 63/64 loss: -0.311212956905365
Batch 64/64 loss: -0.32542890310287476
Epoch 369  Train loss: -0.3384977338360805  Val loss: 0.03413148475266814
Epoch 370
-------------------------------
Batch 1/64 loss: -0.35058510303497314
Batch 2/64 loss: -0.3581031560897827
Batch 3/64 loss: -0.347460001707077
Batch 4/64 loss: -0.3483338952064514
Batch 5/64 loss: -0.3467000126838684
Batch 6/64 loss: -0.3355114161968231
Batch 7/64 loss: -0.35431867837905884
Batch 8/64 loss: -0.2927567660808563
Batch 9/64 loss: -0.3120781481266022
Batch 10/64 loss: -0.3626057207584381
Batch 11/64 loss: -0.3522526025772095
Batch 12/64 loss: -0.3076314330101013
Batch 13/64 loss: -0.32484444975852966
Batch 14/64 loss: -0.3943711519241333
Batch 15/64 loss: -0.38128411769866943
Batch 16/64 loss: -0.34221407771110535
Batch 17/64 loss: -0.3335227966308594
Batch 18/64 loss: -0.3438161611557007
Batch 19/64 loss: -0.333626925945282
Batch 20/64 loss: -0.35176920890808105
Batch 21/64 loss: -0.2870488166809082
Batch 22/64 loss: -0.3631840646266937
Batch 23/64 loss: -0.3305584192276001
Batch 24/64 loss: -0.366155743598938
Batch 25/64 loss: -0.33919215202331543
Batch 26/64 loss: -0.37596333026885986
Batch 27/64 loss: -0.3590359389781952
Batch 28/64 loss: -0.3543858528137207
Batch 29/64 loss: -0.3473014235496521
Batch 30/64 loss: -0.3560176193714142
Batch 31/64 loss: -0.3612216114997864
Batch 32/64 loss: -0.37643855810165405
Batch 33/64 loss: -0.31136077642440796
Batch 34/64 loss: -0.34773552417755127
Batch 35/64 loss: -0.2646196186542511
Batch 36/64 loss: -0.3613983690738678
Batch 37/64 loss: -0.35248899459838867
Batch 38/64 loss: -0.3316171169281006
Batch 39/64 loss: -0.35151588916778564
Batch 40/64 loss: -0.36000025272369385
Batch 41/64 loss: -0.3660685420036316
Batch 42/64 loss: -0.3543035686016083
Batch 43/64 loss: -0.33302614092826843
Batch 44/64 loss: -0.3207244277000427
Batch 45/64 loss: -0.3456876873970032
Batch 46/64 loss: -0.3556075692176819
Batch 47/64 loss: -0.3182954490184784
Batch 48/64 loss: -0.33915600180625916
Batch 49/64 loss: -0.3433581590652466
Batch 50/64 loss: -0.36294621229171753
Batch 51/64 loss: -0.3345704972743988
Batch 52/64 loss: -0.3512740731239319
Batch 53/64 loss: -0.3150121569633484
Batch 54/64 loss: -0.33977025747299194
Batch 55/64 loss: -0.356680303812027
Batch 56/64 loss: -0.31053200364112854
Batch 57/64 loss: -0.3527470827102661
Batch 58/64 loss: -0.2779296636581421
Batch 59/64 loss: -0.33251428604125977
Batch 60/64 loss: -0.34008514881134033
Batch 61/64 loss: -0.35116350650787354
Batch 62/64 loss: -0.3550722002983093
Batch 63/64 loss: -0.3618181645870209
Batch 64/64 loss: -0.3308161497116089
Epoch 370  Train loss: -0.3425487233143227  Val loss: 0.032197858869414966
Epoch 371
-------------------------------
Batch 1/64 loss: -0.3493589162826538
Batch 2/64 loss: -0.3333294987678528
Batch 3/64 loss: -0.33769845962524414
Batch 4/64 loss: -0.29722118377685547
Batch 5/64 loss: -0.3291367292404175
Batch 6/64 loss: -0.32423466444015503
Batch 7/64 loss: -0.3531268537044525
Batch 8/64 loss: -0.2954740524291992
Batch 9/64 loss: -0.3195447623729706
Batch 10/64 loss: -0.36344295740127563
Batch 11/64 loss: -0.357499361038208
Batch 12/64 loss: -0.37571513652801514
Batch 13/64 loss: -0.35470274090766907
Batch 14/64 loss: -0.3186764419078827
Batch 15/64 loss: -0.34668219089508057
Batch 16/64 loss: -0.3483569622039795
Batch 17/64 loss: -0.3366732597351074
Batch 18/64 loss: -0.3518560826778412
Batch 19/64 loss: -0.3383815884590149
Batch 20/64 loss: -0.3550434708595276
Batch 21/64 loss: -0.3449965715408325
Batch 22/64 loss: -0.3464963436126709
Batch 23/64 loss: -0.3296162486076355
Batch 24/64 loss: -0.35406529903411865
Batch 25/64 loss: -0.34301745891571045
Batch 26/64 loss: -0.3593093156814575
Batch 27/64 loss: -0.35027194023132324
Batch 28/64 loss: -0.36845123767852783
Batch 29/64 loss: -0.378131240606308
Batch 30/64 loss: -0.32966625690460205
Batch 31/64 loss: -0.3020797669887543
Batch 32/64 loss: -0.331348180770874
Batch 33/64 loss: -0.35610124468803406
Batch 34/64 loss: -0.3618377149105072
Batch 35/64 loss: -0.2979286313056946
Batch 36/64 loss: -0.27594971656799316
Batch 37/64 loss: -0.34797853231430054
Batch 38/64 loss: -0.36633920669555664
Batch 39/64 loss: -0.35020777583122253
Batch 40/64 loss: -0.3572758734226227
Batch 41/64 loss: -0.35510891675949097
Batch 42/64 loss: -0.3441564440727234
Batch 43/64 loss: -0.35427379608154297
Batch 44/64 loss: -0.3079007565975189
Batch 45/64 loss: -0.33201509714126587
Batch 46/64 loss: -0.3536814749240875
Batch 47/64 loss: -0.36063066124916077
Batch 48/64 loss: -0.35629698634147644
Batch 49/64 loss: -0.3680596351623535
Batch 50/64 loss: -0.3756140470504761
Batch 51/64 loss: -0.32049500942230225
Batch 52/64 loss: -0.335466206073761
Batch 53/64 loss: -0.32150986790657043
Batch 54/64 loss: -0.3449762761592865
Batch 55/64 loss: -0.32443249225616455
Batch 56/64 loss: -0.32658126950263977
Batch 57/64 loss: -0.33571696281433105
Batch 58/64 loss: -0.3067326247692108
Batch 59/64 loss: -0.32351842522621155
Batch 60/64 loss: -0.36870527267456055
Batch 61/64 loss: -0.36200037598609924
Batch 62/64 loss: -0.37007904052734375
Batch 63/64 loss: -0.34333914518356323
Batch 64/64 loss: -0.2779386639595032
Epoch 371  Train loss: -0.3409720572770811  Val loss: 0.034012227533609184
Epoch 372
-------------------------------
Batch 1/64 loss: -0.31997236609458923
Batch 2/64 loss: -0.3932725787162781
Batch 3/64 loss: -0.35550302267074585
Batch 4/64 loss: -0.3701166808605194
Batch 5/64 loss: -0.309261292219162
Batch 6/64 loss: -0.3300197124481201
Batch 7/64 loss: -0.33766794204711914
Batch 8/64 loss: -0.3155207633972168
Batch 9/64 loss: -0.2940095067024231
Batch 10/64 loss: -0.37327563762664795
Batch 11/64 loss: -0.2963513433933258
Batch 12/64 loss: -0.33285313844680786
Batch 13/64 loss: -0.3146365284919739
Batch 14/64 loss: -0.35205078125
Batch 15/64 loss: -0.36066538095474243
Batch 16/64 loss: -0.3519206643104553
Batch 17/64 loss: -0.3012675344944
Batch 18/64 loss: -0.3648247718811035
Batch 19/64 loss: -0.3441668748855591
Batch 20/64 loss: -0.3620961606502533
Batch 21/64 loss: -0.3594273626804352
Batch 22/64 loss: -0.3659414052963257
Batch 23/64 loss: -0.33616727590560913
Batch 24/64 loss: -0.32691770792007446
Batch 25/64 loss: -0.3434765338897705
Batch 26/64 loss: -0.3186911642551422
Batch 27/64 loss: -0.3350769877433777
Batch 28/64 loss: -0.38051241636276245
Batch 29/64 loss: -0.3437989354133606
Batch 30/64 loss: -0.36597177386283875
Batch 31/64 loss: -0.33698683977127075
Batch 32/64 loss: -0.3398573696613312
Batch 33/64 loss: -0.3507033586502075
Batch 34/64 loss: -0.34043169021606445
Batch 35/64 loss: -0.3147192597389221
Batch 36/64 loss: -0.33257997035980225
Batch 37/64 loss: -0.36574846506118774
Batch 38/64 loss: -0.3421749174594879
Batch 39/64 loss: -0.35537758469581604
Batch 40/64 loss: -0.36255574226379395
Batch 41/64 loss: -0.2960856556892395
Batch 42/64 loss: -0.28889521956443787
Batch 43/64 loss: -0.3208673894405365
Batch 44/64 loss: -0.36014068126678467
Batch 45/64 loss: -0.3203463852405548
Batch 46/64 loss: -0.35076457262039185
Batch 47/64 loss: -0.3025873005390167
Batch 48/64 loss: -0.3444209694862366
Batch 49/64 loss: -0.29168349504470825
Batch 50/64 loss: -0.3371230363845825
Batch 51/64 loss: -0.3016733229160309
Batch 52/64 loss: -0.3269956409931183
Batch 53/64 loss: -0.3321971893310547
Batch 54/64 loss: -0.347079336643219
Batch 55/64 loss: -0.3484126627445221
Batch 56/64 loss: -0.3299652338027954
Batch 57/64 loss: -0.3509231507778168
Batch 58/64 loss: -0.3339785933494568
Batch 59/64 loss: -0.3392426669597626
Batch 60/64 loss: -0.34461116790771484
Batch 61/64 loss: -0.3358404040336609
Batch 62/64 loss: -0.2877468764781952
Batch 63/64 loss: -0.3517245650291443
Batch 64/64 loss: -0.2863221764564514
Epoch 372  Train loss: -0.3365430053542642  Val loss: 0.03305972646601831
Epoch 373
-------------------------------
Batch 1/64 loss: -0.365484356880188
Batch 2/64 loss: -0.2891837954521179
Batch 3/64 loss: -0.3245462477207184
Batch 4/64 loss: -0.3354090452194214
Batch 5/64 loss: -0.3390790522098541
Batch 6/64 loss: -0.34036019444465637
Batch 7/64 loss: -0.3333591818809509
Batch 8/64 loss: -0.3644382357597351
Batch 9/64 loss: -0.3665916621685028
Batch 10/64 loss: -0.3252694010734558
Batch 11/64 loss: -0.36099398136138916
Batch 12/64 loss: -0.36101454496383667
Batch 13/64 loss: -0.2939475178718567
Batch 14/64 loss: -0.3504163324832916
Batch 15/64 loss: -0.3114895224571228
Batch 16/64 loss: -0.35313674807548523
Batch 17/64 loss: -0.31856226921081543
Batch 18/64 loss: -0.344441294670105
Batch 19/64 loss: -0.32563307881355286
Batch 20/64 loss: -0.3511689305305481
Batch 21/64 loss: -0.36480608582496643
Batch 22/64 loss: -0.3638935089111328
Batch 23/64 loss: -0.3625800609588623
Batch 24/64 loss: -0.3507310748100281
Batch 25/64 loss: -0.33233433961868286
Batch 26/64 loss: -0.35660192370414734
Batch 27/64 loss: -0.3370036482810974
Batch 28/64 loss: -0.3280290961265564
Batch 29/64 loss: -0.3243849277496338
Batch 30/64 loss: -0.3637092709541321
Batch 31/64 loss: -0.3512513041496277
Batch 32/64 loss: -0.3494470417499542
Batch 33/64 loss: -0.33606135845184326
Batch 34/64 loss: -0.3291630744934082
Batch 35/64 loss: -0.35849785804748535
Batch 36/64 loss: -0.32220137119293213
Batch 37/64 loss: -0.35059964656829834
Batch 38/64 loss: -0.36419641971588135
Batch 39/64 loss: -0.37351304292678833
Batch 40/64 loss: -0.35766664147377014
Batch 41/64 loss: -0.3129956126213074
Batch 42/64 loss: -0.34932973980903625
Batch 43/64 loss: -0.36980104446411133
Batch 44/64 loss: -0.3253219723701477
Batch 45/64 loss: -0.3599470257759094
Batch 46/64 loss: -0.33899635076522827
Batch 47/64 loss: -0.33593595027923584
Batch 48/64 loss: -0.3364035487174988
Batch 49/64 loss: -0.35190367698669434
Batch 50/64 loss: -0.3587946593761444
Batch 51/64 loss: -0.3339015245437622
Batch 52/64 loss: -0.32347428798675537
Batch 53/64 loss: -0.3545632064342499
Batch 54/64 loss: -0.31603705883026123
Batch 55/64 loss: -0.37941595911979675
Batch 56/64 loss: -0.3328530192375183
Batch 57/64 loss: -0.34631264209747314
Batch 58/64 loss: -0.35006994009017944
Batch 59/64 loss: -0.3466216027736664
Batch 60/64 loss: -0.33373236656188965
Batch 61/64 loss: -0.31883835792541504
Batch 62/64 loss: -0.3429969549179077
Batch 63/64 loss: -0.3496765196323395
Batch 64/64 loss: -0.3185122311115265
Epoch 373  Train loss: -0.34255692993893344  Val loss: 0.03326444281745203
Epoch 374
-------------------------------
Batch 1/64 loss: -0.3347944915294647
Batch 2/64 loss: -0.3368728756904602
Batch 3/64 loss: -0.35497868061065674
Batch 4/64 loss: -0.3617916405200958
Batch 5/64 loss: -0.28466466069221497
Batch 6/64 loss: -0.3276987075805664
Batch 7/64 loss: -0.3406042158603668
Batch 8/64 loss: -0.31959909200668335
Batch 9/64 loss: -0.33468347787857056
Batch 10/64 loss: -0.2636870741844177
Batch 11/64 loss: -0.3253506124019623
Batch 12/64 loss: -0.3603612780570984
Batch 13/64 loss: -0.35804545879364014
Batch 14/64 loss: -0.36469072103500366
Batch 15/64 loss: -0.3352387845516205
Batch 16/64 loss: -0.32727736234664917
Batch 17/64 loss: -0.31168729066848755
Batch 18/64 loss: -0.33048611879348755
Batch 19/64 loss: -0.3627170920372009
Batch 20/64 loss: -0.3302496671676636
Batch 21/64 loss: -0.2924739718437195
Batch 22/64 loss: -0.35120442509651184
Batch 23/64 loss: -0.36314916610717773
Batch 24/64 loss: -0.3485746681690216
Batch 25/64 loss: -0.36108458042144775
Batch 26/64 loss: -0.3440939486026764
Batch 27/64 loss: -0.3398962616920471
Batch 28/64 loss: -0.32404711842536926
Batch 29/64 loss: -0.3284526467323303
Batch 30/64 loss: -0.3304843008518219
Batch 31/64 loss: -0.3471231460571289
Batch 32/64 loss: -0.33918577432632446
Batch 33/64 loss: -0.35270410776138306
Batch 34/64 loss: -0.37927472591400146
Batch 35/64 loss: -0.3372952938079834
Batch 36/64 loss: -0.37323111295700073
Batch 37/64 loss: -0.3659732937812805
Batch 38/64 loss: -0.3575502634048462
Batch 39/64 loss: -0.3525795340538025
Batch 40/64 loss: -0.34342557191848755
Batch 41/64 loss: -0.3232046067714691
Batch 42/64 loss: -0.3574534058570862
Batch 43/64 loss: -0.37089967727661133
Batch 44/64 loss: -0.29016047716140747
Batch 45/64 loss: -0.34981364011764526
Batch 46/64 loss: -0.3752555251121521
Batch 47/64 loss: -0.3045608699321747
Batch 48/64 loss: -0.32586950063705444
Batch 49/64 loss: -0.3167629539966583
Batch 50/64 loss: -0.3742743134498596
Batch 51/64 loss: -0.32984960079193115
Batch 52/64 loss: -0.36195486783981323
Batch 53/64 loss: -0.3598436713218689
Batch 54/64 loss: -0.35590028762817383
Batch 55/64 loss: -0.37260347604751587
Batch 56/64 loss: -0.32688549160957336
Batch 57/64 loss: -0.35014665126800537
Batch 58/64 loss: -0.3279399871826172
Batch 59/64 loss: -0.35483574867248535
Batch 60/64 loss: -0.3407035768032074
Batch 61/64 loss: -0.2816222012042999
Batch 62/64 loss: -0.3172195553779602
Batch 63/64 loss: -0.3295929431915283
Batch 64/64 loss: -0.30904674530029297
Epoch 374  Train loss: -0.3392379812165803  Val loss: 0.033338357492820506
Epoch 375
-------------------------------
Batch 1/64 loss: -0.33069276809692383
Batch 2/64 loss: -0.32206782698631287
Batch 3/64 loss: -0.32891106605529785
Batch 4/64 loss: -0.36311522126197815
Batch 5/64 loss: -0.34096670150756836
Batch 6/64 loss: -0.3459855914115906
Batch 7/64 loss: -0.31035125255584717
Batch 8/64 loss: -0.36435818672180176
Batch 9/64 loss: -0.3448087275028229
Batch 10/64 loss: -0.31808751821517944
Batch 11/64 loss: -0.3148270845413208
Batch 12/64 loss: -0.31869053840637207
Batch 13/64 loss: -0.3192296028137207
Batch 14/64 loss: -0.3402690291404724
Batch 15/64 loss: -0.36501407623291016
Batch 16/64 loss: -0.353173166513443
Batch 17/64 loss: -0.361558198928833
Batch 18/64 loss: -0.33399707078933716
Batch 19/64 loss: -0.36173662543296814
Batch 20/64 loss: -0.28911951184272766
Batch 21/64 loss: -0.3211045265197754
Batch 22/64 loss: -0.3605153560638428
Batch 23/64 loss: -0.35115495324134827
Batch 24/64 loss: -0.35592716932296753
Batch 25/64 loss: -0.37089255452156067
Batch 26/64 loss: -0.35618627071380615
Batch 27/64 loss: -0.31695377826690674
Batch 28/64 loss: -0.29616057872772217
Batch 29/64 loss: -0.3238983154296875
Batch 30/64 loss: -0.34593015909194946
Batch 31/64 loss: -0.37006187438964844
Batch 32/64 loss: -0.368604838848114
Batch 33/64 loss: -0.34281784296035767
Batch 34/64 loss: -0.34690409898757935
Batch 35/64 loss: -0.36169275641441345
Batch 36/64 loss: -0.36731481552124023
Batch 37/64 loss: -0.35321712493896484
Batch 38/64 loss: -0.3455171585083008
Batch 39/64 loss: -0.31941771507263184
Batch 40/64 loss: -0.3130468428134918
Batch 41/64 loss: -0.3163502812385559
Batch 42/64 loss: -0.33440524339675903
Batch 43/64 loss: -0.33469927310943604
Batch 44/64 loss: -0.34371209144592285
Batch 45/64 loss: -0.3571631610393524
Batch 46/64 loss: -0.34513646364212036
Batch 47/64 loss: -0.3679521679878235
Batch 48/64 loss: -0.34552228450775146
Batch 49/64 loss: -0.3787159323692322
Batch 50/64 loss: -0.3608608841896057
Batch 51/64 loss: -0.3474366068840027
Batch 52/64 loss: -0.32545411586761475
Batch 53/64 loss: -0.34058326482772827
Batch 54/64 loss: -0.3287777900695801
Batch 55/64 loss: -0.3528445363044739
Batch 56/64 loss: -0.3369008004665375
Batch 57/64 loss: -0.3617156744003296
Batch 58/64 loss: -0.31399214267730713
Batch 59/64 loss: -0.35618704557418823
Batch 60/64 loss: -0.3571538031101227
Batch 61/64 loss: -0.3168349266052246
Batch 62/64 loss: -0.3558858036994934
Batch 63/64 loss: -0.32621991634368896
Batch 64/64 loss: -0.3339375853538513
Epoch 375  Train loss: -0.34147821010327806  Val loss: 0.03298355889893889
Epoch 376
-------------------------------
Batch 1/64 loss: -0.3201642632484436
Batch 2/64 loss: -0.36820071935653687
Batch 3/64 loss: -0.307966411113739
Batch 4/64 loss: -0.37029507756233215
Batch 5/64 loss: -0.36024704575538635
Batch 6/64 loss: -0.32134559750556946
Batch 7/64 loss: -0.34497860074043274
Batch 8/64 loss: -0.3154149055480957
Batch 9/64 loss: -0.36414045095443726
Batch 10/64 loss: -0.36787378787994385
Batch 11/64 loss: -0.3753513693809509
Batch 12/64 loss: -0.3217378854751587
Batch 13/64 loss: -0.359219491481781
Batch 14/64 loss: -0.34632617235183716
Batch 15/64 loss: -0.35659337043762207
Batch 16/64 loss: -0.335716187953949
Batch 17/64 loss: -0.34054499864578247
Batch 18/64 loss: -0.34581172466278076
Batch 19/64 loss: -0.3114032447338104
Batch 20/64 loss: -0.36457353830337524
Batch 21/64 loss: -0.3359057903289795
Batch 22/64 loss: -0.31807881593704224
Batch 23/64 loss: -0.31740373373031616
Batch 24/64 loss: -0.3631833493709564
Batch 25/64 loss: -0.3525177836418152
Batch 26/64 loss: -0.30485770106315613
Batch 27/64 loss: -0.3031783103942871
Batch 28/64 loss: -0.3565295338630676
Batch 29/64 loss: -0.34235453605651855
Batch 30/64 loss: -0.3473320007324219
Batch 31/64 loss: -0.3029962480068207
Batch 32/64 loss: -0.32644879817962646
Batch 33/64 loss: -0.3368793725967407
Batch 34/64 loss: -0.3415706753730774
Batch 35/64 loss: -0.33295106887817383
Batch 36/64 loss: -0.3250369429588318
Batch 37/64 loss: -0.35722827911376953
Batch 38/64 loss: -0.28818684816360474
Batch 39/64 loss: -0.3470430076122284
Batch 40/64 loss: -0.3715202212333679
Batch 41/64 loss: -0.3476063013076782
Batch 42/64 loss: -0.3052489757537842
Batch 43/64 loss: -0.34784814715385437
Batch 44/64 loss: -0.366177499294281
Batch 45/64 loss: -0.3512210249900818
Batch 46/64 loss: -0.3361610174179077
Batch 47/64 loss: -0.28160223364830017
Batch 48/64 loss: -0.3170030117034912
Batch 49/64 loss: -0.3559327721595764
Batch 50/64 loss: -0.2999066114425659
Batch 51/64 loss: -0.35902139544487
Batch 52/64 loss: -0.3124569058418274
Batch 53/64 loss: -0.3639565706253052
Batch 54/64 loss: -0.31560152769088745
Batch 55/64 loss: -0.35730037093162537
Batch 56/64 loss: -0.34899747371673584
Batch 57/64 loss: -0.36093419790267944
Batch 58/64 loss: -0.3736235499382019
Batch 59/64 loss: -0.34864699840545654
Batch 60/64 loss: -0.3584713339805603
Batch 61/64 loss: -0.37494513392448425
Batch 62/64 loss: -0.33523303270339966
Batch 63/64 loss: -0.36289259791374207
Batch 64/64 loss: -0.25648313760757446
Epoch 376  Train loss: -0.3394864140772352  Val loss: 0.03221328684554477
Epoch 377
-------------------------------
Batch 1/64 loss: -0.2959643006324768
Batch 2/64 loss: -0.3591425120830536
Batch 3/64 loss: -0.33076396584510803
Batch 4/64 loss: -0.33796337246894836
Batch 5/64 loss: -0.3419470489025116
Batch 6/64 loss: -0.3456122875213623
Batch 7/64 loss: -0.3318535089492798
Batch 8/64 loss: -0.342859148979187
Batch 9/64 loss: -0.36699390411376953
Batch 10/64 loss: -0.3467809557914734
Batch 11/64 loss: -0.3020554780960083
Batch 12/64 loss: -0.3669051229953766
Batch 13/64 loss: -0.30558788776397705
Batch 14/64 loss: -0.3535917401313782
Batch 15/64 loss: -0.3637683689594269
Batch 16/64 loss: -0.30135664343833923
Batch 17/64 loss: -0.31847208738327026
Batch 18/64 loss: -0.3065038025379181
Batch 19/64 loss: -0.34107762575149536
Batch 20/64 loss: -0.3626183569431305
Batch 21/64 loss: -0.3291561007499695
Batch 22/64 loss: -0.3936588764190674
Batch 23/64 loss: -0.34267759323120117
Batch 24/64 loss: -0.34126853942871094
Batch 25/64 loss: -0.3061404824256897
Batch 26/64 loss: -0.2919465899467468
Batch 27/64 loss: -0.3410148024559021
Batch 28/64 loss: -0.3508763313293457
Batch 29/64 loss: -0.3531492352485657
Batch 30/64 loss: -0.35372862219810486
Batch 31/64 loss: -0.3595755696296692
Batch 32/64 loss: -0.32200491428375244
Batch 33/64 loss: -0.3134676218032837
Batch 34/64 loss: -0.35554516315460205
Batch 35/64 loss: -0.3282668888568878
Batch 36/64 loss: -0.3375059962272644
Batch 37/64 loss: -0.36405491828918457
Batch 38/64 loss: -0.3465730547904968
Batch 39/64 loss: -0.3489602208137512
Batch 40/64 loss: -0.36083537340164185
Batch 41/64 loss: -0.35616135597229004
Batch 42/64 loss: -0.33489567041397095
Batch 43/64 loss: -0.3573252558708191
Batch 44/64 loss: -0.2803501486778259
Batch 45/64 loss: -0.34671562910079956
Batch 46/64 loss: -0.3491879105567932
Batch 47/64 loss: -0.3549230694770813
Batch 48/64 loss: -0.3386135399341583
Batch 49/64 loss: -0.3157367408275604
Batch 50/64 loss: -0.32023584842681885
Batch 51/64 loss: -0.3111593723297119
Batch 52/64 loss: -0.37289053201675415
Batch 53/64 loss: -0.36802756786346436
Batch 54/64 loss: -0.35424429178237915
Batch 55/64 loss: -0.29707834124565125
Batch 56/64 loss: -0.3458259105682373
Batch 57/64 loss: -0.3698907196521759
Batch 58/64 loss: -0.3175228238105774
Batch 59/64 loss: -0.3374107778072357
Batch 60/64 loss: -0.33506372570991516
Batch 61/64 loss: -0.30822762846946716
Batch 62/64 loss: -0.37032732367515564
Batch 63/64 loss: -0.30260753631591797
Batch 64/64 loss: -0.32378089427948
Epoch 377  Train loss: -0.33803109655193253  Val loss: 0.03157267169034768
Epoch 378
-------------------------------
Batch 1/64 loss: -0.37164297699928284
Batch 2/64 loss: -0.294364869594574
Batch 3/64 loss: -0.3560248017311096
Batch 4/64 loss: -0.3512008786201477
Batch 5/64 loss: -0.36766892671585083
Batch 6/64 loss: -0.33418673276901245
Batch 7/64 loss: -0.3419904112815857
Batch 8/64 loss: -0.3391294777393341
Batch 9/64 loss: -0.30318522453308105
Batch 10/64 loss: -0.3377845287322998
Batch 11/64 loss: -0.34815555810928345
Batch 12/64 loss: -0.35093796253204346
Batch 13/64 loss: -0.3265897035598755
Batch 14/64 loss: -0.3509380519390106
Batch 15/64 loss: -0.342174232006073
Batch 16/64 loss: -0.339184045791626
Batch 17/64 loss: -0.3528231382369995
Batch 18/64 loss: -0.33423370122909546
Batch 19/64 loss: -0.34056079387664795
Batch 20/64 loss: -0.33134669065475464
Batch 21/64 loss: -0.3617006540298462
Batch 22/64 loss: -0.3240124583244324
Batch 23/64 loss: -0.34966957569122314
Batch 24/64 loss: -0.34243834018707275
Batch 25/64 loss: -0.36442139744758606
Batch 26/64 loss: -0.33527612686157227
Batch 27/64 loss: -0.3518512547016144
Batch 28/64 loss: -0.3637697100639343
Batch 29/64 loss: -0.33485227823257446
Batch 30/64 loss: -0.3590885400772095
Batch 31/64 loss: -0.34091371297836304
Batch 32/64 loss: -0.3344442844390869
Batch 33/64 loss: -0.35981613397598267
Batch 34/64 loss: -0.3660755157470703
Batch 35/64 loss: -0.35644176602363586
Batch 36/64 loss: -0.34835293889045715
Batch 37/64 loss: -0.33163586258888245
Batch 38/64 loss: -0.3438308835029602
Batch 39/64 loss: -0.37945637106895447
Batch 40/64 loss: -0.33997392654418945
Batch 41/64 loss: -0.3410426378250122
Batch 42/64 loss: -0.2637839913368225
Batch 43/64 loss: -0.34447672963142395
Batch 44/64 loss: -0.3619593381881714
Batch 45/64 loss: -0.3462880253791809
Batch 46/64 loss: -0.3612086772918701
Batch 47/64 loss: -0.3457814157009125
Batch 48/64 loss: -0.33249419927597046
Batch 49/64 loss: -0.34612470865249634
Batch 50/64 loss: -0.3509082794189453
Batch 51/64 loss: -0.3501465916633606
Batch 52/64 loss: -0.37064269185066223
Batch 53/64 loss: -0.34751853346824646
Batch 54/64 loss: -0.339646577835083
Batch 55/64 loss: -0.31989729404449463
Batch 56/64 loss: -0.2626219391822815
Batch 57/64 loss: -0.3561357259750366
Batch 58/64 loss: -0.3033038377761841
Batch 59/64 loss: -0.35458803176879883
Batch 60/64 loss: -0.29676032066345215
Batch 61/64 loss: -0.35931557416915894
Batch 62/64 loss: -0.3148097097873688
Batch 63/64 loss: -0.3417586386203766
Batch 64/64 loss: -0.3276464343070984
Epoch 378  Train loss: -0.34131910169825835  Val loss: 0.031618602497061506
Epoch 379
-------------------------------
Batch 1/64 loss: -0.33185243606567383
Batch 2/64 loss: -0.3627881407737732
Batch 3/64 loss: -0.35808250308036804
Batch 4/64 loss: -0.3803332448005676
Batch 5/64 loss: -0.34905916452407837
Batch 6/64 loss: -0.36991989612579346
Batch 7/64 loss: -0.32162922620773315
Batch 8/64 loss: -0.3685460090637207
Batch 9/64 loss: -0.27458176016807556
Batch 10/64 loss: -0.3275940716266632
Batch 11/64 loss: -0.34220007061958313
Batch 12/64 loss: -0.34213384985923767
Batch 13/64 loss: -0.3629131317138672
Batch 14/64 loss: -0.3239486813545227
Batch 15/64 loss: -0.38741928339004517
Batch 16/64 loss: -0.30506300926208496
Batch 17/64 loss: -0.3302236795425415
Batch 18/64 loss: -0.35411083698272705
Batch 19/64 loss: -0.3304225504398346
Batch 20/64 loss: -0.34496086835861206
Batch 21/64 loss: -0.36904633045196533
Batch 22/64 loss: -0.3761942982673645
Batch 23/64 loss: -0.34176963567733765
Batch 24/64 loss: -0.3334274888038635
Batch 25/64 loss: -0.3485470712184906
Batch 26/64 loss: -0.33360156416893005
Batch 27/64 loss: -0.35252630710601807
Batch 28/64 loss: -0.33487462997436523
Batch 29/64 loss: -0.3442201614379883
Batch 30/64 loss: -0.3335953950881958
Batch 31/64 loss: -0.369951069355011
Batch 32/64 loss: -0.34485071897506714
Batch 33/64 loss: -0.3603615164756775
Batch 34/64 loss: -0.2966488301753998
Batch 35/64 loss: -0.33853116631507874
Batch 36/64 loss: -0.3263174593448639
Batch 37/64 loss: -0.3610956072807312
Batch 38/64 loss: -0.3229483664035797
Batch 39/64 loss: -0.2872488498687744
Batch 40/64 loss: -0.3470103144645691
Batch 41/64 loss: -0.3243238925933838
Batch 42/64 loss: -0.33485713601112366
Batch 43/64 loss: -0.3073820173740387
Batch 44/64 loss: -0.3332487642765045
Batch 45/64 loss: -0.3595699071884155
Batch 46/64 loss: -0.3217819333076477
Batch 47/64 loss: -0.3656556010246277
Batch 48/64 loss: -0.3487435579299927
Batch 49/64 loss: -0.3336550295352936
Batch 50/64 loss: -0.37038785219192505
Batch 51/64 loss: -0.32062122225761414
Batch 52/64 loss: -0.33944666385650635
Batch 53/64 loss: -0.34191542863845825
Batch 54/64 loss: -0.2984127998352051
Batch 55/64 loss: -0.36069154739379883
Batch 56/64 loss: -0.2972555160522461
Batch 57/64 loss: -0.3628438711166382
Batch 58/64 loss: -0.36156541109085083
Batch 59/64 loss: -0.3282230794429779
Batch 60/64 loss: -0.3302226662635803
Batch 61/64 loss: -0.3341245651245117
Batch 62/64 loss: -0.3255600035190582
Batch 63/64 loss: -0.37835025787353516
Batch 64/64 loss: -0.3513556122779846
Epoch 379  Train loss: -0.34090830788892856  Val loss: 0.03192293275262892
Epoch 380
-------------------------------
Batch 1/64 loss: -0.3125540316104889
Batch 2/64 loss: -0.3357688784599304
Batch 3/64 loss: -0.3641197681427002
Batch 4/64 loss: -0.34615087509155273
Batch 5/64 loss: -0.35740357637405396
Batch 6/64 loss: -0.3664705753326416
Batch 7/64 loss: -0.3185412585735321
Batch 8/64 loss: -0.32414475083351135
Batch 9/64 loss: -0.29041731357574463
Batch 10/64 loss: -0.3646719753742218
Batch 11/64 loss: -0.3686957359313965
Batch 12/64 loss: -0.35210996866226196
Batch 13/64 loss: -0.3752215504646301
Batch 14/64 loss: -0.3609347641468048
Batch 15/64 loss: -0.35573047399520874
Batch 16/64 loss: -0.33772143721580505
Batch 17/64 loss: -0.3312802314758301
Batch 18/64 loss: -0.3654938340187073
Batch 19/64 loss: -0.36922016739845276
Batch 20/64 loss: -0.3605839014053345
Batch 21/64 loss: -0.33094853162765503
Batch 22/64 loss: -0.3461359143257141
Batch 23/64 loss: -0.3645581007003784
Batch 24/64 loss: -0.370532751083374
Batch 25/64 loss: -0.31466013193130493
Batch 26/64 loss: -0.35465675592422485
Batch 27/64 loss: -0.3573857545852661
Batch 28/64 loss: -0.3397723436355591
Batch 29/64 loss: -0.35539036989212036
Batch 30/64 loss: -0.35642677545547485
Batch 31/64 loss: -0.295879602432251
Batch 32/64 loss: -0.35910069942474365
Batch 33/64 loss: -0.3536291718482971
Batch 34/64 loss: -0.3582000136375427
Batch 35/64 loss: -0.36692649126052856
Batch 36/64 loss: -0.3251749277114868
Batch 37/64 loss: -0.3520497679710388
Batch 38/64 loss: -0.3234938681125641
Batch 39/64 loss: -0.3493320941925049
Batch 40/64 loss: -0.3638988137245178
Batch 41/64 loss: -0.38876771926879883
Batch 42/64 loss: -0.35980546474456787
Batch 43/64 loss: -0.36585867404937744
Batch 44/64 loss: -0.3555639684200287
Batch 45/64 loss: -0.32507529854774475
Batch 46/64 loss: -0.3545529842376709
Batch 47/64 loss: -0.3297516107559204
Batch 48/64 loss: -0.37214019894599915
Batch 49/64 loss: -0.3118720054626465
Batch 50/64 loss: -0.3516136407852173
Batch 51/64 loss: -0.33001354336738586
Batch 52/64 loss: -0.3462270498275757
Batch 53/64 loss: -0.3582853376865387
Batch 54/64 loss: -0.3249451816082001
Batch 55/64 loss: -0.31451416015625
Batch 56/64 loss: -0.3426675796508789
Batch 57/64 loss: -0.3808228373527527
Batch 58/64 loss: -0.3340226113796234
Batch 59/64 loss: -0.35652583837509155
Batch 60/64 loss: -0.33773452043533325
Batch 61/64 loss: -0.33451321721076965
Batch 62/64 loss: -0.34418368339538574
Batch 63/64 loss: -0.3342553377151489
Batch 64/64 loss: -0.3268704414367676
Epoch 380  Train loss: -0.3465765214433857  Val loss: 0.032960418163705936
Epoch 381
-------------------------------
Batch 1/64 loss: -0.3144340515136719
Batch 2/64 loss: -0.35773980617523193
Batch 3/64 loss: -0.3803507685661316
Batch 4/64 loss: -0.35607457160949707
Batch 5/64 loss: -0.29258593916893005
Batch 6/64 loss: -0.3524460196495056
Batch 7/64 loss: -0.31455522775650024
Batch 8/64 loss: -0.3447372317314148
Batch 9/64 loss: -0.3589893877506256
Batch 10/64 loss: -0.33509019017219543
Batch 11/64 loss: -0.3461339473724365
Batch 12/64 loss: -0.37628060579299927
Batch 13/64 loss: -0.3133150637149811
Batch 14/64 loss: -0.3860946297645569
Batch 15/64 loss: -0.2878703474998474
Batch 16/64 loss: -0.34353774785995483
Batch 17/64 loss: -0.31537926197052
Batch 18/64 loss: -0.34988534450531006
Batch 19/64 loss: -0.3234267234802246
Batch 20/64 loss: -0.3629719316959381
Batch 21/64 loss: -0.33298712968826294
Batch 22/64 loss: -0.36725592613220215
Batch 23/64 loss: -0.3279801607131958
Batch 24/64 loss: -0.3272310793399811
Batch 25/64 loss: -0.34576478600502014
Batch 26/64 loss: -0.3444831967353821
Batch 27/64 loss: -0.3127189874649048
Batch 28/64 loss: -0.3528388738632202
Batch 29/64 loss: -0.3562617301940918
Batch 30/64 loss: -0.33900225162506104
Batch 31/64 loss: -0.3069363236427307
Batch 32/64 loss: -0.30053573846817017
Batch 33/64 loss: -0.33404111862182617
Batch 34/64 loss: -0.35340166091918945
Batch 35/64 loss: -0.3163059651851654
Batch 36/64 loss: -0.3547055721282959
Batch 37/64 loss: -0.30283188819885254
Batch 38/64 loss: -0.344016969203949
Batch 39/64 loss: -0.3724205791950226
Batch 40/64 loss: -0.32766178250312805
Batch 41/64 loss: -0.3514746427536011
Batch 42/64 loss: -0.3642193675041199
Batch 43/64 loss: -0.31515926122665405
Batch 44/64 loss: -0.31728941202163696
Batch 45/64 loss: -0.3186458945274353
Batch 46/64 loss: -0.360244482755661
Batch 47/64 loss: -0.37827548384666443
Batch 48/64 loss: -0.3380016088485718
Batch 49/64 loss: -0.36101388931274414
Batch 50/64 loss: -0.3274644613265991
Batch 51/64 loss: -0.36882901191711426
Batch 52/64 loss: -0.2720009982585907
Batch 53/64 loss: -0.3346906900405884
Batch 54/64 loss: -0.34504252672195435
Batch 55/64 loss: -0.30754780769348145
Batch 56/64 loss: -0.32317084074020386
Batch 57/64 loss: -0.3083593249320984
Batch 58/64 loss: -0.3590904772281647
Batch 59/64 loss: -0.31470781564712524
Batch 60/64 loss: -0.33928200602531433
Batch 61/64 loss: -0.35409438610076904
Batch 62/64 loss: -0.3595590889453888
Batch 63/64 loss: -0.36658868193626404
Batch 64/64 loss: -0.33792629837989807
Epoch 381  Train loss: -0.33834474355566735  Val loss: 0.03379471031660886
Epoch 382
-------------------------------
Batch 1/64 loss: -0.3781578540802002
Batch 2/64 loss: -0.3660723865032196
Batch 3/64 loss: -0.2872896194458008
Batch 4/64 loss: -0.33612674474716187
Batch 5/64 loss: -0.356392502784729
Batch 6/64 loss: -0.3628816604614258
Batch 7/64 loss: -0.28735172748565674
Batch 8/64 loss: -0.36570030450820923
Batch 9/64 loss: -0.30952295660972595
Batch 10/64 loss: -0.34963589906692505
Batch 11/64 loss: -0.3421798348426819
Batch 12/64 loss: -0.3060917854309082
Batch 13/64 loss: -0.3388490080833435
Batch 14/64 loss: -0.33944785594940186
Batch 15/64 loss: -0.34049302339553833
Batch 16/64 loss: -0.3485347628593445
Batch 17/64 loss: -0.2936254143714905
Batch 18/64 loss: -0.3571699857711792
Batch 19/64 loss: -0.3307574689388275
Batch 20/64 loss: -0.37280115485191345
Batch 21/64 loss: -0.3358977437019348
Batch 22/64 loss: -0.3301503360271454
Batch 23/64 loss: -0.36110442876815796
Batch 24/64 loss: -0.34424567222595215
Batch 25/64 loss: -0.34098437428474426
Batch 26/64 loss: -0.3381534516811371
Batch 27/64 loss: -0.3114157021045685
Batch 28/64 loss: -0.36021149158477783
Batch 29/64 loss: -0.3709227442741394
Batch 30/64 loss: -0.3164941072463989
Batch 31/64 loss: -0.3318463861942291
Batch 32/64 loss: -0.35155394673347473
Batch 33/64 loss: -0.3095325231552124
Batch 34/64 loss: -0.34048157930374146
Batch 35/64 loss: -0.3545874059200287
Batch 36/64 loss: -0.37010645866394043
Batch 37/64 loss: -0.3393440842628479
Batch 38/64 loss: -0.35184672474861145
Batch 39/64 loss: -0.35838085412979126
Batch 40/64 loss: -0.34819281101226807
Batch 41/64 loss: -0.30517226457595825
Batch 42/64 loss: -0.33975929021835327
Batch 43/64 loss: -0.3337244987487793
Batch 44/64 loss: -0.352193146944046
Batch 45/64 loss: -0.355563223361969
Batch 46/64 loss: -0.35553255677223206
Batch 47/64 loss: -0.36496230959892273
Batch 48/64 loss: -0.3562930226325989
Batch 49/64 loss: -0.35944151878356934
Batch 50/64 loss: -0.37380099296569824
Batch 51/64 loss: -0.3623531758785248
Batch 52/64 loss: -0.2989288866519928
Batch 53/64 loss: -0.3586214780807495
Batch 54/64 loss: -0.33443933725357056
Batch 55/64 loss: -0.3445855975151062
Batch 56/64 loss: -0.3243946433067322
Batch 57/64 loss: -0.31633758544921875
Batch 58/64 loss: -0.30620691180229187
Batch 59/64 loss: -0.36020785570144653
Batch 60/64 loss: -0.31525662541389465
Batch 61/64 loss: -0.36578214168548584
Batch 62/64 loss: -0.3109452724456787
Batch 63/64 loss: -0.30913975834846497
Batch 64/64 loss: -0.3450767993927002
Epoch 382  Train loss: -0.3403448544296564  Val loss: 0.031924492100260105
Epoch 383
-------------------------------
Batch 1/64 loss: -0.33200502395629883
Batch 2/64 loss: -0.34278467297554016
Batch 3/64 loss: -0.33788859844207764
Batch 4/64 loss: -0.3653920292854309
Batch 5/64 loss: -0.36819595098495483
Batch 6/64 loss: -0.362123966217041
Batch 7/64 loss: -0.3350602090358734
Batch 8/64 loss: -0.3251308798789978
Batch 9/64 loss: -0.35081472992897034
Batch 10/64 loss: -0.3303191065788269
Batch 11/64 loss: -0.35313186049461365
Batch 12/64 loss: -0.332115113735199
Batch 13/64 loss: -0.3207802176475525
Batch 14/64 loss: -0.3291245996952057
Batch 15/64 loss: -0.3716195523738861
Batch 16/64 loss: -0.36211246252059937
Batch 17/64 loss: -0.3652886748313904
Batch 18/64 loss: -0.32497698068618774
Batch 19/64 loss: -0.3580476939678192
Batch 20/64 loss: -0.3594929277896881
Batch 21/64 loss: -0.34009966254234314
Batch 22/64 loss: -0.324523001909256
Batch 23/64 loss: -0.3092055916786194
Batch 24/64 loss: -0.35410964488983154
Batch 25/64 loss: -0.38319844007492065
Batch 26/64 loss: -0.33071771264076233
Batch 27/64 loss: -0.36351701617240906
Batch 28/64 loss: -0.2915303707122803
Batch 29/64 loss: -0.32626211643218994
Batch 30/64 loss: -0.3814625144004822
Batch 31/64 loss: -0.3209119439125061
Batch 32/64 loss: -0.3626940846443176
Batch 33/64 loss: -0.3348885774612427
Batch 34/64 loss: -0.37353652715682983
Batch 35/64 loss: -0.36961647868156433
Batch 36/64 loss: -0.3485643267631531
Batch 37/64 loss: -0.3314627408981323
Batch 38/64 loss: -0.3371644616127014
Batch 39/64 loss: -0.3456924557685852
Batch 40/64 loss: -0.27373966574668884
Batch 41/64 loss: -0.3496277928352356
Batch 42/64 loss: -0.34295690059661865
Batch 43/64 loss: -0.2874031960964203
Batch 44/64 loss: -0.37250059843063354
Batch 45/64 loss: -0.36204051971435547
Batch 46/64 loss: -0.368876576423645
Batch 47/64 loss: -0.3567162752151489
Batch 48/64 loss: -0.35975703597068787
Batch 49/64 loss: -0.33314964175224304
Batch 50/64 loss: -0.252072811126709
Batch 51/64 loss: -0.3275824785232544
Batch 52/64 loss: -0.3588416278362274
Batch 53/64 loss: -0.33125531673431396
Batch 54/64 loss: -0.34542185068130493
Batch 55/64 loss: -0.3431679606437683
Batch 56/64 loss: -0.32064834237098694
Batch 57/64 loss: -0.34995442628860474
Batch 58/64 loss: -0.33244994282722473
Batch 59/64 loss: -0.3409115672111511
Batch 60/64 loss: -0.36961156129837036
Batch 61/64 loss: -0.33798283338546753
Batch 62/64 loss: -0.3370018005371094
Batch 63/64 loss: -0.35794904828071594
Batch 64/64 loss: -0.33681342005729675
Epoch 383  Train loss: -0.3422712589011473  Val loss: 0.033536058725769986
Epoch 384
-------------------------------
Batch 1/64 loss: -0.2858676314353943
Batch 2/64 loss: -0.3545070290565491
Batch 3/64 loss: -0.3182893991470337
Batch 4/64 loss: -0.3853272795677185
Batch 5/64 loss: -0.332819402217865
Batch 6/64 loss: -0.3311505913734436
Batch 7/64 loss: -0.3328203856945038
Batch 8/64 loss: -0.34203076362609863
Batch 9/64 loss: -0.3573116660118103
Batch 10/64 loss: -0.3687172532081604
Batch 11/64 loss: -0.3467845916748047
Batch 12/64 loss: -0.35394710302352905
Batch 13/64 loss: -0.35549667477607727
Batch 14/64 loss: -0.3407436013221741
Batch 15/64 loss: -0.34552422165870667
Batch 16/64 loss: -0.3405345380306244
Batch 17/64 loss: -0.34588706493377686
Batch 18/64 loss: -0.3360814154148102
Batch 19/64 loss: -0.32421228289604187
Batch 20/64 loss: -0.32667019963264465
Batch 21/64 loss: -0.34102535247802734
Batch 22/64 loss: -0.338062584400177
Batch 23/64 loss: -0.3366473317146301
Batch 24/64 loss: -0.36422109603881836
Batch 25/64 loss: -0.33049196004867554
Batch 26/64 loss: -0.3685765564441681
Batch 27/64 loss: -0.36032092571258545
Batch 28/64 loss: -0.3502677083015442
Batch 29/64 loss: -0.31639114022254944
Batch 30/64 loss: -0.33311936259269714
Batch 31/64 loss: -0.34377533197402954
Batch 32/64 loss: -0.36841440200805664
Batch 33/64 loss: -0.36874672770500183
Batch 34/64 loss: -0.33362334966659546
Batch 35/64 loss: -0.3275272250175476
Batch 36/64 loss: -0.3208540081977844
Batch 37/64 loss: -0.3621460199356079
Batch 38/64 loss: -0.3159635663032532
Batch 39/64 loss: -0.34464454650878906
Batch 40/64 loss: -0.3164799213409424
Batch 41/64 loss: -0.3390269875526428
Batch 42/64 loss: -0.3418543040752411
Batch 43/64 loss: -0.36326491832733154
Batch 44/64 loss: -0.3405919075012207
Batch 45/64 loss: -0.3661677837371826
Batch 46/64 loss: -0.31124818325042725
Batch 47/64 loss: -0.35606157779693604
Batch 48/64 loss: -0.3745502829551697
Batch 49/64 loss: -0.35708504915237427
Batch 50/64 loss: -0.39449983835220337
Batch 51/64 loss: -0.2862374484539032
Batch 52/64 loss: -0.3525852560997009
Batch 53/64 loss: -0.35412830114364624
Batch 54/64 loss: -0.3375914692878723
Batch 55/64 loss: -0.3650429844856262
Batch 56/64 loss: -0.34719276428222656
Batch 57/64 loss: -0.3539009392261505
Batch 58/64 loss: -0.3363180160522461
Batch 59/64 loss: -0.37376976013183594
Batch 60/64 loss: -0.34581565856933594
Batch 61/64 loss: -0.3527546525001526
Batch 62/64 loss: -0.3464229106903076
Batch 63/64 loss: -0.357635498046875
Batch 64/64 loss: -0.32275933027267456
Epoch 384  Train loss: -0.3444994227558959  Val loss: 0.0332106673430741
Epoch 385
-------------------------------
Batch 1/64 loss: -0.3625224232673645
Batch 2/64 loss: -0.37019485235214233
Batch 3/64 loss: -0.3344992399215698
Batch 4/64 loss: -0.36574631929397583
Batch 5/64 loss: -0.376400351524353
Batch 6/64 loss: -0.33750325441360474
Batch 7/64 loss: -0.3832613527774811
Batch 8/64 loss: -0.3593182861804962
Batch 9/64 loss: -0.37040045857429504
Batch 10/64 loss: -0.3723621964454651
Batch 11/64 loss: -0.33068791031837463
Batch 12/64 loss: -0.3158305883407593
Batch 13/64 loss: -0.3724992871284485
Batch 14/64 loss: -0.3581594228744507
Batch 15/64 loss: -0.25393444299697876
Batch 16/64 loss: -0.35602906346321106
Batch 17/64 loss: -0.35326939821243286
Batch 18/64 loss: -0.3140505850315094
Batch 19/64 loss: -0.3564983308315277
Batch 20/64 loss: -0.3669355511665344
Batch 21/64 loss: -0.34342122077941895
Batch 22/64 loss: -0.3216722309589386
Batch 23/64 loss: -0.3499273359775543
Batch 24/64 loss: -0.32683539390563965
Batch 25/64 loss: -0.3651130795478821
Batch 26/64 loss: -0.382845938205719
Batch 27/64 loss: -0.2866273522377014
Batch 28/64 loss: -0.33772486448287964
Batch 29/64 loss: -0.34202665090560913
Batch 30/64 loss: -0.3353772461414337
Batch 31/64 loss: -0.3101940155029297
Batch 32/64 loss: -0.3492909073829651
Batch 33/64 loss: -0.30028384923934937
Batch 34/64 loss: -0.31782418489456177
Batch 35/64 loss: -0.3258100748062134
Batch 36/64 loss: -0.34185248613357544
Batch 37/64 loss: -0.31493738293647766
Batch 38/64 loss: -0.37950316071510315
Batch 39/64 loss: -0.3569449186325073
Batch 40/64 loss: -0.3490833044052124
Batch 41/64 loss: -0.3335055708885193
Batch 42/64 loss: -0.3464903235435486
Batch 43/64 loss: -0.34728145599365234
Batch 44/64 loss: -0.3602251410484314
Batch 45/64 loss: -0.3371623754501343
Batch 46/64 loss: -0.37392568588256836
Batch 47/64 loss: -0.34551793336868286
Batch 48/64 loss: -0.35818785429000854
Batch 49/64 loss: -0.33343270421028137
Batch 50/64 loss: -0.33781999349594116
Batch 51/64 loss: -0.37513813376426697
Batch 52/64 loss: -0.3185645341873169
Batch 53/64 loss: -0.3212509751319885
Batch 54/64 loss: -0.35006558895111084
Batch 55/64 loss: -0.38135820627212524
Batch 56/64 loss: -0.37451326847076416
Batch 57/64 loss: -0.3175393342971802
Batch 58/64 loss: -0.3652289807796478
Batch 59/64 loss: -0.32141563296318054
Batch 60/64 loss: -0.36050647497177124
Batch 61/64 loss: -0.3556627333164215
Batch 62/64 loss: -0.273495614528656
Batch 63/64 loss: -0.2779467701911926
Batch 64/64 loss: -0.27167969942092896
Epoch 385  Train loss: -0.342233630956388  Val loss: 0.0350442735599898
Epoch 386
-------------------------------
Batch 1/64 loss: -0.3073241114616394
Batch 2/64 loss: -0.36926132440567017
Batch 3/64 loss: -0.35881924629211426
Batch 4/64 loss: -0.3644658625125885
Batch 5/64 loss: -0.3342588543891907
Batch 6/64 loss: -0.3445844054222107
Batch 7/64 loss: -0.34473663568496704
Batch 8/64 loss: -0.34640467166900635
Batch 9/64 loss: -0.3774293065071106
Batch 10/64 loss: -0.3241673707962036
Batch 11/64 loss: -0.3694078028202057
Batch 12/64 loss: -0.35509973764419556
Batch 13/64 loss: -0.3621506094932556
Batch 14/64 loss: -0.37418416142463684
Batch 15/64 loss: -0.3338739275932312
Batch 16/64 loss: -0.3517242670059204
Batch 17/64 loss: -0.3038822114467621
Batch 18/64 loss: -0.2883304953575134
Batch 19/64 loss: -0.3504231572151184
Batch 20/64 loss: -0.37206941843032837
Batch 21/64 loss: -0.35116690397262573
Batch 22/64 loss: -0.29981064796447754
Batch 23/64 loss: -0.3439260423183441
Batch 24/64 loss: -0.3358141779899597
Batch 25/64 loss: -0.3469310998916626
Batch 26/64 loss: -0.352646142244339
Batch 27/64 loss: -0.3686627149581909
Batch 28/64 loss: -0.3667546510696411
Batch 29/64 loss: -0.3777849078178406
Batch 30/64 loss: -0.340335875749588
Batch 31/64 loss: -0.3572198152542114
Batch 32/64 loss: -0.34665757417678833
Batch 33/64 loss: -0.3456524610519409
Batch 34/64 loss: -0.32675179839134216
Batch 35/64 loss: -0.3663892149925232
Batch 36/64 loss: -0.37737444043159485
Batch 37/64 loss: -0.3488888442516327
Batch 38/64 loss: -0.3740978240966797
Batch 39/64 loss: -0.33574602007865906
Batch 40/64 loss: -0.35038864612579346
Batch 41/64 loss: -0.35020655393600464
Batch 42/64 loss: -0.33219003677368164
Batch 43/64 loss: -0.3601718544960022
Batch 44/64 loss: -0.3276543617248535
Batch 45/64 loss: -0.32964545488357544
Batch 46/64 loss: -0.365281343460083
Batch 47/64 loss: -0.34923532605171204
Batch 48/64 loss: -0.31520599126815796
Batch 49/64 loss: -0.3429059386253357
Batch 50/64 loss: -0.3627081513404846
Batch 51/64 loss: -0.3475455641746521
Batch 52/64 loss: -0.35709941387176514
Batch 53/64 loss: -0.3504481017589569
Batch 54/64 loss: -0.3593684434890747
Batch 55/64 loss: -0.33080828189849854
Batch 56/64 loss: -0.3608437776565552
Batch 57/64 loss: -0.34426629543304443
Batch 58/64 loss: -0.34090739488601685
Batch 59/64 loss: -0.3502613604068756
Batch 60/64 loss: -0.3494826555252075
Batch 61/64 loss: -0.29121342301368713
Batch 62/64 loss: -0.35792872309684753
Batch 63/64 loss: -0.3445523977279663
Batch 64/64 loss: -0.36001378297805786
Epoch 386  Train loss: -0.3472241343236437  Val loss: 0.032703920737984254
Epoch 387
-------------------------------
Batch 1/64 loss: -0.3562769293785095
Batch 2/64 loss: -0.3142738938331604
Batch 3/64 loss: -0.3713395595550537
Batch 4/64 loss: -0.3634602427482605
Batch 5/64 loss: -0.30504316091537476
Batch 6/64 loss: -0.37180477380752563
Batch 7/64 loss: -0.35367199778556824
Batch 8/64 loss: -0.3614174723625183
Batch 9/64 loss: -0.3499198853969574
Batch 10/64 loss: -0.3756243586540222
Batch 11/64 loss: -0.3778475522994995
Batch 12/64 loss: -0.3625195026397705
Batch 13/64 loss: -0.3464033901691437
Batch 14/64 loss: -0.3073303997516632
Batch 15/64 loss: -0.36551254987716675
Batch 16/64 loss: -0.3463539481163025
Batch 17/64 loss: -0.34862425923347473
Batch 18/64 loss: -0.35822612047195435
Batch 19/64 loss: -0.3539227843284607
Batch 20/64 loss: -0.33663690090179443
Batch 21/64 loss: -0.34828972816467285
Batch 22/64 loss: -0.38289156556129456
Batch 23/64 loss: -0.37400269508361816
Batch 24/64 loss: -0.3393779993057251
Batch 25/64 loss: -0.3622415065765381
Batch 26/64 loss: -0.34977036714553833
Batch 27/64 loss: -0.3416345715522766
Batch 28/64 loss: -0.3392239212989807
Batch 29/64 loss: -0.3645402789115906
Batch 30/64 loss: -0.30667829513549805
Batch 31/64 loss: -0.35908618569374084
Batch 32/64 loss: -0.33873653411865234
Batch 33/64 loss: -0.31125062704086304
Batch 34/64 loss: -0.36800938844680786
Batch 35/64 loss: -0.3560527265071869
Batch 36/64 loss: -0.3570502698421478
Batch 37/64 loss: -0.3407648503780365
Batch 38/64 loss: -0.3203032612800598
Batch 39/64 loss: -0.3276268243789673
Batch 40/64 loss: -0.30087190866470337
Batch 41/64 loss: -0.3182273507118225
Batch 42/64 loss: -0.3172626495361328
Batch 43/64 loss: -0.3241751492023468
Batch 44/64 loss: -0.3516559302806854
Batch 45/64 loss: -0.3388509750366211
Batch 46/64 loss: -0.3655341863632202
Batch 47/64 loss: -0.3388030529022217
Batch 48/64 loss: -0.37741923332214355
Batch 49/64 loss: -0.36192044615745544
Batch 50/64 loss: -0.33340930938720703
Batch 51/64 loss: -0.3573269248008728
Batch 52/64 loss: -0.32499778270721436
Batch 53/64 loss: -0.33660098910331726
Batch 54/64 loss: -0.30772021412849426
Batch 55/64 loss: -0.3401065468788147
Batch 56/64 loss: -0.3242006003856659
Batch 57/64 loss: -0.3625061810016632
Batch 58/64 loss: -0.37633419036865234
Batch 59/64 loss: -0.30988627672195435
Batch 60/64 loss: -0.3339788615703583
Batch 61/64 loss: -0.354819655418396
Batch 62/64 loss: -0.3336021304130554
Batch 63/64 loss: -0.3641204237937927
Batch 64/64 loss: -0.3126198649406433
Epoch 387  Train loss: -0.34513783758761835  Val loss: 0.032494130208320225
Epoch 388
-------------------------------
Batch 1/64 loss: -0.3738195598125458
Batch 2/64 loss: -0.3571892976760864
Batch 3/64 loss: -0.3463926315307617
Batch 4/64 loss: -0.3606002926826477
Batch 5/64 loss: -0.3441694378852844
Batch 6/64 loss: -0.37212714552879333
Batch 7/64 loss: -0.33910924196243286
Batch 8/64 loss: -0.33230990171432495
Batch 9/64 loss: -0.3492918014526367
Batch 10/64 loss: -0.3705257773399353
Batch 11/64 loss: -0.3466620147228241
Batch 12/64 loss: -0.3816852569580078
Batch 13/64 loss: -0.32908332347869873
Batch 14/64 loss: -0.34464406967163086
Batch 15/64 loss: -0.3599100708961487
Batch 16/64 loss: -0.3700781464576721
Batch 17/64 loss: -0.35236814618110657
Batch 18/64 loss: -0.3401569724082947
Batch 19/64 loss: -0.3385310769081116
Batch 20/64 loss: -0.327525794506073
Batch 21/64 loss: -0.3623894453048706
Batch 22/64 loss: -0.356209397315979
Batch 23/64 loss: -0.33809322118759155
Batch 24/64 loss: -0.3498784005641937
Batch 25/64 loss: -0.33052825927734375
Batch 26/64 loss: -0.35276657342910767
Batch 27/64 loss: -0.34484583139419556
Batch 28/64 loss: -0.33371227979660034
Batch 29/64 loss: -0.3593524694442749
Batch 30/64 loss: -0.31639835238456726
Batch 31/64 loss: -0.2832834720611572
Batch 32/64 loss: -0.33477750420570374
Batch 33/64 loss: -0.33699366450309753
Batch 34/64 loss: -0.36444830894470215
Batch 35/64 loss: -0.3459948003292084
Batch 36/64 loss: -0.3299127519130707
Batch 37/64 loss: -0.36452996730804443
Batch 38/64 loss: -0.3466872572898865
Batch 39/64 loss: -0.3513714671134949
Batch 40/64 loss: -0.3767854571342468
Batch 41/64 loss: -0.3611908555030823
Batch 42/64 loss: -0.3780280351638794
Batch 43/64 loss: -0.3256373405456543
Batch 44/64 loss: -0.36669889092445374
Batch 45/64 loss: -0.3631052076816559
Batch 46/64 loss: -0.3313441574573517
Batch 47/64 loss: -0.309680700302124
Batch 48/64 loss: -0.3521736264228821
Batch 49/64 loss: -0.3499075770378113
Batch 50/64 loss: -0.34496477246284485
Batch 51/64 loss: -0.33682286739349365
Batch 52/64 loss: -0.3389294743537903
Batch 53/64 loss: -0.3560161590576172
Batch 54/64 loss: -0.3520830273628235
Batch 55/64 loss: -0.323574423789978
Batch 56/64 loss: -0.36107534170150757
Batch 57/64 loss: -0.2687714099884033
Batch 58/64 loss: -0.35687100887298584
Batch 59/64 loss: -0.29490819573402405
Batch 60/64 loss: -0.3641892075538635
Batch 61/64 loss: -0.3543074429035187
Batch 62/64 loss: -0.3385983109474182
Batch 63/64 loss: -0.3600407540798187
Batch 64/64 loss: -0.3446406126022339
Epoch 388  Train loss: -0.346079028821459  Val loss: 0.032759718264091464
Epoch 389
-------------------------------
Batch 1/64 loss: -0.34250083565711975
Batch 2/64 loss: -0.33898860216140747
Batch 3/64 loss: -0.34864088892936707
Batch 4/64 loss: -0.34446197748184204
Batch 5/64 loss: -0.33580297231674194
Batch 6/64 loss: -0.355918288230896
Batch 7/64 loss: -0.3214837908744812
Batch 8/64 loss: -0.36403000354766846
Batch 9/64 loss: -0.36710312962532043
Batch 10/64 loss: -0.3561379015445709
Batch 11/64 loss: -0.30632856488227844
Batch 12/64 loss: -0.337174654006958
Batch 13/64 loss: -0.3480070233345032
Batch 14/64 loss: -0.3548738658428192
Batch 15/64 loss: -0.32816579937934875
Batch 16/64 loss: -0.3682643175125122
Batch 17/64 loss: -0.36337175965309143
Batch 18/64 loss: -0.32575666904449463
Batch 19/64 loss: -0.3351961672306061
Batch 20/64 loss: -0.34566235542297363
Batch 21/64 loss: -0.32427889108657837
Batch 22/64 loss: -0.3489778935909271
Batch 23/64 loss: -0.33211255073547363
Batch 24/64 loss: -0.36094942688941956
Batch 25/64 loss: -0.3297414481639862
Batch 26/64 loss: -0.3575831949710846
Batch 27/64 loss: -0.35510748624801636
Batch 28/64 loss: -0.3343386948108673
Batch 29/64 loss: -0.36443740129470825
Batch 30/64 loss: -0.3570578694343567
Batch 31/64 loss: -0.36858946084976196
Batch 32/64 loss: -0.36965590715408325
Batch 33/64 loss: -0.3693642020225525
Batch 34/64 loss: -0.323314905166626
Batch 35/64 loss: -0.362388014793396
Batch 36/64 loss: -0.30819958448410034
Batch 37/64 loss: -0.37172526121139526
Batch 38/64 loss: -0.3487004339694977
Batch 39/64 loss: -0.3117695748806
Batch 40/64 loss: -0.34021496772766113
Batch 41/64 loss: -0.34421756863594055
Batch 42/64 loss: -0.3369585871696472
Batch 43/64 loss: -0.3239578902721405
Batch 44/64 loss: -0.3611239790916443
Batch 45/64 loss: -0.35235732793807983
Batch 46/64 loss: -0.3038788139820099
Batch 47/64 loss: -0.35798221826553345
Batch 48/64 loss: -0.34794241189956665
Batch 49/64 loss: -0.2988388240337372
Batch 50/64 loss: -0.339287132024765
Batch 51/64 loss: -0.3374893367290497
Batch 52/64 loss: -0.34665465354919434
Batch 53/64 loss: -0.34319576621055603
Batch 54/64 loss: -0.3184035122394562
Batch 55/64 loss: -0.32261866331100464
Batch 56/64 loss: -0.3283345401287079
Batch 57/64 loss: -0.3152531385421753
Batch 58/64 loss: -0.30718228220939636
Batch 59/64 loss: -0.35813313722610474
Batch 60/64 loss: -0.35573840141296387
Batch 61/64 loss: -0.35222357511520386
Batch 62/64 loss: -0.34984421730041504
Batch 63/64 loss: -0.3101855516433716
Batch 64/64 loss: -0.3639981746673584
Epoch 389  Train loss: -0.34213610817404355  Val loss: 0.03083792093283532
Epoch 390
-------------------------------
Batch 1/64 loss: -0.32452392578125
Batch 2/64 loss: -0.36440226435661316
Batch 3/64 loss: -0.3582959771156311
Batch 4/64 loss: -0.32973819971084595
Batch 5/64 loss: -0.36107051372528076
Batch 6/64 loss: -0.3700391352176666
Batch 7/64 loss: -0.3711533546447754
Batch 8/64 loss: -0.3551064133644104
Batch 9/64 loss: -0.3526999056339264
Batch 10/64 loss: -0.2923772931098938
Batch 11/64 loss: -0.33333730697631836
Batch 12/64 loss: -0.3365508019924164
Batch 13/64 loss: -0.37060874700546265
Batch 14/64 loss: -0.32124125957489014
Batch 15/64 loss: -0.34221142530441284
Batch 16/64 loss: -0.375456303358078
Batch 17/64 loss: -0.3436824679374695
Batch 18/64 loss: -0.3145179748535156
Batch 19/64 loss: -0.3508450984954834
Batch 20/64 loss: -0.33118826150894165
Batch 21/64 loss: -0.35088080167770386
Batch 22/64 loss: -0.32374346256256104
Batch 23/64 loss: -0.3816271722316742
Batch 24/64 loss: -0.34978383779525757
Batch 25/64 loss: -0.380466490983963
Batch 26/64 loss: -0.3619893491268158
Batch 27/64 loss: -0.3298499584197998
Batch 28/64 loss: -0.36215725541114807
Batch 29/64 loss: -0.360163152217865
Batch 30/64 loss: -0.35563406348228455
Batch 31/64 loss: -0.3527927100658417
Batch 32/64 loss: -0.33849236369132996
Batch 33/64 loss: -0.37500709295272827
Batch 34/64 loss: -0.3305965065956116
Batch 35/64 loss: -0.34624913334846497
Batch 36/64 loss: -0.3684847354888916
Batch 37/64 loss: -0.28239572048187256
Batch 38/64 loss: -0.3630841374397278
Batch 39/64 loss: -0.36880722641944885
Batch 40/64 loss: -0.3640090823173523
Batch 41/64 loss: -0.3151618242263794
Batch 42/64 loss: -0.36445438861846924
Batch 43/64 loss: -0.3802562355995178
Batch 44/64 loss: -0.3084183633327484
Batch 45/64 loss: -0.3724214434623718
Batch 46/64 loss: -0.32281365990638733
Batch 47/64 loss: -0.3616664707660675
Batch 48/64 loss: -0.36386632919311523
Batch 49/64 loss: -0.33670923113822937
Batch 50/64 loss: -0.3895302414894104
Batch 51/64 loss: -0.30545926094055176
Batch 52/64 loss: -0.33647072315216064
Batch 53/64 loss: -0.314863383769989
Batch 54/64 loss: -0.3515806198120117
Batch 55/64 loss: -0.35827940702438354
Batch 56/64 loss: -0.35185256600379944
Batch 57/64 loss: -0.3481872081756592
Batch 58/64 loss: -0.3160768151283264
Batch 59/64 loss: -0.316997230052948
Batch 60/64 loss: -0.3464582562446594
Batch 61/64 loss: -0.3348040282726288
Batch 62/64 loss: -0.3452290892601013
Batch 63/64 loss: -0.3149009048938751
Batch 64/64 loss: -0.2895948886871338
Epoch 390  Train loss: -0.34539474085265515  Val loss: 0.035187461122204756
Epoch 391
-------------------------------
Batch 1/64 loss: -0.34973734617233276
Batch 2/64 loss: -0.37187451124191284
Batch 3/64 loss: -0.34122759103775024
Batch 4/64 loss: -0.3657655715942383
Batch 5/64 loss: -0.3579927086830139
Batch 6/64 loss: -0.376491904258728
Batch 7/64 loss: -0.3005416989326477
Batch 8/64 loss: -0.3481588363647461
Batch 9/64 loss: -0.34455209970474243
Batch 10/64 loss: -0.35082659125328064
Batch 11/64 loss: -0.345115065574646
Batch 12/64 loss: -0.34956371784210205
Batch 13/64 loss: -0.33085897564888
Batch 14/64 loss: -0.3570023775100708
Batch 15/64 loss: -0.34003669023513794
Batch 16/64 loss: -0.3326234221458435
Batch 17/64 loss: -0.30194103717803955
Batch 18/64 loss: -0.35874319076538086
Batch 19/64 loss: -0.3709561824798584
Batch 20/64 loss: -0.3540855646133423
Batch 21/64 loss: -0.33376240730285645
Batch 22/64 loss: -0.320679247379303
Batch 23/64 loss: -0.34576416015625
Batch 24/64 loss: -0.3672293722629547
Batch 25/64 loss: -0.3263159990310669
Batch 26/64 loss: -0.38684600591659546
Batch 27/64 loss: -0.3214813768863678
Batch 28/64 loss: -0.3401191830635071
Batch 29/64 loss: -0.3349561393260956
Batch 30/64 loss: -0.3479204475879669
Batch 31/64 loss: -0.33737462759017944
Batch 32/64 loss: -0.3548090159893036
Batch 33/64 loss: -0.31599220633506775
Batch 34/64 loss: -0.33858081698417664
Batch 35/64 loss: -0.29808545112609863
Batch 36/64 loss: -0.3472155034542084
Batch 37/64 loss: -0.35868144035339355
Batch 38/64 loss: -0.3416939973831177
Batch 39/64 loss: -0.3432518541812897
Batch 40/64 loss: -0.31026220321655273
Batch 41/64 loss: -0.32997530698776245
Batch 42/64 loss: -0.3296712636947632
Batch 43/64 loss: -0.3279716372489929
Batch 44/64 loss: -0.34559354186058044
Batch 45/64 loss: -0.3675692677497864
Batch 46/64 loss: -0.3642520606517792
Batch 47/64 loss: -0.34903597831726074
Batch 48/64 loss: -0.31548506021499634
Batch 49/64 loss: -0.3487098813056946
Batch 50/64 loss: -0.33261969685554504
Batch 51/64 loss: -0.3609151244163513
Batch 52/64 loss: -0.35850805044174194
Batch 53/64 loss: -0.3551085591316223
Batch 54/64 loss: -0.3533214330673218
Batch 55/64 loss: -0.3422813415527344
Batch 56/64 loss: -0.3540624678134918
Batch 57/64 loss: -0.33142733573913574
Batch 58/64 loss: -0.3343045711517334
Batch 59/64 loss: -0.33307892084121704
Batch 60/64 loss: -0.33197322487831116
Batch 61/64 loss: -0.3733522891998291
Batch 62/64 loss: -0.35006213188171387
Batch 63/64 loss: -0.3613552153110504
Batch 64/64 loss: -0.3637063503265381
Epoch 391  Train loss: -0.3441965594011195  Val loss: 0.03354814474525321
Epoch 392
-------------------------------
Batch 1/64 loss: -0.37788450717926025
Batch 2/64 loss: -0.36655884981155396
Batch 3/64 loss: -0.31797903776168823
Batch 4/64 loss: -0.393320232629776
Batch 5/64 loss: -0.34739986062049866
Batch 6/64 loss: -0.33073997497558594
Batch 7/64 loss: -0.37700405716896057
Batch 8/64 loss: -0.3745060861110687
Batch 9/64 loss: -0.3514050841331482
Batch 10/64 loss: -0.36311227083206177
Batch 11/64 loss: -0.37418627738952637
Batch 12/64 loss: -0.3572736978530884
Batch 13/64 loss: -0.31759411096572876
Batch 14/64 loss: -0.34168362617492676
Batch 15/64 loss: -0.3546775281429291
Batch 16/64 loss: -0.37465792894363403
Batch 17/64 loss: -0.3526114523410797
Batch 18/64 loss: -0.37123361229896545
Batch 19/64 loss: -0.32327163219451904
Batch 20/64 loss: -0.3681508004665375
Batch 21/64 loss: -0.38633885979652405
Batch 22/64 loss: -0.36280930042266846
Batch 23/64 loss: -0.35222113132476807
Batch 24/64 loss: -0.338259756565094
Batch 25/64 loss: -0.3514450192451477
Batch 26/64 loss: -0.35758060216903687
Batch 27/64 loss: -0.3314930200576782
Batch 28/64 loss: -0.3656362295150757
Batch 29/64 loss: -0.3509969115257263
Batch 30/64 loss: -0.3324568271636963
Batch 31/64 loss: -0.3265983462333679
Batch 32/64 loss: -0.3562254309654236
Batch 33/64 loss: -0.34882286190986633
Batch 34/64 loss: -0.383779913187027
Batch 35/64 loss: -0.3645009696483612
Batch 36/64 loss: -0.353744238615036
Batch 37/64 loss: -0.3731250762939453
Batch 38/64 loss: -0.347303569316864
Batch 39/64 loss: -0.30144983530044556
Batch 40/64 loss: -0.3474171757698059
Batch 41/64 loss: -0.3690822422504425
Batch 42/64 loss: -0.3555370569229126
Batch 43/64 loss: -0.2951371669769287
Batch 44/64 loss: -0.3601974844932556
Batch 45/64 loss: -0.3508172035217285
Batch 46/64 loss: -0.35779643058776855
Batch 47/64 loss: -0.3494611382484436
Batch 48/64 loss: -0.31622493267059326
Batch 49/64 loss: -0.347810298204422
Batch 50/64 loss: -0.35055819153785706
Batch 51/64 loss: -0.36407268047332764
Batch 52/64 loss: -0.32139578461647034
Batch 53/64 loss: -0.35616207122802734
Batch 54/64 loss: -0.31938469409942627
Batch 55/64 loss: -0.29676416516304016
Batch 56/64 loss: -0.3462423086166382
Batch 57/64 loss: -0.3529775142669678
Batch 58/64 loss: -0.3559780716896057
Batch 59/64 loss: -0.34732311964035034
Batch 60/64 loss: -0.34509456157684326
Batch 61/64 loss: -0.3100006580352783
Batch 62/64 loss: -0.35315564274787903
Batch 63/64 loss: -0.3405577838420868
Batch 64/64 loss: -0.35468876361846924
Epoch 392  Train loss: -0.3497286819944195  Val loss: 0.03213687512473142
Epoch 393
-------------------------------
Batch 1/64 loss: -0.346571147441864
Batch 2/64 loss: -0.33652806282043457
Batch 3/64 loss: -0.28647223114967346
Batch 4/64 loss: -0.32086533308029175
Batch 5/64 loss: -0.3127315640449524
Batch 6/64 loss: -0.35421767830848694
Batch 7/64 loss: -0.3396650552749634
Batch 8/64 loss: -0.34484386444091797
Batch 9/64 loss: -0.3591763973236084
Batch 10/64 loss: -0.36724698543548584
Batch 11/64 loss: -0.35009926557540894
Batch 12/64 loss: -0.32384780049324036
Batch 13/64 loss: -0.32053399085998535
Batch 14/64 loss: -0.3524516224861145
Batch 15/64 loss: -0.3928811848163605
Batch 16/64 loss: -0.33303511142730713
Batch 17/64 loss: -0.3822622299194336
Batch 18/64 loss: -0.3324100375175476
Batch 19/64 loss: -0.3636541962623596
Batch 20/64 loss: -0.3470366895198822
Batch 21/64 loss: -0.3635924756526947
Batch 22/64 loss: -0.26096904277801514
Batch 23/64 loss: -0.3685690760612488
Batch 24/64 loss: -0.3796497583389282
Batch 25/64 loss: -0.3642154932022095
Batch 26/64 loss: -0.3303554058074951
Batch 27/64 loss: -0.35042905807495117
Batch 28/64 loss: -0.35113364458084106
Batch 29/64 loss: -0.34097540378570557
Batch 30/64 loss: -0.34119874238967896
Batch 31/64 loss: -0.3643052577972412
Batch 32/64 loss: -0.3460102081298828
Batch 33/64 loss: -0.35294511914253235
Batch 34/64 loss: -0.36053842306137085
Batch 35/64 loss: -0.3299477696418762
Batch 36/64 loss: -0.34674274921417236
Batch 37/64 loss: -0.33087652921676636
Batch 38/64 loss: -0.3632044196128845
Batch 39/64 loss: -0.3359348177909851
Batch 40/64 loss: -0.38575536012649536
Batch 41/64 loss: -0.3678470849990845
Batch 42/64 loss: -0.33056125044822693
Batch 43/64 loss: -0.32465195655822754
Batch 44/64 loss: -0.3463382124900818
Batch 45/64 loss: -0.36513763666152954
Batch 46/64 loss: -0.3259732723236084
Batch 47/64 loss: -0.35838356614112854
Batch 48/64 loss: -0.3581153154373169
Batch 49/64 loss: -0.32765892148017883
Batch 50/64 loss: -0.3637176752090454
Batch 51/64 loss: -0.36131876707077026
Batch 52/64 loss: -0.37322965264320374
Batch 53/64 loss: -0.37679022550582886
Batch 54/64 loss: -0.3065667152404785
Batch 55/64 loss: -0.2887958884239197
Batch 56/64 loss: -0.3537723124027252
Batch 57/64 loss: -0.3545154631137848
Batch 58/64 loss: -0.3720621168613434
Batch 59/64 loss: -0.3330496549606323
Batch 60/64 loss: -0.3510570526123047
Batch 61/64 loss: -0.3522678017616272
Batch 62/64 loss: -0.3805358409881592
Batch 63/64 loss: -0.32979267835617065
Batch 64/64 loss: -0.33699649572372437
Epoch 393  Train loss: -0.3464904413503759  Val loss: 0.032268588690413644
Epoch 394
-------------------------------
Batch 1/64 loss: -0.3915727734565735
Batch 2/64 loss: -0.3748021721839905
Batch 3/64 loss: -0.3455299139022827
Batch 4/64 loss: -0.3561798632144928
Batch 5/64 loss: -0.38092532753944397
Batch 6/64 loss: -0.37419939041137695
Batch 7/64 loss: -0.3314969539642334
Batch 8/64 loss: -0.3760851323604584
Batch 9/64 loss: -0.3279436528682709
Batch 10/64 loss: -0.3687285780906677
Batch 11/64 loss: -0.3643212616443634
Batch 12/64 loss: -0.33424830436706543
Batch 13/64 loss: -0.3830264210700989
Batch 14/64 loss: -0.36722326278686523
Batch 15/64 loss: -0.34977012872695923
Batch 16/64 loss: -0.3899025321006775
Batch 17/64 loss: -0.32156136631965637
Batch 18/64 loss: -0.3469610810279846
Batch 19/64 loss: -0.3419049084186554
Batch 20/64 loss: -0.36497628688812256
Batch 21/64 loss: -0.3284286856651306
Batch 22/64 loss: -0.34658241271972656
Batch 23/64 loss: -0.35245996713638306
Batch 24/64 loss: -0.3340563178062439
Batch 25/64 loss: -0.38117459416389465
Batch 26/64 loss: -0.33689576387405396
Batch 27/64 loss: -0.33943402767181396
Batch 28/64 loss: -0.36204347014427185
Batch 29/64 loss: -0.3663423955440521
Batch 30/64 loss: -0.3577503561973572
Batch 31/64 loss: -0.34402450919151306
Batch 32/64 loss: -0.33869850635528564
Batch 33/64 loss: -0.35743457078933716
Batch 34/64 loss: -0.3256097435951233
Batch 35/64 loss: -0.3366367816925049
Batch 36/64 loss: -0.3369830846786499
Batch 37/64 loss: -0.36497563123703003
Batch 38/64 loss: -0.3265194296836853
Batch 39/64 loss: -0.30040058493614197
Batch 40/64 loss: -0.32388836145401
Batch 41/64 loss: -0.30404573678970337
Batch 42/64 loss: -0.3793483376502991
Batch 43/64 loss: -0.36025160551071167
Batch 44/64 loss: -0.3773617744445801
Batch 45/64 loss: -0.35397669672966003
Batch 46/64 loss: -0.36628979444503784
Batch 47/64 loss: -0.3694288730621338
Batch 48/64 loss: -0.3664405941963196
Batch 49/64 loss: -0.3819282352924347
Batch 50/64 loss: -0.3336823284626007
Batch 51/64 loss: -0.29790937900543213
Batch 52/64 loss: -0.33155345916748047
Batch 53/64 loss: -0.3341330587863922
Batch 54/64 loss: -0.25046035647392273
Batch 55/64 loss: -0.3274574279785156
Batch 56/64 loss: -0.23544934391975403
Batch 57/64 loss: -0.34069252014160156
Batch 58/64 loss: -0.3485627770423889
Batch 59/64 loss: -0.3230014443397522
Batch 60/64 loss: -0.3406405448913574
Batch 61/64 loss: -0.3597143292427063
Batch 62/64 loss: -0.3404756784439087
Batch 63/64 loss: -0.36955803632736206
Batch 64/64 loss: -0.3422606289386749
Epoch 394  Train loss: -0.3466785303517884  Val loss: 0.03296492579057045
Epoch 395
-------------------------------
Batch 1/64 loss: -0.3604593575000763
Batch 2/64 loss: -0.32601258158683777
Batch 3/64 loss: -0.3615869879722595
Batch 4/64 loss: -0.3801576495170593
Batch 5/64 loss: -0.3273201584815979
Batch 6/64 loss: -0.38203075528144836
Batch 7/64 loss: -0.3201216161251068
Batch 8/64 loss: -0.3215784728527069
Batch 9/64 loss: -0.31689369678497314
Batch 10/64 loss: -0.3657972812652588
Batch 11/64 loss: -0.35320401191711426
Batch 12/64 loss: -0.34713125228881836
Batch 13/64 loss: -0.3449043035507202
Batch 14/64 loss: -0.31150466203689575
Batch 15/64 loss: -0.35638290643692017
Batch 16/64 loss: -0.3400091826915741
Batch 17/64 loss: -0.3479439616203308
Batch 18/64 loss: -0.3696713447570801
Batch 19/64 loss: -0.3044220805168152
Batch 20/64 loss: -0.321488618850708
Batch 21/64 loss: -0.3552247881889343
Batch 22/64 loss: -0.36979013681411743
Batch 23/64 loss: -0.37371039390563965
Batch 24/64 loss: -0.34832507371902466
Batch 25/64 loss: -0.3349478244781494
Batch 26/64 loss: -0.3473757803440094
Batch 27/64 loss: -0.3472854793071747
Batch 28/64 loss: -0.36621373891830444
Batch 29/64 loss: -0.3004128932952881
Batch 30/64 loss: -0.3795354962348938
Batch 31/64 loss: -0.35123881697654724
Batch 32/64 loss: -0.3488513231277466
Batch 33/64 loss: -0.3788341283798218
Batch 34/64 loss: -0.36607369780540466
Batch 35/64 loss: -0.3418222665786743
Batch 36/64 loss: -0.3416094481945038
Batch 37/64 loss: -0.3489266335964203
Batch 38/64 loss: -0.32939234375953674
Batch 39/64 loss: -0.36240455508232117
Batch 40/64 loss: -0.3424561619758606
Batch 41/64 loss: -0.3505478799343109
Batch 42/64 loss: -0.3725844621658325
Batch 43/64 loss: -0.3530280590057373
Batch 44/64 loss: -0.32607340812683105
Batch 45/64 loss: -0.3307260274887085
Batch 46/64 loss: -0.34169623255729675
Batch 47/64 loss: -0.34343016147613525
Batch 48/64 loss: -0.33834874629974365
Batch 49/64 loss: -0.35319504141807556
Batch 50/64 loss: -0.3738880157470703
Batch 51/64 loss: -0.3470776081085205
Batch 52/64 loss: -0.3582528233528137
Batch 53/64 loss: -0.35594892501831055
Batch 54/64 loss: -0.37085390090942383
Batch 55/64 loss: -0.3589667081832886
Batch 56/64 loss: -0.31665757298469543
Batch 57/64 loss: -0.37038353085517883
Batch 58/64 loss: -0.3382754921913147
Batch 59/64 loss: -0.33277347683906555
Batch 60/64 loss: -0.35839831829071045
Batch 61/64 loss: -0.35145270824432373
Batch 62/64 loss: -0.30429723858833313
Batch 63/64 loss: -0.34298866987228394
Batch 64/64 loss: -0.2909926772117615
Epoch 395  Train loss: -0.3466845706397412  Val loss: 0.03286819359690873
Epoch 396
-------------------------------
Batch 1/64 loss: -0.40021663904190063
Batch 2/64 loss: -0.3564487099647522
Batch 3/64 loss: -0.3364521861076355
Batch 4/64 loss: -0.30897873640060425
Batch 5/64 loss: -0.3532364070415497
Batch 6/64 loss: -0.36442506313323975
Batch 7/64 loss: -0.34531310200691223
Batch 8/64 loss: -0.32939404249191284
Batch 9/64 loss: -0.3511226177215576
Batch 10/64 loss: -0.29675108194351196
Batch 11/64 loss: -0.3901403844356537
Batch 12/64 loss: -0.35819393396377563
Batch 13/64 loss: -0.3608436584472656
Batch 14/64 loss: -0.35960298776626587
Batch 15/64 loss: -0.35181596875190735
Batch 16/64 loss: -0.36151787638664246
Batch 17/64 loss: -0.34005892276763916
Batch 18/64 loss: -0.36787745356559753
Batch 19/64 loss: -0.377669095993042
Batch 20/64 loss: -0.35739535093307495
Batch 21/64 loss: -0.3376990854740143
Batch 22/64 loss: -0.33611488342285156
Batch 23/64 loss: -0.35499221086502075
Batch 24/64 loss: -0.33705589175224304
Batch 25/64 loss: -0.3687443733215332
Batch 26/64 loss: -0.3711589276790619
Batch 27/64 loss: -0.3547101318836212
Batch 28/64 loss: -0.3537292182445526
Batch 29/64 loss: -0.35527485609054565
Batch 30/64 loss: -0.3276449143886566
Batch 31/64 loss: -0.36913424730300903
Batch 32/64 loss: -0.293041855096817
Batch 33/64 loss: -0.38399192690849304
Batch 34/64 loss: -0.3617400527000427
Batch 35/64 loss: -0.3284931480884552
Batch 36/64 loss: -0.35121530294418335
Batch 37/64 loss: -0.33976197242736816
Batch 38/64 loss: -0.32483774423599243
Batch 39/64 loss: -0.3289659917354584
Batch 40/64 loss: -0.371809720993042
Batch 41/64 loss: -0.3526499271392822
Batch 42/64 loss: -0.3680840730667114
Batch 43/64 loss: -0.3531242609024048
Batch 44/64 loss: -0.3400408625602722
Batch 45/64 loss: -0.3624648153781891
Batch 46/64 loss: -0.2617679238319397
Batch 47/64 loss: -0.3482857048511505
Batch 48/64 loss: -0.3181515336036682
Batch 49/64 loss: -0.38617846369743347
Batch 50/64 loss: -0.34867334365844727
Batch 51/64 loss: -0.3476281762123108
Batch 52/64 loss: -0.3521379828453064
Batch 53/64 loss: -0.3289201259613037
Batch 54/64 loss: -0.3522900938987732
Batch 55/64 loss: -0.36145448684692383
Batch 56/64 loss: -0.3342495560646057
Batch 57/64 loss: -0.320758581161499
Batch 58/64 loss: -0.3235045075416565
Batch 59/64 loss: -0.3571352958679199
Batch 60/64 loss: -0.3743182420730591
Batch 61/64 loss: -0.37054887413978577
Batch 62/64 loss: -0.3613567352294922
Batch 63/64 loss: -0.33054620027542114
Batch 64/64 loss: -0.30282947421073914
Epoch 396  Train loss: -0.3482190203432943  Val loss: 0.032146277296584085
Epoch 397
-------------------------------
Batch 1/64 loss: -0.3903236389160156
Batch 2/64 loss: -0.32899993658065796
Batch 3/64 loss: -0.35939255356788635
Batch 4/64 loss: -0.35734856128692627
Batch 5/64 loss: -0.34607499837875366
Batch 6/64 loss: -0.3494189977645874
Batch 7/64 loss: -0.31324848532676697
Batch 8/64 loss: -0.358925461769104
Batch 9/64 loss: -0.341977059841156
Batch 10/64 loss: -0.3585694432258606
Batch 11/64 loss: -0.3297479748725891
Batch 12/64 loss: -0.398028165102005
Batch 13/64 loss: -0.3731750249862671
Batch 14/64 loss: -0.36387962102890015
Batch 15/64 loss: -0.32230693101882935
Batch 16/64 loss: -0.3695438802242279
Batch 17/64 loss: -0.37592506408691406
Batch 18/64 loss: -0.37301790714263916
Batch 19/64 loss: -0.3498198390007019
Batch 20/64 loss: -0.3759826123714447
Batch 21/64 loss: -0.3626560568809509
Batch 22/64 loss: -0.3727802038192749
Batch 23/64 loss: -0.3535839915275574
Batch 24/64 loss: -0.32593661546707153
Batch 25/64 loss: -0.3176997900009155
Batch 26/64 loss: -0.3640477657318115
Batch 27/64 loss: -0.32149097323417664
Batch 28/64 loss: -0.3589736223220825
Batch 29/64 loss: -0.3343193531036377
Batch 30/64 loss: -0.33100563287734985
Batch 31/64 loss: -0.3558056354522705
Batch 32/64 loss: -0.34999948740005493
Batch 33/64 loss: -0.29909753799438477
Batch 34/64 loss: -0.38116219639778137
Batch 35/64 loss: -0.34618067741394043
Batch 36/64 loss: -0.37718665599823
Batch 37/64 loss: -0.38134634494781494
Batch 38/64 loss: -0.356076717376709
Batch 39/64 loss: -0.3094068467617035
Batch 40/64 loss: -0.33031174540519714
Batch 41/64 loss: -0.32547953724861145
Batch 42/64 loss: -0.3744950294494629
Batch 43/64 loss: -0.358467161655426
Batch 44/64 loss: -0.336906373500824
Batch 45/64 loss: -0.3482673764228821
Batch 46/64 loss: -0.3671504259109497
Batch 47/64 loss: -0.35253527760505676
Batch 48/64 loss: -0.38222604990005493
Batch 49/64 loss: -0.3677023649215698
Batch 50/64 loss: -0.366904616355896
Batch 51/64 loss: -0.34053510427474976
Batch 52/64 loss: -0.3296142518520355
Batch 53/64 loss: -0.34598273038864136
Batch 54/64 loss: -0.36019963026046753
Batch 55/64 loss: -0.3824981451034546
Batch 56/64 loss: -0.3525468111038208
Batch 57/64 loss: -0.38599956035614014
Batch 58/64 loss: -0.38014888763427734
Batch 59/64 loss: -0.3795497417449951
Batch 60/64 loss: -0.31374049186706543
Batch 61/64 loss: -0.3309778571128845
Batch 62/64 loss: -0.3401889204978943
Batch 63/64 loss: -0.35127580165863037
Batch 64/64 loss: -0.32338255643844604
Epoch 397  Train loss: -0.35263847954132976  Val loss: 0.030938894068662244
Epoch 398
-------------------------------
Batch 1/64 loss: -0.3409728705883026
Batch 2/64 loss: -0.2886431813240051
Batch 3/64 loss: -0.3707934319972992
Batch 4/64 loss: -0.3628588318824768
Batch 5/64 loss: -0.37721604108810425
Batch 6/64 loss: -0.37659454345703125
Batch 7/64 loss: -0.3365734815597534
Batch 8/64 loss: -0.32025036215782166
Batch 9/64 loss: -0.3757888078689575
Batch 10/64 loss: -0.35549694299697876
Batch 11/64 loss: -0.3627781867980957
Batch 12/64 loss: -0.37347906827926636
Batch 13/64 loss: -0.35989195108413696
Batch 14/64 loss: -0.3679664731025696
Batch 15/64 loss: -0.35564571619033813
Batch 16/64 loss: -0.3807124197483063
Batch 17/64 loss: -0.3642171025276184
Batch 18/64 loss: -0.34184974431991577
Batch 19/64 loss: -0.3838573694229126
Batch 20/64 loss: -0.3745420277118683
Batch 21/64 loss: -0.35106801986694336
Batch 22/64 loss: -0.34329602122306824
Batch 23/64 loss: -0.3512917757034302
Batch 24/64 loss: -0.27609747648239136
Batch 25/64 loss: -0.3370658755302429
Batch 26/64 loss: -0.35270002484321594
Batch 27/64 loss: -0.36802709102630615
Batch 28/64 loss: -0.35793858766555786
Batch 29/64 loss: -0.35419362783432007
Batch 30/64 loss: -0.33342522382736206
Batch 31/64 loss: -0.3703017830848694
Batch 32/64 loss: -0.33878111839294434
Batch 33/64 loss: -0.3815402388572693
Batch 34/64 loss: -0.37467384338378906
Batch 35/64 loss: -0.30970698595046997
Batch 36/64 loss: -0.3383095860481262
Batch 37/64 loss: -0.36731481552124023
Batch 38/64 loss: -0.3130941390991211
Batch 39/64 loss: -0.35019993782043457
Batch 40/64 loss: -0.3642292618751526
Batch 41/64 loss: -0.3452373743057251
Batch 42/64 loss: -0.3668925166130066
Batch 43/64 loss: -0.3442733883857727
Batch 44/64 loss: -0.35936295986175537
Batch 45/64 loss: -0.3347846269607544
Batch 46/64 loss: -0.3388572335243225
Batch 47/64 loss: -0.3442516028881073
Batch 48/64 loss: -0.33894672989845276
Batch 49/64 loss: -0.39424771070480347
Batch 50/64 loss: -0.3431481719017029
Batch 51/64 loss: -0.3671313226222992
Batch 52/64 loss: -0.36260756850242615
Batch 53/64 loss: -0.36753106117248535
Batch 54/64 loss: -0.36384671926498413
Batch 55/64 loss: -0.35347455739974976
Batch 56/64 loss: -0.3494940996170044
Batch 57/64 loss: -0.37398844957351685
Batch 58/64 loss: -0.3275277614593506
Batch 59/64 loss: -0.34760791063308716
Batch 60/64 loss: -0.3234342634677887
Batch 61/64 loss: -0.3371798098087311
Batch 62/64 loss: -0.35573774576187134
Batch 63/64 loss: -0.3734888434410095
Batch 64/64 loss: -0.2680099606513977
Epoch 398  Train loss: -0.3516461786101846  Val loss: 0.03244386505834835
Epoch 399
-------------------------------
Batch 1/64 loss: -0.35710036754608154
Batch 2/64 loss: -0.35788315534591675
Batch 3/64 loss: -0.34276139736175537
Batch 4/64 loss: -0.33311301469802856
Batch 5/64 loss: -0.33615821599960327
Batch 6/64 loss: -0.363902747631073
Batch 7/64 loss: -0.35711464285850525
Batch 8/64 loss: -0.34321141242980957
Batch 9/64 loss: -0.35272443294525146
Batch 10/64 loss: -0.35044777393341064
Batch 11/64 loss: -0.36785271763801575
Batch 12/64 loss: -0.375662624835968
Batch 13/64 loss: -0.3591287434101105
Batch 14/64 loss: -0.3187869191169739
Batch 15/64 loss: -0.35704246163368225
Batch 16/64 loss: -0.37847983837127686
Batch 17/64 loss: -0.33001428842544556
Batch 18/64 loss: -0.3536215126514435
Batch 19/64 loss: -0.35886818170547485
Batch 20/64 loss: -0.37393736839294434
Batch 21/64 loss: -0.3582046627998352
Batch 22/64 loss: -0.3517420291900635
Batch 23/64 loss: -0.34410977363586426
Batch 24/64 loss: -0.37869834899902344
Batch 25/64 loss: -0.35927897691726685
Batch 26/64 loss: -0.3278089761734009
Batch 27/64 loss: -0.34782832860946655
Batch 28/64 loss: -0.3520466387271881
Batch 29/64 loss: -0.3526495099067688
Batch 30/64 loss: -0.305500328540802
Batch 31/64 loss: -0.3515710234642029
Batch 32/64 loss: -0.37244880199432373
Batch 33/64 loss: -0.36456775665283203
Batch 34/64 loss: -0.3645320534706116
Batch 35/64 loss: -0.3703917860984802
Batch 36/64 loss: -0.381039023399353
Batch 37/64 loss: -0.34261059761047363
Batch 38/64 loss: -0.2865624725818634
Batch 39/64 loss: -0.34266671538352966
Batch 40/64 loss: -0.33569416403770447
Batch 41/64 loss: -0.3101344108581543
Batch 42/64 loss: -0.34703329205513
Batch 43/64 loss: -0.3179737329483032
Batch 44/64 loss: -0.3592931628227234
Batch 45/64 loss: -0.372761994600296
Batch 46/64 loss: -0.3496497869491577
Batch 47/64 loss: -0.38479092717170715
Batch 48/64 loss: -0.3486972153186798
Batch 49/64 loss: -0.3409978151321411
Batch 50/64 loss: -0.3434571623802185
Batch 51/64 loss: -0.374441534280777
Batch 52/64 loss: -0.3420032560825348
Batch 53/64 loss: -0.3634643852710724
Batch 54/64 loss: -0.33325690031051636
Batch 55/64 loss: -0.3839453160762787
Batch 56/64 loss: -0.33842840790748596
Batch 57/64 loss: -0.3469688296318054
Batch 58/64 loss: -0.3617895841598511
Batch 59/64 loss: -0.35175061225891113
Batch 60/64 loss: -0.37747567892074585
Batch 61/64 loss: -0.3247339725494385
Batch 62/64 loss: -0.35234254598617554
Batch 63/64 loss: -0.32402482628822327
Batch 64/64 loss: -0.3489552438259125
Epoch 399  Train loss: -0.35088463636005623  Val loss: 0.034534174142424594
Epoch 400
-------------------------------
Batch 1/64 loss: -0.37536290287971497
Batch 2/64 loss: -0.3594696521759033
Batch 3/64 loss: -0.27938222885131836
Batch 4/64 loss: -0.335039347410202
Batch 5/64 loss: -0.37371954321861267
Batch 6/64 loss: -0.37444883584976196
Batch 7/64 loss: -0.34837400913238525
Batch 8/64 loss: -0.36270415782928467
Batch 9/64 loss: -0.3518379330635071
Batch 10/64 loss: -0.31305184960365295
Batch 11/64 loss: -0.3645239472389221
Batch 12/64 loss: -0.3607128858566284
Batch 13/64 loss: -0.3229716420173645
Batch 14/64 loss: -0.373708575963974
Batch 15/64 loss: -0.36676231026649475
Batch 16/64 loss: -0.3442589342594147
Batch 17/64 loss: -0.3581134080886841
Batch 18/64 loss: -0.3167114853858948
Batch 19/64 loss: -0.33037644624710083
Batch 20/64 loss: -0.3376154601573944
Batch 21/64 loss: -0.375429630279541
Batch 22/64 loss: -0.3740265965461731
Batch 23/64 loss: -0.37231650948524475
Batch 24/64 loss: -0.3495601713657379
Batch 25/64 loss: -0.33853429555892944
Batch 26/64 loss: -0.32234519720077515
Batch 27/64 loss: -0.35624080896377563
Batch 28/64 loss: -0.35918790102005005
Batch 29/64 loss: -0.3559035062789917
Batch 30/64 loss: -0.3593069016933441
Batch 31/64 loss: -0.3665509521961212
Batch 32/64 loss: -0.36743175983428955
Batch 33/64 loss: -0.38327857851982117
Batch 34/64 loss: -0.32245033979415894
Batch 35/64 loss: -0.3531510531902313
Batch 36/64 loss: -0.3464006185531616
Batch 37/64 loss: -0.3680174946784973
Batch 38/64 loss: -0.3367600440979004
Batch 39/64 loss: -0.3187987804412842
Batch 40/64 loss: -0.34915807843208313
Batch 41/64 loss: -0.35737013816833496
Batch 42/64 loss: -0.36687442660331726
Batch 43/64 loss: -0.36051785945892334
Batch 44/64 loss: -0.3396686315536499
Batch 45/64 loss: -0.3076625466346741
Batch 46/64 loss: -0.3542606234550476
Batch 47/64 loss: -0.33892664313316345
Batch 48/64 loss: -0.373500794172287
Batch 49/64 loss: -0.3582156300544739
Batch 50/64 loss: -0.27750182151794434
Batch 51/64 loss: -0.30430710315704346
Batch 52/64 loss: -0.32157963514328003
Batch 53/64 loss: -0.34252220392227173
Batch 54/64 loss: -0.3645859360694885
Batch 55/64 loss: -0.3327111303806305
Batch 56/64 loss: -0.3246837854385376
Batch 57/64 loss: -0.34791699051856995
Batch 58/64 loss: -0.3205997347831726
Batch 59/64 loss: -0.38099634647369385
Batch 60/64 loss: -0.3684934377670288
Batch 61/64 loss: -0.31354618072509766
Batch 62/64 loss: -0.37353092432022095
Batch 63/64 loss: -0.3508601486682892
Batch 64/64 loss: -0.3504343628883362
Epoch 400  Train loss: -0.3477278936143015  Val loss: 0.033246333451615165
Epoch 401
-------------------------------
Batch 1/64 loss: -0.3298301100730896
Batch 2/64 loss: -0.31812357902526855
Batch 3/64 loss: -0.3457624316215515
Batch 4/64 loss: -0.3963523507118225
Batch 5/64 loss: -0.3764866888523102
Batch 6/64 loss: -0.3490375280380249
Batch 7/64 loss: -0.3502017557621002
Batch 8/64 loss: -0.3070116639137268
Batch 9/64 loss: -0.37316516041755676
Batch 10/64 loss: -0.33103853464126587
Batch 11/64 loss: -0.3358740210533142
Batch 12/64 loss: -0.3855750560760498
Batch 13/64 loss: -0.3733499050140381
Batch 14/64 loss: -0.3345046937465668
Batch 15/64 loss: -0.35399702191352844
Batch 16/64 loss: -0.355855792760849
Batch 17/64 loss: -0.3778851628303528
Batch 18/64 loss: -0.3097461462020874
Batch 19/64 loss: -0.36424052715301514
Batch 20/64 loss: -0.3330855965614319
Batch 21/64 loss: -0.3512597680091858
Batch 22/64 loss: -0.36436665058135986
Batch 23/64 loss: -0.35133641958236694
Batch 24/64 loss: -0.34515947103500366
Batch 25/64 loss: -0.38184571266174316
Batch 26/64 loss: -0.36625856161117554
Batch 27/64 loss: -0.367496132850647
Batch 28/64 loss: -0.34422820806503296
Batch 29/64 loss: -0.3867620527744293
Batch 30/64 loss: -0.35437047481536865
Batch 31/64 loss: -0.3834710717201233
Batch 32/64 loss: -0.34430286288261414
Batch 33/64 loss: -0.3798918128013611
Batch 34/64 loss: -0.3670269846916199
Batch 35/64 loss: -0.35097575187683105
Batch 36/64 loss: -0.3443801999092102
Batch 37/64 loss: -0.3573170304298401
Batch 38/64 loss: -0.3680917024612427
Batch 39/64 loss: -0.34672003984451294
Batch 40/64 loss: -0.3780153691768646
Batch 41/64 loss: -0.3617764115333557
Batch 42/64 loss: -0.35888317227363586
Batch 43/64 loss: -0.3519279956817627
Batch 44/64 loss: -0.31231462955474854
Batch 45/64 loss: -0.3237490653991699
Batch 46/64 loss: -0.3504038155078888
Batch 47/64 loss: -0.33964964747428894
Batch 48/64 loss: -0.33077940344810486
Batch 49/64 loss: -0.34749069809913635
Batch 50/64 loss: -0.3651888370513916
Batch 51/64 loss: -0.35306769609451294
Batch 52/64 loss: -0.383273720741272
Batch 53/64 loss: -0.34557801485061646
Batch 54/64 loss: -0.3036014437675476
Batch 55/64 loss: -0.3535241186618805
Batch 56/64 loss: -0.3384513854980469
Batch 57/64 loss: -0.33665746450424194
Batch 58/64 loss: -0.36067497730255127
Batch 59/64 loss: -0.29894810914993286
Batch 60/64 loss: -0.3352799713611603
Batch 61/64 loss: -0.3587264120578766
Batch 62/64 loss: -0.3435368239879608
Batch 63/64 loss: -0.3350725769996643
Batch 64/64 loss: -0.297161340713501
Epoch 401  Train loss: -0.35052278275583304  Val loss: 0.03322887953204388
Epoch 402
-------------------------------
Batch 1/64 loss: -0.35338470339775085
Batch 2/64 loss: -0.3778015673160553
Batch 3/64 loss: -0.35074302554130554
Batch 4/64 loss: -0.3717251718044281
Batch 5/64 loss: -0.3365916609764099
Batch 6/64 loss: -0.369976669549942
Batch 7/64 loss: -0.3510536551475525
Batch 8/64 loss: -0.2841886878013611
Batch 9/64 loss: -0.32098889350891113
Batch 10/64 loss: -0.3539617359638214
Batch 11/64 loss: -0.39032796025276184
Batch 12/64 loss: -0.3241313099861145
Batch 13/64 loss: -0.3318207859992981
Batch 14/64 loss: -0.34540268778800964
Batch 15/64 loss: -0.3839360475540161
Batch 16/64 loss: -0.30386269092559814
Batch 17/64 loss: -0.2975296676158905
Batch 18/64 loss: -0.35574841499328613
Batch 19/64 loss: -0.34471389651298523
Batch 20/64 loss: -0.35074132680892944
Batch 21/64 loss: -0.3609123229980469
Batch 22/64 loss: -0.31924378871917725
Batch 23/64 loss: -0.36194467544555664
Batch 24/64 loss: -0.36109739542007446
Batch 25/64 loss: -0.3747718334197998
Batch 26/64 loss: -0.3561020493507385
Batch 27/64 loss: -0.3522338271141052
Batch 28/64 loss: -0.35327044129371643
Batch 29/64 loss: -0.30472707748413086
Batch 30/64 loss: -0.402756929397583
Batch 31/64 loss: -0.3842545747756958
Batch 32/64 loss: -0.3419382572174072
Batch 33/64 loss: -0.31540197134017944
Batch 34/64 loss: -0.34028059244155884
Batch 35/64 loss: -0.3570594787597656
Batch 36/64 loss: -0.3477424383163452
Batch 37/64 loss: -0.3449057340621948
Batch 38/64 loss: -0.35032016038894653
Batch 39/64 loss: -0.38324588537216187
Batch 40/64 loss: -0.3476191759109497
Batch 41/64 loss: -0.34508487582206726
Batch 42/64 loss: -0.31999292969703674
Batch 43/64 loss: -0.3874882459640503
Batch 44/64 loss: -0.3538391590118408
Batch 45/64 loss: -0.3155180811882019
Batch 46/64 loss: -0.33791375160217285
Batch 47/64 loss: -0.32499533891677856
Batch 48/64 loss: -0.34334465861320496
Batch 49/64 loss: -0.37283778190612793
Batch 50/64 loss: -0.3449283540248871
Batch 51/64 loss: -0.3077067732810974
Batch 52/64 loss: -0.3548167943954468
Batch 53/64 loss: -0.37544935941696167
Batch 54/64 loss: -0.3656587302684784
Batch 55/64 loss: -0.3508211672306061
Batch 56/64 loss: -0.3550418019294739
Batch 57/64 loss: -0.3479289412498474
Batch 58/64 loss: -0.35125333070755005
Batch 59/64 loss: -0.3448508381843567
Batch 60/64 loss: -0.37189799547195435
Batch 61/64 loss: -0.3826775550842285
Batch 62/64 loss: -0.36835193634033203
Batch 63/64 loss: -0.3910026550292969
Batch 64/64 loss: -0.286374032497406
Epoch 402  Train loss: -0.34959436468049593  Val loss: 0.03337212649407666
Epoch 403
-------------------------------
Batch 1/64 loss: -0.37178295850753784
Batch 2/64 loss: -0.3692794740200043
Batch 3/64 loss: -0.3718280494213104
Batch 4/64 loss: -0.36744189262390137
Batch 5/64 loss: -0.37502914667129517
Batch 6/64 loss: -0.3608337640762329
Batch 7/64 loss: -0.3717578947544098
Batch 8/64 loss: -0.35768282413482666
Batch 9/64 loss: -0.39476341009140015
Batch 10/64 loss: -0.35746899247169495
Batch 11/64 loss: -0.3451765775680542
Batch 12/64 loss: -0.37762928009033203
Batch 13/64 loss: -0.37958410382270813
Batch 14/64 loss: -0.37673330307006836
Batch 15/64 loss: -0.3869072496891022
Batch 16/64 loss: -0.3421522378921509
Batch 17/64 loss: -0.34931498765945435
Batch 18/64 loss: -0.36703944206237793
Batch 19/64 loss: -0.35606154799461365
Batch 20/64 loss: -0.3443877696990967
Batch 21/64 loss: -0.3247683048248291
Batch 22/64 loss: -0.36228474974632263
Batch 23/64 loss: -0.31682729721069336
Batch 24/64 loss: -0.3475271165370941
Batch 25/64 loss: -0.3659723401069641
Batch 26/64 loss: -0.3564199209213257
Batch 27/64 loss: -0.3255789279937744
Batch 28/64 loss: -0.2858125567436218
Batch 29/64 loss: -0.34473055601119995
Batch 30/64 loss: -0.30138155817985535
Batch 31/64 loss: -0.35062938928604126
Batch 32/64 loss: -0.35595396161079407
Batch 33/64 loss: -0.33959752321243286
Batch 34/64 loss: -0.37304389476776123
Batch 35/64 loss: -0.3441537618637085
Batch 36/64 loss: -0.29822927713394165
Batch 37/64 loss: -0.36575376987457275
Batch 38/64 loss: -0.3470555543899536
Batch 39/64 loss: -0.34663257002830505
Batch 40/64 loss: -0.33985915780067444
Batch 41/64 loss: -0.3679158091545105
Batch 42/64 loss: -0.32309383153915405
Batch 43/64 loss: -0.35055577754974365
Batch 44/64 loss: -0.3599858283996582
Batch 45/64 loss: -0.35386258363723755
Batch 46/64 loss: -0.31343188881874084
Batch 47/64 loss: -0.3547077775001526
Batch 48/64 loss: -0.2905890941619873
Batch 49/64 loss: -0.3103867173194885
Batch 50/64 loss: -0.35526522994041443
Batch 51/64 loss: -0.35137832164764404
Batch 52/64 loss: -0.3699684143066406
Batch 53/64 loss: -0.3427143692970276
Batch 54/64 loss: -0.3620302379131317
Batch 55/64 loss: -0.31925055384635925
Batch 56/64 loss: -0.35513997077941895
Batch 57/64 loss: -0.3500182032585144
Batch 58/64 loss: -0.3386433720588684
Batch 59/64 loss: -0.33970510959625244
Batch 60/64 loss: -0.3777305483818054
Batch 61/64 loss: -0.3558546304702759
Batch 62/64 loss: -0.33170002698898315
Batch 63/64 loss: -0.3714420795440674
Batch 64/64 loss: -0.3655405044555664
Epoch 403  Train loss: -0.3503465544943716  Val loss: 0.03291655692857565
Epoch 404
-------------------------------
Batch 1/64 loss: -0.33177101612091064
Batch 2/64 loss: -0.34361428022384644
Batch 3/64 loss: -0.35652655363082886
Batch 4/64 loss: -0.2787221074104309
Batch 5/64 loss: -0.3621059060096741
Batch 6/64 loss: -0.3648742139339447
Batch 7/64 loss: -0.36150509119033813
Batch 8/64 loss: -0.39716315269470215
Batch 9/64 loss: -0.3521430790424347
Batch 10/64 loss: -0.33930039405822754
Batch 11/64 loss: -0.3238222599029541
Batch 12/64 loss: -0.35631614923477173
Batch 13/64 loss: -0.34449297189712524
Batch 14/64 loss: -0.30550333857536316
Batch 15/64 loss: -0.33454495668411255
Batch 16/64 loss: -0.34381428360939026
Batch 17/64 loss: -0.36899077892303467
Batch 18/64 loss: -0.3577270209789276
Batch 19/64 loss: -0.3606835603713989
Batch 20/64 loss: -0.3569971024990082
Batch 21/64 loss: -0.338756263256073
Batch 22/64 loss: -0.3520635962486267
Batch 23/64 loss: -0.33204466104507446
Batch 24/64 loss: -0.35255974531173706
Batch 25/64 loss: -0.3688490092754364
Batch 26/64 loss: -0.3411393165588379
Batch 27/64 loss: -0.3533075153827667
Batch 28/64 loss: -0.3654985725879669
Batch 29/64 loss: -0.33167386054992676
Batch 30/64 loss: -0.3619331121444702
Batch 31/64 loss: -0.35575538873672485
Batch 32/64 loss: -0.3379049301147461
Batch 33/64 loss: -0.35996371507644653
Batch 34/64 loss: -0.3522809147834778
Batch 35/64 loss: -0.3216009736061096
Batch 36/64 loss: -0.346041738986969
Batch 37/64 loss: -0.3691467344760895
Batch 38/64 loss: -0.3770378828048706
Batch 39/64 loss: -0.3472161591053009
Batch 40/64 loss: -0.3867010474205017
Batch 41/64 loss: -0.33684927225112915
Batch 42/64 loss: -0.3834248483181
Batch 43/64 loss: -0.3465830087661743
Batch 44/64 loss: -0.3408266305923462
Batch 45/64 loss: -0.3618265390396118
Batch 46/64 loss: -0.33216315507888794
Batch 47/64 loss: -0.36112505197525024
Batch 48/64 loss: -0.36032742261886597
Batch 49/64 loss: -0.37270185351371765
Batch 50/64 loss: -0.3653995990753174
Batch 51/64 loss: -0.35804086923599243
Batch 52/64 loss: -0.28790557384490967
Batch 53/64 loss: -0.3201931118965149
Batch 54/64 loss: -0.33683210611343384
Batch 55/64 loss: -0.3862958252429962
Batch 56/64 loss: -0.3476439118385315
Batch 57/64 loss: -0.34722310304641724
Batch 58/64 loss: -0.3806304931640625
Batch 59/64 loss: -0.350935161113739
Batch 60/64 loss: -0.3640478849411011
Batch 61/64 loss: -0.37059861421585083
Batch 62/64 loss: -0.36851686239242554
Batch 63/64 loss: -0.34173569083213806
Batch 64/64 loss: -0.3135465681552887
Epoch 404  Train loss: -0.35057380187745185  Val loss: 0.03208005039142989
Epoch 405
-------------------------------
Batch 1/64 loss: -0.371202677488327
Batch 2/64 loss: -0.3855172395706177
Batch 3/64 loss: -0.3465418219566345
Batch 4/64 loss: -0.35218387842178345
Batch 5/64 loss: -0.36464422941207886
Batch 6/64 loss: -0.375247061252594
Batch 7/64 loss: -0.3198809027671814
Batch 8/64 loss: -0.36052441596984863
Batch 9/64 loss: -0.372181236743927
Batch 10/64 loss: -0.34529829025268555
Batch 11/64 loss: -0.38367772102355957
Batch 12/64 loss: -0.347380667924881
Batch 13/64 loss: -0.34351229667663574
Batch 14/64 loss: -0.34186047315597534
Batch 15/64 loss: -0.3465306758880615
Batch 16/64 loss: -0.3602226972579956
Batch 17/64 loss: -0.3437151312828064
Batch 18/64 loss: -0.39134761691093445
Batch 19/64 loss: -0.34461843967437744
Batch 20/64 loss: -0.36542952060699463
Batch 21/64 loss: -0.3680475354194641
Batch 22/64 loss: -0.3425891101360321
Batch 23/64 loss: -0.3064594864845276
Batch 24/64 loss: -0.3261544108390808
Batch 25/64 loss: -0.38209277391433716
Batch 26/64 loss: -0.3451632857322693
Batch 27/64 loss: -0.35648173093795776
Batch 28/64 loss: -0.38717859983444214
Batch 29/64 loss: -0.36503148078918457
Batch 30/64 loss: -0.33837249875068665
Batch 31/64 loss: -0.36544913053512573
Batch 32/64 loss: -0.31518256664276123
Batch 33/64 loss: -0.3322592079639435
Batch 34/64 loss: -0.3418577313423157
Batch 35/64 loss: -0.366665780544281
Batch 36/64 loss: -0.331138551235199
Batch 37/64 loss: -0.3698033094406128
Batch 38/64 loss: -0.37617579102516174
Batch 39/64 loss: -0.3248963952064514
Batch 40/64 loss: -0.3580068349838257
Batch 41/64 loss: -0.37480172514915466
Batch 42/64 loss: -0.35056817531585693
Batch 43/64 loss: -0.3448505401611328
Batch 44/64 loss: -0.3527034521102905
Batch 45/64 loss: -0.35200992226600647
Batch 46/64 loss: -0.36706215143203735
Batch 47/64 loss: -0.36680617928504944
Batch 48/64 loss: -0.3592822551727295
Batch 49/64 loss: -0.3172813653945923
Batch 50/64 loss: -0.3096392750740051
Batch 51/64 loss: -0.36689746379852295
Batch 52/64 loss: -0.3510103225708008
Batch 53/64 loss: -0.32862991094589233
Batch 54/64 loss: -0.37572309374809265
Batch 55/64 loss: -0.3379487991333008
Batch 56/64 loss: -0.37597793340682983
Batch 57/64 loss: -0.3558572232723236
Batch 58/64 loss: -0.3545878529548645
Batch 59/64 loss: -0.3730414807796478
Batch 60/64 loss: -0.3767494261264801
Batch 61/64 loss: -0.32743874192237854
Batch 62/64 loss: -0.3136710822582245
Batch 63/64 loss: -0.33716022968292236
Batch 64/64 loss: -0.3305150270462036
Epoch 405  Train loss: -0.3525977737763349  Val loss: 0.032847245124607155
Epoch 406
-------------------------------
Batch 1/64 loss: -0.37931448221206665
Batch 2/64 loss: -0.34581971168518066
Batch 3/64 loss: -0.3499763011932373
Batch 4/64 loss: -0.3370763063430786
Batch 5/64 loss: -0.3186987638473511
Batch 6/64 loss: -0.36327439546585083
Batch 7/64 loss: -0.3648705780506134
Batch 8/64 loss: -0.3762650489807129
Batch 9/64 loss: -0.36584481596946716
Batch 10/64 loss: -0.3259255886077881
Batch 11/64 loss: -0.36414724588394165
Batch 12/64 loss: -0.377719908952713
Batch 13/64 loss: -0.3934231102466583
Batch 14/64 loss: -0.35264870524406433
Batch 15/64 loss: -0.3511974811553955
Batch 16/64 loss: -0.2884864807128906
Batch 17/64 loss: -0.3275144696235657
Batch 18/64 loss: -0.3262479305267334
Batch 19/64 loss: -0.3715499937534332
Batch 20/64 loss: -0.3721323013305664
Batch 21/64 loss: -0.35569894313812256
Batch 22/64 loss: -0.35710594058036804
Batch 23/64 loss: -0.38091203570365906
Batch 24/64 loss: -0.3293950855731964
Batch 25/64 loss: -0.3181638717651367
Batch 26/64 loss: -0.3582407236099243
Batch 27/64 loss: -0.39110466837882996
Batch 28/64 loss: -0.3619486093521118
Batch 29/64 loss: -0.3683188557624817
Batch 30/64 loss: -0.3582417368888855
Batch 31/64 loss: -0.3828272223472595
Batch 32/64 loss: -0.30565711855888367
Batch 33/64 loss: -0.3664836883544922
Batch 34/64 loss: -0.3055917024612427
Batch 35/64 loss: -0.35326993465423584
Batch 36/64 loss: -0.37787479162216187
Batch 37/64 loss: -0.3702698349952698
Batch 38/64 loss: -0.36274272203445435
Batch 39/64 loss: -0.36664915084838867
Batch 40/64 loss: -0.3292692303657532
Batch 41/64 loss: -0.3683408498764038
Batch 42/64 loss: -0.35775163769721985
Batch 43/64 loss: -0.3677656650543213
Batch 44/64 loss: -0.3157651424407959
Batch 45/64 loss: -0.3648011088371277
Batch 46/64 loss: -0.33865198493003845
Batch 47/64 loss: -0.3102762997150421
Batch 48/64 loss: -0.352333128452301
Batch 49/64 loss: -0.34976938366889954
Batch 50/64 loss: -0.35584044456481934
Batch 51/64 loss: -0.3663826584815979
Batch 52/64 loss: -0.3419326841831207
Batch 53/64 loss: -0.3635217547416687
Batch 54/64 loss: -0.3406238555908203
Batch 55/64 loss: -0.3679535686969757
Batch 56/64 loss: -0.3538437485694885
Batch 57/64 loss: -0.358689546585083
Batch 58/64 loss: -0.348945677280426
Batch 59/64 loss: -0.3512241244316101
Batch 60/64 loss: -0.3312842845916748
Batch 61/64 loss: -0.36175286769866943
Batch 62/64 loss: -0.3297269642353058
Batch 63/64 loss: -0.3241851031780243
Batch 64/64 loss: -0.28648871183395386
Epoch 406  Train loss: -0.3511863294769736  Val loss: 0.03366844473835529
Epoch 407
-------------------------------
Batch 1/64 loss: -0.3424057960510254
Batch 2/64 loss: -0.3782990574836731
Batch 3/64 loss: -0.3578798174858093
Batch 4/64 loss: -0.3636307716369629
Batch 5/64 loss: -0.3255727291107178
Batch 6/64 loss: -0.35213547945022583
Batch 7/64 loss: -0.3365938365459442
Batch 8/64 loss: -0.35485774278640747
Batch 9/64 loss: -0.3167093098163605
Batch 10/64 loss: -0.3495005965232849
Batch 11/64 loss: -0.286337673664093
Batch 12/64 loss: -0.38379406929016113
Batch 13/64 loss: -0.36113232374191284
Batch 14/64 loss: -0.36579132080078125
Batch 15/64 loss: -0.3455904722213745
Batch 16/64 loss: -0.3531651496887207
Batch 17/64 loss: -0.34707164764404297
Batch 18/64 loss: -0.3565921187400818
Batch 19/64 loss: -0.37639132142066956
Batch 20/64 loss: -0.3522716760635376
Batch 21/64 loss: -0.3425014019012451
Batch 22/64 loss: -0.34734416007995605
Batch 23/64 loss: -0.3187962770462036
Batch 24/64 loss: -0.2959650158882141
Batch 25/64 loss: -0.33443406224250793
Batch 26/64 loss: -0.3800283968448639
Batch 27/64 loss: -0.34833192825317383
Batch 28/64 loss: -0.3448188602924347
Batch 29/64 loss: -0.34082916378974915
Batch 30/64 loss: -0.33776015043258667
Batch 31/64 loss: -0.3607127368450165
Batch 32/64 loss: -0.3510584831237793
Batch 33/64 loss: -0.38253316283226013
Batch 34/64 loss: -0.3004205822944641
Batch 35/64 loss: -0.3550688624382019
Batch 36/64 loss: -0.352070689201355
Batch 37/64 loss: -0.38517534732818604
Batch 38/64 loss: -0.3332429528236389
Batch 39/64 loss: -0.38099026679992676
Batch 40/64 loss: -0.3592994511127472
Batch 41/64 loss: -0.322258859872818
Batch 42/64 loss: -0.3751186728477478
Batch 43/64 loss: -0.31171831488609314
Batch 44/64 loss: -0.3155131936073303
Batch 45/64 loss: -0.36072254180908203
Batch 46/64 loss: -0.37913191318511963
Batch 47/64 loss: -0.3760467767715454
Batch 48/64 loss: -0.36632370948791504
Batch 49/64 loss: -0.3476194143295288
Batch 50/64 loss: -0.3553667366504669
Batch 51/64 loss: -0.3482493758201599
Batch 52/64 loss: -0.35506829619407654
Batch 53/64 loss: -0.3520161807537079
Batch 54/64 loss: -0.363275945186615
Batch 55/64 loss: -0.33148133754730225
Batch 56/64 loss: -0.33779293298721313
Batch 57/64 loss: -0.35734567046165466
Batch 58/64 loss: -0.34987393021583557
Batch 59/64 loss: -0.38012823462486267
Batch 60/64 loss: -0.3481917977333069
Batch 61/64 loss: -0.36963459849357605
Batch 62/64 loss: -0.3433820605278015
Batch 63/64 loss: -0.2843833565711975
Batch 64/64 loss: -0.32311004400253296
Epoch 407  Train loss: -0.3487071567890691  Val loss: 0.03406486826663984
Epoch 408
-------------------------------
Batch 1/64 loss: -0.35054856538772583
Batch 2/64 loss: -0.3110167682170868
Batch 3/64 loss: -0.3532930314540863
Batch 4/64 loss: -0.34668856859207153
Batch 5/64 loss: -0.35090845823287964
Batch 6/64 loss: -0.35776808857917786
Batch 7/64 loss: -0.35371994972229004
Batch 8/64 loss: -0.37630996108055115
Batch 9/64 loss: -0.3510533273220062
Batch 10/64 loss: -0.36890849471092224
Batch 11/64 loss: -0.33038681745529175
Batch 12/64 loss: -0.3524891138076782
Batch 13/64 loss: -0.3751612901687622
Batch 14/64 loss: -0.3651576042175293
Batch 15/64 loss: -0.3830713629722595
Batch 16/64 loss: -0.3677554428577423
Batch 17/64 loss: -0.3615247905254364
Batch 18/64 loss: -0.35547029972076416
Batch 19/64 loss: -0.3317243754863739
Batch 20/64 loss: -0.35513681173324585
Batch 21/64 loss: -0.3511170446872711
Batch 22/64 loss: -0.36468204855918884
Batch 23/64 loss: -0.35301265120506287
Batch 24/64 loss: -0.3322121202945709
Batch 25/64 loss: -0.337849885225296
Batch 26/64 loss: -0.30700621008872986
Batch 27/64 loss: -0.3618852496147156
Batch 28/64 loss: -0.3544081449508667
Batch 29/64 loss: -0.37491297721862793
Batch 30/64 loss: -0.298456609249115
Batch 31/64 loss: -0.35754668712615967
Batch 32/64 loss: -0.3428940773010254
Batch 33/64 loss: -0.371692419052124
Batch 34/64 loss: -0.37273913621902466
Batch 35/64 loss: -0.35255157947540283
Batch 36/64 loss: -0.35210517048835754
Batch 37/64 loss: -0.3422940969467163
Batch 38/64 loss: -0.36852848529815674
Batch 39/64 loss: -0.34903860092163086
Batch 40/64 loss: -0.35290682315826416
Batch 41/64 loss: -0.3702262341976166
Batch 42/64 loss: -0.3710556626319885
Batch 43/64 loss: -0.3251401484012604
Batch 44/64 loss: -0.35925549268722534
Batch 45/64 loss: -0.3093612790107727
Batch 46/64 loss: -0.31452324986457825
Batch 47/64 loss: -0.3233240842819214
Batch 48/64 loss: -0.32362091541290283
Batch 49/64 loss: -0.3654138147830963
Batch 50/64 loss: -0.3542584478855133
Batch 51/64 loss: -0.3556068539619446
Batch 52/64 loss: -0.34547144174575806
Batch 53/64 loss: -0.32376527786254883
Batch 54/64 loss: -0.3513558506965637
Batch 55/64 loss: -0.3216637372970581
Batch 56/64 loss: -0.3618658483028412
Batch 57/64 loss: -0.35690364241600037
Batch 58/64 loss: -0.37407761812210083
Batch 59/64 loss: -0.3731260895729065
Batch 60/64 loss: -0.3876602351665497
Batch 61/64 loss: -0.2644994854927063
Batch 62/64 loss: -0.3686978220939636
Batch 63/64 loss: -0.35010087490081787
Batch 64/64 loss: -0.3047075867652893
Epoch 408  Train loss: -0.34941863429312614  Val loss: 0.03478228062698521
Epoch 409
-------------------------------
Batch 1/64 loss: -0.3614983558654785
Batch 2/64 loss: -0.3828524947166443
Batch 3/64 loss: -0.37870708107948303
Batch 4/64 loss: -0.37946298718452454
Batch 5/64 loss: -0.37240517139434814
Batch 6/64 loss: -0.34894901514053345
Batch 7/64 loss: -0.38213542103767395
Batch 8/64 loss: -0.384747713804245
Batch 9/64 loss: -0.36535853147506714
Batch 10/64 loss: -0.3769526481628418
Batch 11/64 loss: -0.35456013679504395
Batch 12/64 loss: -0.35071972012519836
Batch 13/64 loss: -0.34139156341552734
Batch 14/64 loss: -0.3659960627555847
Batch 15/64 loss: -0.372616708278656
Batch 16/64 loss: -0.3378198742866516
Batch 17/64 loss: -0.4074605703353882
Batch 18/64 loss: -0.36818069219589233
Batch 19/64 loss: -0.37521034479141235
Batch 20/64 loss: -0.3625043034553528
Batch 21/64 loss: -0.32394203543663025
Batch 22/64 loss: -0.38171494007110596
Batch 23/64 loss: -0.3594438135623932
Batch 24/64 loss: -0.3420479893684387
Batch 25/64 loss: -0.3693884611129761
Batch 26/64 loss: -0.3607870042324066
Batch 27/64 loss: -0.3505321741104126
Batch 28/64 loss: -0.3014719486236572
Batch 29/64 loss: -0.3459400534629822
Batch 30/64 loss: -0.35300469398498535
Batch 31/64 loss: -0.3133309483528137
Batch 32/64 loss: -0.32077252864837646
Batch 33/64 loss: -0.369057297706604
Batch 34/64 loss: -0.29346346855163574
Batch 35/64 loss: -0.36302098631858826
Batch 36/64 loss: -0.352301687002182
Batch 37/64 loss: -0.34915581345558167
Batch 38/64 loss: -0.3607895076274872
Batch 39/64 loss: -0.36144545674324036
Batch 40/64 loss: -0.3498678505420685
Batch 41/64 loss: -0.3087611496448517
Batch 42/64 loss: -0.3821309208869934
Batch 43/64 loss: -0.3346807360649109
Batch 44/64 loss: -0.3269643187522888
Batch 45/64 loss: -0.37766575813293457
Batch 46/64 loss: -0.36748701333999634
Batch 47/64 loss: -0.3342313766479492
Batch 48/64 loss: -0.35194212198257446
Batch 49/64 loss: -0.3900304436683655
Batch 50/64 loss: -0.3208138048648834
Batch 51/64 loss: -0.30704542994499207
Batch 52/64 loss: -0.3442123532295227
Batch 53/64 loss: -0.3708767294883728
Batch 54/64 loss: -0.29510101675987244
Batch 55/64 loss: -0.35579150915145874
Batch 56/64 loss: -0.3415818214416504
Batch 57/64 loss: -0.3339608311653137
Batch 58/64 loss: -0.3222389221191406
Batch 59/64 loss: -0.3315308094024658
Batch 60/64 loss: -0.3440847098827362
Batch 61/64 loss: -0.37057065963745117
Batch 62/64 loss: -0.3248925805091858
Batch 63/64 loss: -0.2961764335632324
Batch 64/64 loss: -0.3566526472568512
Epoch 409  Train loss: -0.3512355920146493  Val loss: 0.03617843360835334
Epoch 410
-------------------------------
Batch 1/64 loss: -0.36403727531433105
Batch 2/64 loss: -0.38200312852859497
Batch 3/64 loss: -0.3314294219017029
Batch 4/64 loss: -0.3391558825969696
Batch 5/64 loss: -0.36292511224746704
Batch 6/64 loss: -0.3317369818687439
Batch 7/64 loss: -0.3630513548851013
Batch 8/64 loss: -0.3595828413963318
Batch 9/64 loss: -0.39285972714424133
Batch 10/64 loss: -0.349448561668396
Batch 11/64 loss: -0.35365504026412964
Batch 12/64 loss: -0.3689693212509155
Batch 13/64 loss: -0.3517789840698242
Batch 14/64 loss: -0.3423762321472168
Batch 15/64 loss: -0.35530248284339905
Batch 16/64 loss: -0.34550178050994873
Batch 17/64 loss: -0.34746962785720825
Batch 18/64 loss: -0.3754502832889557
Batch 19/64 loss: -0.37457793951034546
Batch 20/64 loss: -0.3270496129989624
Batch 21/64 loss: -0.36478614807128906
Batch 22/64 loss: -0.32186391949653625
Batch 23/64 loss: -0.387321799993515
Batch 24/64 loss: -0.4005815088748932
Batch 25/64 loss: -0.3168410658836365
Batch 26/64 loss: -0.32412466406822205
Batch 27/64 loss: -0.3689694404602051
Batch 28/64 loss: -0.3448244333267212
Batch 29/64 loss: -0.3919713795185089
Batch 30/64 loss: -0.39452725648880005
Batch 31/64 loss: -0.3326619863510132
Batch 32/64 loss: -0.3745133876800537
Batch 33/64 loss: -0.3621232509613037
Batch 34/64 loss: -0.35933399200439453
Batch 35/64 loss: -0.3536621332168579
Batch 36/64 loss: -0.3793083429336548
Batch 37/64 loss: -0.3696441352367401
Batch 38/64 loss: -0.3503933548927307
Batch 39/64 loss: -0.385748028755188
Batch 40/64 loss: -0.37084758281707764
Batch 41/64 loss: -0.35266417264938354
Batch 42/64 loss: -0.3140961825847626
Batch 43/64 loss: -0.3110646605491638
Batch 44/64 loss: -0.36499446630477905
Batch 45/64 loss: -0.3210947513580322
Batch 46/64 loss: -0.33629992604255676
Batch 47/64 loss: -0.3514559864997864
Batch 48/64 loss: -0.33180248737335205
Batch 49/64 loss: -0.35308557748794556
Batch 50/64 loss: -0.346672922372818
Batch 51/64 loss: -0.34979525208473206
Batch 52/64 loss: -0.3634987473487854
Batch 53/64 loss: -0.359507292509079
Batch 54/64 loss: -0.35827934741973877
Batch 55/64 loss: -0.35660243034362793
Batch 56/64 loss: -0.37771761417388916
Batch 57/64 loss: -0.32642287015914917
Batch 58/64 loss: -0.36959171295166016
Batch 59/64 loss: -0.3558134436607361
Batch 60/64 loss: -0.3472658693790436
Batch 61/64 loss: -0.3203548789024353
Batch 62/64 loss: -0.3096390962600708
Batch 63/64 loss: -0.32845208048820496
Batch 64/64 loss: -0.3777024745941162
Epoch 410  Train loss: -0.3539114984811521  Val loss: 0.03208565056528832
Epoch 411
-------------------------------
Batch 1/64 loss: -0.34460732340812683
Batch 2/64 loss: -0.3480820059776306
Batch 3/64 loss: -0.3616204261779785
Batch 4/64 loss: -0.38150933384895325
Batch 5/64 loss: -0.32887423038482666
Batch 6/64 loss: -0.3705424666404724
Batch 7/64 loss: -0.30859673023223877
Batch 8/64 loss: -0.3512676954269409
Batch 9/64 loss: -0.31641772389411926
Batch 10/64 loss: -0.3842448592185974
Batch 11/64 loss: -0.376893013715744
Batch 12/64 loss: -0.35558784008026123
Batch 13/64 loss: -0.3618086576461792
Batch 14/64 loss: -0.37545353174209595
Batch 15/64 loss: -0.345317006111145
Batch 16/64 loss: -0.3732124865055084
Batch 17/64 loss: -0.32149139046669006
Batch 18/64 loss: -0.34851235151290894
Batch 19/64 loss: -0.3743206858634949
Batch 20/64 loss: -0.3934074938297272
Batch 21/64 loss: -0.3944448232650757
Batch 22/64 loss: -0.34852278232574463
Batch 23/64 loss: -0.38732075691223145
Batch 24/64 loss: -0.3556975722312927
Batch 25/64 loss: -0.33657413721084595
Batch 26/64 loss: -0.3819919526576996
Batch 27/64 loss: -0.33429664373397827
Batch 28/64 loss: -0.3478735089302063
Batch 29/64 loss: -0.33699196577072144
Batch 30/64 loss: -0.35334518551826477
Batch 31/64 loss: -0.31334608793258667
Batch 32/64 loss: -0.35657811164855957
Batch 33/64 loss: -0.37117260694503784
Batch 34/64 loss: -0.3563656210899353
Batch 35/64 loss: -0.37181705236434937
Batch 36/64 loss: -0.3749344050884247
Batch 37/64 loss: -0.347206175327301
Batch 38/64 loss: -0.33752012252807617
Batch 39/64 loss: -0.34050989151000977
Batch 40/64 loss: -0.306856632232666
Batch 41/64 loss: -0.30843234062194824
Batch 42/64 loss: -0.34237542748451233
Batch 43/64 loss: -0.37832361459732056
Batch 44/64 loss: -0.35472291707992554
Batch 45/64 loss: -0.37715303897857666
Batch 46/64 loss: -0.38019639253616333
Batch 47/64 loss: -0.3619459867477417
Batch 48/64 loss: -0.35294318199157715
Batch 49/64 loss: -0.3593354821205139
Batch 50/64 loss: -0.3200073838233948
Batch 51/64 loss: -0.3725441098213196
Batch 52/64 loss: -0.35135647654533386
Batch 53/64 loss: -0.32823047041893005
Batch 54/64 loss: -0.36600539088249207
Batch 55/64 loss: -0.3858211636543274
Batch 56/64 loss: -0.34855592250823975
Batch 57/64 loss: -0.34013256430625916
Batch 58/64 loss: -0.3739855885505676
Batch 59/64 loss: -0.3468131422996521
Batch 60/64 loss: -0.3637063503265381
Batch 61/64 loss: -0.35703104734420776
Batch 62/64 loss: -0.37309449911117554
Batch 63/64 loss: -0.36039480566978455
Batch 64/64 loss: -0.3159313499927521
Epoch 411  Train loss: -0.3547480642795563  Val loss: 0.033870671418114624
Epoch 412
-------------------------------
Batch 1/64 loss: -0.35949820280075073
Batch 2/64 loss: -0.350097119808197
Batch 3/64 loss: -0.35654592514038086
Batch 4/64 loss: -0.3863925039768219
Batch 5/64 loss: -0.3457942605018616
Batch 6/64 loss: -0.3425130546092987
Batch 7/64 loss: -0.3658266067504883
Batch 8/64 loss: -0.3627924919128418
Batch 9/64 loss: -0.34859731793403625
Batch 10/64 loss: -0.3569836914539337
Batch 11/64 loss: -0.3485693037509918
Batch 12/64 loss: -0.3273009955883026
Batch 13/64 loss: -0.3096962571144104
Batch 14/64 loss: -0.3195633888244629
Batch 15/64 loss: -0.3637946844100952
Batch 16/64 loss: -0.32176750898361206
Batch 17/64 loss: -0.3131582736968994
Batch 18/64 loss: -0.3494621515274048
Batch 19/64 loss: -0.3766104280948639
Batch 20/64 loss: -0.3785586357116699
Batch 21/64 loss: -0.38449376821517944
Batch 22/64 loss: -0.3650873303413391
Batch 23/64 loss: -0.3581061363220215
Batch 24/64 loss: -0.36146098375320435
Batch 25/64 loss: -0.3320607841014862
Batch 26/64 loss: -0.3440083861351013
Batch 27/64 loss: -0.32814857363700867
Batch 28/64 loss: -0.35000544786453247
Batch 29/64 loss: -0.35958629846572876
Batch 30/64 loss: -0.3954732418060303
Batch 31/64 loss: -0.3806139826774597
Batch 32/64 loss: -0.3633825182914734
Batch 33/64 loss: -0.3386602997779846
Batch 34/64 loss: -0.3645874857902527
Batch 35/64 loss: -0.34474217891693115
Batch 36/64 loss: -0.35351359844207764
Batch 37/64 loss: -0.3778097927570343
Batch 38/64 loss: -0.3383122980594635
Batch 39/64 loss: -0.37153002619743347
Batch 40/64 loss: -0.3615387976169586
Batch 41/64 loss: -0.3652706742286682
Batch 42/64 loss: -0.34027421474456787
Batch 43/64 loss: -0.36466923356056213
Batch 44/64 loss: -0.3676430583000183
Batch 45/64 loss: -0.34241244196891785
Batch 46/64 loss: -0.3629746437072754
Batch 47/64 loss: -0.36591973900794983
Batch 48/64 loss: -0.35377997159957886
Batch 49/64 loss: -0.37031084299087524
Batch 50/64 loss: -0.34304991364479065
Batch 51/64 loss: -0.3203994035720825
Batch 52/64 loss: -0.36136579513549805
Batch 53/64 loss: -0.3290935158729553
Batch 54/64 loss: -0.3588794767856598
Batch 55/64 loss: -0.3481881022453308
Batch 56/64 loss: -0.3471391797065735
Batch 57/64 loss: -0.35462725162506104
Batch 58/64 loss: -0.3697519898414612
Batch 59/64 loss: -0.3289112448692322
Batch 60/64 loss: -0.30217158794403076
Batch 61/64 loss: -0.36655694246292114
Batch 62/64 loss: -0.33205151557922363
Batch 63/64 loss: -0.3516848683357239
Batch 64/64 loss: -0.36303144693374634
Epoch 412  Train loss: -0.35256539489708694  Val loss: 0.03384691415373812
Epoch 413
-------------------------------
Batch 1/64 loss: -0.37100380659103394
Batch 2/64 loss: -0.34380728006362915
Batch 3/64 loss: -0.3472561240196228
Batch 4/64 loss: -0.30021190643310547
Batch 5/64 loss: -0.37287309765815735
Batch 6/64 loss: -0.3521724045276642
Batch 7/64 loss: -0.3705311119556427
Batch 8/64 loss: -0.35144710540771484
Batch 9/64 loss: -0.3318970799446106
Batch 10/64 loss: -0.35925284028053284
Batch 11/64 loss: -0.37408632040023804
Batch 12/64 loss: -0.3826411962509155
Batch 13/64 loss: -0.3548274636268616
Batch 14/64 loss: -0.3899291753768921
Batch 15/64 loss: -0.3282175660133362
Batch 16/64 loss: -0.3881545662879944
Batch 17/64 loss: -0.37173882126808167
Batch 18/64 loss: -0.3851151764392853
Batch 19/64 loss: -0.36079275608062744
Batch 20/64 loss: -0.34897077083587646
Batch 21/64 loss: -0.3502327799797058
Batch 22/64 loss: -0.36219334602355957
Batch 23/64 loss: -0.3494563102722168
Batch 24/64 loss: -0.33980047702789307
Batch 25/64 loss: -0.353327214717865
Batch 26/64 loss: -0.3615604043006897
Batch 27/64 loss: -0.374036967754364
Batch 28/64 loss: -0.35753190517425537
Batch 29/64 loss: -0.3863924741744995
Batch 30/64 loss: -0.35978060960769653
Batch 31/64 loss: -0.32873594760894775
Batch 32/64 loss: -0.37780046463012695
Batch 33/64 loss: -0.33592236042022705
Batch 34/64 loss: -0.3493591547012329
Batch 35/64 loss: -0.343270868062973
Batch 36/64 loss: -0.389958918094635
Batch 37/64 loss: -0.3812670409679413
Batch 38/64 loss: -0.3539687395095825
Batch 39/64 loss: -0.350236177444458
Batch 40/64 loss: -0.3308136761188507
Batch 41/64 loss: -0.3295597434043884
Batch 42/64 loss: -0.3894418478012085
Batch 43/64 loss: -0.3563981056213379
Batch 44/64 loss: -0.3076171278953552
Batch 45/64 loss: -0.36073368787765503
Batch 46/64 loss: -0.3377036452293396
Batch 47/64 loss: -0.3352311849594116
Batch 48/64 loss: -0.36185672879219055
Batch 49/64 loss: -0.3781638443470001
Batch 50/64 loss: -0.3301524817943573
Batch 51/64 loss: -0.3555658757686615
Batch 52/64 loss: -0.36282879114151
Batch 53/64 loss: -0.35607659816741943
Batch 54/64 loss: -0.3627617359161377
Batch 55/64 loss: -0.32051441073417664
Batch 56/64 loss: -0.33166658878326416
Batch 57/64 loss: -0.30965328216552734
Batch 58/64 loss: -0.3559621572494507
Batch 59/64 loss: -0.3591373860836029
Batch 60/64 loss: -0.3590697646141052
Batch 61/64 loss: -0.3235306739807129
Batch 62/64 loss: -0.3516881465911865
Batch 63/64 loss: -0.3756047785282135
Batch 64/64 loss: -0.3771240711212158
Epoch 413  Train loss: -0.35473465171514773  Val loss: 0.03282901941705815
Epoch 414
-------------------------------
Batch 1/64 loss: -0.36763787269592285
Batch 2/64 loss: -0.3704741597175598
Batch 3/64 loss: -0.3529752790927887
Batch 4/64 loss: -0.367286741733551
Batch 5/64 loss: -0.3163910508155823
Batch 6/64 loss: -0.3703821301460266
Batch 7/64 loss: -0.35913699865341187
Batch 8/64 loss: -0.3535758852958679
Batch 9/64 loss: -0.35631227493286133
Batch 10/64 loss: -0.33287280797958374
Batch 11/64 loss: -0.35260316729545593
Batch 12/64 loss: -0.3711681365966797
Batch 13/64 loss: -0.38446980714797974
Batch 14/64 loss: -0.34464752674102783
Batch 15/64 loss: -0.3385034203529358
Batch 16/64 loss: -0.37564682960510254
Batch 17/64 loss: -0.3592034876346588
Batch 18/64 loss: -0.3207961320877075
Batch 19/64 loss: -0.3238775134086609
Batch 20/64 loss: -0.36559709906578064
Batch 21/64 loss: -0.34875380992889404
Batch 22/64 loss: -0.3011527359485626
Batch 23/64 loss: -0.3421679139137268
Batch 24/64 loss: -0.3640531897544861
Batch 25/64 loss: -0.3669703006744385
Batch 26/64 loss: -0.32028672099113464
Batch 27/64 loss: -0.3357093930244446
Batch 28/64 loss: -0.34246158599853516
Batch 29/64 loss: -0.3602331876754761
Batch 30/64 loss: -0.38656842708587646
Batch 31/64 loss: -0.348041832447052
Batch 32/64 loss: -0.3608490824699402
Batch 33/64 loss: -0.3474588096141815
Batch 34/64 loss: -0.34098702669143677
Batch 35/64 loss: -0.30256134271621704
Batch 36/64 loss: -0.36517420411109924
Batch 37/64 loss: -0.36799052357673645
Batch 38/64 loss: -0.3549708127975464
Batch 39/64 loss: -0.3708750009536743
Batch 40/64 loss: -0.390938937664032
Batch 41/64 loss: -0.371612548828125
Batch 42/64 loss: -0.3339552581310272
Batch 43/64 loss: -0.3517169952392578
Batch 44/64 loss: -0.3374093174934387
Batch 45/64 loss: -0.32389944791793823
Batch 46/64 loss: -0.38297706842422485
Batch 47/64 loss: -0.38375431299209595
Batch 48/64 loss: -0.3364220857620239
Batch 49/64 loss: -0.3328966796398163
Batch 50/64 loss: -0.38015586137771606
Batch 51/64 loss: -0.338312566280365
Batch 52/64 loss: -0.3596841096878052
Batch 53/64 loss: -0.3440541625022888
Batch 54/64 loss: -0.379606693983078
Batch 55/64 loss: -0.37117689847946167
Batch 56/64 loss: -0.35577505826950073
Batch 57/64 loss: -0.36954987049102783
Batch 58/64 loss: -0.32840603590011597
Batch 59/64 loss: -0.3652358055114746
Batch 60/64 loss: -0.38762927055358887
Batch 61/64 loss: -0.3518895208835602
Batch 62/64 loss: -0.34844768047332764
Batch 63/64 loss: -0.3722699284553528
Batch 64/64 loss: -0.30425333976745605
Epoch 414  Train loss: -0.3535182798610014  Val loss: 0.0362011681717286
Epoch 415
-------------------------------
Batch 1/64 loss: -0.38266482949256897
Batch 2/64 loss: -0.32872238755226135
Batch 3/64 loss: -0.3893014192581177
Batch 4/64 loss: -0.3348882496356964
Batch 5/64 loss: -0.3693283796310425
Batch 6/64 loss: -0.35530155897140503
Batch 7/64 loss: -0.33430933952331543
Batch 8/64 loss: -0.34757381677627563
Batch 9/64 loss: -0.37307828664779663
Batch 10/64 loss: -0.3126477599143982
Batch 11/64 loss: -0.3481934070587158
Batch 12/64 loss: -0.3702027499675751
Batch 13/64 loss: -0.36161333322525024
Batch 14/64 loss: -0.3879431188106537
Batch 15/64 loss: -0.37192589044570923
Batch 16/64 loss: -0.320882111787796
Batch 17/64 loss: -0.37237176299095154
Batch 18/64 loss: -0.3455180823802948
Batch 19/64 loss: -0.39139503240585327
Batch 20/64 loss: -0.3410509526729584
Batch 21/64 loss: -0.33470413088798523
Batch 22/64 loss: -0.37038201093673706
Batch 23/64 loss: -0.3293994069099426
Batch 24/64 loss: -0.3505786657333374
Batch 25/64 loss: -0.361599326133728
Batch 26/64 loss: -0.36812853813171387
Batch 27/64 loss: -0.34876111149787903
Batch 28/64 loss: -0.3708806037902832
Batch 29/64 loss: -0.32116055488586426
Batch 30/64 loss: -0.35474860668182373
Batch 31/64 loss: -0.3654376268386841
Batch 32/64 loss: -0.35326069593429565
Batch 33/64 loss: -0.35960787534713745
Batch 34/64 loss: -0.32823318243026733
Batch 35/64 loss: -0.29728448390960693
Batch 36/64 loss: -0.3698269724845886
Batch 37/64 loss: -0.3666971027851105
Batch 38/64 loss: -0.316663920879364
Batch 39/64 loss: -0.31891149282455444
Batch 40/64 loss: -0.3440103828907013
Batch 41/64 loss: -0.36929893493652344
Batch 42/64 loss: -0.3706400990486145
Batch 43/64 loss: -0.3515966832637787
Batch 44/64 loss: -0.35278356075286865
Batch 45/64 loss: -0.3271615207195282
Batch 46/64 loss: -0.37294870615005493
Batch 47/64 loss: -0.39133426547050476
Batch 48/64 loss: -0.3534771502017975
Batch 49/64 loss: -0.3647354543209076
Batch 50/64 loss: -0.29987969994544983
Batch 51/64 loss: -0.38155397772789
Batch 52/64 loss: -0.3576536774635315
Batch 53/64 loss: -0.36204293370246887
Batch 54/64 loss: -0.3930814266204834
Batch 55/64 loss: -0.3799448013305664
Batch 56/64 loss: -0.3605312705039978
Batch 57/64 loss: -0.3903753161430359
Batch 58/64 loss: -0.3054262101650238
Batch 59/64 loss: -0.3589087128639221
Batch 60/64 loss: -0.36728912591934204
Batch 61/64 loss: -0.3631380796432495
Batch 62/64 loss: -0.3591309189796448
Batch 63/64 loss: -0.3594309091567993
Batch 64/64 loss: -0.36028432846069336
Epoch 415  Train loss: -0.35500809164608227  Val loss: 0.03191164842585927
Epoch 416
-------------------------------
Batch 1/64 loss: -0.36540377140045166
Batch 2/64 loss: -0.37149518728256226
Batch 3/64 loss: -0.37674352526664734
Batch 4/64 loss: -0.3611387014389038
Batch 5/64 loss: -0.34535378217697144
Batch 6/64 loss: -0.37217485904693604
Batch 7/64 loss: -0.3893328309059143
Batch 8/64 loss: -0.38026928901672363
Batch 9/64 loss: -0.38783541321754456
Batch 10/64 loss: -0.34700775146484375
Batch 11/64 loss: -0.36670634150505066
Batch 12/64 loss: -0.3386625647544861
Batch 13/64 loss: -0.38555294275283813
Batch 14/64 loss: -0.3584500551223755
Batch 15/64 loss: -0.3806886672973633
Batch 16/64 loss: -0.39335572719573975
Batch 17/64 loss: -0.36441150307655334
Batch 18/64 loss: -0.3681352734565735
Batch 19/64 loss: -0.34520310163497925
Batch 20/64 loss: -0.35741546750068665
Batch 21/64 loss: -0.36021724343299866
Batch 22/64 loss: -0.3342856168746948
Batch 23/64 loss: -0.37188082933425903
Batch 24/64 loss: -0.3262900114059448
Batch 25/64 loss: -0.35039955377578735
Batch 26/64 loss: -0.3484343886375427
Batch 27/64 loss: -0.37610572576522827
Batch 28/64 loss: -0.34523600339889526
Batch 29/64 loss: -0.30273693799972534
Batch 30/64 loss: -0.328716516494751
Batch 31/64 loss: -0.367904394865036
Batch 32/64 loss: -0.37576818466186523
Batch 33/64 loss: -0.3771341145038605
Batch 34/64 loss: -0.3353705406188965
Batch 35/64 loss: -0.33306369185447693
Batch 36/64 loss: -0.36300936341285706
Batch 37/64 loss: -0.36255836486816406
Batch 38/64 loss: -0.33654850721359253
Batch 39/64 loss: -0.3494163751602173
Batch 40/64 loss: -0.35394856333732605
Batch 41/64 loss: -0.34091317653656006
Batch 42/64 loss: -0.2881772816181183
Batch 43/64 loss: -0.3812604248523712
Batch 44/64 loss: -0.31030768156051636
Batch 45/64 loss: -0.34301868081092834
Batch 46/64 loss: -0.3499882221221924
Batch 47/64 loss: -0.3446118235588074
Batch 48/64 loss: -0.3758864998817444
Batch 49/64 loss: -0.37445545196533203
Batch 50/64 loss: -0.3448120355606079
Batch 51/64 loss: -0.37098613381385803
Batch 52/64 loss: -0.3631158769130707
Batch 53/64 loss: -0.39135292172431946
Batch 54/64 loss: -0.3622962236404419
Batch 55/64 loss: -0.33192992210388184
Batch 56/64 loss: -0.37829089164733887
Batch 57/64 loss: -0.3395634889602661
Batch 58/64 loss: -0.35733795166015625
Batch 59/64 loss: -0.3744674623012543
Batch 60/64 loss: -0.319804310798645
Batch 61/64 loss: -0.3394370675086975
Batch 62/64 loss: -0.35159438848495483
Batch 63/64 loss: -0.3663192391395569
Batch 64/64 loss: -0.3471742570400238
Epoch 416  Train loss: -0.35630854169527687  Val loss: 0.03341786373931518
Epoch 417
-------------------------------
Batch 1/64 loss: -0.37068864703178406
Batch 2/64 loss: -0.35240358114242554
Batch 3/64 loss: -0.3252749443054199
Batch 4/64 loss: -0.3515908420085907
Batch 5/64 loss: -0.3380089998245239
Batch 6/64 loss: -0.36166149377822876
Batch 7/64 loss: -0.33913010358810425
Batch 8/64 loss: -0.33837029337882996
Batch 9/64 loss: -0.3766634464263916
Batch 10/64 loss: -0.35862526297569275
Batch 11/64 loss: -0.3470412492752075
Batch 12/64 loss: -0.29207637906074524
Batch 13/64 loss: -0.3680271506309509
Batch 14/64 loss: -0.3566904366016388
Batch 15/64 loss: -0.3460078239440918
Batch 16/64 loss: -0.3325120210647583
Batch 17/64 loss: -0.363839328289032
Batch 18/64 loss: -0.36204034090042114
Batch 19/64 loss: -0.3513849079608917
Batch 20/64 loss: -0.363919198513031
Batch 21/64 loss: -0.3439132571220398
Batch 22/64 loss: -0.34258729219436646
Batch 23/64 loss: -0.34434276819229126
Batch 24/64 loss: -0.3880939483642578
Batch 25/64 loss: -0.361189603805542
Batch 26/64 loss: -0.3760678470134735
Batch 27/64 loss: -0.3487505316734314
Batch 28/64 loss: -0.3483039140701294
Batch 29/64 loss: -0.35981595516204834
Batch 30/64 loss: -0.37362197041511536
Batch 31/64 loss: -0.3551204204559326
Batch 32/64 loss: -0.3477988839149475
Batch 33/64 loss: -0.35802143812179565
Batch 34/64 loss: -0.34115660190582275
Batch 35/64 loss: -0.3481352925300598
Batch 36/64 loss: -0.3534106910228729
Batch 37/64 loss: -0.34199002385139465
Batch 38/64 loss: -0.35108351707458496
Batch 39/64 loss: -0.355698823928833
Batch 40/64 loss: -0.3453548550605774
Batch 41/64 loss: -0.3775864243507385
Batch 42/64 loss: -0.35324999690055847
Batch 43/64 loss: -0.33805087208747864
Batch 44/64 loss: -0.3574811816215515
Batch 45/64 loss: -0.3438946008682251
Batch 46/64 loss: -0.3218410015106201
Batch 47/64 loss: -0.32462364435195923
Batch 48/64 loss: -0.33627623319625854
Batch 49/64 loss: -0.353881299495697
Batch 50/64 loss: -0.38844844698905945
Batch 51/64 loss: -0.3566628098487854
Batch 52/64 loss: -0.36428365111351013
Batch 53/64 loss: -0.3753998875617981
Batch 54/64 loss: -0.35272297263145447
Batch 55/64 loss: -0.3681085705757141
Batch 56/64 loss: -0.36726242303848267
Batch 57/64 loss: -0.347682386636734
Batch 58/64 loss: -0.3647233843803406
Batch 59/64 loss: -0.34264540672302246
Batch 60/64 loss: -0.3524637818336487
Batch 61/64 loss: -0.3615795373916626
Batch 62/64 loss: -0.3410254120826721
Batch 63/64 loss: -0.3674325942993164
Batch 64/64 loss: -0.3489822447299957
Epoch 417  Train loss: -0.35293297709203236  Val loss: 0.03488841724559614
Epoch 418
-------------------------------
Batch 1/64 loss: -0.3647310137748718
Batch 2/64 loss: -0.36308008432388306
Batch 3/64 loss: -0.3805469572544098
Batch 4/64 loss: -0.3271167278289795
Batch 5/64 loss: -0.3193988800048828
Batch 6/64 loss: -0.33780407905578613
Batch 7/64 loss: -0.3692812919616699
Batch 8/64 loss: -0.37883010506629944
Batch 9/64 loss: -0.32549363374710083
Batch 10/64 loss: -0.3542293906211853
Batch 11/64 loss: -0.35584506392478943
Batch 12/64 loss: -0.3269233703613281
Batch 13/64 loss: -0.35325634479522705
Batch 14/64 loss: -0.3562159240245819
Batch 15/64 loss: -0.3668399155139923
Batch 16/64 loss: -0.33127421140670776
Batch 17/64 loss: -0.3340126872062683
Batch 18/64 loss: -0.33474093675613403
Batch 19/64 loss: -0.36697477102279663
Batch 20/64 loss: -0.36687910556793213
Batch 21/64 loss: -0.3166927695274353
Batch 22/64 loss: -0.3650578558444977
Batch 23/64 loss: -0.3697125315666199
Batch 24/64 loss: -0.3934900164604187
Batch 25/64 loss: -0.35148805379867554
Batch 26/64 loss: -0.345958411693573
Batch 27/64 loss: -0.36687228083610535
Batch 28/64 loss: -0.3511039912700653
Batch 29/64 loss: -0.3632816672325134
Batch 30/64 loss: -0.3102186322212219
Batch 31/64 loss: -0.36114296317100525
Batch 32/64 loss: -0.3531588912010193
Batch 33/64 loss: -0.35340631008148193
Batch 34/64 loss: -0.3506430387496948
Batch 35/64 loss: -0.3334200084209442
Batch 36/64 loss: -0.3587961196899414
Batch 37/64 loss: -0.38173872232437134
Batch 38/64 loss: -0.3563544452190399
Batch 39/64 loss: -0.35488301515579224
Batch 40/64 loss: -0.3395564556121826
Batch 41/64 loss: -0.37089380621910095
Batch 42/64 loss: -0.3867717683315277
Batch 43/64 loss: -0.3738117218017578
Batch 44/64 loss: -0.37052053213119507
Batch 45/64 loss: -0.3507661521434784
Batch 46/64 loss: -0.34951773285865784
Batch 47/64 loss: -0.3338368535041809
Batch 48/64 loss: -0.3661468029022217
Batch 49/64 loss: -0.3813948631286621
Batch 50/64 loss: -0.35713517665863037
Batch 51/64 loss: -0.3595116138458252
Batch 52/64 loss: -0.33202844858169556
Batch 53/64 loss: -0.37622055411338806
Batch 54/64 loss: -0.34960588812828064
Batch 55/64 loss: -0.33220571279525757
Batch 56/64 loss: -0.36762431263923645
Batch 57/64 loss: -0.3484874367713928
Batch 58/64 loss: -0.33229494094848633
Batch 59/64 loss: -0.37625497579574585
Batch 60/64 loss: -0.34197884798049927
Batch 61/64 loss: -0.34124690294265747
Batch 62/64 loss: -0.3725060224533081
Batch 63/64 loss: -0.374780535697937
Batch 64/64 loss: -0.2653560936450958
Epoch 418  Train loss: -0.35349034269650775  Val loss: 0.03266726224283172
Epoch 419
-------------------------------
Batch 1/64 loss: -0.3460114598274231
Batch 2/64 loss: -0.3828713893890381
Batch 3/64 loss: -0.3444885015487671
Batch 4/64 loss: -0.3599220812320709
Batch 5/64 loss: -0.3190067410469055
Batch 6/64 loss: -0.371219277381897
Batch 7/64 loss: -0.33148330450057983
Batch 8/64 loss: -0.37309378385543823
Batch 9/64 loss: -0.38875094056129456
Batch 10/64 loss: -0.34680262207984924
Batch 11/64 loss: -0.34338194131851196
Batch 12/64 loss: -0.36325567960739136
Batch 13/64 loss: -0.3943561911582947
Batch 14/64 loss: -0.3735522925853729
Batch 15/64 loss: -0.38641685247421265
Batch 16/64 loss: -0.3543972969055176
Batch 17/64 loss: -0.3358769416809082
Batch 18/64 loss: -0.3696857690811157
Batch 19/64 loss: -0.3667910695075989
Batch 20/64 loss: -0.37458711862564087
Batch 21/64 loss: -0.35679298639297485
Batch 22/64 loss: -0.3716224730014801
Batch 23/64 loss: -0.36605748534202576
Batch 24/64 loss: -0.36347413063049316
Batch 25/64 loss: -0.35303112864494324
Batch 26/64 loss: -0.3741114139556885
Batch 27/64 loss: -0.35809558629989624
Batch 28/64 loss: -0.3733305335044861
Batch 29/64 loss: -0.366508811712265
Batch 30/64 loss: -0.34011924266815186
Batch 31/64 loss: -0.3748167157173157
Batch 32/64 loss: -0.3603282570838928
Batch 33/64 loss: -0.39855000376701355
Batch 34/64 loss: -0.38054394721984863
Batch 35/64 loss: -0.35460516810417175
Batch 36/64 loss: -0.37544411420822144
Batch 37/64 loss: -0.3617737293243408
Batch 38/64 loss: -0.33997124433517456
Batch 39/64 loss: -0.32732218503952026
Batch 40/64 loss: -0.3497304618358612
Batch 41/64 loss: -0.3769305944442749
Batch 42/64 loss: -0.3508875370025635
Batch 43/64 loss: -0.36835789680480957
Batch 44/64 loss: -0.3343861699104309
Batch 45/64 loss: -0.32323479652404785
Batch 46/64 loss: -0.3589542806148529
Batch 47/64 loss: -0.35501277446746826
Batch 48/64 loss: -0.36582380533218384
Batch 49/64 loss: -0.3488307595252991
Batch 50/64 loss: -0.3462958335876465
Batch 51/64 loss: -0.37366098165512085
Batch 52/64 loss: -0.3471093475818634
Batch 53/64 loss: -0.3715057373046875
Batch 54/64 loss: -0.36814218759536743
Batch 55/64 loss: -0.38770267367362976
Batch 56/64 loss: -0.3538012206554413
Batch 57/64 loss: -0.3592258095741272
Batch 58/64 loss: -0.3338356614112854
Batch 59/64 loss: -0.3877909183502197
Batch 60/64 loss: -0.37958431243896484
Batch 61/64 loss: -0.31037598848342896
Batch 62/64 loss: -0.35233306884765625
Batch 63/64 loss: -0.3441161513328552
Batch 64/64 loss: -0.3661862909793854
Epoch 419  Train loss: -0.35991716232954285  Val loss: 0.033564478261364286
Epoch 420
-------------------------------
Batch 1/64 loss: -0.37179723381996155
Batch 2/64 loss: -0.3723127245903015
Batch 3/64 loss: -0.3850681781768799
Batch 4/64 loss: -0.32564055919647217
Batch 5/64 loss: -0.3434024155139923
Batch 6/64 loss: -0.37527602910995483
Batch 7/64 loss: -0.3522213399410248
Batch 8/64 loss: -0.3683280944824219
Batch 9/64 loss: -0.3862883448600769
Batch 10/64 loss: -0.3620973527431488
Batch 11/64 loss: -0.3361095190048218
Batch 12/64 loss: -0.3680490553379059
Batch 13/64 loss: -0.29404976963996887
Batch 14/64 loss: -0.3425405025482178
Batch 15/64 loss: -0.3534305691719055
Batch 16/64 loss: -0.3666607737541199
Batch 17/64 loss: -0.3874608874320984
Batch 18/64 loss: -0.3438850939273834
Batch 19/64 loss: -0.34110724925994873
Batch 20/64 loss: -0.34077000617980957
Batch 21/64 loss: -0.3100407123565674
Batch 22/64 loss: -0.35663050413131714
Batch 23/64 loss: -0.35791832208633423
Batch 24/64 loss: -0.36133766174316406
Batch 25/64 loss: -0.3446432054042816
Batch 26/64 loss: -0.34572479128837585
Batch 27/64 loss: -0.34930264949798584
Batch 28/64 loss: -0.33797410130500793
Batch 29/64 loss: -0.34810546040534973
Batch 30/64 loss: -0.3815515637397766
Batch 31/64 loss: -0.343495637178421
Batch 32/64 loss: -0.3310557007789612
Batch 33/64 loss: -0.35277611017227173
Batch 34/64 loss: -0.3093309998512268
Batch 35/64 loss: -0.36018869280815125
Batch 36/64 loss: -0.3815235197544098
Batch 37/64 loss: -0.34984254837036133
Batch 38/64 loss: -0.3758624494075775
Batch 39/64 loss: -0.33795666694641113
Batch 40/64 loss: -0.34290796518325806
Batch 41/64 loss: -0.34689587354660034
Batch 42/64 loss: -0.3621707260608673
Batch 43/64 loss: -0.3556612730026245
Batch 44/64 loss: -0.3897702991962433
Batch 45/64 loss: -0.3777312636375427
Batch 46/64 loss: -0.3524024784564972
Batch 47/64 loss: -0.35069096088409424
Batch 48/64 loss: -0.36119234561920166
Batch 49/64 loss: -0.38280394673347473
Batch 50/64 loss: -0.37999439239501953
Batch 51/64 loss: -0.3507300019264221
Batch 52/64 loss: -0.3390684425830841
Batch 53/64 loss: -0.3602781891822815
Batch 54/64 loss: -0.32225245237350464
Batch 55/64 loss: -0.31462377309799194
Batch 56/64 loss: -0.3536633253097534
Batch 57/64 loss: -0.3417193293571472
Batch 58/64 loss: -0.35428565740585327
Batch 59/64 loss: -0.31304019689559937
Batch 60/64 loss: -0.3581661581993103
Batch 61/64 loss: -0.35764947533607483
Batch 62/64 loss: -0.3751649856567383
Batch 63/64 loss: -0.3472921550273895
Batch 64/64 loss: -0.35921573638916016
Epoch 420  Train loss: -0.3531188151415657  Val loss: 0.03392171777810428
Epoch 421
-------------------------------
Batch 1/64 loss: -0.3466799259185791
Batch 2/64 loss: -0.33511805534362793
Batch 3/64 loss: -0.3671545386314392
Batch 4/64 loss: -0.3691100776195526
Batch 5/64 loss: -0.3518317639827728
Batch 6/64 loss: -0.3580668866634369
Batch 7/64 loss: -0.3755756616592407
Batch 8/64 loss: -0.3479667007923126
Batch 9/64 loss: -0.3495337963104248
Batch 10/64 loss: -0.34413856267929077
Batch 11/64 loss: -0.3644261658191681
Batch 12/64 loss: -0.36908185482025146
Batch 13/64 loss: -0.30691641569137573
Batch 14/64 loss: -0.38421016931533813
Batch 15/64 loss: -0.3604724407196045
Batch 16/64 loss: -0.3251882791519165
Batch 17/64 loss: -0.330719530582428
Batch 18/64 loss: -0.28861695528030396
Batch 19/64 loss: -0.35794761776924133
Batch 20/64 loss: -0.3396477997303009
Batch 21/64 loss: -0.3719525635242462
Batch 22/64 loss: -0.29387038946151733
Batch 23/64 loss: -0.3720858097076416
Batch 24/64 loss: -0.3582693934440613
Batch 25/64 loss: -0.33724039793014526
Batch 26/64 loss: -0.3960329294204712
Batch 27/64 loss: -0.39369839429855347
Batch 28/64 loss: -0.37399381399154663
Batch 29/64 loss: -0.32774820923805237
Batch 30/64 loss: -0.3754191994667053
Batch 31/64 loss: -0.3388843238353729
Batch 32/64 loss: -0.36583375930786133
Batch 33/64 loss: -0.3760024309158325
Batch 34/64 loss: -0.3995944559574127
Batch 35/64 loss: -0.3513333201408386
Batch 36/64 loss: -0.32748544216156006
Batch 37/64 loss: -0.37531277537345886
Batch 38/64 loss: -0.38144832849502563
Batch 39/64 loss: -0.3976482152938843
Batch 40/64 loss: -0.3560014069080353
Batch 41/64 loss: -0.3773743510246277
Batch 42/64 loss: -0.3642892837524414
Batch 43/64 loss: -0.37220174074172974
Batch 44/64 loss: -0.3762611150741577
Batch 45/64 loss: -0.33028900623321533
Batch 46/64 loss: -0.37079834938049316
Batch 47/64 loss: -0.30982089042663574
Batch 48/64 loss: -0.37297332286834717
Batch 49/64 loss: -0.32387831807136536
Batch 50/64 loss: -0.36270129680633545
Batch 51/64 loss: -0.3669432997703552
Batch 52/64 loss: -0.34958958625793457
Batch 53/64 loss: -0.3337893784046173
Batch 54/64 loss: -0.3352769613265991
Batch 55/64 loss: -0.33930957317352295
Batch 56/64 loss: -0.3551434278488159
Batch 57/64 loss: -0.3837645351886749
Batch 58/64 loss: -0.3586174547672272
Batch 59/64 loss: -0.34960031509399414
Batch 60/64 loss: -0.3198336958885193
Batch 61/64 loss: -0.3757454752922058
Batch 62/64 loss: -0.3624456524848938
Batch 63/64 loss: -0.34810158610343933
Batch 64/64 loss: -0.37329912185668945
Epoch 421  Train loss: -0.3554663798388313  Val loss: 0.034267864071626436
Epoch 422
-------------------------------
Batch 1/64 loss: -0.3662160634994507
Batch 2/64 loss: -0.381719172000885
Batch 3/64 loss: -0.30489635467529297
Batch 4/64 loss: -0.39574408531188965
Batch 5/64 loss: -0.3636208474636078
Batch 6/64 loss: -0.3586898446083069
Batch 7/64 loss: -0.3509279191493988
Batch 8/64 loss: -0.34160280227661133
Batch 9/64 loss: -0.35300880670547485
Batch 10/64 loss: -0.3614639341831207
Batch 11/64 loss: -0.3292240798473358
Batch 12/64 loss: -0.3831273317337036
Batch 13/64 loss: -0.35322993993759155
Batch 14/64 loss: -0.34247395396232605
Batch 15/64 loss: -0.36942213773727417
Batch 16/64 loss: -0.36845505237579346
Batch 17/64 loss: -0.3591178357601166
Batch 18/64 loss: -0.3755500912666321
Batch 19/64 loss: -0.3792867660522461
Batch 20/64 loss: -0.38968124985694885
Batch 21/64 loss: -0.3839247226715088
Batch 22/64 loss: -0.33859413862228394
Batch 23/64 loss: -0.36343804001808167
Batch 24/64 loss: -0.3672513961791992
Batch 25/64 loss: -0.36984553933143616
Batch 26/64 loss: -0.3721308708190918
Batch 27/64 loss: -0.3663950562477112
Batch 28/64 loss: -0.3779131770133972
Batch 29/64 loss: -0.3607219457626343
Batch 30/64 loss: -0.3186399042606354
Batch 31/64 loss: -0.33910876512527466
Batch 32/64 loss: -0.31374210119247437
Batch 33/64 loss: -0.36399948596954346
Batch 34/64 loss: -0.33338022232055664
Batch 35/64 loss: -0.36332422494888306
Batch 36/64 loss: -0.3345317244529724
Batch 37/64 loss: -0.34280431270599365
Batch 38/64 loss: -0.3705492317676544
Batch 39/64 loss: -0.3551371693611145
Batch 40/64 loss: -0.36360180377960205
Batch 41/64 loss: -0.3597182631492615
Batch 42/64 loss: -0.33415019512176514
Batch 43/64 loss: -0.36356741189956665
Batch 44/64 loss: -0.3551971912384033
Batch 45/64 loss: -0.38021406531333923
Batch 46/64 loss: -0.36549797654151917
Batch 47/64 loss: -0.3574228286743164
Batch 48/64 loss: -0.3578011989593506
Batch 49/64 loss: -0.33800023794174194
Batch 50/64 loss: -0.32698601484298706
Batch 51/64 loss: -0.3633182644844055
Batch 52/64 loss: -0.35871055722236633
Batch 53/64 loss: -0.3624827265739441
Batch 54/64 loss: -0.38007646799087524
Batch 55/64 loss: -0.36264142394065857
Batch 56/64 loss: -0.35775548219680786
Batch 57/64 loss: -0.33560144901275635
Batch 58/64 loss: -0.3151906132698059
Batch 59/64 loss: -0.33838027715682983
Batch 60/64 loss: -0.3919275403022766
Batch 61/64 loss: -0.3595762252807617
Batch 62/64 loss: -0.31938040256500244
Batch 63/64 loss: -0.33013299107551575
Batch 64/64 loss: -0.32983943819999695
Epoch 422  Train loss: -0.3558839448526794  Val loss: 0.03458306879522055
Epoch 423
-------------------------------
Batch 1/64 loss: -0.34681951999664307
Batch 2/64 loss: -0.3995893597602844
Batch 3/64 loss: -0.3722599446773529
Batch 4/64 loss: -0.36723214387893677
Batch 5/64 loss: -0.33788514137268066
Batch 6/64 loss: -0.36679792404174805
Batch 7/64 loss: -0.391049861907959
Batch 8/64 loss: -0.3882400095462799
Batch 9/64 loss: -0.36333662271499634
Batch 10/64 loss: -0.35946327447891235
Batch 11/64 loss: -0.2990989685058594
Batch 12/64 loss: -0.3343251347541809
Batch 13/64 loss: -0.3717285990715027
Batch 14/64 loss: -0.3632747530937195
Batch 15/64 loss: -0.39200493693351746
Batch 16/64 loss: -0.4032575488090515
Batch 17/64 loss: -0.35481828451156616
Batch 18/64 loss: -0.29722753167152405
Batch 19/64 loss: -0.3340972661972046
Batch 20/64 loss: -0.3722754716873169
Batch 21/64 loss: -0.3202866315841675
Batch 22/64 loss: -0.34962543845176697
Batch 23/64 loss: -0.35944345593452454
Batch 24/64 loss: -0.38331902027130127
Batch 25/64 loss: -0.3747034966945648
Batch 26/64 loss: -0.35531100630760193
Batch 27/64 loss: -0.35061633586883545
Batch 28/64 loss: -0.36350420117378235
Batch 29/64 loss: -0.37721163034439087
Batch 30/64 loss: -0.36101555824279785
Batch 31/64 loss: -0.34384381771087646
Batch 32/64 loss: -0.3417177200317383
Batch 33/64 loss: -0.3706360459327698
Batch 34/64 loss: -0.34626132249832153
Batch 35/64 loss: -0.34861916303634644
Batch 36/64 loss: -0.34688103199005127
Batch 37/64 loss: -0.3536771237850189
Batch 38/64 loss: -0.316482275724411
Batch 39/64 loss: -0.36836570501327515
Batch 40/64 loss: -0.37048307061195374
Batch 41/64 loss: -0.35750168561935425
Batch 42/64 loss: -0.36230695247650146
Batch 43/64 loss: -0.3137500286102295
Batch 44/64 loss: -0.3659989833831787
Batch 45/64 loss: -0.3799486756324768
Batch 46/64 loss: -0.3218880593776703
Batch 47/64 loss: -0.37849289178848267
Batch 48/64 loss: -0.324486643075943
Batch 49/64 loss: -0.3289479613304138
Batch 50/64 loss: -0.354927659034729
Batch 51/64 loss: -0.38713181018829346
Batch 52/64 loss: -0.3709828853607178
Batch 53/64 loss: -0.3701573312282562
Batch 54/64 loss: -0.3477206826210022
Batch 55/64 loss: -0.33867642283439636
Batch 56/64 loss: -0.3393515348434448
Batch 57/64 loss: -0.37733781337738037
Batch 58/64 loss: -0.3398548662662506
Batch 59/64 loss: -0.3787911832332611
Batch 60/64 loss: -0.33513644337654114
Batch 61/64 loss: -0.35511451959609985
Batch 62/64 loss: -0.3799362778663635
Batch 63/64 loss: -0.3004626929759979
Batch 64/64 loss: -0.3787692189216614
Epoch 423  Train loss: -0.35623164340561514  Val loss: 0.03521144205761939
Epoch 424
-------------------------------
Batch 1/64 loss: -0.359866201877594
Batch 2/64 loss: -0.3814395070075989
Batch 3/64 loss: -0.36237236857414246
Batch 4/64 loss: -0.3536277115345001
Batch 5/64 loss: -0.32017838954925537
Batch 6/64 loss: -0.34787261486053467
Batch 7/64 loss: -0.3422433137893677
Batch 8/64 loss: -0.3738853931427002
Batch 9/64 loss: -0.35861173272132874
Batch 10/64 loss: -0.3858456313610077
Batch 11/64 loss: -0.38624757528305054
Batch 12/64 loss: -0.367200642824173
Batch 13/64 loss: -0.370627224445343
Batch 14/64 loss: -0.3489513397216797
Batch 15/64 loss: -0.38276299834251404
Batch 16/64 loss: -0.33748817443847656
Batch 17/64 loss: -0.30397191643714905
Batch 18/64 loss: -0.33268284797668457
Batch 19/64 loss: -0.3780302405357361
Batch 20/64 loss: -0.3306511640548706
Batch 21/64 loss: -0.3371056914329529
Batch 22/64 loss: -0.37934496998786926
Batch 23/64 loss: -0.357876718044281
Batch 24/64 loss: -0.3672206997871399
Batch 25/64 loss: -0.3432641625404358
Batch 26/64 loss: -0.35908517241477966
Batch 27/64 loss: -0.2768910229206085
Batch 28/64 loss: -0.3542085587978363
Batch 29/64 loss: -0.34436315298080444
Batch 30/64 loss: -0.3879173696041107
Batch 31/64 loss: -0.38125163316726685
Batch 32/64 loss: -0.3670080304145813
Batch 33/64 loss: -0.39105522632598877
Batch 34/64 loss: -0.3659120500087738
Batch 35/64 loss: -0.36954307556152344
Batch 36/64 loss: -0.3322351574897766
Batch 37/64 loss: -0.3460097908973694
Batch 38/64 loss: -0.36133742332458496
Batch 39/64 loss: -0.3626650273799896
Batch 40/64 loss: -0.3599587678909302
Batch 41/64 loss: -0.32954421639442444
Batch 42/64 loss: -0.3584362864494324
Batch 43/64 loss: -0.3549621105194092
Batch 44/64 loss: -0.3731529712677002
Batch 45/64 loss: -0.33845770359039307
Batch 46/64 loss: -0.36441048979759216
Batch 47/64 loss: -0.33314746618270874
Batch 48/64 loss: -0.3656383156776428
Batch 49/64 loss: -0.36341357231140137
Batch 50/64 loss: -0.39850717782974243
Batch 51/64 loss: -0.3566572666168213
Batch 52/64 loss: -0.37264615297317505
Batch 53/64 loss: -0.363835871219635
Batch 54/64 loss: -0.31263768672943115
Batch 55/64 loss: -0.34041351079940796
Batch 56/64 loss: -0.3214489221572876
Batch 57/64 loss: -0.3564605712890625
Batch 58/64 loss: -0.3745698928833008
Batch 59/64 loss: -0.37553924322128296
Batch 60/64 loss: -0.36439746618270874
Batch 61/64 loss: -0.38197991251945496
Batch 62/64 loss: -0.30104953050613403
Batch 63/64 loss: -0.3477671146392822
Batch 64/64 loss: -0.3452586233615875
Epoch 424  Train loss: -0.35571495072514403  Val loss: 0.033853996660291534
Epoch 425
-------------------------------
Batch 1/64 loss: -0.3418419361114502
Batch 2/64 loss: -0.36497998237609863
Batch 3/64 loss: -0.3602437973022461
Batch 4/64 loss: -0.3657571077346802
Batch 5/64 loss: -0.33826690912246704
Batch 6/64 loss: -0.3851433992385864
Batch 7/64 loss: -0.35997670888900757
Batch 8/64 loss: -0.3895711898803711
Batch 9/64 loss: -0.36425167322158813
Batch 10/64 loss: -0.3495597839355469
Batch 11/64 loss: -0.3757675290107727
Batch 12/64 loss: -0.3534371256828308
Batch 13/64 loss: -0.38789597153663635
Batch 14/64 loss: -0.3548625707626343
Batch 15/64 loss: -0.38285771012306213
Batch 16/64 loss: -0.36390435695648193
Batch 17/64 loss: -0.35150763392448425
Batch 18/64 loss: -0.3226630687713623
Batch 19/64 loss: -0.3575747013092041
Batch 20/64 loss: -0.3557499051094055
Batch 21/64 loss: -0.3134807348251343
Batch 22/64 loss: -0.3683076500892639
Batch 23/64 loss: -0.37463313341140747
Batch 24/64 loss: -0.3816782832145691
Batch 25/64 loss: -0.35475003719329834
Batch 26/64 loss: -0.37600624561309814
Batch 27/64 loss: -0.35489460825920105
Batch 28/64 loss: -0.33250629901885986
Batch 29/64 loss: -0.3775167465209961
Batch 30/64 loss: -0.3064992427825928
Batch 31/64 loss: -0.37106192111968994
Batch 32/64 loss: -0.3293234407901764
Batch 33/64 loss: -0.37974926829338074
Batch 34/64 loss: -0.3261721730232239
Batch 35/64 loss: -0.3665813207626343
Batch 36/64 loss: -0.3413994312286377
Batch 37/64 loss: -0.36198699474334717
Batch 38/64 loss: -0.35073286294937134
Batch 39/64 loss: -0.3809010982513428
Batch 40/64 loss: -0.33162128925323486
Batch 41/64 loss: -0.3609943687915802
Batch 42/64 loss: -0.37034714221954346
Batch 43/64 loss: -0.39075228571891785
Batch 44/64 loss: -0.350949764251709
Batch 45/64 loss: -0.33877289295196533
Batch 46/64 loss: -0.35989367961883545
Batch 47/64 loss: -0.3854562044143677
Batch 48/64 loss: -0.3373795747756958
Batch 49/64 loss: -0.34924328327178955
Batch 50/64 loss: -0.37524715065956116
Batch 51/64 loss: -0.36990201473236084
Batch 52/64 loss: -0.32368507981300354
Batch 53/64 loss: -0.354427695274353
Batch 54/64 loss: -0.3432770371437073
Batch 55/64 loss: -0.3462325632572174
Batch 56/64 loss: -0.3622097969055176
Batch 57/64 loss: -0.3555424213409424
Batch 58/64 loss: -0.35580897331237793
Batch 59/64 loss: -0.37782907485961914
Batch 60/64 loss: -0.3268820643424988
Batch 61/64 loss: -0.3934265375137329
Batch 62/64 loss: -0.32043981552124023
Batch 63/64 loss: -0.3533136248588562
Batch 64/64 loss: -0.3438599109649658
Epoch 425  Train loss: -0.35710625601749796  Val loss: 0.03655271227007469
Epoch 426
-------------------------------
Batch 1/64 loss: -0.3615529239177704
Batch 2/64 loss: -0.3610909581184387
Batch 3/64 loss: -0.3581080436706543
Batch 4/64 loss: -0.35103780031204224
Batch 5/64 loss: -0.35068389773368835
Batch 6/64 loss: -0.3759364187717438
Batch 7/64 loss: -0.3927847743034363
Batch 8/64 loss: -0.34287166595458984
Batch 9/64 loss: -0.37322503328323364
Batch 10/64 loss: -0.3641928434371948
Batch 11/64 loss: -0.34714165329933167
Batch 12/64 loss: -0.36224353313446045
Batch 13/64 loss: -0.31388288736343384
Batch 14/64 loss: -0.37361425161361694
Batch 15/64 loss: -0.4019322395324707
Batch 16/64 loss: -0.3787332773208618
Batch 17/64 loss: -0.3350364565849304
Batch 18/64 loss: -0.3590390384197235
Batch 19/64 loss: -0.37786200642585754
Batch 20/64 loss: -0.3367205858230591
Batch 21/64 loss: -0.35717901587486267
Batch 22/64 loss: -0.3783581554889679
Batch 23/64 loss: -0.3609863817691803
Batch 24/64 loss: -0.3613675832748413
Batch 25/64 loss: -0.31474602222442627
Batch 26/64 loss: -0.30394214391708374
Batch 27/64 loss: -0.34223634004592896
Batch 28/64 loss: -0.33870112895965576
Batch 29/64 loss: -0.35460707545280457
Batch 30/64 loss: -0.3392045497894287
Batch 31/64 loss: -0.37886232137680054
Batch 32/64 loss: -0.35969048738479614
Batch 33/64 loss: -0.34583812952041626
Batch 34/64 loss: -0.35146817564964294
Batch 35/64 loss: -0.3213125467300415
Batch 36/64 loss: -0.3558371663093567
Batch 37/64 loss: -0.3693634867668152
Batch 38/64 loss: -0.3361286520957947
Batch 39/64 loss: -0.3345823287963867
Batch 40/64 loss: -0.3371899724006653
Batch 41/64 loss: -0.3676144480705261
Batch 42/64 loss: -0.34881845116615295
Batch 43/64 loss: -0.35382694005966187
Batch 44/64 loss: -0.3656734228134155
Batch 45/64 loss: -0.377046138048172
Batch 46/64 loss: -0.3312622308731079
Batch 47/64 loss: -0.3460569381713867
Batch 48/64 loss: -0.34032148122787476
Batch 49/64 loss: -0.3401634395122528
Batch 50/64 loss: -0.3852805495262146
Batch 51/64 loss: -0.3220027983188629
Batch 52/64 loss: -0.3416890501976013
Batch 53/64 loss: -0.3667939305305481
Batch 54/64 loss: -0.38497626781463623
Batch 55/64 loss: -0.35146522521972656
Batch 56/64 loss: -0.34328606724739075
Batch 57/64 loss: -0.3794199824333191
Batch 58/64 loss: -0.32502245903015137
Batch 59/64 loss: -0.3774452209472656
Batch 60/64 loss: -0.3713269829750061
Batch 61/64 loss: -0.3499450087547302
Batch 62/64 loss: -0.34944820404052734
Batch 63/64 loss: -0.3291277587413788
Batch 64/64 loss: -0.3585125207901001
Epoch 426  Train loss: -0.35413630335938695  Val loss: 0.03604640891052194
Epoch 427
-------------------------------
Batch 1/64 loss: -0.361478716135025
Batch 2/64 loss: -0.3239058554172516
Batch 3/64 loss: -0.3307028114795685
Batch 4/64 loss: -0.38164108991622925
Batch 5/64 loss: -0.34770119190216064
Batch 6/64 loss: -0.3772748112678528
Batch 7/64 loss: -0.32850906252861023
Batch 8/64 loss: -0.3298271894454956
Batch 9/64 loss: -0.37664908170700073
Batch 10/64 loss: -0.371867835521698
Batch 11/64 loss: -0.3669552206993103
Batch 12/64 loss: -0.3698599934577942
Batch 13/64 loss: -0.3822571337223053
Batch 14/64 loss: -0.3446139097213745
Batch 15/64 loss: -0.3710363209247589
Batch 16/64 loss: -0.36760953068733215
Batch 17/64 loss: -0.34786951541900635
Batch 18/64 loss: -0.3613128662109375
Batch 19/64 loss: -0.3711545467376709
Batch 20/64 loss: -0.3497355580329895
Batch 21/64 loss: -0.37391889095306396
Batch 22/64 loss: -0.36038193106651306
Batch 23/64 loss: -0.3736589550971985
Batch 24/64 loss: -0.35423630475997925
Batch 25/64 loss: -0.35344967246055603
Batch 26/64 loss: -0.37496697902679443
Batch 27/64 loss: -0.3908321261405945
Batch 28/64 loss: -0.3592328131198883
Batch 29/64 loss: -0.35511359572410583
Batch 30/64 loss: -0.3587101995944977
Batch 31/64 loss: -0.32965177297592163
Batch 32/64 loss: -0.3624563217163086
Batch 33/64 loss: -0.3394104838371277
Batch 34/64 loss: -0.359552800655365
Batch 35/64 loss: -0.33025670051574707
Batch 36/64 loss: -0.3790864944458008
Batch 37/64 loss: -0.3516557216644287
Batch 38/64 loss: -0.37427759170532227
Batch 39/64 loss: -0.34727248549461365
Batch 40/64 loss: -0.3695271909236908
Batch 41/64 loss: -0.3821951746940613
Batch 42/64 loss: -0.354419469833374
Batch 43/64 loss: -0.36308616399765015
Batch 44/64 loss: -0.36579883098602295
Batch 45/64 loss: -0.3684965968132019
Batch 46/64 loss: -0.3822449743747711
Batch 47/64 loss: -0.34212440252304077
Batch 48/64 loss: -0.34210288524627686
Batch 49/64 loss: -0.33259648084640503
Batch 50/64 loss: -0.36083731055259705
Batch 51/64 loss: -0.33740532398223877
Batch 52/64 loss: -0.3510061502456665
Batch 53/64 loss: -0.29755914211273193
Batch 54/64 loss: -0.40073269605636597
Batch 55/64 loss: -0.2883349061012268
Batch 56/64 loss: -0.34754395484924316
Batch 57/64 loss: -0.35557636618614197
Batch 58/64 loss: -0.35954397916793823
Batch 59/64 loss: -0.3822277784347534
Batch 60/64 loss: -0.338321715593338
Batch 61/64 loss: -0.37034615874290466
Batch 62/64 loss: -0.39339393377304077
Batch 63/64 loss: -0.35367274284362793
Batch 64/64 loss: -0.3019742965698242
Epoch 427  Train loss: -0.3569515158148373  Val loss: 0.035437880512775015
Epoch 428
-------------------------------
Batch 1/64 loss: -0.3636940121650696
Batch 2/64 loss: -0.3761710524559021
Batch 3/64 loss: -0.3477064371109009
Batch 4/64 loss: -0.3760572671890259
Batch 5/64 loss: -0.4048111140727997
Batch 6/64 loss: -0.3388381600379944
Batch 7/64 loss: -0.40066421031951904
Batch 8/64 loss: -0.37496429681777954
Batch 9/64 loss: -0.35945236682891846
Batch 10/64 loss: -0.39697545766830444
Batch 11/64 loss: -0.34048590064048767
Batch 12/64 loss: -0.3573194742202759
Batch 13/64 loss: -0.3659021258354187
Batch 14/64 loss: -0.33749163150787354
Batch 15/64 loss: -0.38439494371414185
Batch 16/64 loss: -0.3111709952354431
Batch 17/64 loss: -0.3680320978164673
Batch 18/64 loss: -0.34913238883018494
Batch 19/64 loss: -0.39643430709838867
Batch 20/64 loss: -0.340908944606781
Batch 21/64 loss: -0.3088962733745575
Batch 22/64 loss: -0.3654227554798126
Batch 23/64 loss: -0.36072683334350586
Batch 24/64 loss: -0.34440553188323975
Batch 25/64 loss: -0.3629370331764221
Batch 26/64 loss: -0.30323898792266846
Batch 27/64 loss: -0.3289846181869507
Batch 28/64 loss: -0.37207216024398804
Batch 29/64 loss: -0.369692862033844
Batch 30/64 loss: -0.34628480672836304
Batch 31/64 loss: -0.36647287011146545
Batch 32/64 loss: -0.33952146768569946
Batch 33/64 loss: -0.37381911277770996
Batch 34/64 loss: -0.35729002952575684
Batch 35/64 loss: -0.37533700466156006
Batch 36/64 loss: -0.3163333535194397
Batch 37/64 loss: -0.3365142047405243
Batch 38/64 loss: -0.3465743064880371
Batch 39/64 loss: -0.3629448413848877
Batch 40/64 loss: -0.31063467264175415
Batch 41/64 loss: -0.36345595121383667
Batch 42/64 loss: -0.3476961851119995
Batch 43/64 loss: -0.39476412534713745
Batch 44/64 loss: -0.36881884932518005
Batch 45/64 loss: -0.37958550453186035
Batch 46/64 loss: -0.3410782814025879
Batch 47/64 loss: -0.3474537134170532
Batch 48/64 loss: -0.37741756439208984
Batch 49/64 loss: -0.36227455735206604
Batch 50/64 loss: -0.3525688946247101
Batch 51/64 loss: -0.3749961853027344
Batch 52/64 loss: -0.36128947138786316
Batch 53/64 loss: -0.3639271855354309
Batch 54/64 loss: -0.3654799461364746
Batch 55/64 loss: -0.37463200092315674
Batch 56/64 loss: -0.37256038188934326
Batch 57/64 loss: -0.381987065076828
Batch 58/64 loss: -0.3738301396369934
Batch 59/64 loss: -0.3212331533432007
Batch 60/64 loss: -0.3657185137271881
Batch 61/64 loss: -0.32578766345977783
Batch 62/64 loss: -0.35570693016052246
Batch 63/64 loss: -0.36426469683647156
Batch 64/64 loss: -0.3287997245788574
Epoch 428  Train loss: -0.3579896104102041  Val loss: 0.033867003786604837
Epoch 429
-------------------------------
Batch 1/64 loss: -0.34028711915016174
Batch 2/64 loss: -0.35932594537734985
Batch 3/64 loss: -0.34935760498046875
Batch 4/64 loss: -0.3657265603542328
Batch 5/64 loss: -0.3462621867656708
Batch 6/64 loss: -0.3328472375869751
Batch 7/64 loss: -0.40364211797714233
Batch 8/64 loss: -0.3372343182563782
Batch 9/64 loss: -0.3797686994075775
Batch 10/64 loss: -0.3496209979057312
Batch 11/64 loss: -0.36814361810684204
Batch 12/64 loss: -0.38092976808547974
Batch 13/64 loss: -0.35513120889663696
Batch 14/64 loss: -0.3532784581184387
Batch 15/64 loss: -0.38245874643325806
Batch 16/64 loss: -0.3511491119861603
Batch 17/64 loss: -0.39297711849212646
Batch 18/64 loss: -0.35628455877304077
Batch 19/64 loss: -0.3762091398239136
Batch 20/64 loss: -0.3427802324295044
Batch 21/64 loss: -0.35841405391693115
Batch 22/64 loss: -0.34216275811195374
Batch 23/64 loss: -0.35568714141845703
Batch 24/64 loss: -0.3726719915866852
Batch 25/64 loss: -0.3898455500602722
Batch 26/64 loss: -0.3392447531223297
Batch 27/64 loss: -0.3952874541282654
Batch 28/64 loss: -0.3528304696083069
Batch 29/64 loss: -0.36899736523628235
Batch 30/64 loss: -0.3385864496231079
Batch 31/64 loss: -0.3655783236026764
Batch 32/64 loss: -0.3539465069770813
Batch 33/64 loss: -0.3733213245868683
Batch 34/64 loss: -0.3352115750312805
Batch 35/64 loss: -0.32506006956100464
Batch 36/64 loss: -0.38550347089767456
Batch 37/64 loss: -0.36073288321495056
Batch 38/64 loss: -0.3714027404785156
Batch 39/64 loss: -0.3158857226371765
Batch 40/64 loss: -0.36641383171081543
Batch 41/64 loss: -0.29418572783470154
Batch 42/64 loss: -0.3617062568664551
Batch 43/64 loss: -0.3580036163330078
Batch 44/64 loss: -0.34435248374938965
Batch 45/64 loss: -0.3257153630256653
Batch 46/64 loss: -0.34940946102142334
Batch 47/64 loss: -0.3562043309211731
Batch 48/64 loss: -0.3689483404159546
Batch 49/64 loss: -0.36914390325546265
Batch 50/64 loss: -0.3240438401699066
Batch 51/64 loss: -0.3705487847328186
Batch 52/64 loss: -0.3488468527793884
Batch 53/64 loss: -0.36336803436279297
Batch 54/64 loss: -0.3682863116264343
Batch 55/64 loss: -0.3310002088546753
Batch 56/64 loss: -0.38495004177093506
Batch 57/64 loss: -0.3611186146736145
Batch 58/64 loss: -0.3784600794315338
Batch 59/64 loss: -0.3662378191947937
Batch 60/64 loss: -0.3752376139163971
Batch 61/64 loss: -0.3964490294456482
Batch 62/64 loss: -0.3574260473251343
Batch 63/64 loss: -0.3684154152870178
Batch 64/64 loss: -0.34418439865112305
Epoch 429  Train loss: -0.35875130447686887  Val loss: 0.032340901004489754
Epoch 430
-------------------------------
Batch 1/64 loss: -0.36719030141830444
Batch 2/64 loss: -0.3551687002182007
Batch 3/64 loss: -0.360934853553772
Batch 4/64 loss: -0.3534913659095764
Batch 5/64 loss: -0.3934812545776367
Batch 6/64 loss: -0.39110296964645386
Batch 7/64 loss: -0.36327046155929565
Batch 8/64 loss: -0.34800466895103455
Batch 9/64 loss: -0.36642593145370483
Batch 10/64 loss: -0.38701677322387695
Batch 11/64 loss: -0.3436669707298279
Batch 12/64 loss: -0.3592449426651001
Batch 13/64 loss: -0.3267732262611389
Batch 14/64 loss: -0.3434062898159027
Batch 15/64 loss: -0.32214784622192383
Batch 16/64 loss: -0.34820854663848877
Batch 17/64 loss: -0.344199001789093
Batch 18/64 loss: -0.3287205398082733
Batch 19/64 loss: -0.3392346203327179
Batch 20/64 loss: -0.3753855228424072
Batch 21/64 loss: -0.3749345541000366
Batch 22/64 loss: -0.34763479232788086
Batch 23/64 loss: -0.366228848695755
Batch 24/64 loss: -0.3804613947868347
Batch 25/64 loss: -0.39117616415023804
Batch 26/64 loss: -0.33743929862976074
Batch 27/64 loss: -0.40343189239501953
Batch 28/64 loss: -0.29254981875419617
Batch 29/64 loss: -0.3487333059310913
Batch 30/64 loss: -0.3270877003669739
Batch 31/64 loss: -0.3568763732910156
Batch 32/64 loss: -0.3880980610847473
Batch 33/64 loss: -0.3313719928264618
Batch 34/64 loss: -0.3670070171356201
Batch 35/64 loss: -0.37918907403945923
Batch 36/64 loss: -0.384522020816803
Batch 37/64 loss: -0.3277016282081604
Batch 38/64 loss: -0.3601740002632141
Batch 39/64 loss: -0.3564905524253845
Batch 40/64 loss: -0.36924871802330017
Batch 41/64 loss: -0.3754343092441559
Batch 42/64 loss: -0.3638216257095337
Batch 43/64 loss: -0.3491906523704529
Batch 44/64 loss: -0.3441588282585144
Batch 45/64 loss: -0.3597015142440796
Batch 46/64 loss: -0.352999210357666
Batch 47/64 loss: -0.35516083240509033
Batch 48/64 loss: -0.3599447011947632
Batch 49/64 loss: -0.3549325168132782
Batch 50/64 loss: -0.3199201226234436
Batch 51/64 loss: -0.37763792276382446
Batch 52/64 loss: -0.34297290444374084
Batch 53/64 loss: -0.3788909316062927
Batch 54/64 loss: -0.3662225008010864
Batch 55/64 loss: -0.3366595506668091
Batch 56/64 loss: -0.3719692528247833
Batch 57/64 loss: -0.36054790019989014
Batch 58/64 loss: -0.3669857382774353
Batch 59/64 loss: -0.3393236994743347
Batch 60/64 loss: -0.34073030948638916
Batch 61/64 loss: -0.39741647243499756
Batch 62/64 loss: -0.3954654633998871
Batch 63/64 loss: -0.3512067496776581
Batch 64/64 loss: -0.3240024149417877
Epoch 430  Train loss: -0.35783101986436283  Val loss: 0.035510444968836416
Epoch 431
-------------------------------
Batch 1/64 loss: -0.38478583097457886
Batch 2/64 loss: -0.39750632643699646
Batch 3/64 loss: -0.40440666675567627
Batch 4/64 loss: -0.36701202392578125
Batch 5/64 loss: -0.3627296984195709
Batch 6/64 loss: -0.384052574634552
Batch 7/64 loss: -0.3482985496520996
Batch 8/64 loss: -0.3791477084159851
Batch 9/64 loss: -0.3541776239871979
Batch 10/64 loss: -0.3625413179397583
Batch 11/64 loss: -0.3803054392337799
Batch 12/64 loss: -0.3383635878562927
Batch 13/64 loss: -0.3239503502845764
Batch 14/64 loss: -0.3550220727920532
Batch 15/64 loss: -0.38754016160964966
Batch 16/64 loss: -0.3433510661125183
Batch 17/64 loss: -0.3552019000053406
Batch 18/64 loss: -0.28881722688674927
Batch 19/64 loss: -0.35269802808761597
Batch 20/64 loss: -0.33855849504470825
Batch 21/64 loss: -0.38084250688552856
Batch 22/64 loss: -0.3510819971561432
Batch 23/64 loss: -0.3619794547557831
Batch 24/64 loss: -0.35995644330978394
Batch 25/64 loss: -0.3360881209373474
Batch 26/64 loss: -0.3292931318283081
Batch 27/64 loss: -0.3638768792152405
Batch 28/64 loss: -0.3714609742164612
Batch 29/64 loss: -0.38040053844451904
Batch 30/64 loss: -0.3462972044944763
Batch 31/64 loss: -0.34347832202911377
Batch 32/64 loss: -0.36962854862213135
Batch 33/64 loss: -0.38212233781814575
Batch 34/64 loss: -0.37053877115249634
Batch 35/64 loss: -0.35403281450271606
Batch 36/64 loss: -0.3835042715072632
Batch 37/64 loss: -0.35872191190719604
Batch 38/64 loss: -0.2863147258758545
Batch 39/64 loss: -0.3630814552307129
Batch 40/64 loss: -0.3485226035118103
Batch 41/64 loss: -0.37792181968688965
Batch 42/64 loss: -0.37611812353134155
Batch 43/64 loss: -0.339793860912323
Batch 44/64 loss: -0.36793872714042664
Batch 45/64 loss: -0.35475072264671326
Batch 46/64 loss: -0.3495897054672241
Batch 47/64 loss: -0.3529338836669922
Batch 48/64 loss: -0.35043665766716003
Batch 49/64 loss: -0.3858439326286316
Batch 50/64 loss: -0.34719979763031006
Batch 51/64 loss: -0.32027575373649597
Batch 52/64 loss: -0.3573658764362335
Batch 53/64 loss: -0.39094018936157227
Batch 54/64 loss: -0.3655121624469757
Batch 55/64 loss: -0.36599209904670715
Batch 56/64 loss: -0.38121992349624634
Batch 57/64 loss: -0.33639445900917053
Batch 58/64 loss: -0.3936735987663269
Batch 59/64 loss: -0.39216744899749756
Batch 60/64 loss: -0.36267513036727905
Batch 61/64 loss: -0.3754574954509735
Batch 62/64 loss: -0.3529007136821747
Batch 63/64 loss: -0.3343219757080078
Batch 64/64 loss: -0.32336190342903137
Epoch 431  Train loss: -0.3595864336864621  Val loss: 0.034832255332330656
Epoch 432
-------------------------------
Batch 1/64 loss: -0.396978497505188
Batch 2/64 loss: -0.3642481863498688
Batch 3/64 loss: -0.3853346109390259
Batch 4/64 loss: -0.3576568365097046
Batch 5/64 loss: -0.3542291522026062
Batch 6/64 loss: -0.4108300507068634
Batch 7/64 loss: -0.33109450340270996
Batch 8/64 loss: -0.3643523156642914
Batch 9/64 loss: -0.3521902561187744
Batch 10/64 loss: -0.36410796642303467
Batch 11/64 loss: -0.3598863184452057
Batch 12/64 loss: -0.3553082346916199
Batch 13/64 loss: -0.3046848773956299
Batch 14/64 loss: -0.30396556854248047
Batch 15/64 loss: -0.3047533631324768
Batch 16/64 loss: -0.3768528699874878
Batch 17/64 loss: -0.3583340048789978
Batch 18/64 loss: -0.3560994863510132
Batch 19/64 loss: -0.37731173634529114
Batch 20/64 loss: -0.3775874376296997
Batch 21/64 loss: -0.3322001099586487
Batch 22/64 loss: -0.35894232988357544
Batch 23/64 loss: -0.32914185523986816
Batch 24/64 loss: -0.369804322719574
Batch 25/64 loss: -0.3538208603858948
Batch 26/64 loss: -0.36705124378204346
Batch 27/64 loss: -0.3605246841907501
Batch 28/64 loss: -0.37171927094459534
Batch 29/64 loss: -0.34991973638534546
Batch 30/64 loss: -0.3550315499305725
Batch 31/64 loss: -0.3434610962867737
Batch 32/64 loss: -0.38682156801223755
Batch 33/64 loss: -0.3904056251049042
Batch 34/64 loss: -0.36404532194137573
Batch 35/64 loss: -0.3543738126754761
Batch 36/64 loss: -0.3084966242313385
Batch 37/64 loss: -0.36357763409614563
Batch 38/64 loss: -0.34866440296173096
Batch 39/64 loss: -0.37361785769462585
Batch 40/64 loss: -0.3818822503089905
Batch 41/64 loss: -0.37982863187789917
Batch 42/64 loss: -0.3646913170814514
Batch 43/64 loss: -0.33762234449386597
Batch 44/64 loss: -0.36898723244667053
Batch 45/64 loss: -0.38058409094810486
Batch 46/64 loss: -0.3531370162963867
Batch 47/64 loss: -0.3371639847755432
Batch 48/64 loss: -0.3444103002548218
Batch 49/64 loss: -0.3700357973575592
Batch 50/64 loss: -0.3733016848564148
Batch 51/64 loss: -0.38771772384643555
Batch 52/64 loss: -0.3985769748687744
Batch 53/64 loss: -0.34929999709129333
Batch 54/64 loss: -0.36727389693260193
Batch 55/64 loss: -0.3651745021343231
Batch 56/64 loss: -0.3650503158569336
Batch 57/64 loss: -0.3728944659233093
Batch 58/64 loss: -0.36439192295074463
Batch 59/64 loss: -0.3849066495895386
Batch 60/64 loss: -0.35899484157562256
Batch 61/64 loss: -0.3407660126686096
Batch 62/64 loss: -0.3488445281982422
Batch 63/64 loss: -0.3316517472267151
Batch 64/64 loss: -0.3453707993030548
Epoch 432  Train loss: -0.35958654915585236  Val loss: 0.03367021034673317
Epoch 433
-------------------------------
Batch 1/64 loss: -0.3640882074832916
Batch 2/64 loss: -0.3553909957408905
Batch 3/64 loss: -0.3651195466518402
Batch 4/64 loss: -0.31107982993125916
Batch 5/64 loss: -0.34159383177757263
Batch 6/64 loss: -0.3372333347797394
Batch 7/64 loss: -0.3492988646030426
Batch 8/64 loss: -0.34260761737823486
Batch 9/64 loss: -0.37617480754852295
Batch 10/64 loss: -0.35192006826400757
Batch 11/64 loss: -0.36761441826820374
Batch 12/64 loss: -0.36930978298187256
Batch 13/64 loss: -0.3514888286590576
Batch 14/64 loss: -0.3730708956718445
Batch 15/64 loss: -0.35768353939056396
Batch 16/64 loss: -0.3745753765106201
Batch 17/64 loss: -0.380420982837677
Batch 18/64 loss: -0.3730725646018982
Batch 19/64 loss: -0.3550371527671814
Batch 20/64 loss: -0.2867025136947632
Batch 21/64 loss: -0.2832595705986023
Batch 22/64 loss: -0.3300734758377075
Batch 23/64 loss: -0.3577611744403839
Batch 24/64 loss: -0.3626802861690521
Batch 25/64 loss: -0.3683681786060333
Batch 26/64 loss: -0.4120926856994629
Batch 27/64 loss: -0.3641557991504669
Batch 28/64 loss: -0.3937067985534668
Batch 29/64 loss: -0.378154456615448
Batch 30/64 loss: -0.2596838176250458
Batch 31/64 loss: -0.34131187200546265
Batch 32/64 loss: -0.345852792263031
Batch 33/64 loss: -0.35807716846466064
Batch 34/64 loss: -0.3772536516189575
Batch 35/64 loss: -0.29685741662979126
Batch 36/64 loss: -0.3861442506313324
Batch 37/64 loss: -0.3759628236293793
Batch 38/64 loss: -0.3481532335281372
Batch 39/64 loss: -0.3750602602958679
Batch 40/64 loss: -0.34955811500549316
Batch 41/64 loss: -0.38510486483573914
Batch 42/64 loss: -0.37168586254119873
Batch 43/64 loss: -0.3644205331802368
Batch 44/64 loss: -0.3924271762371063
Batch 45/64 loss: -0.39982032775878906
Batch 46/64 loss: -0.3851165175437927
Batch 47/64 loss: -0.32169464230537415
Batch 48/64 loss: -0.3633858561515808
Batch 49/64 loss: -0.3795532286167145
Batch 50/64 loss: -0.3761076033115387
Batch 51/64 loss: -0.34561315178871155
Batch 52/64 loss: -0.40110939741134644
Batch 53/64 loss: -0.3630075454711914
Batch 54/64 loss: -0.3681477904319763
Batch 55/64 loss: -0.31317758560180664
Batch 56/64 loss: -0.36565154790878296
Batch 57/64 loss: -0.3799881339073181
Batch 58/64 loss: -0.3692229390144348
Batch 59/64 loss: -0.3520311713218689
Batch 60/64 loss: -0.37672170996665955
Batch 61/64 loss: -0.3631337285041809
Batch 62/64 loss: -0.37896132469177246
Batch 63/64 loss: -0.3604048788547516
Batch 64/64 loss: -0.360349178314209
Epoch 433  Train loss: -0.359127849223567  Val loss: 0.03490550759731699
Epoch 434
-------------------------------
Batch 1/64 loss: -0.3790377378463745
Batch 2/64 loss: -0.3491963744163513
Batch 3/64 loss: -0.34551841020584106
Batch 4/64 loss: -0.3981334865093231
Batch 5/64 loss: -0.36228251457214355
Batch 6/64 loss: -0.39029186964035034
Batch 7/64 loss: -0.3868861496448517
Batch 8/64 loss: -0.3434962332248688
Batch 9/64 loss: -0.33189913630485535
Batch 10/64 loss: -0.366854190826416
Batch 11/64 loss: -0.3555879592895508
Batch 12/64 loss: -0.3390022814273834
Batch 13/64 loss: -0.3601260185241699
Batch 14/64 loss: -0.3827841281890869
Batch 15/64 loss: -0.39566051959991455
Batch 16/64 loss: -0.3467557430267334
Batch 17/64 loss: -0.38325217366218567
Batch 18/64 loss: -0.3866809904575348
Batch 19/64 loss: -0.3634736239910126
Batch 20/64 loss: -0.37096452713012695
Batch 21/64 loss: -0.3896263837814331
Batch 22/64 loss: -0.3488161265850067
Batch 23/64 loss: -0.35168206691741943
Batch 24/64 loss: -0.39948225021362305
Batch 25/64 loss: -0.3711315095424652
Batch 26/64 loss: -0.3828069567680359
Batch 27/64 loss: -0.377627432346344
Batch 28/64 loss: -0.38444891571998596
Batch 29/64 loss: -0.3501160740852356
Batch 30/64 loss: -0.3547055423259735
Batch 31/64 loss: -0.3791618347167969
Batch 32/64 loss: -0.3452379107475281
Batch 33/64 loss: -0.3532276749610901
Batch 34/64 loss: -0.37685874104499817
Batch 35/64 loss: -0.39123421907424927
Batch 36/64 loss: -0.37774235010147095
Batch 37/64 loss: -0.37073707580566406
Batch 38/64 loss: -0.35730141401290894
Batch 39/64 loss: -0.37826672196388245
Batch 40/64 loss: -0.3922927975654602
Batch 41/64 loss: -0.34292078018188477
Batch 42/64 loss: -0.3777010440826416
Batch 43/64 loss: -0.37563249468803406
Batch 44/64 loss: -0.34283119440078735
Batch 45/64 loss: -0.3462601900100708
Batch 46/64 loss: -0.3563506007194519
Batch 47/64 loss: -0.3855271339416504
Batch 48/64 loss: -0.3486935496330261
Batch 49/64 loss: -0.31958824396133423
Batch 50/64 loss: -0.38640958070755005
Batch 51/64 loss: -0.36094024777412415
Batch 52/64 loss: -0.36596769094467163
Batch 53/64 loss: -0.39043891429901123
Batch 54/64 loss: -0.3429989218711853
Batch 55/64 loss: -0.3692125082015991
Batch 56/64 loss: -0.3380850553512573
Batch 57/64 loss: -0.36192601919174194
Batch 58/64 loss: -0.354569673538208
Batch 59/64 loss: -0.36798691749572754
Batch 60/64 loss: -0.32597815990448
Batch 61/64 loss: -0.3675016462802887
Batch 62/64 loss: -0.3086096942424774
Batch 63/64 loss: -0.3407145142555237
Batch 64/64 loss: -0.3177390694618225
Epoch 434  Train loss: -0.3636947003065371  Val loss: 0.03553186085625613
Epoch 435
-------------------------------
Batch 1/64 loss: -0.35514962673187256
Batch 2/64 loss: -0.36713671684265137
Batch 3/64 loss: -0.3791956901550293
Batch 4/64 loss: -0.3707033395767212
Batch 5/64 loss: -0.36615705490112305
Batch 6/64 loss: -0.3692537546157837
Batch 7/64 loss: -0.37546730041503906
Batch 8/64 loss: -0.362298846244812
Batch 9/64 loss: -0.35713472962379456
Batch 10/64 loss: -0.345794677734375
Batch 11/64 loss: -0.3488052487373352
Batch 12/64 loss: -0.3837926387786865
Batch 13/64 loss: -0.33987289667129517
Batch 14/64 loss: -0.3332509994506836
Batch 15/64 loss: -0.37373149394989014
Batch 16/64 loss: -0.36666837334632874
Batch 17/64 loss: -0.3725113272666931
Batch 18/64 loss: -0.3934028148651123
Batch 19/64 loss: -0.37294644117355347
Batch 20/64 loss: -0.3133915960788727
Batch 21/64 loss: -0.36244112253189087
Batch 22/64 loss: -0.38054418563842773
Batch 23/64 loss: -0.3622114360332489
Batch 24/64 loss: -0.37096428871154785
Batch 25/64 loss: -0.35207700729370117
Batch 26/64 loss: -0.35838156938552856
Batch 27/64 loss: -0.3393329977989197
Batch 28/64 loss: -0.37502244114875793
Batch 29/64 loss: -0.3606210947036743
Batch 30/64 loss: -0.39208167791366577
Batch 31/64 loss: -0.3607417047023773
Batch 32/64 loss: -0.4002121686935425
Batch 33/64 loss: -0.3605762720108032
Batch 34/64 loss: -0.3552740812301636
Batch 35/64 loss: -0.37921780347824097
Batch 36/64 loss: -0.3831138610839844
Batch 37/64 loss: -0.3735259771347046
Batch 38/64 loss: -0.33009785413742065
Batch 39/64 loss: -0.36455869674682617
Batch 40/64 loss: -0.35408374667167664
Batch 41/64 loss: -0.32649466395378113
Batch 42/64 loss: -0.3636583685874939
Batch 43/64 loss: -0.3353555202484131
Batch 44/64 loss: -0.37793219089508057
Batch 45/64 loss: -0.3644510507583618
Batch 46/64 loss: -0.35338664054870605
Batch 47/64 loss: -0.3457608222961426
Batch 48/64 loss: -0.3797227442264557
Batch 49/64 loss: -0.34507057070732117
Batch 50/64 loss: -0.35917454957962036
Batch 51/64 loss: -0.3546878695487976
Batch 52/64 loss: -0.3775419592857361
Batch 53/64 loss: -0.3659228980541229
Batch 54/64 loss: -0.3213270306587219
Batch 55/64 loss: -0.3103477656841278
Batch 56/64 loss: -0.3435642123222351
Batch 57/64 loss: -0.35632622241973877
Batch 58/64 loss: -0.36899545788764954
Batch 59/64 loss: -0.32853183150291443
Batch 60/64 loss: -0.3591514825820923
Batch 61/64 loss: -0.35780033469200134
Batch 62/64 loss: -0.36173224449157715
Batch 63/64 loss: -0.3656490445137024
Batch 64/64 loss: -0.23311328887939453
Epoch 435  Train loss: -0.3585751528833427  Val loss: 0.035883145643673404
Epoch 436
-------------------------------
Batch 1/64 loss: -0.3360821604728699
Batch 2/64 loss: -0.3320596218109131
Batch 3/64 loss: -0.35868579149246216
Batch 4/64 loss: -0.3356751799583435
Batch 5/64 loss: -0.3769907057285309
Batch 6/64 loss: -0.35495585203170776
Batch 7/64 loss: -0.3770463168621063
Batch 8/64 loss: -0.38940325379371643
Batch 9/64 loss: -0.31606340408325195
Batch 10/64 loss: -0.33886581659317017
Batch 11/64 loss: -0.36268115043640137
Batch 12/64 loss: -0.3564881682395935
Batch 13/64 loss: -0.3190467357635498
Batch 14/64 loss: -0.3310832381248474
Batch 15/64 loss: -0.32432588934898376
Batch 16/64 loss: -0.31826478242874146
Batch 17/64 loss: -0.3489609956741333
Batch 18/64 loss: -0.36506474018096924
Batch 19/64 loss: -0.3820003867149353
Batch 20/64 loss: -0.35883212089538574
Batch 21/64 loss: -0.3831196427345276
Batch 22/64 loss: -0.3635556399822235
Batch 23/64 loss: -0.3711894154548645
Batch 24/64 loss: -0.37068384885787964
Batch 25/64 loss: -0.35045164823532104
Batch 26/64 loss: -0.3550529181957245
Batch 27/64 loss: -0.37225088477134705
Batch 28/64 loss: -0.3928256928920746
Batch 29/64 loss: -0.35541975498199463
Batch 30/64 loss: -0.33609551191329956
Batch 31/64 loss: -0.3346802294254303
Batch 32/64 loss: -0.354370653629303
Batch 33/64 loss: -0.37944525480270386
Batch 34/64 loss: -0.3381838798522949
Batch 35/64 loss: -0.3639774024486542
Batch 36/64 loss: -0.3857762813568115
Batch 37/64 loss: -0.33351239562034607
Batch 38/64 loss: -0.3535063862800598
Batch 39/64 loss: -0.33005645871162415
Batch 40/64 loss: -0.3258059620857239
Batch 41/64 loss: -0.3773822784423828
Batch 42/64 loss: -0.36886894702911377
Batch 43/64 loss: -0.3802531957626343
Batch 44/64 loss: -0.3680940270423889
Batch 45/64 loss: -0.3457219898700714
Batch 46/64 loss: -0.35168424248695374
Batch 47/64 loss: -0.3457663655281067
Batch 48/64 loss: -0.3439636826515198
Batch 49/64 loss: -0.3807458281517029
Batch 50/64 loss: -0.3805572986602783
Batch 51/64 loss: -0.3872509300708771
Batch 52/64 loss: -0.3634381890296936
Batch 53/64 loss: -0.36558058857917786
Batch 54/64 loss: -0.36825764179229736
Batch 55/64 loss: -0.36461058259010315
Batch 56/64 loss: -0.3873600363731384
Batch 57/64 loss: -0.3685944974422455
Batch 58/64 loss: -0.2954747974872589
Batch 59/64 loss: -0.3611634373664856
Batch 60/64 loss: -0.37683355808258057
Batch 61/64 loss: -0.3554631769657135
Batch 62/64 loss: -0.3395431637763977
Batch 63/64 loss: -0.3776341676712036
Batch 64/64 loss: -0.3697148263454437
Epoch 436  Train loss: -0.3570833712231879  Val loss: 0.03463487977424438
Epoch 437
-------------------------------
Batch 1/64 loss: -0.40141335129737854
Batch 2/64 loss: -0.35410061478614807
Batch 3/64 loss: -0.3808547556400299
Batch 4/64 loss: -0.3627687096595764
Batch 5/64 loss: -0.3662837743759155
Batch 6/64 loss: -0.3752693831920624
Batch 7/64 loss: -0.37353456020355225
Batch 8/64 loss: -0.36920058727264404
Batch 9/64 loss: -0.34053945541381836
Batch 10/64 loss: -0.32857394218444824
Batch 11/64 loss: -0.3533686399459839
Batch 12/64 loss: -0.3065285086631775
Batch 13/64 loss: -0.36894410848617554
Batch 14/64 loss: -0.31915420293807983
Batch 15/64 loss: -0.3618817925453186
Batch 16/64 loss: -0.3841152489185333
Batch 17/64 loss: -0.38038378953933716
Batch 18/64 loss: -0.3664080500602722
Batch 19/64 loss: -0.3212294280529022
Batch 20/64 loss: -0.36621326208114624
Batch 21/64 loss: -0.3431178331375122
Batch 22/64 loss: -0.3863333761692047
Batch 23/64 loss: -0.37853777408599854
Batch 24/64 loss: -0.3610441982746124
Batch 25/64 loss: -0.3497653007507324
Batch 26/64 loss: -0.36906060576438904
Batch 27/64 loss: -0.3667565584182739
Batch 28/64 loss: -0.3987041413784027
Batch 29/64 loss: -0.3891744613647461
Batch 30/64 loss: -0.393970787525177
Batch 31/64 loss: -0.3753555119037628
Batch 32/64 loss: -0.36583974957466125
Batch 33/64 loss: -0.3399214744567871
Batch 34/64 loss: -0.3851478099822998
Batch 35/64 loss: -0.3801910877227783
Batch 36/64 loss: -0.3482885956764221
Batch 37/64 loss: -0.3527863025665283
Batch 38/64 loss: -0.3715866804122925
Batch 39/64 loss: -0.38085806369781494
Batch 40/64 loss: -0.35226088762283325
Batch 41/64 loss: -0.3655105531215668
Batch 42/64 loss: -0.3616538643836975
Batch 43/64 loss: -0.35271453857421875
Batch 44/64 loss: -0.3719768226146698
Batch 45/64 loss: -0.3571512699127197
Batch 46/64 loss: -0.3379470109939575
Batch 47/64 loss: -0.35933491587638855
Batch 48/64 loss: -0.3479178547859192
Batch 49/64 loss: -0.3774120807647705
Batch 50/64 loss: -0.34422141313552856
Batch 51/64 loss: -0.387136846780777
Batch 52/64 loss: -0.38005530834198
Batch 53/64 loss: -0.36982959508895874
Batch 54/64 loss: -0.3837639391422272
Batch 55/64 loss: -0.3696187138557434
Batch 56/64 loss: -0.3262987732887268
Batch 57/64 loss: -0.36267632246017456
Batch 58/64 loss: -0.32388681173324585
Batch 59/64 loss: -0.3584381937980652
Batch 60/64 loss: -0.35679036378860474
Batch 61/64 loss: -0.3731551170349121
Batch 62/64 loss: -0.33358362317085266
Batch 63/64 loss: -0.36828339099884033
Batch 64/64 loss: -0.3620172142982483
Epoch 437  Train loss: -0.3625150996095994  Val loss: 0.03503097455526136
Epoch 438
-------------------------------
Batch 1/64 loss: -0.30638766288757324
Batch 2/64 loss: -0.36799687147140503
Batch 3/64 loss: -0.35908955335617065
Batch 4/64 loss: -0.3883086144924164
Batch 5/64 loss: -0.34896981716156006
Batch 6/64 loss: -0.37728720903396606
Batch 7/64 loss: -0.36753061413764954
Batch 8/64 loss: -0.3521516025066376
Batch 9/64 loss: -0.3615450859069824
Batch 10/64 loss: -0.37933677434921265
Batch 11/64 loss: -0.3541910648345947
Batch 12/64 loss: -0.3558524250984192
Batch 13/64 loss: -0.361611545085907
Batch 14/64 loss: -0.34968793392181396
Batch 15/64 loss: -0.3893023133277893
Batch 16/64 loss: -0.3790910840034485
Batch 17/64 loss: -0.3843141794204712
Batch 18/64 loss: -0.33715611696243286
Batch 19/64 loss: -0.3902663290500641
Batch 20/64 loss: -0.31493857502937317
Batch 21/64 loss: -0.3610990047454834
Batch 22/64 loss: -0.39142462611198425
Batch 23/64 loss: -0.33373504877090454
Batch 24/64 loss: -0.35981035232543945
Batch 25/64 loss: -0.37626534700393677
Batch 26/64 loss: -0.37125101685523987
Batch 27/64 loss: -0.3174525499343872
Batch 28/64 loss: -0.372552752494812
Batch 29/64 loss: -0.36639273166656494
Batch 30/64 loss: -0.3676881790161133
Batch 31/64 loss: -0.39158758521080017
Batch 32/64 loss: -0.3465307354927063
Batch 33/64 loss: -0.3105023503303528
Batch 34/64 loss: -0.37157559394836426
Batch 35/64 loss: -0.344693660736084
Batch 36/64 loss: -0.38205569982528687
Batch 37/64 loss: -0.32216963171958923
Batch 38/64 loss: -0.37495073676109314
Batch 39/64 loss: -0.3536226749420166
Batch 40/64 loss: -0.35710862278938293
Batch 41/64 loss: -0.3430989980697632
Batch 42/64 loss: -0.3909018337726593
Batch 43/64 loss: -0.3639540374279022
Batch 44/64 loss: -0.3267001509666443
Batch 45/64 loss: -0.3554161787033081
Batch 46/64 loss: -0.3694482445716858
Batch 47/64 loss: -0.38056543469429016
Batch 48/64 loss: -0.3551507294178009
Batch 49/64 loss: -0.33903902769088745
Batch 50/64 loss: -0.35697507858276367
Batch 51/64 loss: -0.3580353856086731
Batch 52/64 loss: -0.3425898551940918
Batch 53/64 loss: -0.35417208075523376
Batch 54/64 loss: -0.35753172636032104
Batch 55/64 loss: -0.3290269076824188
Batch 56/64 loss: -0.37310484051704407
Batch 57/64 loss: -0.3691757917404175
Batch 58/64 loss: -0.38396114110946655
Batch 59/64 loss: -0.3804704546928406
Batch 60/64 loss: -0.3437575399875641
Batch 61/64 loss: -0.32938122749328613
Batch 62/64 loss: -0.3590052127838135
Batch 63/64 loss: -0.37230801582336426
Batch 64/64 loss: -0.3236405849456787
Epoch 438  Train loss: -0.3588076016482185  Val loss: 0.0342323909100798
Epoch 439
-------------------------------
Batch 1/64 loss: -0.3758849501609802
Batch 2/64 loss: -0.3636845350265503
Batch 3/64 loss: -0.3518204689025879
Batch 4/64 loss: -0.3877558708190918
Batch 5/64 loss: -0.3793524503707886
Batch 6/64 loss: -0.40781742334365845
Batch 7/64 loss: -0.35123366117477417
Batch 8/64 loss: -0.3859015107154846
Batch 9/64 loss: -0.3782953917980194
Batch 10/64 loss: -0.3328290581703186
Batch 11/64 loss: -0.3627636432647705
Batch 12/64 loss: -0.3900470733642578
Batch 13/64 loss: -0.4001796543598175
Batch 14/64 loss: -0.361819326877594
Batch 15/64 loss: -0.3430747985839844
Batch 16/64 loss: -0.3604673743247986
Batch 17/64 loss: -0.41023901104927063
Batch 18/64 loss: -0.3204653263092041
Batch 19/64 loss: -0.38755178451538086
Batch 20/64 loss: -0.3704058825969696
Batch 21/64 loss: -0.359914630651474
Batch 22/64 loss: -0.33167293667793274
Batch 23/64 loss: -0.34334808588027954
Batch 24/64 loss: -0.3829353451728821
Batch 25/64 loss: -0.35349735617637634
Batch 26/64 loss: -0.3544657230377197
Batch 27/64 loss: -0.37350979447364807
Batch 28/64 loss: -0.3217676877975464
Batch 29/64 loss: -0.36332347989082336
Batch 30/64 loss: -0.35388872027397156
Batch 31/64 loss: -0.34802210330963135
Batch 32/64 loss: -0.3701716661453247
Batch 33/64 loss: -0.38555648922920227
Batch 34/64 loss: -0.36685100197792053
Batch 35/64 loss: -0.3604373335838318
Batch 36/64 loss: -0.34353744983673096
Batch 37/64 loss: -0.3733353316783905
Batch 38/64 loss: -0.3938385844230652
Batch 39/64 loss: -0.3103558421134949
Batch 40/64 loss: -0.3810667395591736
Batch 41/64 loss: -0.39204907417297363
Batch 42/64 loss: -0.3660491704940796
Batch 43/64 loss: -0.37070581316947937
Batch 44/64 loss: -0.36276137828826904
Batch 45/64 loss: -0.37346506118774414
Batch 46/64 loss: -0.39666688442230225
Batch 47/64 loss: -0.38249704241752625
Batch 48/64 loss: -0.3600468635559082
Batch 49/64 loss: -0.34632885456085205
Batch 50/64 loss: -0.330607533454895
Batch 51/64 loss: -0.3686959147453308
Batch 52/64 loss: -0.3546927869319916
Batch 53/64 loss: -0.3590050935745239
Batch 54/64 loss: -0.303761750459671
Batch 55/64 loss: -0.36602213978767395
Batch 56/64 loss: -0.38345956802368164
Batch 57/64 loss: -0.3364362120628357
Batch 58/64 loss: -0.34057971835136414
Batch 59/64 loss: -0.38060349225997925
Batch 60/64 loss: -0.3491857945919037
Batch 61/64 loss: -0.35701513290405273
Batch 62/64 loss: -0.3323593735694885
Batch 63/64 loss: -0.30388081073760986
Batch 64/64 loss: -0.3436902165412903
Epoch 439  Train loss: -0.3618467235097698  Val loss: 0.037236731486631834
Epoch 440
-------------------------------
Batch 1/64 loss: -0.36543768644332886
Batch 2/64 loss: -0.31171107292175293
Batch 3/64 loss: -0.3520693778991699
Batch 4/64 loss: -0.3629065155982971
Batch 5/64 loss: -0.3497006893157959
Batch 6/64 loss: -0.35843369364738464
Batch 7/64 loss: -0.3491881489753723
Batch 8/64 loss: -0.3704056143760681
Batch 9/64 loss: -0.3772430419921875
Batch 10/64 loss: -0.36149996519088745
Batch 11/64 loss: -0.3194175064563751
Batch 12/64 loss: -0.3114464282989502
Batch 13/64 loss: -0.34827959537506104
Batch 14/64 loss: -0.36514660716056824
Batch 15/64 loss: -0.3319213390350342
Batch 16/64 loss: -0.40093994140625
Batch 17/64 loss: -0.3158223628997803
Batch 18/64 loss: -0.35883548855781555
Batch 19/64 loss: -0.37672650814056396
Batch 20/64 loss: -0.300653874874115
Batch 21/64 loss: -0.35813379287719727
Batch 22/64 loss: -0.35853081941604614
Batch 23/64 loss: -0.3806670606136322
Batch 24/64 loss: -0.3601158857345581
Batch 25/64 loss: -0.34560221433639526
Batch 26/64 loss: -0.3629891872406006
Batch 27/64 loss: -0.3826255202293396
Batch 28/64 loss: -0.32915735244750977
Batch 29/64 loss: -0.3628588616847992
Batch 30/64 loss: -0.39310258626937866
Batch 31/64 loss: -0.3609856963157654
Batch 32/64 loss: -0.39085423946380615
Batch 33/64 loss: -0.38131454586982727
Batch 34/64 loss: -0.3489431142807007
Batch 35/64 loss: -0.3422161936759949
Batch 36/64 loss: -0.36489802598953247
Batch 37/64 loss: -0.33164674043655396
Batch 38/64 loss: -0.36223673820495605
Batch 39/64 loss: -0.31873854994773865
Batch 40/64 loss: -0.37067466974258423
Batch 41/64 loss: -0.35030674934387207
Batch 42/64 loss: -0.3789799213409424
Batch 43/64 loss: -0.36968544125556946
Batch 44/64 loss: -0.38109809160232544
Batch 45/64 loss: -0.33488109707832336
Batch 46/64 loss: -0.3736461400985718
Batch 47/64 loss: -0.3946816921234131
Batch 48/64 loss: -0.36422204971313477
Batch 49/64 loss: -0.3274003267288208
Batch 50/64 loss: -0.4021047055721283
Batch 51/64 loss: -0.31991076469421387
Batch 52/64 loss: -0.37560200691223145
Batch 53/64 loss: -0.3860171437263489
Batch 54/64 loss: -0.3493957221508026
Batch 55/64 loss: -0.38210785388946533
Batch 56/64 loss: -0.3980540931224823
Batch 57/64 loss: -0.3933534026145935
Batch 58/64 loss: -0.37108054757118225
Batch 59/64 loss: -0.34644222259521484
Batch 60/64 loss: -0.3451213836669922
Batch 61/64 loss: -0.34633558988571167
Batch 62/64 loss: -0.3753337264060974
Batch 63/64 loss: -0.3482506573200226
Batch 64/64 loss: -0.3495272994041443
Epoch 440  Train loss: -0.3587487695263881  Val loss: 0.036094553282170774
Epoch 441
-------------------------------
Batch 1/64 loss: -0.38223493099212646
Batch 2/64 loss: -0.33551377058029175
Batch 3/64 loss: -0.32714346051216125
Batch 4/64 loss: -0.3378806412220001
Batch 5/64 loss: -0.35737431049346924
Batch 6/64 loss: -0.36310091614723206
Batch 7/64 loss: -0.36740052700042725
Batch 8/64 loss: -0.3658812642097473
Batch 9/64 loss: -0.3305265009403229
Batch 10/64 loss: -0.34669429063796997
Batch 11/64 loss: -0.376190721988678
Batch 12/64 loss: -0.3769279718399048
Batch 13/64 loss: -0.3596097230911255
Batch 14/64 loss: -0.3524893820285797
Batch 15/64 loss: -0.3582594692707062
Batch 16/64 loss: -0.37162017822265625
Batch 17/64 loss: -0.3830057382583618
Batch 18/64 loss: -0.37665045261383057
Batch 19/64 loss: -0.35590124130249023
Batch 20/64 loss: -0.3935721218585968
Batch 21/64 loss: -0.38036394119262695
Batch 22/64 loss: -0.3577588200569153
Batch 23/64 loss: -0.3423469662666321
Batch 24/64 loss: -0.3981660008430481
Batch 25/64 loss: -0.35167959332466125
Batch 26/64 loss: -0.4054357409477234
Batch 27/64 loss: -0.3633347153663635
Batch 28/64 loss: -0.36194324493408203
Batch 29/64 loss: -0.3687223196029663
Batch 30/64 loss: -0.35768115520477295
Batch 31/64 loss: -0.3766183853149414
Batch 32/64 loss: -0.3824254274368286
Batch 33/64 loss: -0.37541663646698
Batch 34/64 loss: -0.3956661820411682
Batch 35/64 loss: -0.35786089301109314
Batch 36/64 loss: -0.3843632936477661
Batch 37/64 loss: -0.34900084137916565
Batch 38/64 loss: -0.3721056878566742
Batch 39/64 loss: -0.3556666970252991
Batch 40/64 loss: -0.34816044569015503
Batch 41/64 loss: -0.37416255474090576
Batch 42/64 loss: -0.3570142388343811
Batch 43/64 loss: -0.39899224042892456
Batch 44/64 loss: -0.3600618243217468
Batch 45/64 loss: -0.3574867248535156
Batch 46/64 loss: -0.3710421025753021
Batch 47/64 loss: -0.3565572202205658
Batch 48/64 loss: -0.3622576594352722
Batch 49/64 loss: -0.34317684173583984
Batch 50/64 loss: -0.3547089695930481
Batch 51/64 loss: -0.3249727487564087
Batch 52/64 loss: -0.3735044002532959
Batch 53/64 loss: -0.3475082218647003
Batch 54/64 loss: -0.369717001914978
Batch 55/64 loss: -0.3507225215435028
Batch 56/64 loss: -0.34830474853515625
Batch 57/64 loss: -0.36776480078697205
Batch 58/64 loss: -0.3528985381126404
Batch 59/64 loss: -0.38640081882476807
Batch 60/64 loss: -0.37391167879104614
Batch 61/64 loss: -0.32719317078590393
Batch 62/64 loss: -0.3514956831932068
Batch 63/64 loss: -0.3677860200405121
Batch 64/64 loss: -0.35281649231910706
Epoch 441  Train loss: -0.36305800314043085  Val loss: 0.03474811876762364
Epoch 442
-------------------------------
Batch 1/64 loss: -0.37978869676589966
Batch 2/64 loss: -0.3565402626991272
Batch 3/64 loss: -0.35627514123916626
Batch 4/64 loss: -0.34918519854545593
Batch 5/64 loss: -0.3550422787666321
Batch 6/64 loss: -0.3465921878814697
Batch 7/64 loss: -0.33338725566864014
Batch 8/64 loss: -0.37043899297714233
Batch 9/64 loss: -0.38168656826019287
Batch 10/64 loss: -0.39307212829589844
Batch 11/64 loss: -0.36753034591674805
Batch 12/64 loss: -0.3828643560409546
Batch 13/64 loss: -0.3473513424396515
Batch 14/64 loss: -0.3672941327095032
Batch 15/64 loss: -0.38700515031814575
Batch 16/64 loss: -0.3668946623802185
Batch 17/64 loss: -0.3431541323661804
Batch 18/64 loss: -0.3443186283111572
Batch 19/64 loss: -0.3778812885284424
Batch 20/64 loss: -0.3343026041984558
Batch 21/64 loss: -0.4005137085914612
Batch 22/64 loss: -0.35632139444351196
Batch 23/64 loss: -0.35768353939056396
Batch 24/64 loss: -0.3723007142543793
Batch 25/64 loss: -0.3656507730484009
Batch 26/64 loss: -0.3580535054206848
Batch 27/64 loss: -0.3606589138507843
Batch 28/64 loss: -0.3794238567352295
Batch 29/64 loss: -0.3394627571105957
Batch 30/64 loss: -0.3806731104850769
Batch 31/64 loss: -0.36537253856658936
Batch 32/64 loss: -0.31843894720077515
Batch 33/64 loss: -0.3239220976829529
Batch 34/64 loss: -0.3630262613296509
Batch 35/64 loss: -0.34095779061317444
Batch 36/64 loss: -0.3674370348453522
Batch 37/64 loss: -0.3960399627685547
Batch 38/64 loss: -0.3498822748661041
Batch 39/64 loss: -0.3843573033809662
Batch 40/64 loss: -0.4067913889884949
Batch 41/64 loss: -0.3642074465751648
Batch 42/64 loss: -0.35979312658309937
Batch 43/64 loss: -0.3384893834590912
Batch 44/64 loss: -0.3092540204524994
Batch 45/64 loss: -0.3124277889728546
Batch 46/64 loss: -0.3512042760848999
Batch 47/64 loss: -0.3633328080177307
Batch 48/64 loss: -0.38282790780067444
Batch 49/64 loss: -0.38196855783462524
Batch 50/64 loss: -0.334984689950943
Batch 51/64 loss: -0.377488911151886
Batch 52/64 loss: -0.37993887066841125
Batch 53/64 loss: -0.33174341917037964
Batch 54/64 loss: -0.379519522190094
Batch 55/64 loss: -0.35280361771583557
Batch 56/64 loss: -0.3214137852191925
Batch 57/64 loss: -0.36462247371673584
Batch 58/64 loss: -0.3865154981613159
Batch 59/64 loss: -0.3863793611526489
Batch 60/64 loss: -0.36713892221450806
Batch 61/64 loss: -0.3640708923339844
Batch 62/64 loss: -0.3632023334503174
Batch 63/64 loss: -0.3793990910053253
Batch 64/64 loss: -0.38528501987457275
Epoch 442  Train loss: -0.36187080308502795  Val loss: 0.03631980435545092
Epoch 443
-------------------------------
Batch 1/64 loss: -0.3970799446105957
Batch 2/64 loss: -0.3781479001045227
Batch 3/64 loss: -0.366329550743103
Batch 4/64 loss: -0.3594117760658264
Batch 5/64 loss: -0.33451393246650696
Batch 6/64 loss: -0.3912734389305115
Batch 7/64 loss: -0.33818119764328003
Batch 8/64 loss: -0.3664436936378479
Batch 9/64 loss: -0.36543184518814087
Batch 10/64 loss: -0.39281973242759705
Batch 11/64 loss: -0.40169090032577515
Batch 12/64 loss: -0.3494824767112732
Batch 13/64 loss: -0.36801937222480774
Batch 14/64 loss: -0.37019461393356323
Batch 15/64 loss: -0.3773788809776306
Batch 16/64 loss: -0.3388741910457611
Batch 17/64 loss: -0.35922321677207947
Batch 18/64 loss: -0.3858269453048706
Batch 19/64 loss: -0.4054626226425171
Batch 20/64 loss: -0.3708239495754242
Batch 21/64 loss: -0.38173845410346985
Batch 22/64 loss: -0.3576480746269226
Batch 23/64 loss: -0.3559085726737976
Batch 24/64 loss: -0.388339638710022
Batch 25/64 loss: -0.39424222707748413
Batch 26/64 loss: -0.3724808096885681
Batch 27/64 loss: -0.39296987652778625
Batch 28/64 loss: -0.3686867356300354
Batch 29/64 loss: -0.32295769453048706
Batch 30/64 loss: -0.36217740178108215
Batch 31/64 loss: -0.3574593663215637
Batch 32/64 loss: -0.3582046627998352
Batch 33/64 loss: -0.3544424772262573
Batch 34/64 loss: -0.37468618154525757
Batch 35/64 loss: -0.37017977237701416
Batch 36/64 loss: -0.3562539219856262
Batch 37/64 loss: -0.35478538274765015
Batch 38/64 loss: -0.38081198930740356
Batch 39/64 loss: -0.357877254486084
Batch 40/64 loss: -0.3547264039516449
Batch 41/64 loss: -0.3922007083892822
Batch 42/64 loss: -0.38683971762657166
Batch 43/64 loss: -0.35878700017929077
Batch 44/64 loss: -0.3780141770839691
Batch 45/64 loss: -0.29891616106033325
Batch 46/64 loss: -0.30892622470855713
Batch 47/64 loss: -0.2878439426422119
Batch 48/64 loss: -0.36947721242904663
Batch 49/64 loss: -0.33304524421691895
Batch 50/64 loss: -0.3634253144264221
Batch 51/64 loss: -0.36566656827926636
Batch 52/64 loss: -0.3665819764137268
Batch 53/64 loss: -0.36003029346466064
Batch 54/64 loss: -0.3682359457015991
Batch 55/64 loss: -0.34550797939300537
Batch 56/64 loss: -0.3847353160381317
Batch 57/64 loss: -0.3700416684150696
Batch 58/64 loss: -0.3440900444984436
Batch 59/64 loss: -0.34643709659576416
Batch 60/64 loss: -0.37142860889434814
Batch 61/64 loss: -0.34776175022125244
Batch 62/64 loss: -0.3576008379459381
Batch 63/64 loss: -0.3900828957557678
Batch 64/64 loss: -0.3303670883178711
Epoch 443  Train loss: -0.3635551228242762  Val loss: 0.03630205814781058
Epoch 444
-------------------------------
Batch 1/64 loss: -0.36866432428359985
Batch 2/64 loss: -0.3692416250705719
Batch 3/64 loss: -0.34015920758247375
Batch 4/64 loss: -0.38705527782440186
Batch 5/64 loss: -0.3734135627746582
Batch 6/64 loss: -0.376828670501709
Batch 7/64 loss: -0.38489487767219543
Batch 8/64 loss: -0.36677098274230957
Batch 9/64 loss: -0.3611781597137451
Batch 10/64 loss: -0.3627834916114807
Batch 11/64 loss: -0.3675491213798523
Batch 12/64 loss: -0.37884587049484253
Batch 13/64 loss: -0.37519025802612305
Batch 14/64 loss: -0.4098740220069885
Batch 15/64 loss: -0.36347025632858276
Batch 16/64 loss: -0.3362365961074829
Batch 17/64 loss: -0.3622700572013855
Batch 18/64 loss: -0.29472815990448
Batch 19/64 loss: -0.35435816645622253
Batch 20/64 loss: -0.36450886726379395
Batch 21/64 loss: -0.3671569228172302
Batch 22/64 loss: -0.36540189385414124
Batch 23/64 loss: -0.3239896893501282
Batch 24/64 loss: -0.36438190937042236
Batch 25/64 loss: -0.3945935368537903
Batch 26/64 loss: -0.35953089594841003
Batch 27/64 loss: -0.36737000942230225
Batch 28/64 loss: -0.36501118540763855
Batch 29/64 loss: -0.36892247200012207
Batch 30/64 loss: -0.3830969035625458
Batch 31/64 loss: -0.3704316318035126
Batch 32/64 loss: -0.34776872396469116
Batch 33/64 loss: -0.3682364821434021
Batch 34/64 loss: -0.3714335560798645
Batch 35/64 loss: -0.38270634412765503
Batch 36/64 loss: -0.3644099235534668
Batch 37/64 loss: -0.3641985058784485
Batch 38/64 loss: -0.370975136756897
Batch 39/64 loss: -0.3757479190826416
Batch 40/64 loss: -0.3697268068790436
Batch 41/64 loss: -0.3670663833618164
Batch 42/64 loss: -0.3432501554489136
Batch 43/64 loss: -0.38621002435684204
Batch 44/64 loss: -0.3448818325996399
Batch 45/64 loss: -0.3374922275543213
Batch 46/64 loss: -0.33540624380111694
Batch 47/64 loss: -0.35495665669441223
Batch 48/64 loss: -0.39462393522262573
Batch 49/64 loss: -0.3470523953437805
Batch 50/64 loss: -0.3510691523551941
Batch 51/64 loss: -0.3829072117805481
Batch 52/64 loss: -0.3538496196269989
Batch 53/64 loss: -0.30452531576156616
Batch 54/64 loss: -0.3432173728942871
Batch 55/64 loss: -0.37302812933921814
Batch 56/64 loss: -0.37014225125312805
Batch 57/64 loss: -0.3735765218734741
Batch 58/64 loss: -0.3462822735309601
Batch 59/64 loss: -0.3756226897239685
Batch 60/64 loss: -0.3861038088798523
Batch 61/64 loss: -0.3383885622024536
Batch 62/64 loss: -0.3877824544906616
Batch 63/64 loss: -0.29029643535614014
Batch 64/64 loss: -0.3613389730453491
Epoch 444  Train loss: -0.36238192763983035  Val loss: 0.034679891317570745
Epoch 445
-------------------------------
Batch 1/64 loss: -0.3965247869491577
Batch 2/64 loss: -0.341888964176178
Batch 3/64 loss: -0.3921850621700287
Batch 4/64 loss: -0.39015504717826843
Batch 5/64 loss: -0.38888227939605713
Batch 6/64 loss: -0.3629033863544464
Batch 7/64 loss: -0.34240537881851196
Batch 8/64 loss: -0.3504248559474945
Batch 9/64 loss: -0.3691067695617676
Batch 10/64 loss: -0.3631000220775604
Batch 11/64 loss: -0.3612346649169922
Batch 12/64 loss: -0.3647504448890686
Batch 13/64 loss: -0.3030446767807007
Batch 14/64 loss: -0.3690994679927826
Batch 15/64 loss: -0.36847400665283203
Batch 16/64 loss: -0.32545024156570435
Batch 17/64 loss: -0.29247209429740906
Batch 18/64 loss: -0.3709799647331238
Batch 19/64 loss: -0.32667118310928345
Batch 20/64 loss: -0.3622357249259949
Batch 21/64 loss: -0.3619769215583801
Batch 22/64 loss: -0.3810396194458008
Batch 23/64 loss: -0.37643611431121826
Batch 24/64 loss: -0.36294764280319214
Batch 25/64 loss: -0.3449211120605469
Batch 26/64 loss: -0.37996459007263184
Batch 27/64 loss: -0.3297460377216339
Batch 28/64 loss: -0.347028911113739
Batch 29/64 loss: -0.3872392475605011
Batch 30/64 loss: -0.3533942699432373
Batch 31/64 loss: -0.3756476044654846
Batch 32/64 loss: -0.3684544563293457
Batch 33/64 loss: -0.29178035259246826
Batch 34/64 loss: -0.36781424283981323
Batch 35/64 loss: -0.3883015811443329
Batch 36/64 loss: -0.38403546810150146
Batch 37/64 loss: -0.385720819234848
Batch 38/64 loss: -0.37627553939819336
Batch 39/64 loss: -0.37381821870803833
Batch 40/64 loss: -0.35738569498062134
Batch 41/64 loss: -0.3690923750400543
Batch 42/64 loss: -0.36381644010543823
Batch 43/64 loss: -0.37897348403930664
Batch 44/64 loss: -0.3693464994430542
Batch 45/64 loss: -0.35604336857795715
Batch 46/64 loss: -0.3975124955177307
Batch 47/64 loss: -0.37670058012008667
Batch 48/64 loss: -0.39499759674072266
Batch 49/64 loss: -0.36894845962524414
Batch 50/64 loss: -0.3522137999534607
Batch 51/64 loss: -0.3918851315975189
Batch 52/64 loss: -0.3481186032295227
Batch 53/64 loss: -0.38823455572128296
Batch 54/64 loss: -0.3790610730648041
Batch 55/64 loss: -0.3652130365371704
Batch 56/64 loss: -0.3653544485569
Batch 57/64 loss: -0.3501676321029663
Batch 58/64 loss: -0.3958152234554291
Batch 59/64 loss: -0.31915196776390076
Batch 60/64 loss: -0.38018983602523804
Batch 61/64 loss: -0.2480502426624298
Batch 62/64 loss: -0.35016852617263794
Batch 63/64 loss: -0.3698773980140686
Batch 64/64 loss: -0.37912243604660034
Epoch 445  Train loss: -0.36234026773303163  Val loss: 0.03510582959119397
Epoch 446
-------------------------------
Batch 1/64 loss: -0.40020859241485596
Batch 2/64 loss: -0.3820957839488983
Batch 3/64 loss: -0.36588484048843384
Batch 4/64 loss: -0.3789118528366089
Batch 5/64 loss: -0.381021648645401
Batch 6/64 loss: -0.3366018533706665
Batch 7/64 loss: -0.3328465223312378
Batch 8/64 loss: -0.3694825768470764
Batch 9/64 loss: -0.36588069796562195
Batch 10/64 loss: -0.37952977418899536
Batch 11/64 loss: -0.38640087842941284
Batch 12/64 loss: -0.3847847282886505
Batch 13/64 loss: -0.3708040416240692
Batch 14/64 loss: -0.35657626390457153
Batch 15/64 loss: -0.3856548070907593
Batch 16/64 loss: -0.327327698469162
Batch 17/64 loss: -0.3855922222137451
Batch 18/64 loss: -0.32606083154678345
Batch 19/64 loss: -0.33326536417007446
Batch 20/64 loss: -0.29293468594551086
Batch 21/64 loss: -0.3946492671966553
Batch 22/64 loss: -0.39379650354385376
Batch 23/64 loss: -0.3877251148223877
Batch 24/64 loss: -0.2756425738334656
Batch 25/64 loss: -0.3961864411830902
Batch 26/64 loss: -0.3156471848487854
Batch 27/64 loss: -0.37370261549949646
Batch 28/64 loss: -0.362588107585907
Batch 29/64 loss: -0.35995256900787354
Batch 30/64 loss: -0.3350357413291931
Batch 31/64 loss: -0.3748897314071655
Batch 32/64 loss: -0.3517206609249115
Batch 33/64 loss: -0.30614715814590454
Batch 34/64 loss: -0.3755396604537964
Batch 35/64 loss: -0.3428107500076294
Batch 36/64 loss: -0.3611637353897095
Batch 37/64 loss: -0.35962486267089844
Batch 38/64 loss: -0.3439778983592987
Batch 39/64 loss: -0.3402673006057739
Batch 40/64 loss: -0.37285056710243225
Batch 41/64 loss: -0.3139672875404358
Batch 42/64 loss: -0.37181347608566284
Batch 43/64 loss: -0.38425353169441223
Batch 44/64 loss: -0.374449223279953
Batch 45/64 loss: -0.3943727910518646
Batch 46/64 loss: -0.33258146047592163
Batch 47/64 loss: -0.38311678171157837
Batch 48/64 loss: -0.3372025191783905
Batch 49/64 loss: -0.3725047707557678
Batch 50/64 loss: -0.3815150260925293
Batch 51/64 loss: -0.36786335706710815
Batch 52/64 loss: -0.34024256467819214
Batch 53/64 loss: -0.3722791075706482
Batch 54/64 loss: -0.36597883701324463
Batch 55/64 loss: -0.3543645143508911
Batch 56/64 loss: -0.38647037744522095
Batch 57/64 loss: -0.37966272234916687
Batch 58/64 loss: -0.3306013345718384
Batch 59/64 loss: -0.380470335483551
Batch 60/64 loss: -0.37389418482780457
Batch 61/64 loss: -0.35640883445739746
Batch 62/64 loss: -0.3288519084453583
Batch 63/64 loss: -0.27175405621528625
Batch 64/64 loss: -0.31933748722076416
Epoch 446  Train loss: -0.3585867643356323  Val loss: 0.03558517034930462
Epoch 447
-------------------------------
Batch 1/64 loss: -0.36706048250198364
Batch 2/64 loss: -0.3490411043167114
Batch 3/64 loss: -0.3182567059993744
Batch 4/64 loss: -0.3083624839782715
Batch 5/64 loss: -0.28134214878082275
Batch 6/64 loss: -0.3577727675437927
Batch 7/64 loss: -0.3653750419616699
Batch 8/64 loss: -0.34620726108551025
Batch 9/64 loss: -0.3539765477180481
Batch 10/64 loss: -0.3805343806743622
Batch 11/64 loss: -0.3681281507015228
Batch 12/64 loss: -0.3772980272769928
Batch 13/64 loss: -0.3498207926750183
Batch 14/64 loss: -0.3815779983997345
Batch 15/64 loss: -0.37579458951950073
Batch 16/64 loss: -0.3428376019001007
Batch 17/64 loss: -0.36552560329437256
Batch 18/64 loss: -0.31497785449028015
Batch 19/64 loss: -0.36133643984794617
Batch 20/64 loss: -0.37293577194213867
Batch 21/64 loss: -0.34509173035621643
Batch 22/64 loss: -0.3725801706314087
Batch 23/64 loss: -0.38260650634765625
Batch 24/64 loss: -0.36515289545059204
Batch 25/64 loss: -0.3930124342441559
Batch 26/64 loss: -0.3655204772949219
Batch 27/64 loss: -0.34939950704574585
Batch 28/64 loss: -0.3596174120903015
Batch 29/64 loss: -0.35578328371047974
Batch 30/64 loss: -0.3629738688468933
Batch 31/64 loss: -0.37454357743263245
Batch 32/64 loss: -0.32465478777885437
Batch 33/64 loss: -0.3886262774467468
Batch 34/64 loss: -0.38148951530456543
Batch 35/64 loss: -0.33826154470443726
Batch 36/64 loss: -0.36449143290519714
Batch 37/64 loss: -0.394164115190506
Batch 38/64 loss: -0.3637644350528717
Batch 39/64 loss: -0.3887970447540283
Batch 40/64 loss: -0.32921507954597473
Batch 41/64 loss: -0.33118119835853577
Batch 42/64 loss: -0.3376805782318115
Batch 43/64 loss: -0.34122610092163086
Batch 44/64 loss: -0.3635143041610718
Batch 45/64 loss: -0.3928004503250122
Batch 46/64 loss: -0.3716747760772705
Batch 47/64 loss: -0.2856815457344055
Batch 48/64 loss: -0.36780059337615967
Batch 49/64 loss: -0.35432159900665283
Batch 50/64 loss: -0.3584311604499817
Batch 51/64 loss: -0.36623436212539673
Batch 52/64 loss: -0.34096047282218933
Batch 53/64 loss: -0.3484821915626526
Batch 54/64 loss: -0.39013195037841797
Batch 55/64 loss: -0.37738052010536194
Batch 56/64 loss: -0.37239354848861694
Batch 57/64 loss: -0.3809816241264343
Batch 58/64 loss: -0.39191222190856934
Batch 59/64 loss: -0.3582937717437744
Batch 60/64 loss: -0.39217913150787354
Batch 61/64 loss: -0.33736205101013184
Batch 62/64 loss: -0.38326945900917053
Batch 63/64 loss: -0.37652504444122314
Batch 64/64 loss: -0.36858251690864563
Epoch 447  Train loss: -0.35976099442033205  Val loss: 0.035778440355844925
Epoch 448
-------------------------------
Batch 1/64 loss: -0.4197697043418884
Batch 2/64 loss: -0.36386731266975403
Batch 3/64 loss: -0.317167729139328
Batch 4/64 loss: -0.3928859829902649
Batch 5/64 loss: -0.30665910243988037
Batch 6/64 loss: -0.4023711681365967
Batch 7/64 loss: -0.3204001188278198
Batch 8/64 loss: -0.3919799327850342
Batch 9/64 loss: -0.359642893075943
Batch 10/64 loss: -0.34608322381973267
Batch 11/64 loss: -0.37203311920166016
Batch 12/64 loss: -0.3699851632118225
Batch 13/64 loss: -0.3420788645744324
Batch 14/64 loss: -0.37045639753341675
Batch 15/64 loss: -0.3661973476409912
Batch 16/64 loss: -0.3572441637516022
Batch 17/64 loss: -0.3509771227836609
Batch 18/64 loss: -0.3903694748878479
Batch 19/64 loss: -0.37847524881362915
Batch 20/64 loss: -0.36273789405822754
Batch 21/64 loss: -0.3662121593952179
Batch 22/64 loss: -0.3663756251335144
Batch 23/64 loss: -0.37224358320236206
Batch 24/64 loss: -0.34437865018844604
Batch 25/64 loss: -0.38751137256622314
Batch 26/64 loss: -0.39531031250953674
Batch 27/64 loss: -0.3612286150455475
Batch 28/64 loss: -0.3341500163078308
Batch 29/64 loss: -0.3631943464279175
Batch 30/64 loss: -0.3846447467803955
Batch 31/64 loss: -0.344774067401886
Batch 32/64 loss: -0.3572339117527008
Batch 33/64 loss: -0.35482266545295715
Batch 34/64 loss: -0.37659305334091187
Batch 35/64 loss: -0.343852162361145
Batch 36/64 loss: -0.3441365361213684
Batch 37/64 loss: -0.3544274866580963
Batch 38/64 loss: -0.35682299733161926
Batch 39/64 loss: -0.37841612100601196
Batch 40/64 loss: -0.3707742691040039
Batch 41/64 loss: -0.3396948575973511
Batch 42/64 loss: -0.3259882926940918
Batch 43/64 loss: -0.4061439633369446
Batch 44/64 loss: -0.3726176619529724
Batch 45/64 loss: -0.35917502641677856
Batch 46/64 loss: -0.3505653738975525
Batch 47/64 loss: -0.38940078020095825
Batch 48/64 loss: -0.34065213799476624
Batch 49/64 loss: -0.35622769594192505
Batch 50/64 loss: -0.38455817103385925
Batch 51/64 loss: -0.3633631765842438
Batch 52/64 loss: -0.35567301511764526
Batch 53/64 loss: -0.38022905588150024
Batch 54/64 loss: -0.40208184719085693
Batch 55/64 loss: -0.3397139310836792
Batch 56/64 loss: -0.38738977909088135
Batch 57/64 loss: -0.3750740885734558
Batch 58/64 loss: -0.36331042647361755
Batch 59/64 loss: -0.3399811387062073
Batch 60/64 loss: -0.32342684268951416
Batch 61/64 loss: -0.3283940851688385
Batch 62/64 loss: -0.37604910135269165
Batch 63/64 loss: -0.37487220764160156
Batch 64/64 loss: -0.34841829538345337
Epoch 448  Train loss: -0.3628922516224431  Val loss: 0.03510032896323712
Epoch 449
-------------------------------
Batch 1/64 loss: -0.38605839014053345
Batch 2/64 loss: -0.3968331217765808
Batch 3/64 loss: -0.34281831979751587
Batch 4/64 loss: -0.3084411025047302
Batch 5/64 loss: -0.3676137924194336
Batch 6/64 loss: -0.3520774841308594
Batch 7/64 loss: -0.3611769676208496
Batch 8/64 loss: -0.4031006693840027
Batch 9/64 loss: -0.3521571755409241
Batch 10/64 loss: -0.36467868089675903
Batch 11/64 loss: -0.3745875358581543
Batch 12/64 loss: -0.350752055644989
Batch 13/64 loss: -0.35472622513771057
Batch 14/64 loss: -0.39957883954048157
Batch 15/64 loss: -0.3719122111797333
Batch 16/64 loss: -0.36073318123817444
Batch 17/64 loss: -0.3351885676383972
Batch 18/64 loss: -0.37805530428886414
Batch 19/64 loss: -0.38074997067451477
Batch 20/64 loss: -0.3648275136947632
Batch 21/64 loss: -0.32539355754852295
Batch 22/64 loss: -0.3678608536720276
Batch 23/64 loss: -0.3675047755241394
Batch 24/64 loss: -0.32838112115859985
Batch 25/64 loss: -0.3964361846446991
Batch 26/64 loss: -0.38128095865249634
Batch 27/64 loss: -0.3523637652397156
Batch 28/64 loss: -0.3935737907886505
Batch 29/64 loss: -0.3930027186870575
Batch 30/64 loss: -0.3440317213535309
Batch 31/64 loss: -0.37778300046920776
Batch 32/64 loss: -0.3771933913230896
Batch 33/64 loss: -0.34328868985176086
Batch 34/64 loss: -0.3788278102874756
Batch 35/64 loss: -0.38871580362319946
Batch 36/64 loss: -0.3880408704280853
Batch 37/64 loss: -0.30220669507980347
Batch 38/64 loss: -0.3603798747062683
Batch 39/64 loss: -0.3699273467063904
Batch 40/64 loss: -0.38075029850006104
Batch 41/64 loss: -0.3476998805999756
Batch 42/64 loss: -0.35088586807250977
Batch 43/64 loss: -0.3912907838821411
Batch 44/64 loss: -0.38626813888549805
Batch 45/64 loss: -0.3429747223854065
Batch 46/64 loss: -0.3915027379989624
Batch 47/64 loss: -0.3825821876525879
Batch 48/64 loss: -0.39780914783477783
Batch 49/64 loss: -0.3621528148651123
Batch 50/64 loss: -0.35414445400238037
Batch 51/64 loss: -0.40149354934692383
Batch 52/64 loss: -0.3584743142127991
Batch 53/64 loss: -0.3674577474594116
Batch 54/64 loss: -0.32872235774993896
Batch 55/64 loss: -0.3887612521648407
Batch 56/64 loss: -0.33265894651412964
Batch 57/64 loss: -0.36953967809677124
Batch 58/64 loss: -0.39336052536964417
Batch 59/64 loss: -0.389736533164978
Batch 60/64 loss: -0.3731209635734558
Batch 61/64 loss: -0.3683922290802002
Batch 62/64 loss: -0.3702237606048584
Batch 63/64 loss: -0.31387194991111755
Batch 64/64 loss: -0.2778654396533966
Epoch 449  Train loss: -0.3654044542826858  Val loss: 0.036723123792930155
Epoch 450
-------------------------------
Batch 1/64 loss: -0.3855580687522888
Batch 2/64 loss: -0.32726624608039856
Batch 3/64 loss: -0.3622046113014221
Batch 4/64 loss: -0.3525221347808838
Batch 5/64 loss: -0.3694569170475006
Batch 6/64 loss: -0.36266809701919556
Batch 7/64 loss: -0.3977973461151123
Batch 8/64 loss: -0.3684728741645813
Batch 9/64 loss: -0.37928447127342224
Batch 10/64 loss: -0.3868209719657898
Batch 11/64 loss: -0.3504694998264313
Batch 12/64 loss: -0.37549328804016113
Batch 13/64 loss: -0.3374554514884949
Batch 14/64 loss: -0.3748301863670349
Batch 15/64 loss: -0.36325550079345703
Batch 16/64 loss: -0.42171478271484375
Batch 17/64 loss: -0.36826279759407043
Batch 18/64 loss: -0.39556068181991577
Batch 19/64 loss: -0.3722308278083801
Batch 20/64 loss: -0.353797972202301
Batch 21/64 loss: -0.4021371304988861
Batch 22/64 loss: -0.38323596119880676
Batch 23/64 loss: -0.3666759133338928
Batch 24/64 loss: -0.34003913402557373
Batch 25/64 loss: -0.3766937553882599
Batch 26/64 loss: -0.37195757031440735
Batch 27/64 loss: -0.33377158641815186
Batch 28/64 loss: -0.37241464853286743
Batch 29/64 loss: -0.3535915017127991
Batch 30/64 loss: -0.3886220455169678
Batch 31/64 loss: -0.3429046869277954
Batch 32/64 loss: -0.3747350573539734
Batch 33/64 loss: -0.39951619505882263
Batch 34/64 loss: -0.36619624495506287
Batch 35/64 loss: -0.3849923610687256
Batch 36/64 loss: -0.34539899230003357
Batch 37/64 loss: -0.356173574924469
Batch 38/64 loss: -0.3612016439437866
Batch 39/64 loss: -0.34811145067214966
Batch 40/64 loss: -0.37926605343818665
Batch 41/64 loss: -0.34830132126808167
Batch 42/64 loss: -0.38790881633758545
Batch 43/64 loss: -0.34300267696380615
Batch 44/64 loss: -0.3616035282611847
Batch 45/64 loss: -0.3621165156364441
Batch 46/64 loss: -0.3600897789001465
Batch 47/64 loss: -0.38179296255111694
Batch 48/64 loss: -0.34984031319618225
Batch 49/64 loss: -0.36238178610801697
Batch 50/64 loss: -0.3492865562438965
Batch 51/64 loss: -0.39132022857666016
Batch 52/64 loss: -0.3202952742576599
Batch 53/64 loss: -0.37860411405563354
Batch 54/64 loss: -0.36222121119499207
Batch 55/64 loss: -0.3379744291305542
Batch 56/64 loss: -0.3952069878578186
Batch 57/64 loss: -0.3853560984134674
Batch 58/64 loss: -0.3611893653869629
Batch 59/64 loss: -0.30517953634262085
Batch 60/64 loss: -0.3225068747997284
Batch 61/64 loss: -0.35536548495292664
Batch 62/64 loss: -0.3652167022228241
Batch 63/64 loss: -0.37220513820648193
Batch 64/64 loss: -0.3799748718738556
Epoch 450  Train loss: -0.365438511207992  Val loss: 0.03712210007959215
Epoch 451
-------------------------------
Batch 1/64 loss: -0.36281394958496094
Batch 2/64 loss: -0.3389890789985657
Batch 3/64 loss: -0.35808035731315613
Batch 4/64 loss: -0.38610053062438965
Batch 5/64 loss: -0.3916987478733063
Batch 6/64 loss: -0.35200828313827515
Batch 7/64 loss: -0.3139147162437439
Batch 8/64 loss: -0.3828303813934326
Batch 9/64 loss: -0.38912463188171387
Batch 10/64 loss: -0.40097498893737793
Batch 11/64 loss: -0.3023022413253784
Batch 12/64 loss: -0.35208410024642944
Batch 13/64 loss: -0.3623628616333008
Batch 14/64 loss: -0.3947608470916748
Batch 15/64 loss: -0.35091572999954224
Batch 16/64 loss: -0.3923494517803192
Batch 17/64 loss: -0.38923579454421997
Batch 18/64 loss: -0.3774881958961487
Batch 19/64 loss: -0.37935757637023926
Batch 20/64 loss: -0.3558225631713867
Batch 21/64 loss: -0.382629930973053
Batch 22/64 loss: -0.4030211567878723
Batch 23/64 loss: -0.36331894993782043
Batch 24/64 loss: -0.38673049211502075
Batch 25/64 loss: -0.3902459442615509
Batch 26/64 loss: -0.36423295736312866
Batch 27/64 loss: -0.3465854823589325
Batch 28/64 loss: -0.32007068395614624
Batch 29/64 loss: -0.3749670386314392
Batch 30/64 loss: -0.35471004247665405
Batch 31/64 loss: -0.367761492729187
Batch 32/64 loss: -0.36470726132392883
Batch 33/64 loss: -0.35985180735588074
Batch 34/64 loss: -0.31737637519836426
Batch 35/64 loss: -0.36726248264312744
Batch 36/64 loss: -0.36722755432128906
Batch 37/64 loss: -0.37166479229927063
Batch 38/64 loss: -0.3492758870124817
Batch 39/64 loss: -0.36431795358657837
Batch 40/64 loss: -0.377547949552536
Batch 41/64 loss: -0.3523583710193634
Batch 42/64 loss: -0.3730204999446869
Batch 43/64 loss: -0.34486615657806396
Batch 44/64 loss: -0.39115068316459656
Batch 45/64 loss: -0.3650246858596802
Batch 46/64 loss: -0.36965519189834595
Batch 47/64 loss: -0.38673919439315796
Batch 48/64 loss: -0.3551790416240692
Batch 49/64 loss: -0.3410743474960327
Batch 50/64 loss: -0.3379981815814972
Batch 51/64 loss: -0.3618384599685669
Batch 52/64 loss: -0.3252895176410675
Batch 53/64 loss: -0.37072598934173584
Batch 54/64 loss: -0.3673814535140991
Batch 55/64 loss: -0.40121138095855713
Batch 56/64 loss: -0.38493478298187256
Batch 57/64 loss: -0.36806726455688477
Batch 58/64 loss: -0.3827642798423767
Batch 59/64 loss: -0.3740687966346741
Batch 60/64 loss: -0.34399330615997314
Batch 61/64 loss: -0.3408879041671753
Batch 62/64 loss: -0.3611481189727783
Batch 63/64 loss: -0.3449302911758423
Batch 64/64 loss: -0.37369412183761597
Epoch 451  Train loss: -0.36472627064760993  Val loss: 0.03629323809417253
Epoch 452
-------------------------------
Batch 1/64 loss: -0.3308035731315613
Batch 2/64 loss: -0.3758281469345093
Batch 3/64 loss: -0.38049423694610596
Batch 4/64 loss: -0.3594106137752533
Batch 5/64 loss: -0.36169081926345825
Batch 6/64 loss: -0.37270188331604004
Batch 7/64 loss: -0.3888785243034363
Batch 8/64 loss: -0.3500818610191345
Batch 9/64 loss: -0.388555645942688
Batch 10/64 loss: -0.39719414710998535
Batch 11/64 loss: -0.3532426953315735
Batch 12/64 loss: -0.3689364790916443
Batch 13/64 loss: -0.3937293291091919
Batch 14/64 loss: -0.3619585633277893
Batch 15/64 loss: -0.3820011615753174
Batch 16/64 loss: -0.3639862537384033
Batch 17/64 loss: -0.3940975069999695
Batch 18/64 loss: -0.3315229117870331
Batch 19/64 loss: -0.344526469707489
Batch 20/64 loss: -0.3888899087905884
Batch 21/64 loss: -0.3608638644218445
Batch 22/64 loss: -0.3542725443840027
Batch 23/64 loss: -0.3507441282272339
Batch 24/64 loss: -0.3405241072177887
Batch 25/64 loss: -0.3601631224155426
Batch 26/64 loss: -0.36554038524627686
Batch 27/64 loss: -0.3796532452106476
Batch 28/64 loss: -0.36958378553390503
Batch 29/64 loss: -0.379239022731781
Batch 30/64 loss: -0.38680779933929443
Batch 31/64 loss: -0.3380427956581116
Batch 32/64 loss: -0.33160772919654846
Batch 33/64 loss: -0.37292957305908203
Batch 34/64 loss: -0.34461599588394165
Batch 35/64 loss: -0.3557366728782654
Batch 36/64 loss: -0.3547481596469879
Batch 37/64 loss: -0.3350410461425781
Batch 38/64 loss: -0.3781662583351135
Batch 39/64 loss: -0.38378894329071045
Batch 40/64 loss: -0.32643163204193115
Batch 41/64 loss: -0.3675591051578522
Batch 42/64 loss: -0.3585087060928345
Batch 43/64 loss: -0.371660053730011
Batch 44/64 loss: -0.3597656488418579
Batch 45/64 loss: -0.3579387962818146
Batch 46/64 loss: -0.38875633478164673
Batch 47/64 loss: -0.35987207293510437
Batch 48/64 loss: -0.38530808687210083
Batch 49/64 loss: -0.38287362456321716
Batch 50/64 loss: -0.39189857244491577
Batch 51/64 loss: -0.3798641562461853
Batch 52/64 loss: -0.3411828279495239
Batch 53/64 loss: -0.3319208025932312
Batch 54/64 loss: -0.37491708993911743
Batch 55/64 loss: -0.3611688017845154
Batch 56/64 loss: -0.3671668469905853
Batch 57/64 loss: -0.38837113976478577
Batch 58/64 loss: -0.37986302375793457
Batch 59/64 loss: -0.3787113428115845
Batch 60/64 loss: -0.2991224229335785
Batch 61/64 loss: -0.3571818470954895
Batch 62/64 loss: -0.3621700406074524
Batch 63/64 loss: -0.340603232383728
Batch 64/64 loss: -0.3831847310066223
Epoch 452  Train loss: -0.3644047790882634  Val loss: 0.0355983794349985
Epoch 453
-------------------------------
Batch 1/64 loss: -0.37521618604660034
Batch 2/64 loss: -0.38149070739746094
Batch 3/64 loss: -0.40228161215782166
Batch 4/64 loss: -0.36403167247772217
Batch 5/64 loss: -0.4107958674430847
Batch 6/64 loss: -0.37294167280197144
Batch 7/64 loss: -0.37117063999176025
Batch 8/64 loss: -0.3743100166320801
Batch 9/64 loss: -0.3391028642654419
Batch 10/64 loss: -0.34563541412353516
Batch 11/64 loss: -0.3850279152393341
Batch 12/64 loss: -0.354495108127594
Batch 13/64 loss: -0.3669097423553467
Batch 14/64 loss: -0.3401425778865814
Batch 15/64 loss: -0.34549686312675476
Batch 16/64 loss: -0.3910999596118927
Batch 17/64 loss: -0.40532881021499634
Batch 18/64 loss: -0.34748828411102295
Batch 19/64 loss: -0.39082375168800354
Batch 20/64 loss: -0.33466246724128723
Batch 21/64 loss: -0.39024293422698975
Batch 22/64 loss: -0.360270231962204
Batch 23/64 loss: -0.35291731357574463
Batch 24/64 loss: -0.30931365489959717
Batch 25/64 loss: -0.35135677456855774
Batch 26/64 loss: -0.3500226140022278
Batch 27/64 loss: -0.38090330362319946
Batch 28/64 loss: -0.2988942861557007
Batch 29/64 loss: -0.3573259115219116
Batch 30/64 loss: -0.3301865756511688
Batch 31/64 loss: -0.3767565190792084
Batch 32/64 loss: -0.3807811439037323
Batch 33/64 loss: -0.33864420652389526
Batch 34/64 loss: -0.3791337013244629
Batch 35/64 loss: -0.37303626537323
Batch 36/64 loss: -0.38687872886657715
Batch 37/64 loss: -0.37467584013938904
Batch 38/64 loss: -0.3649359345436096
Batch 39/64 loss: -0.372236967086792
Batch 40/64 loss: -0.3744007647037506
Batch 41/64 loss: -0.39242327213287354
Batch 42/64 loss: -0.3932792544364929
Batch 43/64 loss: -0.37215226888656616
Batch 44/64 loss: -0.3881693482398987
Batch 45/64 loss: -0.3958074152469635
Batch 46/64 loss: -0.36071842908859253
Batch 47/64 loss: -0.39204689860343933
Batch 48/64 loss: -0.3674201965332031
Batch 49/64 loss: -0.3670129179954529
Batch 50/64 loss: -0.3392786383628845
Batch 51/64 loss: -0.3755864202976227
Batch 52/64 loss: -0.3472220301628113
Batch 53/64 loss: -0.38773486018180847
Batch 54/64 loss: -0.3273732662200928
Batch 55/64 loss: -0.3602752387523651
Batch 56/64 loss: -0.3952633738517761
Batch 57/64 loss: -0.3826737701892853
Batch 58/64 loss: -0.3843569755554199
Batch 59/64 loss: -0.3499758243560791
Batch 60/64 loss: -0.3515605330467224
Batch 61/64 loss: -0.403699666261673
Batch 62/64 loss: -0.36839035153388977
Batch 63/64 loss: -0.32487425208091736
Batch 64/64 loss: -0.35030847787857056
Epoch 453  Train loss: -0.3669238018054588  Val loss: 0.0351358443191371
Epoch 454
-------------------------------
Batch 1/64 loss: -0.3889046311378479
Batch 2/64 loss: -0.3637869358062744
Batch 3/64 loss: -0.3652392327785492
Batch 4/64 loss: -0.3434446454048157
Batch 5/64 loss: -0.37492606043815613
Batch 6/64 loss: -0.3554667830467224
Batch 7/64 loss: -0.3954135775566101
Batch 8/64 loss: -0.3243238627910614
Batch 9/64 loss: -0.3759065270423889
Batch 10/64 loss: -0.3858027458190918
Batch 11/64 loss: -0.37668609619140625
Batch 12/64 loss: -0.3364635705947876
Batch 13/64 loss: -0.3526018261909485
Batch 14/64 loss: -0.37253740429878235
Batch 15/64 loss: -0.3786965012550354
Batch 16/64 loss: -0.39704573154449463
Batch 17/64 loss: -0.37166857719421387
Batch 18/64 loss: -0.37595126032829285
Batch 19/64 loss: -0.38438114523887634
Batch 20/64 loss: -0.3592385947704315
Batch 21/64 loss: -0.36258846521377563
Batch 22/64 loss: -0.36017072200775146
Batch 23/64 loss: -0.3845078647136688
Batch 24/64 loss: -0.3169907033443451
Batch 25/64 loss: -0.375687837600708
Batch 26/64 loss: -0.35776227712631226
Batch 27/64 loss: -0.29089468717575073
Batch 28/64 loss: -0.3120141625404358
Batch 29/64 loss: -0.3489057421684265
Batch 30/64 loss: -0.3946063220500946
Batch 31/64 loss: -0.28075480461120605
Batch 32/64 loss: -0.37549060583114624
Batch 33/64 loss: -0.3492248058319092
Batch 34/64 loss: -0.3713839650154114
Batch 35/64 loss: -0.34147948026657104
Batch 36/64 loss: -0.4012914001941681
Batch 37/64 loss: -0.37038707733154297
Batch 38/64 loss: -0.3888310492038727
Batch 39/64 loss: -0.3931729197502136
Batch 40/64 loss: -0.3309090733528137
Batch 41/64 loss: -0.3693763017654419
Batch 42/64 loss: -0.35543811321258545
Batch 43/64 loss: -0.3849385380744934
Batch 44/64 loss: -0.32698026299476624
Batch 45/64 loss: -0.3870425224304199
Batch 46/64 loss: -0.2799094319343567
Batch 47/64 loss: -0.3719973564147949
Batch 48/64 loss: -0.3684104084968567
Batch 49/64 loss: -0.37174272537231445
Batch 50/64 loss: -0.37054023146629333
Batch 51/64 loss: -0.3897606134414673
Batch 52/64 loss: -0.32434672117233276
Batch 53/64 loss: -0.3729916214942932
Batch 54/64 loss: -0.36121970415115356
Batch 55/64 loss: -0.369333416223526
Batch 56/64 loss: -0.35948097705841064
Batch 57/64 loss: -0.377393901348114
Batch 58/64 loss: -0.3470267951488495
Batch 59/64 loss: -0.38423120975494385
Batch 60/64 loss: -0.35116082429885864
Batch 61/64 loss: -0.3783929944038391
Batch 62/64 loss: -0.37784332036972046
Batch 63/64 loss: -0.3804928660392761
Batch 64/64 loss: -0.3752412796020508
Epoch 454  Train loss: -0.3627768077102362  Val loss: 0.03571382974021623
Epoch 455
-------------------------------
Batch 1/64 loss: -0.334665983915329
Batch 2/64 loss: -0.3403435945510864
Batch 3/64 loss: -0.3815431594848633
Batch 4/64 loss: -0.41163909435272217
Batch 5/64 loss: -0.3380381166934967
Batch 6/64 loss: -0.38391774892807007
Batch 7/64 loss: -0.3700217008590698
Batch 8/64 loss: -0.33007511496543884
Batch 9/64 loss: -0.3499305248260498
Batch 10/64 loss: -0.3824033737182617
Batch 11/64 loss: -0.3659150004386902
Batch 12/64 loss: -0.3791065812110901
Batch 13/64 loss: -0.3652989864349365
Batch 14/64 loss: -0.3581242263317108
Batch 15/64 loss: -0.37272489070892334
Batch 16/64 loss: -0.36884891986846924
Batch 17/64 loss: -0.35522985458374023
Batch 18/64 loss: -0.31282275915145874
Batch 19/64 loss: -0.3837624788284302
Batch 20/64 loss: -0.3911547064781189
Batch 21/64 loss: -0.37548577785491943
Batch 22/64 loss: -0.3720244765281677
Batch 23/64 loss: -0.3880891501903534
Batch 24/64 loss: -0.34104326367378235
Batch 25/64 loss: -0.36361929774284363
Batch 26/64 loss: -0.37285855412483215
Batch 27/64 loss: -0.3755597174167633
Batch 28/64 loss: -0.37896233797073364
Batch 29/64 loss: -0.3077685832977295
Batch 30/64 loss: -0.3676627278327942
Batch 31/64 loss: -0.37760472297668457
Batch 32/64 loss: -0.3550221621990204
Batch 33/64 loss: -0.4142657518386841
Batch 34/64 loss: -0.39311039447784424
Batch 35/64 loss: -0.3680074214935303
Batch 36/64 loss: -0.3586694896221161
Batch 37/64 loss: -0.3726266920566559
Batch 38/64 loss: -0.3251644968986511
Batch 39/64 loss: -0.3756069540977478
Batch 40/64 loss: -0.3230920135974884
Batch 41/64 loss: -0.3915506601333618
Batch 42/64 loss: -0.3550795614719391
Batch 43/64 loss: -0.3456670045852661
Batch 44/64 loss: -0.3229619264602661
Batch 45/64 loss: -0.38855716586112976
Batch 46/64 loss: -0.37235528230667114
Batch 47/64 loss: -0.37178942561149597
Batch 48/64 loss: -0.32492029666900635
Batch 49/64 loss: -0.3711793124675751
Batch 50/64 loss: -0.35687828063964844
Batch 51/64 loss: -0.36978858709335327
Batch 52/64 loss: -0.3476480543613434
Batch 53/64 loss: -0.3715200126171112
Batch 54/64 loss: -0.36395972967147827
Batch 55/64 loss: -0.3748432397842407
Batch 56/64 loss: -0.3694511353969574
Batch 57/64 loss: -0.3842625021934509
Batch 58/64 loss: -0.3422654867172241
Batch 59/64 loss: -0.3663421869277954
Batch 60/64 loss: -0.36797523498535156
Batch 61/64 loss: -0.3655376136302948
Batch 62/64 loss: -0.3870491087436676
Batch 63/64 loss: -0.35568931698799133
Batch 64/64 loss: -0.34211161732673645
Epoch 455  Train loss: -0.364010441420125  Val loss: 0.03942933873212624
Epoch 456
-------------------------------
Batch 1/64 loss: -0.333015501499176
Batch 2/64 loss: -0.35727575421333313
Batch 3/64 loss: -0.39836186170578003
Batch 4/64 loss: -0.3526403307914734
Batch 5/64 loss: -0.36553943157196045
Batch 6/64 loss: -0.3794708549976349
Batch 7/64 loss: -0.3845382332801819
Batch 8/64 loss: -0.3793051242828369
Batch 9/64 loss: -0.3636879026889801
Batch 10/64 loss: -0.3856646716594696
Batch 11/64 loss: -0.30428534746170044
Batch 12/64 loss: -0.34479203820228577
Batch 13/64 loss: -0.3916253447532654
Batch 14/64 loss: -0.3592268228530884
Batch 15/64 loss: -0.35836276412010193
Batch 16/64 loss: -0.37676602602005005
Batch 17/64 loss: -0.3441646695137024
Batch 18/64 loss: -0.3803834915161133
Batch 19/64 loss: -0.3746497631072998
Batch 20/64 loss: -0.3620547354221344
Batch 21/64 loss: -0.3774157166481018
Batch 22/64 loss: -0.36746448278427124
Batch 23/64 loss: -0.371125191450119
Batch 24/64 loss: -0.32429927587509155
Batch 25/64 loss: -0.34920191764831543
Batch 26/64 loss: -0.39022117853164673
Batch 27/64 loss: -0.3511428236961365
Batch 28/64 loss: -0.3699994683265686
Batch 29/64 loss: -0.347601056098938
Batch 30/64 loss: -0.3732036352157593
Batch 31/64 loss: -0.37803953886032104
Batch 32/64 loss: -0.23682284355163574
Batch 33/64 loss: -0.36793988943099976
Batch 34/64 loss: -0.36114901304244995
Batch 35/64 loss: -0.3463106155395508
Batch 36/64 loss: -0.4029373526573181
Batch 37/64 loss: -0.3956471383571625
Batch 38/64 loss: -0.34849560260772705
Batch 39/64 loss: -0.3999312222003937
Batch 40/64 loss: -0.3255387544631958
Batch 41/64 loss: -0.34081706404685974
Batch 42/64 loss: -0.36612188816070557
Batch 43/64 loss: -0.36472609639167786
Batch 44/64 loss: -0.38476336002349854
Batch 45/64 loss: -0.35737088322639465
Batch 46/64 loss: -0.3888135552406311
Batch 47/64 loss: -0.34781408309936523
Batch 48/64 loss: -0.35166141390800476
Batch 49/64 loss: -0.3719986379146576
Batch 50/64 loss: -0.3762936294078827
Batch 51/64 loss: -0.361995667219162
Batch 52/64 loss: -0.3960065245628357
Batch 53/64 loss: -0.3508167266845703
Batch 54/64 loss: -0.37676259875297546
Batch 55/64 loss: -0.3902233839035034
Batch 56/64 loss: -0.3638807535171509
Batch 57/64 loss: -0.36250555515289307
Batch 58/64 loss: -0.38590216636657715
Batch 59/64 loss: -0.36717063188552856
Batch 60/64 loss: -0.3560405373573303
Batch 61/64 loss: -0.3566746711730957
Batch 62/64 loss: -0.3470263183116913
Batch 63/64 loss: -0.3904426097869873
Batch 64/64 loss: -0.3496975898742676
Epoch 456  Train loss: -0.36389646015915217  Val loss: 0.036262926367140305
Epoch 457
-------------------------------
Batch 1/64 loss: -0.3368721306324005
Batch 2/64 loss: -0.39096373319625854
Batch 3/64 loss: -0.3567710518836975
Batch 4/64 loss: -0.40963083505630493
Batch 5/64 loss: -0.383503794670105
Batch 6/64 loss: -0.35115164518356323
Batch 7/64 loss: -0.35098713636398315
Batch 8/64 loss: -0.3834301233291626
Batch 9/64 loss: -0.3678014874458313
Batch 10/64 loss: -0.37819284200668335
Batch 11/64 loss: -0.3901406526565552
Batch 12/64 loss: -0.33106136322021484
Batch 13/64 loss: -0.33344510197639465
Batch 14/64 loss: -0.38277000188827515
Batch 15/64 loss: -0.39079323410987854
Batch 16/64 loss: -0.32692229747772217
Batch 17/64 loss: -0.3553718328475952
Batch 18/64 loss: -0.39815306663513184
Batch 19/64 loss: -0.34205543994903564
Batch 20/64 loss: -0.393272340297699
Batch 21/64 loss: -0.3918466866016388
Batch 22/64 loss: -0.35309943556785583
Batch 23/64 loss: -0.3755229711532593
Batch 24/64 loss: -0.35811465978622437
Batch 25/64 loss: -0.40209001302719116
Batch 26/64 loss: -0.33982524275779724
Batch 27/64 loss: -0.3622860312461853
Batch 28/64 loss: -0.40101873874664307
Batch 29/64 loss: -0.3596898913383484
Batch 30/64 loss: -0.37413927912712097
Batch 31/64 loss: -0.3812262713909149
Batch 32/64 loss: -0.36992859840393066
Batch 33/64 loss: -0.35499581694602966
Batch 34/64 loss: -0.3213881254196167
Batch 35/64 loss: -0.32940584421157837
Batch 36/64 loss: -0.3692764341831207
Batch 37/64 loss: -0.35915136337280273
Batch 38/64 loss: -0.3303067982196808
Batch 39/64 loss: -0.3663238286972046
Batch 40/64 loss: -0.377697616815567
Batch 41/64 loss: -0.3568045496940613
Batch 42/64 loss: -0.330059289932251
Batch 43/64 loss: -0.3598037362098694
Batch 44/64 loss: -0.40615057945251465
Batch 45/64 loss: -0.35219359397888184
Batch 46/64 loss: -0.3762109577655792
Batch 47/64 loss: -0.394024133682251
Batch 48/64 loss: -0.3779964745044708
Batch 49/64 loss: -0.3424842953681946
Batch 50/64 loss: -0.3104533553123474
Batch 51/64 loss: -0.38595283031463623
Batch 52/64 loss: -0.3690268099308014
Batch 53/64 loss: -0.35579705238342285
Batch 54/64 loss: -0.38019347190856934
Batch 55/64 loss: -0.3819635808467865
Batch 56/64 loss: -0.3622478246688843
Batch 57/64 loss: -0.3641777038574219
Batch 58/64 loss: -0.35610735416412354
Batch 59/64 loss: -0.38907521963119507
Batch 60/64 loss: -0.3486723303794861
Batch 61/64 loss: -0.405106782913208
Batch 62/64 loss: -0.40579232573509216
Batch 63/64 loss: -0.39705535769462585
Batch 64/64 loss: -0.34614360332489014
Epoch 457  Train loss: -0.36702087953978896  Val loss: 0.03547468300127901
Epoch 458
-------------------------------
Batch 1/64 loss: -0.3712306618690491
Batch 2/64 loss: -0.4043777287006378
Batch 3/64 loss: -0.3626514971256256
Batch 4/64 loss: -0.38651150465011597
Batch 5/64 loss: -0.34616726636886597
Batch 6/64 loss: -0.36685383319854736
Batch 7/64 loss: -0.35046011209487915
Batch 8/64 loss: -0.37062984704971313
Batch 9/64 loss: -0.358095645904541
Batch 10/64 loss: -0.37472444772720337
Batch 11/64 loss: -0.39609038829803467
Batch 12/64 loss: -0.37820157408714294
Batch 13/64 loss: -0.37350568175315857
Batch 14/64 loss: -0.36632251739501953
Batch 15/64 loss: -0.3681883215904236
Batch 16/64 loss: -0.40983420610427856
Batch 17/64 loss: -0.4066813886165619
Batch 18/64 loss: -0.36983275413513184
Batch 19/64 loss: -0.32309722900390625
Batch 20/64 loss: -0.3500923812389374
Batch 21/64 loss: -0.3661566376686096
Batch 22/64 loss: -0.3917074203491211
Batch 23/64 loss: -0.37841659784317017
Batch 24/64 loss: -0.3585367798805237
Batch 25/64 loss: -0.3971448838710785
Batch 26/64 loss: -0.37794721126556396
Batch 27/64 loss: -0.32575371861457825
Batch 28/64 loss: -0.36949342489242554
Batch 29/64 loss: -0.33301979303359985
Batch 30/64 loss: -0.38854914903640747
Batch 31/64 loss: -0.3395569920539856
Batch 32/64 loss: -0.3698212206363678
Batch 33/64 loss: -0.3755207061767578
Batch 34/64 loss: -0.3582910895347595
Batch 35/64 loss: -0.3852185308933258
Batch 36/64 loss: -0.3598819971084595
Batch 37/64 loss: -0.3666287362575531
Batch 38/64 loss: -0.35279184579849243
Batch 39/64 loss: -0.39207226037979126
Batch 40/64 loss: -0.3698871433734894
Batch 41/64 loss: -0.3629060983657837
Batch 42/64 loss: -0.3386951684951782
Batch 43/64 loss: -0.3789026737213135
Batch 44/64 loss: -0.3709874451160431
Batch 45/64 loss: -0.34839075803756714
Batch 46/64 loss: -0.37131795287132263
Batch 47/64 loss: -0.3885611295700073
Batch 48/64 loss: -0.3492724895477295
Batch 49/64 loss: -0.3627610206604004
Batch 50/64 loss: -0.3814944922924042
Batch 51/64 loss: -0.38634827733039856
Batch 52/64 loss: -0.36541685461997986
Batch 53/64 loss: -0.3339603841304779
Batch 54/64 loss: -0.36699041724205017
Batch 55/64 loss: -0.36081960797309875
Batch 56/64 loss: -0.3875245153903961
Batch 57/64 loss: -0.34412384033203125
Batch 58/64 loss: -0.3830130100250244
Batch 59/64 loss: -0.3736114501953125
Batch 60/64 loss: -0.37539142370224
Batch 61/64 loss: -0.3643448054790497
Batch 62/64 loss: -0.3977985382080078
Batch 63/64 loss: -0.38541552424430847
Batch 64/64 loss: -0.34527942538261414
Epoch 458  Train loss: -0.36905023642614776  Val loss: 0.03488022027556429
Epoch 459
-------------------------------
Batch 1/64 loss: -0.40161705017089844
Batch 2/64 loss: -0.39567503333091736
Batch 3/64 loss: -0.39810195565223694
Batch 4/64 loss: -0.38772737979888916
Batch 5/64 loss: -0.39015084505081177
Batch 6/64 loss: -0.3330211043357849
Batch 7/64 loss: -0.37804242968559265
Batch 8/64 loss: -0.39371299743652344
Batch 9/64 loss: -0.3697265386581421
Batch 10/64 loss: -0.3973650336265564
Batch 11/64 loss: -0.3764367401599884
Batch 12/64 loss: -0.3338852524757385
Batch 13/64 loss: -0.37707623839378357
Batch 14/64 loss: -0.3982442617416382
Batch 15/64 loss: -0.37403833866119385
Batch 16/64 loss: -0.3664199709892273
Batch 17/64 loss: -0.3313071131706238
Batch 18/64 loss: -0.3758915364742279
Batch 19/64 loss: -0.35137924551963806
Batch 20/64 loss: -0.37154197692871094
Batch 21/64 loss: -0.38281476497650146
Batch 22/64 loss: -0.3561071753501892
Batch 23/64 loss: -0.4043612480163574
Batch 24/64 loss: -0.35461342334747314
Batch 25/64 loss: -0.39403635263442993
Batch 26/64 loss: -0.3444819748401642
Batch 27/64 loss: -0.34657835960388184
Batch 28/64 loss: -0.3586661219596863
Batch 29/64 loss: -0.39015260338783264
Batch 30/64 loss: -0.37633687257766724
Batch 31/64 loss: -0.37111276388168335
Batch 32/64 loss: -0.3726990222930908
Batch 33/64 loss: -0.37504130601882935
Batch 34/64 loss: -0.3790096044540405
Batch 35/64 loss: -0.3713032305240631
Batch 36/64 loss: -0.33268457651138306
Batch 37/64 loss: -0.3955157399177551
Batch 38/64 loss: -0.32965609431266785
Batch 39/64 loss: -0.35931435227394104
Batch 40/64 loss: -0.3531425893306732
Batch 41/64 loss: -0.36065757274627686
Batch 42/64 loss: -0.2986811399459839
Batch 43/64 loss: -0.36543428897857666
Batch 44/64 loss: -0.34577348828315735
Batch 45/64 loss: -0.36038750410079956
Batch 46/64 loss: -0.3585474491119385
Batch 47/64 loss: -0.3406831622123718
Batch 48/64 loss: -0.3363749384880066
Batch 49/64 loss: -0.34763431549072266
Batch 50/64 loss: -0.3726474642753601
Batch 51/64 loss: -0.35487040877342224
Batch 52/64 loss: -0.3611367642879486
Batch 53/64 loss: -0.3700166344642639
Batch 54/64 loss: -0.35714077949523926
Batch 55/64 loss: -0.37468647956848145
Batch 56/64 loss: -0.3480582535266876
Batch 57/64 loss: -0.40539848804473877
Batch 58/64 loss: -0.35538268089294434
Batch 59/64 loss: -0.3728083372116089
Batch 60/64 loss: -0.364889919757843
Batch 61/64 loss: -0.38518914580345154
Batch 62/64 loss: -0.34315186738967896
Batch 63/64 loss: -0.352608859539032
Batch 64/64 loss: -0.34196609258651733
Epoch 459  Train loss: -0.36608037224002915  Val loss: 0.036615248398272855
Epoch 460
-------------------------------
Batch 1/64 loss: -0.3746856451034546
Batch 2/64 loss: -0.3742021322250366
Batch 3/64 loss: -0.36481404304504395
Batch 4/64 loss: -0.3355983793735504
Batch 5/64 loss: -0.3704153895378113
Batch 6/64 loss: -0.3896257281303406
Batch 7/64 loss: -0.3878307044506073
Batch 8/64 loss: -0.36498796939849854
Batch 9/64 loss: -0.299479216337204
Batch 10/64 loss: -0.3815411329269409
Batch 11/64 loss: -0.34796810150146484
Batch 12/64 loss: -0.39371445775032043
Batch 13/64 loss: -0.37908610701560974
Batch 14/64 loss: -0.3616560697555542
Batch 15/64 loss: -0.38877052068710327
Batch 16/64 loss: -0.3541049063205719
Batch 17/64 loss: -0.37353330850601196
Batch 18/64 loss: -0.37194502353668213
Batch 19/64 loss: -0.3338633179664612
Batch 20/64 loss: -0.38394445180892944
Batch 21/64 loss: -0.39526212215423584
Batch 22/64 loss: -0.39727094769477844
Batch 23/64 loss: -0.34268438816070557
Batch 24/64 loss: -0.3857729136943817
Batch 25/64 loss: -0.3962160050868988
Batch 26/64 loss: -0.34011510014533997
Batch 27/64 loss: -0.3227875828742981
Batch 28/64 loss: -0.38285166025161743
Batch 29/64 loss: -0.4036949574947357
Batch 30/64 loss: -0.33607545495033264
Batch 31/64 loss: -0.34620556235313416
Batch 32/64 loss: -0.37716251611709595
Batch 33/64 loss: -0.3941260576248169
Batch 34/64 loss: -0.33130455017089844
Batch 35/64 loss: -0.3780650794506073
Batch 36/64 loss: -0.3322562873363495
Batch 37/64 loss: -0.3195348083972931
Batch 38/64 loss: -0.34143152832984924
Batch 39/64 loss: -0.39440762996673584
Batch 40/64 loss: -0.38564303517341614
Batch 41/64 loss: -0.3673822581768036
Batch 42/64 loss: -0.3670158386230469
Batch 43/64 loss: -0.3210917115211487
Batch 44/64 loss: -0.34599825739860535
Batch 45/64 loss: -0.38006752729415894
Batch 46/64 loss: -0.3446924686431885
Batch 47/64 loss: -0.37382978200912476
Batch 48/64 loss: -0.34920305013656616
Batch 49/64 loss: -0.37673771381378174
Batch 50/64 loss: -0.39330828189849854
Batch 51/64 loss: -0.3820563554763794
Batch 52/64 loss: -0.38098961114883423
Batch 53/64 loss: -0.3423379063606262
Batch 54/64 loss: -0.37367770075798035
Batch 55/64 loss: -0.3647168278694153
Batch 56/64 loss: -0.33169662952423096
Batch 57/64 loss: -0.39858248829841614
Batch 58/64 loss: -0.3348638415336609
Batch 59/64 loss: -0.31719261407852173
Batch 60/64 loss: -0.31981220841407776
Batch 61/64 loss: -0.38195157051086426
Batch 62/64 loss: -0.37573742866516113
Batch 63/64 loss: -0.36909905076026917
Batch 64/64 loss: -0.2865254878997803
Epoch 460  Train loss: -0.36300502010420255  Val loss: 0.03602978679322705
Epoch 461
-------------------------------
Batch 1/64 loss: -0.3808213472366333
Batch 2/64 loss: -0.3551763892173767
Batch 3/64 loss: -0.3858330249786377
Batch 4/64 loss: -0.3275545835494995
Batch 5/64 loss: -0.368952214717865
Batch 6/64 loss: -0.3070206046104431
Batch 7/64 loss: -0.396835058927536
Batch 8/64 loss: -0.3808034062385559
Batch 9/64 loss: -0.39057621359825134
Batch 10/64 loss: -0.35021013021469116
Batch 11/64 loss: -0.387340247631073
Batch 12/64 loss: -0.331707626581192
Batch 13/64 loss: -0.41487473249435425
Batch 14/64 loss: -0.3593781590461731
Batch 15/64 loss: -0.33049148321151733
Batch 16/64 loss: -0.35728055238723755
Batch 17/64 loss: -0.4028725028038025
Batch 18/64 loss: -0.3843527138233185
Batch 19/64 loss: -0.389039009809494
Batch 20/64 loss: -0.3410067558288574
Batch 21/64 loss: -0.3484637439250946
Batch 22/64 loss: -0.3459581732749939
Batch 23/64 loss: -0.3577486276626587
Batch 24/64 loss: -0.33763086795806885
Batch 25/64 loss: -0.33539479970932007
Batch 26/64 loss: -0.3504776954650879
Batch 27/64 loss: -0.3472244143486023
Batch 28/64 loss: -0.36438247561454773
Batch 29/64 loss: -0.34174078702926636
Batch 30/64 loss: -0.3802230954170227
Batch 31/64 loss: -0.35200977325439453
Batch 32/64 loss: -0.3499943017959595
Batch 33/64 loss: -0.3438988924026489
Batch 34/64 loss: -0.3544657826423645
Batch 35/64 loss: -0.36761465668678284
Batch 36/64 loss: -0.3735218048095703
Batch 37/64 loss: -0.38651445508003235
Batch 38/64 loss: -0.3760337829589844
Batch 39/64 loss: -0.3387576937675476
Batch 40/64 loss: -0.3648619055747986
Batch 41/64 loss: -0.36954769492149353
Batch 42/64 loss: -0.40408554673194885
Batch 43/64 loss: -0.3788609504699707
Batch 44/64 loss: -0.36388686299324036
Batch 45/64 loss: -0.32373538613319397
Batch 46/64 loss: -0.3652385473251343
Batch 47/64 loss: -0.3559078574180603
Batch 48/64 loss: -0.39930662512779236
Batch 49/64 loss: -0.34692737460136414
Batch 50/64 loss: -0.33943477272987366
Batch 51/64 loss: -0.3628728985786438
Batch 52/64 loss: -0.3865135610103607
Batch 53/64 loss: -0.3765445053577423
Batch 54/64 loss: -0.35966914892196655
Batch 55/64 loss: -0.35434895753860474
Batch 56/64 loss: -0.40000152587890625
Batch 57/64 loss: -0.38793420791625977
Batch 58/64 loss: -0.3630971312522888
Batch 59/64 loss: -0.3527122735977173
Batch 60/64 loss: -0.369564950466156
Batch 61/64 loss: -0.38206005096435547
Batch 62/64 loss: -0.33604884147644043
Batch 63/64 loss: -0.3630102574825287
Batch 64/64 loss: -0.39729735255241394
Epoch 461  Train loss: -0.3638953322289037  Val loss: 0.03395925907744575
Epoch 462
-------------------------------
Batch 1/64 loss: -0.37189462780952454
Batch 2/64 loss: -0.3268100619316101
Batch 3/64 loss: -0.37397682666778564
Batch 4/64 loss: -0.40157845616340637
Batch 5/64 loss: -0.3952024281024933
Batch 6/64 loss: -0.33017492294311523
Batch 7/64 loss: -0.30668511986732483
Batch 8/64 loss: -0.37782901525497437
Batch 9/64 loss: -0.39383944869041443
Batch 10/64 loss: -0.3997279107570648
Batch 11/64 loss: -0.3710314929485321
Batch 12/64 loss: -0.3588976263999939
Batch 13/64 loss: -0.39218389987945557
Batch 14/64 loss: -0.3591756224632263
Batch 15/64 loss: -0.3910825848579407
Batch 16/64 loss: -0.3903884291648865
Batch 17/64 loss: -0.39368101954460144
Batch 18/64 loss: -0.3786529004573822
Batch 19/64 loss: -0.3282422125339508
Batch 20/64 loss: -0.37082093954086304
Batch 21/64 loss: -0.34211266040802
Batch 22/64 loss: -0.4046207666397095
Batch 23/64 loss: -0.3786735534667969
Batch 24/64 loss: -0.3363923132419586
Batch 25/64 loss: -0.3970191478729248
Batch 26/64 loss: -0.36358582973480225
Batch 27/64 loss: -0.31386980414390564
Batch 28/64 loss: -0.31289148330688477
Batch 29/64 loss: -0.3979758620262146
Batch 30/64 loss: -0.3151170611381531
Batch 31/64 loss: -0.37342607975006104
Batch 32/64 loss: -0.3866393566131592
Batch 33/64 loss: -0.3653792440891266
Batch 34/64 loss: -0.3751983642578125
Batch 35/64 loss: -0.38407719135284424
Batch 36/64 loss: -0.3554607331752777
Batch 37/64 loss: -0.36622917652130127
Batch 38/64 loss: -0.36217182874679565
Batch 39/64 loss: -0.36700868606567383
Batch 40/64 loss: -0.35178789496421814
Batch 41/64 loss: -0.3785582184791565
Batch 42/64 loss: -0.3105110824108124
Batch 43/64 loss: -0.327674925327301
Batch 44/64 loss: -0.3764902949333191
Batch 45/64 loss: -0.3328690528869629
Batch 46/64 loss: -0.3562941551208496
Batch 47/64 loss: -0.3712414503097534
Batch 48/64 loss: -0.38562554121017456
Batch 49/64 loss: -0.3680364489555359
Batch 50/64 loss: -0.3792179822921753
Batch 51/64 loss: -0.3803592324256897
Batch 52/64 loss: -0.35087889432907104
Batch 53/64 loss: -0.3696924149990082
Batch 54/64 loss: -0.40364062786102295
Batch 55/64 loss: -0.3815491199493408
Batch 56/64 loss: -0.3573355972766876
Batch 57/64 loss: -0.38722509145736694
Batch 58/64 loss: -0.4094089865684509
Batch 59/64 loss: -0.3936062157154083
Batch 60/64 loss: -0.3512687087059021
Batch 61/64 loss: -0.3888453245162964
Batch 62/64 loss: -0.3686988949775696
Batch 63/64 loss: -0.32822054624557495
Batch 64/64 loss: -0.33441177010536194
Epoch 462  Train loss: -0.36658149361610415  Val loss: 0.03674701706240677
Epoch 463
-------------------------------
Batch 1/64 loss: -0.3581387996673584
Batch 2/64 loss: -0.3325170874595642
Batch 3/64 loss: -0.3857742249965668
Batch 4/64 loss: -0.3781043291091919
Batch 5/64 loss: -0.3327852785587311
Batch 6/64 loss: -0.3607574701309204
Batch 7/64 loss: -0.301763653755188
Batch 8/64 loss: -0.3765285015106201
Batch 9/64 loss: -0.4043470621109009
Batch 10/64 loss: -0.33767348527908325
Batch 11/64 loss: -0.35223865509033203
Batch 12/64 loss: -0.3935062289237976
Batch 13/64 loss: -0.37248075008392334
Batch 14/64 loss: -0.34860530495643616
Batch 15/64 loss: -0.3888443410396576
Batch 16/64 loss: -0.35485273599624634
Batch 17/64 loss: -0.39708107709884644
Batch 18/64 loss: -0.3675895929336548
Batch 19/64 loss: -0.39055904746055603
Batch 20/64 loss: -0.35320302844047546
Batch 21/64 loss: -0.37315213680267334
Batch 22/64 loss: -0.3485020399093628
Batch 23/64 loss: -0.3914031386375427
Batch 24/64 loss: -0.38851502537727356
Batch 25/64 loss: -0.3431438207626343
Batch 26/64 loss: -0.36320215463638306
Batch 27/64 loss: -0.36545801162719727
Batch 28/64 loss: -0.3936033248901367
Batch 29/64 loss: -0.32602521777153015
Batch 30/64 loss: -0.355122447013855
Batch 31/64 loss: -0.37830203771591187
Batch 32/64 loss: -0.3781149685382843
Batch 33/64 loss: -0.3767913281917572
Batch 34/64 loss: -0.3540629744529724
Batch 35/64 loss: -0.36678576469421387
Batch 36/64 loss: -0.37306591868400574
Batch 37/64 loss: -0.36846816539764404
Batch 38/64 loss: -0.37118855118751526
Batch 39/64 loss: -0.3833123445510864
Batch 40/64 loss: -0.35425662994384766
Batch 41/64 loss: -0.3670269846916199
Batch 42/64 loss: -0.35356903076171875
Batch 43/64 loss: -0.3678628206253052
Batch 44/64 loss: -0.40214037895202637
Batch 45/64 loss: -0.34080708026885986
Batch 46/64 loss: -0.35877159237861633
Batch 47/64 loss: -0.345136821269989
Batch 48/64 loss: -0.4034349322319031
Batch 49/64 loss: -0.3707537353038788
Batch 50/64 loss: -0.37602460384368896
Batch 51/64 loss: -0.3497432470321655
Batch 52/64 loss: -0.3591618835926056
Batch 53/64 loss: -0.372463583946228
Batch 54/64 loss: -0.3005483150482178
Batch 55/64 loss: -0.3723159730434418
Batch 56/64 loss: -0.35598957538604736
Batch 57/64 loss: -0.3693142235279083
Batch 58/64 loss: -0.3543454110622406
Batch 59/64 loss: -0.3495810925960541
Batch 60/64 loss: -0.3783667981624603
Batch 61/64 loss: -0.3600732684135437
Batch 62/64 loss: -0.37631887197494507
Batch 63/64 loss: -0.3698495626449585
Batch 64/64 loss: -0.31857830286026
Epoch 463  Train loss: -0.36442917912614115  Val loss: 0.03635858239996474
Epoch 464
-------------------------------
Batch 1/64 loss: -0.36880558729171753
Batch 2/64 loss: -0.3629803955554962
Batch 3/64 loss: -0.3768313527107239
Batch 4/64 loss: -0.334678053855896
Batch 5/64 loss: -0.38783684372901917
Batch 6/64 loss: -0.35286563634872437
Batch 7/64 loss: -0.3886660933494568
Batch 8/64 loss: -0.3807826638221741
Batch 9/64 loss: -0.37515509128570557
Batch 10/64 loss: -0.3591323792934418
Batch 11/64 loss: -0.33976006507873535
Batch 12/64 loss: -0.3513173460960388
Batch 13/64 loss: -0.39240938425064087
Batch 14/64 loss: -0.3578096330165863
Batch 15/64 loss: -0.3646961748600006
Batch 16/64 loss: -0.4042513966560364
Batch 17/64 loss: -0.39173322916030884
Batch 18/64 loss: -0.37433308362960815
Batch 19/64 loss: -0.34114980697631836
Batch 20/64 loss: -0.41298383474349976
Batch 21/64 loss: -0.384959876537323
Batch 22/64 loss: -0.3642736077308655
Batch 23/64 loss: -0.3465690016746521
Batch 24/64 loss: -0.3672370910644531
Batch 25/64 loss: -0.38269543647766113
Batch 26/64 loss: -0.36390531063079834
Batch 27/64 loss: -0.36768829822540283
Batch 28/64 loss: -0.3856312036514282
Batch 29/64 loss: -0.37279701232910156
Batch 30/64 loss: -0.4059164524078369
Batch 31/64 loss: -0.35972219705581665
Batch 32/64 loss: -0.3943134546279907
Batch 33/64 loss: -0.36913543939590454
Batch 34/64 loss: -0.3775337338447571
Batch 35/64 loss: -0.3594972789287567
Batch 36/64 loss: -0.37516385316848755
Batch 37/64 loss: -0.397769570350647
Batch 38/64 loss: -0.3833993673324585
Batch 39/64 loss: -0.36476993560791016
Batch 40/64 loss: -0.36234304308891296
Batch 41/64 loss: -0.35299399495124817
Batch 42/64 loss: -0.33181560039520264
Batch 43/64 loss: -0.35834357142448425
Batch 44/64 loss: -0.379568487405777
Batch 45/64 loss: -0.39378097653388977
Batch 46/64 loss: -0.34480804204940796
Batch 47/64 loss: -0.38350051641464233
Batch 48/64 loss: -0.3816477954387665
Batch 49/64 loss: -0.39291688799858093
Batch 50/64 loss: -0.38925421237945557
Batch 51/64 loss: -0.3875978887081146
Batch 52/64 loss: -0.4020489454269409
Batch 53/64 loss: -0.2940784692764282
Batch 54/64 loss: -0.38422712683677673
Batch 55/64 loss: -0.38288575410842896
Batch 56/64 loss: -0.3842242956161499
Batch 57/64 loss: -0.3805016279220581
Batch 58/64 loss: -0.38496506214141846
Batch 59/64 loss: -0.3190736770629883
Batch 60/64 loss: -0.39305850863456726
Batch 61/64 loss: -0.3762608766555786
Batch 62/64 loss: -0.3767763078212738
Batch 63/64 loss: -0.34267324209213257
Batch 64/64 loss: -0.3195206820964813
Epoch 464  Train loss: -0.3711394759954191  Val loss: 0.036588107597377295
Epoch 465
-------------------------------
Batch 1/64 loss: -0.3386918902397156
Batch 2/64 loss: -0.3698340058326721
Batch 3/64 loss: -0.3802604079246521
Batch 4/64 loss: -0.3527987003326416
Batch 5/64 loss: -0.3132588565349579
Batch 6/64 loss: -0.35717564821243286
Batch 7/64 loss: -0.3642117977142334
Batch 8/64 loss: -0.34095999598503113
Batch 9/64 loss: -0.37509021162986755
Batch 10/64 loss: -0.3971727192401886
Batch 11/64 loss: -0.3632792532444
Batch 12/64 loss: -0.380130410194397
Batch 13/64 loss: -0.3715517520904541
Batch 14/64 loss: -0.36214393377304077
Batch 15/64 loss: -0.35594290494918823
Batch 16/64 loss: -0.36912187933921814
Batch 17/64 loss: -0.34824520349502563
Batch 18/64 loss: -0.3783598840236664
Batch 19/64 loss: -0.36031875014305115
Batch 20/64 loss: -0.38404133915901184
Batch 21/64 loss: -0.38897061347961426
Batch 22/64 loss: -0.37315332889556885
Batch 23/64 loss: -0.3439754843711853
Batch 24/64 loss: -0.37192627787590027
Batch 25/64 loss: -0.3741139769554138
Batch 26/64 loss: -0.34760865569114685
Batch 27/64 loss: -0.3726300597190857
Batch 28/64 loss: -0.3998941481113434
Batch 29/64 loss: -0.4021422863006592
Batch 30/64 loss: -0.3707021474838257
Batch 31/64 loss: -0.3693145215511322
Batch 32/64 loss: -0.3906916379928589
Batch 33/64 loss: -0.347076416015625
Batch 34/64 loss: -0.33513695001602173
Batch 35/64 loss: -0.3282608389854431
Batch 36/64 loss: -0.36857983469963074
Batch 37/64 loss: -0.39931434392929077
Batch 38/64 loss: -0.3586307168006897
Batch 39/64 loss: -0.3562261462211609
Batch 40/64 loss: -0.3792543411254883
Batch 41/64 loss: -0.34789785742759705
Batch 42/64 loss: -0.37638911604881287
Batch 43/64 loss: -0.3890787661075592
Batch 44/64 loss: -0.39183536171913147
Batch 45/64 loss: -0.3758730888366699
Batch 46/64 loss: -0.36725476384162903
Batch 47/64 loss: -0.37800031900405884
Batch 48/64 loss: -0.37245649099349976
Batch 49/64 loss: -0.39054858684539795
Batch 50/64 loss: -0.35618093609809875
Batch 51/64 loss: -0.3511807322502136
Batch 52/64 loss: -0.3682900071144104
Batch 53/64 loss: -0.35352182388305664
Batch 54/64 loss: -0.3472132086753845
Batch 55/64 loss: -0.3408474624156952
Batch 56/64 loss: -0.3795374631881714
Batch 57/64 loss: -0.3684847354888916
Batch 58/64 loss: -0.3595307469367981
Batch 59/64 loss: -0.3446391224861145
Batch 60/64 loss: -0.3327372074127197
Batch 61/64 loss: -0.3849582076072693
Batch 62/64 loss: -0.3775082230567932
Batch 63/64 loss: -0.3623421788215637
Batch 64/64 loss: -0.3452712595462799
Epoch 465  Train loss: -0.36573258229330474  Val loss: 0.034739372042036545
Epoch 466
-------------------------------
Batch 1/64 loss: -0.38152164220809937
Batch 2/64 loss: -0.39795905351638794
Batch 3/64 loss: -0.4074390232563019
Batch 4/64 loss: -0.34859731793403625
Batch 5/64 loss: -0.3296315670013428
Batch 6/64 loss: -0.31932851672172546
Batch 7/64 loss: -0.36550676822662354
Batch 8/64 loss: -0.3679839074611664
Batch 9/64 loss: -0.3279346823692322
Batch 10/64 loss: -0.329448401927948
Batch 11/64 loss: -0.3550620377063751
Batch 12/64 loss: -0.34657591581344604
Batch 13/64 loss: -0.3194396495819092
Batch 14/64 loss: -0.3716791570186615
Batch 15/64 loss: -0.390622079372406
Batch 16/64 loss: -0.3420306146144867
Batch 17/64 loss: -0.34798893332481384
Batch 18/64 loss: -0.3457503020763397
Batch 19/64 loss: -0.3510303497314453
Batch 20/64 loss: -0.3563022315502167
Batch 21/64 loss: -0.352080374956131
Batch 22/64 loss: -0.35846608877182007
Batch 23/64 loss: -0.36620017886161804
Batch 24/64 loss: -0.3916778266429901
Batch 25/64 loss: -0.3491806387901306
Batch 26/64 loss: -0.39268791675567627
Batch 27/64 loss: -0.38361600041389465
Batch 28/64 loss: -0.35846951603889465
Batch 29/64 loss: -0.3203810751438141
Batch 30/64 loss: -0.40930867195129395
Batch 31/64 loss: -0.39529645442962646
Batch 32/64 loss: -0.38205453753471375
Batch 33/64 loss: -0.3778305649757385
Batch 34/64 loss: -0.3567736744880676
Batch 35/64 loss: -0.35344135761260986
Batch 36/64 loss: -0.3279932737350464
Batch 37/64 loss: -0.38934803009033203
Batch 38/64 loss: -0.3450984060764313
Batch 39/64 loss: -0.36571842432022095
Batch 40/64 loss: -0.37241441011428833
Batch 41/64 loss: -0.3793565034866333
Batch 42/64 loss: -0.36894312500953674
Batch 43/64 loss: -0.3586788773536682
Batch 44/64 loss: -0.36604684591293335
Batch 45/64 loss: -0.33828842639923096
Batch 46/64 loss: -0.35656923055648804
Batch 47/64 loss: -0.363457590341568
Batch 48/64 loss: -0.3318222463130951
Batch 49/64 loss: -0.3774457573890686
Batch 50/64 loss: -0.3669978976249695
Batch 51/64 loss: -0.37149155139923096
Batch 52/64 loss: -0.3755536675453186
Batch 53/64 loss: -0.37745487689971924
Batch 54/64 loss: -0.371589332818985
Batch 55/64 loss: -0.3326031565666199
Batch 56/64 loss: -0.34391114115715027
Batch 57/64 loss: -0.3768482208251953
Batch 58/64 loss: -0.38737428188323975
Batch 59/64 loss: -0.37374234199523926
Batch 60/64 loss: -0.35600823163986206
Batch 61/64 loss: -0.33677008748054504
Batch 62/64 loss: -0.3967955410480499
Batch 63/64 loss: -0.3764839172363281
Batch 64/64 loss: -0.377082884311676
Epoch 466  Train loss: -0.36261826800365077  Val loss: 0.038302602227201166
Epoch 467
-------------------------------
Batch 1/64 loss: -0.37237727642059326
Batch 2/64 loss: -0.35727789998054504
Batch 3/64 loss: -0.35120517015457153
Batch 4/64 loss: -0.36281052231788635
Batch 5/64 loss: -0.34702685475349426
Batch 6/64 loss: -0.3950619697570801
Batch 7/64 loss: -0.364013671875
Batch 8/64 loss: -0.3693057894706726
Batch 9/64 loss: -0.38388311862945557
Batch 10/64 loss: -0.37387311458587646
Batch 11/64 loss: -0.3565935790538788
Batch 12/64 loss: -0.3534680902957916
Batch 13/64 loss: -0.3582952618598938
Batch 14/64 loss: -0.3662378787994385
Batch 15/64 loss: -0.35652974247932434
Batch 16/64 loss: -0.38044750690460205
Batch 17/64 loss: -0.38804760575294495
Batch 18/64 loss: -0.3417544364929199
Batch 19/64 loss: -0.3755483031272888
Batch 20/64 loss: -0.3656606078147888
Batch 21/64 loss: -0.38850608468055725
Batch 22/64 loss: -0.3813711702823639
Batch 23/64 loss: -0.377292275428772
Batch 24/64 loss: -0.3814513683319092
Batch 25/64 loss: -0.4085596799850464
Batch 26/64 loss: -0.3819597661495209
Batch 27/64 loss: -0.38260525465011597
Batch 28/64 loss: -0.32141390442848206
Batch 29/64 loss: -0.3588840663433075
Batch 30/64 loss: -0.3305619955062866
Batch 31/64 loss: -0.3982813358306885
Batch 32/64 loss: -0.39036667346954346
Batch 33/64 loss: -0.38571447134017944
Batch 34/64 loss: -0.35757771134376526
Batch 35/64 loss: -0.36977899074554443
Batch 36/64 loss: -0.3257686495780945
Batch 37/64 loss: -0.3988763093948364
Batch 38/64 loss: -0.35871389508247375
Batch 39/64 loss: -0.34557250142097473
Batch 40/64 loss: -0.38581687211990356
Batch 41/64 loss: -0.3799217641353607
Batch 42/64 loss: -0.3591505289077759
Batch 43/64 loss: -0.38352447748184204
Batch 44/64 loss: -0.37177106738090515
Batch 45/64 loss: -0.39197778701782227
Batch 46/64 loss: -0.3906664252281189
Batch 47/64 loss: -0.3462951183319092
Batch 48/64 loss: -0.3349701762199402
Batch 49/64 loss: -0.37863707542419434
Batch 50/64 loss: -0.32532191276550293
Batch 51/64 loss: -0.39512598514556885
Batch 52/64 loss: -0.34500664472579956
Batch 53/64 loss: -0.41417360305786133
Batch 54/64 loss: -0.383189857006073
Batch 55/64 loss: -0.34317582845687866
Batch 56/64 loss: -0.3514341115951538
Batch 57/64 loss: -0.355272114276886
Batch 58/64 loss: -0.35929638147354126
Batch 59/64 loss: -0.37030094861984253
Batch 60/64 loss: -0.3702939748764038
Batch 61/64 loss: -0.37089619040489197
Batch 62/64 loss: -0.391698956489563
Batch 63/64 loss: -0.37502503395080566
Batch 64/64 loss: -0.3721297085285187
Epoch 467  Train loss: -0.3688582685648226  Val loss: 0.03360697842135872
Epoch 468
-------------------------------
Batch 1/64 loss: -0.35850244760513306
Batch 2/64 loss: -0.322304368019104
Batch 3/64 loss: -0.374305784702301
Batch 4/64 loss: -0.3358350396156311
Batch 5/64 loss: -0.37418633699417114
Batch 6/64 loss: -0.37087783217430115
Batch 7/64 loss: -0.36009830236434937
Batch 8/64 loss: -0.4026501178741455
Batch 9/64 loss: -0.36432772874832153
Batch 10/64 loss: -0.37378907203674316
Batch 11/64 loss: -0.36263859272003174
Batch 12/64 loss: -0.3928147852420807
Batch 13/64 loss: -0.39387840032577515
Batch 14/64 loss: -0.3585461974143982
Batch 15/64 loss: -0.3509543538093567
Batch 16/64 loss: -0.3999256491661072
Batch 17/64 loss: -0.34868311882019043
Batch 18/64 loss: -0.35604724287986755
Batch 19/64 loss: -0.394144207239151
Batch 20/64 loss: -0.3815499544143677
Batch 21/64 loss: -0.3819221258163452
Batch 22/64 loss: -0.39154112339019775
Batch 23/64 loss: -0.3790495991706848
Batch 24/64 loss: -0.3682737946510315
Batch 25/64 loss: -0.3681923747062683
Batch 26/64 loss: -0.3792967200279236
Batch 27/64 loss: -0.37369096279144287
Batch 28/64 loss: -0.33396947383880615
Batch 29/64 loss: -0.311930775642395
Batch 30/64 loss: -0.4080333709716797
Batch 31/64 loss: -0.3596281111240387
Batch 32/64 loss: -0.36030474305152893
Batch 33/64 loss: -0.33817407488822937
Batch 34/64 loss: -0.35744133591651917
Batch 35/64 loss: -0.3636661171913147
Batch 36/64 loss: -0.3733188509941101
Batch 37/64 loss: -0.38650941848754883
Batch 38/64 loss: -0.33966439962387085
Batch 39/64 loss: -0.383661687374115
Batch 40/64 loss: -0.40298745036125183
Batch 41/64 loss: -0.38537293672561646
Batch 42/64 loss: -0.3702343702316284
Batch 43/64 loss: -0.38970524072647095
Batch 44/64 loss: -0.3438073992729187
Batch 45/64 loss: -0.39185237884521484
Batch 46/64 loss: -0.378426730632782
Batch 47/64 loss: -0.37613117694854736
Batch 48/64 loss: -0.37914979457855225
Batch 49/64 loss: -0.3870214819908142
Batch 50/64 loss: -0.3684881329536438
Batch 51/64 loss: -0.38553833961486816
Batch 52/64 loss: -0.3559301197528839
Batch 53/64 loss: -0.3619764745235443
Batch 54/64 loss: -0.3936173617839813
Batch 55/64 loss: -0.3387693762779236
Batch 56/64 loss: -0.3666607737541199
Batch 57/64 loss: -0.41165870428085327
Batch 58/64 loss: -0.37539130449295044
Batch 59/64 loss: -0.3920706510543823
Batch 60/64 loss: -0.35534173250198364
Batch 61/64 loss: -0.3517422378063202
Batch 62/64 loss: -0.34854549169540405
Batch 63/64 loss: -0.3708827495574951
Batch 64/64 loss: -0.36217382550239563
Epoch 468  Train loss: -0.36999625680493375  Val loss: 0.03383855475592859
Epoch 469
-------------------------------
Batch 1/64 loss: -0.3985814154148102
Batch 2/64 loss: -0.35588330030441284
Batch 3/64 loss: -0.3753349184989929
Batch 4/64 loss: -0.3585778772830963
Batch 5/64 loss: -0.41023892164230347
Batch 6/64 loss: -0.38086771965026855
Batch 7/64 loss: -0.3347802758216858
Batch 8/64 loss: -0.363075315952301
Batch 9/64 loss: -0.36815112829208374
Batch 10/64 loss: -0.3749983608722687
Batch 11/64 loss: -0.37393221259117126
Batch 12/64 loss: -0.3902847170829773
Batch 13/64 loss: -0.3613045811653137
Batch 14/64 loss: -0.28220194578170776
Batch 15/64 loss: -0.32965147495269775
Batch 16/64 loss: -0.36573508381843567
Batch 17/64 loss: -0.36398130655288696
Batch 18/64 loss: -0.4052883982658386
Batch 19/64 loss: -0.40644484758377075
Batch 20/64 loss: -0.3629120886325836
Batch 21/64 loss: -0.37123626470565796
Batch 22/64 loss: -0.3929900527000427
Batch 23/64 loss: -0.40105390548706055
Batch 24/64 loss: -0.395940363407135
Batch 25/64 loss: -0.3753829598426819
Batch 26/64 loss: -0.39388930797576904
Batch 27/64 loss: -0.37672948837280273
Batch 28/64 loss: -0.38143837451934814
Batch 29/64 loss: -0.35528144240379333
Batch 30/64 loss: -0.38081467151641846
Batch 31/64 loss: -0.36361733078956604
Batch 32/64 loss: -0.3558567762374878
Batch 33/64 loss: -0.31082433462142944
Batch 34/64 loss: -0.3967607915401459
Batch 35/64 loss: -0.36659979820251465
Batch 36/64 loss: -0.36131298542022705
Batch 37/64 loss: -0.3576740026473999
Batch 38/64 loss: -0.3865543007850647
Batch 39/64 loss: -0.33404114842414856
Batch 40/64 loss: -0.399344265460968
Batch 41/64 loss: -0.3461678624153137
Batch 42/64 loss: -0.33466097712516785
Batch 43/64 loss: -0.32961875200271606
Batch 44/64 loss: -0.4003501832485199
Batch 45/64 loss: -0.37071892619132996
Batch 46/64 loss: -0.3740948438644409
Batch 47/64 loss: -0.35361963510513306
Batch 48/64 loss: -0.35120609402656555
Batch 49/64 loss: -0.4054639935493469
Batch 50/64 loss: -0.3690352141857147
Batch 51/64 loss: -0.31871381402015686
Batch 52/64 loss: -0.3674609959125519
Batch 53/64 loss: -0.3482966423034668
Batch 54/64 loss: -0.37184131145477295
Batch 55/64 loss: -0.35025325417518616
Batch 56/64 loss: -0.36035823822021484
Batch 57/64 loss: -0.36537033319473267
Batch 58/64 loss: -0.3044624328613281
Batch 59/64 loss: -0.3313330411911011
Batch 60/64 loss: -0.38546884059906006
Batch 61/64 loss: -0.30173879861831665
Batch 62/64 loss: -0.392337441444397
Batch 63/64 loss: -0.3681944012641907
Batch 64/64 loss: -0.37009525299072266
Epoch 469  Train loss: -0.36545734779507505  Val loss: 0.03682106979114493
Epoch 470
-------------------------------
Batch 1/64 loss: -0.374448299407959
Batch 2/64 loss: -0.3430919647216797
Batch 3/64 loss: -0.37719255685806274
Batch 4/64 loss: -0.39958345890045166
Batch 5/64 loss: -0.329741895198822
Batch 6/64 loss: -0.40879178047180176
Batch 7/64 loss: -0.3532969355583191
Batch 8/64 loss: -0.36863893270492554
Batch 9/64 loss: -0.389388769865036
Batch 10/64 loss: -0.38656386733055115
Batch 11/64 loss: -0.37781184911727905
Batch 12/64 loss: -0.3514450192451477
Batch 13/64 loss: -0.3849603533744812
Batch 14/64 loss: -0.37918931245803833
Batch 15/64 loss: -0.3702772855758667
Batch 16/64 loss: -0.3687530755996704
Batch 17/64 loss: -0.32584309577941895
Batch 18/64 loss: -0.33551865816116333
Batch 19/64 loss: -0.3646722435951233
Batch 20/64 loss: -0.33360612392425537
Batch 21/64 loss: -0.34979861974716187
Batch 22/64 loss: -0.3763105273246765
Batch 23/64 loss: -0.3726443946361542
Batch 24/64 loss: -0.38199329376220703
Batch 25/64 loss: -0.35358044505119324
Batch 26/64 loss: -0.3547958433628082
Batch 27/64 loss: -0.37865227460861206
Batch 28/64 loss: -0.3974216878414154
Batch 29/64 loss: -0.36368587613105774
Batch 30/64 loss: -0.3171173930168152
Batch 31/64 loss: -0.3896770477294922
Batch 32/64 loss: -0.3755522072315216
Batch 33/64 loss: -0.3860880136489868
Batch 34/64 loss: -0.3519686460494995
Batch 35/64 loss: -0.38694262504577637
Batch 36/64 loss: -0.38068631291389465
Batch 37/64 loss: -0.3796064853668213
Batch 38/64 loss: -0.3408060073852539
Batch 39/64 loss: -0.39157891273498535
Batch 40/64 loss: -0.3966841399669647
Batch 41/64 loss: -0.3875257968902588
Batch 42/64 loss: -0.3832448720932007
Batch 43/64 loss: -0.37717577815055847
Batch 44/64 loss: -0.37773579359054565
Batch 45/64 loss: -0.37418219447135925
Batch 46/64 loss: -0.363697350025177
Batch 47/64 loss: -0.3879561424255371
Batch 48/64 loss: -0.35684260725975037
Batch 49/64 loss: -0.38057923316955566
Batch 50/64 loss: -0.3696925640106201
Batch 51/64 loss: -0.3630804717540741
Batch 52/64 loss: -0.37658536434173584
Batch 53/64 loss: -0.3550254702568054
Batch 54/64 loss: -0.40358465909957886
Batch 55/64 loss: -0.3687426447868347
Batch 56/64 loss: -0.4004785716533661
Batch 57/64 loss: -0.371151864528656
Batch 58/64 loss: -0.330621600151062
Batch 59/64 loss: -0.37800151109695435
Batch 60/64 loss: -0.37820854783058167
Batch 61/64 loss: -0.3798854649066925
Batch 62/64 loss: -0.32489126920700073
Batch 63/64 loss: -0.3953714072704315
Batch 64/64 loss: -0.3524579107761383
Epoch 470  Train loss: -0.3701491897012673  Val loss: 0.036393948641839306
Epoch 471
-------------------------------
Batch 1/64 loss: -0.35595405101776123
Batch 2/64 loss: -0.3531244993209839
Batch 3/64 loss: -0.3865050673484802
Batch 4/64 loss: -0.39863938093185425
Batch 5/64 loss: -0.39439845085144043
Batch 6/64 loss: -0.32711783051490784
Batch 7/64 loss: -0.3755667805671692
Batch 8/64 loss: -0.38246265053749084
Batch 9/64 loss: -0.377440482378006
Batch 10/64 loss: -0.3816734552383423
Batch 11/64 loss: -0.3694835901260376
Batch 12/64 loss: -0.4024695158004761
Batch 13/64 loss: -0.3938492238521576
Batch 14/64 loss: -0.3808906078338623
Batch 15/64 loss: -0.3707806468009949
Batch 16/64 loss: -0.39951422810554504
Batch 17/64 loss: -0.34807318449020386
Batch 18/64 loss: -0.3790758550167084
Batch 19/64 loss: -0.34085705876350403
Batch 20/64 loss: -0.37767744064331055
Batch 21/64 loss: -0.37194469571113586
Batch 22/64 loss: -0.3707795739173889
Batch 23/64 loss: -0.3470812439918518
Batch 24/64 loss: -0.34407368302345276
Batch 25/64 loss: -0.35183221101760864
Batch 26/64 loss: -0.35560280084609985
Batch 27/64 loss: -0.37484636902809143
Batch 28/64 loss: -0.35726338624954224
Batch 29/64 loss: -0.3780476450920105
Batch 30/64 loss: -0.38702332973480225
Batch 31/64 loss: -0.35746705532073975
Batch 32/64 loss: -0.40191173553466797
Batch 33/64 loss: -0.3778870105743408
Batch 34/64 loss: -0.373459130525589
Batch 35/64 loss: -0.3679625391960144
Batch 36/64 loss: -0.37333616614341736
Batch 37/64 loss: -0.37423980236053467
Batch 38/64 loss: -0.3654366135597229
Batch 39/64 loss: -0.34037744998931885
Batch 40/64 loss: -0.364155113697052
Batch 41/64 loss: -0.384701669216156
Batch 42/64 loss: -0.367057740688324
Batch 43/64 loss: -0.37851008772850037
Batch 44/64 loss: -0.40052297711372375
Batch 45/64 loss: -0.3626514673233032
Batch 46/64 loss: -0.35114023089408875
Batch 47/64 loss: -0.37667137384414673
Batch 48/64 loss: -0.380204439163208
Batch 49/64 loss: -0.39753609895706177
Batch 50/64 loss: -0.3433845639228821
Batch 51/64 loss: -0.3940708637237549
Batch 52/64 loss: -0.3784780204296112
Batch 53/64 loss: -0.3485770523548126
Batch 54/64 loss: -0.3488471508026123
Batch 55/64 loss: -0.3854525685310364
Batch 56/64 loss: -0.32052159309387207
Batch 57/64 loss: -0.3708444833755493
Batch 58/64 loss: -0.368829607963562
Batch 59/64 loss: -0.3489112854003906
Batch 60/64 loss: -0.3779315948486328
Batch 61/64 loss: -0.3664278984069824
Batch 62/64 loss: -0.35434848070144653
Batch 63/64 loss: -0.3806344270706177
Batch 64/64 loss: -0.3667181134223938
Epoch 471  Train loss: -0.3700639343729206  Val loss: 0.038458641452068315
Epoch 472
-------------------------------
Batch 1/64 loss: -0.34814029932022095
Batch 2/64 loss: -0.3543446660041809
Batch 3/64 loss: -0.33472713828086853
Batch 4/64 loss: -0.3585638105869293
Batch 5/64 loss: -0.3879389464855194
Batch 6/64 loss: -0.39293789863586426
Batch 7/64 loss: -0.3740098476409912
Batch 8/64 loss: -0.37064802646636963
Batch 9/64 loss: -0.37992435693740845
Batch 10/64 loss: -0.38387995958328247
Batch 11/64 loss: -0.36384767293930054
Batch 12/64 loss: -0.3977752923965454
Batch 13/64 loss: -0.34597212076187134
Batch 14/64 loss: -0.3818212151527405
Batch 15/64 loss: -0.3882755637168884
Batch 16/64 loss: -0.3726055324077606
Batch 17/64 loss: -0.3784564733505249
Batch 18/64 loss: -0.3655450642108917
Batch 19/64 loss: -0.3204840123653412
Batch 20/64 loss: -0.3876984715461731
Batch 21/64 loss: -0.38265639543533325
Batch 22/64 loss: -0.3793444037437439
Batch 23/64 loss: -0.3667360544204712
Batch 24/64 loss: -0.37120383977890015
Batch 25/64 loss: -0.37851783633232117
Batch 26/64 loss: -0.36572757363319397
Batch 27/64 loss: -0.33891308307647705
Batch 28/64 loss: -0.38058727979660034
Batch 29/64 loss: -0.38328707218170166
Batch 30/64 loss: -0.3706195652484894
Batch 31/64 loss: -0.3757951855659485
Batch 32/64 loss: -0.37686923146247864
Batch 33/64 loss: -0.4010016918182373
Batch 34/64 loss: -0.3834138512611389
Batch 35/64 loss: -0.37806275486946106
Batch 36/64 loss: -0.3482973873615265
Batch 37/64 loss: -0.3911854028701782
Batch 38/64 loss: -0.34961143136024475
Batch 39/64 loss: -0.3818798065185547
Batch 40/64 loss: -0.35895800590515137
Batch 41/64 loss: -0.3561010956764221
Batch 42/64 loss: -0.3805481791496277
Batch 43/64 loss: -0.3969374895095825
Batch 44/64 loss: -0.33532261848449707
Batch 45/64 loss: -0.37848541140556335
Batch 46/64 loss: -0.39296963810920715
Batch 47/64 loss: -0.3943708539009094
Batch 48/64 loss: -0.3952745497226715
Batch 49/64 loss: -0.37730520963668823
Batch 50/64 loss: -0.38868042826652527
Batch 51/64 loss: -0.3738277554512024
Batch 52/64 loss: -0.4018908739089966
Batch 53/64 loss: -0.35113823413848877
Batch 54/64 loss: -0.3435601592063904
Batch 55/64 loss: -0.3851526379585266
Batch 56/64 loss: -0.35117650032043457
Batch 57/64 loss: -0.39198559522628784
Batch 58/64 loss: -0.33585649728775024
Batch 59/64 loss: -0.38266265392303467
Batch 60/64 loss: -0.39126354455947876
Batch 61/64 loss: -0.3597829341888428
Batch 62/64 loss: -0.35097649693489075
Batch 63/64 loss: -0.3951432704925537
Batch 64/64 loss: -0.2631058990955353
Epoch 472  Train loss: -0.3711059807562361  Val loss: 0.03535936644806485
Epoch 473
-------------------------------
Batch 1/64 loss: -0.3356989622116089
Batch 2/64 loss: -0.36985138058662415
Batch 3/64 loss: -0.39493557810783386
Batch 4/64 loss: -0.3966257870197296
Batch 5/64 loss: -0.35920780897140503
Batch 6/64 loss: -0.39989909529685974
Batch 7/64 loss: -0.3876221776008606
Batch 8/64 loss: -0.34784674644470215
Batch 9/64 loss: -0.31335556507110596
Batch 10/64 loss: -0.3666735291481018
Batch 11/64 loss: -0.40385180711746216
Batch 12/64 loss: -0.3309868574142456
Batch 13/64 loss: -0.3879265785217285
Batch 14/64 loss: -0.3698604106903076
Batch 15/64 loss: -0.33976614475250244
Batch 16/64 loss: -0.3633692264556885
Batch 17/64 loss: -0.3812750577926636
Batch 18/64 loss: -0.361430823802948
Batch 19/64 loss: -0.36722689867019653
Batch 20/64 loss: -0.329659104347229
Batch 21/64 loss: -0.37287190556526184
Batch 22/64 loss: -0.36525586247444153
Batch 23/64 loss: -0.33845528960227966
Batch 24/64 loss: -0.4020572602748871
Batch 25/64 loss: -0.3768810033798218
Batch 26/64 loss: -0.3655490279197693
Batch 27/64 loss: -0.358426034450531
Batch 28/64 loss: -0.37080004811286926
Batch 29/64 loss: -0.37597891688346863
Batch 30/64 loss: -0.36889398097991943
Batch 31/64 loss: -0.3786061406135559
Batch 32/64 loss: -0.377271831035614
Batch 33/64 loss: -0.3327573537826538
Batch 34/64 loss: -0.3303191661834717
Batch 35/64 loss: -0.3691343069076538
Batch 36/64 loss: -0.3599102795124054
Batch 37/64 loss: -0.3485569953918457
Batch 38/64 loss: -0.4019070267677307
Batch 39/64 loss: -0.39834511280059814
Batch 40/64 loss: -0.3730146586894989
Batch 41/64 loss: -0.3700626492500305
Batch 42/64 loss: -0.3989454209804535
Batch 43/64 loss: -0.33054178953170776
Batch 44/64 loss: -0.3280826210975647
Batch 45/64 loss: -0.4015960395336151
Batch 46/64 loss: -0.40040743350982666
Batch 47/64 loss: -0.3886987566947937
Batch 48/64 loss: -0.39258188009262085
Batch 49/64 loss: -0.37993767857551575
Batch 50/64 loss: -0.32089048624038696
Batch 51/64 loss: -0.36445707082748413
Batch 52/64 loss: -0.33873605728149414
Batch 53/64 loss: -0.4077072739601135
Batch 54/64 loss: -0.36856019496917725
Batch 55/64 loss: -0.3757176995277405
Batch 56/64 loss: -0.3845043182373047
Batch 57/64 loss: -0.4077550172805786
Batch 58/64 loss: -0.39149603247642517
Batch 59/64 loss: -0.372857928276062
Batch 60/64 loss: -0.3348892033100128
Batch 61/64 loss: -0.367410272359848
Batch 62/64 loss: -0.36888909339904785
Batch 63/64 loss: -0.38202762603759766
Batch 64/64 loss: -0.3431481122970581
Epoch 473  Train loss: -0.3687243195141063  Val loss: 0.036691090085662105
Epoch 474
-------------------------------
Batch 1/64 loss: -0.3481931984424591
Batch 2/64 loss: -0.3610316812992096
Batch 3/64 loss: -0.3543349504470825
Batch 4/64 loss: -0.3310362696647644
Batch 5/64 loss: -0.3648321032524109
Batch 6/64 loss: -0.39150312542915344
Batch 7/64 loss: -0.3934951424598694
Batch 8/64 loss: -0.363287091255188
Batch 9/64 loss: -0.3673707842826843
Batch 10/64 loss: -0.37911275029182434
Batch 11/64 loss: -0.38635343313217163
Batch 12/64 loss: -0.39262107014656067
Batch 13/64 loss: -0.3614546060562134
Batch 14/64 loss: -0.4067344069480896
Batch 15/64 loss: -0.3756231665611267
Batch 16/64 loss: -0.3667390048503876
Batch 17/64 loss: -0.36537182331085205
Batch 18/64 loss: -0.37920019030570984
Batch 19/64 loss: -0.3663749694824219
Batch 20/64 loss: -0.38486263155937195
Batch 21/64 loss: -0.39231544733047485
Batch 22/64 loss: -0.3713635802268982
Batch 23/64 loss: -0.3629816174507141
Batch 24/64 loss: -0.36724674701690674
Batch 25/64 loss: -0.3491760194301605
Batch 26/64 loss: -0.3642386496067047
Batch 27/64 loss: -0.33893269300460815
Batch 28/64 loss: -0.36772388219833374
Batch 29/64 loss: -0.3872222602367401
Batch 30/64 loss: -0.39403146505355835
Batch 31/64 loss: -0.3126232326030731
Batch 32/64 loss: -0.3714028596878052
Batch 33/64 loss: -0.3660878539085388
Batch 34/64 loss: -0.3858761787414551
Batch 35/64 loss: -0.38070040941238403
Batch 36/64 loss: -0.4055388569831848
Batch 37/64 loss: -0.3252628743648529
Batch 38/64 loss: -0.33011823892593384
Batch 39/64 loss: -0.3554796874523163
Batch 40/64 loss: -0.3797292709350586
Batch 41/64 loss: -0.38333600759506226
Batch 42/64 loss: -0.3530832529067993
Batch 43/64 loss: -0.37428319454193115
Batch 44/64 loss: -0.38005495071411133
Batch 45/64 loss: -0.3659680187702179
Batch 46/64 loss: -0.36043989658355713
Batch 47/64 loss: -0.3691136837005615
Batch 48/64 loss: -0.38033103942871094
Batch 49/64 loss: -0.38569819927215576
Batch 50/64 loss: -0.414106547832489
Batch 51/64 loss: -0.3681771159172058
Batch 52/64 loss: -0.3577495217323303
Batch 53/64 loss: -0.3491482138633728
Batch 54/64 loss: -0.36767271161079407
Batch 55/64 loss: -0.3118430972099304
Batch 56/64 loss: -0.38416561484336853
Batch 57/64 loss: -0.35292932391166687
Batch 58/64 loss: -0.39390096068382263
Batch 59/64 loss: -0.3543432056903839
Batch 60/64 loss: -0.412087619304657
Batch 61/64 loss: -0.3701888918876648
Batch 62/64 loss: -0.3846486210823059
Batch 63/64 loss: -0.39254724979400635
Batch 64/64 loss: -0.37040358781814575
Epoch 474  Train loss: -0.3700580996625564  Val loss: 0.036557875548031735
Epoch 475
-------------------------------
Batch 1/64 loss: -0.3916252851486206
Batch 2/64 loss: -0.4006724953651428
Batch 3/64 loss: -0.4086587429046631
Batch 4/64 loss: -0.3768511414527893
Batch 5/64 loss: -0.364510178565979
Batch 6/64 loss: -0.37573403120040894
Batch 7/64 loss: -0.3933180570602417
Batch 8/64 loss: -0.3919854164123535
Batch 9/64 loss: -0.37475407123565674
Batch 10/64 loss: -0.3997485637664795
Batch 11/64 loss: -0.3826410174369812
Batch 12/64 loss: -0.39980798959732056
Batch 13/64 loss: -0.32784247398376465
Batch 14/64 loss: -0.36661016941070557
Batch 15/64 loss: -0.3746686577796936
Batch 16/64 loss: -0.40468528866767883
Batch 17/64 loss: -0.3677580952644348
Batch 18/64 loss: -0.4024948477745056
Batch 19/64 loss: -0.36191433668136597
Batch 20/64 loss: -0.3431489169597626
Batch 21/64 loss: -0.39042389392852783
Batch 22/64 loss: -0.3502824902534485
Batch 23/64 loss: -0.2932116389274597
Batch 24/64 loss: -0.3724273443222046
Batch 25/64 loss: -0.39376193284988403
Batch 26/64 loss: -0.37966033816337585
Batch 27/64 loss: -0.38031864166259766
Batch 28/64 loss: -0.3674467206001282
Batch 29/64 loss: -0.3955693244934082
Batch 30/64 loss: -0.3390648365020752
Batch 31/64 loss: -0.36893612146377563
Batch 32/64 loss: -0.37882912158966064
Batch 33/64 loss: -0.3453564941883087
Batch 34/64 loss: -0.3471355140209198
Batch 35/64 loss: -0.37387609481811523
Batch 36/64 loss: -0.31698858737945557
Batch 37/64 loss: -0.3615860939025879
Batch 38/64 loss: -0.3740309476852417
Batch 39/64 loss: -0.37374478578567505
Batch 40/64 loss: -0.3471393585205078
Batch 41/64 loss: -0.34921249747276306
Batch 42/64 loss: -0.37136727571487427
Batch 43/64 loss: -0.39919084310531616
Batch 44/64 loss: -0.36671900749206543
Batch 45/64 loss: -0.37028974294662476
Batch 46/64 loss: -0.3745451271533966
Batch 47/64 loss: -0.36515408754348755
Batch 48/64 loss: -0.38875776529312134
Batch 49/64 loss: -0.3613094091415405
Batch 50/64 loss: -0.3759433627128601
Batch 51/64 loss: -0.35385847091674805
Batch 52/64 loss: -0.3851790428161621
Batch 53/64 loss: -0.3817552924156189
Batch 54/64 loss: -0.34186673164367676
Batch 55/64 loss: -0.3776663839817047
Batch 56/64 loss: -0.3679395914077759
Batch 57/64 loss: -0.370635986328125
Batch 58/64 loss: -0.36479315161705017
Batch 59/64 loss: -0.35024160146713257
Batch 60/64 loss: -0.3615180552005768
Batch 61/64 loss: -0.32411468029022217
Batch 62/64 loss: -0.34485864639282227
Batch 63/64 loss: -0.35431379079818726
Batch 64/64 loss: -0.3842839002609253
Epoch 475  Train loss: -0.36939080041997574  Val loss: 0.03675021666431755
Epoch 476
-------------------------------
Batch 1/64 loss: -0.4050275683403015
Batch 2/64 loss: -0.3849816620349884
Batch 3/64 loss: -0.4078090786933899
Batch 4/64 loss: -0.3452989459037781
Batch 5/64 loss: -0.3261149525642395
Batch 6/64 loss: -0.34083521366119385
Batch 7/64 loss: -0.4054766893386841
Batch 8/64 loss: -0.40220481157302856
Batch 9/64 loss: -0.3794409930706024
Batch 10/64 loss: -0.3581323027610779
Batch 11/64 loss: -0.3319222629070282
Batch 12/64 loss: -0.34886634349823
Batch 13/64 loss: -0.38363003730773926
Batch 14/64 loss: -0.3854454755783081
Batch 15/64 loss: -0.33841824531555176
Batch 16/64 loss: -0.35270053148269653
Batch 17/64 loss: -0.34970980882644653
Batch 18/64 loss: -0.40065398812294006
Batch 19/64 loss: -0.3333311378955841
Batch 20/64 loss: -0.39856117963790894
Batch 21/64 loss: -0.27712151408195496
Batch 22/64 loss: -0.38696354627609253
Batch 23/64 loss: -0.3921639621257782
Batch 24/64 loss: -0.36816149950027466
Batch 25/64 loss: -0.3459482789039612
Batch 26/64 loss: -0.3547329604625702
Batch 27/64 loss: -0.39026397466659546
Batch 28/64 loss: -0.37743866443634033
Batch 29/64 loss: -0.39862775802612305
Batch 30/64 loss: -0.3530255854129791
Batch 31/64 loss: -0.38054996728897095
Batch 32/64 loss: -0.3562714755535126
Batch 33/64 loss: -0.3726729154586792
Batch 34/64 loss: -0.40179580450057983
Batch 35/64 loss: -0.3907959759235382
Batch 36/64 loss: -0.3658524453639984
Batch 37/64 loss: -0.3601328730583191
Batch 38/64 loss: -0.36323004961013794
Batch 39/64 loss: -0.3263065814971924
Batch 40/64 loss: -0.3820725977420807
Batch 41/64 loss: -0.3495635390281677
Batch 42/64 loss: -0.3670044243335724
Batch 43/64 loss: -0.3586965799331665
Batch 44/64 loss: -0.28359687328338623
Batch 45/64 loss: -0.3696603775024414
Batch 46/64 loss: -0.36558595299720764
Batch 47/64 loss: -0.34796828031539917
Batch 48/64 loss: -0.38637152314186096
Batch 49/64 loss: -0.3782566487789154
Batch 50/64 loss: -0.3562504053115845
Batch 51/64 loss: -0.38915085792541504
Batch 52/64 loss: -0.3623376488685608
Batch 53/64 loss: -0.3920791745185852
Batch 54/64 loss: -0.33701831102371216
Batch 55/64 loss: -0.3922646641731262
Batch 56/64 loss: -0.40395307540893555
Batch 57/64 loss: -0.3780471682548523
Batch 58/64 loss: -0.332683265209198
Batch 59/64 loss: -0.36719536781311035
Batch 60/64 loss: -0.347993940114975
Batch 61/64 loss: -0.3931960463523865
Batch 62/64 loss: -0.3832557797431946
Batch 63/64 loss: -0.3817179203033447
Batch 64/64 loss: -0.37033993005752563
Epoch 476  Train loss: -0.3674398812593198  Val loss: 0.037878940195562094
Epoch 477
-------------------------------
Batch 1/64 loss: -0.35706567764282227
Batch 2/64 loss: -0.302002489566803
Batch 3/64 loss: -0.3437443971633911
Batch 4/64 loss: -0.3684360384941101
Batch 5/64 loss: -0.3795519471168518
Batch 6/64 loss: -0.4017064571380615
Batch 7/64 loss: -0.37786054611206055
Batch 8/64 loss: -0.33910149335861206
Batch 9/64 loss: -0.3959220051765442
Batch 10/64 loss: -0.380465030670166
Batch 11/64 loss: -0.31008827686309814
Batch 12/64 loss: -0.36920738220214844
Batch 13/64 loss: -0.39068102836608887
Batch 14/64 loss: -0.37614551186561584
Batch 15/64 loss: -0.3643760681152344
Batch 16/64 loss: -0.34413495659828186
Batch 17/64 loss: -0.3714594841003418
Batch 18/64 loss: -0.3541928827762604
Batch 19/64 loss: -0.3657754063606262
Batch 20/64 loss: -0.34969162940979004
Batch 21/64 loss: -0.3817903995513916
Batch 22/64 loss: -0.38492029905319214
Batch 23/64 loss: -0.3494044542312622
Batch 24/64 loss: -0.37239140272140503
Batch 25/64 loss: -0.3430606722831726
Batch 26/64 loss: -0.36951714754104614
Batch 27/64 loss: -0.3649691045284271
Batch 28/64 loss: -0.3379950523376465
Batch 29/64 loss: -0.36897385120391846
Batch 30/64 loss: -0.3257659077644348
Batch 31/64 loss: -0.38670653104782104
Batch 32/64 loss: -0.3976435959339142
Batch 33/64 loss: -0.37256985902786255
Batch 34/64 loss: -0.38351309299468994
Batch 35/64 loss: -0.37225255370140076
Batch 36/64 loss: -0.34175580739974976
Batch 37/64 loss: -0.3517324924468994
Batch 38/64 loss: -0.38104721903800964
Batch 39/64 loss: -0.38244563341140747
Batch 40/64 loss: -0.3982996344566345
Batch 41/64 loss: -0.36816084384918213
Batch 42/64 loss: -0.37755322456359863
Batch 43/64 loss: -0.38626188039779663
Batch 44/64 loss: -0.3780444264411926
Batch 45/64 loss: -0.3795420527458191
Batch 46/64 loss: -0.361555814743042
Batch 47/64 loss: -0.31399229168891907
Batch 48/64 loss: -0.39526066184043884
Batch 49/64 loss: -0.38811516761779785
Batch 50/64 loss: -0.3981340825557709
Batch 51/64 loss: -0.3645089268684387
Batch 52/64 loss: -0.34556472301483154
Batch 53/64 loss: -0.3787427544593811
Batch 54/64 loss: -0.3638175427913666
Batch 55/64 loss: -0.3277670741081238
Batch 56/64 loss: -0.3952634930610657
Batch 57/64 loss: -0.38634344935417175
Batch 58/64 loss: -0.3733181655406952
Batch 59/64 loss: -0.3806486129760742
Batch 60/64 loss: -0.3823665976524353
Batch 61/64 loss: -0.37898844480514526
Batch 62/64 loss: -0.39840227365493774
Batch 63/64 loss: -0.3716740608215332
Batch 64/64 loss: -0.31773534417152405
Epoch 477  Train loss: -0.3676971214659074  Val loss: 0.03631151234571057
Epoch 478
-------------------------------
Batch 1/64 loss: -0.3473113477230072
Batch 2/64 loss: -0.3912738263607025
Batch 3/64 loss: -0.3726116418838501
Batch 4/64 loss: -0.37380126118659973
Batch 5/64 loss: -0.36546042561531067
Batch 6/64 loss: -0.3958284854888916
Batch 7/64 loss: -0.3754439353942871
Batch 8/64 loss: -0.377616286277771
Batch 9/64 loss: -0.40422722697257996
Batch 10/64 loss: -0.38676124811172485
Batch 11/64 loss: -0.3542298972606659
Batch 12/64 loss: -0.3801489770412445
Batch 13/64 loss: -0.3713076412677765
Batch 14/64 loss: -0.3777359127998352
Batch 15/64 loss: -0.41177743673324585
Batch 16/64 loss: -0.3529525399208069
Batch 17/64 loss: -0.3885200619697571
Batch 18/64 loss: -0.36738842725753784
Batch 19/64 loss: -0.34908533096313477
Batch 20/64 loss: -0.3527814745903015
Batch 21/64 loss: -0.3553730845451355
Batch 22/64 loss: -0.3813473582267761
Batch 23/64 loss: -0.40120500326156616
Batch 24/64 loss: -0.37322723865509033
Batch 25/64 loss: -0.36531704664230347
Batch 26/64 loss: -0.3680066466331482
Batch 27/64 loss: -0.36540457606315613
Batch 28/64 loss: -0.3732994794845581
Batch 29/64 loss: -0.36176586151123047
Batch 30/64 loss: -0.3500826954841614
Batch 31/64 loss: -0.31323128938674927
Batch 32/64 loss: -0.3530976176261902
Batch 33/64 loss: -0.3146192729473114
Batch 34/64 loss: -0.3846948742866516
Batch 35/64 loss: -0.36259371042251587
Batch 36/64 loss: -0.3705053925514221
Batch 37/64 loss: -0.379784494638443
Batch 38/64 loss: -0.33104074001312256
Batch 39/64 loss: -0.35948872566223145
Batch 40/64 loss: -0.3987230956554413
Batch 41/64 loss: -0.3633905053138733
Batch 42/64 loss: -0.3438044786453247
Batch 43/64 loss: -0.3850918114185333
Batch 44/64 loss: -0.3382273316383362
Batch 45/64 loss: -0.35166385769844055
Batch 46/64 loss: -0.371283620595932
Batch 47/64 loss: -0.3458089232444763
Batch 48/64 loss: -0.3870919942855835
Batch 49/64 loss: -0.3661350607872009
Batch 50/64 loss: -0.3721637725830078
Batch 51/64 loss: -0.36705827713012695
Batch 52/64 loss: -0.3750482201576233
Batch 53/64 loss: -0.3289332389831543
Batch 54/64 loss: -0.3944612741470337
Batch 55/64 loss: -0.3492366075515747
Batch 56/64 loss: -0.35299238562583923
Batch 57/64 loss: -0.3976428210735321
Batch 58/64 loss: -0.329194575548172
Batch 59/64 loss: -0.37932294607162476
Batch 60/64 loss: -0.37953829765319824
Batch 61/64 loss: -0.3815979063510895
Batch 62/64 loss: -0.393057644367218
Batch 63/64 loss: -0.39731937646865845
Batch 64/64 loss: -0.3857937455177307
Epoch 478  Train loss: -0.3686036364704955  Val loss: 0.03609369239446633
Epoch 479
-------------------------------
Batch 1/64 loss: -0.38456833362579346
Batch 2/64 loss: -0.3835363984107971
Batch 3/64 loss: -0.3550102114677429
Batch 4/64 loss: -0.3859761357307434
Batch 5/64 loss: -0.36264562606811523
Batch 6/64 loss: -0.40491926670074463
Batch 7/64 loss: -0.36542683839797974
Batch 8/64 loss: -0.3989101052284241
Batch 9/64 loss: -0.39039143919944763
Batch 10/64 loss: -0.39084988832473755
Batch 11/64 loss: -0.41111016273498535
Batch 12/64 loss: -0.37759149074554443
Batch 13/64 loss: -0.3931717872619629
Batch 14/64 loss: -0.36133304238319397
Batch 15/64 loss: -0.38300955295562744
Batch 16/64 loss: -0.38191330432891846
Batch 17/64 loss: -0.42235493659973145
Batch 18/64 loss: -0.3452698886394501
Batch 19/64 loss: -0.37232473492622375
Batch 20/64 loss: -0.3955816626548767
Batch 21/64 loss: -0.39597633481025696
Batch 22/64 loss: -0.36310142278671265
Batch 23/64 loss: -0.3935999572277069
Batch 24/64 loss: -0.3812219500541687
Batch 25/64 loss: -0.39062994718551636
Batch 26/64 loss: -0.39457255601882935
Batch 27/64 loss: -0.37174245715141296
Batch 28/64 loss: -0.30124181509017944
Batch 29/64 loss: -0.3824039101600647
Batch 30/64 loss: -0.36752641201019287
Batch 31/64 loss: -0.3664299249649048
Batch 32/64 loss: -0.37446948885917664
Batch 33/64 loss: -0.3466572165489197
Batch 34/64 loss: -0.35539090633392334
Batch 35/64 loss: -0.34491854906082153
Batch 36/64 loss: -0.38470232486724854
Batch 37/64 loss: -0.3748222589492798
Batch 38/64 loss: -0.3740918040275574
Batch 39/64 loss: -0.3483027517795563
Batch 40/64 loss: -0.37619030475616455
Batch 41/64 loss: -0.36717408895492554
Batch 42/64 loss: -0.3962421417236328
Batch 43/64 loss: -0.38470029830932617
Batch 44/64 loss: -0.3783513605594635
Batch 45/64 loss: -0.3573547601699829
Batch 46/64 loss: -0.3724789619445801
Batch 47/64 loss: -0.3659392297267914
Batch 48/64 loss: -0.3451145887374878
Batch 49/64 loss: -0.35117533802986145
Batch 50/64 loss: -0.3596630394458771
Batch 51/64 loss: -0.33585259318351746
Batch 52/64 loss: -0.3693028688430786
Batch 53/64 loss: -0.3616383671760559
Batch 54/64 loss: -0.3460434675216675
Batch 55/64 loss: -0.34134960174560547
Batch 56/64 loss: -0.382880836725235
Batch 57/64 loss: -0.3637828826904297
Batch 58/64 loss: -0.3384374976158142
Batch 59/64 loss: -0.3832705318927765
Batch 60/64 loss: -0.30094677209854126
Batch 61/64 loss: -0.28573334217071533
Batch 62/64 loss: -0.3881365656852722
Batch 63/64 loss: -0.36137306690216064
Batch 64/64 loss: -0.35764169692993164
Epoch 479  Train loss: -0.36955389135024125  Val loss: 0.03736234448619725
Epoch 480
-------------------------------
Batch 1/64 loss: -0.36102843284606934
Batch 2/64 loss: -0.3633711338043213
Batch 3/64 loss: -0.30998754501342773
Batch 4/64 loss: -0.37476277351379395
Batch 5/64 loss: -0.35007935762405396
Batch 6/64 loss: -0.3865818679332733
Batch 7/64 loss: -0.3437381982803345
Batch 8/64 loss: -0.36933818459510803
Batch 9/64 loss: -0.39035218954086304
Batch 10/64 loss: -0.38668951392173767
Batch 11/64 loss: -0.3878628611564636
Batch 12/64 loss: -0.3096488416194916
Batch 13/64 loss: -0.3503496050834656
Batch 14/64 loss: -0.38543763756752014
Batch 15/64 loss: -0.3729651868343353
Batch 16/64 loss: -0.3273826241493225
Batch 17/64 loss: -0.3665879964828491
Batch 18/64 loss: -0.3859391212463379
Batch 19/64 loss: -0.38389354944229126
Batch 20/64 loss: -0.38111400604248047
Batch 21/64 loss: -0.38114506006240845
Batch 22/64 loss: -0.36684003472328186
Batch 23/64 loss: -0.38654446601867676
Batch 24/64 loss: -0.3968087136745453
Batch 25/64 loss: -0.3780050575733185
Batch 26/64 loss: -0.36112815141677856
Batch 27/64 loss: -0.3678596019744873
Batch 28/64 loss: -0.3812630772590637
Batch 29/64 loss: -0.3763411343097687
Batch 30/64 loss: -0.39808642864227295
Batch 31/64 loss: -0.39311081171035767
Batch 32/64 loss: -0.35027411580085754
Batch 33/64 loss: -0.3872690200805664
Batch 34/64 loss: -0.34510716795921326
Batch 35/64 loss: -0.371788889169693
Batch 36/64 loss: -0.37202906608581543
Batch 37/64 loss: -0.3926270008087158
Batch 38/64 loss: -0.3703526556491852
Batch 39/64 loss: -0.3564383089542389
Batch 40/64 loss: -0.3505944311618805
Batch 41/64 loss: -0.36761078238487244
Batch 42/64 loss: -0.318148672580719
Batch 43/64 loss: -0.3636634349822998
Batch 44/64 loss: -0.3348347544670105
Batch 45/64 loss: -0.3786115348339081
Batch 46/64 loss: -0.3927195072174072
Batch 47/64 loss: -0.38681769371032715
Batch 48/64 loss: -0.3425390124320984
Batch 49/64 loss: -0.35663366317749023
Batch 50/64 loss: -0.398715615272522
Batch 51/64 loss: -0.35726457834243774
Batch 52/64 loss: -0.36229631304740906
Batch 53/64 loss: -0.33237749338150024
Batch 54/64 loss: -0.38906604051589966
Batch 55/64 loss: -0.3524688482284546
Batch 56/64 loss: -0.3805999755859375
Batch 57/64 loss: -0.3537702262401581
Batch 58/64 loss: -0.3933056592941284
Batch 59/64 loss: -0.3545922636985779
Batch 60/64 loss: -0.3855511546134949
Batch 61/64 loss: -0.3930712640285492
Batch 62/64 loss: -0.3472713828086853
Batch 63/64 loss: -0.33552074432373047
Batch 64/64 loss: -0.38898205757141113
Epoch 480  Train loss: -0.36768487808751127  Val loss: 0.0392638414176469
Epoch 481
-------------------------------
Batch 1/64 loss: -0.3800243139266968
Batch 2/64 loss: -0.402390718460083
Batch 3/64 loss: -0.36940860748291016
Batch 4/64 loss: -0.39789241552352905
Batch 5/64 loss: -0.3857056796550751
Batch 6/64 loss: -0.40156877040863037
Batch 7/64 loss: -0.3733940124511719
Batch 8/64 loss: -0.348646879196167
Batch 9/64 loss: -0.33213576674461365
Batch 10/64 loss: -0.3643585443496704
Batch 11/64 loss: -0.36104637384414673
Batch 12/64 loss: -0.3443736732006073
Batch 13/64 loss: -0.38428670167922974
Batch 14/64 loss: -0.3529290556907654
Batch 15/64 loss: -0.3695201873779297
Batch 16/64 loss: -0.3575489819049835
Batch 17/64 loss: -0.34692180156707764
Batch 18/64 loss: -0.35494399070739746
Batch 19/64 loss: -0.36288803815841675
Batch 20/64 loss: -0.312788188457489
Batch 21/64 loss: -0.336675763130188
Batch 22/64 loss: -0.33059075474739075
Batch 23/64 loss: -0.39344748854637146
Batch 24/64 loss: -0.3697398900985718
Batch 25/64 loss: -0.3722398579120636
Batch 26/64 loss: -0.38758793473243713
Batch 27/64 loss: -0.3782292306423187
Batch 28/64 loss: -0.41411393880844116
Batch 29/64 loss: -0.39946967363357544
Batch 30/64 loss: -0.3972043991088867
Batch 31/64 loss: -0.37958991527557373
Batch 32/64 loss: -0.36632633209228516
Batch 33/64 loss: -0.3824943006038666
Batch 34/64 loss: -0.3729588985443115
Batch 35/64 loss: -0.3877602815628052
Batch 36/64 loss: -0.3940943479537964
Batch 37/64 loss: -0.3594468832015991
Batch 38/64 loss: -0.37191036343574524
Batch 39/64 loss: -0.3739618957042694
Batch 40/64 loss: -0.3859195411205292
Batch 41/64 loss: -0.3939703106880188
Batch 42/64 loss: -0.35739874839782715
Batch 43/64 loss: -0.3818647563457489
Batch 44/64 loss: -0.40737640857696533
Batch 45/64 loss: -0.3472233712673187
Batch 46/64 loss: -0.3590080142021179
Batch 47/64 loss: -0.4044438600540161
Batch 48/64 loss: -0.37269723415374756
Batch 49/64 loss: -0.38728341460227966
Batch 50/64 loss: -0.39573222398757935
Batch 51/64 loss: -0.33543235063552856
Batch 52/64 loss: -0.34157758951187134
Batch 53/64 loss: -0.36641156673431396
Batch 54/64 loss: -0.3489590883255005
Batch 55/64 loss: -0.351765513420105
Batch 56/64 loss: -0.3988785743713379
Batch 57/64 loss: -0.3598700165748596
Batch 58/64 loss: -0.39804863929748535
Batch 59/64 loss: -0.4013029634952545
Batch 60/64 loss: -0.3692697286605835
Batch 61/64 loss: -0.38603729009628296
Batch 62/64 loss: -0.40075579285621643
Batch 63/64 loss: -0.3906156122684479
Batch 64/64 loss: -0.3598114550113678
Epoch 481  Train loss: -0.37305593806154586  Val loss: 0.03628198899763966
Epoch 482
-------------------------------
Batch 1/64 loss: -0.3783071041107178
Batch 2/64 loss: -0.3847593367099762
Batch 3/64 loss: -0.38943013548851013
Batch 4/64 loss: -0.39512544870376587
Batch 5/64 loss: -0.388399213552475
Batch 6/64 loss: -0.3566931486129761
Batch 7/64 loss: -0.3590194582939148
Batch 8/64 loss: -0.3698722720146179
Batch 9/64 loss: -0.33615589141845703
Batch 10/64 loss: -0.38717398047447205
Batch 11/64 loss: -0.3720018267631531
Batch 12/64 loss: -0.3293244242668152
Batch 13/64 loss: -0.3736361861228943
Batch 14/64 loss: -0.40940946340560913
Batch 15/64 loss: -0.3956426978111267
Batch 16/64 loss: -0.36884528398513794
Batch 17/64 loss: -0.38128191232681274
Batch 18/64 loss: -0.3348914384841919
Batch 19/64 loss: -0.4041978120803833
Batch 20/64 loss: -0.4009406864643097
Batch 21/64 loss: -0.3769627511501312
Batch 22/64 loss: -0.4033339321613312
Batch 23/64 loss: -0.37981343269348145
Batch 24/64 loss: -0.3752095699310303
Batch 25/64 loss: -0.3730282187461853
Batch 26/64 loss: -0.39280712604522705
Batch 27/64 loss: -0.3860357999801636
Batch 28/64 loss: -0.4143557548522949
Batch 29/64 loss: -0.34722089767456055
Batch 30/64 loss: -0.37674909830093384
Batch 31/64 loss: -0.40181636810302734
Batch 32/64 loss: -0.3717021048069
Batch 33/64 loss: -0.31690162420272827
Batch 34/64 loss: -0.38355380296707153
Batch 35/64 loss: -0.3692359924316406
Batch 36/64 loss: -0.37383216619491577
Batch 37/64 loss: -0.37877941131591797
Batch 38/64 loss: -0.3732786178588867
Batch 39/64 loss: -0.3778786361217499
Batch 40/64 loss: -0.35391733050346375
Batch 41/64 loss: -0.38704901933670044
Batch 42/64 loss: -0.38153478503227234
Batch 43/64 loss: -0.38635504245758057
Batch 44/64 loss: -0.37194252014160156
Batch 45/64 loss: -0.3465164005756378
Batch 46/64 loss: -0.3926050066947937
Batch 47/64 loss: -0.3576536476612091
Batch 48/64 loss: -0.3664204478263855
Batch 49/64 loss: -0.3507731854915619
Batch 50/64 loss: -0.3104995787143707
Batch 51/64 loss: -0.342673122882843
Batch 52/64 loss: -0.36397427320480347
Batch 53/64 loss: -0.36547744274139404
Batch 54/64 loss: -0.3761144280433655
Batch 55/64 loss: -0.3507755398750305
Batch 56/64 loss: -0.39324578642845154
Batch 57/64 loss: -0.3755212426185608
Batch 58/64 loss: -0.37691783905029297
Batch 59/64 loss: -0.3785882592201233
Batch 60/64 loss: -0.3774329721927643
Batch 61/64 loss: -0.37942540645599365
Batch 62/64 loss: -0.356184720993042
Batch 63/64 loss: -0.37249913811683655
Batch 64/64 loss: -0.3695615828037262
Epoch 482  Train loss: -0.37300190352926066  Val loss: 0.03623395441324031
Epoch 483
-------------------------------
Batch 1/64 loss: -0.36128363013267517
Batch 2/64 loss: -0.3465490937232971
Batch 3/64 loss: -0.3833952844142914
Batch 4/64 loss: -0.39785799384117126
Batch 5/64 loss: -0.36209404468536377
Batch 6/64 loss: -0.4025210738182068
Batch 7/64 loss: -0.3643330931663513
Batch 8/64 loss: -0.37684667110443115
Batch 9/64 loss: -0.37928593158721924
Batch 10/64 loss: -0.39915114641189575
Batch 11/64 loss: -0.3858409523963928
Batch 12/64 loss: -0.3487950563430786
Batch 13/64 loss: -0.3362009525299072
Batch 14/64 loss: -0.40189453959465027
Batch 15/64 loss: -0.37783271074295044
Batch 16/64 loss: -0.3852689266204834
Batch 17/64 loss: -0.3723145127296448
Batch 18/64 loss: -0.39721283316612244
Batch 19/64 loss: -0.30363890528678894
Batch 20/64 loss: -0.38243454694747925
Batch 21/64 loss: -0.36187127232551575
Batch 22/64 loss: -0.38394325971603394
Batch 23/64 loss: -0.3456351161003113
Batch 24/64 loss: -0.37347427010536194
Batch 25/64 loss: -0.3973678946495056
Batch 26/64 loss: -0.389903724193573
Batch 27/64 loss: -0.3782442808151245
Batch 28/64 loss: -0.3528200685977936
Batch 29/64 loss: -0.3932042419910431
Batch 30/64 loss: -0.3371666371822357
Batch 31/64 loss: -0.37200725078582764
Batch 32/64 loss: -0.3726956844329834
Batch 33/64 loss: -0.37948697805404663
Batch 34/64 loss: -0.3604303300380707
Batch 35/64 loss: -0.3912774324417114
Batch 36/64 loss: -0.38264596462249756
Batch 37/64 loss: -0.3646068572998047
Batch 38/64 loss: -0.39931923151016235
Batch 39/64 loss: -0.3622724413871765
Batch 40/64 loss: -0.37175559997558594
Batch 41/64 loss: -0.39343881607055664
Batch 42/64 loss: -0.3521674573421478
Batch 43/64 loss: -0.34696638584136963
Batch 44/64 loss: -0.36703020334243774
Batch 45/64 loss: -0.3539254069328308
Batch 46/64 loss: -0.3503528833389282
Batch 47/64 loss: -0.3547349274158478
Batch 48/64 loss: -0.3766171932220459
Batch 49/64 loss: -0.36359578371047974
Batch 50/64 loss: -0.35151925683021545
Batch 51/64 loss: -0.36165371537208557
Batch 52/64 loss: -0.37021616101264954
Batch 53/64 loss: -0.3913114368915558
Batch 54/64 loss: -0.30468857288360596
Batch 55/64 loss: -0.3022363781929016
Batch 56/64 loss: -0.32370853424072266
Batch 57/64 loss: -0.36543402075767517
Batch 58/64 loss: -0.36814048886299133
Batch 59/64 loss: -0.3740192651748657
Batch 60/64 loss: -0.3597266674041748
Batch 61/64 loss: -0.39017146825790405
Batch 62/64 loss: -0.38084495067596436
Batch 63/64 loss: -0.3720349669456482
Batch 64/64 loss: -0.3554891049861908
Epoch 483  Train loss: -0.3682514855674669  Val loss: 0.03806320006904733
Epoch 484
-------------------------------
Batch 1/64 loss: -0.3921118378639221
Batch 2/64 loss: -0.348715603351593
Batch 3/64 loss: -0.3738916516304016
Batch 4/64 loss: -0.3922014832496643
Batch 5/64 loss: -0.388633131980896
Batch 6/64 loss: -0.3647635579109192
Batch 7/64 loss: -0.29843446612358093
Batch 8/64 loss: -0.3348146677017212
Batch 9/64 loss: -0.39657342433929443
Batch 10/64 loss: -0.37244710326194763
Batch 11/64 loss: -0.37256067991256714
Batch 12/64 loss: -0.3945454955101013
Batch 13/64 loss: -0.37275978922843933
Batch 14/64 loss: -0.38978368043899536
Batch 15/64 loss: -0.34591609239578247
Batch 16/64 loss: -0.38739290833473206
Batch 17/64 loss: -0.3079740107059479
Batch 18/64 loss: -0.37589824199676514
Batch 19/64 loss: -0.35413098335266113
Batch 20/64 loss: -0.4010114371776581
Batch 21/64 loss: -0.3757730722427368
Batch 22/64 loss: -0.38071638345718384
Batch 23/64 loss: -0.38114744424819946
Batch 24/64 loss: -0.31826621294021606
Batch 25/64 loss: -0.37071287631988525
Batch 26/64 loss: -0.3798927664756775
Batch 27/64 loss: -0.36994898319244385
Batch 28/64 loss: -0.3678165674209595
Batch 29/64 loss: -0.3376927971839905
Batch 30/64 loss: -0.39220380783081055
Batch 31/64 loss: -0.37734901905059814
Batch 32/64 loss: -0.3765721023082733
Batch 33/64 loss: -0.3693588674068451
Batch 34/64 loss: -0.37253907322883606
Batch 35/64 loss: -0.34117650985717773
Batch 36/64 loss: -0.29974842071533203
Batch 37/64 loss: -0.35307663679122925
Batch 38/64 loss: -0.3255992829799652
Batch 39/64 loss: -0.3686956763267517
Batch 40/64 loss: -0.35926681756973267
Batch 41/64 loss: -0.3432600498199463
Batch 42/64 loss: -0.3766910433769226
Batch 43/64 loss: -0.4070075750350952
Batch 44/64 loss: -0.35569870471954346
Batch 45/64 loss: -0.3748628795146942
Batch 46/64 loss: -0.35490942001342773
Batch 47/64 loss: -0.39831602573394775
Batch 48/64 loss: -0.3644295632839203
Batch 49/64 loss: -0.3707258701324463
Batch 50/64 loss: -0.34669065475463867
Batch 51/64 loss: -0.35196900367736816
Batch 52/64 loss: -0.34872332215309143
Batch 53/64 loss: -0.3779526352882385
Batch 54/64 loss: -0.34908974170684814
Batch 55/64 loss: -0.3910779058933258
Batch 56/64 loss: -0.39695489406585693
Batch 57/64 loss: -0.35470879077911377
Batch 58/64 loss: -0.3915286064147949
Batch 59/64 loss: -0.3775317072868347
Batch 60/64 loss: -0.37823060154914856
Batch 61/64 loss: -0.3760198652744293
Batch 62/64 loss: -0.3522478938102722
Batch 63/64 loss: -0.38922786712646484
Batch 64/64 loss: -0.34085190296173096
Epoch 484  Train loss: -0.36651932725719377  Val loss: 0.035571578032372334
Epoch 485
-------------------------------
Batch 1/64 loss: -0.3758811950683594
Batch 2/64 loss: -0.38827022910118103
Batch 3/64 loss: -0.3900020718574524
Batch 4/64 loss: -0.37769702076911926
Batch 5/64 loss: -0.39800551533699036
Batch 6/64 loss: -0.3480646014213562
Batch 7/64 loss: -0.3824208974838257
Batch 8/64 loss: -0.3678789734840393
Batch 9/64 loss: -0.3744409680366516
Batch 10/64 loss: -0.3583073019981384
Batch 11/64 loss: -0.37555691599845886
Batch 12/64 loss: -0.3785941004753113
Batch 13/64 loss: -0.3337250351905823
Batch 14/64 loss: -0.34245961904525757
Batch 15/64 loss: -0.36480724811553955
Batch 16/64 loss: -0.37338021397590637
Batch 17/64 loss: -0.3981018364429474
Batch 18/64 loss: -0.3607949912548065
Batch 19/64 loss: -0.38732025027275085
Batch 20/64 loss: -0.38646456599235535
Batch 21/64 loss: -0.3761763274669647
Batch 22/64 loss: -0.3062165677547455
Batch 23/64 loss: -0.4133048355579376
Batch 24/64 loss: -0.3789433240890503
Batch 25/64 loss: -0.33940595388412476
Batch 26/64 loss: -0.3669189214706421
Batch 27/64 loss: -0.3485356867313385
Batch 28/64 loss: -0.3897940218448639
Batch 29/64 loss: -0.3712640106678009
Batch 30/64 loss: -0.36876457929611206
Batch 31/64 loss: -0.4168323576450348
Batch 32/64 loss: -0.3735596239566803
Batch 33/64 loss: -0.4003210961818695
Batch 34/64 loss: -0.39649179577827454
Batch 35/64 loss: -0.38811683654785156
Batch 36/64 loss: -0.2974405586719513
Batch 37/64 loss: -0.3895758092403412
Batch 38/64 loss: -0.33298128843307495
Batch 39/64 loss: -0.37136703729629517
Batch 40/64 loss: -0.326368510723114
Batch 41/64 loss: -0.3732651174068451
Batch 42/64 loss: -0.36953920125961304
Batch 43/64 loss: -0.3611935079097748
Batch 44/64 loss: -0.40337952971458435
Batch 45/64 loss: -0.3555033206939697
Batch 46/64 loss: -0.3832017183303833
Batch 47/64 loss: -0.3374817371368408
Batch 48/64 loss: -0.3575732707977295
Batch 49/64 loss: -0.3998515009880066
Batch 50/64 loss: -0.3656695485115051
Batch 51/64 loss: -0.36029374599456787
Batch 52/64 loss: -0.38237452507019043
Batch 53/64 loss: -0.3320831060409546
Batch 54/64 loss: -0.3764224052429199
Batch 55/64 loss: -0.38109713792800903
Batch 56/64 loss: -0.36378130316734314
Batch 57/64 loss: -0.3782660961151123
Batch 58/64 loss: -0.36909395456314087
Batch 59/64 loss: -0.3764733076095581
Batch 60/64 loss: -0.3795475661754608
Batch 61/64 loss: -0.397494912147522
Batch 62/64 loss: -0.36921557784080505
Batch 63/64 loss: -0.3691210150718689
Batch 64/64 loss: -0.3598463535308838
Epoch 485  Train loss: -0.37060957749684653  Val loss: 0.036365704642948006
Epoch 486
-------------------------------
Batch 1/64 loss: -0.3538082540035248
Batch 2/64 loss: -0.35716313123703003
Batch 3/64 loss: -0.3614870309829712
Batch 4/64 loss: -0.3685108423233032
Batch 5/64 loss: -0.3856361210346222
Batch 6/64 loss: -0.37023553252220154
Batch 7/64 loss: -0.3766505718231201
Batch 8/64 loss: -0.38334596157073975
Batch 9/64 loss: -0.3751291036605835
Batch 10/64 loss: -0.33718550205230713
Batch 11/64 loss: -0.40247076749801636
Batch 12/64 loss: -0.3734903335571289
Batch 13/64 loss: -0.3556251525878906
Batch 14/64 loss: -0.38103270530700684
Batch 15/64 loss: -0.354230135679245
Batch 16/64 loss: -0.34426942467689514
Batch 17/64 loss: -0.3319928050041199
Batch 18/64 loss: -0.3843533992767334
Batch 19/64 loss: -0.37708568572998047
Batch 20/64 loss: -0.38998425006866455
Batch 21/64 loss: -0.3753051161766052
Batch 22/64 loss: -0.37125080823898315
Batch 23/64 loss: -0.3742482364177704
Batch 24/64 loss: -0.3587665557861328
Batch 25/64 loss: -0.3828314542770386
Batch 26/64 loss: -0.3849421441555023
Batch 27/64 loss: -0.3918982744216919
Batch 28/64 loss: -0.3202260136604309
Batch 29/64 loss: -0.40233564376831055
Batch 30/64 loss: -0.3993936777114868
Batch 31/64 loss: -0.37332579493522644
Batch 32/64 loss: -0.3602900505065918
Batch 33/64 loss: -0.4082009494304657
Batch 34/64 loss: -0.36500439047813416
Batch 35/64 loss: -0.4002948999404907
Batch 36/64 loss: -0.38155341148376465
Batch 37/64 loss: -0.4035646319389343
Batch 38/64 loss: -0.31352508068084717
Batch 39/64 loss: -0.37076425552368164
Batch 40/64 loss: -0.3713090419769287
Batch 41/64 loss: -0.33859720826148987
Batch 42/64 loss: -0.3943977952003479
Batch 43/64 loss: -0.34246233105659485
Batch 44/64 loss: -0.39286500215530396
Batch 45/64 loss: -0.3791716396808624
Batch 46/64 loss: -0.36533984541893005
Batch 47/64 loss: -0.35484957695007324
Batch 48/64 loss: -0.4063391089439392
Batch 49/64 loss: -0.35251569747924805
Batch 50/64 loss: -0.38542523980140686
Batch 51/64 loss: -0.3802028298377991
Batch 52/64 loss: -0.3497065305709839
Batch 53/64 loss: -0.390534371137619
Batch 54/64 loss: -0.39224138855934143
Batch 55/64 loss: -0.38561996817588806
Batch 56/64 loss: -0.35919761657714844
Batch 57/64 loss: -0.37367427349090576
Batch 58/64 loss: -0.3833557367324829
Batch 59/64 loss: -0.3786872625350952
Batch 60/64 loss: -0.4010465145111084
Batch 61/64 loss: -0.38413459062576294
Batch 62/64 loss: -0.38808268308639526
Batch 63/64 loss: -0.3715057969093323
Batch 64/64 loss: -0.39582157135009766
Epoch 486  Train loss: -0.3736397855422076  Val loss: 0.03625956556641359
Epoch 487
-------------------------------
Batch 1/64 loss: -0.3408629596233368
Batch 2/64 loss: -0.38622885942459106
Batch 3/64 loss: -0.36310726404190063
Batch 4/64 loss: -0.3867837190628052
Batch 5/64 loss: -0.3993634283542633
Batch 6/64 loss: -0.41293901205062866
Batch 7/64 loss: -0.39311882853507996
Batch 8/64 loss: -0.36688899993896484
Batch 9/64 loss: -0.42353126406669617
Batch 10/64 loss: -0.41031044721603394
Batch 11/64 loss: -0.39893898367881775
Batch 12/64 loss: -0.32563647627830505
Batch 13/64 loss: -0.370810866355896
Batch 14/64 loss: -0.40224549174308777
Batch 15/64 loss: -0.3924386501312256
Batch 16/64 loss: -0.3335072994232178
Batch 17/64 loss: -0.3253856897354126
Batch 18/64 loss: -0.3208748698234558
Batch 19/64 loss: -0.3838689625263214
Batch 20/64 loss: -0.4006849229335785
Batch 21/64 loss: -0.3665241003036499
Batch 22/64 loss: -0.39778411388397217
Batch 23/64 loss: -0.37330588698387146
Batch 24/64 loss: -0.3805130422115326
Batch 25/64 loss: -0.38844361901283264
Batch 26/64 loss: -0.39077097177505493
Batch 27/64 loss: -0.37741971015930176
Batch 28/64 loss: -0.3728252649307251
Batch 29/64 loss: -0.3891109228134155
Batch 30/64 loss: -0.3630821108818054
Batch 31/64 loss: -0.3843303322792053
Batch 32/64 loss: -0.33018720149993896
Batch 33/64 loss: -0.39885276556015015
Batch 34/64 loss: -0.3549680709838867
Batch 35/64 loss: -0.37725287675857544
Batch 36/64 loss: -0.36237549781799316
Batch 37/64 loss: -0.37018388509750366
Batch 38/64 loss: -0.33667296171188354
Batch 39/64 loss: -0.3785932958126068
Batch 40/64 loss: -0.3723931312561035
Batch 41/64 loss: -0.37112748622894287
Batch 42/64 loss: -0.35020843148231506
Batch 43/64 loss: -0.32055020332336426
Batch 44/64 loss: -0.3870547413825989
Batch 45/64 loss: -0.34101930260658264
Batch 46/64 loss: -0.3818460702896118
Batch 47/64 loss: -0.3854385018348694
Batch 48/64 loss: -0.37036383152008057
Batch 49/64 loss: -0.37932729721069336
Batch 50/64 loss: -0.37103715538978577
Batch 51/64 loss: -0.2936287522315979
Batch 52/64 loss: -0.37522515654563904
Batch 53/64 loss: -0.39390748739242554
Batch 54/64 loss: -0.3940698802471161
Batch 55/64 loss: -0.38496270775794983
Batch 56/64 loss: -0.365867555141449
Batch 57/64 loss: -0.3232000470161438
Batch 58/64 loss: -0.3656468689441681
Batch 59/64 loss: -0.3842867314815521
Batch 60/64 loss: -0.38706421852111816
Batch 61/64 loss: -0.3808485269546509
Batch 62/64 loss: -0.3542296290397644
Batch 63/64 loss: -0.38725990056991577
Batch 64/64 loss: -0.3651360869407654
Epoch 487  Train loss: -0.3721590479214986  Val loss: 0.035344569134138706
Epoch 488
-------------------------------
Batch 1/64 loss: -0.3552372455596924
Batch 2/64 loss: -0.3904563784599304
Batch 3/64 loss: -0.35651206970214844
Batch 4/64 loss: -0.37360602617263794
Batch 5/64 loss: -0.3027278780937195
Batch 6/64 loss: -0.39654695987701416
Batch 7/64 loss: -0.39424002170562744
Batch 8/64 loss: -0.3852432668209076
Batch 9/64 loss: -0.3688617944717407
Batch 10/64 loss: -0.3913518190383911
Batch 11/64 loss: -0.3604891300201416
Batch 12/64 loss: -0.3706187307834625
Batch 13/64 loss: -0.35023602843284607
Batch 14/64 loss: -0.4068775177001953
Batch 15/64 loss: -0.39057260751724243
Batch 16/64 loss: -0.34494543075561523
Batch 17/64 loss: -0.4074980616569519
Batch 18/64 loss: -0.37207892537117004
Batch 19/64 loss: -0.35173097252845764
Batch 20/64 loss: -0.37925153970718384
Batch 21/64 loss: -0.3652045726776123
Batch 22/64 loss: -0.38689833879470825
Batch 23/64 loss: -0.40630805492401123
Batch 24/64 loss: -0.3569502830505371
Batch 25/64 loss: -0.4005640745162964
Batch 26/64 loss: -0.3503403663635254
Batch 27/64 loss: -0.38882797956466675
Batch 28/64 loss: -0.3270268142223358
Batch 29/64 loss: -0.38281673192977905
Batch 30/64 loss: -0.34988856315612793
Batch 31/64 loss: -0.3874225616455078
Batch 32/64 loss: -0.36505475640296936
Batch 33/64 loss: -0.34912988543510437
Batch 34/64 loss: -0.3926624655723572
Batch 35/64 loss: -0.35497015714645386
Batch 36/64 loss: -0.363644003868103
Batch 37/64 loss: -0.3846870958805084
Batch 38/64 loss: -0.40177348256111145
Batch 39/64 loss: -0.4110223054885864
Batch 40/64 loss: -0.3782910108566284
Batch 41/64 loss: -0.3820173442363739
Batch 42/64 loss: -0.37567323446273804
Batch 43/64 loss: -0.3944500982761383
Batch 44/64 loss: -0.3710997998714447
Batch 45/64 loss: -0.3254241347312927
Batch 46/64 loss: -0.34832555055618286
Batch 47/64 loss: -0.36436721682548523
Batch 48/64 loss: -0.3171905279159546
Batch 49/64 loss: -0.39309456944465637
Batch 50/64 loss: -0.36165541410446167
Batch 51/64 loss: -0.3792378008365631
Batch 52/64 loss: -0.3971126079559326
Batch 53/64 loss: -0.37814998626708984
Batch 54/64 loss: -0.3939078152179718
Batch 55/64 loss: -0.3588586151599884
Batch 56/64 loss: -0.3632897734642029
Batch 57/64 loss: -0.3997654318809509
Batch 58/64 loss: -0.37698453664779663
Batch 59/64 loss: -0.3689042925834656
Batch 60/64 loss: -0.3522599935531616
Batch 61/64 loss: -0.405284583568573
Batch 62/64 loss: -0.3651830554008484
Batch 63/64 loss: -0.3870674967765808
Batch 64/64 loss: -0.34812241792678833
Epoch 488  Train loss: -0.37290920159396  Val loss: 0.036835479367639604
Epoch 489
-------------------------------
Batch 1/64 loss: -0.399225115776062
Batch 2/64 loss: -0.3534397482872009
Batch 3/64 loss: -0.3727749288082123
Batch 4/64 loss: -0.4133272171020508
Batch 5/64 loss: -0.37671178579330444
Batch 6/64 loss: -0.38000330328941345
Batch 7/64 loss: -0.3921971023082733
Batch 8/64 loss: -0.3677005469799042
Batch 9/64 loss: -0.39115869998931885
Batch 10/64 loss: -0.39768725633621216
Batch 11/64 loss: -0.3949683904647827
Batch 12/64 loss: -0.3763536810874939
Batch 13/64 loss: -0.40534961223602295
Batch 14/64 loss: -0.40411877632141113
Batch 15/64 loss: -0.3341101408004761
Batch 16/64 loss: -0.37370216846466064
Batch 17/64 loss: -0.40157341957092285
Batch 18/64 loss: -0.4102413058280945
Batch 19/64 loss: -0.3408478796482086
Batch 20/64 loss: -0.4172300696372986
Batch 21/64 loss: -0.36684393882751465
Batch 22/64 loss: -0.3926425278186798
Batch 23/64 loss: -0.33358144760131836
Batch 24/64 loss: -0.3499014377593994
Batch 25/64 loss: -0.3909258246421814
Batch 26/64 loss: -0.38551318645477295
Batch 27/64 loss: -0.31304919719696045
Batch 28/64 loss: -0.3809661269187927
Batch 29/64 loss: -0.37723681330680847
Batch 30/64 loss: -0.3892098069190979
Batch 31/64 loss: -0.3340238630771637
Batch 32/64 loss: -0.4036177396774292
Batch 33/64 loss: -0.4047926664352417
Batch 34/64 loss: -0.386566698551178
Batch 35/64 loss: -0.39702609181404114
Batch 36/64 loss: -0.3978196680545807
Batch 37/64 loss: -0.37929385900497437
Batch 38/64 loss: -0.3523061275482178
Batch 39/64 loss: -0.3680996000766754
Batch 40/64 loss: -0.33052778244018555
Batch 41/64 loss: -0.35239332914352417
Batch 42/64 loss: -0.3171879053115845
Batch 43/64 loss: -0.3319178819656372
Batch 44/64 loss: -0.34347766637802124
Batch 45/64 loss: -0.3621794879436493
Batch 46/64 loss: -0.3743242621421814
Batch 47/64 loss: -0.4075119197368622
Batch 48/64 loss: -0.39329013228416443
Batch 49/64 loss: -0.3276212811470032
Batch 50/64 loss: -0.33735495805740356
Batch 51/64 loss: -0.3622325658798218
Batch 52/64 loss: -0.3643617033958435
Batch 53/64 loss: -0.3980153203010559
Batch 54/64 loss: -0.3570488691329956
Batch 55/64 loss: -0.377408504486084
Batch 56/64 loss: -0.39980578422546387
Batch 57/64 loss: -0.3777911961078644
Batch 58/64 loss: -0.3751719295978546
Batch 59/64 loss: -0.3764932453632355
Batch 60/64 loss: -0.3976289629936218
Batch 61/64 loss: -0.3997306823730469
Batch 62/64 loss: -0.3555068373680115
Batch 63/64 loss: -0.3760988116264343
Batch 64/64 loss: -0.35764196515083313
Epoch 489  Train loss: -0.37442274923418084  Val loss: 0.03502778908641068
Epoch 490
-------------------------------
Batch 1/64 loss: -0.39802905917167664
Batch 2/64 loss: -0.3348765969276428
Batch 3/64 loss: -0.40486907958984375
Batch 4/64 loss: -0.39136457443237305
Batch 5/64 loss: -0.3312344551086426
Batch 6/64 loss: -0.3589874505996704
Batch 7/64 loss: -0.35542380809783936
Batch 8/64 loss: -0.37866657972335815
Batch 9/64 loss: -0.3520556688308716
Batch 10/64 loss: -0.3739374876022339
Batch 11/64 loss: -0.3455268144607544
Batch 12/64 loss: -0.30970537662506104
Batch 13/64 loss: -0.37478721141815186
Batch 14/64 loss: -0.38456687331199646
Batch 15/64 loss: -0.38972634077072144
Batch 16/64 loss: -0.39127326011657715
Batch 17/64 loss: -0.35246121883392334
Batch 18/64 loss: -0.3974030017852783
Batch 19/64 loss: -0.3896508514881134
Batch 20/64 loss: -0.3588394522666931
Batch 21/64 loss: -0.37228429317474365
Batch 22/64 loss: -0.3927670419216156
Batch 23/64 loss: -0.3929276466369629
Batch 24/64 loss: -0.4106379449367523
Batch 25/64 loss: -0.4064120948314667
Batch 26/64 loss: -0.38837724924087524
Batch 27/64 loss: -0.3566476106643677
Batch 28/64 loss: -0.3669150471687317
Batch 29/64 loss: -0.3739125728607178
Batch 30/64 loss: -0.3187546133995056
Batch 31/64 loss: -0.3779652714729309
Batch 32/64 loss: -0.3519522249698639
Batch 33/64 loss: -0.3951466679573059
Batch 34/64 loss: -0.37036219239234924
Batch 35/64 loss: -0.3977290391921997
Batch 36/64 loss: -0.3508981466293335
Batch 37/64 loss: -0.36233702301979065
Batch 38/64 loss: -0.37425220012664795
Batch 39/64 loss: -0.36873167753219604
Batch 40/64 loss: -0.38081932067871094
Batch 41/64 loss: -0.4190768897533417
Batch 42/64 loss: -0.3264668583869934
Batch 43/64 loss: -0.3288905918598175
Batch 44/64 loss: -0.3820902109146118
Batch 45/64 loss: -0.3895142674446106
Batch 46/64 loss: -0.31771016120910645
Batch 47/64 loss: -0.36403918266296387
Batch 48/64 loss: -0.3939865827560425
Batch 49/64 loss: -0.39123690128326416
Batch 50/64 loss: -0.3369317054748535
Batch 51/64 loss: -0.397830605506897
Batch 52/64 loss: -0.39034003019332886
Batch 53/64 loss: -0.36841678619384766
Batch 54/64 loss: -0.3745168447494507
Batch 55/64 loss: -0.3636804223060608
Batch 56/64 loss: -0.3855022192001343
Batch 57/64 loss: -0.3728780150413513
Batch 58/64 loss: -0.36000707745552063
Batch 59/64 loss: -0.4028829336166382
Batch 60/64 loss: -0.38794654607772827
Batch 61/64 loss: -0.4082832336425781
Batch 62/64 loss: -0.40592581033706665
Batch 63/64 loss: -0.37985605001449585
Batch 64/64 loss: -0.36424192786216736
Epoch 490  Train loss: -0.37340245352071877  Val loss: 0.03764905409304956
Epoch 491
-------------------------------
Batch 1/64 loss: -0.3236940801143646
Batch 2/64 loss: -0.36227482557296753
Batch 3/64 loss: -0.34410592913627625
Batch 4/64 loss: -0.3770548701286316
Batch 5/64 loss: -0.3450571298599243
Batch 6/64 loss: -0.3861389458179474
Batch 7/64 loss: -0.38118600845336914
Batch 8/64 loss: -0.39475080370903015
Batch 9/64 loss: -0.37581437826156616
Batch 10/64 loss: -0.4078189432621002
Batch 11/64 loss: -0.3854618966579437
Batch 12/64 loss: -0.3028867840766907
Batch 13/64 loss: -0.38391757011413574
Batch 14/64 loss: -0.3631002604961395
Batch 15/64 loss: -0.3856218457221985
Batch 16/64 loss: -0.37638136744499207
Batch 17/64 loss: -0.4076307713985443
Batch 18/64 loss: -0.3451921343803406
Batch 19/64 loss: -0.3276626169681549
Batch 20/64 loss: -0.38112181425094604
Batch 21/64 loss: -0.39029160141944885
Batch 22/64 loss: -0.3428572714328766
Batch 23/64 loss: -0.3703712522983551
Batch 24/64 loss: -0.372068852186203
Batch 25/64 loss: -0.30252087116241455
Batch 26/64 loss: -0.3552485406398773
Batch 27/64 loss: -0.3524159789085388
Batch 28/64 loss: -0.3944990634918213
Batch 29/64 loss: -0.36906805634498596
Batch 30/64 loss: -0.36006125807762146
Batch 31/64 loss: -0.3844881057739258
Batch 32/64 loss: -0.3576952815055847
Batch 33/64 loss: -0.3631397485733032
Batch 34/64 loss: -0.35694777965545654
Batch 35/64 loss: -0.38712796568870544
Batch 36/64 loss: -0.36864644289016724
Batch 37/64 loss: -0.3846377730369568
Batch 38/64 loss: -0.37676307559013367
Batch 39/64 loss: -0.38972008228302
Batch 40/64 loss: -0.3835104703903198
Batch 41/64 loss: -0.38558244705200195
Batch 42/64 loss: -0.3847275972366333
Batch 43/64 loss: -0.4040091335773468
Batch 44/64 loss: -0.37479740381240845
Batch 45/64 loss: -0.38552582263946533
Batch 46/64 loss: -0.379821240901947
Batch 47/64 loss: -0.3796538710594177
Batch 48/64 loss: -0.3492966592311859
Batch 49/64 loss: -0.36392974853515625
Batch 50/64 loss: -0.38519954681396484
Batch 51/64 loss: -0.38597530126571655
Batch 52/64 loss: -0.34883105754852295
Batch 53/64 loss: -0.346618115901947
Batch 54/64 loss: -0.33910179138183594
Batch 55/64 loss: -0.3966529369354248
Batch 56/64 loss: -0.386932909488678
Batch 57/64 loss: -0.39013147354125977
Batch 58/64 loss: -0.39271417260169983
Batch 59/64 loss: -0.35200121998786926
Batch 60/64 loss: -0.3554350733757019
Batch 61/64 loss: -0.3896200954914093
Batch 62/64 loss: -0.38596826791763306
Batch 63/64 loss: -0.38537389039993286
Batch 64/64 loss: -0.3461042046546936
Epoch 491  Train loss: -0.3706106725861045  Val loss: 0.036903190039277486
Epoch 492
-------------------------------
Batch 1/64 loss: -0.37813228368759155
Batch 2/64 loss: -0.3397298753261566
Batch 3/64 loss: -0.30950453877449036
Batch 4/64 loss: -0.3496800661087036
Batch 5/64 loss: -0.3678966760635376
Batch 6/64 loss: -0.38376063108444214
Batch 7/64 loss: -0.3910192847251892
Batch 8/64 loss: -0.3787999451160431
Batch 9/64 loss: -0.3952678143978119
Batch 10/64 loss: -0.3924640417098999
Batch 11/64 loss: -0.3664221465587616
Batch 12/64 loss: -0.386380672454834
Batch 13/64 loss: -0.36395955085754395
Batch 14/64 loss: -0.35945555567741394
Batch 15/64 loss: -0.3932960033416748
Batch 16/64 loss: -0.40835365653038025
Batch 17/64 loss: -0.38171231746673584
Batch 18/64 loss: -0.40292155742645264
Batch 19/64 loss: -0.39266738295555115
Batch 20/64 loss: -0.3924688696861267
Batch 21/64 loss: -0.3713659346103668
Batch 22/64 loss: -0.3977210223674774
Batch 23/64 loss: -0.36198335886001587
Batch 24/64 loss: -0.3543953597545624
Batch 25/64 loss: -0.3653908371925354
Batch 26/64 loss: -0.3900354206562042
Batch 27/64 loss: -0.33430907130241394
Batch 28/64 loss: -0.37523525953292847
Batch 29/64 loss: -0.3303840756416321
Batch 30/64 loss: -0.37611472606658936
Batch 31/64 loss: -0.35444825887680054
Batch 32/64 loss: -0.37047410011291504
Batch 33/64 loss: -0.3887929916381836
Batch 34/64 loss: -0.37901949882507324
Batch 35/64 loss: -0.34206151962280273
Batch 36/64 loss: -0.3816445469856262
Batch 37/64 loss: -0.39028504490852356
Batch 38/64 loss: -0.4033219814300537
Batch 39/64 loss: -0.410835862159729
Batch 40/64 loss: -0.402474045753479
Batch 41/64 loss: -0.4087734818458557
Batch 42/64 loss: -0.3521856665611267
Batch 43/64 loss: -0.3591740131378174
Batch 44/64 loss: -0.38054949045181274
Batch 45/64 loss: -0.39467066526412964
Batch 46/64 loss: -0.37376055121421814
Batch 47/64 loss: -0.3484790027141571
Batch 48/64 loss: -0.34743642807006836
Batch 49/64 loss: -0.3762405514717102
Batch 50/64 loss: -0.3794178366661072
Batch 51/64 loss: -0.3792349696159363
Batch 52/64 loss: -0.3556387424468994
Batch 53/64 loss: -0.35855093598365784
Batch 54/64 loss: -0.38731974363327026
Batch 55/64 loss: -0.3674720525741577
Batch 56/64 loss: -0.3983786106109619
Batch 57/64 loss: -0.4078676104545593
Batch 58/64 loss: -0.39101478457450867
Batch 59/64 loss: -0.3650837540626526
Batch 60/64 loss: -0.3208005726337433
Batch 61/64 loss: -0.3350905776023865
Batch 62/64 loss: -0.34857451915740967
Batch 63/64 loss: -0.3572177290916443
Batch 64/64 loss: -0.26726460456848145
Epoch 492  Train loss: -0.3718833965413711  Val loss: 0.03642443180903537
Epoch 493
-------------------------------
Batch 1/64 loss: -0.34537941217422485
Batch 2/64 loss: -0.34357213973999023
Batch 3/64 loss: -0.3737950623035431
Batch 4/64 loss: -0.3864886164665222
Batch 5/64 loss: -0.3872501850128174
Batch 6/64 loss: -0.3655695915222168
Batch 7/64 loss: -0.3891602158546448
Batch 8/64 loss: -0.3767557442188263
Batch 9/64 loss: -0.39380043745040894
Batch 10/64 loss: -0.399973601102829
Batch 11/64 loss: -0.37518683075904846
Batch 12/64 loss: -0.38600093126296997
Batch 13/64 loss: -0.40709009766578674
Batch 14/64 loss: -0.38245272636413574
Batch 15/64 loss: -0.3500058352947235
Batch 16/64 loss: -0.36105602979660034
Batch 17/64 loss: -0.3856924772262573
Batch 18/64 loss: -0.3690347671508789
Batch 19/64 loss: -0.3944210708141327
Batch 20/64 loss: -0.34835171699523926
Batch 21/64 loss: -0.41087520122528076
Batch 22/64 loss: -0.3798792362213135
Batch 23/64 loss: -0.3634326457977295
Batch 24/64 loss: -0.38278672099113464
Batch 25/64 loss: -0.35494762659072876
Batch 26/64 loss: -0.38016510009765625
Batch 27/64 loss: -0.3605825901031494
Batch 28/64 loss: -0.33552420139312744
Batch 29/64 loss: -0.3660493493080139
Batch 30/64 loss: -0.3454768657684326
Batch 31/64 loss: -0.4103839695453644
Batch 32/64 loss: -0.4012327790260315
Batch 33/64 loss: -0.3166007101535797
Batch 34/64 loss: -0.3917481303215027
Batch 35/64 loss: -0.36691752076148987
Batch 36/64 loss: -0.3476625084877014
Batch 37/64 loss: -0.3975842595100403
Batch 38/64 loss: -0.29815828800201416
Batch 39/64 loss: -0.3808174133300781
Batch 40/64 loss: -0.3899122178554535
Batch 41/64 loss: -0.3182431161403656
Batch 42/64 loss: -0.4035153388977051
Batch 43/64 loss: -0.40361812710762024
Batch 44/64 loss: -0.3725871443748474
Batch 45/64 loss: -0.3990042209625244
Batch 46/64 loss: -0.3767220079898834
Batch 47/64 loss: -0.3977644443511963
Batch 48/64 loss: -0.38494861125946045
Batch 49/64 loss: -0.3812069296836853
Batch 50/64 loss: -0.38300323486328125
Batch 51/64 loss: -0.3815293312072754
Batch 52/64 loss: -0.36242616176605225
Batch 53/64 loss: -0.33308202028274536
Batch 54/64 loss: -0.30135244131088257
Batch 55/64 loss: -0.384807288646698
Batch 56/64 loss: -0.39059212803840637
Batch 57/64 loss: -0.38949906826019287
Batch 58/64 loss: -0.392283171415329
Batch 59/64 loss: -0.3810025155544281
Batch 60/64 loss: -0.4064089059829712
Batch 61/64 loss: -0.38877183198928833
Batch 62/64 loss: -0.3470458984375
Batch 63/64 loss: -0.37562131881713867
Batch 64/64 loss: -0.3342783451080322
Epoch 493  Train loss: -0.37345127591899796  Val loss: 0.03672869098964835
Epoch 494
-------------------------------
Batch 1/64 loss: -0.36931082606315613
Batch 2/64 loss: -0.36199691891670227
Batch 3/64 loss: -0.4252368211746216
Batch 4/64 loss: -0.369626522064209
Batch 5/64 loss: -0.3395135998725891
Batch 6/64 loss: -0.3680976927280426
Batch 7/64 loss: -0.36392292380332947
Batch 8/64 loss: -0.4145737588405609
Batch 9/64 loss: -0.3722500205039978
Batch 10/64 loss: -0.365366131067276
Batch 11/64 loss: -0.38722747564315796
Batch 12/64 loss: -0.3404569923877716
Batch 13/64 loss: -0.3557574152946472
Batch 14/64 loss: -0.3923451900482178
Batch 15/64 loss: -0.3732820153236389
Batch 16/64 loss: -0.38880017399787903
Batch 17/64 loss: -0.39139634370803833
Batch 18/64 loss: -0.37191611528396606
Batch 19/64 loss: -0.3697536587715149
Batch 20/64 loss: -0.37861594557762146
Batch 21/64 loss: -0.3529590666294098
Batch 22/64 loss: -0.36619311571121216
Batch 23/64 loss: -0.3568456470966339
Batch 24/64 loss: -0.3885451853275299
Batch 25/64 loss: -0.40226778388023376
Batch 26/64 loss: -0.3887578845024109
Batch 27/64 loss: -0.3462831377983093
Batch 28/64 loss: -0.3617115318775177
Batch 29/64 loss: -0.39269474148750305
Batch 30/64 loss: -0.3796735405921936
Batch 31/64 loss: -0.3623965382575989
Batch 32/64 loss: -0.39029863476753235
Batch 33/64 loss: -0.38238808512687683
Batch 34/64 loss: -0.40470537543296814
Batch 35/64 loss: -0.35388121008872986
Batch 36/64 loss: -0.395036518573761
Batch 37/64 loss: -0.3932621479034424
Batch 38/64 loss: -0.3840121626853943
Batch 39/64 loss: -0.3866186738014221
Batch 40/64 loss: -0.380218505859375
Batch 41/64 loss: -0.40872982144355774
Batch 42/64 loss: -0.35007670521736145
Batch 43/64 loss: -0.3977898955345154
Batch 44/64 loss: -0.3621346056461334
Batch 45/64 loss: -0.3942858874797821
Batch 46/64 loss: -0.388449490070343
Batch 47/64 loss: -0.3793715834617615
Batch 48/64 loss: -0.37308269739151
Batch 49/64 loss: -0.33717697858810425
Batch 50/64 loss: -0.3315524458885193
Batch 51/64 loss: -0.347814679145813
Batch 52/64 loss: -0.39330509305000305
Batch 53/64 loss: -0.37988534569740295
Batch 54/64 loss: -0.39638203382492065
Batch 55/64 loss: -0.3939184844493866
Batch 56/64 loss: -0.406147837638855
Batch 57/64 loss: -0.3470858037471771
Batch 58/64 loss: -0.3385699391365051
Batch 59/64 loss: -0.3862144947052002
Batch 60/64 loss: -0.4026998281478882
Batch 61/64 loss: -0.3732224702835083
Batch 62/64 loss: -0.3969942331314087
Batch 63/64 loss: -0.37337592244148254
Batch 64/64 loss: -0.38702821731567383
Epoch 494  Train loss: -0.37673307400123746  Val loss: 0.03676336860329015
Epoch 495
-------------------------------
Batch 1/64 loss: -0.40484532713890076
Batch 2/64 loss: -0.37199118733406067
Batch 3/64 loss: -0.3942357897758484
Batch 4/64 loss: -0.3566223382949829
Batch 5/64 loss: -0.34092581272125244
Batch 6/64 loss: -0.3999401926994324
Batch 7/64 loss: -0.39576253294944763
Batch 8/64 loss: -0.2928906977176666
Batch 9/64 loss: -0.350449800491333
Batch 10/64 loss: -0.397236168384552
Batch 11/64 loss: -0.3847450017929077
Batch 12/64 loss: -0.39169639348983765
Batch 13/64 loss: -0.381777822971344
Batch 14/64 loss: -0.3695577383041382
Batch 15/64 loss: -0.386581689119339
Batch 16/64 loss: -0.3808290660381317
Batch 17/64 loss: -0.39062273502349854
Batch 18/64 loss: -0.36039575934410095
Batch 19/64 loss: -0.37277132272720337
Batch 20/64 loss: -0.42047348618507385
Batch 21/64 loss: -0.3945240378379822
Batch 22/64 loss: -0.3581240773200989
Batch 23/64 loss: -0.3823505640029907
Batch 24/64 loss: -0.35908257961273193
Batch 25/64 loss: -0.3634692430496216
Batch 26/64 loss: -0.3995467722415924
Batch 27/64 loss: -0.3826742172241211
Batch 28/64 loss: -0.3701528310775757
Batch 29/64 loss: -0.39008522033691406
Batch 30/64 loss: -0.3589714765548706
Batch 31/64 loss: -0.37870514392852783
Batch 32/64 loss: -0.37449419498443604
Batch 33/64 loss: -0.3887872099876404
Batch 34/64 loss: -0.35002976655960083
Batch 35/64 loss: -0.3887631893157959
Batch 36/64 loss: -0.38689765334129333
Batch 37/64 loss: -0.37538859248161316
Batch 38/64 loss: -0.38836246728897095
Batch 39/64 loss: -0.35861384868621826
Batch 40/64 loss: -0.4031078815460205
Batch 41/64 loss: -0.3354279398918152
Batch 42/64 loss: -0.3785752058029175
Batch 43/64 loss: -0.37640443444252014
Batch 44/64 loss: -0.37665465474128723
Batch 45/64 loss: -0.34572455286979675
Batch 46/64 loss: -0.3649519681930542
Batch 47/64 loss: -0.3607940077781677
Batch 48/64 loss: -0.3703484535217285
Batch 49/64 loss: -0.366676390171051
Batch 50/64 loss: -0.3939658999443054
Batch 51/64 loss: -0.38417530059814453
Batch 52/64 loss: -0.2594583034515381
Batch 53/64 loss: -0.3646801710128784
Batch 54/64 loss: -0.3760358691215515
Batch 55/64 loss: -0.3791555166244507
Batch 56/64 loss: -0.3898165822029114
Batch 57/64 loss: -0.39557546377182007
Batch 58/64 loss: -0.3632736802101135
Batch 59/64 loss: -0.3913300633430481
Batch 60/64 loss: -0.38318580389022827
Batch 61/64 loss: -0.3760708272457123
Batch 62/64 loss: -0.3741607069969177
Batch 63/64 loss: -0.336972177028656
Batch 64/64 loss: -0.3859381377696991
Epoch 495  Train loss: -0.373793716173546  Val loss: 0.03875824368696442
Epoch 496
-------------------------------
Batch 1/64 loss: -0.37284839153289795
Batch 2/64 loss: -0.39838168025016785
Batch 3/64 loss: -0.39135831594467163
Batch 4/64 loss: -0.3769370913505554
Batch 5/64 loss: -0.36307451128959656
Batch 6/64 loss: -0.3838464617729187
Batch 7/64 loss: -0.3910290598869324
Batch 8/64 loss: -0.3878920078277588
Batch 9/64 loss: -0.3968546986579895
Batch 10/64 loss: -0.3722347617149353
Batch 11/64 loss: -0.37210774421691895
Batch 12/64 loss: -0.38706734776496887
Batch 13/64 loss: -0.3874260485172272
Batch 14/64 loss: -0.3819596767425537
Batch 15/64 loss: -0.3605578541755676
Batch 16/64 loss: -0.39570093154907227
Batch 17/64 loss: -0.35378849506378174
Batch 18/64 loss: -0.36627310514450073
Batch 19/64 loss: -0.34210026264190674
Batch 20/64 loss: -0.39048951864242554
Batch 21/64 loss: -0.3766469955444336
Batch 22/64 loss: -0.3873961567878723
Batch 23/64 loss: -0.35963910818099976
Batch 24/64 loss: -0.3534524440765381
Batch 25/64 loss: -0.37629377841949463
Batch 26/64 loss: -0.4036388695240021
Batch 27/64 loss: -0.36857712268829346
Batch 28/64 loss: -0.3772428631782532
Batch 29/64 loss: -0.3600172698497772
Batch 30/64 loss: -0.4088662564754486
Batch 31/64 loss: -0.3805984556674957
Batch 32/64 loss: -0.3924856185913086
Batch 33/64 loss: -0.36288705468177795
Batch 34/64 loss: -0.34577465057373047
Batch 35/64 loss: -0.31472349166870117
Batch 36/64 loss: -0.39429229497909546
Batch 37/64 loss: -0.38759660720825195
Batch 38/64 loss: -0.38082897663116455
Batch 39/64 loss: -0.37171441316604614
Batch 40/64 loss: -0.36972951889038086
Batch 41/64 loss: -0.3605119585990906
Batch 42/64 loss: -0.34786176681518555
Batch 43/64 loss: -0.3709922134876251
Batch 44/64 loss: -0.344794362783432
Batch 45/64 loss: -0.370419979095459
Batch 46/64 loss: -0.35821732878685
Batch 47/64 loss: -0.35418999195098877
Batch 48/64 loss: -0.39385658502578735
Batch 49/64 loss: -0.3718763291835785
Batch 50/64 loss: -0.3674192428588867
Batch 51/64 loss: -0.39026588201522827
Batch 52/64 loss: -0.37856003642082214
Batch 53/64 loss: -0.37671154737472534
Batch 54/64 loss: -0.36470460891723633
Batch 55/64 loss: -0.36664554476737976
Batch 56/64 loss: -0.39244019985198975
Batch 57/64 loss: -0.36718830466270447
Batch 58/64 loss: -0.3871104121208191
Batch 59/64 loss: -0.39303672313690186
Batch 60/64 loss: -0.39283138513565063
Batch 61/64 loss: -0.3512253165245056
Batch 62/64 loss: -0.3389810025691986
Batch 63/64 loss: -0.35153084993362427
Batch 64/64 loss: -0.3889155387878418
Epoch 496  Train loss: -0.37376295117770925  Val loss: 0.03915697561506553
Epoch 497
-------------------------------
Batch 1/64 loss: -0.37130725383758545
Batch 2/64 loss: -0.3733189105987549
Batch 3/64 loss: -0.3987094759941101
Batch 4/64 loss: -0.40297144651412964
Batch 5/64 loss: -0.405107706785202
Batch 6/64 loss: -0.3735935688018799
Batch 7/64 loss: -0.3525363802909851
Batch 8/64 loss: -0.31293100118637085
Batch 9/64 loss: -0.40275031328201294
Batch 10/64 loss: -0.39466601610183716
Batch 11/64 loss: -0.37218305468559265
Batch 12/64 loss: -0.3558019995689392
Batch 13/64 loss: -0.3876056969165802
Batch 14/64 loss: -0.3426007926464081
Batch 15/64 loss: -0.3881925940513611
Batch 16/64 loss: -0.39193862676620483
Batch 17/64 loss: -0.3272305130958557
Batch 18/64 loss: -0.3969520628452301
Batch 19/64 loss: -0.3862782120704651
Batch 20/64 loss: -0.403908908367157
Batch 21/64 loss: -0.37986478209495544
Batch 22/64 loss: -0.3938552141189575
Batch 23/64 loss: -0.40652719140052795
Batch 24/64 loss: -0.39465099573135376
Batch 25/64 loss: -0.38124948740005493
Batch 26/64 loss: -0.38607311248779297
Batch 27/64 loss: -0.38730454444885254
Batch 28/64 loss: -0.36638373136520386
Batch 29/64 loss: -0.3943609595298767
Batch 30/64 loss: -0.3754289448261261
Batch 31/64 loss: -0.39739006757736206
Batch 32/64 loss: -0.4080643653869629
Batch 33/64 loss: -0.37105002999305725
Batch 34/64 loss: -0.312261700630188
Batch 35/64 loss: -0.3988420367240906
Batch 36/64 loss: -0.3660561442375183
Batch 37/64 loss: -0.32906264066696167
Batch 38/64 loss: -0.34505146741867065
Batch 39/64 loss: -0.3445066511631012
Batch 40/64 loss: -0.39169061183929443
Batch 41/64 loss: -0.36691322922706604
Batch 42/64 loss: -0.3350673019886017
Batch 43/64 loss: -0.3570697009563446
Batch 44/64 loss: -0.3688861131668091
Batch 45/64 loss: -0.38236185908317566
Batch 46/64 loss: -0.3905528783798218
Batch 47/64 loss: -0.36411917209625244
Batch 48/64 loss: -0.3978055417537689
Batch 49/64 loss: -0.38717037439346313
Batch 50/64 loss: -0.3622795343399048
Batch 51/64 loss: -0.358569473028183
Batch 52/64 loss: -0.34791573882102966
Batch 53/64 loss: -0.3733489513397217
Batch 54/64 loss: -0.4140782356262207
Batch 55/64 loss: -0.38215571641921997
Batch 56/64 loss: -0.39116016030311584
Batch 57/64 loss: -0.3853786289691925
Batch 58/64 loss: -0.4104907512664795
Batch 59/64 loss: -0.38296160101890564
Batch 60/64 loss: -0.35514888167381287
Batch 61/64 loss: -0.3534875810146332
Batch 62/64 loss: -0.35831573605537415
Batch 63/64 loss: -0.3975798487663269
Batch 64/64 loss: -0.37451618909835815
Epoch 497  Train loss: -0.3760621704307257  Val loss: 0.03557686051961892
Epoch 498
-------------------------------
Batch 1/64 loss: -0.3903384506702423
Batch 2/64 loss: -0.40596914291381836
Batch 3/64 loss: -0.2617897689342499
Batch 4/64 loss: -0.3857497572898865
Batch 5/64 loss: -0.3758595585823059
Batch 6/64 loss: -0.3854542672634125
Batch 7/64 loss: -0.4001997709274292
Batch 8/64 loss: -0.3672391176223755
Batch 9/64 loss: -0.3926466107368469
Batch 10/64 loss: -0.3442172706127167
Batch 11/64 loss: -0.353121280670166
Batch 12/64 loss: -0.3904569745063782
Batch 13/64 loss: -0.3758087158203125
Batch 14/64 loss: -0.3757132291793823
Batch 15/64 loss: -0.40130239725112915
Batch 16/64 loss: -0.34997448325157166
Batch 17/64 loss: -0.3927910029888153
Batch 18/64 loss: -0.33454737067222595
Batch 19/64 loss: -0.38190770149230957
Batch 20/64 loss: -0.38830310106277466
Batch 21/64 loss: -0.38118815422058105
Batch 22/64 loss: -0.37879180908203125
Batch 23/64 loss: -0.3965698778629303
Batch 24/64 loss: -0.4020773768424988
Batch 25/64 loss: -0.36710602045059204
Batch 26/64 loss: -0.3664834797382355
Batch 27/64 loss: -0.38100099563598633
Batch 28/64 loss: -0.31718650460243225
Batch 29/64 loss: -0.2991166412830353
Batch 30/64 loss: -0.3797142505645752
Batch 31/64 loss: -0.3608573377132416
Batch 32/64 loss: -0.37108922004699707
Batch 33/64 loss: -0.39837896823883057
Batch 34/64 loss: -0.3893265128135681
Batch 35/64 loss: -0.37466782331466675
Batch 36/64 loss: -0.39956048130989075
Batch 37/64 loss: -0.3692787289619446
Batch 38/64 loss: -0.3794463276863098
Batch 39/64 loss: -0.36828577518463135
Batch 40/64 loss: -0.37709468603134155
Batch 41/64 loss: -0.390612930059433
Batch 42/64 loss: -0.36525243520736694
Batch 43/64 loss: -0.37554121017456055
Batch 44/64 loss: -0.34577953815460205
Batch 45/64 loss: -0.38730791211128235
Batch 46/64 loss: -0.39347028732299805
Batch 47/64 loss: -0.39116230607032776
Batch 48/64 loss: -0.3807793855667114
Batch 49/64 loss: -0.3923856019973755
Batch 50/64 loss: -0.3932424783706665
Batch 51/64 loss: -0.3682294487953186
Batch 52/64 loss: -0.35484546422958374
Batch 53/64 loss: -0.3017778992652893
Batch 54/64 loss: -0.35069558024406433
Batch 55/64 loss: -0.36133965849876404
Batch 56/64 loss: -0.3410347104072571
Batch 57/64 loss: -0.37993890047073364
Batch 58/64 loss: -0.4009430408477783
Batch 59/64 loss: -0.3658694326877594
Batch 60/64 loss: -0.36963117122650146
Batch 61/64 loss: -0.37790676951408386
Batch 62/64 loss: -0.3663237690925598
Batch 63/64 loss: -0.3925270736217499
Batch 64/64 loss: -0.34443917870521545
Epoch 498  Train loss: -0.372008428737229  Val loss: 0.03814381688730823
Epoch 499
-------------------------------
Batch 1/64 loss: -0.3975091874599457
Batch 2/64 loss: -0.3835936188697815
Batch 3/64 loss: -0.3928280770778656
Batch 4/64 loss: -0.373099684715271
Batch 5/64 loss: -0.37207216024398804
Batch 6/64 loss: -0.37818312644958496
Batch 7/64 loss: -0.35674747824668884
Batch 8/64 loss: -0.3757273554801941
Batch 9/64 loss: -0.40959829092025757
Batch 10/64 loss: -0.3857262134552002
Batch 11/64 loss: -0.39443284273147583
Batch 12/64 loss: -0.3686997890472412
Batch 13/64 loss: -0.40199702978134155
Batch 14/64 loss: -0.3817216455936432
Batch 15/64 loss: -0.3576395511627197
Batch 16/64 loss: -0.33246052265167236
Batch 17/64 loss: -0.39685672521591187
Batch 18/64 loss: -0.35141637921333313
Batch 19/64 loss: -0.3600448966026306
Batch 20/64 loss: -0.39057016372680664
Batch 21/64 loss: -0.36927974224090576
Batch 22/64 loss: -0.39060163497924805
Batch 23/64 loss: -0.39094245433807373
Batch 24/64 loss: -0.4043165445327759
Batch 25/64 loss: -0.41048839688301086
Batch 26/64 loss: -0.38615310192108154
Batch 27/64 loss: -0.37436723709106445
Batch 28/64 loss: -0.3434126377105713
Batch 29/64 loss: -0.3583458662033081
Batch 30/64 loss: -0.38821280002593994
Batch 31/64 loss: -0.34153273701667786
Batch 32/64 loss: -0.3394351601600647
Batch 33/64 loss: -0.3510855734348297
Batch 34/64 loss: -0.3911553621292114
Batch 35/64 loss: -0.31940019130706787
Batch 36/64 loss: -0.4093911647796631
Batch 37/64 loss: -0.4013570547103882
Batch 38/64 loss: -0.3790353536605835
Batch 39/64 loss: -0.40403181314468384
Batch 40/64 loss: -0.3934185802936554
Batch 41/64 loss: -0.389156699180603
Batch 42/64 loss: -0.3727438747882843
Batch 43/64 loss: -0.31299450993537903
Batch 44/64 loss: -0.38670897483825684
Batch 45/64 loss: -0.35652443766593933
Batch 46/64 loss: -0.3755193054676056
Batch 47/64 loss: -0.37370219826698303
Batch 48/64 loss: -0.35485002398490906
Batch 49/64 loss: -0.3657954931259155
Batch 50/64 loss: -0.37610000371932983
Batch 51/64 loss: -0.3655591905117035
Batch 52/64 loss: -0.34917259216308594
Batch 53/64 loss: -0.36619019508361816
Batch 54/64 loss: -0.37058642506599426
Batch 55/64 loss: -0.3679594099521637
Batch 56/64 loss: -0.3443604111671448
Batch 57/64 loss: -0.35728704929351807
Batch 58/64 loss: -0.3732687830924988
Batch 59/64 loss: -0.3803820312023163
Batch 60/64 loss: -0.3749679625034332
Batch 61/64 loss: -0.3632160723209381
Batch 62/64 loss: -0.36922723054885864
Batch 63/64 loss: -0.37539029121398926
Batch 64/64 loss: -0.3618185222148895
Epoch 499  Train loss: -0.3733320031680313  Val loss: 0.03730606213468047
Epoch 500
-------------------------------
Batch 1/64 loss: -0.36447617411613464
Batch 2/64 loss: -0.3741883635520935
Batch 3/64 loss: -0.35212060809135437
Batch 4/64 loss: -0.3544771075248718
Batch 5/64 loss: -0.3627983331680298
Batch 6/64 loss: -0.3710356652736664
Batch 7/64 loss: -0.3977230191230774
Batch 8/64 loss: -0.36441653966903687
Batch 9/64 loss: -0.3871675729751587
Batch 10/64 loss: -0.3900981545448303
Batch 11/64 loss: -0.37503713369369507
Batch 12/64 loss: -0.40159863233566284
Batch 13/64 loss: -0.38725024461746216
Batch 14/64 loss: -0.39478883147239685
Batch 15/64 loss: -0.3047788739204407
Batch 16/64 loss: -0.3691958487033844
Batch 17/64 loss: -0.3272531032562256
Batch 18/64 loss: -0.373107373714447
Batch 19/64 loss: -0.39159664511680603
Batch 20/64 loss: -0.39328598976135254
Batch 21/64 loss: -0.36676883697509766
Batch 22/64 loss: -0.3219308853149414
Batch 23/64 loss: -0.37731650471687317
Batch 24/64 loss: -0.39540085196495056
Batch 25/64 loss: -0.39960426092147827
Batch 26/64 loss: -0.3924780488014221
Batch 27/64 loss: -0.3828127086162567
Batch 28/64 loss: -0.4006604552268982
Batch 29/64 loss: -0.3817927837371826
Batch 30/64 loss: -0.3789452612400055
Batch 31/64 loss: -0.4018760621547699
Batch 32/64 loss: -0.3517904579639435
Batch 33/64 loss: -0.37969914078712463
Batch 34/64 loss: -0.405889630317688
Batch 35/64 loss: -0.33875584602355957
Batch 36/64 loss: -0.38676032423973083
Batch 37/64 loss: -0.3812296390533447
Batch 38/64 loss: -0.35733261704444885
Batch 39/64 loss: -0.35590142011642456
Batch 40/64 loss: -0.3618146777153015
Batch 41/64 loss: -0.378734827041626
Batch 42/64 loss: -0.3854093849658966
Batch 43/64 loss: -0.37897026538848877
Batch 44/64 loss: -0.3491324782371521
Batch 45/64 loss: -0.41137829422950745
Batch 46/64 loss: -0.38016682863235474
Batch 47/64 loss: -0.3799046277999878
Batch 48/64 loss: -0.3652147948741913
Batch 49/64 loss: -0.36140236258506775
Batch 50/64 loss: -0.3707907199859619
Batch 51/64 loss: -0.381960391998291
Batch 52/64 loss: -0.3590971529483795
Batch 53/64 loss: -0.3685917258262634
Batch 54/64 loss: -0.35037708282470703
Batch 55/64 loss: -0.3859660029411316
Batch 56/64 loss: -0.3863137662410736
Batch 57/64 loss: -0.3605089485645294
Batch 58/64 loss: -0.3956444263458252
Batch 59/64 loss: -0.38353097438812256
Batch 60/64 loss: -0.38195762038230896
Batch 61/64 loss: -0.3300561010837555
Batch 62/64 loss: -0.3215963840484619
Batch 63/64 loss: -0.3892499804496765
Batch 64/64 loss: -0.39396849274635315
Epoch 500  Train loss: -0.3734366453161427  Val loss: 0.03712646555654781
SLIC undersegmentation error: 0.12412920962199316
SLIC inter-cluster variation: 0.13904419774313004
SLIC number of superpixels: 21483
SLIC superpixels per image: 73.82474226804123
Model loaded
Test metrics:
0.004645793298675432 0.3183697594501719 23.98051416275158 tensor(0.2572, dtype=torch.float64) 0.8293520453866826 3.5576291430277696 26792
Inference time: 0.0037876273348569052 seconds
Relabeled undersegmentation error: 0.09668041237113403
Relabeled inter-cluster variation: 0.05414773954542279
Relabeled mean superpixels count: 327.54639175257734
Original mean superpixels count: 92.06872852233677
Done!
Job id: 420579
Job id: 422904
