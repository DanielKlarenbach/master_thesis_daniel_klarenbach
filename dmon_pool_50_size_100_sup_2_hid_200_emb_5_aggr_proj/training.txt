Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 4.637378215789795
Batch 2/64 loss: 4.689788341522217
Batch 3/64 loss: 4.630141258239746
Batch 4/64 loss: 4.609372615814209
Batch 5/64 loss: 4.614858627319336
Batch 6/64 loss: 4.603020191192627
Batch 7/64 loss: 4.604787826538086
Batch 8/64 loss: 4.591606616973877
Batch 9/64 loss: 4.6033244132995605
Batch 10/64 loss: 4.5946502685546875
Batch 11/64 loss: 4.588875770568848
Batch 12/64 loss: 4.590104103088379
Batch 13/64 loss: 4.588926315307617
Batch 14/64 loss: 4.584547996520996
Batch 15/64 loss: 4.5893683433532715
Batch 16/64 loss: 4.5869903564453125
Batch 17/64 loss: 4.581432342529297
Batch 18/64 loss: 4.583488464355469
Batch 19/64 loss: 4.582211971282959
Batch 20/64 loss: 4.581741809844971
Batch 21/64 loss: 4.581796169281006
Batch 22/64 loss: 4.583549499511719
Batch 23/64 loss: 4.579840183258057
Batch 24/64 loss: 4.581202507019043
Batch 25/64 loss: 4.580681800842285
Batch 26/64 loss: 4.58087682723999
Batch 27/64 loss: 4.579564094543457
Batch 28/64 loss: 4.580143928527832
Batch 29/64 loss: 4.5794267654418945
Batch 30/64 loss: 4.579743385314941
Batch 31/64 loss: 4.579068660736084
Batch 32/64 loss: 4.5791916847229
Batch 33/64 loss: 4.578647613525391
Batch 34/64 loss: 4.578338146209717
Batch 35/64 loss: 4.577396392822266
Batch 36/64 loss: 4.578419208526611
Batch 37/64 loss: 4.578973770141602
Batch 38/64 loss: 4.5776214599609375
Batch 39/64 loss: 4.5778961181640625
Batch 40/64 loss: 4.579164028167725
Batch 41/64 loss: 4.576486587524414
Batch 42/64 loss: 4.578218460083008
Batch 43/64 loss: 4.577911853790283
Batch 44/64 loss: 4.579442977905273
Batch 45/64 loss: 4.577895164489746
Batch 46/64 loss: 4.577963829040527
Batch 47/64 loss: 4.576529026031494
Batch 48/64 loss: 4.576992988586426
Batch 49/64 loss: 4.577697277069092
Batch 50/64 loss: 4.577681064605713
Batch 51/64 loss: 4.575699806213379
Batch 52/64 loss: 4.5755696296691895
Batch 53/64 loss: 4.5764665603637695
Batch 54/64 loss: 4.575234889984131
Batch 55/64 loss: 4.575677394866943
Batch 56/64 loss: 4.573790073394775
Batch 57/64 loss: 4.574209213256836
Batch 58/64 loss: 4.572927951812744
Batch 59/64 loss: 4.574665069580078
Batch 60/64 loss: 4.5741963386535645
Batch 61/64 loss: 4.5738372802734375
Batch 62/64 loss: 4.572081089019775
Batch 63/64 loss: 4.571402549743652
Batch 64/64 loss: 3.781087875366211
Epoch 1  Train loss: 4.575789036470301  Val loss: 4.543250120997019
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 4.57383394241333
Batch 2/64 loss: 4.569601058959961
Batch 3/64 loss: 4.567545413970947
Batch 4/64 loss: 4.570265293121338
Batch 5/64 loss: 4.570412635803223
Batch 6/64 loss: 4.567012786865234
Batch 7/64 loss: 4.56446647644043
Batch 8/64 loss: 4.565505027770996
Batch 9/64 loss: 4.564830303192139
Batch 10/64 loss: 4.566966533660889
Batch 11/64 loss: 4.561392307281494
Batch 12/64 loss: 4.572084903717041
Batch 13/64 loss: 4.572732448577881
Batch 14/64 loss: 4.563248634338379
Batch 15/64 loss: 4.56390380859375
Batch 16/64 loss: 4.55933952331543
Batch 17/64 loss: 4.560327053070068
Batch 18/64 loss: 4.560807228088379
Batch 19/64 loss: 4.558544158935547
Batch 20/64 loss: 4.56370735168457
Batch 21/64 loss: 4.569442272186279
Batch 22/64 loss: 4.558028697967529
Batch 23/64 loss: 4.559188365936279
Batch 24/64 loss: 4.5667595863342285
Batch 25/64 loss: 4.552091121673584
Batch 26/64 loss: 4.549515247344971
Batch 27/64 loss: 4.552876949310303
Batch 28/64 loss: 4.547676086425781
Batch 29/64 loss: 4.545342922210693
Batch 30/64 loss: 4.5444512367248535
Batch 31/64 loss: 4.543282508850098
Batch 32/64 loss: 4.570174217224121
Batch 33/64 loss: 4.53273344039917
Batch 34/64 loss: 4.537228584289551
Batch 35/64 loss: 4.54585599899292
Batch 36/64 loss: 4.54323673248291
Batch 37/64 loss: 4.541975975036621
Batch 38/64 loss: 4.557943344116211
Batch 39/64 loss: 4.528424263000488
Batch 40/64 loss: 4.532506465911865
Batch 41/64 loss: 4.526848316192627
Batch 42/64 loss: 4.5247721672058105
Batch 43/64 loss: 4.502201080322266
Batch 44/64 loss: 4.489859580993652
Batch 45/64 loss: 4.500939846038818
Batch 46/64 loss: 4.469022750854492
Batch 47/64 loss: 4.447851657867432
Batch 48/64 loss: 4.46390438079834
Batch 49/64 loss: 4.483986854553223
Batch 50/64 loss: 4.429543972015381
Batch 51/64 loss: 4.462815761566162
Batch 52/64 loss: 4.444092273712158
Batch 53/64 loss: 4.487245082855225
Batch 54/64 loss: 4.47286319732666
Batch 55/64 loss: 4.483557224273682
Batch 56/64 loss: 4.445958614349365
Batch 57/64 loss: 4.406524181365967
Batch 58/64 loss: 4.469379901885986
Batch 59/64 loss: 4.415439605712891
Batch 60/64 loss: 4.4361724853515625
Batch 61/64 loss: 4.453803539276123
Batch 62/64 loss: 4.543765544891357
Batch 63/64 loss: 5.460811614990234
Batch 64/64 loss: 3.6091103553771973
Epoch 2  Train loss: 4.530563973445519  Val loss: 5.967995889408073
Epoch 3
-------------------------------
Batch 1/64 loss: 4.615797519683838
Batch 2/64 loss: 4.52734899520874
Batch 3/64 loss: 4.4782891273498535
Batch 4/64 loss: 4.486528396606445
Batch 5/64 loss: 4.449748516082764
Batch 6/64 loss: 4.445028305053711
Batch 7/64 loss: 4.511392116546631
Batch 8/64 loss: 4.492424964904785
Batch 9/64 loss: 4.448785305023193
Batch 10/64 loss: 4.463894844055176
Batch 11/64 loss: 4.397092819213867
Batch 12/64 loss: 4.447569370269775
Batch 13/64 loss: 4.415627479553223
Batch 14/64 loss: 4.437649726867676
Batch 15/64 loss: 4.478280067443848
Batch 16/64 loss: 4.406371593475342
Batch 17/64 loss: 4.444518566131592
Batch 18/64 loss: 4.344781875610352
Batch 19/64 loss: 4.46021032333374
Batch 20/64 loss: 4.37664794921875
Batch 21/64 loss: 4.4330034255981445
Batch 22/64 loss: 4.392117023468018
Batch 23/64 loss: 4.335129261016846
Batch 24/64 loss: 4.360426902770996
Batch 25/64 loss: 4.437256336212158
Batch 26/64 loss: 4.380600929260254
Batch 27/64 loss: 4.313559532165527
Batch 28/64 loss: 4.330931663513184
Batch 29/64 loss: 4.349648952484131
Batch 30/64 loss: 4.270923614501953
Batch 31/64 loss: 4.438416481018066
Batch 32/64 loss: 4.3078083992004395
Batch 33/64 loss: 4.380056381225586
Batch 34/64 loss: 4.31456184387207
Batch 35/64 loss: 4.298184871673584
Batch 36/64 loss: 4.301085472106934
Batch 37/64 loss: 4.34293794631958
Batch 38/64 loss: 4.372533321380615
Batch 39/64 loss: 4.283995628356934
Batch 40/64 loss: 4.289852619171143
Batch 41/64 loss: 4.316338539123535
Batch 42/64 loss: 4.379439353942871
Batch 43/64 loss: 4.215018272399902
Batch 44/64 loss: 4.302801609039307
Batch 45/64 loss: 4.4437408447265625
Batch 46/64 loss: 4.269565582275391
Batch 47/64 loss: 4.28820276260376
Batch 48/64 loss: 4.278952598571777
Batch 49/64 loss: 4.278162002563477
Batch 50/64 loss: 4.267450332641602
Batch 51/64 loss: 4.294830322265625
Batch 52/64 loss: 4.229830265045166
Batch 53/64 loss: 4.30007266998291
Batch 54/64 loss: 4.2858381271362305
Batch 55/64 loss: 4.302921295166016
Batch 56/64 loss: 4.275788307189941
Batch 57/64 loss: 4.455137252807617
Batch 58/64 loss: 4.273488998413086
Batch 59/64 loss: 4.215507507324219
Batch 60/64 loss: 4.2425994873046875
Batch 61/64 loss: 4.19056510925293
Batch 62/64 loss: 4.232792377471924
Batch 63/64 loss: 4.252474308013916
Batch 64/64 loss: 3.1434826850891113
Epoch 3  Train loss: 4.34525722989849  Val loss: 4.2672046820322675
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 4.196054935455322
Batch 2/64 loss: 4.182564735412598
Batch 3/64 loss: 4.232507705688477
Batch 4/64 loss: 4.172502040863037
Batch 5/64 loss: 4.119283199310303
Batch 6/64 loss: 4.134195327758789
Batch 7/64 loss: 4.106731414794922
Batch 8/64 loss: 4.183413028717041
Batch 9/64 loss: 4.2115960121154785
Batch 10/64 loss: 4.262120723724365
Batch 11/64 loss: 4.290992259979248
Batch 12/64 loss: 4.221591472625732
Batch 13/64 loss: 4.183061122894287
Batch 14/64 loss: 4.3172526359558105
Batch 15/64 loss: 4.296419143676758
Batch 16/64 loss: 4.241620063781738
Batch 17/64 loss: 4.135666370391846
Batch 18/64 loss: 4.149122714996338
Batch 19/64 loss: 4.160735130310059
Batch 20/64 loss: 4.313879013061523
Batch 21/64 loss: 4.119390964508057
Batch 22/64 loss: 4.109661102294922
Batch 23/64 loss: 4.126227378845215
Batch 24/64 loss: 4.255500793457031
Batch 25/64 loss: 4.173527240753174
Batch 26/64 loss: 4.118507385253906
Batch 27/64 loss: 4.058154106140137
Batch 28/64 loss: 4.1241068840026855
Batch 29/64 loss: 4.122467994689941
Batch 30/64 loss: 4.103886127471924
Batch 31/64 loss: 4.13086462020874
Batch 32/64 loss: 4.147120475769043
Batch 33/64 loss: 4.098831653594971
Batch 34/64 loss: 4.098008155822754
Batch 35/64 loss: 4.189831256866455
Batch 36/64 loss: 4.294022560119629
Batch 37/64 loss: 4.2636213302612305
Batch 38/64 loss: 4.187694549560547
Batch 39/64 loss: 4.1561431884765625
Batch 40/64 loss: 4.074031829833984
Batch 41/64 loss: 4.066917419433594
Batch 42/64 loss: 4.193084716796875
Batch 43/64 loss: 4.055908679962158
Batch 44/64 loss: 4.273553848266602
Batch 45/64 loss: 4.146010398864746
Batch 46/64 loss: 4.330775260925293
Batch 47/64 loss: 4.2091450691223145
Batch 48/64 loss: 4.125539302825928
Batch 49/64 loss: 4.108758926391602
Batch 50/64 loss: 4.183764934539795
Batch 51/64 loss: 4.067584991455078
Batch 52/64 loss: 4.212859630584717
Batch 53/64 loss: 4.2512969970703125
Batch 54/64 loss: 4.079301834106445
Batch 55/64 loss: 4.268500804901123
Batch 56/64 loss: 4.128650665283203
Batch 57/64 loss: 4.097475528717041
Batch 58/64 loss: 4.085587501525879
Batch 59/64 loss: 4.107874870300293
Batch 60/64 loss: 4.158236503601074
Batch 61/64 loss: 4.164401054382324
Batch 62/64 loss: 4.041625022888184
Batch 63/64 loss: 4.024886131286621
Batch 64/64 loss: 2.8102152347564697
Epoch 4  Train loss: 4.149840159509696  Val loss: 4.178326328185825
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 4.06105899810791
Batch 2/64 loss: 4.014670372009277
Batch 3/64 loss: 4.055192470550537
Batch 4/64 loss: 4.0195136070251465
Batch 5/64 loss: 3.9506869316101074
Batch 6/64 loss: 4.165919780731201
Batch 7/64 loss: 4.1526899337768555
Batch 8/64 loss: 4.099281311035156
Batch 9/64 loss: 4.022218704223633
Batch 10/64 loss: 3.9137144088745117
Batch 11/64 loss: 4.124271869659424
Batch 12/64 loss: 3.912598133087158
Batch 13/64 loss: 4.027674674987793
Batch 14/64 loss: 4.001399040222168
Batch 15/64 loss: 3.900087833404541
Batch 16/64 loss: 3.9133005142211914
Batch 17/64 loss: 3.8554418087005615
Batch 18/64 loss: 3.932002067565918
Batch 19/64 loss: 4.004036903381348
Batch 20/64 loss: 3.9407711029052734
Batch 21/64 loss: 3.8649656772613525
Batch 22/64 loss: 3.88340425491333
Batch 23/64 loss: 3.9379544258117676
Batch 24/64 loss: 3.9267611503601074
Batch 25/64 loss: 3.905380964279175
Batch 26/64 loss: 4.0237717628479
Batch 27/64 loss: 4.187083721160889
Batch 28/64 loss: 3.915555000305176
Batch 29/64 loss: 3.874169111251831
Batch 30/64 loss: 4.127416133880615
Batch 31/64 loss: 4.054248809814453
Batch 32/64 loss: 3.894280433654785
Batch 33/64 loss: 3.8822004795074463
Batch 34/64 loss: 3.9007601737976074
Batch 35/64 loss: 4.03631067276001
Batch 36/64 loss: 3.8566722869873047
Batch 37/64 loss: 3.9921445846557617
Batch 38/64 loss: 4.031667232513428
Batch 39/64 loss: 4.314938068389893
Batch 40/64 loss: 4.01092004776001
Batch 41/64 loss: 4.065800189971924
Batch 42/64 loss: 3.8510608673095703
Batch 43/64 loss: 4.036135673522949
Batch 44/64 loss: 3.832042694091797
Batch 45/64 loss: 4.004371166229248
Batch 46/64 loss: 3.861006259918213
Batch 47/64 loss: 3.8968796730041504
Batch 48/64 loss: 3.851367950439453
Batch 49/64 loss: 4.123875617980957
Batch 50/64 loss: 3.7599716186523438
Batch 51/64 loss: 3.885817289352417
Batch 52/64 loss: 3.9054901599884033
Batch 53/64 loss: 3.8019776344299316
Batch 54/64 loss: 3.93613338470459
Batch 55/64 loss: 3.7053322792053223
Batch 56/64 loss: 3.903533697128296
Batch 57/64 loss: 4.226840019226074
Batch 58/64 loss: 3.9361343383789062
Batch 59/64 loss: 3.8586370944976807
Batch 60/64 loss: 3.7698140144348145
Batch 61/64 loss: 3.7982869148254395
Batch 62/64 loss: 3.950042724609375
Batch 63/64 loss: 3.7303872108459473
Batch 64/64 loss: 2.2036969661712646
Epoch 5  Train loss: 3.9377387710646086  Val loss: 3.8989252781949912
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 3.660189151763916
Batch 2/64 loss: 4.039261817932129
Batch 3/64 loss: 3.9168081283569336
Batch 4/64 loss: 3.813497304916382
Batch 5/64 loss: 3.6671504974365234
Batch 6/64 loss: 3.773179531097412
Batch 7/64 loss: 4.064713478088379
Batch 8/64 loss: 3.675199508666992
Batch 9/64 loss: 3.7928764820098877
Batch 10/64 loss: 3.907183885574341
Batch 11/64 loss: 3.8174824714660645
Batch 12/64 loss: 3.8272390365600586
Batch 13/64 loss: 3.710287570953369
Batch 14/64 loss: 3.831599712371826
Batch 15/64 loss: 3.656615734100342
Batch 16/64 loss: 3.700939655303955
Batch 17/64 loss: 3.941399097442627
Batch 18/64 loss: 3.8802425861358643
Batch 19/64 loss: 3.675158977508545
Batch 20/64 loss: 3.644528865814209
Batch 21/64 loss: 3.6783323287963867
Batch 22/64 loss: 3.792943000793457
Batch 23/64 loss: 3.789503812789917
Batch 24/64 loss: 3.732767105102539
Batch 25/64 loss: 3.7031784057617188
Batch 26/64 loss: 3.6002306938171387
Batch 27/64 loss: 3.71988582611084
Batch 28/64 loss: 3.713449001312256
Batch 29/64 loss: 3.7045209407806396
Batch 30/64 loss: 3.7023563385009766
Batch 31/64 loss: 3.6548821926116943
Batch 32/64 loss: 3.60821270942688
Batch 33/64 loss: 3.5922694206237793
Batch 34/64 loss: 3.7269234657287598
Batch 35/64 loss: 3.568223476409912
Batch 36/64 loss: 3.7025506496429443
Batch 37/64 loss: 3.5246901512145996
Batch 38/64 loss: 3.565791368484497
Batch 39/64 loss: 3.668417453765869
Batch 40/64 loss: 3.6005642414093018
Batch 41/64 loss: 3.561566114425659
Batch 42/64 loss: 3.6895225048065186
Batch 43/64 loss: 3.5431740283966064
Batch 44/64 loss: 3.5526044368743896
Batch 45/64 loss: 3.5132639408111572
Batch 46/64 loss: 3.5033891201019287
Batch 47/64 loss: 3.4881927967071533
Batch 48/64 loss: 3.881516933441162
Batch 49/64 loss: 3.558349609375
Batch 50/64 loss: 3.782681703567505
Batch 51/64 loss: 3.4759583473205566
Batch 52/64 loss: 3.4929771423339844
Batch 53/64 loss: 4.053278923034668
Batch 54/64 loss: 3.723641872406006
Batch 55/64 loss: 3.578274965286255
Batch 56/64 loss: 3.6025733947753906
Batch 57/64 loss: 3.5673317909240723
Batch 58/64 loss: 3.4444897174835205
Batch 59/64 loss: 3.8074800968170166
Batch 60/64 loss: 3.428035020828247
Batch 61/64 loss: 3.5761547088623047
Batch 62/64 loss: 3.568953037261963
Batch 63/64 loss: 3.3997836112976074
Batch 64/64 loss: 1.8778727054595947
Epoch 6  Train loss: 3.66347991251478  Val loss: 3.6440960861153617
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 3.8797428607940674
Batch 2/64 loss: 3.666137456893921
Batch 3/64 loss: 3.418405532836914
Batch 4/64 loss: 3.5309274196624756
Batch 5/64 loss: 3.339768648147583
Batch 6/64 loss: 3.735882043838501
Batch 7/64 loss: 3.5661702156066895
Batch 8/64 loss: 3.4368386268615723
Batch 9/64 loss: 3.5070247650146484
Batch 10/64 loss: 3.467576742172241
Batch 11/64 loss: 3.488574266433716
Batch 12/64 loss: 3.4914398193359375
Batch 13/64 loss: 3.3313827514648438
Batch 14/64 loss: 3.386287212371826
Batch 15/64 loss: 3.478804111480713
Batch 16/64 loss: 3.4404876232147217
Batch 17/64 loss: 3.4927244186401367
Batch 18/64 loss: 3.3717057704925537
Batch 19/64 loss: 3.3332631587982178
Batch 20/64 loss: 3.315945625305176
Batch 21/64 loss: 3.333667516708374
Batch 22/64 loss: 3.4882140159606934
Batch 23/64 loss: 3.4651637077331543
Batch 24/64 loss: 3.548712968826294
Batch 25/64 loss: 3.433075189590454
Batch 26/64 loss: 3.5913102626800537
Batch 27/64 loss: 3.3691158294677734
Batch 28/64 loss: 3.5713181495666504
Batch 29/64 loss: 3.189298152923584
Batch 30/64 loss: 3.1941866874694824
Batch 31/64 loss: 3.4686315059661865
Batch 32/64 loss: 3.5285465717315674
Batch 33/64 loss: 3.227320909500122
Batch 34/64 loss: 3.2558388710021973
Batch 35/64 loss: 3.1363751888275146
Batch 36/64 loss: 3.361725330352783
Batch 37/64 loss: 3.3681752681732178
Batch 38/64 loss: 3.3604397773742676
Batch 39/64 loss: 3.254647731781006
Batch 40/64 loss: 3.1949050426483154
Batch 41/64 loss: 3.0891993045806885
Batch 42/64 loss: 3.145188570022583
Batch 43/64 loss: 3.032435655593872
Batch 44/64 loss: 3.1666698455810547
Batch 45/64 loss: 3.112602949142456
Batch 46/64 loss: 2.952207565307617
Batch 47/64 loss: 3.1793839931488037
Batch 48/64 loss: 3.1647770404815674
Batch 49/64 loss: 3.184221029281616
Batch 50/64 loss: 3.5062053203582764
Batch 51/64 loss: 3.251973867416382
Batch 52/64 loss: 3.0811829566955566
Batch 53/64 loss: 3.375718832015991
Batch 54/64 loss: 3.2542660236358643
Batch 55/64 loss: 2.9578633308410645
Batch 56/64 loss: 3.0372626781463623
Batch 57/64 loss: 2.9314212799072266
Batch 58/64 loss: 3.4485466480255127
Batch 59/64 loss: 2.9875426292419434
Batch 60/64 loss: 3.1241331100463867
Batch 61/64 loss: 3.543703556060791
Batch 62/64 loss: 3.0017383098602295
Batch 63/64 loss: 3.015745162963867
Batch 64/64 loss: 0.928164005279541
Epoch 7  Train loss: 3.2981940456465177  Val loss: 3.106728976534814
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 2.9794201850891113
Batch 2/64 loss: 3.0957274436950684
Batch 3/64 loss: 2.985618829727173
Batch 4/64 loss: 3.117597818374634
Batch 5/64 loss: 3.1045491695404053
Batch 6/64 loss: 2.826934337615967
Batch 7/64 loss: 3.1222143173217773
Batch 8/64 loss: 3.2301676273345947
Batch 9/64 loss: 2.765765428543091
Batch 10/64 loss: 3.211561918258667
Batch 11/64 loss: 3.0211706161499023
Batch 12/64 loss: 2.979099750518799
Batch 13/64 loss: 3.152318000793457
Batch 14/64 loss: 2.8915247917175293
Batch 15/64 loss: 2.9445960521698
Batch 16/64 loss: 2.9404139518737793
Batch 17/64 loss: 2.88369083404541
Batch 18/64 loss: 2.804776430130005
Batch 19/64 loss: 3.0605151653289795
Batch 20/64 loss: 2.745286226272583
Batch 21/64 loss: 2.992032527923584
Batch 22/64 loss: 3.057976722717285
Batch 23/64 loss: 2.972548484802246
Batch 24/64 loss: 2.748171091079712
Batch 25/64 loss: 2.891469717025757
Batch 26/64 loss: 2.9534313678741455
Batch 27/64 loss: 2.873457193374634
Batch 28/64 loss: 2.835108518600464
Batch 29/64 loss: 3.2253973484039307
Batch 30/64 loss: 2.873516082763672
Batch 31/64 loss: 2.8365490436553955
Batch 32/64 loss: 3.1904187202453613
Batch 33/64 loss: 2.853393316268921
Batch 34/64 loss: 2.9339914321899414
Batch 35/64 loss: 2.4995710849761963
Batch 36/64 loss: 2.8862030506134033
Batch 37/64 loss: 2.875835657119751
Batch 38/64 loss: 2.7251484394073486
Batch 39/64 loss: 2.737071990966797
Batch 40/64 loss: 2.827503204345703
Batch 41/64 loss: 2.579540252685547
Batch 42/64 loss: 2.6756949424743652
Batch 43/64 loss: 2.6860809326171875
Batch 44/64 loss: 2.727285623550415
Batch 45/64 loss: 2.6396255493164062
Batch 46/64 loss: 2.69922137260437
Batch 47/64 loss: 2.5116195678710938
Batch 48/64 loss: 2.423962354660034
Batch 49/64 loss: 2.661741256713867
Batch 50/64 loss: 2.7931692600250244
Batch 51/64 loss: 3.116925001144409
Batch 52/64 loss: 2.7589638233184814
Batch 53/64 loss: 2.6373236179351807
Batch 54/64 loss: 2.6630353927612305
Batch 55/64 loss: 2.8063385486602783
Batch 56/64 loss: 2.7338201999664307
Batch 57/64 loss: 2.7709195613861084
Batch 58/64 loss: 2.5717177391052246
Batch 59/64 loss: 2.3882272243499756
Batch 60/64 loss: 2.575206756591797
Batch 61/64 loss: 2.7678370475769043
Batch 62/64 loss: 2.8084137439727783
Batch 63/64 loss: 2.8598244190216064
Batch 64/64 loss: 0.2697153091430664
Epoch 8  Train loss: 2.8189886205336627  Val loss: 2.6996572107793537
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 2.9291012287139893
Batch 2/64 loss: 2.676759719848633
Batch 3/64 loss: 2.531094551086426
Batch 4/64 loss: 2.4099512100219727
Batch 5/64 loss: 2.4142603874206543
Batch 6/64 loss: 2.611764907836914
Batch 7/64 loss: 2.295832633972168
Batch 8/64 loss: 2.5066537857055664
Batch 9/64 loss: 2.7190146446228027
Batch 10/64 loss: 2.90250301361084
Batch 11/64 loss: 2.6976492404937744
Batch 12/64 loss: 2.527618885040283
Batch 13/64 loss: 2.537937641143799
Batch 14/64 loss: 2.8361408710479736
Batch 15/64 loss: 3.027473211288452
Batch 16/64 loss: 2.617879867553711
Batch 17/64 loss: 2.704843759536743
Batch 18/64 loss: 2.6063075065612793
Batch 19/64 loss: 2.7755517959594727
Batch 20/64 loss: 2.891092538833618
Batch 21/64 loss: 2.5075950622558594
Batch 22/64 loss: 2.683952808380127
Batch 23/64 loss: 2.4672374725341797
Batch 24/64 loss: 2.27817964553833
Batch 25/64 loss: 2.7599010467529297
Batch 26/64 loss: 2.907679557800293
Batch 27/64 loss: 2.6789255142211914
Batch 28/64 loss: 2.925584316253662
Batch 29/64 loss: 2.6539201736450195
Batch 30/64 loss: 2.323939323425293
Batch 31/64 loss: 2.7715063095092773
Batch 32/64 loss: 2.468923568725586
Batch 33/64 loss: 2.4956836700439453
Batch 34/64 loss: 2.6344404220581055
Batch 35/64 loss: 2.3456735610961914
Batch 36/64 loss: 2.5313720703125
Batch 37/64 loss: 2.6303815841674805
Batch 38/64 loss: 2.571033000946045
Batch 39/64 loss: 2.7149038314819336
Batch 40/64 loss: 2.83048677444458
Batch 41/64 loss: 2.510899543762207
Batch 42/64 loss: 2.671142578125
Batch 43/64 loss: 2.373661994934082
Batch 44/64 loss: 2.8215956687927246
Batch 45/64 loss: 2.7734951972961426
Batch 46/64 loss: 2.465880870819092
Batch 47/64 loss: 2.5029420852661133
Batch 48/64 loss: 2.382077693939209
Batch 49/64 loss: 2.6462221145629883
Batch 50/64 loss: 2.6215505599975586
Batch 51/64 loss: 2.211060047149658
Batch 52/64 loss: 2.557953357696533
Batch 53/64 loss: 2.5283432006835938
Batch 54/64 loss: 2.344310760498047
Batch 55/64 loss: 2.420520305633545
Batch 56/64 loss: 2.5369319915771484
Batch 57/64 loss: 2.5919132232666016
Batch 58/64 loss: 3.028632879257202
Batch 59/64 loss: 2.2701282501220703
Batch 60/64 loss: 2.578427314758301
Batch 61/64 loss: 2.6113648414611816
Batch 62/64 loss: 2.5919928550720215
Batch 63/64 loss: 2.4157447814941406
Batch 64/64 loss: 0.4848203659057617
Epoch 9  Train loss: 2.5760181726193894  Val loss: 2.589541884222391
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 2.2926931381225586
Batch 2/64 loss: 2.5556259155273438
Batch 3/64 loss: 2.4195351600646973
Batch 4/64 loss: 2.522634506225586
Batch 5/64 loss: 2.5553646087646484
Batch 6/64 loss: 2.350391387939453
Batch 7/64 loss: 2.6524100303649902
Batch 8/64 loss: 2.381070613861084
Batch 9/64 loss: 2.5388784408569336
Batch 10/64 loss: 2.4631195068359375
Batch 11/64 loss: 2.2664804458618164
Batch 12/64 loss: 2.363576889038086
Batch 13/64 loss: 2.543478488922119
Batch 14/64 loss: 2.2839365005493164
Batch 15/64 loss: 2.9910783767700195
Batch 16/64 loss: 2.4254884719848633
Batch 17/64 loss: 2.5019493103027344
Batch 18/64 loss: 2.363480567932129
Batch 19/64 loss: 2.444082736968994
Batch 20/64 loss: 2.6828489303588867
Batch 21/64 loss: 2.505594253540039
Batch 22/64 loss: 2.4986796379089355
Batch 23/64 loss: 2.158543586730957
Batch 24/64 loss: 2.627964496612549
Batch 25/64 loss: 2.448437213897705
Batch 26/64 loss: 2.511195659637451
Batch 27/64 loss: 2.4847970008850098
Batch 28/64 loss: 2.341270923614502
Batch 29/64 loss: 2.6033873558044434
Batch 30/64 loss: 2.7343626022338867
Batch 31/64 loss: 2.622422218322754
Batch 32/64 loss: 2.219351291656494
Batch 33/64 loss: 2.2518978118896484
Batch 34/64 loss: 2.5168991088867188
Batch 35/64 loss: 2.3148298263549805
Batch 36/64 loss: 2.570150375366211
Batch 37/64 loss: 2.5431928634643555
Batch 38/64 loss: 2.482917308807373
Batch 39/64 loss: 2.4775404930114746
Batch 40/64 loss: 2.1868739128112793
Batch 41/64 loss: 2.727749824523926
Batch 42/64 loss: 2.1171560287475586
Batch 43/64 loss: 2.4771571159362793
Batch 44/64 loss: 2.252650737762451
Batch 45/64 loss: 2.750865936279297
Batch 46/64 loss: 2.15706205368042
Batch 47/64 loss: 2.166566848754883
Batch 48/64 loss: 2.0418052673339844
Batch 49/64 loss: 2.0089192390441895
Batch 50/64 loss: 2.4622955322265625
Batch 51/64 loss: 2.2769975662231445
Batch 52/64 loss: 1.86124849319458
Batch 53/64 loss: 2.3597021102905273
Batch 54/64 loss: 2.315969944000244
Batch 55/64 loss: 2.184948444366455
Batch 56/64 loss: 2.16849422454834
Batch 57/64 loss: 1.9554309844970703
Batch 58/64 loss: 2.0985565185546875
Batch 59/64 loss: 2.244980812072754
Batch 60/64 loss: 2.070244312286377
Batch 61/64 loss: 2.8264894485473633
Batch 62/64 loss: 2.222926139831543
Batch 63/64 loss: 2.4054346084594727
Batch 64/64 loss: -0.1322789192199707
Epoch 10  Train loss: 2.3647509799284094  Val loss: 2.2681059427687393
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: 2.5118584632873535
Batch 2/64 loss: 2.2604575157165527
Batch 3/64 loss: 2.22605562210083
Batch 4/64 loss: 2.3114194869995117
Batch 5/64 loss: 2.401275634765625
Batch 6/64 loss: 2.3956475257873535
Batch 7/64 loss: 2.262655735015869
Batch 8/64 loss: 2.3730716705322266
Batch 9/64 loss: 2.054759979248047
Batch 10/64 loss: 2.451274871826172
Batch 11/64 loss: 2.223349094390869
Batch 12/64 loss: 2.124413013458252
Batch 13/64 loss: 2.022050380706787
Batch 14/64 loss: 2.205270290374756
Batch 15/64 loss: 2.40818452835083
Batch 16/64 loss: 1.8552913665771484
Batch 17/64 loss: 2.3416905403137207
Batch 18/64 loss: 2.4008188247680664
Batch 19/64 loss: 2.015657424926758
Batch 20/64 loss: 1.8260283470153809
Batch 21/64 loss: 2.2509799003601074
Batch 22/64 loss: 2.0272650718688965
Batch 23/64 loss: 2.140380382537842
Batch 24/64 loss: 2.2843451499938965
Batch 25/64 loss: 2.181636333465576
Batch 26/64 loss: 2.1048455238342285
Batch 27/64 loss: 2.135181427001953
Batch 28/64 loss: 2.1905364990234375
Batch 29/64 loss: 2.22471284866333
Batch 30/64 loss: 1.8216452598571777
Batch 31/64 loss: 1.8872780799865723
Batch 32/64 loss: 2.043092727661133
Batch 33/64 loss: 1.8875484466552734
Batch 34/64 loss: 1.95936918258667
Batch 35/64 loss: 2.095399856567383
Batch 36/64 loss: 2.2343430519104004
Batch 37/64 loss: 2.1061673164367676
Batch 38/64 loss: 2.2328591346740723
Batch 39/64 loss: 1.842684268951416
Batch 40/64 loss: 2.1699209213256836
Batch 41/64 loss: 2.1168723106384277
Batch 42/64 loss: 1.9115614891052246
Batch 43/64 loss: 1.9253511428833008
Batch 44/64 loss: 2.3283491134643555
Batch 45/64 loss: 2.4552950859069824
Batch 46/64 loss: 2.2318077087402344
Batch 47/64 loss: 2.079235553741455
Batch 48/64 loss: 2.265244483947754
Batch 49/64 loss: 2.0131630897521973
Batch 50/64 loss: 2.1789236068725586
Batch 51/64 loss: 2.119537830352783
Batch 52/64 loss: 1.926928997039795
Batch 53/64 loss: 1.9063310623168945
Batch 54/64 loss: 2.0607776641845703
Batch 55/64 loss: 1.776942253112793
Batch 56/64 loss: 2.3756723403930664
Batch 57/64 loss: 1.9692111015319824
Batch 58/64 loss: 2.4527249336242676
Batch 59/64 loss: 2.1375346183776855
Batch 60/64 loss: 1.8993830680847168
Batch 61/64 loss: 1.8929424285888672
Batch 62/64 loss: 2.0437378883361816
Batch 63/64 loss: 2.0292224884033203
Batch 64/64 loss: -0.9915494918823242
Epoch 11  Train loss: 2.0995217229805743  Val loss: 1.9112295301509477
Saving best model, epoch: 11
Epoch 12
-------------------------------
Batch 1/64 loss: 2.249253749847412
Batch 2/64 loss: 1.7817769050598145
Batch 3/64 loss: 2.0482635498046875
Batch 4/64 loss: 2.205923557281494
Batch 5/64 loss: 2.081068992614746
Batch 6/64 loss: 1.7383389472961426
Batch 7/64 loss: 2.279991626739502
Batch 8/64 loss: 1.9303112030029297
Batch 9/64 loss: 1.9268989562988281
Batch 10/64 loss: 2.842681884765625
Batch 11/64 loss: 1.7954068183898926
Batch 12/64 loss: 2.077577590942383
Batch 13/64 loss: 1.9572548866271973
Batch 14/64 loss: 2.1334991455078125
Batch 15/64 loss: 2.067225933074951
Batch 16/64 loss: 2.30130672454834
Batch 17/64 loss: 2.182084560394287
Batch 18/64 loss: 1.7908601760864258
Batch 19/64 loss: 2.247408390045166
Batch 20/64 loss: 1.884446144104004
Batch 21/64 loss: 2.0130600929260254
Batch 22/64 loss: 2.0864181518554688
Batch 23/64 loss: 1.8210201263427734
Batch 24/64 loss: 1.8182697296142578
Batch 25/64 loss: 1.7122688293457031
Batch 26/64 loss: 1.958592414855957
Batch 27/64 loss: 2.163139820098877
Batch 28/64 loss: 1.8559083938598633
Batch 29/64 loss: 1.4281682968139648
Batch 30/64 loss: 1.8376827239990234
Batch 31/64 loss: 2.748318672180176
Batch 32/64 loss: 1.878737449645996
Batch 33/64 loss: 1.6979680061340332
Batch 34/64 loss: 1.9981489181518555
Batch 35/64 loss: 2.2336554527282715
Batch 36/64 loss: 1.6764020919799805
Batch 37/64 loss: 2.1594371795654297
Batch 38/64 loss: 2.0017528533935547
Batch 39/64 loss: 2.2020182609558105
Batch 40/64 loss: 1.8307194709777832
Batch 41/64 loss: 2.3813047409057617
Batch 42/64 loss: 2.3311171531677246
Batch 43/64 loss: 2.0570993423461914
Batch 44/64 loss: 1.8557615280151367
Batch 45/64 loss: 2.094161033630371
Batch 46/64 loss: 1.6326384544372559
Batch 47/64 loss: 2.0401206016540527
Batch 48/64 loss: 1.7786498069763184
Batch 49/64 loss: 1.552079677581787
Batch 50/64 loss: 1.9924964904785156
Batch 51/64 loss: 2.1013078689575195
Batch 52/64 loss: 1.8006110191345215
Batch 53/64 loss: 2.1898856163024902
Batch 54/64 loss: 2.1074819564819336
Batch 55/64 loss: 1.9399943351745605
Batch 56/64 loss: 2.064779758453369
Batch 57/64 loss: 1.9341979026794434
Batch 58/64 loss: 1.6761693954467773
Batch 59/64 loss: 1.825904369354248
Batch 60/64 loss: 1.9689087867736816
Batch 61/64 loss: 2.212827205657959
Batch 62/64 loss: 1.5407967567443848
Batch 63/64 loss: 1.8996105194091797
Batch 64/64 loss: -0.9953417778015137
Epoch 12  Train loss: 1.9588182692434273  Val loss: 1.813835537310728
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 1.9787750244140625
Batch 2/64 loss: 1.8482050895690918
Batch 3/64 loss: 1.5631494522094727
Batch 4/64 loss: 2.2715253829956055
Batch 5/64 loss: 1.6677474975585938
Batch 6/64 loss: 2.0103087425231934
Batch 7/64 loss: 2.1569366455078125
Batch 8/64 loss: 1.9809308052062988
Batch 9/64 loss: 1.7930655479431152
Batch 10/64 loss: 2.0643200874328613
Batch 11/64 loss: 1.8795528411865234
Batch 12/64 loss: 2.0592169761657715
Batch 13/64 loss: 1.810286045074463
Batch 14/64 loss: 2.1560473442077637
Batch 15/64 loss: 1.6823515892028809
Batch 16/64 loss: 1.6185765266418457
Batch 17/64 loss: 2.020144462585449
Batch 18/64 loss: 1.8115777969360352
Batch 19/64 loss: 1.796921730041504
Batch 20/64 loss: 1.6439714431762695
Batch 21/64 loss: 1.8820910453796387
Batch 22/64 loss: 1.7081351280212402
Batch 23/64 loss: 1.5272326469421387
Batch 24/64 loss: 2.0123543739318848
Batch 25/64 loss: 1.6987180709838867
Batch 26/64 loss: 1.8532423973083496
Batch 27/64 loss: 1.4703335762023926
Batch 28/64 loss: 1.749995231628418
Batch 29/64 loss: 1.462416172027588
Batch 30/64 loss: 1.7995367050170898
Batch 31/64 loss: 1.9774179458618164
Batch 32/64 loss: 1.9975218772888184
Batch 33/64 loss: 1.8483829498291016
Batch 34/64 loss: 1.846120834350586
Batch 35/64 loss: 2.00266695022583
Batch 36/64 loss: 1.8780827522277832
Batch 37/64 loss: 1.6721034049987793
Batch 38/64 loss: 2.014803409576416
Batch 39/64 loss: 1.7616968154907227
Batch 40/64 loss: 1.8994479179382324
Batch 41/64 loss: 1.5548605918884277
Batch 42/64 loss: 2.116513729095459
Batch 43/64 loss: 1.6449565887451172
Batch 44/64 loss: 1.784304141998291
Batch 45/64 loss: 1.632441520690918
Batch 46/64 loss: 2.0813355445861816
Batch 47/64 loss: 1.9016165733337402
Batch 48/64 loss: 2.097182273864746
Batch 49/64 loss: 2.247559070587158
Batch 50/64 loss: 1.9067368507385254
Batch 51/64 loss: 1.8071990013122559
Batch 52/64 loss: 1.631664752960205
Batch 53/64 loss: 1.698258399963379
Batch 54/64 loss: 1.4943408966064453
Batch 55/64 loss: 1.5570740699768066
Batch 56/64 loss: 1.4754362106323242
Batch 57/64 loss: 2.209939956665039
Batch 58/64 loss: 1.786475658416748
Batch 59/64 loss: 1.468134880065918
Batch 60/64 loss: 1.6774544715881348
Batch 61/64 loss: 1.5681896209716797
Batch 62/64 loss: 1.5664172172546387
Batch 63/64 loss: 1.6825757026672363
Batch 64/64 loss: -1.3497567176818848
Epoch 13  Train loss: 1.7796746889750164  Val loss: 1.5813736735340655
Saving best model, epoch: 13
Epoch 14
-------------------------------
Batch 1/64 loss: 1.5249953269958496
Batch 2/64 loss: 1.6712641716003418
Batch 3/64 loss: 1.6281304359436035
Batch 4/64 loss: 1.7003493309020996
Batch 5/64 loss: 1.6066508293151855
Batch 6/64 loss: 1.8291516304016113
Batch 7/64 loss: 1.486271858215332
Batch 8/64 loss: 1.5413498878479004
Batch 9/64 loss: 2.0131893157958984
Batch 10/64 loss: 2.283356189727783
Batch 11/64 loss: 2.0833520889282227
Batch 12/64 loss: 2.1538333892822266
Batch 13/64 loss: 1.9056973457336426
Batch 14/64 loss: 1.775963306427002
Batch 15/64 loss: 1.9072246551513672
Batch 16/64 loss: 2.1419410705566406
Batch 17/64 loss: 1.9417691230773926
Batch 18/64 loss: 2.200359344482422
Batch 19/64 loss: 2.2107443809509277
Batch 20/64 loss: 2.5729832649230957
Batch 21/64 loss: 1.9762001037597656
Batch 22/64 loss: 1.851862907409668
Batch 23/64 loss: 2.016225814819336
Batch 24/64 loss: 1.7012453079223633
Batch 25/64 loss: 2.137272834777832
Batch 26/64 loss: 1.7448439598083496
Batch 27/64 loss: 1.715982437133789
Batch 28/64 loss: 1.8177227973937988
Batch 29/64 loss: 2.1097183227539062
Batch 30/64 loss: 1.7712030410766602
Batch 31/64 loss: 1.5679035186767578
Batch 32/64 loss: 1.487588882446289
Batch 33/64 loss: 1.804060459136963
Batch 34/64 loss: 1.8956847190856934
Batch 35/64 loss: 1.662282943725586
Batch 36/64 loss: 1.927938461303711
Batch 37/64 loss: 1.8435029983520508
Batch 38/64 loss: 1.6167082786560059
Batch 39/64 loss: 2.1052303314208984
Batch 40/64 loss: 2.0525197982788086
Batch 41/64 loss: 2.1074957847595215
Batch 42/64 loss: 1.898643970489502
Batch 43/64 loss: 2.199826240539551
Batch 44/64 loss: 1.6561274528503418
Batch 45/64 loss: 1.7359251976013184
Batch 46/64 loss: 1.6959519386291504
Batch 47/64 loss: 2.427412986755371
Batch 48/64 loss: 1.5647001266479492
Batch 49/64 loss: 1.6621427536010742
Batch 50/64 loss: 2.070739269256592
Batch 51/64 loss: 1.599543571472168
Batch 52/64 loss: 2.163466453552246
Batch 53/64 loss: 1.9421172142028809
Batch 54/64 loss: 1.84226655960083
Batch 55/64 loss: 1.6278038024902344
Batch 56/64 loss: 1.7693219184875488
Batch 57/64 loss: 1.4356622695922852
Batch 58/64 loss: 1.7650775909423828
Batch 59/64 loss: 2.2333450317382812
Batch 60/64 loss: 1.5671663284301758
Batch 61/64 loss: 1.3730649948120117
Batch 62/64 loss: 1.7310380935668945
Batch 63/64 loss: 1.777618408203125
Batch 64/64 loss: -1.438918113708496
Epoch 14  Train loss: 1.8157104978374405  Val loss: 1.5575100941346682
Saving best model, epoch: 14
Epoch 15
-------------------------------
Batch 1/64 loss: 1.7789406776428223
Batch 2/64 loss: 1.843991756439209
Batch 3/64 loss: 1.4510436058044434
Batch 4/64 loss: 1.4614486694335938
Batch 5/64 loss: 1.7910428047180176
Batch 6/64 loss: 1.7035126686096191
Batch 7/64 loss: 2.272881031036377
Batch 8/64 loss: 1.6440529823303223
Batch 9/64 loss: 1.730860710144043
Batch 10/64 loss: 1.6851019859313965
Batch 11/64 loss: 2.076719284057617
Batch 12/64 loss: 1.5006814002990723
Batch 13/64 loss: 1.784505844116211
Batch 14/64 loss: 1.8850774765014648
Batch 15/64 loss: 1.5183191299438477
Batch 16/64 loss: 1.6621198654174805
Batch 17/64 loss: 1.4841461181640625
Batch 18/64 loss: 1.7786917686462402
Batch 19/64 loss: 1.512939453125
Batch 20/64 loss: 1.4144892692565918
Batch 21/64 loss: 1.6509413719177246
Batch 22/64 loss: 1.3876752853393555
Batch 23/64 loss: 1.7670822143554688
Batch 24/64 loss: 1.7598199844360352
Batch 25/64 loss: 1.6482787132263184
Batch 26/64 loss: 1.442826271057129
Batch 27/64 loss: 1.5067152976989746
Batch 28/64 loss: 1.1793179512023926
Batch 29/64 loss: 1.248044490814209
Batch 30/64 loss: 1.8134546279907227
Batch 31/64 loss: 1.7415943145751953
Batch 32/64 loss: 2.3890562057495117
Batch 33/64 loss: 1.7401251792907715
Batch 34/64 loss: 1.6152105331420898
Batch 35/64 loss: 1.5771160125732422
Batch 36/64 loss: 1.6906938552856445
Batch 37/64 loss: 1.535097599029541
Batch 38/64 loss: 1.57706880569458
Batch 39/64 loss: 1.9402356147766113
Batch 40/64 loss: 1.6240777969360352
Batch 41/64 loss: 1.5500030517578125
Batch 42/64 loss: 2.275599956512451
Batch 43/64 loss: 1.468980312347412
Batch 44/64 loss: 1.3746728897094727
Batch 45/64 loss: 1.815535545349121
Batch 46/64 loss: 1.695173740386963
Batch 47/64 loss: 1.5932879447937012
Batch 48/64 loss: 1.381424903869629
Batch 49/64 loss: 1.591592788696289
Batch 50/64 loss: 1.645057201385498
Batch 51/64 loss: 1.4616026878356934
Batch 52/64 loss: 1.4178667068481445
Batch 53/64 loss: 2.260097026824951
Batch 54/64 loss: 1.4686546325683594
Batch 55/64 loss: 1.6396117210388184
Batch 56/64 loss: 1.566195011138916
Batch 57/64 loss: 2.251005172729492
Batch 58/64 loss: 1.6584405899047852
Batch 59/64 loss: 1.912595272064209
Batch 60/64 loss: 1.8129644393920898
Batch 61/64 loss: 1.553077220916748
Batch 62/64 loss: 1.6855363845825195
Batch 63/64 loss: 1.7806172370910645
Batch 64/64 loss: -0.8757400512695312
Epoch 15  Train loss: 1.6473378200157016  Val loss: 1.6090822318165572
Epoch 16
-------------------------------
Batch 1/64 loss: 1.162653923034668
Batch 2/64 loss: 1.7117137908935547
Batch 3/64 loss: 1.748682975769043
Batch 4/64 loss: 1.7176322937011719
Batch 5/64 loss: 1.7566075325012207
Batch 6/64 loss: 1.745553970336914
Batch 7/64 loss: 1.206965446472168
Batch 8/64 loss: 1.398697853088379
Batch 9/64 loss: 1.4489073753356934
Batch 10/64 loss: 1.500556468963623
Batch 11/64 loss: 1.904029369354248
Batch 12/64 loss: 1.5172243118286133
Batch 13/64 loss: 1.8150668144226074
Batch 14/64 loss: 1.4718165397644043
Batch 15/64 loss: 1.9354748725891113
Batch 16/64 loss: 1.6606597900390625
Batch 17/64 loss: 1.6619133949279785
Batch 18/64 loss: 1.3604974746704102
Batch 19/64 loss: 1.925124168395996
Batch 20/64 loss: 1.3678040504455566
Batch 21/64 loss: 1.590597152709961
Batch 22/64 loss: 1.6796913146972656
Batch 23/64 loss: 2.8995614051818848
Batch 24/64 loss: 1.4899702072143555
Batch 25/64 loss: 1.5132570266723633
Batch 26/64 loss: 2.3550071716308594
Batch 27/64 loss: 1.4247841835021973
Batch 28/64 loss: 1.376305103302002
Batch 29/64 loss: 1.7472472190856934
Batch 30/64 loss: 1.7370104789733887
Batch 31/64 loss: 1.798830509185791
Batch 32/64 loss: 1.40211820602417
Batch 33/64 loss: 1.6656198501586914
Batch 34/64 loss: 1.2625384330749512
Batch 35/64 loss: 1.778001308441162
Batch 36/64 loss: 1.578279972076416
Batch 37/64 loss: 1.7121853828430176
Batch 38/64 loss: 1.972449779510498
Batch 39/64 loss: 1.4107651710510254
Batch 40/64 loss: 1.4457306861877441
Batch 41/64 loss: 1.6359939575195312
Batch 42/64 loss: 1.1369023323059082
Batch 43/64 loss: 1.4172868728637695
Batch 44/64 loss: 1.5174269676208496
Batch 45/64 loss: 1.7670440673828125
Batch 46/64 loss: 1.4082117080688477
Batch 47/64 loss: 1.7760334014892578
Batch 48/64 loss: 1.2872824668884277
Batch 49/64 loss: 1.7096824645996094
Batch 50/64 loss: 1.4581832885742188
Batch 51/64 loss: 1.6743483543395996
Batch 52/64 loss: 1.3964571952819824
Batch 53/64 loss: 1.882951259613037
Batch 54/64 loss: 1.5718636512756348
Batch 55/64 loss: 1.5507826805114746
Batch 56/64 loss: 1.5069289207458496
Batch 57/64 loss: 1.7323546409606934
Batch 58/64 loss: 1.2546615600585938
Batch 59/64 loss: 1.892686367034912
Batch 60/64 loss: 1.4461555480957031
Batch 61/64 loss: 1.410261631011963
Batch 62/64 loss: 1.2145566940307617
Batch 63/64 loss: 1.5120654106140137
Batch 64/64 loss: -1.9393720626831055
Epoch 16  Train loss: 1.5617745156381644  Val loss: 1.4470162932405766
Saving best model, epoch: 16
Epoch 17
-------------------------------
Batch 1/64 loss: 1.5330376625061035
Batch 2/64 loss: 1.4015836715698242
Batch 3/64 loss: 1.2884821891784668
Batch 4/64 loss: 1.2574453353881836
Batch 5/64 loss: 1.5955109596252441
Batch 6/64 loss: 1.3090643882751465
Batch 7/64 loss: 1.1867070198059082
Batch 8/64 loss: 1.440159797668457
Batch 9/64 loss: 1.8145127296447754
Batch 10/64 loss: 1.383467674255371
Batch 11/64 loss: 1.296316146850586
Batch 12/64 loss: 1.500861644744873
Batch 13/64 loss: 1.3854780197143555
Batch 14/64 loss: 1.4290828704833984
Batch 15/64 loss: 1.6156401634216309
Batch 16/64 loss: 1.5813908576965332
Batch 17/64 loss: 1.6779484748840332
Batch 18/64 loss: 1.2759675979614258
Batch 19/64 loss: 1.282996654510498
Batch 20/64 loss: 1.4862842559814453
Batch 21/64 loss: 1.2313117980957031
Batch 22/64 loss: 1.3667564392089844
Batch 23/64 loss: 1.9357123374938965
Batch 24/64 loss: 1.8435721397399902
Batch 25/64 loss: 1.6484160423278809
Batch 26/64 loss: 1.4766926765441895
Batch 27/64 loss: 1.5032882690429688
Batch 28/64 loss: 1.6226587295532227
Batch 29/64 loss: 1.54805326461792
Batch 30/64 loss: 1.549032211303711
Batch 31/64 loss: 1.5327401161193848
Batch 32/64 loss: 1.7250661849975586
Batch 33/64 loss: 1.4669342041015625
Batch 34/64 loss: 1.6529831886291504
Batch 35/64 loss: 1.5351381301879883
Batch 36/64 loss: 1.6160082817077637
Batch 37/64 loss: 1.3344974517822266
Batch 38/64 loss: 1.2601656913757324
Batch 39/64 loss: 1.874666690826416
Batch 40/64 loss: 1.6620759963989258
Batch 41/64 loss: 1.8768296241760254
Batch 42/64 loss: 1.145737648010254
Batch 43/64 loss: 1.6080641746520996
Batch 44/64 loss: 1.278745174407959
Batch 45/64 loss: 1.3027448654174805
Batch 46/64 loss: 1.5372581481933594
Batch 47/64 loss: 1.438539981842041
Batch 48/64 loss: 0.949918270111084
Batch 49/64 loss: 1.6747727394104004
Batch 50/64 loss: 1.451716423034668
Batch 51/64 loss: 1.2745132446289062
Batch 52/64 loss: 1.2450852394104004
Batch 53/64 loss: 1.2179756164550781
Batch 54/64 loss: 1.3622822761535645
Batch 55/64 loss: 1.325627326965332
Batch 56/64 loss: 1.416050910949707
Batch 57/64 loss: 1.550832748413086
Batch 58/64 loss: 1.4139938354492188
Batch 59/64 loss: 1.4861726760864258
Batch 60/64 loss: 1.2644023895263672
Batch 61/64 loss: 1.7750468254089355
Batch 62/64 loss: 1.5744743347167969
Batch 63/64 loss: 1.3555259704589844
Batch 64/64 loss: -1.6328401565551758
Epoch 17  Train loss: 1.434186482896992  Val loss: 1.3929187473152922
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 1.1606478691101074
Batch 2/64 loss: 1.5105252265930176
Batch 3/64 loss: 1.7715435028076172
Batch 4/64 loss: 1.256115436553955
Batch 5/64 loss: 1.791193962097168
Batch 6/64 loss: 1.243619441986084
Batch 7/64 loss: 1.2238554954528809
Batch 8/64 loss: 1.4091253280639648
Batch 9/64 loss: 1.3277602195739746
Batch 10/64 loss: 1.80548095703125
Batch 11/64 loss: 1.7289118766784668
Batch 12/64 loss: 1.292271614074707
Batch 13/64 loss: 1.1508278846740723
Batch 14/64 loss: 1.1786751747131348
Batch 15/64 loss: 1.322636604309082
Batch 16/64 loss: 1.7179718017578125
Batch 17/64 loss: 1.2927045822143555
Batch 18/64 loss: 1.695152759552002
Batch 19/64 loss: 1.9100313186645508
Batch 20/64 loss: 1.336348056793213
Batch 21/64 loss: 1.579707145690918
Batch 22/64 loss: 2.3372950553894043
Batch 23/64 loss: 1.7201552391052246
Batch 24/64 loss: 1.2203688621520996
Batch 25/64 loss: 1.1634807586669922
Batch 26/64 loss: 1.7634000778198242
Batch 27/64 loss: 1.4302620887756348
Batch 28/64 loss: 1.5657219886779785
Batch 29/64 loss: 1.3829317092895508
Batch 30/64 loss: 1.1742215156555176
Batch 31/64 loss: 1.421102523803711
Batch 32/64 loss: 1.5497512817382812
Batch 33/64 loss: 1.629054069519043
Batch 34/64 loss: 1.2740421295166016
Batch 35/64 loss: 1.3762001991271973
Batch 36/64 loss: 1.487722396850586
Batch 37/64 loss: 1.4981794357299805
Batch 38/64 loss: 1.9031529426574707
Batch 39/64 loss: 1.5178399085998535
Batch 40/64 loss: 1.482182502746582
Batch 41/64 loss: 1.3266124725341797
Batch 42/64 loss: 2.1362791061401367
Batch 43/64 loss: 1.520970344543457
Batch 44/64 loss: 1.109057903289795
Batch 45/64 loss: 1.2555923461914062
Batch 46/64 loss: 1.586207389831543
Batch 47/64 loss: 1.5802826881408691
Batch 48/64 loss: 1.475290298461914
Batch 49/64 loss: 1.4140205383300781
Batch 50/64 loss: 1.0971088409423828
Batch 51/64 loss: 1.397414207458496
Batch 52/64 loss: 0.9720077514648438
Batch 53/64 loss: 1.0752134323120117
Batch 54/64 loss: 1.2377772331237793
Batch 55/64 loss: 1.4170665740966797
Batch 56/64 loss: 1.492408275604248
Batch 57/64 loss: 0.9986581802368164
Batch 58/64 loss: 1.394144058227539
Batch 59/64 loss: 1.341501235961914
Batch 60/64 loss: 1.7498235702514648
Batch 61/64 loss: 1.3615260124206543
Batch 62/64 loss: 1.3272428512573242
Batch 63/64 loss: 1.075364112854004
Batch 64/64 loss: -1.8889679908752441
Epoch 18  Train loss: 1.404345323525223  Val loss: 1.3399781230389047
Saving best model, epoch: 18
Epoch 19
-------------------------------
Batch 1/64 loss: 1.1979432106018066
Batch 2/64 loss: 1.6141037940979004
Batch 3/64 loss: 1.218787670135498
Batch 4/64 loss: 1.4042181968688965
Batch 5/64 loss: 1.027055263519287
Batch 6/64 loss: 1.205751895904541
Batch 7/64 loss: 1.363865852355957
Batch 8/64 loss: 1.740708351135254
Batch 9/64 loss: 1.2692604064941406
Batch 10/64 loss: 1.9736542701721191
Batch 11/64 loss: 1.227689266204834
Batch 12/64 loss: 0.9990935325622559
Batch 13/64 loss: 1.7362399101257324
Batch 14/64 loss: 1.6196331977844238
Batch 15/64 loss: 1.7507948875427246
Batch 16/64 loss: 1.1242904663085938
Batch 17/64 loss: 1.5045738220214844
Batch 18/64 loss: 1.3545336723327637
Batch 19/64 loss: 1.3689932823181152
Batch 20/64 loss: 1.2819571495056152
Batch 21/64 loss: 1.3055062294006348
Batch 22/64 loss: 1.218885898590088
Batch 23/64 loss: 1.6324520111083984
Batch 24/64 loss: 1.20426607131958
Batch 25/64 loss: 1.3421435356140137
Batch 26/64 loss: 1.5712313652038574
Batch 27/64 loss: 1.2880849838256836
Batch 28/64 loss: 1.6292915344238281
Batch 29/64 loss: 1.3027796745300293
Batch 30/64 loss: 1.4047784805297852
Batch 31/64 loss: 1.3165254592895508
Batch 32/64 loss: 1.4494295120239258
Batch 33/64 loss: 1.0351662635803223
Batch 34/64 loss: 1.0184931755065918
Batch 35/64 loss: 1.075906753540039
Batch 36/64 loss: 0.9833784103393555
Batch 37/64 loss: 1.4997773170471191
Batch 38/64 loss: 1.112877368927002
Batch 39/64 loss: 1.4429497718811035
Batch 40/64 loss: 1.2628960609436035
Batch 41/64 loss: 1.429417610168457
Batch 42/64 loss: 1.5160531997680664
Batch 43/64 loss: 1.0946526527404785
Batch 44/64 loss: 1.3958778381347656
Batch 45/64 loss: 1.0071792602539062
Batch 46/64 loss: 1.2755780220031738
Batch 47/64 loss: 1.27030611038208
Batch 48/64 loss: 1.7038030624389648
Batch 49/64 loss: 1.445690631866455
Batch 50/64 loss: 1.0801968574523926
Batch 51/64 loss: 1.3770675659179688
Batch 52/64 loss: 1.199206829071045
Batch 53/64 loss: 1.2006840705871582
Batch 54/64 loss: 1.0729784965515137
Batch 55/64 loss: 1.3721542358398438
Batch 56/64 loss: 1.176602840423584
Batch 57/64 loss: 1.296285629272461
Batch 58/64 loss: 1.6127309799194336
Batch 59/64 loss: 1.3448123931884766
Batch 60/64 loss: 1.6441068649291992
Batch 61/64 loss: 0.9285793304443359
Batch 62/64 loss: 1.238774299621582
Batch 63/64 loss: 1.380115032196045
Batch 64/64 loss: -1.6565122604370117
Epoch 19  Train loss: 1.3003990210738836  Val loss: 1.1470370079643537
Saving best model, epoch: 19
Epoch 20
-------------------------------
Batch 1/64 loss: 1.1073989868164062
Batch 2/64 loss: 1.1669816970825195
Batch 3/64 loss: 1.3758339881896973
Batch 4/64 loss: 1.2285723686218262
Batch 5/64 loss: 1.3702707290649414
Batch 6/64 loss: 1.244431972503662
Batch 7/64 loss: 1.3371105194091797
Batch 8/64 loss: 1.3412408828735352
Batch 9/64 loss: 1.7549090385437012
Batch 10/64 loss: 1.2056694030761719
Batch 11/64 loss: 1.3395614624023438
Batch 12/64 loss: 1.45808744430542
Batch 13/64 loss: 1.375411033630371
Batch 14/64 loss: 1.0912518501281738
Batch 15/64 loss: 0.9100041389465332
Batch 16/64 loss: 1.5839385986328125
Batch 17/64 loss: 1.1492576599121094
Batch 18/64 loss: 1.4279179573059082
Batch 19/64 loss: 1.714062213897705
Batch 20/64 loss: 1.430436611175537
Batch 21/64 loss: 1.2870001792907715
Batch 22/64 loss: 1.043745994567871
Batch 23/64 loss: 1.2577872276306152
Batch 24/64 loss: 1.4354100227355957
Batch 25/64 loss: 1.2444109916687012
Batch 26/64 loss: 1.3458480834960938
Batch 27/64 loss: 1.1956162452697754
Batch 28/64 loss: 1.3002080917358398
Batch 29/64 loss: 1.2077102661132812
Batch 30/64 loss: 1.2393808364868164
Batch 31/64 loss: 1.579312801361084
Batch 32/64 loss: 1.1091089248657227
Batch 33/64 loss: 1.1937932968139648
Batch 34/64 loss: 1.1768221855163574
Batch 35/64 loss: 1.2419428825378418
Batch 36/64 loss: 1.6282505989074707
Batch 37/64 loss: 1.0956883430480957
Batch 38/64 loss: 1.259521484375
Batch 39/64 loss: 1.9200339317321777
Batch 40/64 loss: 1.492856502532959
Batch 41/64 loss: 1.3330650329589844
Batch 42/64 loss: 1.5972766876220703
Batch 43/64 loss: 1.5985231399536133
Batch 44/64 loss: 1.1093707084655762
Batch 45/64 loss: 1.357015609741211
Batch 46/64 loss: 1.2888026237487793
Batch 47/64 loss: 1.527205467224121
Batch 48/64 loss: 1.2247943878173828
Batch 49/64 loss: 1.2571163177490234
Batch 50/64 loss: 1.2616939544677734
Batch 51/64 loss: 1.6297578811645508
Batch 52/64 loss: 1.484055995941162
Batch 53/64 loss: 1.4997034072875977
Batch 54/64 loss: 1.5309739112854004
Batch 55/64 loss: 1.523728370666504
Batch 56/64 loss: 1.653672695159912
Batch 57/64 loss: 1.2959113121032715
Batch 58/64 loss: 1.408442497253418
Batch 59/64 loss: 1.6803665161132812
Batch 60/64 loss: 1.282545566558838
Batch 61/64 loss: 1.2961602210998535
Batch 62/64 loss: 0.9261598587036133
Batch 63/64 loss: 1.4316463470458984
Batch 64/64 loss: -2.2578282356262207
Epoch 20  Train loss: 1.3077868985194785  Val loss: 1.1702494604890699
Epoch 21
-------------------------------
Batch 1/64 loss: 1.1279897689819336
Batch 2/64 loss: 1.6133031845092773
Batch 3/64 loss: 1.1016902923583984
Batch 4/64 loss: 1.1651325225830078
Batch 5/64 loss: 0.9779281616210938
Batch 6/64 loss: 1.4978632926940918
Batch 7/64 loss: 1.1744728088378906
Batch 8/64 loss: 1.0858798027038574
Batch 9/64 loss: 1.5552620887756348
Batch 10/64 loss: 1.3168683052062988
Batch 11/64 loss: 1.0487232208251953
Batch 12/64 loss: 1.2362275123596191
Batch 13/64 loss: 2.0851664543151855
Batch 14/64 loss: 1.598909854888916
Batch 15/64 loss: 1.5047416687011719
Batch 16/64 loss: 1.619781494140625
Batch 17/64 loss: 1.3869528770446777
Batch 18/64 loss: 1.1628093719482422
Batch 19/64 loss: 1.2976670265197754
Batch 20/64 loss: 1.2980804443359375
Batch 21/64 loss: 1.2953801155090332
Batch 22/64 loss: 3.159236431121826
Batch 23/64 loss: 1.6111316680908203
Batch 24/64 loss: 1.7646245956420898
Batch 25/64 loss: 1.8982439041137695
Batch 26/64 loss: 1.5843524932861328
Batch 27/64 loss: 1.6101346015930176
Batch 28/64 loss: 1.5445451736450195
Batch 29/64 loss: 1.9753398895263672
Batch 30/64 loss: 1.959291934967041
Batch 31/64 loss: 1.8592877388000488
Batch 32/64 loss: 1.606180191040039
Batch 33/64 loss: 1.3802919387817383
Batch 34/64 loss: 1.9435944557189941
Batch 35/64 loss: 1.8441963195800781
Batch 36/64 loss: 1.4954442977905273
Batch 37/64 loss: 1.9058585166931152
Batch 38/64 loss: 1.5972933769226074
Batch 39/64 loss: 1.6129150390625
Batch 40/64 loss: 1.3732562065124512
Batch 41/64 loss: 1.4943113327026367
Batch 42/64 loss: 1.7487239837646484
Batch 43/64 loss: 1.4016375541687012
Batch 44/64 loss: 1.3813142776489258
Batch 45/64 loss: 1.6637935638427734
Batch 46/64 loss: 1.6645069122314453
Batch 47/64 loss: 1.3190159797668457
Batch 48/64 loss: 1.6009068489074707
Batch 49/64 loss: 1.6151080131530762
Batch 50/64 loss: 1.8368511199951172
Batch 51/64 loss: 1.4522719383239746
Batch 52/64 loss: 1.5100927352905273
Batch 53/64 loss: 1.5265922546386719
Batch 54/64 loss: 1.798875331878662
Batch 55/64 loss: 1.5136876106262207
Batch 56/64 loss: 1.6695470809936523
Batch 57/64 loss: 1.3580060005187988
Batch 58/64 loss: 1.1777901649475098
Batch 59/64 loss: 1.2175192832946777
Batch 60/64 loss: 1.7452402114868164
Batch 61/64 loss: 1.7858061790466309
Batch 62/64 loss: 1.4375715255737305
Batch 63/64 loss: 1.52524995803833
Batch 64/64 loss: -1.545668125152588
Epoch 21  Train loss: 1.5084112597446815  Val loss: 1.3736719937668633
Epoch 22
-------------------------------
Batch 1/64 loss: 1.7004594802856445
Batch 2/64 loss: 1.4553122520446777
Batch 3/64 loss: 1.5591144561767578
Batch 4/64 loss: 1.3758625984191895
Batch 5/64 loss: 1.6422910690307617
Batch 6/64 loss: 1.616816520690918
Batch 7/64 loss: 1.5400638580322266
Batch 8/64 loss: 1.3036599159240723
Batch 9/64 loss: 1.589235782623291
Batch 10/64 loss: 1.2631902694702148
Batch 11/64 loss: 1.1249933242797852
Batch 12/64 loss: 1.567601203918457
Batch 13/64 loss: 1.2820119857788086
Batch 14/64 loss: 1.655080795288086
Batch 15/64 loss: 1.420541763305664
Batch 16/64 loss: 1.1236591339111328
Batch 17/64 loss: 1.1856207847595215
Batch 18/64 loss: 1.4634037017822266
Batch 19/64 loss: 1.4272351264953613
Batch 20/64 loss: 1.1077680587768555
Batch 21/64 loss: 1.3582897186279297
Batch 22/64 loss: 1.5440220832824707
Batch 23/64 loss: 1.3070769309997559
Batch 24/64 loss: 1.5083208084106445
Batch 25/64 loss: 1.1726832389831543
Batch 26/64 loss: 1.3439879417419434
Batch 27/64 loss: 1.0191059112548828
Batch 28/64 loss: 1.5037884712219238
Batch 29/64 loss: 1.5161948204040527
Batch 30/64 loss: 1.171048641204834
Batch 31/64 loss: 1.2058887481689453
Batch 32/64 loss: 1.180262565612793
Batch 33/64 loss: 1.104783535003662
Batch 34/64 loss: 1.2808103561401367
Batch 35/64 loss: 1.1431260108947754
Batch 36/64 loss: 1.6271109580993652
Batch 37/64 loss: 1.4606990814208984
Batch 38/64 loss: 1.2354974746704102
Batch 39/64 loss: 1.1598567962646484
Batch 40/64 loss: 1.3788352012634277
Batch 41/64 loss: 1.6515603065490723
Batch 42/64 loss: 1.3858966827392578
Batch 43/64 loss: 0.8778810501098633
Batch 44/64 loss: 1.2309927940368652
Batch 45/64 loss: 1.8984088897705078
Batch 46/64 loss: 1.003410816192627
Batch 47/64 loss: 1.2033371925354004
Batch 48/64 loss: 1.2257494926452637
Batch 49/64 loss: 1.02000093460083
Batch 50/64 loss: 1.3248538970947266
Batch 51/64 loss: 1.2316946983337402
Batch 52/64 loss: 1.3018403053283691
Batch 53/64 loss: 1.787053108215332
Batch 54/64 loss: 1.1656365394592285
Batch 55/64 loss: 1.0949077606201172
Batch 56/64 loss: 1.1821928024291992
Batch 57/64 loss: 1.114518642425537
Batch 58/64 loss: 1.101335048675537
Batch 59/64 loss: 0.9061408042907715
Batch 60/64 loss: 1.5154523849487305
Batch 61/64 loss: 1.2748403549194336
Batch 62/64 loss: 0.9790854454040527
Batch 63/64 loss: 0.8256325721740723
Batch 64/64 loss: -2.0438013076782227
Epoch 22  Train loss: 1.2767824771357517  Val loss: 1.0479398511119724
Saving best model, epoch: 22
Epoch 23
-------------------------------
Batch 1/64 loss: 1.165541648864746
Batch 2/64 loss: 1.2725000381469727
Batch 3/64 loss: 1.1078801155090332
Batch 4/64 loss: 1.2641816139221191
Batch 5/64 loss: 0.957939624786377
Batch 6/64 loss: 0.8208928108215332
Batch 7/64 loss: 1.1866707801818848
Batch 8/64 loss: 1.0144824981689453
Batch 9/64 loss: 1.1513080596923828
Batch 10/64 loss: 1.436269760131836
Batch 11/64 loss: 0.9840497970581055
Batch 12/64 loss: 1.6008853912353516
Batch 13/64 loss: 1.1486058235168457
Batch 14/64 loss: 1.4768342971801758
Batch 15/64 loss: 1.165112018585205
Batch 16/64 loss: 1.0490713119506836
Batch 17/64 loss: 1.1406373977661133
Batch 18/64 loss: 1.3358039855957031
Batch 19/64 loss: 1.1155471801757812
Batch 20/64 loss: 1.1661381721496582
Batch 21/64 loss: 0.7431950569152832
Batch 22/64 loss: 0.6932797431945801
Batch 23/64 loss: 1.1957979202270508
Batch 24/64 loss: 1.2186741828918457
Batch 25/64 loss: 1.248365879058838
Batch 26/64 loss: 1.115023136138916
Batch 27/64 loss: 1.5373921394348145
Batch 28/64 loss: 1.3126916885375977
Batch 29/64 loss: 1.4330596923828125
Batch 30/64 loss: 1.195796012878418
Batch 31/64 loss: 1.209310531616211
Batch 32/64 loss: 1.372878074645996
Batch 33/64 loss: 1.1676826477050781
Batch 34/64 loss: 1.4014720916748047
Batch 35/64 loss: 1.7060155868530273
Batch 36/64 loss: 1.4929194450378418
Batch 37/64 loss: 1.3819127082824707
Batch 38/64 loss: 1.2123894691467285
Batch 39/64 loss: 1.6097726821899414
Batch 40/64 loss: 1.529801368713379
Batch 41/64 loss: 1.4613709449768066
Batch 42/64 loss: 1.0911664962768555
Batch 43/64 loss: 1.0027599334716797
Batch 44/64 loss: 0.8618645668029785
Batch 45/64 loss: 0.9510655403137207
Batch 46/64 loss: 1.103780746459961
Batch 47/64 loss: 1.2259206771850586
Batch 48/64 loss: 1.1762723922729492
Batch 49/64 loss: 0.8006715774536133
Batch 50/64 loss: 1.4221067428588867
Batch 51/64 loss: 1.5317397117614746
Batch 52/64 loss: 1.8098416328430176
Batch 53/64 loss: 1.6460070610046387
Batch 54/64 loss: 1.0711641311645508
Batch 55/64 loss: 1.1383814811706543
Batch 56/64 loss: 1.2343621253967285
Batch 57/64 loss: 1.1150717735290527
Batch 58/64 loss: 1.6603202819824219
Batch 59/64 loss: 1.0171499252319336
Batch 60/64 loss: 1.0725007057189941
Batch 61/64 loss: 1.288116455078125
Batch 62/64 loss: 1.2897229194641113
Batch 63/64 loss: 1.373774528503418
Batch 64/64 loss: -1.8551592826843262
Epoch 23  Train loss: 1.1967301219117408  Val loss: 1.1064630816482597
Epoch 24
-------------------------------
Batch 1/64 loss: 1.1860785484313965
Batch 2/64 loss: 1.5888056755065918
Batch 3/64 loss: 1.176130771636963
Batch 4/64 loss: 0.8567256927490234
Batch 5/64 loss: 1.1292905807495117
Batch 6/64 loss: 1.07020902633667
Batch 7/64 loss: 1.5261473655700684
Batch 8/64 loss: 1.0962576866149902
Batch 9/64 loss: 0.6931548118591309
Batch 10/64 loss: 1.259702205657959
Batch 11/64 loss: 1.0324273109436035
Batch 12/64 loss: 1.551072120666504
Batch 13/64 loss: 1.215104103088379
Batch 14/64 loss: 1.1021571159362793
Batch 15/64 loss: 1.151768684387207
Batch 16/64 loss: 1.0684008598327637
Batch 17/64 loss: 0.8990359306335449
Batch 18/64 loss: 1.3790745735168457
Batch 19/64 loss: 0.8196902275085449
Batch 20/64 loss: 1.1539535522460938
Batch 21/64 loss: 1.1145524978637695
Batch 22/64 loss: 1.034665584564209
Batch 23/64 loss: 1.3452143669128418
Batch 24/64 loss: 1.281628131866455
Batch 25/64 loss: 0.8905801773071289
Batch 26/64 loss: 1.425358772277832
Batch 27/64 loss: 0.9951939582824707
Batch 28/64 loss: 0.8817400932312012
Batch 29/64 loss: 1.2099008560180664
Batch 30/64 loss: 1.5547857284545898
Batch 31/64 loss: 1.3394098281860352
Batch 32/64 loss: 1.4591574668884277
Batch 33/64 loss: 1.271195411682129
Batch 34/64 loss: 0.802706241607666
Batch 35/64 loss: 1.6693201065063477
Batch 36/64 loss: 1.2201251983642578
Batch 37/64 loss: 1.320509910583496
Batch 38/64 loss: 0.6583294868469238
Batch 39/64 loss: 1.0482115745544434
Batch 40/64 loss: 1.3277349472045898
Batch 41/64 loss: 0.9746532440185547
Batch 42/64 loss: 0.968813419342041
Batch 43/64 loss: 1.1037802696228027
Batch 44/64 loss: 0.8780694007873535
Batch 45/64 loss: 0.8688416481018066
Batch 46/64 loss: 1.6182241439819336
Batch 47/64 loss: 1.1752781867980957
Batch 48/64 loss: 0.8982577323913574
Batch 49/64 loss: 1.0959668159484863
Batch 50/64 loss: 1.6855859756469727
Batch 51/64 loss: 0.8601055145263672
Batch 52/64 loss: 1.053347110748291
Batch 53/64 loss: 0.9267082214355469
Batch 54/64 loss: 1.1093192100524902
Batch 55/64 loss: 1.0000004768371582
Batch 56/64 loss: 0.9375185966491699
Batch 57/64 loss: 1.009225845336914
Batch 58/64 loss: 1.055060863494873
Batch 59/64 loss: 1.0301833152770996
Batch 60/64 loss: 2.003702163696289
Batch 61/64 loss: 1.118903636932373
Batch 62/64 loss: 0.9261984825134277
Batch 63/64 loss: 1.306276798248291
Batch 64/64 loss: -1.9234070777893066
Epoch 24  Train loss: 1.1132074187783634  Val loss: 1.0084315231165935
Saving best model, epoch: 24
Epoch 25
-------------------------------
Batch 1/64 loss: 0.9357624053955078
Batch 2/64 loss: 0.8858213424682617
Batch 3/64 loss: 1.1095829010009766
Batch 4/64 loss: 0.8647975921630859
Batch 5/64 loss: 1.0487432479858398
Batch 6/64 loss: 1.5146450996398926
Batch 7/64 loss: 1.1126327514648438
Batch 8/64 loss: 0.9705300331115723
Batch 9/64 loss: 0.8532729148864746
Batch 10/64 loss: 1.2466630935668945
Batch 11/64 loss: 1.4127225875854492
Batch 12/64 loss: 0.8952608108520508
Batch 13/64 loss: 0.543215274810791
Batch 14/64 loss: 1.239837646484375
Batch 15/64 loss: 1.435410976409912
Batch 16/64 loss: 1.282851219177246
Batch 17/64 loss: 1.3271327018737793
Batch 18/64 loss: 1.1024069786071777
Batch 19/64 loss: 1.1630682945251465
Batch 20/64 loss: 1.5009651184082031
Batch 21/64 loss: 1.0058584213256836
Batch 22/64 loss: 1.3285140991210938
Batch 23/64 loss: 1.3252363204956055
Batch 24/64 loss: 0.7164969444274902
Batch 25/64 loss: 0.8970150947570801
Batch 26/64 loss: 0.8301253318786621
Batch 27/64 loss: 1.1529545783996582
Batch 28/64 loss: 1.3540005683898926
Batch 29/64 loss: 1.2563037872314453
Batch 30/64 loss: 1.2707815170288086
Batch 31/64 loss: 1.0848278999328613
Batch 32/64 loss: 1.1971206665039062
Batch 33/64 loss: 0.9067144393920898
Batch 34/64 loss: 1.279308795928955
Batch 35/64 loss: 1.040520191192627
Batch 36/64 loss: 0.9425487518310547
Batch 37/64 loss: 1.4482135772705078
Batch 38/64 loss: 1.0369019508361816
Batch 39/64 loss: 1.1475329399108887
Batch 40/64 loss: 1.0141997337341309
Batch 41/64 loss: 1.3826956748962402
Batch 42/64 loss: 1.2906923294067383
Batch 43/64 loss: 1.2463302612304688
Batch 44/64 loss: 0.6046309471130371
Batch 45/64 loss: 1.3484129905700684
Batch 46/64 loss: 0.9444212913513184
Batch 47/64 loss: 1.3872075080871582
Batch 48/64 loss: 1.1877217292785645
Batch 49/64 loss: 1.275092601776123
Batch 50/64 loss: 1.4786152839660645
Batch 51/64 loss: 1.1894984245300293
Batch 52/64 loss: 1.2598271369934082
Batch 53/64 loss: 1.1868152618408203
Batch 54/64 loss: 1.1788697242736816
Batch 55/64 loss: 1.4755306243896484
Batch 56/64 loss: 1.0854487419128418
Batch 57/64 loss: 1.3741326332092285
Batch 58/64 loss: 0.6701846122741699
Batch 59/64 loss: 0.8844456672668457
Batch 60/64 loss: 1.141653060913086
Batch 61/64 loss: 0.853053092956543
Batch 62/64 loss: 1.0654077529907227
Batch 63/64 loss: 1.2634129524230957
Batch 64/64 loss: -2.464282989501953
Epoch 25  Train loss: 1.0918648888083065  Val loss: 1.0522213572079373
Epoch 26
-------------------------------
Batch 1/64 loss: 0.8869509696960449
Batch 2/64 loss: 1.0846672058105469
Batch 3/64 loss: 0.9985809326171875
Batch 4/64 loss: 0.8463711738586426
Batch 5/64 loss: 1.2805166244506836
Batch 6/64 loss: 1.2041816711425781
Batch 7/64 loss: 1.0211920738220215
Batch 8/64 loss: 0.685966968536377
Batch 9/64 loss: 1.0428972244262695
Batch 10/64 loss: 1.2504639625549316
Batch 11/64 loss: 1.3146305084228516
Batch 12/64 loss: 1.2367801666259766
Batch 13/64 loss: 1.0453948974609375
Batch 14/64 loss: 0.8853864669799805
Batch 15/64 loss: 0.6891603469848633
Batch 16/64 loss: 0.9972920417785645
Batch 17/64 loss: 1.1308436393737793
Batch 18/64 loss: 0.8762021064758301
Batch 19/64 loss: 1.0331745147705078
Batch 20/64 loss: 1.2718358039855957
Batch 21/64 loss: 1.1757359504699707
Batch 22/64 loss: 0.8869190216064453
Batch 23/64 loss: 1.2657933235168457
Batch 24/64 loss: 1.0142149925231934
Batch 25/64 loss: 0.9429750442504883
Batch 26/64 loss: 1.475111484527588
Batch 27/64 loss: 1.281167984008789
Batch 28/64 loss: 1.3988237380981445
Batch 29/64 loss: 0.9933404922485352
Batch 30/64 loss: 1.0972752571105957
Batch 31/64 loss: 1.3251557350158691
Batch 32/64 loss: 0.9032163619995117
Batch 33/64 loss: 1.5386114120483398
Batch 34/64 loss: 1.1170129776000977
Batch 35/64 loss: 1.0531673431396484
Batch 36/64 loss: 1.4274563789367676
Batch 37/64 loss: 1.1364655494689941
Batch 38/64 loss: 1.2418146133422852
Batch 39/64 loss: 0.8558969497680664
Batch 40/64 loss: 1.0225486755371094
Batch 41/64 loss: 1.1652979850769043
Batch 42/64 loss: 1.0352063179016113
Batch 43/64 loss: 1.6189045906066895
Batch 44/64 loss: 1.1213760375976562
Batch 45/64 loss: 1.3432445526123047
Batch 46/64 loss: 0.9816908836364746
Batch 47/64 loss: 0.8221302032470703
Batch 48/64 loss: 1.111839771270752
Batch 49/64 loss: 0.9869270324707031
Batch 50/64 loss: 1.1136484146118164
Batch 51/64 loss: 1.4299921989440918
Batch 52/64 loss: 1.0585145950317383
Batch 53/64 loss: 1.5699548721313477
Batch 54/64 loss: 1.3054471015930176
Batch 55/64 loss: 0.9655070304870605
Batch 56/64 loss: 0.9213724136352539
Batch 57/64 loss: 1.1562700271606445
Batch 58/64 loss: 1.456207275390625
Batch 59/64 loss: 0.6327376365661621
Batch 60/64 loss: 0.8670201301574707
Batch 61/64 loss: 1.4923372268676758
Batch 62/64 loss: 1.2027459144592285
Batch 63/64 loss: 0.9439811706542969
Batch 64/64 loss: -1.9398431777954102
Epoch 26  Train loss: 1.0789437424902824  Val loss: 0.8828499456451521
Saving best model, epoch: 26
Epoch 27
-------------------------------
Batch 1/64 loss: 0.8349905014038086
Batch 2/64 loss: 0.8737950325012207
Batch 3/64 loss: 1.0367956161499023
Batch 4/64 loss: 1.2139005661010742
Batch 5/64 loss: 0.761817455291748
Batch 6/64 loss: 1.0947661399841309
Batch 7/64 loss: 1.2639265060424805
Batch 8/64 loss: 1.0775413513183594
Batch 9/64 loss: 0.8837227821350098
Batch 10/64 loss: 0.7450928688049316
Batch 11/64 loss: 1.1551189422607422
Batch 12/64 loss: 1.1918725967407227
Batch 13/64 loss: 1.1207866668701172
Batch 14/64 loss: 0.9896512031555176
Batch 15/64 loss: 1.2138724327087402
Batch 16/64 loss: 0.7916722297668457
Batch 17/64 loss: 1.1386628150939941
Batch 18/64 loss: 1.1860079765319824
Batch 19/64 loss: 1.2771310806274414
Batch 20/64 loss: 0.7149429321289062
Batch 21/64 loss: 0.8900742530822754
Batch 22/64 loss: 1.5042009353637695
Batch 23/64 loss: 0.9827814102172852
Batch 24/64 loss: 0.8633971214294434
Batch 25/64 loss: 1.595552921295166
Batch 26/64 loss: 0.9578337669372559
Batch 27/64 loss: 0.816535472869873
Batch 28/64 loss: 0.8933863639831543
Batch 29/64 loss: 1.254438877105713
Batch 30/64 loss: 1.2176127433776855
Batch 31/64 loss: 0.9664034843444824
Batch 32/64 loss: 1.0869202613830566
Batch 33/64 loss: 1.0968174934387207
Batch 34/64 loss: 0.9946880340576172
Batch 35/64 loss: 1.2034974098205566
Batch 36/64 loss: 1.4345660209655762
Batch 37/64 loss: 0.8167014122009277
Batch 38/64 loss: 1.0446853637695312
Batch 39/64 loss: 0.49378252029418945
Batch 40/64 loss: 1.366140365600586
Batch 41/64 loss: 1.1938533782958984
Batch 42/64 loss: 0.9703712463378906
Batch 43/64 loss: 0.775881290435791
Batch 44/64 loss: 0.8645362854003906
Batch 45/64 loss: 0.8996434211730957
Batch 46/64 loss: 0.6822528839111328
Batch 47/64 loss: 0.9386024475097656
Batch 48/64 loss: 1.4136443138122559
Batch 49/64 loss: 1.0515680313110352
Batch 50/64 loss: 1.1728839874267578
Batch 51/64 loss: 1.2972025871276855
Batch 52/64 loss: 1.1733760833740234
Batch 53/64 loss: 1.0718564987182617
Batch 54/64 loss: 1.136399745941162
Batch 55/64 loss: 1.4863824844360352
Batch 56/64 loss: 1.33735990524292
Batch 57/64 loss: 0.6943097114562988
Batch 58/64 loss: 1.061906337738037
Batch 59/64 loss: 0.8551120758056641
Batch 60/64 loss: 0.9779887199401855
Batch 61/64 loss: 0.9724230766296387
Batch 62/64 loss: 1.0921435356140137
Batch 63/64 loss: 1.0356273651123047
Batch 64/64 loss: -1.7551894187927246
Epoch 27  Train loss: 1.0178041925617294  Val loss: 0.9113710672175351
Epoch 28
-------------------------------
Batch 1/64 loss: 0.9484405517578125
Batch 2/64 loss: 0.5539455413818359
Batch 3/64 loss: 1.2707200050354004
Batch 4/64 loss: 0.7813334465026855
Batch 5/64 loss: 0.8751029968261719
Batch 6/64 loss: 0.9582552909851074
Batch 7/64 loss: 1.2700839042663574
Batch 8/64 loss: 0.7098736763000488
Batch 9/64 loss: 0.7738080024719238
Batch 10/64 loss: 0.856325626373291
Batch 11/64 loss: 1.2752137184143066
Batch 12/64 loss: 0.8965530395507812
Batch 13/64 loss: 1.0322532653808594
Batch 14/64 loss: 0.5140509605407715
Batch 15/64 loss: 1.0786943435668945
Batch 16/64 loss: 0.9176383018493652
Batch 17/64 loss: 1.1522703170776367
Batch 18/64 loss: 0.9460906982421875
Batch 19/64 loss: 1.0365629196166992
Batch 20/64 loss: 1.1521110534667969
Batch 21/64 loss: 1.413869857788086
Batch 22/64 loss: 1.5839037895202637
Batch 23/64 loss: 0.6851835250854492
Batch 24/64 loss: 1.3914971351623535
Batch 25/64 loss: 0.9442024230957031
Batch 26/64 loss: 0.8996138572692871
Batch 27/64 loss: 1.0388193130493164
Batch 28/64 loss: 0.9489603042602539
Batch 29/64 loss: 0.9925384521484375
Batch 30/64 loss: 1.2581801414489746
Batch 31/64 loss: 1.2604117393493652
Batch 32/64 loss: 1.039109706878662
Batch 33/64 loss: 0.7081437110900879
Batch 34/64 loss: 1.815352439880371
Batch 35/64 loss: 1.6403374671936035
Batch 36/64 loss: 1.032060146331787
Batch 37/64 loss: 1.055769920349121
Batch 38/64 loss: 1.2337236404418945
Batch 39/64 loss: 1.019202709197998
Batch 40/64 loss: 1.5309739112854004
Batch 41/64 loss: 1.130904197692871
Batch 42/64 loss: 0.993372917175293
Batch 43/64 loss: 1.3324384689331055
Batch 44/64 loss: 1.5142731666564941
Batch 45/64 loss: 0.9708380699157715
Batch 46/64 loss: 1.0069694519042969
Batch 47/64 loss: 0.7553596496582031
Batch 48/64 loss: 0.7553424835205078
Batch 49/64 loss: 1.0305233001708984
Batch 50/64 loss: 1.0319795608520508
Batch 51/64 loss: 1.102531909942627
Batch 52/64 loss: 0.868107795715332
Batch 53/64 loss: 0.9913396835327148
Batch 54/64 loss: 0.9368867874145508
Batch 55/64 loss: 0.6883559226989746
Batch 56/64 loss: 0.8936481475830078
Batch 57/64 loss: 1.3296222686767578
Batch 58/64 loss: 0.7383866310119629
Batch 59/64 loss: 0.8459811210632324
Batch 60/64 loss: 0.6775679588317871
Batch 61/64 loss: 1.3133678436279297
Batch 62/64 loss: 1.1288275718688965
Batch 63/64 loss: 1.0794296264648438
Batch 64/64 loss: -2.3944287300109863
Epoch 28  Train loss: 1.0009633700052898  Val loss: 0.8591614228343636
Saving best model, epoch: 28
Epoch 29
-------------------------------
Batch 1/64 loss: 0.9768896102905273
Batch 2/64 loss: 1.110072135925293
Batch 3/64 loss: 1.2451915740966797
Batch 4/64 loss: 1.1188044548034668
Batch 5/64 loss: 0.8373827934265137
Batch 6/64 loss: 1.2085294723510742
Batch 7/64 loss: 1.019402027130127
Batch 8/64 loss: 1.0807418823242188
Batch 9/64 loss: 1.0241684913635254
Batch 10/64 loss: 0.9002842903137207
Batch 11/64 loss: 0.542302131652832
Batch 12/64 loss: 0.8085060119628906
Batch 13/64 loss: 0.9467239379882812
Batch 14/64 loss: 0.7766847610473633
Batch 15/64 loss: 1.282562255859375
Batch 16/64 loss: 0.968113899230957
Batch 17/64 loss: 0.6478347778320312
Batch 18/64 loss: 1.2914495468139648
Batch 19/64 loss: 1.4230241775512695
Batch 20/64 loss: 0.9949741363525391
Batch 21/64 loss: 1.129939079284668
Batch 22/64 loss: 0.8719100952148438
Batch 23/64 loss: 1.2038803100585938
Batch 24/64 loss: 0.7711224555969238
Batch 25/64 loss: 0.9077305793762207
Batch 26/64 loss: 1.027148723602295
Batch 27/64 loss: 0.9096136093139648
Batch 28/64 loss: 0.8870153427124023
Batch 29/64 loss: 1.0021400451660156
Batch 30/64 loss: 0.8545536994934082
Batch 31/64 loss: 1.3029723167419434
Batch 32/64 loss: 1.244880199432373
Batch 33/64 loss: 1.0494742393493652
Batch 34/64 loss: 0.6733336448669434
Batch 35/64 loss: 0.7044186592102051
Batch 36/64 loss: 1.3283748626708984
Batch 37/64 loss: 0.8054542541503906
Batch 38/64 loss: 1.0535974502563477
Batch 39/64 loss: 1.1974945068359375
Batch 40/64 loss: 1.1132550239562988
Batch 41/64 loss: 0.7207674980163574
Batch 42/64 loss: 0.8658351898193359
Batch 43/64 loss: 0.9492573738098145
Batch 44/64 loss: 1.2019906044006348
Batch 45/64 loss: 0.9728856086730957
Batch 46/64 loss: 0.9083051681518555
Batch 47/64 loss: 0.45688915252685547
Batch 48/64 loss: 1.6811909675598145
Batch 49/64 loss: 0.9277958869934082
Batch 50/64 loss: 0.8911423683166504
Batch 51/64 loss: 0.9902873039245605
Batch 52/64 loss: 0.9567680358886719
Batch 53/64 loss: 0.544435977935791
Batch 54/64 loss: 1.4380202293395996
Batch 55/64 loss: 1.0634994506835938
Batch 56/64 loss: 1.245187759399414
Batch 57/64 loss: 1.3233628273010254
Batch 58/64 loss: 1.197585105895996
Batch 59/64 loss: 0.8678450584411621
Batch 60/64 loss: 0.8656673431396484
Batch 61/64 loss: 0.47411012649536133
Batch 62/64 loss: 1.3889718055725098
Batch 63/64 loss: 0.8865809440612793
Batch 64/64 loss: -2.1341395378112793
Epoch 29  Train loss: 0.9640741740956026  Val loss: 0.9517704481931076
Epoch 30
-------------------------------
Batch 1/64 loss: 0.9820981025695801
Batch 2/64 loss: 0.7263216972351074
Batch 3/64 loss: 1.2304706573486328
Batch 4/64 loss: 0.7305569648742676
Batch 5/64 loss: 0.9577040672302246
Batch 6/64 loss: 0.9471111297607422
Batch 7/64 loss: 0.967383861541748
Batch 8/64 loss: 1.0532035827636719
Batch 9/64 loss: 1.0836338996887207
Batch 10/64 loss: 1.4422540664672852
Batch 11/64 loss: 0.8567538261413574
Batch 12/64 loss: 1.0638256072998047
Batch 13/64 loss: 0.8924341201782227
Batch 14/64 loss: 0.757850170135498
Batch 15/64 loss: 0.9177279472351074
Batch 16/64 loss: 0.8686504364013672
Batch 17/64 loss: 0.6687631607055664
Batch 18/64 loss: 0.8847441673278809
Batch 19/64 loss: 0.9616775512695312
Batch 20/64 loss: 0.5952754020690918
Batch 21/64 loss: 1.0514802932739258
Batch 22/64 loss: 0.6653962135314941
Batch 23/64 loss: 0.9261837005615234
Batch 24/64 loss: 0.7343721389770508
Batch 25/64 loss: 0.7629752159118652
Batch 26/64 loss: 0.8681564331054688
Batch 27/64 loss: 1.1289725303649902
Batch 28/64 loss: 0.8435907363891602
Batch 29/64 loss: 0.7169680595397949
Batch 30/64 loss: 1.0384621620178223
Batch 31/64 loss: 1.2349882125854492
Batch 32/64 loss: 1.1693062782287598
Batch 33/64 loss: 0.859438419342041
Batch 34/64 loss: 1.0148792266845703
Batch 35/64 loss: 1.2604308128356934
Batch 36/64 loss: 1.2935523986816406
Batch 37/64 loss: 1.1835312843322754
Batch 38/64 loss: 1.3148002624511719
Batch 39/64 loss: 0.9991044998168945
Batch 40/64 loss: 1.0363726615905762
Batch 41/64 loss: 1.1115360260009766
Batch 42/64 loss: 1.0807843208312988
Batch 43/64 loss: 0.4610457420349121
Batch 44/64 loss: 1.2056384086608887
Batch 45/64 loss: 1.1964988708496094
Batch 46/64 loss: 0.6730556488037109
Batch 47/64 loss: 0.9651923179626465
Batch 48/64 loss: 1.6138348579406738
Batch 49/64 loss: 0.8574347496032715
Batch 50/64 loss: 0.8017520904541016
Batch 51/64 loss: 0.6039514541625977
Batch 52/64 loss: 0.9883904457092285
Batch 53/64 loss: 1.2359771728515625
Batch 54/64 loss: 0.8485641479492188
Batch 55/64 loss: 0.6771297454833984
Batch 56/64 loss: 0.9300460815429688
Batch 57/64 loss: 0.7886133193969727
Batch 58/64 loss: 0.6781744956970215
Batch 59/64 loss: 0.9366436004638672
Batch 60/64 loss: 0.9012956619262695
Batch 61/64 loss: 1.142463207244873
Batch 62/64 loss: 1.6852431297302246
Batch 63/64 loss: 0.7787394523620605
Batch 64/64 loss: -2.319932460784912
Epoch 30  Train loss: 0.9272699225182627  Val loss: 0.7121458151905807
Saving best model, epoch: 30
Epoch 31
-------------------------------
Batch 1/64 loss: 0.9376754760742188
Batch 2/64 loss: 0.5388164520263672
Batch 3/64 loss: 0.6954574584960938
Batch 4/64 loss: 0.9083442687988281
Batch 5/64 loss: 0.8182907104492188
Batch 6/64 loss: 0.9502625465393066
Batch 7/64 loss: 1.0009922981262207
Batch 8/64 loss: 0.6741695404052734
Batch 9/64 loss: 1.0156364440917969
Batch 10/64 loss: 1.1504120826721191
Batch 11/64 loss: 1.0605378150939941
Batch 12/64 loss: 1.1146354675292969
Batch 13/64 loss: 1.371368408203125
Batch 14/64 loss: 1.0535268783569336
Batch 15/64 loss: 0.9774637222290039
Batch 16/64 loss: 0.7421083450317383
Batch 17/64 loss: 1.2508544921875
Batch 18/64 loss: 0.998046875
Batch 19/64 loss: 1.1468286514282227
Batch 20/64 loss: 0.9773530960083008
Batch 21/64 loss: 1.0863103866577148
Batch 22/64 loss: 0.682373046875
Batch 23/64 loss: 1.1030373573303223
Batch 24/64 loss: 1.0871834754943848
Batch 25/64 loss: 0.6716775894165039
Batch 26/64 loss: 0.9002895355224609
Batch 27/64 loss: 1.0340304374694824
Batch 28/64 loss: 1.3235316276550293
Batch 29/64 loss: 0.7327923774719238
Batch 30/64 loss: 0.834296703338623
Batch 31/64 loss: 1.4332756996154785
Batch 32/64 loss: 1.0391240119934082
Batch 33/64 loss: 1.1891422271728516
Batch 34/64 loss: 0.8662362098693848
Batch 35/64 loss: 0.7867579460144043
Batch 36/64 loss: 0.8358564376831055
Batch 37/64 loss: 0.6927099227905273
Batch 38/64 loss: 0.8793702125549316
Batch 39/64 loss: 0.7938938140869141
Batch 40/64 loss: 0.9649834632873535
Batch 41/64 loss: 1.0399565696716309
Batch 42/64 loss: 0.7182126045227051
Batch 43/64 loss: 0.7863998413085938
Batch 44/64 loss: 0.9471349716186523
Batch 45/64 loss: 1.0333590507507324
Batch 46/64 loss: 1.1283297538757324
Batch 47/64 loss: 0.7974815368652344
Batch 48/64 loss: 0.9396638870239258
Batch 49/64 loss: 0.8939805030822754
Batch 50/64 loss: 0.7667889595031738
Batch 51/64 loss: 0.9319791793823242
Batch 52/64 loss: 0.7984194755554199
Batch 53/64 loss: 1.155184268951416
Batch 54/64 loss: 0.7919731140136719
Batch 55/64 loss: 0.7713375091552734
Batch 56/64 loss: 0.5993695259094238
Batch 57/64 loss: 0.8490619659423828
Batch 58/64 loss: 0.895991325378418
Batch 59/64 loss: 0.6599979400634766
Batch 60/64 loss: 1.3029217720031738
Batch 61/64 loss: 0.6751632690429688
Batch 62/64 loss: 0.8732233047485352
Batch 63/64 loss: 0.9629979133605957
Batch 64/64 loss: -2.1075520515441895
Epoch 31  Train loss: 0.8950261602214739  Val loss: 0.7970483524283183
Epoch 32
-------------------------------
Batch 1/64 loss: 0.6262269020080566
Batch 2/64 loss: 0.6194682121276855
Batch 3/64 loss: 0.9715428352355957
Batch 4/64 loss: 0.8053030967712402
Batch 5/64 loss: 0.841672420501709
Batch 6/64 loss: 1.0544915199279785
Batch 7/64 loss: 1.0186405181884766
Batch 8/64 loss: 0.9141726493835449
Batch 9/64 loss: 0.6888861656188965
Batch 10/64 loss: 0.9617877006530762
Batch 11/64 loss: 0.645843505859375
Batch 12/64 loss: 1.0452370643615723
Batch 13/64 loss: 1.0191974639892578
Batch 14/64 loss: 1.1052155494689941
Batch 15/64 loss: 1.1281442642211914
Batch 16/64 loss: 0.9476704597473145
Batch 17/64 loss: 0.7781596183776855
Batch 18/64 loss: 0.6435251235961914
Batch 19/64 loss: 1.1551995277404785
Batch 20/64 loss: 0.5611677169799805
Batch 21/64 loss: 0.7082219123840332
Batch 22/64 loss: 0.9104914665222168
Batch 23/64 loss: 1.0378227233886719
Batch 24/64 loss: 1.0784778594970703
Batch 25/64 loss: 0.6772270202636719
Batch 26/64 loss: 0.6150970458984375
Batch 27/64 loss: 1.020981788635254
Batch 28/64 loss: 0.7574667930603027
Batch 29/64 loss: 0.8117671012878418
Batch 30/64 loss: 0.42171764373779297
Batch 31/64 loss: 0.9837660789489746
Batch 32/64 loss: 1.0240836143493652
Batch 33/64 loss: 0.5943355560302734
Batch 34/64 loss: 0.7831258773803711
Batch 35/64 loss: 0.9017066955566406
Batch 36/64 loss: 0.7072420120239258
Batch 37/64 loss: 0.44435834884643555
Batch 38/64 loss: 0.8409638404846191
Batch 39/64 loss: 0.662879467010498
Batch 40/64 loss: 1.0668139457702637
Batch 41/64 loss: 0.9074029922485352
Batch 42/64 loss: 1.0066328048706055
Batch 43/64 loss: 1.2272748947143555
Batch 44/64 loss: 1.0730934143066406
Batch 45/64 loss: 1.3331403732299805
Batch 46/64 loss: 1.0089340209960938
Batch 47/64 loss: 0.9583802223205566
Batch 48/64 loss: 1.4265122413635254
Batch 49/64 loss: 1.0855965614318848
Batch 50/64 loss: 1.0079174041748047
Batch 51/64 loss: 1.1413655281066895
Batch 52/64 loss: 0.8169159889221191
Batch 53/64 loss: 0.7493386268615723
Batch 54/64 loss: 0.8653683662414551
Batch 55/64 loss: 0.9741487503051758
Batch 56/64 loss: 0.8920259475708008
Batch 57/64 loss: 1.3967089653015137
Batch 58/64 loss: 0.5791182518005371
Batch 59/64 loss: 0.715430736541748
Batch 60/64 loss: 0.6412725448608398
Batch 61/64 loss: 0.5279135704040527
Batch 62/64 loss: 0.8009719848632812
Batch 63/64 loss: 0.8911747932434082
Batch 64/64 loss: -2.7795467376708984
Epoch 32  Train loss: 0.8395620084276386  Val loss: 0.7069666098892894
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: 0.5411801338195801
Batch 2/64 loss: 0.8945302963256836
Batch 3/64 loss: 0.58392333984375
Batch 4/64 loss: 0.726313591003418
Batch 5/64 loss: 1.0796518325805664
Batch 6/64 loss: 0.9591093063354492
Batch 7/64 loss: 0.6438660621643066
Batch 8/64 loss: 0.9258275032043457
Batch 9/64 loss: 1.0477657318115234
Batch 10/64 loss: 0.614469051361084
Batch 11/64 loss: 0.9847397804260254
Batch 12/64 loss: 1.3281569480895996
Batch 13/64 loss: 0.6170554161071777
Batch 14/64 loss: 0.813232421875
Batch 15/64 loss: 0.7055516242980957
Batch 16/64 loss: 0.9785637855529785
Batch 17/64 loss: 1.0833230018615723
Batch 18/64 loss: 0.9137701988220215
Batch 19/64 loss: 0.7708315849304199
Batch 20/64 loss: 1.166837215423584
Batch 21/64 loss: 1.3051066398620605
Batch 22/64 loss: 0.848726749420166
Batch 23/64 loss: 0.9632444381713867
Batch 24/64 loss: 1.201709270477295
Batch 25/64 loss: 0.7016959190368652
Batch 26/64 loss: 0.9711437225341797
Batch 27/64 loss: 1.0996384620666504
Batch 28/64 loss: 0.49039125442504883
Batch 29/64 loss: 1.1757850646972656
Batch 30/64 loss: 0.937929630279541
Batch 31/64 loss: 0.7005467414855957
Batch 32/64 loss: 0.6867818832397461
Batch 33/64 loss: 0.6779403686523438
Batch 34/64 loss: 0.6641626358032227
Batch 35/64 loss: 0.8804035186767578
Batch 36/64 loss: 0.8024454116821289
Batch 37/64 loss: 0.8474240303039551
Batch 38/64 loss: 1.0793704986572266
Batch 39/64 loss: 0.6873807907104492
Batch 40/64 loss: 1.1529784202575684
Batch 41/64 loss: 0.7705020904541016
Batch 42/64 loss: 0.8662381172180176
Batch 43/64 loss: 0.5453982353210449
Batch 44/64 loss: 1.0972514152526855
Batch 45/64 loss: 0.9357686042785645
Batch 46/64 loss: 0.9094142913818359
Batch 47/64 loss: 0.6135225296020508
Batch 48/64 loss: 0.8974428176879883
Batch 49/64 loss: 0.9019594192504883
Batch 50/64 loss: 0.6612968444824219
Batch 51/64 loss: 0.5336813926696777
Batch 52/64 loss: 0.615837574005127
Batch 53/64 loss: 1.276597499847412
Batch 54/64 loss: 0.672637939453125
Batch 55/64 loss: 0.5985665321350098
Batch 56/64 loss: 0.6231842041015625
Batch 57/64 loss: 0.9514732360839844
Batch 58/64 loss: 0.7012419700622559
Batch 59/64 loss: 1.3104939460754395
Batch 60/64 loss: 0.5542278289794922
Batch 61/64 loss: 0.6266646385192871
Batch 62/64 loss: 0.6597690582275391
Batch 63/64 loss: 0.7272391319274902
Batch 64/64 loss: -2.569244384765625
Epoch 33  Train loss: 0.8059134165445964  Val loss: 0.6552695572581079
Saving best model, epoch: 33
Epoch 34
-------------------------------
Batch 1/64 loss: 0.7095932960510254
Batch 2/64 loss: 0.8017997741699219
Batch 3/64 loss: 0.7207770347595215
Batch 4/64 loss: 0.5418801307678223
Batch 5/64 loss: 0.9139914512634277
Batch 6/64 loss: 1.1027483940124512
Batch 7/64 loss: 0.5936789512634277
Batch 8/64 loss: 1.2252345085144043
Batch 9/64 loss: 1.4354214668273926
Batch 10/64 loss: 0.6203279495239258
Batch 11/64 loss: 1.4670257568359375
Batch 12/64 loss: 0.9509077072143555
Batch 13/64 loss: 0.6518511772155762
Batch 14/64 loss: 1.001408576965332
Batch 15/64 loss: 0.8846473693847656
Batch 16/64 loss: 0.9862737655639648
Batch 17/64 loss: 1.066117286682129
Batch 18/64 loss: 1.6280450820922852
Batch 19/64 loss: 1.061516284942627
Batch 20/64 loss: 0.7073864936828613
Batch 21/64 loss: 0.7748451232910156
Batch 22/64 loss: 0.9247469902038574
Batch 23/64 loss: 1.4135518074035645
Batch 24/64 loss: 0.5683012008666992
Batch 25/64 loss: 0.9996776580810547
Batch 26/64 loss: 0.9142618179321289
Batch 27/64 loss: 1.3863558769226074
Batch 28/64 loss: 0.7516975402832031
Batch 29/64 loss: 0.8729791641235352
Batch 30/64 loss: 1.0877580642700195
Batch 31/64 loss: 0.9308867454528809
Batch 32/64 loss: 1.0137228965759277
Batch 33/64 loss: 0.8179745674133301
Batch 34/64 loss: 0.9177169799804688
Batch 35/64 loss: 0.8011541366577148
Batch 36/64 loss: 1.049875259399414
Batch 37/64 loss: 0.9071931838989258
Batch 38/64 loss: 0.608619213104248
Batch 39/64 loss: 0.7564606666564941
Batch 40/64 loss: 0.711707592010498
Batch 41/64 loss: 1.1840767860412598
Batch 42/64 loss: 1.193741798400879
Batch 43/64 loss: 0.868436336517334
Batch 44/64 loss: 0.46170663833618164
Batch 45/64 loss: 1.1501307487487793
Batch 46/64 loss: 1.4366145133972168
Batch 47/64 loss: 0.5774421691894531
Batch 48/64 loss: 0.6204485893249512
Batch 49/64 loss: 0.8722047805786133
Batch 50/64 loss: 1.0652942657470703
Batch 51/64 loss: 0.8716449737548828
Batch 52/64 loss: 0.594123363494873
Batch 53/64 loss: 1.59507417678833
Batch 54/64 loss: 1.294830322265625
Batch 55/64 loss: 1.187697410583496
Batch 56/64 loss: 0.606109619140625
Batch 57/64 loss: 0.5337944030761719
Batch 58/64 loss: 1.1675786972045898
Batch 59/64 loss: 0.817162036895752
Batch 60/64 loss: 0.4700789451599121
Batch 61/64 loss: 1.997631549835205
Batch 62/64 loss: 0.838533878326416
Batch 63/64 loss: 0.9221796989440918
Batch 64/64 loss: -2.4410319328308105
Epoch 34  Train loss: 0.906288324617872  Val loss: 0.9568101614201602
Epoch 35
-------------------------------
Batch 1/64 loss: 0.8148231506347656
Batch 2/64 loss: 1.1079168319702148
Batch 3/64 loss: 1.528376579284668
Batch 4/64 loss: 0.7521705627441406
Batch 5/64 loss: 1.140429973602295
Batch 6/64 loss: 0.8761835098266602
Batch 7/64 loss: 0.8885836601257324
Batch 8/64 loss: 0.9841604232788086
Batch 9/64 loss: 1.543877124786377
Batch 10/64 loss: 1.0527734756469727
Batch 11/64 loss: 2.0465564727783203
Batch 12/64 loss: 1.068528175354004
Batch 13/64 loss: 1.3183856010437012
Batch 14/64 loss: 1.4177780151367188
Batch 15/64 loss: 1.4937729835510254
Batch 16/64 loss: 0.9367008209228516
Batch 17/64 loss: 1.4269227981567383
Batch 18/64 loss: 1.075429916381836
Batch 19/64 loss: 1.1141252517700195
Batch 20/64 loss: 0.8934454917907715
Batch 21/64 loss: 1.0329837799072266
Batch 22/64 loss: 0.8892860412597656
Batch 23/64 loss: 0.6997332572937012
Batch 24/64 loss: 0.7107791900634766
Batch 25/64 loss: 0.989314079284668
Batch 26/64 loss: 1.020561695098877
Batch 27/64 loss: 0.6348390579223633
Batch 28/64 loss: 1.2508668899536133
Batch 29/64 loss: 0.8839168548583984
Batch 30/64 loss: 1.0958762168884277
Batch 31/64 loss: 0.7986235618591309
Batch 32/64 loss: 0.7958807945251465
Batch 33/64 loss: 1.035595417022705
Batch 34/64 loss: 0.8264117240905762
Batch 35/64 loss: 0.7500553131103516
Batch 36/64 loss: 1.2533740997314453
Batch 37/64 loss: 0.8484344482421875
Batch 38/64 loss: 0.7350673675537109
Batch 39/64 loss: 0.6378369331359863
Batch 40/64 loss: 0.8046231269836426
Batch 41/64 loss: 1.05369234085083
Batch 42/64 loss: 0.7845773696899414
Batch 43/64 loss: 0.9762849807739258
Batch 44/64 loss: 0.9641757011413574
Batch 45/64 loss: 0.8843817710876465
Batch 46/64 loss: 1.0044541358947754
Batch 47/64 loss: 1.440441608428955
Batch 48/64 loss: 0.942164421081543
Batch 49/64 loss: 0.910219669342041
Batch 50/64 loss: 1.4332289695739746
Batch 51/64 loss: 0.5723228454589844
Batch 52/64 loss: 1.0235495567321777
Batch 53/64 loss: 0.8225250244140625
Batch 54/64 loss: 0.6842770576477051
Batch 55/64 loss: 1.028757095336914
Batch 56/64 loss: 0.8193912506103516
Batch 57/64 loss: 0.9208550453186035
Batch 58/64 loss: 0.9150457382202148
Batch 59/64 loss: 0.43370771408081055
Batch 60/64 loss: 0.9927177429199219
Batch 61/64 loss: 0.7565488815307617
Batch 62/64 loss: 0.5095338821411133
Batch 63/64 loss: 0.6557826995849609
Batch 64/64 loss: -1.8531675338745117
Epoch 35  Train loss: 0.9460982042200425  Val loss: 0.6943470014329628
Epoch 36
-------------------------------
Batch 1/64 loss: 0.685694694519043
Batch 2/64 loss: 1.0286884307861328
Batch 3/64 loss: 1.0405731201171875
Batch 4/64 loss: 0.8857641220092773
Batch 5/64 loss: 1.0345768928527832
Batch 6/64 loss: 0.6777243614196777
Batch 7/64 loss: 0.7541980743408203
Batch 8/64 loss: 0.5873684883117676
Batch 9/64 loss: 1.087099552154541
Batch 10/64 loss: 1.3139457702636719
Batch 11/64 loss: 1.1893582344055176
Batch 12/64 loss: 0.5991458892822266
Batch 13/64 loss: 1.3124971389770508
Batch 14/64 loss: 0.7861671447753906
Batch 15/64 loss: 0.9162282943725586
Batch 16/64 loss: 0.9460372924804688
Batch 17/64 loss: 0.8551006317138672
Batch 18/64 loss: 0.851409912109375
Batch 19/64 loss: 1.2812151908874512
Batch 20/64 loss: 0.44606685638427734
Batch 21/64 loss: 1.0767168998718262
Batch 22/64 loss: 0.6868677139282227
Batch 23/64 loss: 0.7708325386047363
Batch 24/64 loss: 0.7569727897644043
Batch 25/64 loss: 0.5405035018920898
Batch 26/64 loss: 0.6484127044677734
Batch 27/64 loss: 1.0068564414978027
Batch 28/64 loss: 0.40143537521362305
Batch 29/64 loss: 0.493100643157959
Batch 30/64 loss: 0.38547420501708984
Batch 31/64 loss: 1.0299997329711914
Batch 32/64 loss: 1.5402603149414062
Batch 33/64 loss: 0.7130675315856934
Batch 34/64 loss: 0.6677136421203613
Batch 35/64 loss: 0.6918931007385254
Batch 36/64 loss: 1.4792275428771973
Batch 37/64 loss: 1.026991844177246
Batch 38/64 loss: 1.1733150482177734
Batch 39/64 loss: 0.536534309387207
Batch 40/64 loss: 1.5423059463500977
Batch 41/64 loss: 1.034726619720459
Batch 42/64 loss: 0.7811470031738281
Batch 43/64 loss: 1.2611284255981445
Batch 44/64 loss: 1.0279607772827148
Batch 45/64 loss: 0.795067310333252
Batch 46/64 loss: 0.8607549667358398
Batch 47/64 loss: 0.4226102828979492
Batch 48/64 loss: 0.985344409942627
Batch 49/64 loss: 0.8960332870483398
Batch 50/64 loss: 0.9171686172485352
Batch 51/64 loss: 0.9453306198120117
Batch 52/64 loss: 0.8962960243225098
Batch 53/64 loss: 0.7527251243591309
Batch 54/64 loss: 0.5545763969421387
Batch 55/64 loss: 1.2338743209838867
Batch 56/64 loss: 0.5007691383361816
Batch 57/64 loss: 0.6847896575927734
Batch 58/64 loss: 1.390986442565918
Batch 59/64 loss: 0.8359313011169434
Batch 60/64 loss: 1.10856294631958
Batch 61/64 loss: 0.9021034240722656
Batch 62/64 loss: 1.0089926719665527
Batch 63/64 loss: 0.45697927474975586
Batch 64/64 loss: -2.5608878135681152
Epoch 36  Train loss: 0.843616236892401  Val loss: 0.6941422269106731
Epoch 37
-------------------------------
Batch 1/64 loss: 0.886052131652832
Batch 2/64 loss: 0.8185176849365234
Batch 3/64 loss: 0.7884411811828613
Batch 4/64 loss: 0.7454304695129395
Batch 5/64 loss: 0.6898880004882812
Batch 6/64 loss: 0.5974364280700684
Batch 7/64 loss: 0.9895753860473633
Batch 8/64 loss: 0.8778576850891113
Batch 9/64 loss: 0.7397713661193848
Batch 10/64 loss: 0.6976819038391113
Batch 11/64 loss: 0.6955404281616211
Batch 12/64 loss: 0.8864765167236328
Batch 13/64 loss: 1.3253602981567383
Batch 14/64 loss: 1.0138287544250488
Batch 15/64 loss: 0.7983341217041016
Batch 16/64 loss: 1.5008625984191895
Batch 17/64 loss: 0.8474316596984863
Batch 18/64 loss: 0.7676582336425781
Batch 19/64 loss: 0.7119922637939453
Batch 20/64 loss: 1.1678385734558105
Batch 21/64 loss: 0.8957295417785645
Batch 22/64 loss: 1.1064605712890625
Batch 23/64 loss: 0.9431061744689941
Batch 24/64 loss: 0.9529428482055664
Batch 25/64 loss: 0.5925850868225098
Batch 26/64 loss: 0.7884769439697266
Batch 27/64 loss: 0.7427010536193848
Batch 28/64 loss: 1.118706226348877
Batch 29/64 loss: 0.4623098373413086
Batch 30/64 loss: 0.8171672821044922
Batch 31/64 loss: 0.8699254989624023
Batch 32/64 loss: 0.7727746963500977
Batch 33/64 loss: 0.7127246856689453
Batch 34/64 loss: 0.904853343963623
Batch 35/64 loss: 1.5043330192565918
Batch 36/64 loss: 0.5721640586853027
Batch 37/64 loss: 0.7480802536010742
Batch 38/64 loss: 0.6503472328186035
Batch 39/64 loss: 1.3758411407470703
Batch 40/64 loss: 0.9115085601806641
Batch 41/64 loss: 0.7682809829711914
Batch 42/64 loss: 0.7264800071716309
Batch 43/64 loss: 0.9256877899169922
Batch 44/64 loss: 0.8779644966125488
Batch 45/64 loss: 1.0490245819091797
Batch 46/64 loss: 0.644768238067627
Batch 47/64 loss: 0.9897828102111816
Batch 48/64 loss: 0.7464632987976074
Batch 49/64 loss: 0.7934622764587402
Batch 50/64 loss: 1.118067741394043
Batch 51/64 loss: 0.9324679374694824
Batch 52/64 loss: 0.8865914344787598
Batch 53/64 loss: 0.5948605537414551
Batch 54/64 loss: 0.6420378684997559
Batch 55/64 loss: 1.2206454277038574
Batch 56/64 loss: 0.7154974937438965
Batch 57/64 loss: 0.5698304176330566
Batch 58/64 loss: 0.7412285804748535
Batch 59/64 loss: 0.4892702102661133
Batch 60/64 loss: 0.6951389312744141
Batch 61/64 loss: 0.8326587677001953
Batch 62/64 loss: 0.5944881439208984
Batch 63/64 loss: 0.4901924133300781
Batch 64/64 loss: -2.762444019317627
Epoch 37  Train loss: 0.7994003314597934  Val loss: 0.6528486926940709
Saving best model, epoch: 37
Epoch 38
-------------------------------
Batch 1/64 loss: 1.0701332092285156
Batch 2/64 loss: 0.4698915481567383
Batch 3/64 loss: 0.7194633483886719
Batch 4/64 loss: 0.9894661903381348
Batch 5/64 loss: 0.701622486114502
Batch 6/64 loss: 0.8053898811340332
Batch 7/64 loss: 0.761871337890625
Batch 8/64 loss: 1.0570411682128906
Batch 9/64 loss: 0.6965560913085938
Batch 10/64 loss: 0.5805206298828125
Batch 11/64 loss: 0.9623494148254395
Batch 12/64 loss: 0.41800594329833984
Batch 13/64 loss: 0.7849907875061035
Batch 14/64 loss: 0.5269989967346191
Batch 15/64 loss: 0.5934147834777832
Batch 16/64 loss: 0.6242790222167969
Batch 17/64 loss: 0.960230827331543
Batch 18/64 loss: 0.6079244613647461
Batch 19/64 loss: 0.5130643844604492
Batch 20/64 loss: 0.8770699501037598
Batch 21/64 loss: 0.9625544548034668
Batch 22/64 loss: 0.4929518699645996
Batch 23/64 loss: 0.6436834335327148
Batch 24/64 loss: 0.6700935363769531
Batch 25/64 loss: 1.045051097869873
Batch 26/64 loss: 0.8841924667358398
Batch 27/64 loss: 0.4473237991333008
Batch 28/64 loss: 0.8294892311096191
Batch 29/64 loss: 0.8764495849609375
Batch 30/64 loss: 0.7597799301147461
Batch 31/64 loss: 1.0167922973632812
Batch 32/64 loss: 0.4491739273071289
Batch 33/64 loss: 1.2888550758361816
Batch 34/64 loss: 0.7406768798828125
Batch 35/64 loss: 0.3200368881225586
Batch 36/64 loss: 0.7983865737915039
Batch 37/64 loss: 0.806093692779541
Batch 38/64 loss: 0.8502602577209473
Batch 39/64 loss: 1.907148838043213
Batch 40/64 loss: 0.6884765625
Batch 41/64 loss: 0.9027800559997559
Batch 42/64 loss: 0.8086380958557129
Batch 43/64 loss: 0.6082797050476074
Batch 44/64 loss: 0.7023968696594238
Batch 45/64 loss: 0.8857755661010742
Batch 46/64 loss: 1.0008845329284668
Batch 47/64 loss: 1.0811004638671875
Batch 48/64 loss: 0.7658047676086426
Batch 49/64 loss: 0.5355916023254395
Batch 50/64 loss: 1.0801925659179688
Batch 51/64 loss: 0.5193824768066406
Batch 52/64 loss: 0.3031802177429199
Batch 53/64 loss: 0.5566310882568359
Batch 54/64 loss: 0.41689300537109375
Batch 55/64 loss: 1.1299934387207031
Batch 56/64 loss: 1.0118975639343262
Batch 57/64 loss: 0.4506373405456543
Batch 58/64 loss: 0.6438183784484863
Batch 59/64 loss: 0.7337436676025391
Batch 60/64 loss: 0.6581473350524902
Batch 61/64 loss: 0.9010324478149414
Batch 62/64 loss: 0.7800302505493164
Batch 63/64 loss: 0.4533090591430664
Batch 64/64 loss: -2.520130157470703
Epoch 38  Train loss: 0.7252987880332797  Val loss: 0.6224143496903357
Saving best model, epoch: 38
Epoch 39
-------------------------------
Batch 1/64 loss: 0.8778142929077148
Batch 2/64 loss: 0.4066028594970703
Batch 3/64 loss: 0.6101365089416504
Batch 4/64 loss: 0.7860112190246582
Batch 5/64 loss: 0.5431275367736816
Batch 6/64 loss: 0.7174720764160156
Batch 7/64 loss: 0.9369645118713379
Batch 8/64 loss: 1.0389070510864258
Batch 9/64 loss: 0.7136125564575195
Batch 10/64 loss: 0.4668002128601074
Batch 11/64 loss: 0.5691084861755371
Batch 12/64 loss: 0.5408430099487305
Batch 13/64 loss: 1.3399958610534668
Batch 14/64 loss: 1.7945375442504883
Batch 15/64 loss: 1.0126380920410156
Batch 16/64 loss: 1.0584907531738281
Batch 17/64 loss: 0.708076000213623
Batch 18/64 loss: 0.74432373046875
Batch 19/64 loss: 1.0948576927185059
Batch 20/64 loss: 0.849113941192627
Batch 21/64 loss: 0.8027987480163574
Batch 22/64 loss: 1.0883874893188477
Batch 23/64 loss: 0.886049747467041
Batch 24/64 loss: 0.8511576652526855
Batch 25/64 loss: 0.48110389709472656
Batch 26/64 loss: 0.9150094985961914
Batch 27/64 loss: 0.934074878692627
Batch 28/64 loss: 1.0308890342712402
Batch 29/64 loss: 0.8304171562194824
Batch 30/64 loss: 1.3521718978881836
Batch 31/64 loss: 0.5956611633300781
Batch 32/64 loss: 0.26964282989501953
Batch 33/64 loss: 0.7122817039489746
Batch 34/64 loss: 0.5415201187133789
Batch 35/64 loss: 0.4599738121032715
Batch 36/64 loss: 0.6629014015197754
Batch 37/64 loss: 0.6498308181762695
Batch 38/64 loss: 0.4429469108581543
Batch 39/64 loss: 0.8888401985168457
Batch 40/64 loss: 0.7698206901550293
Batch 41/64 loss: 0.7319936752319336
Batch 42/64 loss: 0.8397645950317383
Batch 43/64 loss: 1.6015386581420898
Batch 44/64 loss: 1.5488486289978027
Batch 45/64 loss: 0.5679616928100586
Batch 46/64 loss: 0.7909150123596191
Batch 47/64 loss: 0.5867648124694824
Batch 48/64 loss: 0.5368490219116211
Batch 49/64 loss: 0.42757225036621094
Batch 50/64 loss: 1.2109246253967285
Batch 51/64 loss: 0.7733287811279297
Batch 52/64 loss: 0.34980297088623047
Batch 53/64 loss: 1.104874610900879
Batch 54/64 loss: 0.4418759346008301
Batch 55/64 loss: 0.7343521118164062
Batch 56/64 loss: 0.7708215713500977
Batch 57/64 loss: 0.8290472030639648
Batch 58/64 loss: 0.7147188186645508
Batch 59/64 loss: 0.36989831924438477
Batch 60/64 loss: 0.7519655227661133
Batch 61/64 loss: 0.31148338317871094
Batch 62/64 loss: 0.9586758613586426
Batch 63/64 loss: 0.6836404800415039
Batch 64/64 loss: -3.1043972969055176
Epoch 39  Train loss: 0.7417134771160051  Val loss: 0.6080307583628651
Saving best model, epoch: 39
Epoch 40
-------------------------------
Batch 1/64 loss: 0.5423269271850586
Batch 2/64 loss: 0.8328633308410645
Batch 3/64 loss: 1.0783772468566895
Batch 4/64 loss: 0.5637822151184082
Batch 5/64 loss: 0.551978588104248
Batch 6/64 loss: 1.6946187019348145
Batch 7/64 loss: 1.075423240661621
Batch 8/64 loss: 0.7092924118041992
Batch 9/64 loss: 0.8404684066772461
Batch 10/64 loss: 0.730553150177002
Batch 11/64 loss: 0.6731991767883301
Batch 12/64 loss: 0.5577898025512695
Batch 13/64 loss: 0.7366886138916016
Batch 14/64 loss: 1.0549416542053223
Batch 15/64 loss: 0.7074246406555176
Batch 16/64 loss: 0.6064324378967285
Batch 17/64 loss: 0.35920286178588867
Batch 18/64 loss: 0.8422250747680664
Batch 19/64 loss: 0.4638810157775879
Batch 20/64 loss: 0.5742611885070801
Batch 21/64 loss: 0.8567314147949219
Batch 22/64 loss: 0.47586822509765625
Batch 23/64 loss: 0.5993857383728027
Batch 24/64 loss: 0.9731607437133789
Batch 25/64 loss: 0.7510356903076172
Batch 26/64 loss: 0.755155086517334
Batch 27/64 loss: 0.9977264404296875
Batch 28/64 loss: 0.4486837387084961
Batch 29/64 loss: 0.6977391242980957
Batch 30/64 loss: 0.41802501678466797
Batch 31/64 loss: 0.5819463729858398
Batch 32/64 loss: 0.6872334480285645
Batch 33/64 loss: 0.6896533966064453
Batch 34/64 loss: 0.9345221519470215
Batch 35/64 loss: 1.1047329902648926
Batch 36/64 loss: 0.8161659240722656
Batch 37/64 loss: 0.6200871467590332
Batch 38/64 loss: 0.6194839477539062
Batch 39/64 loss: 0.480832576751709
Batch 40/64 loss: 0.7014813423156738
Batch 41/64 loss: 1.084470272064209
Batch 42/64 loss: 0.5868139266967773
Batch 43/64 loss: 0.6647791862487793
Batch 44/64 loss: 1.1563167572021484
Batch 45/64 loss: 0.5182557106018066
Batch 46/64 loss: 0.9571199417114258
Batch 47/64 loss: 0.5563778877258301
Batch 48/64 loss: 0.5395369529724121
Batch 49/64 loss: 0.8730835914611816
Batch 50/64 loss: 0.6854567527770996
Batch 51/64 loss: 0.8446383476257324
Batch 52/64 loss: 1.2330331802368164
Batch 53/64 loss: 0.5538783073425293
Batch 54/64 loss: 0.737706184387207
Batch 55/64 loss: 0.7735342979431152
Batch 56/64 loss: 0.9101767539978027
Batch 57/64 loss: 0.7976765632629395
Batch 58/64 loss: 0.6865954399108887
Batch 59/64 loss: 0.6567378044128418
Batch 60/64 loss: 0.34950923919677734
Batch 61/64 loss: 0.2388463020324707
Batch 62/64 loss: 0.5185441970825195
Batch 63/64 loss: 0.8034305572509766
Batch 64/64 loss: -2.726889133453369
Epoch 40  Train loss: 0.691556588341208  Val loss: 0.686117952222267
Epoch 41
-------------------------------
Batch 1/64 loss: 1.0342097282409668
Batch 2/64 loss: 0.5429844856262207
Batch 3/64 loss: 0.5768923759460449
Batch 4/64 loss: 1.4877305030822754
Batch 5/64 loss: 0.7430696487426758
Batch 6/64 loss: 0.5046944618225098
Batch 7/64 loss: 0.42967748641967773
Batch 8/64 loss: 1.143099308013916
Batch 9/64 loss: 0.6841244697570801
Batch 10/64 loss: 0.2950620651245117
Batch 11/64 loss: 0.5514273643493652
Batch 12/64 loss: 1.8410139083862305
Batch 13/64 loss: 0.3397345542907715
Batch 14/64 loss: 0.7521796226501465
Batch 15/64 loss: 1.1221942901611328
Batch 16/64 loss: 0.7367935180664062
Batch 17/64 loss: 0.7534127235412598
Batch 18/64 loss: 0.542698860168457
Batch 19/64 loss: 1.2310147285461426
Batch 20/64 loss: 0.679685115814209
Batch 21/64 loss: 0.9496183395385742
Batch 22/64 loss: 0.927863597869873
Batch 23/64 loss: 0.7965235710144043
Batch 24/64 loss: 0.7790088653564453
Batch 25/64 loss: 0.9785337448120117
Batch 26/64 loss: 0.4732990264892578
Batch 27/64 loss: 0.7019062042236328
Batch 28/64 loss: 0.7857279777526855
Batch 29/64 loss: 0.8314838409423828
Batch 30/64 loss: 0.726895809173584
Batch 31/64 loss: 0.9419121742248535
Batch 32/64 loss: 0.48525094985961914
Batch 33/64 loss: 0.7221617698669434
Batch 34/64 loss: 1.188112735748291
Batch 35/64 loss: 0.8315401077270508
Batch 36/64 loss: 0.8480434417724609
Batch 37/64 loss: 0.6987676620483398
Batch 38/64 loss: 0.6824555397033691
Batch 39/64 loss: 0.6149897575378418
Batch 40/64 loss: 0.8189659118652344
Batch 41/64 loss: 0.7695178985595703
Batch 42/64 loss: 1.1295461654663086
Batch 43/64 loss: 0.7211556434631348
Batch 44/64 loss: 0.7624940872192383
Batch 45/64 loss: 0.8067140579223633
Batch 46/64 loss: 0.5608038902282715
Batch 47/64 loss: 0.867032527923584
Batch 48/64 loss: 1.2650232315063477
Batch 49/64 loss: 1.2616095542907715
Batch 50/64 loss: 0.6419754028320312
Batch 51/64 loss: 0.5131993293762207
Batch 52/64 loss: 0.5296521186828613
Batch 53/64 loss: 0.8538436889648438
Batch 54/64 loss: 0.8202242851257324
Batch 55/64 loss: 0.9887285232543945
Batch 56/64 loss: 0.6279215812683105
Batch 57/64 loss: 0.2763023376464844
Batch 58/64 loss: 0.45757484436035156
Batch 59/64 loss: 0.4172697067260742
Batch 60/64 loss: 0.7464122772216797
Batch 61/64 loss: 0.9563131332397461
Batch 62/64 loss: 1.121732234954834
Batch 63/64 loss: 0.5626606941223145
Batch 64/64 loss: -2.664372444152832
Epoch 41  Train loss: 0.7440656961179247  Val loss: 0.47831043099210024
Saving best model, epoch: 41
Epoch 42
-------------------------------
Batch 1/64 loss: 0.8895225524902344
Batch 2/64 loss: 0.8181657791137695
Batch 3/64 loss: 0.7546052932739258
Batch 4/64 loss: 0.9698934555053711
Batch 5/64 loss: 0.7029523849487305
Batch 6/64 loss: 0.5068449974060059
Batch 7/64 loss: 0.7237973213195801
Batch 8/64 loss: 0.529207706451416
Batch 9/64 loss: 0.7133574485778809
Batch 10/64 loss: 0.7579565048217773
Batch 11/64 loss: 0.5876893997192383
Batch 12/64 loss: 0.3592090606689453
Batch 13/64 loss: 0.3084697723388672
Batch 14/64 loss: 0.6573276519775391
Batch 15/64 loss: 0.7151198387145996
Batch 16/64 loss: 0.9965581893920898
Batch 17/64 loss: 0.5644993782043457
Batch 18/64 loss: 0.8308959007263184
Batch 19/64 loss: 1.3247566223144531
Batch 20/64 loss: 0.9773621559143066
Batch 21/64 loss: 0.5976953506469727
Batch 22/64 loss: 0.4874558448791504
Batch 23/64 loss: 0.7882542610168457
Batch 24/64 loss: 0.5540480613708496
Batch 25/64 loss: 0.5993900299072266
Batch 26/64 loss: 0.5158510208129883
Batch 27/64 loss: 0.8199477195739746
Batch 28/64 loss: 0.9474520683288574
Batch 29/64 loss: 0.7303652763366699
Batch 30/64 loss: 0.24763870239257812
Batch 31/64 loss: 0.7616782188415527
Batch 32/64 loss: 0.6462197303771973
Batch 33/64 loss: 0.6616816520690918
Batch 34/64 loss: 0.9528918266296387
Batch 35/64 loss: 0.6403374671936035
Batch 36/64 loss: 0.4860653877258301
Batch 37/64 loss: 0.9430756568908691
Batch 38/64 loss: 0.9522528648376465
Batch 39/64 loss: 1.0022845268249512
Batch 40/64 loss: 0.6381502151489258
Batch 41/64 loss: 0.6958780288696289
Batch 42/64 loss: 0.6077713966369629
Batch 43/64 loss: 0.6945667266845703
Batch 44/64 loss: 1.440291404724121
Batch 45/64 loss: 0.5239644050598145
Batch 46/64 loss: 0.7130532264709473
Batch 47/64 loss: 0.9240493774414062
Batch 48/64 loss: 0.6053447723388672
Batch 49/64 loss: 0.9109487533569336
Batch 50/64 loss: 0.6484007835388184
Batch 51/64 loss: 0.46175479888916016
Batch 52/64 loss: 0.9435381889343262
Batch 53/64 loss: 0.883544921875
Batch 54/64 loss: 0.6945333480834961
Batch 55/64 loss: 0.5330777168273926
Batch 56/64 loss: 0.35106420516967773
Batch 57/64 loss: 0.9636149406433105
Batch 58/64 loss: 0.654139518737793
Batch 59/64 loss: 0.2651176452636719
Batch 60/64 loss: 0.6116256713867188
Batch 61/64 loss: 0.919093132019043
Batch 62/64 loss: 0.8103408813476562
Batch 63/64 loss: 0.24506759643554688
Batch 64/64 loss: -2.7590017318725586
Epoch 42  Train loss: 0.6696856068629845  Val loss: 0.6866759008558345
Epoch 43
-------------------------------
Batch 1/64 loss: 0.7282099723815918
Batch 2/64 loss: 0.9828314781188965
Batch 3/64 loss: 0.6043095588684082
Batch 4/64 loss: 0.2734713554382324
Batch 5/64 loss: 1.1233501434326172
Batch 6/64 loss: 0.8711638450622559
Batch 7/64 loss: 0.436098575592041
Batch 8/64 loss: 0.5964646339416504
Batch 9/64 loss: 1.0038847923278809
Batch 10/64 loss: 1.0176234245300293
Batch 11/64 loss: 0.7580609321594238
Batch 12/64 loss: 0.48393678665161133
Batch 13/64 loss: 0.6016659736633301
Batch 14/64 loss: 0.4786543846130371
Batch 15/64 loss: 0.9951415061950684
Batch 16/64 loss: 0.5848755836486816
Batch 17/64 loss: 0.6476807594299316
Batch 18/64 loss: 0.19109201431274414
Batch 19/64 loss: 0.37677717208862305
Batch 20/64 loss: 0.8274326324462891
Batch 21/64 loss: 0.2646913528442383
Batch 22/64 loss: 0.8450384140014648
Batch 23/64 loss: 0.300994873046875
Batch 24/64 loss: 0.764775276184082
Batch 25/64 loss: 0.5025496482849121
Batch 26/64 loss: 1.178171157836914
Batch 27/64 loss: 0.5462846755981445
Batch 28/64 loss: 0.6738038063049316
Batch 29/64 loss: 0.8137640953063965
Batch 30/64 loss: 0.8692469596862793
Batch 31/64 loss: 0.26786231994628906
Batch 32/64 loss: 0.4638686180114746
Batch 33/64 loss: 0.4837965965270996
Batch 34/64 loss: 0.5662941932678223
Batch 35/64 loss: 0.6379604339599609
Batch 36/64 loss: 1.022359848022461
Batch 37/64 loss: 0.3325233459472656
Batch 38/64 loss: 0.5350818634033203
Batch 39/64 loss: 0.7381062507629395
Batch 40/64 loss: 0.7056741714477539
Batch 41/64 loss: 0.7235574722290039
Batch 42/64 loss: 0.6816210746765137
Batch 43/64 loss: 0.5972614288330078
Batch 44/64 loss: 0.40970849990844727
Batch 45/64 loss: 0.574129581451416
Batch 46/64 loss: 0.619417667388916
Batch 47/64 loss: 0.7104029655456543
Batch 48/64 loss: 0.33232688903808594
Batch 49/64 loss: 0.4962582588195801
Batch 50/64 loss: 0.7823915481567383
Batch 51/64 loss: 0.5337915420532227
Batch 52/64 loss: 0.2417125701904297
Batch 53/64 loss: 1.0150647163391113
Batch 54/64 loss: 0.33823347091674805
Batch 55/64 loss: 0.6140427589416504
Batch 56/64 loss: 0.6707959175109863
Batch 57/64 loss: 0.4479670524597168
Batch 58/64 loss: 0.7242236137390137
Batch 59/64 loss: 0.9655590057373047
Batch 60/64 loss: 0.5367083549499512
Batch 61/64 loss: 0.4684476852416992
Batch 62/64 loss: 0.38407182693481445
Batch 63/64 loss: 0.5852255821228027
Batch 64/64 loss: -2.6010446548461914
Epoch 43  Train loss: 0.5897679908602845  Val loss: 0.47551374664831
Saving best model, epoch: 43
Epoch 44
-------------------------------
Batch 1/64 loss: 0.9506254196166992
Batch 2/64 loss: 0.7363195419311523
Batch 3/64 loss: 0.6586933135986328
Batch 4/64 loss: 0.335479736328125
Batch 5/64 loss: 1.0664963722229004
Batch 6/64 loss: 0.678276538848877
Batch 7/64 loss: 0.9520025253295898
Batch 8/64 loss: 0.7957301139831543
Batch 9/64 loss: 0.6957626342773438
Batch 10/64 loss: 0.16312694549560547
Batch 11/64 loss: 0.4507260322570801
Batch 12/64 loss: 0.6608772277832031
Batch 13/64 loss: 0.9308290481567383
Batch 14/64 loss: 0.40619754791259766
Batch 15/64 loss: 0.590660572052002
Batch 16/64 loss: 0.5110092163085938
Batch 17/64 loss: 0.5399737358093262
Batch 18/64 loss: 0.6934666633605957
Batch 19/64 loss: 0.5116105079650879
Batch 20/64 loss: 0.6754312515258789
Batch 21/64 loss: 0.5316810607910156
Batch 22/64 loss: 0.20104360580444336
Batch 23/64 loss: 0.650935173034668
Batch 24/64 loss: 0.2730441093444824
Batch 25/64 loss: 1.0990571975708008
Batch 26/64 loss: 0.5781702995300293
Batch 27/64 loss: 0.5967960357666016
Batch 28/64 loss: 0.43444252014160156
Batch 29/64 loss: 0.6040759086608887
Batch 30/64 loss: 0.6257843971252441
Batch 31/64 loss: 0.3726005554199219
Batch 32/64 loss: 0.5757231712341309
Batch 33/64 loss: 0.5961413383483887
Batch 34/64 loss: 0.5359587669372559
Batch 35/64 loss: 0.7357687950134277
Batch 36/64 loss: 0.7602725028991699
Batch 37/64 loss: 0.8061285018920898
Batch 38/64 loss: 0.7340879440307617
Batch 39/64 loss: 0.5526285171508789
Batch 40/64 loss: 0.7872085571289062
Batch 41/64 loss: 0.5725102424621582
Batch 42/64 loss: 0.875758171081543
Batch 43/64 loss: 0.6725468635559082
Batch 44/64 loss: 0.988896369934082
Batch 45/64 loss: 0.5103464126586914
Batch 46/64 loss: 0.47702741622924805
Batch 47/64 loss: 0.7116918563842773
Batch 48/64 loss: 1.0452604293823242
Batch 49/64 loss: 0.9094624519348145
Batch 50/64 loss: 0.5085840225219727
Batch 51/64 loss: 0.6658487319946289
Batch 52/64 loss: 1.0671281814575195
Batch 53/64 loss: 1.260225772857666
Batch 54/64 loss: 0.3547368049621582
Batch 55/64 loss: 0.6643791198730469
Batch 56/64 loss: 0.6793422698974609
Batch 57/64 loss: 0.5629124641418457
Batch 58/64 loss: 0.5657401084899902
Batch 59/64 loss: 0.1807088851928711
Batch 60/64 loss: 0.46915435791015625
Batch 61/64 loss: 0.7414751052856445
Batch 62/64 loss: 0.7317852973937988
Batch 63/64 loss: 0.37857627868652344
Batch 64/64 loss: -2.4547080993652344
Epoch 44  Train loss: 0.6087515363506243  Val loss: 0.5389818145647082
Epoch 45
-------------------------------
Batch 1/64 loss: 0.28049373626708984
Batch 2/64 loss: 1.1547341346740723
Batch 3/64 loss: 0.7889084815979004
Batch 4/64 loss: 0.48422861099243164
Batch 5/64 loss: 0.7322096824645996
Batch 6/64 loss: 0.3224654197692871
Batch 7/64 loss: 0.6481013298034668
Batch 8/64 loss: 0.7352886199951172
Batch 9/64 loss: 0.45520925521850586
Batch 10/64 loss: 0.7350273132324219
Batch 11/64 loss: 0.6312141418457031
Batch 12/64 loss: 0.6708765029907227
Batch 13/64 loss: 0.9395008087158203
Batch 14/64 loss: 0.7061343193054199
Batch 15/64 loss: 0.4231076240539551
Batch 16/64 loss: 0.5785670280456543
Batch 17/64 loss: 0.6636672019958496
Batch 18/64 loss: 0.7722382545471191
Batch 19/64 loss: 0.6691656112670898
Batch 20/64 loss: 0.544405460357666
Batch 21/64 loss: 0.5584588050842285
Batch 22/64 loss: 0.33681726455688477
Batch 23/64 loss: 0.9239292144775391
Batch 24/64 loss: 0.8143320083618164
Batch 25/64 loss: 0.9289841651916504
Batch 26/64 loss: 0.5632462501525879
Batch 27/64 loss: 0.6095881462097168
Batch 28/64 loss: 0.6213021278381348
Batch 29/64 loss: 0.7635622024536133
Batch 30/64 loss: 0.7318921089172363
Batch 31/64 loss: 0.43004274368286133
Batch 32/64 loss: 0.6605420112609863
Batch 33/64 loss: 0.6713995933532715
Batch 34/64 loss: 0.5407567024230957
Batch 35/64 loss: 0.4316372871398926
Batch 36/64 loss: 0.760535717010498
Batch 37/64 loss: 0.35663938522338867
Batch 38/64 loss: 0.370147705078125
Batch 39/64 loss: 0.5620865821838379
Batch 40/64 loss: 0.6418328285217285
Batch 41/64 loss: 0.26529979705810547
Batch 42/64 loss: 0.5565395355224609
Batch 43/64 loss: 0.4041743278503418
Batch 44/64 loss: 0.2869715690612793
Batch 45/64 loss: 0.9330625534057617
Batch 46/64 loss: 0.5488071441650391
Batch 47/64 loss: 0.7977132797241211
Batch 48/64 loss: 0.4160771369934082
Batch 49/64 loss: 0.6623830795288086
Batch 50/64 loss: 0.48123693466186523
Batch 51/64 loss: 0.4258894920349121
Batch 52/64 loss: 0.4137401580810547
Batch 53/64 loss: 0.1981353759765625
Batch 54/64 loss: 0.6436753273010254
Batch 55/64 loss: 0.5913567543029785
Batch 56/64 loss: 0.3580203056335449
Batch 57/64 loss: 0.6242027282714844
Batch 58/64 loss: 0.7272734642028809
Batch 59/64 loss: 1.078789234161377
Batch 60/64 loss: 0.4808945655822754
Batch 61/64 loss: 0.5763936042785645
Batch 62/64 loss: 0.6005058288574219
Batch 63/64 loss: 0.5595269203186035
Batch 64/64 loss: -3.1334891319274902
Epoch 45  Train loss: 0.5567659396751254  Val loss: 0.34213180148724426
Saving best model, epoch: 45
Epoch 46
-------------------------------
Batch 1/64 loss: 0.6663899421691895
Batch 2/64 loss: 0.8231949806213379
Batch 3/64 loss: 0.5361933708190918
Batch 4/64 loss: 0.33385419845581055
Batch 5/64 loss: 0.2362508773803711
Batch 6/64 loss: 0.42730188369750977
Batch 7/64 loss: 0.6550960540771484
Batch 8/64 loss: 0.7608757019042969
Batch 9/64 loss: 0.8337302207946777
Batch 10/64 loss: 0.3967304229736328
Batch 11/64 loss: 0.30750036239624023
Batch 12/64 loss: 0.7324142456054688
Batch 13/64 loss: 0.9437198638916016
Batch 14/64 loss: 0.5018653869628906
Batch 15/64 loss: 0.597529411315918
Batch 16/64 loss: 0.41919803619384766
Batch 17/64 loss: 0.34811878204345703
Batch 18/64 loss: 0.6870322227478027
Batch 19/64 loss: 0.6103916168212891
Batch 20/64 loss: 0.6714324951171875
Batch 21/64 loss: 0.1160883903503418
Batch 22/64 loss: 0.5253329277038574
Batch 23/64 loss: 1.21836519241333
Batch 24/64 loss: 0.5329594612121582
Batch 25/64 loss: 0.12790536880493164
Batch 26/64 loss: 0.8275356292724609
Batch 27/64 loss: 0.6017193794250488
Batch 28/64 loss: 0.4932065010070801
Batch 29/64 loss: 0.7445497512817383
Batch 30/64 loss: 0.22872591018676758
Batch 31/64 loss: 0.34735584259033203
Batch 32/64 loss: 0.8073024749755859
Batch 33/64 loss: 0.39385461807250977
Batch 34/64 loss: 0.43492746353149414
Batch 35/64 loss: 0.8854594230651855
Batch 36/64 loss: 0.7104759216308594
Batch 37/64 loss: 0.33840084075927734
Batch 38/64 loss: 0.6437382698059082
Batch 39/64 loss: 0.8156557083129883
Batch 40/64 loss: 0.616783618927002
Batch 41/64 loss: 0.9663844108581543
Batch 42/64 loss: 0.809319019317627
Batch 43/64 loss: 0.6642484664916992
Batch 44/64 loss: 0.36023378372192383
Batch 45/64 loss: 0.659268856048584
Batch 46/64 loss: 0.6517271995544434
Batch 47/64 loss: 0.869316577911377
Batch 48/64 loss: 0.44365787506103516
Batch 49/64 loss: 0.14738750457763672
Batch 50/64 loss: 0.5333528518676758
Batch 51/64 loss: 0.45374155044555664
Batch 52/64 loss: 0.6219272613525391
Batch 53/64 loss: 0.3248414993286133
Batch 54/64 loss: 0.40343427658081055
Batch 55/64 loss: 0.4387645721435547
Batch 56/64 loss: 1.0356731414794922
Batch 57/64 loss: 1.0297489166259766
Batch 58/64 loss: 0.6499176025390625
Batch 59/64 loss: 0.15180635452270508
Batch 60/64 loss: 1.1782736778259277
Batch 61/64 loss: 0.4425230026245117
Batch 62/64 loss: 1.044975757598877
Batch 63/64 loss: 0.32109928131103516
Batch 64/64 loss: -2.7558465003967285
Epoch 46  Train loss: 0.5495513934715122  Val loss: 0.5010260421385879
Epoch 47
-------------------------------
Batch 1/64 loss: 0.7250304222106934
Batch 2/64 loss: 0.5937395095825195
Batch 3/64 loss: 0.693483829498291
Batch 4/64 loss: 1.298604965209961
Batch 5/64 loss: 0.38629961013793945
Batch 6/64 loss: 0.3008756637573242
Batch 7/64 loss: 0.3136711120605469
Batch 8/64 loss: 0.8694300651550293
Batch 9/64 loss: 0.24686765670776367
Batch 10/64 loss: 0.7755188941955566
Batch 11/64 loss: 0.5230765342712402
Batch 12/64 loss: 0.4402337074279785
Batch 13/64 loss: 0.4093146324157715
Batch 14/64 loss: 0.7303991317749023
Batch 15/64 loss: 0.4330282211303711
Batch 16/64 loss: 0.8837084770202637
Batch 17/64 loss: 0.5793924331665039
Batch 18/64 loss: 0.4801492691040039
Batch 19/64 loss: 0.6969842910766602
Batch 20/64 loss: 0.9437198638916016
Batch 21/64 loss: 0.8972291946411133
Batch 22/64 loss: 0.11704587936401367
Batch 23/64 loss: 0.6153712272644043
Batch 24/64 loss: 0.32178497314453125
Batch 25/64 loss: 0.6657652854919434
Batch 26/64 loss: 0.26222848892211914
Batch 27/64 loss: 1.562643051147461
Batch 28/64 loss: 0.7370266914367676
Batch 29/64 loss: 1.097689151763916
Batch 30/64 loss: 0.40156126022338867
Batch 31/64 loss: 0.9163908958435059
Batch 32/64 loss: 1.2863306999206543
Batch 33/64 loss: 1.2498250007629395
Batch 34/64 loss: 0.7686262130737305
Batch 35/64 loss: 1.0562286376953125
Batch 36/64 loss: 1.5996627807617188
Batch 37/64 loss: 0.5523843765258789
Batch 38/64 loss: 1.1780104637145996
Batch 39/64 loss: 1.3421711921691895
Batch 40/64 loss: 1.6699285507202148
Batch 41/64 loss: 1.0990877151489258
Batch 42/64 loss: 0.830873966217041
Batch 43/64 loss: 0.9842123985290527
Batch 44/64 loss: 1.2821903228759766
Batch 45/64 loss: 1.3338837623596191
Batch 46/64 loss: 0.5506925582885742
Batch 47/64 loss: 1.4797682762145996
Batch 48/64 loss: 0.48587465286254883
Batch 49/64 loss: 0.8310995101928711
Batch 50/64 loss: 1.1372370719909668
Batch 51/64 loss: 0.8630504608154297
Batch 52/64 loss: 1.5999999046325684
Batch 53/64 loss: 0.8331136703491211
Batch 54/64 loss: 0.940068244934082
Batch 55/64 loss: 0.8297648429870605
Batch 56/64 loss: 0.6978864669799805
Batch 57/64 loss: 0.7673249244689941
Batch 58/64 loss: 1.1862788200378418
Batch 59/64 loss: 0.7368783950805664
Batch 60/64 loss: 1.0952363014221191
Batch 61/64 loss: 0.667302131652832
Batch 62/64 loss: 0.9421634674072266
Batch 63/64 loss: 0.587580680847168
Batch 64/64 loss: -3.052598476409912
Epoch 47  Train loss: 0.7857812077391382  Val loss: 0.7155082086517229
Epoch 48
-------------------------------
Batch 1/64 loss: 0.49571704864501953
Batch 2/64 loss: 0.9018421173095703
Batch 3/64 loss: 0.8065223693847656
Batch 4/64 loss: 0.8830366134643555
Batch 5/64 loss: 0.27899980545043945
Batch 6/64 loss: 0.4207768440246582
Batch 7/64 loss: 0.3984203338623047
Batch 8/64 loss: 0.5856971740722656
Batch 9/64 loss: 0.9491620063781738
Batch 10/64 loss: 1.3833270072937012
Batch 11/64 loss: 0.8554987907409668
Batch 12/64 loss: 1.1983790397644043
Batch 13/64 loss: 0.7209668159484863
Batch 14/64 loss: 1.0196576118469238
Batch 15/64 loss: 0.4380168914794922
Batch 16/64 loss: 0.7970104217529297
Batch 17/64 loss: 0.7940292358398438
Batch 18/64 loss: 0.637725830078125
Batch 19/64 loss: 0.5117912292480469
Batch 20/64 loss: 1.0429563522338867
Batch 21/64 loss: 0.43964147567749023
Batch 22/64 loss: 0.5420961380004883
Batch 23/64 loss: 0.705146312713623
Batch 24/64 loss: 0.3125033378601074
Batch 25/64 loss: 0.5044407844543457
Batch 26/64 loss: 0.6868257522583008
Batch 27/64 loss: 0.5471858978271484
Batch 28/64 loss: 0.7155575752258301
Batch 29/64 loss: 0.7366585731506348
Batch 30/64 loss: 0.4508628845214844
Batch 31/64 loss: 1.4161529541015625
Batch 32/64 loss: 0.47015953063964844
Batch 33/64 loss: 0.9598112106323242
Batch 34/64 loss: 1.5178723335266113
Batch 35/64 loss: 0.9129352569580078
Batch 36/64 loss: 0.5101814270019531
Batch 37/64 loss: 0.556304931640625
Batch 38/64 loss: 0.9338898658752441
Batch 39/64 loss: 0.851801872253418
Batch 40/64 loss: 1.020949363708496
Batch 41/64 loss: 0.4458909034729004
Batch 42/64 loss: 0.543365478515625
Batch 43/64 loss: 0.9162797927856445
Batch 44/64 loss: 1.0246272087097168
Batch 45/64 loss: 0.4464082717895508
Batch 46/64 loss: 0.5246844291687012
Batch 47/64 loss: 0.5394363403320312
Batch 48/64 loss: 0.5316286087036133
Batch 49/64 loss: 0.46278905868530273
Batch 50/64 loss: 0.6826839447021484
Batch 51/64 loss: 0.3916511535644531
Batch 52/64 loss: 1.2256932258605957
Batch 53/64 loss: 0.9825348854064941
Batch 54/64 loss: 0.2697596549987793
Batch 55/64 loss: 1.007068157196045
Batch 56/64 loss: 0.6496138572692871
Batch 57/64 loss: 0.3341226577758789
Batch 58/64 loss: 0.664055347442627
Batch 59/64 loss: 0.7929096221923828
Batch 60/64 loss: 0.8629469871520996
Batch 61/64 loss: 0.6228418350219727
Batch 62/64 loss: 0.5827479362487793
Batch 63/64 loss: 1.1162734031677246
Batch 64/64 loss: -3.333890914916992
Epoch 48  Train loss: 0.6749820484834559  Val loss: 0.5145048423321387
Epoch 49
-------------------------------
Batch 1/64 loss: 1.3450794219970703
Batch 2/64 loss: 0.4758033752441406
Batch 3/64 loss: 0.696476936340332
Batch 4/64 loss: 1.1592216491699219
Batch 5/64 loss: 0.7891011238098145
Batch 6/64 loss: 0.5947227478027344
Batch 7/64 loss: 0.33016014099121094
Batch 8/64 loss: 0.3711090087890625
Batch 9/64 loss: 0.3637070655822754
Batch 10/64 loss: 0.6592612266540527
Batch 11/64 loss: 1.2314152717590332
Batch 12/64 loss: 1.1353068351745605
Batch 13/64 loss: 0.4064173698425293
Batch 14/64 loss: 0.21937274932861328
Batch 15/64 loss: 0.8111782073974609
Batch 16/64 loss: 0.5135564804077148
Batch 17/64 loss: 0.8364696502685547
Batch 18/64 loss: 0.30358409881591797
Batch 19/64 loss: 0.5089478492736816
Batch 20/64 loss: 0.3257894515991211
Batch 21/64 loss: 0.8289637565612793
Batch 22/64 loss: 0.44646644592285156
Batch 23/64 loss: 0.49130725860595703
Batch 24/64 loss: 0.7522530555725098
Batch 25/64 loss: 0.4858274459838867
Batch 26/64 loss: 0.9734029769897461
Batch 27/64 loss: 0.5584540367126465
Batch 28/64 loss: 1.0856513977050781
Batch 29/64 loss: 0.35126256942749023
Batch 30/64 loss: 0.35701751708984375
Batch 31/64 loss: 0.5871338844299316
Batch 32/64 loss: 1.1772775650024414
Batch 33/64 loss: 0.39643383026123047
Batch 34/64 loss: 0.6783127784729004
Batch 35/64 loss: 0.6042461395263672
Batch 36/64 loss: 0.5365438461303711
Batch 37/64 loss: 0.44393110275268555
Batch 38/64 loss: 0.7550692558288574
Batch 39/64 loss: 0.5189599990844727
Batch 40/64 loss: 0.8311305046081543
Batch 41/64 loss: 1.0195832252502441
Batch 42/64 loss: 0.8530588150024414
Batch 43/64 loss: 0.5554170608520508
Batch 44/64 loss: 0.6996145248413086
Batch 45/64 loss: 0.39336061477661133
Batch 46/64 loss: 1.0511164665222168
Batch 47/64 loss: 0.648676872253418
Batch 48/64 loss: 0.5262365341186523
Batch 49/64 loss: 0.5577130317687988
Batch 50/64 loss: 0.6807780265808105
Batch 51/64 loss: 0.45824623107910156
Batch 52/64 loss: 0.7614989280700684
Batch 53/64 loss: 1.6217083930969238
Batch 54/64 loss: 0.256716251373291
Batch 55/64 loss: 0.8023910522460938
Batch 56/64 loss: 0.9149494171142578
Batch 57/64 loss: 0.6745867729187012
Batch 58/64 loss: 0.5764937400817871
Batch 59/64 loss: 0.3058195114135742
Batch 60/64 loss: 0.6154966354370117
Batch 61/64 loss: 0.5581269264221191
Batch 62/64 loss: 0.8396396636962891
Batch 63/64 loss: 0.6937785148620605
Batch 64/64 loss: -3.059814453125
Epoch 49  Train loss: 0.6228466258329504  Val loss: 0.5462918756753719
Epoch 50
-------------------------------
Batch 1/64 loss: 0.3905658721923828
Batch 2/64 loss: 0.7340049743652344
Batch 3/64 loss: 0.309356689453125
Batch 4/64 loss: 1.3021740913391113
Batch 5/64 loss: 0.7572169303894043
Batch 6/64 loss: 0.41736507415771484
Batch 7/64 loss: 0.5747919082641602
Batch 8/64 loss: 1.141129970550537
Batch 9/64 loss: 0.5308103561401367
Batch 10/64 loss: 0.3158421516418457
Batch 11/64 loss: 0.40099620819091797
Batch 12/64 loss: 1.2946338653564453
Batch 13/64 loss: 0.754906177520752
Batch 14/64 loss: 0.5520915985107422
Batch 15/64 loss: 0.4940462112426758
Batch 16/64 loss: 0.6536574363708496
Batch 17/64 loss: 0.8767237663269043
Batch 18/64 loss: 0.5683174133300781
Batch 19/64 loss: 0.689049243927002
Batch 20/64 loss: 0.925074577331543
Batch 21/64 loss: 0.5175328254699707
Batch 22/64 loss: 0.7203688621520996
Batch 23/64 loss: 0.9978022575378418
Batch 24/64 loss: 0.698974609375
Batch 25/64 loss: 0.791661262512207
Batch 26/64 loss: 0.7719173431396484
Batch 27/64 loss: 0.20961570739746094
Batch 28/64 loss: 0.3262596130371094
Batch 29/64 loss: 0.8389530181884766
Batch 30/64 loss: 0.21667098999023438
Batch 31/64 loss: 1.0650992393493652
Batch 32/64 loss: 0.6280050277709961
Batch 33/64 loss: 0.33243560791015625
Batch 34/64 loss: 0.47129011154174805
Batch 35/64 loss: 0.14983844757080078
Batch 36/64 loss: 0.3255133628845215
Batch 37/64 loss: 0.5503640174865723
Batch 38/64 loss: 0.6108908653259277
Batch 39/64 loss: 0.5447487831115723
Batch 40/64 loss: 0.2386922836303711
Batch 41/64 loss: 0.40291547775268555
Batch 42/64 loss: 0.3666648864746094
Batch 43/64 loss: 0.6942763328552246
Batch 44/64 loss: 0.5537347793579102
Batch 45/64 loss: 0.856806755065918
Batch 46/64 loss: 0.6267924308776855
Batch 47/64 loss: 1.13655424118042
Batch 48/64 loss: 0.15778636932373047
Batch 49/64 loss: 0.6817402839660645
Batch 50/64 loss: 0.32624292373657227
Batch 51/64 loss: 0.27636051177978516
Batch 52/64 loss: 0.6850004196166992
Batch 53/64 loss: 0.4202094078063965
Batch 54/64 loss: 0.9005098342895508
Batch 55/64 loss: 0.6389994621276855
Batch 56/64 loss: 0.8845067024230957
Batch 57/64 loss: 0.25661659240722656
Batch 58/64 loss: 0.979863166809082
Batch 59/64 loss: 0.056223392486572266
Batch 60/64 loss: 0.31292152404785156
Batch 61/64 loss: 0.0751791000366211
Batch 62/64 loss: 0.9940695762634277
Batch 63/64 loss: 0.3120865821838379
Batch 64/64 loss: -2.5055294036865234
Epoch 50  Train loss: 0.5553929796405866  Val loss: 0.4040410281046969
Epoch 51
-------------------------------
Batch 1/64 loss: 0.34461307525634766
Batch 2/64 loss: 0.5687141418457031
Batch 3/64 loss: 0.33511781692504883
Batch 4/64 loss: 0.5775537490844727
Batch 5/64 loss: 0.9151577949523926
Batch 6/64 loss: 0.3264737129211426
Batch 7/64 loss: 0.3362107276916504
Batch 8/64 loss: 0.2108006477355957
Batch 9/64 loss: 0.32485198974609375
Batch 10/64 loss: 0.35079145431518555
Batch 11/64 loss: 0.9132070541381836
Batch 12/64 loss: 0.2871074676513672
Batch 13/64 loss: 0.20858478546142578
Batch 14/64 loss: 0.7569155693054199
Batch 15/64 loss: 0.540489673614502
Batch 16/64 loss: 0.618044376373291
Batch 17/64 loss: 0.12992048263549805
Batch 18/64 loss: 0.6795568466186523
Batch 19/64 loss: 0.3391423225402832
Batch 20/64 loss: 0.9308757781982422
Batch 21/64 loss: 0.39410877227783203
Batch 22/64 loss: 0.8424715995788574
Batch 23/64 loss: 0.39784908294677734
Batch 24/64 loss: 0.33300256729125977
Batch 25/64 loss: 0.9681057929992676
Batch 26/64 loss: 0.7708015441894531
Batch 27/64 loss: 0.7896876335144043
Batch 28/64 loss: 0.5458250045776367
Batch 29/64 loss: 0.8560056686401367
Batch 30/64 loss: 0.5379390716552734
Batch 31/64 loss: 0.6284580230712891
Batch 32/64 loss: 0.4510674476623535
Batch 33/64 loss: 0.5733780860900879
Batch 34/64 loss: 0.38387584686279297
Batch 35/64 loss: 0.39420652389526367
Batch 36/64 loss: 0.7949185371398926
Batch 37/64 loss: 0.31130552291870117
Batch 38/64 loss: 0.4853944778442383
Batch 39/64 loss: 0.6408600807189941
Batch 40/64 loss: 0.6420483589172363
Batch 41/64 loss: 0.3076815605163574
Batch 42/64 loss: 0.7359223365783691
Batch 43/64 loss: 0.5691266059875488
Batch 44/64 loss: 0.6818532943725586
Batch 45/64 loss: 0.7168416976928711
Batch 46/64 loss: 0.5125246047973633
Batch 47/64 loss: 0.24925565719604492
Batch 48/64 loss: 0.5278801918029785
Batch 49/64 loss: 0.20823144912719727
Batch 50/64 loss: 0.5571942329406738
Batch 51/64 loss: 0.26249265670776367
Batch 52/64 loss: 0.7147841453552246
Batch 53/64 loss: 0.2985258102416992
Batch 54/64 loss: 1.1352968215942383
Batch 55/64 loss: 0.4826793670654297
Batch 56/64 loss: 0.7779192924499512
Batch 57/64 loss: 0.31175994873046875
Batch 58/64 loss: 0.9168591499328613
Batch 59/64 loss: 0.31412792205810547
Batch 60/64 loss: 0.3074789047241211
Batch 61/64 loss: 0.31542301177978516
Batch 62/64 loss: 0.9448328018188477
Batch 63/64 loss: 0.7371888160705566
Batch 64/64 loss: -3.262404441833496
Epoch 51  Train loss: 0.4952865263995002  Val loss: 0.41753513296854866
Epoch 52
-------------------------------
Batch 1/64 loss: 0.8154153823852539
Batch 2/64 loss: 0.5904874801635742
Batch 3/64 loss: 0.6922307014465332
Batch 4/64 loss: 0.14701318740844727
Batch 5/64 loss: 0.5261945724487305
Batch 6/64 loss: 0.5253138542175293
Batch 7/64 loss: 0.5357489585876465
Batch 8/64 loss: 0.3565201759338379
Batch 9/64 loss: 0.18659591674804688
Batch 10/64 loss: 0.6099257469177246
Batch 11/64 loss: 0.5561842918395996
Batch 12/64 loss: 0.30959129333496094
Batch 13/64 loss: 0.4037141799926758
Batch 14/64 loss: 0.9202628135681152
Batch 15/64 loss: 0.5843915939331055
Batch 16/64 loss: 0.7965636253356934
Batch 17/64 loss: 0.05614137649536133
Batch 18/64 loss: 0.8346076011657715
Batch 19/64 loss: 0.5251522064208984
Batch 20/64 loss: 0.2511296272277832
Batch 21/64 loss: 0.23933696746826172
Batch 22/64 loss: 1.2869501113891602
Batch 23/64 loss: 0.4238462448120117
Batch 24/64 loss: 0.8445730209350586
Batch 25/64 loss: 1.2445993423461914
Batch 26/64 loss: 0.23166131973266602
Batch 27/64 loss: 0.31494903564453125
Batch 28/64 loss: 0.1334080696105957
Batch 29/64 loss: 0.2128767967224121
Batch 30/64 loss: 0.5625629425048828
Batch 31/64 loss: 0.31947755813598633
Batch 32/64 loss: 0.5256099700927734
Batch 33/64 loss: 0.46453189849853516
Batch 34/64 loss: 0.6007747650146484
Batch 35/64 loss: 0.8042254447937012
Batch 36/64 loss: 0.4119234085083008
Batch 37/64 loss: 0.47907209396362305
Batch 38/64 loss: 0.4836440086364746
Batch 39/64 loss: 0.4190053939819336
Batch 40/64 loss: 0.5593051910400391
Batch 41/64 loss: 0.48775625228881836
Batch 42/64 loss: 0.4309840202331543
Batch 43/64 loss: 0.8648180961608887
Batch 44/64 loss: 0.5302553176879883
Batch 45/64 loss: 0.741945743560791
Batch 46/64 loss: 0.553865909576416
Batch 47/64 loss: 0.7432923316955566
Batch 48/64 loss: 0.4060349464416504
Batch 49/64 loss: 0.32139062881469727
Batch 50/64 loss: 0.8698225021362305
Batch 51/64 loss: 0.4923067092895508
Batch 52/64 loss: 0.45958805084228516
Batch 53/64 loss: 0.6242227554321289
Batch 54/64 loss: 0.735325813293457
Batch 55/64 loss: 0.19106674194335938
Batch 56/64 loss: 0.36089563369750977
Batch 57/64 loss: 0.8154425621032715
Batch 58/64 loss: 0.29048871994018555
Batch 59/64 loss: 0.3453402519226074
Batch 60/64 loss: 0.3751649856567383
Batch 61/64 loss: 0.3124260902404785
Batch 62/64 loss: 0.29701662063598633
Batch 63/64 loss: 0.9279751777648926
Batch 64/64 loss: -3.0872859954833984
Epoch 52  Train loss: 0.4807447770062615  Val loss: 0.48399379245194374
Epoch 53
-------------------------------
Batch 1/64 loss: 0.6175098419189453
Batch 2/64 loss: 0.6988105773925781
Batch 3/64 loss: 0.1285877227783203
Batch 4/64 loss: 1.0375819206237793
Batch 5/64 loss: 0.4842562675476074
Batch 6/64 loss: 0.7088723182678223
Batch 7/64 loss: 0.6388711929321289
Batch 8/64 loss: 0.6505327224731445
Batch 9/64 loss: 0.4035830497741699
Batch 10/64 loss: 0.4394803047180176
Batch 11/64 loss: 0.5261802673339844
Batch 12/64 loss: 0.5123190879821777
Batch 13/64 loss: 0.9855976104736328
Batch 14/64 loss: 0.5574760437011719
Batch 15/64 loss: 0.3993721008300781
Batch 16/64 loss: 0.31252479553222656
Batch 17/64 loss: 0.21200037002563477
Batch 18/64 loss: 0.8479223251342773
Batch 19/64 loss: 0.30638551712036133
Batch 20/64 loss: 0.48832082748413086
Batch 21/64 loss: 0.4410524368286133
Batch 22/64 loss: 0.503262996673584
Batch 23/64 loss: 0.2088484764099121
Batch 24/64 loss: 0.551520824432373
Batch 25/64 loss: 0.4129467010498047
Batch 26/64 loss: 0.46251964569091797
Batch 27/64 loss: 1.1672191619873047
Batch 28/64 loss: 0.7162089347839355
Batch 29/64 loss: 0.22626209259033203
Batch 30/64 loss: 0.4188528060913086
Batch 31/64 loss: 0.6065578460693359
Batch 32/64 loss: 0.6210527420043945
Batch 33/64 loss: 1.1788544654846191
Batch 34/64 loss: 0.16506195068359375
Batch 35/64 loss: 0.8766789436340332
Batch 36/64 loss: 0.39895105361938477
Batch 37/64 loss: 0.5855774879455566
Batch 38/64 loss: 0.5811915397644043
Batch 39/64 loss: 0.597743034362793
Batch 40/64 loss: 0.33883190155029297
Batch 41/64 loss: 0.28403186798095703
Batch 42/64 loss: 0.44290971755981445
Batch 43/64 loss: 0.31125402450561523
Batch 44/64 loss: 0.4655575752258301
Batch 45/64 loss: 0.17349481582641602
Batch 46/64 loss: 0.5522708892822266
Batch 47/64 loss: 0.3929939270019531
Batch 48/64 loss: 0.12843894958496094
Batch 49/64 loss: 0.9735302925109863
Batch 50/64 loss: 0.2906026840209961
Batch 51/64 loss: 0.18945693969726562
Batch 52/64 loss: 0.6343002319335938
Batch 53/64 loss: 0.508725643157959
Batch 54/64 loss: 0.1483306884765625
Batch 55/64 loss: 0.46156787872314453
Batch 56/64 loss: 0.5756916999816895
Batch 57/64 loss: 0.2743568420410156
Batch 58/64 loss: 0.8512792587280273
Batch 59/64 loss: 0.30320072174072266
Batch 60/64 loss: 0.279879093170166
Batch 61/64 loss: 0.3232688903808594
Batch 62/64 loss: 0.2754802703857422
Batch 63/64 loss: 0.4587273597717285
Batch 64/64 loss: -3.002953052520752
Epoch 53  Train loss: 0.45588259416468  Val loss: 0.2906855553695836
Saving best model, epoch: 53
Epoch 54
-------------------------------
Batch 1/64 loss: 0.24985837936401367
Batch 2/64 loss: 0.11642646789550781
Batch 3/64 loss: 0.48525571823120117
Batch 4/64 loss: 0.1491861343383789
Batch 5/64 loss: 0.1854991912841797
Batch 6/64 loss: 0.25838565826416016
Batch 7/64 loss: 0.16316747665405273
Batch 8/64 loss: 0.3200664520263672
Batch 9/64 loss: 0.37168264389038086
Batch 10/64 loss: 1.06939697265625
Batch 11/64 loss: 1.080838680267334
Batch 12/64 loss: 0.4753398895263672
Batch 13/64 loss: 0.8697857856750488
Batch 14/64 loss: 0.5604534149169922
Batch 15/64 loss: 0.7899785041809082
Batch 16/64 loss: 1.429372787475586
Batch 17/64 loss: 0.5408573150634766
Batch 18/64 loss: 0.8514933586120605
Batch 19/64 loss: 0.49930810928344727
Batch 20/64 loss: 0.6351532936096191
Batch 21/64 loss: 0.3557090759277344
Batch 22/64 loss: 0.517855167388916
Batch 23/64 loss: 0.6051754951477051
Batch 24/64 loss: 0.850125789642334
Batch 25/64 loss: 0.5363163948059082
Batch 26/64 loss: 0.3851289749145508
Batch 27/64 loss: 0.7192893028259277
Batch 28/64 loss: 0.5846681594848633
Batch 29/64 loss: 0.5981783866882324
Batch 30/64 loss: 0.891782283782959
Batch 31/64 loss: 0.5829224586486816
Batch 32/64 loss: 0.24473905563354492
Batch 33/64 loss: 0.23962116241455078
Batch 34/64 loss: 0.7623167037963867
Batch 35/64 loss: 0.686375617980957
Batch 36/64 loss: 0.21721601486206055
Batch 37/64 loss: 0.41112518310546875
Batch 38/64 loss: 0.563507080078125
Batch 39/64 loss: 0.5992050170898438
Batch 40/64 loss: 0.37367725372314453
Batch 41/64 loss: 0.4036874771118164
Batch 42/64 loss: 0.39216136932373047
Batch 43/64 loss: 0.3692035675048828
Batch 44/64 loss: 0.5789103507995605
Batch 45/64 loss: 0.6530561447143555
Batch 46/64 loss: 0.30280065536499023
Batch 47/64 loss: 0.3521914482116699
Batch 48/64 loss: 0.44281768798828125
Batch 49/64 loss: 0.01017618179321289
Batch 50/64 loss: -0.011040210723876953
Batch 51/64 loss: 0.7348008155822754
Batch 52/64 loss: 0.7151403427124023
Batch 53/64 loss: 0.17544126510620117
Batch 54/64 loss: 0.26278018951416016
Batch 55/64 loss: 0.5784726142883301
Batch 56/64 loss: 0.9710068702697754
Batch 57/64 loss: 0.4150276184082031
Batch 58/64 loss: 0.3532557487487793
Batch 59/64 loss: 0.1580190658569336
Batch 60/64 loss: 0.587989330291748
Batch 61/64 loss: 0.3822474479675293
Batch 62/64 loss: 0.5625996589660645
Batch 63/64 loss: 0.3565940856933594
Batch 64/64 loss: -3.212231159210205
Epoch 54  Train loss: 0.45748403399598364  Val loss: 0.315389023613684
Epoch 55
-------------------------------
Batch 1/64 loss: 0.4376411437988281
Batch 2/64 loss: 0.12818479537963867
Batch 3/64 loss: 0.7232489585876465
Batch 4/64 loss: 0.18468952178955078
Batch 5/64 loss: 0.48386478424072266
Batch 6/64 loss: 0.20810985565185547
Batch 7/64 loss: 0.6399006843566895
Batch 8/64 loss: 0.45093870162963867
Batch 9/64 loss: 0.15371322631835938
Batch 10/64 loss: 0.7810096740722656
Batch 11/64 loss: 0.5859389305114746
Batch 12/64 loss: 0.4601325988769531
Batch 13/64 loss: 0.5977659225463867
Batch 14/64 loss: 0.7779936790466309
Batch 15/64 loss: 0.13048505783081055
Batch 16/64 loss: 0.4363212585449219
Batch 17/64 loss: 0.8729648590087891
Batch 18/64 loss: 0.6593523025512695
Batch 19/64 loss: 0.1987295150756836
Batch 20/64 loss: 0.36661291122436523
Batch 21/64 loss: 0.3613553047180176
Batch 22/64 loss: 0.4960041046142578
Batch 23/64 loss: 0.7254819869995117
Batch 24/64 loss: 0.6425333023071289
Batch 25/64 loss: 0.49571704864501953
Batch 26/64 loss: 0.5425610542297363
Batch 27/64 loss: 1.2490177154541016
Batch 28/64 loss: 0.37372922897338867
Batch 29/64 loss: 0.9960832595825195
Batch 30/64 loss: 0.054060935974121094
Batch 31/64 loss: 0.3187894821166992
Batch 32/64 loss: 0.35357046127319336
Batch 33/64 loss: 0.2339787483215332
Batch 34/64 loss: 0.4827437400817871
Batch 35/64 loss: 0.10076189041137695
Batch 36/64 loss: 0.23038959503173828
Batch 37/64 loss: 0.6770215034484863
Batch 38/64 loss: 0.49225282669067383
Batch 39/64 loss: 0.5430264472961426
Batch 40/64 loss: 0.4948306083679199
Batch 41/64 loss: 0.840660572052002
Batch 42/64 loss: 0.7863707542419434
Batch 43/64 loss: 0.17901086807250977
Batch 44/64 loss: 0.14230966567993164
Batch 45/64 loss: 0.2921867370605469
Batch 46/64 loss: 0.34150123596191406
Batch 47/64 loss: 0.5678143501281738
Batch 48/64 loss: 0.3535752296447754
Batch 49/64 loss: 0.2656416893005371
Batch 50/64 loss: 0.5328636169433594
Batch 51/64 loss: 0.7790837287902832
Batch 52/64 loss: 0.08943605422973633
Batch 53/64 loss: 0.1523427963256836
Batch 54/64 loss: 0.3655242919921875
Batch 55/64 loss: 0.17766380310058594
Batch 56/64 loss: 0.6741642951965332
Batch 57/64 loss: 0.6478171348571777
Batch 58/64 loss: 0.6644606590270996
Batch 59/64 loss: 0.10513114929199219
Batch 60/64 loss: 0.5764555931091309
Batch 61/64 loss: 0.5708208084106445
Batch 62/64 loss: 0.5740118026733398
Batch 63/64 loss: 0.934298038482666
Batch 64/64 loss: -3.2278575897216797
Epoch 55  Train loss: 0.4287648518880208  Val loss: 0.2912631215098797
Epoch 56
-------------------------------
Batch 1/64 loss: 0.689600944519043
Batch 2/64 loss: 0.42823028564453125
Batch 3/64 loss: 0.8806781768798828
Batch 4/64 loss: 0.6854128837585449
Batch 5/64 loss: 0.23726654052734375
Batch 6/64 loss: 0.3961019515991211
Batch 7/64 loss: 0.13683032989501953
Batch 8/64 loss: 0.32119226455688477
Batch 9/64 loss: 0.7680015563964844
Batch 10/64 loss: 0.31233930587768555
Batch 11/64 loss: 0.14849281311035156
Batch 12/64 loss: 0.6023893356323242
Batch 13/64 loss: 0.47458744049072266
Batch 14/64 loss: -0.043186187744140625
Batch 15/64 loss: 0.2863478660583496
Batch 16/64 loss: 0.19252443313598633
Batch 17/64 loss: 0.3176698684692383
Batch 18/64 loss: 0.727752685546875
Batch 19/64 loss: 0.4024815559387207
Batch 20/64 loss: 0.44817113876342773
Batch 21/64 loss: 0.43964147567749023
Batch 22/64 loss: 0.22932147979736328
Batch 23/64 loss: 0.5403351783752441
Batch 24/64 loss: 0.2569308280944824
Batch 25/64 loss: 0.10002422332763672
Batch 26/64 loss: 0.5012283325195312
Batch 27/64 loss: 0.5041847229003906
Batch 28/64 loss: 0.7279524803161621
Batch 29/64 loss: 0.13927984237670898
Batch 30/64 loss: 0.6968741416931152
Batch 31/64 loss: 0.29320287704467773
Batch 32/64 loss: 1.309920310974121
Batch 33/64 loss: 0.6287460327148438
Batch 34/64 loss: 0.4767470359802246
Batch 35/64 loss: 0.34442138671875
Batch 36/64 loss: 1.0530776977539062
Batch 37/64 loss: 0.40117692947387695
Batch 38/64 loss: 0.32742786407470703
Batch 39/64 loss: 0.46705102920532227
Batch 40/64 loss: 0.6402044296264648
Batch 41/64 loss: 0.17313289642333984
Batch 42/64 loss: 0.11531305313110352
Batch 43/64 loss: 0.2877354621887207
Batch 44/64 loss: 0.5436468124389648
Batch 45/64 loss: 0.3627610206604004
Batch 46/64 loss: 0.22475719451904297
Batch 47/64 loss: 0.6980171203613281
Batch 48/64 loss: 0.7215943336486816
Batch 49/64 loss: 0.8152008056640625
Batch 50/64 loss: 0.7075324058532715
Batch 51/64 loss: 0.7767291069030762
Batch 52/64 loss: 0.4099578857421875
Batch 53/64 loss: 0.44017553329467773
Batch 54/64 loss: 0.28468942642211914
Batch 55/64 loss: 0.3374009132385254
Batch 56/64 loss: 0.5115213394165039
Batch 57/64 loss: 0.37906599044799805
Batch 58/64 loss: 0.28340864181518555
Batch 59/64 loss: 0.20035696029663086
Batch 60/64 loss: 0.022498607635498047
Batch 61/64 loss: 0.10630989074707031
Batch 62/64 loss: 0.792943000793457
Batch 63/64 loss: 0.10661506652832031
Batch 64/64 loss: -3.2414822578430176
Epoch 56  Train loss: 0.39781780803904815  Val loss: 0.2521971869714481
Saving best model, epoch: 56
Epoch 57
-------------------------------
Batch 1/64 loss: 0.22558927536010742
Batch 2/64 loss: 0.9414892196655273
Batch 3/64 loss: 0.4555644989013672
Batch 4/64 loss: 0.19778060913085938
Batch 5/64 loss: 0.3304605484008789
Batch 6/64 loss: 0.3324432373046875
Batch 7/64 loss: 1.278310775756836
Batch 8/64 loss: 0.3640451431274414
Batch 9/64 loss: 0.15927648544311523
Batch 10/64 loss: 0.3002161979675293
Batch 11/64 loss: 0.16570520401000977
Batch 12/64 loss: 0.32032155990600586
Batch 13/64 loss: 0.6151485443115234
Batch 14/64 loss: 0.6432552337646484
Batch 15/64 loss: 0.4630718231201172
Batch 16/64 loss: 0.45255470275878906
Batch 17/64 loss: 0.45593738555908203
Batch 18/64 loss: 0.25739049911499023
Batch 19/64 loss: 0.17690372467041016
Batch 20/64 loss: 0.3859524726867676
Batch 21/64 loss: 0.09299468994140625
Batch 22/64 loss: 0.27161169052124023
Batch 23/64 loss: 0.8604035377502441
Batch 24/64 loss: 0.37351322174072266
Batch 25/64 loss: 0.378173828125
Batch 26/64 loss: 0.6251287460327148
Batch 27/64 loss: 0.44222450256347656
Batch 28/64 loss: 0.12632322311401367
Batch 29/64 loss: 0.38437604904174805
Batch 30/64 loss: 0.2708754539489746
Batch 31/64 loss: 0.03508424758911133
Batch 32/64 loss: 0.4153776168823242
Batch 33/64 loss: 0.34151506423950195
Batch 34/64 loss: 0.7486104965209961
Batch 35/64 loss: 0.10639619827270508
Batch 36/64 loss: 0.4103860855102539
Batch 37/64 loss: 0.9228324890136719
Batch 38/64 loss: 0.2863650321960449
Batch 39/64 loss: 0.30326032638549805
Batch 40/64 loss: 0.5769400596618652
Batch 41/64 loss: 0.6956334114074707
Batch 42/64 loss: 0.3515129089355469
Batch 43/64 loss: 0.5086469650268555
Batch 44/64 loss: 0.6522455215454102
Batch 45/64 loss: 0.6587533950805664
Batch 46/64 loss: 0.2434215545654297
Batch 47/64 loss: 0.6687374114990234
Batch 48/64 loss: 0.4447746276855469
Batch 49/64 loss: 0.8321890830993652
Batch 50/64 loss: 0.5604510307312012
Batch 51/64 loss: 1.3853893280029297
Batch 52/64 loss: 0.74090576171875
Batch 53/64 loss: 0.649019718170166
Batch 54/64 loss: 0.2444171905517578
Batch 55/64 loss: 0.7277202606201172
Batch 56/64 loss: 0.5060544013977051
Batch 57/64 loss: 0.24062871932983398
Batch 58/64 loss: 0.38068532943725586
Batch 59/64 loss: 0.23695945739746094
Batch 60/64 loss: 0.26878881454467773
Batch 61/64 loss: 0.525886058807373
Batch 62/64 loss: 0.17914867401123047
Batch 63/64 loss: 0.38478851318359375
Batch 64/64 loss: -3.393037796020508
Epoch 57  Train loss: 0.408404541015625  Val loss: 0.33356154661407994
Epoch 58
-------------------------------
Batch 1/64 loss: 0.07045269012451172
Batch 2/64 loss: 0.910222053527832
Batch 3/64 loss: 0.4942812919616699
Batch 4/64 loss: 0.25663232803344727
Batch 5/64 loss: 0.29416847229003906
Batch 6/64 loss: 0.2352137565612793
Batch 7/64 loss: 0.4648551940917969
Batch 8/64 loss: 0.3519406318664551
Batch 9/64 loss: 0.3808255195617676
Batch 10/64 loss: 0.936028003692627
Batch 11/64 loss: 0.7594022750854492
Batch 12/64 loss: 0.27404212951660156
Batch 13/64 loss: 0.34107065200805664
Batch 14/64 loss: 0.46500682830810547
Batch 15/64 loss: 0.14236021041870117
Batch 16/64 loss: 0.25845766067504883
Batch 17/64 loss: 0.33255481719970703
Batch 18/64 loss: 0.23383235931396484
Batch 19/64 loss: 0.15940427780151367
Batch 20/64 loss: 0.04410123825073242
Batch 21/64 loss: 0.8312616348266602
Batch 22/64 loss: 0.3289508819580078
Batch 23/64 loss: 0.8281378746032715
Batch 24/64 loss: 0.3735523223876953
Batch 25/64 loss: 0.9775385856628418
Batch 26/64 loss: 0.24298381805419922
Batch 27/64 loss: 0.9000692367553711
Batch 28/64 loss: 0.5347132682800293
Batch 29/64 loss: 0.32697296142578125
Batch 30/64 loss: 0.5561423301696777
Batch 31/64 loss: 0.4971151351928711
Batch 32/64 loss: -0.1565847396850586
Batch 33/64 loss: 0.546445369720459
Batch 34/64 loss: 0.5265731811523438
Batch 35/64 loss: 0.44750022888183594
Batch 36/64 loss: 0.2457265853881836
Batch 37/64 loss: 0.8202223777770996
Batch 38/64 loss: 0.7551283836364746
Batch 39/64 loss: 0.5342440605163574
Batch 40/64 loss: 0.4770083427429199
Batch 41/64 loss: 0.2597188949584961
Batch 42/64 loss: 0.20058059692382812
Batch 43/64 loss: 0.43024492263793945
Batch 44/64 loss: 0.6030569076538086
Batch 45/64 loss: 0.3171658515930176
Batch 46/64 loss: 0.15684223175048828
Batch 47/64 loss: 0.8819737434387207
Batch 48/64 loss: 0.667510986328125
Batch 49/64 loss: 0.28777456283569336
Batch 50/64 loss: 0.29904794692993164
Batch 51/64 loss: 0.609102725982666
Batch 52/64 loss: 0.06203031539916992
Batch 53/64 loss: 0.5536761283874512
Batch 54/64 loss: 0.22745513916015625
Batch 55/64 loss: 0.006814002990722656
Batch 56/64 loss: 0.8898730278015137
Batch 57/64 loss: 0.9688291549682617
Batch 58/64 loss: 0.7194724082946777
Batch 59/64 loss: 0.4897599220275879
Batch 60/64 loss: 0.31331682205200195
Batch 61/64 loss: 0.7807555198669434
Batch 62/64 loss: 0.38968563079833984
Batch 63/64 loss: -0.033654212951660156
Batch 64/64 loss: -3.2495198249816895
Epoch 58  Train loss: 0.40223447201298734  Val loss: 0.5267281548673755
Epoch 59
-------------------------------
Batch 1/64 loss: 0.22826337814331055
Batch 2/64 loss: 0.35056114196777344
Batch 3/64 loss: 0.999457836151123
Batch 4/64 loss: 0.9197158813476562
Batch 5/64 loss: 1.0360326766967773
Batch 6/64 loss: 0.5941624641418457
Batch 7/64 loss: 0.09635686874389648
Batch 8/64 loss: 0.032068729400634766
Batch 9/64 loss: 0.17066144943237305
Batch 10/64 loss: 0.3488006591796875
Batch 11/64 loss: 0.6517066955566406
Batch 12/64 loss: 0.34528207778930664
Batch 13/64 loss: 0.2659635543823242
Batch 14/64 loss: 0.5499329566955566
Batch 15/64 loss: 0.07173490524291992
Batch 16/64 loss: 0.24094533920288086
Batch 17/64 loss: 0.7467751502990723
Batch 18/64 loss: 0.1473398208618164
Batch 19/64 loss: 0.6422901153564453
Batch 20/64 loss: 0.5239496231079102
Batch 21/64 loss: 0.6348514556884766
Batch 22/64 loss: 0.4323549270629883
Batch 23/64 loss: 0.33175230026245117
Batch 24/64 loss: 0.5038108825683594
Batch 25/64 loss: 0.40469837188720703
Batch 26/64 loss: 0.5535011291503906
Batch 27/64 loss: 0.8094744682312012
Batch 28/64 loss: 0.5250892639160156
Batch 29/64 loss: 0.5958065986633301
Batch 30/64 loss: 0.5059409141540527
Batch 31/64 loss: 0.15979290008544922
Batch 32/64 loss: 0.26899051666259766
Batch 33/64 loss: 0.22515296936035156
Batch 34/64 loss: 0.7904481887817383
Batch 35/64 loss: -0.026111602783203125
Batch 36/64 loss: 0.44434642791748047
Batch 37/64 loss: 0.5312719345092773
Batch 38/64 loss: 0.3832216262817383
Batch 39/64 loss: 1.0623769760131836
Batch 40/64 loss: 0.6596665382385254
Batch 41/64 loss: 0.1065511703491211
Batch 42/64 loss: 0.5361528396606445
Batch 43/64 loss: 0.907280445098877
Batch 44/64 loss: 0.31911802291870117
Batch 45/64 loss: 0.31258535385131836
Batch 46/64 loss: 0.34332942962646484
Batch 47/64 loss: 0.16390514373779297
Batch 48/64 loss: 0.31154346466064453
Batch 49/64 loss: 0.845374584197998
Batch 50/64 loss: 0.22132587432861328
Batch 51/64 loss: 0.3079051971435547
Batch 52/64 loss: 0.8523502349853516
Batch 53/64 loss: 0.2723722457885742
Batch 54/64 loss: 0.24729394912719727
Batch 55/64 loss: 0.19015073776245117
Batch 56/64 loss: 0.3449225425720215
Batch 57/64 loss: 0.4221220016479492
Batch 58/64 loss: 0.27175188064575195
Batch 59/64 loss: 0.22536516189575195
Batch 60/64 loss: 0.3372535705566406
Batch 61/64 loss: 0.3465385437011719
Batch 62/64 loss: 0.4083423614501953
Batch 63/64 loss: 0.07592439651489258
Batch 64/64 loss: -2.622823715209961
Epoch 59  Train loss: 0.3946788937437768  Val loss: 0.1958525352871295
Saving best model, epoch: 59
Epoch 60
-------------------------------
Batch 1/64 loss: 0.15212011337280273
Batch 2/64 loss: 0.47069406509399414
Batch 3/64 loss: -0.035616397857666016
Batch 4/64 loss: 0.3973531723022461
Batch 5/64 loss: 0.5558853149414062
Batch 6/64 loss: 0.5642819404602051
Batch 7/64 loss: 0.15575313568115234
Batch 8/64 loss: 0.24480247497558594
Batch 9/64 loss: 0.6813106536865234
Batch 10/64 loss: 0.3045058250427246
Batch 11/64 loss: 0.31992626190185547
Batch 12/64 loss: 0.10403156280517578
Batch 13/64 loss: 0.6825942993164062
Batch 14/64 loss: 0.35423898696899414
Batch 15/64 loss: 0.39905643463134766
Batch 16/64 loss: -0.03521299362182617
Batch 17/64 loss: 0.2591996192932129
Batch 18/64 loss: 0.3508749008178711
Batch 19/64 loss: 0.1884016990661621
Batch 20/64 loss: 0.2718367576599121
Batch 21/64 loss: 0.4830284118652344
Batch 22/64 loss: 0.7210583686828613
Batch 23/64 loss: 0.39040374755859375
Batch 24/64 loss: 0.5984311103820801
Batch 25/64 loss: 0.1904754638671875
Batch 26/64 loss: 0.08983564376831055
Batch 27/64 loss: -0.283907413482666
Batch 28/64 loss: 0.1113286018371582
Batch 29/64 loss: 0.48033618927001953
Batch 30/64 loss: 0.4607667922973633
Batch 31/64 loss: 0.6709604263305664
Batch 32/64 loss: 0.2866835594177246
Batch 33/64 loss: 0.20587635040283203
Batch 34/64 loss: 0.7885913848876953
Batch 35/64 loss: -0.06796789169311523
Batch 36/64 loss: 0.12065744400024414
Batch 37/64 loss: 0.39296627044677734
Batch 38/64 loss: 0.1589503288269043
Batch 39/64 loss: 0.9227323532104492
Batch 40/64 loss: 0.11460256576538086
Batch 41/64 loss: 0.21978044509887695
Batch 42/64 loss: 0.8997855186462402
Batch 43/64 loss: 0.778437614440918
Batch 44/64 loss: 0.17018985748291016
Batch 45/64 loss: 0.12691593170166016
Batch 46/64 loss: -0.06926774978637695
Batch 47/64 loss: 0.2832818031311035
Batch 48/64 loss: 0.1647510528564453
Batch 49/64 loss: 0.17103862762451172
Batch 50/64 loss: 0.0428013801574707
Batch 51/64 loss: 0.3327169418334961
Batch 52/64 loss: 0.05303955078125
Batch 53/64 loss: 0.27238035202026367
Batch 54/64 loss: 0.18427181243896484
Batch 55/64 loss: 0.7442331314086914
Batch 56/64 loss: 0.3005495071411133
Batch 57/64 loss: 0.08416509628295898
Batch 58/64 loss: 0.35742998123168945
Batch 59/64 loss: 0.5292353630065918
Batch 60/64 loss: 0.6296782493591309
Batch 61/64 loss: 0.33789825439453125
Batch 62/64 loss: 0.4339580535888672
Batch 63/64 loss: 0.6079750061035156
Batch 64/64 loss: -2.4221272468566895
Epoch 60  Train loss: 0.2989882020389332  Val loss: 0.18844727716085427
Saving best model, epoch: 60
Epoch 61
-------------------------------
Batch 1/64 loss: -0.05777740478515625
Batch 2/64 loss: 0.07970476150512695
Batch 3/64 loss: 0.7168889045715332
Batch 4/64 loss: 0.1477670669555664
Batch 5/64 loss: 0.2531299591064453
Batch 6/64 loss: 0.2054605484008789
Batch 7/64 loss: 0.2363133430480957
Batch 8/64 loss: 0.4453120231628418
Batch 9/64 loss: 0.3840761184692383
Batch 10/64 loss: 0.16559505462646484
Batch 11/64 loss: 0.12870454788208008
Batch 12/64 loss: 0.0633687973022461
Batch 13/64 loss: 0.2039179801940918
Batch 14/64 loss: 0.0010666847229003906
Batch 15/64 loss: 0.3535490036010742
Batch 16/64 loss: 0.26905059814453125
Batch 17/64 loss: 0.07260942459106445
Batch 18/64 loss: 0.8433384895324707
Batch 19/64 loss: 1.086400032043457
Batch 20/64 loss: 0.635378360748291
Batch 21/64 loss: 0.1346888542175293
Batch 22/64 loss: 0.36210012435913086
Batch 23/64 loss: 0.49762439727783203
Batch 24/64 loss: 0.04279041290283203
Batch 25/64 loss: 0.40936994552612305
Batch 26/64 loss: 0.12160158157348633
Batch 27/64 loss: 0.31023645401000977
Batch 28/64 loss: 0.09824895858764648
Batch 29/64 loss: 0.32332277297973633
Batch 30/64 loss: 0.5856437683105469
Batch 31/64 loss: 0.07409429550170898
Batch 32/64 loss: 0.5761957168579102
Batch 33/64 loss: 0.3867487907409668
Batch 34/64 loss: 0.5236196517944336
Batch 35/64 loss: 0.13848447799682617
Batch 36/64 loss: 0.04106330871582031
Batch 37/64 loss: 0.15518712997436523
Batch 38/64 loss: 0.5312867164611816
Batch 39/64 loss: 0.48088836669921875
Batch 40/64 loss: 0.7428045272827148
Batch 41/64 loss: 0.1869363784790039
Batch 42/64 loss: 1.2345390319824219
Batch 43/64 loss: 0.08506011962890625
Batch 44/64 loss: 0.03031015396118164
Batch 45/64 loss: 0.24463939666748047
Batch 46/64 loss: 0.4485645294189453
Batch 47/64 loss: 0.48192358016967773
Batch 48/64 loss: 0.8564567565917969
Batch 49/64 loss: 0.2720456123352051
Batch 50/64 loss: 0.5151247978210449
Batch 51/64 loss: 0.14711380004882812
Batch 52/64 loss: 0.051081180572509766
Batch 53/64 loss: 0.5397367477416992
Batch 54/64 loss: 0.14599084854125977
Batch 55/64 loss: 0.5019259452819824
Batch 56/64 loss: 0.2900424003601074
Batch 57/64 loss: 0.3924379348754883
Batch 58/64 loss: 0.021058082580566406
Batch 59/64 loss: -0.10249090194702148
Batch 60/64 loss: 0.2550220489501953
Batch 61/64 loss: 0.16291522979736328
Batch 62/64 loss: 0.3480710983276367
Batch 63/64 loss: 0.048207759857177734
Batch 64/64 loss: -2.7747225761413574
Epoch 61  Train loss: 0.27992980620440316  Val loss: 0.14982001969904424
Saving best model, epoch: 61
Epoch 62
-------------------------------
Batch 1/64 loss: 0.5182809829711914
Batch 2/64 loss: 0.1729288101196289
Batch 3/64 loss: 0.47710084915161133
Batch 4/64 loss: 0.7065701484680176
Batch 5/64 loss: 0.14417171478271484
Batch 6/64 loss: -0.06431388854980469
Batch 7/64 loss: 0.33748769760131836
Batch 8/64 loss: 0.4734334945678711
Batch 9/64 loss: 0.18764305114746094
Batch 10/64 loss: 0.024853229522705078
Batch 11/64 loss: -0.06302118301391602
Batch 12/64 loss: 0.13111495971679688
Batch 13/64 loss: 0.2420973777770996
Batch 14/64 loss: 0.22509098052978516
Batch 15/64 loss: 0.6663417816162109
Batch 16/64 loss: 0.19444561004638672
Batch 17/64 loss: 0.4360623359680176
Batch 18/64 loss: 0.2874293327331543
Batch 19/64 loss: 0.763577938079834
Batch 20/64 loss: 0.22551965713500977
Batch 21/64 loss: -0.09273576736450195
Batch 22/64 loss: 0.218353271484375
Batch 23/64 loss: 0.9021997451782227
Batch 24/64 loss: 0.2516813278198242
Batch 25/64 loss: 0.2514677047729492
Batch 26/64 loss: 0.48649120330810547
Batch 27/64 loss: 0.20196056365966797
Batch 28/64 loss: 0.31941938400268555
Batch 29/64 loss: 0.014734268188476562
Batch 30/64 loss: 1.0144987106323242
Batch 31/64 loss: 0.1410975456237793
Batch 32/64 loss: 0.4860539436340332
Batch 33/64 loss: 0.4324674606323242
Batch 34/64 loss: 0.6564469337463379
Batch 35/64 loss: 0.15667009353637695
Batch 36/64 loss: -0.3527069091796875
Batch 37/64 loss: 0.5240716934204102
Batch 38/64 loss: 0.5164713859558105
Batch 39/64 loss: 0.5512495040893555
Batch 40/64 loss: 0.14102411270141602
Batch 41/64 loss: 0.07337808609008789
Batch 42/64 loss: 0.4210066795349121
Batch 43/64 loss: 0.1543259620666504
Batch 44/64 loss: 0.4888339042663574
Batch 45/64 loss: 0.28096818923950195
Batch 46/64 loss: 0.45026445388793945
Batch 47/64 loss: 0.9475808143615723
Batch 48/64 loss: 0.4574570655822754
Batch 49/64 loss: 0.8415193557739258
Batch 50/64 loss: 0.18759441375732422
Batch 51/64 loss: 0.4992866516113281
Batch 52/64 loss: 0.5094356536865234
Batch 53/64 loss: -0.046502113342285156
Batch 54/64 loss: 0.2836785316467285
Batch 55/64 loss: 0.628870964050293
Batch 56/64 loss: 0.2862114906311035
Batch 57/64 loss: 0.5289897918701172
Batch 58/64 loss: 0.3549799919128418
Batch 59/64 loss: -0.08626747131347656
Batch 60/64 loss: 0.3430161476135254
Batch 61/64 loss: -0.24318456649780273
Batch 62/64 loss: 0.09602069854736328
Batch 63/64 loss: 0.2723875045776367
Batch 64/64 loss: -3.102344036102295
Epoch 62  Train loss: 0.28722863103829177  Val loss: 0.25400244329393523
Epoch 63
-------------------------------
Batch 1/64 loss: 0.42157411575317383
Batch 2/64 loss: 0.8820662498474121
Batch 3/64 loss: 0.5022087097167969
Batch 4/64 loss: 0.5508584976196289
Batch 5/64 loss: 0.12913894653320312
Batch 6/64 loss: -0.11734485626220703
Batch 7/64 loss: 0.46464014053344727
Batch 8/64 loss: -0.023822307586669922
Batch 9/64 loss: 0.40004444122314453
Batch 10/64 loss: 0.21487140655517578
Batch 11/64 loss: 0.33325862884521484
Batch 12/64 loss: 0.5886821746826172
Batch 13/64 loss: 0.6439032554626465
Batch 14/64 loss: 0.5157122611999512
Batch 15/64 loss: 1.1933374404907227
Batch 16/64 loss: 0.31577396392822266
Batch 17/64 loss: 0.7126369476318359
Batch 18/64 loss: 0.6970663070678711
Batch 19/64 loss: 0.6738824844360352
Batch 20/64 loss: 0.24952936172485352
Batch 21/64 loss: -0.2113051414489746
Batch 22/64 loss: 0.2478184700012207
Batch 23/64 loss: 0.299954891204834
Batch 24/64 loss: -0.1054372787475586
Batch 25/64 loss: 0.0992422103881836
Batch 26/64 loss: 0.14833450317382812
Batch 27/64 loss: 0.6736798286437988
Batch 28/64 loss: 0.13712310791015625
Batch 29/64 loss: 0.2973804473876953
Batch 30/64 loss: 0.7524623870849609
Batch 31/64 loss: 0.8074789047241211
Batch 32/64 loss: 0.5623860359191895
Batch 33/64 loss: 0.24406719207763672
Batch 34/64 loss: 0.42481040954589844
Batch 35/64 loss: 0.31479358673095703
Batch 36/64 loss: 0.5409502983093262
Batch 37/64 loss: 0.21434307098388672
Batch 38/64 loss: 0.26073360443115234
Batch 39/64 loss: 0.12820863723754883
Batch 40/64 loss: 0.563133716583252
Batch 41/64 loss: 0.06391477584838867
Batch 42/64 loss: 0.47887659072875977
Batch 43/64 loss: -0.02200031280517578
Batch 44/64 loss: 0.4079318046569824
Batch 45/64 loss: 0.5433859825134277
Batch 46/64 loss: 0.597747802734375
Batch 47/64 loss: -0.09999322891235352
Batch 48/64 loss: -0.11705493927001953
Batch 49/64 loss: 0.27127838134765625
Batch 50/64 loss: 0.05111503601074219
Batch 51/64 loss: 0.30885839462280273
Batch 52/64 loss: 0.35427236557006836
Batch 53/64 loss: 0.07214117050170898
Batch 54/64 loss: 0.19151830673217773
Batch 55/64 loss: 0.35819435119628906
Batch 56/64 loss: 0.10094785690307617
Batch 57/64 loss: -0.2555866241455078
Batch 58/64 loss: 0.16538190841674805
Batch 59/64 loss: 0.29959774017333984
Batch 60/64 loss: 0.27188539505004883
Batch 61/64 loss: -0.0365142822265625
Batch 62/64 loss: 0.17487239837646484
Batch 63/64 loss: -0.17299509048461914
Batch 64/64 loss: -3.2897567749023438
Epoch 63  Train loss: 0.27119427849264705  Val loss: 0.09047490542696923
Saving best model, epoch: 63
Epoch 64
-------------------------------
Batch 1/64 loss: 0.08037328720092773
Batch 2/64 loss: 0.4324636459350586
Batch 3/64 loss: 0.1418304443359375
Batch 4/64 loss: -0.12847614288330078
Batch 5/64 loss: 0.4043459892272949
Batch 6/64 loss: 0.3439464569091797
Batch 7/64 loss: -0.14348793029785156
Batch 8/64 loss: 0.4343409538269043
Batch 9/64 loss: 0.13375091552734375
Batch 10/64 loss: 0.5071821212768555
Batch 11/64 loss: 0.273160457611084
Batch 12/64 loss: 0.04585409164428711
Batch 13/64 loss: -0.17698001861572266
Batch 14/64 loss: 0.4338817596435547
Batch 15/64 loss: 0.6340575218200684
Batch 16/64 loss: 0.09718894958496094
Batch 17/64 loss: 0.6053371429443359
Batch 18/64 loss: 0.2798738479614258
Batch 19/64 loss: 0.23911428451538086
Batch 20/64 loss: 0.2983708381652832
Batch 21/64 loss: -0.07082319259643555
Batch 22/64 loss: 0.2470874786376953
Batch 23/64 loss: 0.5876321792602539
Batch 24/64 loss: 0.5610570907592773
Batch 25/64 loss: 0.1120309829711914
Batch 26/64 loss: 0.2375812530517578
Batch 27/64 loss: -0.048305511474609375
Batch 28/64 loss: 0.37204551696777344
Batch 29/64 loss: 0.43099212646484375
Batch 30/64 loss: 0.28166627883911133
Batch 31/64 loss: 0.10429859161376953
Batch 32/64 loss: 0.16965246200561523
Batch 33/64 loss: 0.0899052619934082
Batch 34/64 loss: 0.3327641487121582
Batch 35/64 loss: 0.39554500579833984
Batch 36/64 loss: 0.5698223114013672
Batch 37/64 loss: 0.15485906600952148
Batch 38/64 loss: 0.2529621124267578
Batch 39/64 loss: 0.3772239685058594
Batch 40/64 loss: 0.6069278717041016
Batch 41/64 loss: 0.7267446517944336
Batch 42/64 loss: 0.48926401138305664
Batch 43/64 loss: 0.5505356788635254
Batch 44/64 loss: -0.1811075210571289
Batch 45/64 loss: 0.42769622802734375
Batch 46/64 loss: 0.2607154846191406
Batch 47/64 loss: 0.26278162002563477
Batch 48/64 loss: 0.10852336883544922
Batch 49/64 loss: 0.3713035583496094
Batch 50/64 loss: 0.20662784576416016
Batch 51/64 loss: 0.07594156265258789
Batch 52/64 loss: 0.08693313598632812
Batch 53/64 loss: 0.4920949935913086
Batch 54/64 loss: 0.49494028091430664
Batch 55/64 loss: 0.47516727447509766
Batch 56/64 loss: 0.7756009101867676
Batch 57/64 loss: 0.2726631164550781
Batch 58/64 loss: 0.3441319465637207
Batch 59/64 loss: 0.3041543960571289
Batch 60/64 loss: 0.17536592483520508
Batch 61/64 loss: 0.9618191719055176
Batch 62/64 loss: 0.22292041778564453
Batch 63/64 loss: 0.23996305465698242
Batch 64/64 loss: -3.270954132080078
Epoch 64  Train loss: 0.25713912365483305  Val loss: 0.20273571735395188
Epoch 65
-------------------------------
Batch 1/64 loss: -0.00669097900390625
Batch 2/64 loss: 0.6472530364990234
Batch 3/64 loss: 0.48373985290527344
Batch 4/64 loss: 0.21438980102539062
Batch 5/64 loss: 0.08544111251831055
Batch 6/64 loss: 0.6938281059265137
Batch 7/64 loss: 0.29392337799072266
Batch 8/64 loss: 0.27889537811279297
Batch 9/64 loss: 0.10099363327026367
Batch 10/64 loss: -0.03972434997558594
Batch 11/64 loss: 0.47995567321777344
Batch 12/64 loss: 0.24696874618530273
Batch 13/64 loss: 0.3119359016418457
Batch 14/64 loss: 0.018638134002685547
Batch 15/64 loss: 0.12075090408325195
Batch 16/64 loss: 0.21979045867919922
Batch 17/64 loss: 0.07182598114013672
Batch 18/64 loss: 0.39222288131713867
Batch 19/64 loss: 0.5530052185058594
Batch 20/64 loss: 0.05976104736328125
Batch 21/64 loss: 0.2453784942626953
Batch 22/64 loss: 0.16527843475341797
Batch 23/64 loss: 0.4709920883178711
Batch 24/64 loss: -0.042734622955322266
Batch 25/64 loss: 0.3502016067504883
Batch 26/64 loss: 0.3122677803039551
Batch 27/64 loss: 0.18725109100341797
Batch 28/64 loss: 0.14694976806640625
Batch 29/64 loss: 0.2793760299682617
Batch 30/64 loss: -0.11275625228881836
Batch 31/64 loss: 0.38014984130859375
Batch 32/64 loss: 0.1049346923828125
Batch 33/64 loss: 0.06275510787963867
Batch 34/64 loss: 0.5587911605834961
Batch 35/64 loss: 0.2772865295410156
Batch 36/64 loss: 0.09537792205810547
Batch 37/64 loss: -0.0404210090637207
Batch 38/64 loss: 0.6792073249816895
Batch 39/64 loss: 0.24668550491333008
Batch 40/64 loss: 0.4358048439025879
Batch 41/64 loss: 0.33099985122680664
Batch 42/64 loss: 0.3805551528930664
Batch 43/64 loss: -0.017873287200927734
Batch 44/64 loss: 0.3023710250854492
Batch 45/64 loss: 0.22406005859375
Batch 46/64 loss: 0.31104230880737305
Batch 47/64 loss: 0.2859311103820801
Batch 48/64 loss: 0.28019189834594727
Batch 49/64 loss: 0.22099924087524414
Batch 50/64 loss: 0.3016538619995117
Batch 51/64 loss: 0.47887086868286133
Batch 52/64 loss: 0.1789259910583496
Batch 53/64 loss: 0.5918254852294922
Batch 54/64 loss: 0.47339820861816406
Batch 55/64 loss: 0.008540153503417969
Batch 56/64 loss: 0.9643816947937012
Batch 57/64 loss: -0.1362895965576172
Batch 58/64 loss: 0.4399127960205078
Batch 59/64 loss: 0.1932511329650879
Batch 60/64 loss: 0.3218111991882324
Batch 61/64 loss: 1.1370878219604492
Batch 62/64 loss: 0.5005145072937012
Batch 63/64 loss: 0.2631649971008301
Batch 64/64 loss: -3.3606228828430176
Epoch 65  Train loss: 0.24383591483621037  Val loss: 0.262955236271075
Epoch 66
-------------------------------
Batch 1/64 loss: 0.24750947952270508
Batch 2/64 loss: 0.3480520248413086
Batch 3/64 loss: 0.6717209815979004
Batch 4/64 loss: 0.02818155288696289
Batch 5/64 loss: 0.5324440002441406
Batch 6/64 loss: 0.24559688568115234
Batch 7/64 loss: 0.20436429977416992
Batch 8/64 loss: 0.5147333145141602
Batch 9/64 loss: 0.15311288833618164
Batch 10/64 loss: 0.32291364669799805
Batch 11/64 loss: 0.11026954650878906
Batch 12/64 loss: 0.004673004150390625
Batch 13/64 loss: 0.18701410293579102
Batch 14/64 loss: 0.0928654670715332
Batch 15/64 loss: 0.045392513275146484
Batch 16/64 loss: 0.18395709991455078
Batch 17/64 loss: 0.34688663482666016
Batch 18/64 loss: 0.521543025970459
Batch 19/64 loss: 0.10431909561157227
Batch 20/64 loss: 0.46408939361572266
Batch 21/64 loss: 0.6205019950866699
Batch 22/64 loss: 0.32256412506103516
Batch 23/64 loss: 0.05878019332885742
Batch 24/64 loss: 0.28105592727661133
Batch 25/64 loss: 0.19377946853637695
Batch 26/64 loss: 0.038919925689697266
Batch 27/64 loss: 0.1314387321472168
Batch 28/64 loss: 0.5002636909484863
Batch 29/64 loss: 0.25543928146362305
Batch 30/64 loss: 0.5484843254089355
Batch 31/64 loss: 0.18173837661743164
Batch 32/64 loss: 0.6171026229858398
Batch 33/64 loss: -0.006719112396240234
Batch 34/64 loss: 0.42435455322265625
Batch 35/64 loss: 0.6013035774230957
Batch 36/64 loss: 0.009088516235351562
Batch 37/64 loss: 0.35164403915405273
Batch 38/64 loss: 0.30076122283935547
Batch 39/64 loss: 0.20075702667236328
Batch 40/64 loss: 0.06034421920776367
Batch 41/64 loss: -0.09956026077270508
Batch 42/64 loss: 0.2278585433959961
Batch 43/64 loss: 0.615849494934082
Batch 44/64 loss: -0.046652793884277344
Batch 45/64 loss: 0.49726438522338867
Batch 46/64 loss: 0.08243560791015625
Batch 47/64 loss: 0.9059281349182129
Batch 48/64 loss: 0.3493514060974121
Batch 49/64 loss: 0.4658026695251465
Batch 50/64 loss: 0.10887336730957031
Batch 51/64 loss: 0.37641000747680664
Batch 52/64 loss: 0.09946537017822266
Batch 53/64 loss: 0.6408371925354004
Batch 54/64 loss: 0.23389339447021484
Batch 55/64 loss: 0.7302980422973633
Batch 56/64 loss: -0.09880828857421875
Batch 57/64 loss: 0.6261453628540039
Batch 58/64 loss: 0.3990325927734375
Batch 59/64 loss: 0.12778282165527344
Batch 60/64 loss: 0.770592212677002
Batch 61/64 loss: -0.3174872398376465
Batch 62/64 loss: 0.07094955444335938
Batch 63/64 loss: 0.07370758056640625
Batch 64/64 loss: -3.4893250465393066
Epoch 66  Train loss: 0.23912497314752318  Val loss: 0.09603454484972347
Epoch 67
-------------------------------
Batch 1/64 loss: 0.31160497665405273
Batch 2/64 loss: -0.050427913665771484
Batch 3/64 loss: 0.6500544548034668
Batch 4/64 loss: 0.5082716941833496
Batch 5/64 loss: 0.24683618545532227
Batch 6/64 loss: 0.3182811737060547
Batch 7/64 loss: -0.16072511672973633
Batch 8/64 loss: 0.1874241828918457
Batch 9/64 loss: 0.6140146255493164
Batch 10/64 loss: -0.16828203201293945
Batch 11/64 loss: 0.07426214218139648
Batch 12/64 loss: 0.25493812561035156
Batch 13/64 loss: 0.7689743041992188
Batch 14/64 loss: 0.5390162467956543
Batch 15/64 loss: 0.1449580192565918
Batch 16/64 loss: 0.06430530548095703
Batch 17/64 loss: 0.3427119255065918
Batch 18/64 loss: 0.6000351905822754
Batch 19/64 loss: 0.8882126808166504
Batch 20/64 loss: 0.5547065734863281
Batch 21/64 loss: 0.15324640274047852
Batch 22/64 loss: 0.1620645523071289
Batch 23/64 loss: 0.16957998275756836
Batch 24/64 loss: 0.6057953834533691
Batch 25/64 loss: 0.34250974655151367
Batch 26/64 loss: 0.11976814270019531
Batch 27/64 loss: 0.5065708160400391
Batch 28/64 loss: 0.04779243469238281
Batch 29/64 loss: 0.15490293502807617
Batch 30/64 loss: 0.19139575958251953
Batch 31/64 loss: -0.12150716781616211
Batch 32/64 loss: -0.1529536247253418
Batch 33/64 loss: 0.25096940994262695
Batch 34/64 loss: 0.22499322891235352
Batch 35/64 loss: 0.22588729858398438
Batch 36/64 loss: -0.05442667007446289
Batch 37/64 loss: 0.4365224838256836
Batch 38/64 loss: -0.02775287628173828
Batch 39/64 loss: 0.35146284103393555
Batch 40/64 loss: -0.005194187164306641
Batch 41/64 loss: 0.4181332588195801
Batch 42/64 loss: 0.15505743026733398
Batch 43/64 loss: 0.17897796630859375
Batch 44/64 loss: 0.6431498527526855
Batch 45/64 loss: 0.00870513916015625
Batch 46/64 loss: 0.3129754066467285
Batch 47/64 loss: -0.09805011749267578
Batch 48/64 loss: 0.06054115295410156
Batch 49/64 loss: 0.22453546524047852
Batch 50/64 loss: 0.19059419631958008
Batch 51/64 loss: -0.05962181091308594
Batch 52/64 loss: 0.13537311553955078
Batch 53/64 loss: 0.4185357093811035
Batch 54/64 loss: 0.5211358070373535
Batch 55/64 loss: -0.0554203987121582
Batch 56/64 loss: 0.018259048461914062
Batch 57/64 loss: 0.41753149032592773
Batch 58/64 loss: 0.651181697845459
Batch 59/64 loss: 0.48530101776123047
Batch 60/64 loss: 0.7968711853027344
Batch 61/64 loss: 0.5662860870361328
Batch 62/64 loss: 0.1687479019165039
Batch 63/64 loss: 0.04204559326171875
Batch 64/64 loss: -3.4336295127868652
Epoch 67  Train loss: 0.21798311682308422  Val loss: 0.1761565454227408
Epoch 68
-------------------------------
Batch 1/64 loss: 0.6447415351867676
Batch 2/64 loss: 0.3496098518371582
Batch 3/64 loss: 0.21616697311401367
Batch 4/64 loss: 0.28046083450317383
Batch 5/64 loss: 0.3085289001464844
Batch 6/64 loss: 0.41588687896728516
Batch 7/64 loss: 0.6624083518981934
Batch 8/64 loss: 0.3215012550354004
Batch 9/64 loss: -0.038127899169921875
Batch 10/64 loss: 0.27825021743774414
Batch 11/64 loss: -0.01736307144165039
Batch 12/64 loss: 0.3145771026611328
Batch 13/64 loss: 0.7409501075744629
Batch 14/64 loss: 0.23870372772216797
Batch 15/64 loss: 0.32139015197753906
Batch 16/64 loss: 0.32735395431518555
Batch 17/64 loss: 0.42841482162475586
Batch 18/64 loss: 0.1514430046081543
Batch 19/64 loss: 0.2349228858947754
Batch 20/64 loss: 0.4505133628845215
Batch 21/64 loss: 0.20044183731079102
Batch 22/64 loss: 0.13761329650878906
Batch 23/64 loss: 0.36073780059814453
Batch 24/64 loss: -0.010931015014648438
Batch 25/64 loss: 0.4983334541320801
Batch 26/64 loss: 0.10631227493286133
Batch 27/64 loss: 0.3566598892211914
Batch 28/64 loss: -0.034094810485839844
Batch 29/64 loss: 0.1535959243774414
Batch 30/64 loss: 0.11252307891845703
Batch 31/64 loss: 0.05615377426147461
Batch 32/64 loss: 0.25464439392089844
Batch 33/64 loss: 0.7809906005859375
Batch 34/64 loss: -0.022020816802978516
Batch 35/64 loss: 0.35157012939453125
Batch 36/64 loss: 0.38153839111328125
Batch 37/64 loss: 0.08518791198730469
Batch 38/64 loss: 0.5672402381896973
Batch 39/64 loss: 0.39968252182006836
Batch 40/64 loss: 0.3763127326965332
Batch 41/64 loss: 0.5652241706848145
Batch 42/64 loss: 0.07297849655151367
Batch 43/64 loss: 0.42457103729248047
Batch 44/64 loss: 0.0404667854309082
Batch 45/64 loss: 0.19820022583007812
Batch 46/64 loss: -0.115692138671875
Batch 47/64 loss: 0.5574727058410645
Batch 48/64 loss: 0.08602094650268555
Batch 49/64 loss: 0.3055233955383301
Batch 50/64 loss: 0.3447704315185547
Batch 51/64 loss: 0.2235260009765625
Batch 52/64 loss: 0.35091161727905273
Batch 53/64 loss: 0.3435378074645996
Batch 54/64 loss: 0.33574962615966797
Batch 55/64 loss: -0.11273717880249023
Batch 56/64 loss: 0.01983499526977539
Batch 57/64 loss: 0.11016702651977539
Batch 58/64 loss: -0.18656253814697266
Batch 59/64 loss: 0.20530319213867188
Batch 60/64 loss: -0.03163957595825195
Batch 61/64 loss: 0.6006507873535156
Batch 62/64 loss: 0.23176813125610352
Batch 63/64 loss: 0.32246875762939453
Batch 64/64 loss: -2.554114818572998
Epoch 68  Train loss: 0.23089808856739719  Val loss: 0.09712764569574206
Epoch 69
-------------------------------
Batch 1/64 loss: 0.05077362060546875
Batch 2/64 loss: 0.5847353935241699
Batch 3/64 loss: 0.9506096839904785
Batch 4/64 loss: 0.08586359024047852
Batch 5/64 loss: -0.15436458587646484
Batch 6/64 loss: 0.5855364799499512
Batch 7/64 loss: 0.33542633056640625
Batch 8/64 loss: 0.019195079803466797
Batch 9/64 loss: 0.12609100341796875
Batch 10/64 loss: -0.03475761413574219
Batch 11/64 loss: -0.03740835189819336
Batch 12/64 loss: 0.02193307876586914
Batch 13/64 loss: 0.5366291999816895
Batch 14/64 loss: -0.03294706344604492
Batch 15/64 loss: 0.22676897048950195
Batch 16/64 loss: 0.25417661666870117
Batch 17/64 loss: -0.04342794418334961
Batch 18/64 loss: 0.2369213104248047
Batch 19/64 loss: 0.19542694091796875
Batch 20/64 loss: 0.4638528823852539
Batch 21/64 loss: 0.21511030197143555
Batch 22/64 loss: 0.11461019515991211
Batch 23/64 loss: 0.21244096755981445
Batch 24/64 loss: -0.051163673400878906
Batch 25/64 loss: -0.0858144760131836
Batch 26/64 loss: 0.30576372146606445
Batch 27/64 loss: 0.9173140525817871
Batch 28/64 loss: 0.4168052673339844
Batch 29/64 loss: 0.6074872016906738
Batch 30/64 loss: 0.4629826545715332
Batch 31/64 loss: 0.19765758514404297
Batch 32/64 loss: -0.06363821029663086
Batch 33/64 loss: 0.22721099853515625
Batch 34/64 loss: 0.27797603607177734
Batch 35/64 loss: -0.04742860794067383
Batch 36/64 loss: 0.5902104377746582
Batch 37/64 loss: -0.20720958709716797
Batch 38/64 loss: 0.36039113998413086
Batch 39/64 loss: 0.10210275650024414
Batch 40/64 loss: 0.29974365234375
Batch 41/64 loss: 0.3411898612976074
Batch 42/64 loss: -0.14457035064697266
Batch 43/64 loss: 0.7153792381286621
Batch 44/64 loss: 0.07564353942871094
Batch 45/64 loss: -0.1691269874572754
Batch 46/64 loss: 0.20612144470214844
Batch 47/64 loss: 0.5436124801635742
Batch 48/64 loss: 0.35266685485839844
Batch 49/64 loss: 0.1258840560913086
Batch 50/64 loss: 0.4589385986328125
Batch 51/64 loss: -0.03836679458618164
Batch 52/64 loss: 0.6380987167358398
Batch 53/64 loss: 0.15597200393676758
Batch 54/64 loss: -0.036513328552246094
Batch 55/64 loss: 0.6459450721740723
Batch 56/64 loss: 0.15934228897094727
Batch 57/64 loss: 0.1575922966003418
Batch 58/64 loss: 0.12892866134643555
Batch 59/64 loss: 0.43319177627563477
Batch 60/64 loss: 0.5015859603881836
Batch 61/64 loss: 0.39685583114624023
Batch 62/64 loss: 0.4330906867980957
Batch 63/64 loss: 0.2659616470336914
Batch 64/64 loss: -3.6109132766723633
Epoch 69  Train loss: 0.2017698138367896  Val loss: 0.07550143212387242
Saving best model, epoch: 69
Epoch 70
-------------------------------
Batch 1/64 loss: 0.1920146942138672
Batch 2/64 loss: 0.8239927291870117
Batch 3/64 loss: 0.2790350914001465
Batch 4/64 loss: 0.13401222229003906
Batch 5/64 loss: 0.43777894973754883
Batch 6/64 loss: -0.02863454818725586
Batch 7/64 loss: 0.6074995994567871
Batch 8/64 loss: 0.8101835250854492
Batch 9/64 loss: 0.2038745880126953
Batch 10/64 loss: 0.25754356384277344
Batch 11/64 loss: 0.18871259689331055
Batch 12/64 loss: 0.2407069206237793
Batch 13/64 loss: 0.14735984802246094
Batch 14/64 loss: 0.4144759178161621
Batch 15/64 loss: 0.5576686859130859
Batch 16/64 loss: 0.2888603210449219
Batch 17/64 loss: 0.15014314651489258
Batch 18/64 loss: 0.271273136138916
Batch 19/64 loss: 0.04294443130493164
Batch 20/64 loss: 0.011143684387207031
Batch 21/64 loss: 0.1586146354675293
Batch 22/64 loss: 0.5752348899841309
Batch 23/64 loss: 0.43541717529296875
Batch 24/64 loss: 0.13600492477416992
Batch 25/64 loss: -0.17187070846557617
Batch 26/64 loss: 0.2649822235107422
Batch 27/64 loss: 0.20434188842773438
Batch 28/64 loss: 0.4681410789489746
Batch 29/64 loss: 0.29500675201416016
Batch 30/64 loss: 0.5989828109741211
Batch 31/64 loss: 0.43807315826416016
Batch 32/64 loss: 0.006659030914306641
Batch 33/64 loss: 0.44931983947753906
Batch 34/64 loss: 0.3667459487915039
Batch 35/64 loss: 0.5559115409851074
Batch 36/64 loss: 0.5476622581481934
Batch 37/64 loss: 0.33396339416503906
Batch 38/64 loss: 0.011208057403564453
Batch 39/64 loss: 0.19564008712768555
Batch 40/64 loss: 0.15804529190063477
Batch 41/64 loss: -0.07741403579711914
Batch 42/64 loss: 0.22678709030151367
Batch 43/64 loss: 0.21976757049560547
Batch 44/64 loss: 0.31711816787719727
Batch 45/64 loss: 0.6739521026611328
Batch 46/64 loss: -0.09918355941772461
Batch 47/64 loss: 0.4846992492675781
Batch 48/64 loss: 0.23152637481689453
Batch 49/64 loss: 0.20247125625610352
Batch 50/64 loss: 0.11557245254516602
Batch 51/64 loss: 0.2711677551269531
Batch 52/64 loss: 0.27254772186279297
Batch 53/64 loss: 0.24185800552368164
Batch 54/64 loss: 0.5528950691223145
Batch 55/64 loss: 0.09161996841430664
Batch 56/64 loss: 0.06067657470703125
Batch 57/64 loss: 0.08251428604125977
Batch 58/64 loss: 0.44039392471313477
Batch 59/64 loss: 0.10236120223999023
Batch 60/64 loss: 0.7530021667480469
Batch 61/64 loss: -0.004258632659912109
Batch 62/64 loss: 0.24651336669921875
Batch 63/64 loss: 0.42021656036376953
Batch 64/64 loss: -3.0561914443969727
Epoch 70  Train loss: 0.24460132449280983  Val loss: 0.15768896673143523
Epoch 71
-------------------------------
Batch 1/64 loss: 0.45897436141967773
Batch 2/64 loss: 0.05349016189575195
Batch 3/64 loss: 0.16200637817382812
Batch 4/64 loss: 0.1734004020690918
Batch 5/64 loss: 0.1671290397644043
Batch 6/64 loss: 0.19559669494628906
Batch 7/64 loss: 0.18084192276000977
Batch 8/64 loss: 0.6499605178833008
Batch 9/64 loss: 0.25536346435546875
Batch 10/64 loss: 0.43063831329345703
Batch 11/64 loss: 0.07744550704956055
Batch 12/64 loss: 1.0901298522949219
Batch 13/64 loss: -0.17948102951049805
Batch 14/64 loss: -0.11810159683227539
Batch 15/64 loss: 0.1750321388244629
Batch 16/64 loss: 0.2759709358215332
Batch 17/64 loss: 0.47265100479125977
Batch 18/64 loss: 0.10629892349243164
Batch 19/64 loss: 1.1046204566955566
Batch 20/64 loss: 0.2665081024169922
Batch 21/64 loss: 0.26067352294921875
Batch 22/64 loss: 0.6605520248413086
Batch 23/64 loss: 0.3009915351867676
Batch 24/64 loss: 0.1370706558227539
Batch 25/64 loss: 0.04792451858520508
Batch 26/64 loss: 0.2760472297668457
Batch 27/64 loss: 0.2123274803161621
Batch 28/64 loss: 0.3244013786315918
Batch 29/64 loss: 0.4356498718261719
Batch 30/64 loss: 0.44660234451293945
Batch 31/64 loss: 0.20431089401245117
Batch 32/64 loss: 0.02826976776123047
Batch 33/64 loss: -0.1501903533935547
Batch 34/64 loss: 0.2830467224121094
Batch 35/64 loss: 0.4107346534729004
Batch 36/64 loss: 0.47089195251464844
Batch 37/64 loss: 0.08728361129760742
Batch 38/64 loss: 0.7928671836853027
Batch 39/64 loss: 0.30071353912353516
Batch 40/64 loss: 0.10237836837768555
Batch 41/64 loss: 0.021618366241455078
Batch 42/64 loss: 0.34164857864379883
Batch 43/64 loss: 0.10007905960083008
Batch 44/64 loss: 0.011897087097167969
Batch 45/64 loss: 0.1606888771057129
Batch 46/64 loss: 0.47162485122680664
Batch 47/64 loss: 0.5410370826721191
Batch 48/64 loss: 0.7066235542297363
Batch 49/64 loss: 0.12126827239990234
Batch 50/64 loss: 0.5699214935302734
Batch 51/64 loss: 0.8675045967102051
Batch 52/64 loss: 0.2890462875366211
Batch 53/64 loss: 0.21112823486328125
Batch 54/64 loss: 0.35436534881591797
Batch 55/64 loss: 0.36378002166748047
Batch 56/64 loss: 0.39433956146240234
Batch 57/64 loss: 0.002266407012939453
Batch 58/64 loss: 0.14043903350830078
Batch 59/64 loss: 0.6281418800354004
Batch 60/64 loss: 0.24323749542236328
Batch 61/64 loss: 0.3167386054992676
Batch 62/64 loss: 0.09147453308105469
Batch 63/64 loss: 0.5769920349121094
Batch 64/64 loss: -3.0155324935913086
Epoch 71  Train loss: 0.26502328386493756  Val loss: 0.07857833285511974
Epoch 72
-------------------------------
Batch 1/64 loss: 0.4575462341308594
Batch 2/64 loss: 0.8382401466369629
Batch 3/64 loss: -0.14041948318481445
Batch 4/64 loss: 0.4479966163635254
Batch 5/64 loss: 0.1072988510131836
Batch 6/64 loss: 0.13292169570922852
Batch 7/64 loss: 0.1672835350036621
Batch 8/64 loss: 0.09105491638183594
Batch 9/64 loss: 0.015190601348876953
Batch 10/64 loss: 0.26952409744262695
Batch 11/64 loss: -0.30186986923217773
Batch 12/64 loss: 0.4835042953491211
Batch 13/64 loss: 0.1640019416809082
Batch 14/64 loss: -0.02703237533569336
Batch 15/64 loss: 0.031151771545410156
Batch 16/64 loss: 0.15103530883789062
Batch 17/64 loss: 0.3432126045227051
Batch 18/64 loss: 0.6547355651855469
Batch 19/64 loss: 0.10885429382324219
Batch 20/64 loss: 0.30521297454833984
Batch 21/64 loss: 0.3795809745788574
Batch 22/64 loss: 0.7554483413696289
Batch 23/64 loss: -0.2493448257446289
Batch 24/64 loss: 0.29905080795288086
Batch 25/64 loss: 0.9538421630859375
Batch 26/64 loss: 0.11313104629516602
Batch 27/64 loss: 0.37149572372436523
Batch 28/64 loss: 0.2228851318359375
Batch 29/64 loss: -0.03787040710449219
Batch 30/64 loss: 0.18109512329101562
Batch 31/64 loss: 0.030109882354736328
Batch 32/64 loss: 0.679837703704834
Batch 33/64 loss: 0.2719278335571289
Batch 34/64 loss: -0.14990234375
Batch 35/64 loss: -0.019542217254638672
Batch 36/64 loss: -0.0027976036071777344
Batch 37/64 loss: 0.11749696731567383
Batch 38/64 loss: 0.6962018013000488
Batch 39/64 loss: 0.029085636138916016
Batch 40/64 loss: -0.12797927856445312
Batch 41/64 loss: 0.680788516998291
Batch 42/64 loss: 0.7812838554382324
Batch 43/64 loss: 0.0991983413696289
Batch 44/64 loss: -0.30364370346069336
Batch 45/64 loss: 0.16500568389892578
Batch 46/64 loss: 0.12179374694824219
Batch 47/64 loss: 0.1430659294128418
Batch 48/64 loss: 0.34208059310913086
Batch 49/64 loss: -0.10210180282592773
Batch 50/64 loss: -0.1741018295288086
Batch 51/64 loss: 0.4676513671875
Batch 52/64 loss: 0.34905004501342773
Batch 53/64 loss: 0.15163612365722656
Batch 54/64 loss: -0.14315223693847656
Batch 55/64 loss: 0.2797250747680664
Batch 56/64 loss: -0.11921215057373047
Batch 57/64 loss: 0.5690779685974121
Batch 58/64 loss: 0.2826390266418457
Batch 59/64 loss: 0.1636514663696289
Batch 60/64 loss: 0.06528663635253906
Batch 61/64 loss: -0.050547122955322266
Batch 62/64 loss: 0.020372867584228516
Batch 63/64 loss: 0.05062723159790039
Batch 64/64 loss: -3.1127710342407227
Epoch 72  Train loss: 0.16186342800364775  Val loss: -0.008913544854757302
Saving best model, epoch: 72
Epoch 73
-------------------------------
Batch 1/64 loss: 0.011269092559814453
Batch 2/64 loss: 1.0230979919433594
Batch 3/64 loss: -0.059673309326171875
Batch 4/64 loss: 0.5263423919677734
Batch 5/64 loss: 0.03779125213623047
Batch 6/64 loss: 0.6146416664123535
Batch 7/64 loss: -0.032225608825683594
Batch 8/64 loss: 0.42456531524658203
Batch 9/64 loss: -0.17176151275634766
Batch 10/64 loss: 0.6619930267333984
Batch 11/64 loss: 0.2258453369140625
Batch 12/64 loss: -0.3584108352661133
Batch 13/64 loss: -0.08614110946655273
Batch 14/64 loss: -0.013648509979248047
Batch 15/64 loss: 0.16692113876342773
Batch 16/64 loss: -0.1140756607055664
Batch 17/64 loss: 0.39356088638305664
Batch 18/64 loss: 0.24074649810791016
Batch 19/64 loss: 0.4352889060974121
Batch 20/64 loss: -0.035809993743896484
Batch 21/64 loss: 0.15353965759277344
Batch 22/64 loss: 0.17786455154418945
Batch 23/64 loss: 0.11407661437988281
Batch 24/64 loss: 0.5641231536865234
Batch 25/64 loss: -0.09635114669799805
Batch 26/64 loss: 0.2958674430847168
Batch 27/64 loss: 0.04053688049316406
Batch 28/64 loss: 0.48029327392578125
Batch 29/64 loss: 0.3600177764892578
Batch 30/64 loss: 0.5457754135131836
Batch 31/64 loss: 0.30272340774536133
Batch 32/64 loss: 0.19108104705810547
Batch 33/64 loss: 0.32598352432250977
Batch 34/64 loss: 0.10375499725341797
Batch 35/64 loss: 0.10002613067626953
Batch 36/64 loss: 0.36852073669433594
Batch 37/64 loss: -0.1678175926208496
Batch 38/64 loss: 0.1742234230041504
Batch 39/64 loss: -0.12742042541503906
Batch 40/64 loss: 0.14234685897827148
Batch 41/64 loss: -0.11593294143676758
Batch 42/64 loss: -0.1904129981994629
Batch 43/64 loss: -0.2449488639831543
Batch 44/64 loss: 0.0738515853881836
Batch 45/64 loss: 0.7126879692077637
Batch 46/64 loss: 0.30903005599975586
Batch 47/64 loss: 0.21490955352783203
Batch 48/64 loss: 0.13669490814208984
Batch 49/64 loss: 0.04311561584472656
Batch 50/64 loss: 0.04241943359375
Batch 51/64 loss: 0.5156984329223633
Batch 52/64 loss: 0.028038501739501953
Batch 53/64 loss: 0.1599879264831543
Batch 54/64 loss: 0.7615861892700195
Batch 55/64 loss: 0.12471580505371094
Batch 56/64 loss: -0.010268688201904297
Batch 57/64 loss: 0.3849825859069824
Batch 58/64 loss: 0.15704011917114258
Batch 59/64 loss: 0.753532886505127
Batch 60/64 loss: -0.17739391326904297
Batch 61/64 loss: 0.28942251205444336
Batch 62/64 loss: 0.06395864486694336
Batch 63/64 loss: 0.19303321838378906
Batch 64/64 loss: -3.597935676574707
Epoch 73  Train loss: 0.1484985014971565  Val loss: 0.16781569018806378
Epoch 74
-------------------------------
Batch 1/64 loss: 0.19963312149047852
Batch 2/64 loss: 0.4182877540588379
Batch 3/64 loss: 0.10326576232910156
Batch 4/64 loss: 0.4954514503479004
Batch 5/64 loss: 0.6363487243652344
Batch 6/64 loss: 0.5200996398925781
Batch 7/64 loss: 0.008807182312011719
Batch 8/64 loss: -0.24370956420898438
Batch 9/64 loss: 0.4448118209838867
Batch 10/64 loss: 0.3047657012939453
Batch 11/64 loss: 0.2254171371459961
Batch 12/64 loss: 0.3234214782714844
Batch 13/64 loss: 0.32741403579711914
Batch 14/64 loss: -0.039684295654296875
Batch 15/64 loss: 0.2311081886291504
Batch 16/64 loss: 0.6754484176635742
Batch 17/64 loss: 0.4792819023132324
Batch 18/64 loss: 0.08917474746704102
Batch 19/64 loss: 0.7361245155334473
Batch 20/64 loss: 0.17553997039794922
Batch 21/64 loss: -0.20085620880126953
Batch 22/64 loss: 0.12128448486328125
Batch 23/64 loss: 0.7095956802368164
Batch 24/64 loss: 0.47466230392456055
Batch 25/64 loss: 0.1258225440979004
Batch 26/64 loss: 0.5652036666870117
Batch 27/64 loss: 0.2553400993347168
Batch 28/64 loss: 0.21372461318969727
Batch 29/64 loss: 0.29536008834838867
Batch 30/64 loss: 0.5904407501220703
Batch 31/64 loss: -0.25406694412231445
Batch 32/64 loss: 0.09134101867675781
Batch 33/64 loss: 0.08637714385986328
Batch 34/64 loss: 0.18711280822753906
Batch 35/64 loss: 0.5679435729980469
Batch 36/64 loss: -0.15775012969970703
Batch 37/64 loss: 0.5263996124267578
Batch 38/64 loss: 0.15283632278442383
Batch 39/64 loss: 0.0317988395690918
Batch 40/64 loss: 0.07681751251220703
Batch 41/64 loss: -0.07204008102416992
Batch 42/64 loss: 0.06300163269042969
Batch 43/64 loss: 0.4461193084716797
Batch 44/64 loss: -0.12085914611816406
Batch 45/64 loss: -0.1535487174987793
Batch 46/64 loss: 0.08378410339355469
Batch 47/64 loss: 0.5391707420349121
Batch 48/64 loss: 0.3332347869873047
Batch 49/64 loss: 0.26157474517822266
Batch 50/64 loss: -0.14326858520507812
Batch 51/64 loss: 0.3989253044128418
Batch 52/64 loss: -0.11708545684814453
Batch 53/64 loss: 0.1127476692199707
Batch 54/64 loss: 0.04305458068847656
Batch 55/64 loss: -0.014385223388671875
Batch 56/64 loss: -0.09134674072265625
Batch 57/64 loss: -0.03162527084350586
Batch 58/64 loss: 0.36873960494995117
Batch 59/64 loss: 0.7295236587524414
Batch 60/64 loss: 0.08879804611206055
Batch 61/64 loss: 0.14286565780639648
Batch 62/64 loss: -0.08975648880004883
Batch 63/64 loss: 0.22879981994628906
Batch 64/64 loss: -3.701110363006592
Epoch 74  Train loss: 0.1694272415310729  Val loss: 0.013854469220662854
Epoch 75
-------------------------------
Batch 1/64 loss: -0.03231191635131836
Batch 2/64 loss: 0.29639244079589844
Batch 3/64 loss: 0.20232152938842773
Batch 4/64 loss: 0.5530605316162109
Batch 5/64 loss: 0.41658639907836914
Batch 6/64 loss: 0.3662996292114258
Batch 7/64 loss: 0.49929141998291016
Batch 8/64 loss: 0.29711437225341797
Batch 9/64 loss: 0.2841644287109375
Batch 10/64 loss: -0.08507442474365234
Batch 11/64 loss: 0.3292121887207031
Batch 12/64 loss: -0.05027008056640625
Batch 13/64 loss: 0.23049640655517578
Batch 14/64 loss: -0.10201883316040039
Batch 15/64 loss: 0.23761367797851562
Batch 16/64 loss: 0.19589471817016602
Batch 17/64 loss: -0.07654523849487305
Batch 18/64 loss: -0.14716053009033203
Batch 19/64 loss: 0.2063126564025879
Batch 20/64 loss: 0.11113452911376953
Batch 21/64 loss: 0.30492591857910156
Batch 22/64 loss: 0.0670166015625
Batch 23/64 loss: 0.12904977798461914
Batch 24/64 loss: 0.2649526596069336
Batch 25/64 loss: 0.023834705352783203
Batch 26/64 loss: 0.3260197639465332
Batch 27/64 loss: 0.44620418548583984
Batch 28/64 loss: -0.0583806037902832
Batch 29/64 loss: 0.21575593948364258
Batch 30/64 loss: 0.721562385559082
Batch 31/64 loss: -0.250335693359375
Batch 32/64 loss: 0.05081653594970703
Batch 33/64 loss: -0.015249252319335938
Batch 34/64 loss: 0.6196584701538086
Batch 35/64 loss: 0.03670692443847656
Batch 36/64 loss: 0.2530689239501953
Batch 37/64 loss: 0.23209762573242188
Batch 38/64 loss: 0.32250356674194336
Batch 39/64 loss: 0.27768945693969727
Batch 40/64 loss: 0.25110530853271484
Batch 41/64 loss: 0.5952634811401367
Batch 42/64 loss: 0.15059137344360352
Batch 43/64 loss: 0.0971841812133789
Batch 44/64 loss: 0.22285127639770508
Batch 45/64 loss: 0.8673076629638672
Batch 46/64 loss: 0.0995020866394043
Batch 47/64 loss: 0.03600883483886719
Batch 48/64 loss: -0.10403060913085938
Batch 49/64 loss: 0.1275196075439453
Batch 50/64 loss: 0.036862850189208984
Batch 51/64 loss: 0.23346519470214844
Batch 52/64 loss: 0.6188268661499023
Batch 53/64 loss: 0.19377565383911133
Batch 54/64 loss: -0.13607072830200195
Batch 55/64 loss: 0.010136127471923828
Batch 56/64 loss: 0.2510814666748047
Batch 57/64 loss: -0.012801170349121094
Batch 58/64 loss: 0.04695749282836914
Batch 59/64 loss: 0.017908573150634766
Batch 60/64 loss: 0.2867269515991211
Batch 61/64 loss: -0.10187435150146484
Batch 62/64 loss: 0.45768260955810547
Batch 63/64 loss: 0.018707275390625
Batch 64/64 loss: -3.496532440185547
Epoch 75  Train loss: 0.14655216441434973  Val loss: 0.09941402579500913
Epoch 76
-------------------------------
Batch 1/64 loss: 0.6936659812927246
Batch 2/64 loss: -0.06247520446777344
Batch 3/64 loss: 0.2901611328125
Batch 4/64 loss: 0.02693033218383789
Batch 5/64 loss: 0.19042491912841797
Batch 6/64 loss: 0.03720378875732422
Batch 7/64 loss: 0.8765583038330078
Batch 8/64 loss: 0.17249441146850586
Batch 9/64 loss: -0.05420351028442383
Batch 10/64 loss: 0.04574251174926758
Batch 11/64 loss: -0.11887073516845703
Batch 12/64 loss: 0.4383101463317871
Batch 13/64 loss: 0.24095630645751953
Batch 14/64 loss: 0.2234210968017578
Batch 15/64 loss: 0.476015567779541
Batch 16/64 loss: 0.11814594268798828
Batch 17/64 loss: -0.04604291915893555
Batch 18/64 loss: 0.06766510009765625
Batch 19/64 loss: 0.10857725143432617
Batch 20/64 loss: 0.37720775604248047
Batch 21/64 loss: 0.36690568923950195
Batch 22/64 loss: 0.22712135314941406
Batch 23/64 loss: -0.026224613189697266
Batch 24/64 loss: -0.016046524047851562
Batch 25/64 loss: 0.6693181991577148
Batch 26/64 loss: -0.10239982604980469
Batch 27/64 loss: 0.6845107078552246
Batch 28/64 loss: 0.2107834815979004
Batch 29/64 loss: 0.3708682060241699
Batch 30/64 loss: 0.21693897247314453
Batch 31/64 loss: 0.5945596694946289
Batch 32/64 loss: 0.4102640151977539
Batch 33/64 loss: 0.37529611587524414
Batch 34/64 loss: 0.41875743865966797
Batch 35/64 loss: 0.2951202392578125
Batch 36/64 loss: 0.2574758529663086
Batch 37/64 loss: 0.5841522216796875
Batch 38/64 loss: -0.026891708374023438
Batch 39/64 loss: -0.21208667755126953
Batch 40/64 loss: 0.44543886184692383
Batch 41/64 loss: 0.21173763275146484
Batch 42/64 loss: 0.7534260749816895
Batch 43/64 loss: 0.3629722595214844
Batch 44/64 loss: 0.6649293899536133
Batch 45/64 loss: 0.4290752410888672
Batch 46/64 loss: 0.29072999954223633
Batch 47/64 loss: 0.035121917724609375
Batch 48/64 loss: 0.18316984176635742
Batch 49/64 loss: -0.10586166381835938
Batch 50/64 loss: 0.10450172424316406
Batch 51/64 loss: 0.20261573791503906
Batch 52/64 loss: 0.3522348403930664
Batch 53/64 loss: 0.4533238410949707
Batch 54/64 loss: 0.5455336570739746
Batch 55/64 loss: 0.3849039077758789
Batch 56/64 loss: 0.20817089080810547
Batch 57/64 loss: -0.3030557632446289
Batch 58/64 loss: 0.014277458190917969
Batch 59/64 loss: 0.127960205078125
Batch 60/64 loss: 0.1613607406616211
Batch 61/64 loss: 0.053818702697753906
Batch 62/64 loss: 0.3725314140319824
Batch 63/64 loss: 0.1649470329284668
Batch 64/64 loss: -3.026646614074707
Epoch 76  Train loss: 0.20775199964934704  Val loss: 0.3332028470907834
Epoch 77
-------------------------------
Batch 1/64 loss: 0.28591251373291016
Batch 2/64 loss: 0.26930713653564453
Batch 3/64 loss: 0.0712575912475586
Batch 4/64 loss: -0.26632070541381836
Batch 5/64 loss: 0.5545330047607422
Batch 6/64 loss: -0.0507349967956543
Batch 7/64 loss: 0.6424598693847656
Batch 8/64 loss: 0.16582679748535156
Batch 9/64 loss: 0.06672048568725586
Batch 10/64 loss: 0.26866817474365234
Batch 11/64 loss: 0.16400480270385742
Batch 12/64 loss: 0.22725629806518555
Batch 13/64 loss: 0.06789064407348633
Batch 14/64 loss: 0.06883096694946289
Batch 15/64 loss: -0.11011219024658203
Batch 16/64 loss: 0.16628360748291016
Batch 17/64 loss: 0.20143842697143555
Batch 18/64 loss: 0.8492922782897949
Batch 19/64 loss: 0.2933464050292969
Batch 20/64 loss: 0.4757118225097656
Batch 21/64 loss: 0.1335740089416504
Batch 22/64 loss: 0.48547983169555664
Batch 23/64 loss: 0.40309858322143555
Batch 24/64 loss: 0.5581803321838379
Batch 25/64 loss: 0.13469505310058594
Batch 26/64 loss: 0.22945213317871094
Batch 27/64 loss: 0.4296555519104004
Batch 28/64 loss: 0.08059024810791016
Batch 29/64 loss: -0.24041986465454102
Batch 30/64 loss: 0.12644433975219727
Batch 31/64 loss: 0.1557755470275879
Batch 32/64 loss: -0.10812568664550781
Batch 33/64 loss: 0.6147594451904297
Batch 34/64 loss: 0.06269168853759766
Batch 35/64 loss: 0.1825714111328125
Batch 36/64 loss: 0.2420806884765625
Batch 37/64 loss: 0.6243352890014648
Batch 38/64 loss: 0.24673128128051758
Batch 39/64 loss: -0.1272883415222168
Batch 40/64 loss: 0.3119344711303711
Batch 41/64 loss: 1.0594043731689453
Batch 42/64 loss: 0.29677295684814453
Batch 43/64 loss: 0.16420459747314453
Batch 44/64 loss: 0.6719970703125
Batch 45/64 loss: 0.7117242813110352
Batch 46/64 loss: 0.307858943939209
Batch 47/64 loss: -0.0668344497680664
Batch 48/64 loss: 0.4152979850769043
Batch 49/64 loss: -0.34293508529663086
Batch 50/64 loss: 0.3010721206665039
Batch 51/64 loss: 0.21178674697875977
Batch 52/64 loss: -0.08997249603271484
Batch 53/64 loss: 0.49022626876831055
Batch 54/64 loss: -0.0012159347534179688
Batch 55/64 loss: -0.025620460510253906
Batch 56/64 loss: 0.19893312454223633
Batch 57/64 loss: 0.442873477935791
Batch 58/64 loss: 0.39987707138061523
Batch 59/64 loss: -0.24056720733642578
Batch 60/64 loss: 0.10033559799194336
Batch 61/64 loss: 0.11449718475341797
Batch 62/64 loss: 0.056389808654785156
Batch 63/64 loss: -0.13822698593139648
Batch 64/64 loss: -3.906444549560547
Epoch 77  Train loss: 0.1735817179960363  Val loss: 0.1373633158575628
Epoch 78
-------------------------------
Batch 1/64 loss: -0.05726766586303711
Batch 2/64 loss: 0.11087846755981445
Batch 3/64 loss: 0.04211902618408203
Batch 4/64 loss: 0.18778133392333984
Batch 5/64 loss: -0.21928024291992188
Batch 6/64 loss: -0.08982992172241211
Batch 7/64 loss: 0.3319582939147949
Batch 8/64 loss: 0.3933887481689453
Batch 9/64 loss: -0.307492733001709
Batch 10/64 loss: 0.6384057998657227
Batch 11/64 loss: 0.24550676345825195
Batch 12/64 loss: 0.1344609260559082
Batch 13/64 loss: 0.37569141387939453
Batch 14/64 loss: 0.2256464958190918
Batch 15/64 loss: -0.20663166046142578
Batch 16/64 loss: 0.20023727416992188
Batch 17/64 loss: 0.18075037002563477
Batch 18/64 loss: 0.28644657135009766
Batch 19/64 loss: -0.04802274703979492
Batch 20/64 loss: -0.06377601623535156
Batch 21/64 loss: -0.1276412010192871
Batch 22/64 loss: 0.08199262619018555
Batch 23/64 loss: 0.06558465957641602
Batch 24/64 loss: 0.35840320587158203
Batch 25/64 loss: 0.6699137687683105
Batch 26/64 loss: -0.10974979400634766
Batch 27/64 loss: 0.1141514778137207
Batch 28/64 loss: 0.44484519958496094
Batch 29/64 loss: -0.0473170280456543
Batch 30/64 loss: -0.1737675666809082
Batch 31/64 loss: 1.0981802940368652
Batch 32/64 loss: -0.10637474060058594
Batch 33/64 loss: 0.21549558639526367
Batch 34/64 loss: -0.1889023780822754
Batch 35/64 loss: 0.2119283676147461
Batch 36/64 loss: 0.07618999481201172
Batch 37/64 loss: -0.036539554595947266
Batch 38/64 loss: 0.20112180709838867
Batch 39/64 loss: -0.10046100616455078
Batch 40/64 loss: -0.07306528091430664
Batch 41/64 loss: -0.16190147399902344
Batch 42/64 loss: 0.009300708770751953
Batch 43/64 loss: 0.902015209197998
Batch 44/64 loss: 0.1549539566040039
Batch 45/64 loss: 0.3773322105407715
Batch 46/64 loss: 0.09348392486572266
Batch 47/64 loss: -0.2635641098022461
Batch 48/64 loss: 0.15365886688232422
Batch 49/64 loss: 0.11389970779418945
Batch 50/64 loss: 0.39705514907836914
Batch 51/64 loss: -0.20212411880493164
Batch 52/64 loss: 0.5511837005615234
Batch 53/64 loss: 0.3490638732910156
Batch 54/64 loss: -0.20257139205932617
Batch 55/64 loss: 0.11606264114379883
Batch 56/64 loss: 0.08862829208374023
Batch 57/64 loss: 0.4534163475036621
Batch 58/64 loss: 0.19560527801513672
Batch 59/64 loss: 0.12579822540283203
Batch 60/64 loss: -0.1908411979675293
Batch 61/64 loss: 0.23093557357788086
Batch 62/64 loss: 0.35581302642822266
Batch 63/64 loss: -0.08608865737915039
Batch 64/64 loss: -3.319058418273926
Epoch 78  Train loss: 0.09422401353424671  Val loss: -0.028344826190332368
Saving best model, epoch: 78
Epoch 79
-------------------------------
Batch 1/64 loss: 0.4135308265686035
Batch 2/64 loss: 0.42410945892333984
Batch 3/64 loss: 0.13254404067993164
Batch 4/64 loss: 0.08166074752807617
Batch 5/64 loss: -0.24560785293579102
Batch 6/64 loss: -0.28193044662475586
Batch 7/64 loss: 0.3096017837524414
Batch 8/64 loss: 0.13228130340576172
Batch 9/64 loss: 0.3187236785888672
Batch 10/64 loss: 0.19603776931762695
Batch 11/64 loss: -0.04230213165283203
Batch 12/64 loss: 0.5908069610595703
Batch 13/64 loss: 0.10570955276489258
Batch 14/64 loss: 0.2483835220336914
Batch 15/64 loss: 0.27514076232910156
Batch 16/64 loss: 0.017960071563720703
Batch 17/64 loss: 0.2277202606201172
Batch 18/64 loss: 0.015170574188232422
Batch 19/64 loss: -0.13019514083862305
Batch 20/64 loss: 0.04675436019897461
Batch 21/64 loss: 0.16091489791870117
Batch 22/64 loss: 0.21497583389282227
Batch 23/64 loss: -0.27783775329589844
Batch 24/64 loss: 0.37381887435913086
Batch 25/64 loss: -0.27443456649780273
Batch 26/64 loss: 0.18778657913208008
Batch 27/64 loss: -0.061237335205078125
Batch 28/64 loss: -0.10609674453735352
Batch 29/64 loss: 0.10347843170166016
Batch 30/64 loss: 0.3904547691345215
Batch 31/64 loss: 0.1811075210571289
Batch 32/64 loss: -0.0522913932800293
Batch 33/64 loss: 0.003040790557861328
Batch 34/64 loss: 0.029494285583496094
Batch 35/64 loss: 0.03252077102661133
Batch 36/64 loss: -0.01802968978881836
Batch 37/64 loss: 0.2632589340209961
Batch 38/64 loss: 0.10577821731567383
Batch 39/64 loss: 0.029629230499267578
Batch 40/64 loss: -0.03622245788574219
Batch 41/64 loss: 0.5989699363708496
Batch 42/64 loss: 0.21129083633422852
Batch 43/64 loss: 0.650296688079834
Batch 44/64 loss: 0.3289790153503418
Batch 45/64 loss: 0.19211626052856445
Batch 46/64 loss: 0.40535926818847656
Batch 47/64 loss: -0.21824359893798828
Batch 48/64 loss: 0.6151247024536133
Batch 49/64 loss: 0.08307838439941406
Batch 50/64 loss: -0.13283538818359375
Batch 51/64 loss: 0.924562931060791
Batch 52/64 loss: 0.5373759269714355
Batch 53/64 loss: -0.16896915435791016
Batch 54/64 loss: 0.12263822555541992
Batch 55/64 loss: 0.3205728530883789
Batch 56/64 loss: 0.0499882698059082
Batch 57/64 loss: 0.3643364906311035
Batch 58/64 loss: 0.028096675872802734
Batch 59/64 loss: -0.32326650619506836
Batch 60/64 loss: 0.07586193084716797
Batch 61/64 loss: -0.13631916046142578
Batch 62/64 loss: 0.17044734954833984
Batch 63/64 loss: 0.3203439712524414
Batch 64/64 loss: -3.483581066131592
Epoch 79  Train loss: 0.10185614754171933  Val loss: 0.00967396739422251
Epoch 80
-------------------------------
Batch 1/64 loss: -0.004500389099121094
Batch 2/64 loss: 0.22082757949829102
Batch 3/64 loss: 0.191436767578125
Batch 4/64 loss: -0.03899192810058594
Batch 5/64 loss: 1.2623562812805176
Batch 6/64 loss: 0.035797119140625
Batch 7/64 loss: 0.42279720306396484
Batch 8/64 loss: 0.01194620132446289
Batch 9/64 loss: 0.38448143005371094
Batch 10/64 loss: 0.14365005493164062
Batch 11/64 loss: 0.5221729278564453
Batch 12/64 loss: 0.44762754440307617
Batch 13/64 loss: 0.10569000244140625
Batch 14/64 loss: 0.04556083679199219
Batch 15/64 loss: 1.0187368392944336
Batch 16/64 loss: 0.2631525993347168
Batch 17/64 loss: 0.3401303291320801
Batch 18/64 loss: 0.3314170837402344
Batch 19/64 loss: 0.40350961685180664
Batch 20/64 loss: 0.3161592483520508
Batch 21/64 loss: -0.07657718658447266
Batch 22/64 loss: 0.037709712982177734
Batch 23/64 loss: 0.44843387603759766
Batch 24/64 loss: 0.28795623779296875
Batch 25/64 loss: 0.38303184509277344
Batch 26/64 loss: 0.2752842903137207
Batch 27/64 loss: 0.020830154418945312
Batch 28/64 loss: 0.005246639251708984
Batch 29/64 loss: -0.13466453552246094
Batch 30/64 loss: 0.1237635612487793
Batch 31/64 loss: 0.415128231048584
Batch 32/64 loss: -0.0397186279296875
Batch 33/64 loss: -0.1998729705810547
Batch 34/64 loss: -0.2261190414428711
Batch 35/64 loss: 0.15799713134765625
Batch 36/64 loss: 0.20739507675170898
Batch 37/64 loss: 0.0756525993347168
Batch 38/64 loss: -0.10401248931884766
Batch 39/64 loss: 0.008081436157226562
Batch 40/64 loss: 0.16089725494384766
Batch 41/64 loss: 0.21680593490600586
Batch 42/64 loss: -0.17592287063598633
Batch 43/64 loss: -0.28383636474609375
Batch 44/64 loss: 0.18801212310791016
Batch 45/64 loss: 0.19311952590942383
Batch 46/64 loss: 0.02246236801147461
Batch 47/64 loss: 0.17291688919067383
Batch 48/64 loss: 0.08554601669311523
Batch 49/64 loss: 0.21746397018432617
Batch 50/64 loss: 0.21803998947143555
Batch 51/64 loss: -0.29857826232910156
Batch 52/64 loss: 0.4727210998535156
Batch 53/64 loss: 0.04965019226074219
Batch 54/64 loss: 0.2552504539489746
Batch 55/64 loss: -0.07119941711425781
Batch 56/64 loss: 0.07085943222045898
Batch 57/64 loss: -0.015113353729248047
Batch 58/64 loss: 0.0033464431762695312
Batch 59/64 loss: 0.4621562957763672
Batch 60/64 loss: -0.08735322952270508
Batch 61/64 loss: 0.30283260345458984
Batch 62/64 loss: 0.1503748893737793
Batch 63/64 loss: 0.3731064796447754
Batch 64/64 loss: -3.6387248039245605
Epoch 80  Train loss: 0.12618067685295553  Val loss: -0.12357332944050688
Saving best model, epoch: 80
Epoch 81
-------------------------------
Batch 1/64 loss: 0.14949655532836914
Batch 2/64 loss: -0.3190937042236328
Batch 3/64 loss: 0.11015176773071289
Batch 4/64 loss: -0.07906341552734375
Batch 5/64 loss: -0.03145408630371094
Batch 6/64 loss: -0.1730790138244629
Batch 7/64 loss: 0.8149418830871582
Batch 8/64 loss: -0.16706323623657227
Batch 9/64 loss: -0.20395898818969727
Batch 10/64 loss: 0.015011787414550781
Batch 11/64 loss: -0.23914527893066406
Batch 12/64 loss: -0.16171789169311523
Batch 13/64 loss: 0.29543256759643555
Batch 14/64 loss: 0.603306770324707
Batch 15/64 loss: 0.15314817428588867
Batch 16/64 loss: 0.8108277320861816
Batch 17/64 loss: -0.2708554267883301
Batch 18/64 loss: 0.16845035552978516
Batch 19/64 loss: -0.022588253021240234
Batch 20/64 loss: 0.018603801727294922
Batch 21/64 loss: 0.26981067657470703
Batch 22/64 loss: 0.3167409896850586
Batch 23/64 loss: 0.5022249221801758
Batch 24/64 loss: 0.29212522506713867
Batch 25/64 loss: 0.4269423484802246
Batch 26/64 loss: 0.31826257705688477
Batch 27/64 loss: 0.3873472213745117
Batch 28/64 loss: -0.0731048583984375
Batch 29/64 loss: 0.2823600769042969
Batch 30/64 loss: 0.18523168563842773
Batch 31/64 loss: -0.03118276596069336
Batch 32/64 loss: 0.03287315368652344
Batch 33/64 loss: 0.02709817886352539
Batch 34/64 loss: 0.047393798828125
Batch 35/64 loss: 0.047174930572509766
Batch 36/64 loss: 0.3671302795410156
Batch 37/64 loss: 0.5090022087097168
Batch 38/64 loss: -0.11462020874023438
Batch 39/64 loss: 0.5410337448120117
Batch 40/64 loss: -0.286221981048584
Batch 41/64 loss: 0.17224788665771484
Batch 42/64 loss: -0.20125865936279297
Batch 43/64 loss: -0.07489919662475586
Batch 44/64 loss: -0.1678924560546875
Batch 45/64 loss: 0.39220619201660156
Batch 46/64 loss: 0.1300368309020996
Batch 47/64 loss: 0.046709537506103516
Batch 48/64 loss: -0.18815231323242188
Batch 49/64 loss: 0.3366575241088867
Batch 50/64 loss: -0.10137224197387695
Batch 51/64 loss: 0.07297992706298828
Batch 52/64 loss: 0.7004446983337402
Batch 53/64 loss: -0.043619632720947266
Batch 54/64 loss: 0.012873172760009766
Batch 55/64 loss: 0.10855674743652344
Batch 56/64 loss: -0.05819511413574219
Batch 57/64 loss: 0.1145486831665039
Batch 58/64 loss: 0.4680905342102051
Batch 59/64 loss: 0.20262718200683594
Batch 60/64 loss: 0.05173206329345703
Batch 61/64 loss: 0.614962100982666
Batch 62/64 loss: 0.44675254821777344
Batch 63/64 loss: 0.14016485214233398
Batch 64/64 loss: -3.2390456199645996
Epoch 81  Train loss: 0.09828848558313706  Val loss: -0.07436614675620168
Epoch 82
-------------------------------
Batch 1/64 loss: -0.22631311416625977
Batch 2/64 loss: 0.31900978088378906
Batch 3/64 loss: -0.04904603958129883
Batch 4/64 loss: 0.4176640510559082
Batch 5/64 loss: 0.1331939697265625
Batch 6/64 loss: 0.14019107818603516
Batch 7/64 loss: 0.03867149353027344
Batch 8/64 loss: 0.5616493225097656
Batch 9/64 loss: 0.5679211616516113
Batch 10/64 loss: -0.06563377380371094
Batch 11/64 loss: 0.24089384078979492
Batch 12/64 loss: -0.1195068359375
Batch 13/64 loss: 0.3518533706665039
Batch 14/64 loss: -0.17322778701782227
Batch 15/64 loss: 0.298673152923584
Batch 16/64 loss: 0.1744847297668457
Batch 17/64 loss: 0.2650585174560547
Batch 18/64 loss: 0.2747673988342285
Batch 19/64 loss: 0.047833919525146484
Batch 20/64 loss: 0.10985374450683594
Batch 21/64 loss: -0.2558283805847168
Batch 22/64 loss: 0.2784600257873535
Batch 23/64 loss: 0.501370906829834
Batch 24/64 loss: 0.5062532424926758
Batch 25/64 loss: -0.03644561767578125
Batch 26/64 loss: -0.3193330764770508
Batch 27/64 loss: 0.028667926788330078
Batch 28/64 loss: 0.28766918182373047
Batch 29/64 loss: -0.03206348419189453
Batch 30/64 loss: 0.16436338424682617
Batch 31/64 loss: -0.28782081604003906
Batch 32/64 loss: -0.08451128005981445
Batch 33/64 loss: -0.01662445068359375
Batch 34/64 loss: 0.8104262351989746
Batch 35/64 loss: -0.1809554100036621
Batch 36/64 loss: -0.2915644645690918
Batch 37/64 loss: 0.016515731811523438
Batch 38/64 loss: 0.30954933166503906
Batch 39/64 loss: 0.010943412780761719
Batch 40/64 loss: -0.04516172409057617
Batch 41/64 loss: -0.0169830322265625
Batch 42/64 loss: -0.08492565155029297
Batch 43/64 loss: -0.16628646850585938
Batch 44/64 loss: 0.1572585105895996
Batch 45/64 loss: -0.00146484375
Batch 46/64 loss: 0.3130340576171875
Batch 47/64 loss: -0.0029773712158203125
Batch 48/64 loss: 0.22263669967651367
Batch 49/64 loss: 0.20364999771118164
Batch 50/64 loss: 0.0655508041381836
Batch 51/64 loss: 0.18210744857788086
Batch 52/64 loss: 0.3040332794189453
Batch 53/64 loss: -0.14650726318359375
Batch 54/64 loss: 0.16373491287231445
Batch 55/64 loss: 0.060884952545166016
Batch 56/64 loss: 0.3427858352661133
Batch 57/64 loss: 0.2764101028442383
Batch 58/64 loss: 0.9322061538696289
Batch 59/64 loss: 0.008302688598632812
Batch 60/64 loss: 0.21179437637329102
Batch 61/64 loss: 0.18666648864746094
Batch 62/64 loss: 0.12636137008666992
Batch 63/64 loss: -0.13686752319335938
Batch 64/64 loss: -3.1189732551574707
Epoch 82  Train loss: 0.08680907043756224  Val loss: 0.21613271129909659
Epoch 83
-------------------------------
Batch 1/64 loss: -0.08281946182250977
Batch 2/64 loss: 0.18172931671142578
Batch 3/64 loss: 0.6122207641601562
Batch 4/64 loss: -0.19182872772216797
Batch 5/64 loss: 0.11337089538574219
Batch 6/64 loss: 0.6320548057556152
Batch 7/64 loss: 0.16010761260986328
Batch 8/64 loss: -0.1079411506652832
Batch 9/64 loss: -0.25542593002319336
Batch 10/64 loss: 0.33907127380371094
Batch 11/64 loss: 0.37351560592651367
Batch 12/64 loss: 0.09314250946044922
Batch 13/64 loss: -0.0660409927368164
Batch 14/64 loss: 0.07319164276123047
Batch 15/64 loss: 0.023441314697265625
Batch 16/64 loss: -0.17336463928222656
Batch 17/64 loss: 0.007624149322509766
Batch 18/64 loss: -0.10394906997680664
Batch 19/64 loss: 0.5743355751037598
Batch 20/64 loss: 0.02692556381225586
Batch 21/64 loss: -0.10883188247680664
Batch 22/64 loss: 0.031241416931152344
Batch 23/64 loss: -0.04160594940185547
Batch 24/64 loss: 0.31038808822631836
Batch 25/64 loss: 0.3165159225463867
Batch 26/64 loss: 0.28426361083984375
Batch 27/64 loss: 0.32245302200317383
Batch 28/64 loss: 0.07021522521972656
Batch 29/64 loss: 0.15325164794921875
Batch 30/64 loss: -0.10404300689697266
Batch 31/64 loss: 0.8200511932373047
Batch 32/64 loss: -0.1559600830078125
Batch 33/64 loss: -0.12226057052612305
Batch 34/64 loss: -0.07249593734741211
Batch 35/64 loss: 0.5017738342285156
Batch 36/64 loss: -0.052060604095458984
Batch 37/64 loss: 0.08596277236938477
Batch 38/64 loss: -0.13933277130126953
Batch 39/64 loss: 0.044422149658203125
Batch 40/64 loss: 0.47279834747314453
Batch 41/64 loss: 0.146331787109375
Batch 42/64 loss: 0.09049129486083984
Batch 43/64 loss: 0.2533993721008301
Batch 44/64 loss: -0.06703329086303711
Batch 45/64 loss: 0.4029726982116699
Batch 46/64 loss: 0.23712921142578125
Batch 47/64 loss: 0.10594463348388672
Batch 48/64 loss: 0.11975526809692383
Batch 49/64 loss: -0.05210113525390625
Batch 50/64 loss: 0.6104168891906738
Batch 51/64 loss: -0.20547151565551758
Batch 52/64 loss: -0.09718179702758789
Batch 53/64 loss: 0.0381927490234375
Batch 54/64 loss: 0.1295757293701172
Batch 55/64 loss: 0.34160852432250977
Batch 56/64 loss: 0.07966184616088867
Batch 57/64 loss: -0.13683462142944336
Batch 58/64 loss: 0.036000728607177734
Batch 59/64 loss: -0.002925872802734375
Batch 60/64 loss: -0.06073284149169922
Batch 61/64 loss: 0.03986835479736328
Batch 62/64 loss: 0.0025186538696289062
Batch 63/64 loss: 0.061733245849609375
Batch 64/64 loss: -2.3073768615722656
Epoch 83  Train loss: 0.08139442743039599  Val loss: -0.03570053257893041
Epoch 84
-------------------------------
Batch 1/64 loss: -0.08055686950683594
Batch 2/64 loss: 0.39143800735473633
Batch 3/64 loss: 0.2734694480895996
Batch 4/64 loss: -0.04067659378051758
Batch 5/64 loss: -0.14282894134521484
Batch 6/64 loss: 0.34307861328125
Batch 7/64 loss: -0.11603212356567383
Batch 8/64 loss: -0.026566028594970703
Batch 9/64 loss: 0.19664669036865234
Batch 10/64 loss: 0.05919456481933594
Batch 11/64 loss: 0.32398223876953125
Batch 12/64 loss: 0.5011143684387207
Batch 13/64 loss: -0.1375746726989746
Batch 14/64 loss: 0.6420164108276367
Batch 15/64 loss: 0.1168508529663086
Batch 16/64 loss: 0.30138540267944336
Batch 17/64 loss: 0.02614450454711914
Batch 18/64 loss: 0.07978296279907227
Batch 19/64 loss: -0.16698026657104492
Batch 20/64 loss: -0.06457138061523438
Batch 21/64 loss: 0.4168705940246582
Batch 22/64 loss: 0.08443260192871094
Batch 23/64 loss: 0.34157419204711914
Batch 24/64 loss: 0.3667020797729492
Batch 25/64 loss: 0.2082042694091797
Batch 26/64 loss: -0.23406314849853516
Batch 27/64 loss: -0.0949854850769043
Batch 28/64 loss: 0.19194364547729492
Batch 29/64 loss: 0.061348915100097656
Batch 30/64 loss: 0.07212686538696289
Batch 31/64 loss: 0.5131549835205078
Batch 32/64 loss: 1.7270116806030273
Batch 33/64 loss: 0.044597625732421875
Batch 34/64 loss: 0.09799432754516602
Batch 35/64 loss: 0.42853212356567383
Batch 36/64 loss: 0.41565847396850586
Batch 37/64 loss: 0.0851283073425293
Batch 38/64 loss: 0.6629629135131836
Batch 39/64 loss: 0.1078953742980957
Batch 40/64 loss: 0.0952920913696289
Batch 41/64 loss: 0.6488990783691406
Batch 42/64 loss: 0.6993346214294434
Batch 43/64 loss: 0.05531930923461914
Batch 44/64 loss: 0.291506290435791
Batch 45/64 loss: 0.8441667556762695
Batch 46/64 loss: 0.21347332000732422
Batch 47/64 loss: 0.9985957145690918
Batch 48/64 loss: 0.2013101577758789
Batch 49/64 loss: 0.11930274963378906
Batch 50/64 loss: 0.27262401580810547
Batch 51/64 loss: 0.03330564498901367
Batch 52/64 loss: 0.6946468353271484
Batch 53/64 loss: 0.08456945419311523
Batch 54/64 loss: 0.13858461380004883
Batch 55/64 loss: 0.4754371643066406
Batch 56/64 loss: 0.21477270126342773
Batch 57/64 loss: 0.18420982360839844
Batch 58/64 loss: 0.20927715301513672
Batch 59/64 loss: 0.3930792808532715
Batch 60/64 loss: 0.4565315246582031
Batch 61/64 loss: 0.0907282829284668
Batch 62/64 loss: 0.29914140701293945
Batch 63/64 loss: 0.18143701553344727
Batch 64/64 loss: -3.4283885955810547
Epoch 84  Train loss: 0.2086378209731158  Val loss: 0.05267801973008618
Epoch 85
-------------------------------
Batch 1/64 loss: 0.14857196807861328
Batch 2/64 loss: -0.008257389068603516
Batch 3/64 loss: 0.19343996047973633
Batch 4/64 loss: 0.05778360366821289
Batch 5/64 loss: 0.3009376525878906
Batch 6/64 loss: 0.1728529930114746
Batch 7/64 loss: 0.25159692764282227
Batch 8/64 loss: 0.3764805793762207
Batch 9/64 loss: 0.11224126815795898
Batch 10/64 loss: 0.224395751953125
Batch 11/64 loss: 0.0357050895690918
Batch 12/64 loss: -0.04522228240966797
Batch 13/64 loss: 0.18169832229614258
Batch 14/64 loss: 0.4725680351257324
Batch 15/64 loss: 0.07923078536987305
Batch 16/64 loss: 0.30347442626953125
Batch 17/64 loss: -0.36278772354125977
Batch 18/64 loss: 0.18149566650390625
Batch 19/64 loss: 0.1893444061279297
Batch 20/64 loss: 0.35735082626342773
Batch 21/64 loss: 0.08257341384887695
Batch 22/64 loss: 0.18854188919067383
Batch 23/64 loss: 0.07421684265136719
Batch 24/64 loss: 0.0002465248107910156
Batch 25/64 loss: 0.047331809997558594
Batch 26/64 loss: 0.15038681030273438
Batch 27/64 loss: 0.3811154365539551
Batch 28/64 loss: 0.17265558242797852
Batch 29/64 loss: 0.09352922439575195
Batch 30/64 loss: 0.029624462127685547
Batch 31/64 loss: 0.29078006744384766
Batch 32/64 loss: 0.7287158966064453
Batch 33/64 loss: -0.21721982955932617
Batch 34/64 loss: 0.44769763946533203
Batch 35/64 loss: -0.031998634338378906
Batch 36/64 loss: 0.04910087585449219
Batch 37/64 loss: 0.49657106399536133
Batch 38/64 loss: 0.49889183044433594
Batch 39/64 loss: -0.47125959396362305
Batch 40/64 loss: 0.0075321197509765625
Batch 41/64 loss: -0.2785201072692871
Batch 42/64 loss: -0.1074972152709961
Batch 43/64 loss: -0.10047721862792969
Batch 44/64 loss: 0.28946876525878906
Batch 45/64 loss: -0.19556188583374023
Batch 46/64 loss: 0.23385238647460938
Batch 47/64 loss: 0.12157011032104492
Batch 48/64 loss: 0.31830835342407227
Batch 49/64 loss: -0.013587474822998047
Batch 50/64 loss: -0.023494720458984375
Batch 51/64 loss: 0.2533884048461914
Batch 52/64 loss: 0.25088071823120117
Batch 53/64 loss: 0.22890329360961914
Batch 54/64 loss: 0.6014604568481445
Batch 55/64 loss: 0.18919992446899414
Batch 56/64 loss: 0.06699562072753906
Batch 57/64 loss: -0.00993204116821289
Batch 58/64 loss: 0.25524282455444336
Batch 59/64 loss: -0.07608413696289062
Batch 60/64 loss: 0.5520844459533691
Batch 61/64 loss: -0.0373072624206543
Batch 62/64 loss: 0.41319942474365234
Batch 63/64 loss: 0.1116328239440918
Batch 64/64 loss: -2.601135730743408
Epoch 85  Train loss: 0.11505581163892559  Val loss: 0.015144793847991838
Epoch 86
-------------------------------
Batch 1/64 loss: 0.13221216201782227
Batch 2/64 loss: 0.42214393615722656
Batch 3/64 loss: -0.3159356117248535
Batch 4/64 loss: 0.47754716873168945
Batch 5/64 loss: 0.11386346817016602
Batch 6/64 loss: -0.08935165405273438
Batch 7/64 loss: -0.09172487258911133
Batch 8/64 loss: 0.12069940567016602
Batch 9/64 loss: 0.2223668098449707
Batch 10/64 loss: -0.0313115119934082
Batch 11/64 loss: 0.024562835693359375
Batch 12/64 loss: 0.48158836364746094
Batch 13/64 loss: 0.22869873046875
Batch 14/64 loss: 0.40633296966552734
Batch 15/64 loss: 0.03202533721923828
Batch 16/64 loss: 0.27751827239990234
Batch 17/64 loss: 0.31263256072998047
Batch 18/64 loss: 0.33552980422973633
Batch 19/64 loss: -0.025442123413085938
Batch 20/64 loss: 0.12090349197387695
Batch 21/64 loss: 0.3943209648132324
Batch 22/64 loss: 0.3325786590576172
Batch 23/64 loss: 0.544588565826416
Batch 24/64 loss: 0.17412662506103516
Batch 25/64 loss: -0.026016712188720703
Batch 26/64 loss: -0.14728689193725586
Batch 27/64 loss: 0.13506317138671875
Batch 28/64 loss: 0.22411537170410156
Batch 29/64 loss: 0.22774457931518555
Batch 30/64 loss: -0.04549455642700195
Batch 31/64 loss: -0.05667829513549805
Batch 32/64 loss: 0.15226984024047852
Batch 33/64 loss: -0.2978672981262207
Batch 34/64 loss: 0.010776996612548828
Batch 35/64 loss: -0.24937772750854492
Batch 36/64 loss: 0.06358718872070312
Batch 37/64 loss: 0.23177528381347656
Batch 38/64 loss: -0.0704350471496582
Batch 39/64 loss: 0.41351747512817383
Batch 40/64 loss: 0.3009219169616699
Batch 41/64 loss: -0.06585836410522461
Batch 42/64 loss: 0.02569723129272461
Batch 43/64 loss: -0.09121227264404297
Batch 44/64 loss: 0.9527802467346191
Batch 45/64 loss: -0.07976102828979492
Batch 46/64 loss: 0.12426519393920898
Batch 47/64 loss: -0.24314403533935547
Batch 48/64 loss: 0.34511661529541016
Batch 49/64 loss: -0.15399932861328125
Batch 50/64 loss: 0.18135309219360352
Batch 51/64 loss: 0.12248611450195312
Batch 52/64 loss: 0.17552423477172852
Batch 53/64 loss: 0.20427989959716797
Batch 54/64 loss: 0.09362030029296875
Batch 55/64 loss: 0.026984691619873047
Batch 56/64 loss: 0.021753311157226562
Batch 57/64 loss: -0.1633439064025879
Batch 58/64 loss: 0.057684898376464844
Batch 59/64 loss: -0.2211155891418457
Batch 60/64 loss: 0.00485992431640625
Batch 61/64 loss: 0.4214286804199219
Batch 62/64 loss: 0.1438302993774414
Batch 63/64 loss: 0.03751087188720703
Batch 64/64 loss: -3.8799057006835938
Epoch 86  Train loss: 0.07024159150965073  Val loss: -0.07382174619694346
Epoch 87
-------------------------------
Batch 1/64 loss: 0.5041427612304688
Batch 2/64 loss: 0.1531834602355957
Batch 3/64 loss: -0.10793018341064453
Batch 4/64 loss: 0.7017574310302734
Batch 5/64 loss: -0.17657089233398438
Batch 6/64 loss: 0.08551979064941406
Batch 7/64 loss: 0.1633753776550293
Batch 8/64 loss: 0.1427288055419922
Batch 9/64 loss: 0.3752412796020508
Batch 10/64 loss: 0.08205461502075195
Batch 11/64 loss: -0.035861968994140625
Batch 12/64 loss: -0.08838319778442383
Batch 13/64 loss: 0.1428689956665039
Batch 14/64 loss: 0.21179962158203125
Batch 15/64 loss: 0.2480754852294922
Batch 16/64 loss: -0.20512819290161133
Batch 17/64 loss: -0.3145909309387207
Batch 18/64 loss: -0.4542655944824219
Batch 19/64 loss: 0.32324647903442383
Batch 20/64 loss: -0.019185543060302734
Batch 21/64 loss: -0.14062166213989258
Batch 22/64 loss: 0.21499967575073242
Batch 23/64 loss: -0.07119321823120117
Batch 24/64 loss: -0.24248933792114258
Batch 25/64 loss: -0.1937255859375
Batch 26/64 loss: 0.5309209823608398
Batch 27/64 loss: -0.3189120292663574
Batch 28/64 loss: 0.12757396697998047
Batch 29/64 loss: -0.05417585372924805
Batch 30/64 loss: 0.17063236236572266
Batch 31/64 loss: -0.056554317474365234
Batch 32/64 loss: -0.034613609313964844
Batch 33/64 loss: 0.05967903137207031
Batch 34/64 loss: 0.5894427299499512
Batch 35/64 loss: 0.19291305541992188
Batch 36/64 loss: 0.47190237045288086
Batch 37/64 loss: 0.305086612701416
Batch 38/64 loss: 0.6946840286254883
Batch 39/64 loss: 0.5776734352111816
Batch 40/64 loss: 0.1756901741027832
Batch 41/64 loss: -0.11295938491821289
Batch 42/64 loss: 0.20601415634155273
Batch 43/64 loss: 0.1747145652770996
Batch 44/64 loss: 0.24645662307739258
Batch 45/64 loss: 2.877744674682617
Batch 46/64 loss: -0.3495020866394043
Batch 47/64 loss: 0.293917179107666
Batch 48/64 loss: 0.010265350341796875
Batch 49/64 loss: 0.8589649200439453
Batch 50/64 loss: 0.49051523208618164
Batch 51/64 loss: 0.850621223449707
Batch 52/64 loss: 0.6107029914855957
Batch 53/64 loss: 0.9921817779541016
Batch 54/64 loss: 0.64697265625
Batch 55/64 loss: 0.2420806884765625
Batch 56/64 loss: 1.1286201477050781
Batch 57/64 loss: 0.18176603317260742
Batch 58/64 loss: 0.27081823348999023
Batch 59/64 loss: 0.05267524719238281
Batch 60/64 loss: 0.6088781356811523
Batch 61/64 loss: 0.20688629150390625
Batch 62/64 loss: 0.31128597259521484
Batch 63/64 loss: -0.2777566909790039
Batch 64/64 loss: -2.878575325012207
Epoch 87  Train loss: 0.20539486828972311  Val loss: 0.11331398298650264
Epoch 88
-------------------------------
Batch 1/64 loss: 0.563906192779541
Batch 2/64 loss: -0.24386072158813477
Batch 3/64 loss: 0.3902587890625
Batch 4/64 loss: -0.0472412109375
Batch 5/64 loss: 0.010755538940429688
Batch 6/64 loss: 0.5796422958374023
Batch 7/64 loss: -0.10547637939453125
Batch 8/64 loss: -0.35628318786621094
Batch 9/64 loss: 0.0378575325012207
Batch 10/64 loss: 0.66644287109375
Batch 11/64 loss: 0.5932669639587402
Batch 12/64 loss: -0.014824867248535156
Batch 13/64 loss: 0.3607015609741211
Batch 14/64 loss: 0.5711369514465332
Batch 15/64 loss: 0.09079551696777344
Batch 16/64 loss: 0.14289474487304688
Batch 17/64 loss: 0.12442302703857422
Batch 18/64 loss: 0.15967035293579102
Batch 19/64 loss: 0.3176913261413574
Batch 20/64 loss: 0.13060426712036133
Batch 21/64 loss: 0.1719350814819336
Batch 22/64 loss: 0.2468266487121582
Batch 23/64 loss: 0.4015512466430664
Batch 24/64 loss: 0.12393903732299805
Batch 25/64 loss: 0.16832304000854492
Batch 26/64 loss: -0.22742462158203125
Batch 27/64 loss: -0.31803131103515625
Batch 28/64 loss: 0.10117769241333008
Batch 29/64 loss: 0.3405036926269531
Batch 30/64 loss: -0.09674501419067383
Batch 31/64 loss: -0.047438621520996094
Batch 32/64 loss: 0.6579046249389648
Batch 33/64 loss: 0.3257465362548828
Batch 34/64 loss: 0.5758829116821289
Batch 35/64 loss: -0.2582840919494629
Batch 36/64 loss: -0.0018696784973144531
Batch 37/64 loss: 0.20644521713256836
Batch 38/64 loss: 0.16272258758544922
Batch 39/64 loss: 0.06689929962158203
Batch 40/64 loss: 0.46011781692504883
Batch 41/64 loss: 0.026955604553222656
Batch 42/64 loss: -0.055306434631347656
Batch 43/64 loss: -0.2058391571044922
Batch 44/64 loss: 0.5180521011352539
Batch 45/64 loss: 0.07644033432006836
Batch 46/64 loss: 0.11766529083251953
Batch 47/64 loss: -0.1550006866455078
Batch 48/64 loss: 0.23292064666748047
Batch 49/64 loss: -0.2546353340148926
Batch 50/64 loss: -0.2427539825439453
Batch 51/64 loss: -0.038014888763427734
Batch 52/64 loss: 0.12954950332641602
Batch 53/64 loss: -0.2629542350769043
Batch 54/64 loss: -0.4428539276123047
Batch 55/64 loss: -0.08953189849853516
Batch 56/64 loss: 0.47707366943359375
Batch 57/64 loss: 0.05603456497192383
Batch 58/64 loss: -0.05748558044433594
Batch 59/64 loss: 0.46127796173095703
Batch 60/64 loss: 0.35489749908447266
Batch 61/64 loss: 0.37836360931396484
Batch 62/64 loss: -0.10226249694824219
Batch 63/64 loss: 0.3346266746520996
Batch 64/64 loss: -3.683505058288574
Epoch 88  Train loss: 0.08670013652128332  Val loss: 0.04991465827443756
Epoch 89
-------------------------------
Batch 1/64 loss: 0.6462287902832031
Batch 2/64 loss: 0.26793336868286133
Batch 3/64 loss: 0.328427791595459
Batch 4/64 loss: 0.14808988571166992
Batch 5/64 loss: -0.0597538948059082
Batch 6/64 loss: 0.09866189956665039
Batch 7/64 loss: 0.08218526840209961
Batch 8/64 loss: -0.027837276458740234
Batch 9/64 loss: 0.27594566345214844
Batch 10/64 loss: 0.01016855239868164
Batch 11/64 loss: -0.009937286376953125
Batch 12/64 loss: -0.21890878677368164
Batch 13/64 loss: -0.03219747543334961
Batch 14/64 loss: 0.3820919990539551
Batch 15/64 loss: 0.10786199569702148
Batch 16/64 loss: -0.07434701919555664
Batch 17/64 loss: 0.2801079750061035
Batch 18/64 loss: -0.10827970504760742
Batch 19/64 loss: 0.012072086334228516
Batch 20/64 loss: -0.07871580123901367
Batch 21/64 loss: 0.0922694206237793
Batch 22/64 loss: 0.07960700988769531
Batch 23/64 loss: 0.1758885383605957
Batch 24/64 loss: -0.2870159149169922
Batch 25/64 loss: 0.030025482177734375
Batch 26/64 loss: 0.12013435363769531
Batch 27/64 loss: 0.5830554962158203
Batch 28/64 loss: -0.26037168502807617
Batch 29/64 loss: 0.21767759323120117
Batch 30/64 loss: 0.664675235748291
Batch 31/64 loss: 0.05345964431762695
Batch 32/64 loss: 0.597102165222168
Batch 33/64 loss: -0.309755802154541
Batch 34/64 loss: 0.18277931213378906
Batch 35/64 loss: -0.27208757400512695
Batch 36/64 loss: 0.06073904037475586
Batch 37/64 loss: 0.11285209655761719
Batch 38/64 loss: -0.11014795303344727
Batch 39/64 loss: -0.05911445617675781
Batch 40/64 loss: 0.36684513092041016
Batch 41/64 loss: 0.22533464431762695
Batch 42/64 loss: -0.06508731842041016
Batch 43/64 loss: -0.0901947021484375
Batch 44/64 loss: 0.5672836303710938
Batch 45/64 loss: 0.23615550994873047
Batch 46/64 loss: -0.09804201126098633
Batch 47/64 loss: 0.0713968276977539
Batch 48/64 loss: 1.550602912902832
Batch 49/64 loss: 0.04867839813232422
Batch 50/64 loss: 0.15589189529418945
Batch 51/64 loss: 0.759315013885498
Batch 52/64 loss: 0.7179746627807617
Batch 53/64 loss: 0.06599044799804688
Batch 54/64 loss: -0.16207504272460938
Batch 55/64 loss: 0.4020071029663086
Batch 56/64 loss: 0.6930279731750488
Batch 57/64 loss: 0.38854026794433594
Batch 58/64 loss: -0.029946327209472656
Batch 59/64 loss: 0.5798482894897461
Batch 60/64 loss: 0.5825033187866211
Batch 61/64 loss: -0.12868309020996094
Batch 62/64 loss: 0.2354111671447754
Batch 63/64 loss: 0.44438600540161133
Batch 64/64 loss: -2.8406219482421875
Epoch 89  Train loss: 0.1425610710592831  Val loss: 0.2694897602513893
Epoch 90
-------------------------------
Batch 1/64 loss: 0.28034162521362305
Batch 2/64 loss: 0.3600430488586426
Batch 3/64 loss: 0.4079737663269043
Batch 4/64 loss: 0.029712200164794922
Batch 5/64 loss: 1.0194611549377441
Batch 6/64 loss: 0.33277463912963867
Batch 7/64 loss: -0.2549600601196289
Batch 8/64 loss: 0.494565486907959
Batch 9/64 loss: 0.1106109619140625
Batch 10/64 loss: 0.5073652267456055
Batch 11/64 loss: 0.14161109924316406
Batch 12/64 loss: 0.1878972053527832
Batch 13/64 loss: 0.08941125869750977
Batch 14/64 loss: 0.3040904998779297
Batch 15/64 loss: 0.05750131607055664
Batch 16/64 loss: -0.08116483688354492
Batch 17/64 loss: 0.8591899871826172
Batch 18/64 loss: 0.016246795654296875
Batch 19/64 loss: 0.019458770751953125
Batch 20/64 loss: 0.052838802337646484
Batch 21/64 loss: -0.016179561614990234
Batch 22/64 loss: 0.22844409942626953
Batch 23/64 loss: -0.1192631721496582
Batch 24/64 loss: -0.057582855224609375
Batch 25/64 loss: 0.2232799530029297
Batch 26/64 loss: 0.2666044235229492
Batch 27/64 loss: 0.10429573059082031
Batch 28/64 loss: 0.2965998649597168
Batch 29/64 loss: 0.2581825256347656
Batch 30/64 loss: 0.32885217666625977
Batch 31/64 loss: -0.06936120986938477
Batch 32/64 loss: 0.303741455078125
Batch 33/64 loss: -0.10234737396240234
Batch 34/64 loss: 0.10920858383178711
Batch 35/64 loss: 0.13644790649414062
Batch 36/64 loss: -0.0359644889831543
Batch 37/64 loss: 0.010308265686035156
Batch 38/64 loss: -0.03217744827270508
Batch 39/64 loss: -0.17792701721191406
Batch 40/64 loss: -0.2691612243652344
Batch 41/64 loss: 0.04780864715576172
Batch 42/64 loss: -0.1154627799987793
Batch 43/64 loss: -0.023265361785888672
Batch 44/64 loss: 0.13228988647460938
Batch 45/64 loss: 0.6635680198669434
Batch 46/64 loss: 0.03467082977294922
Batch 47/64 loss: 0.3549537658691406
Batch 48/64 loss: -0.06560087203979492
Batch 49/64 loss: -0.07339048385620117
Batch 50/64 loss: -0.27365684509277344
Batch 51/64 loss: 0.18933534622192383
Batch 52/64 loss: 0.37056493759155273
Batch 53/64 loss: 0.06229400634765625
Batch 54/64 loss: -0.20584583282470703
Batch 55/64 loss: -0.20962238311767578
Batch 56/64 loss: -0.08002710342407227
Batch 57/64 loss: -0.10135793685913086
Batch 58/64 loss: 0.48986148834228516
Batch 59/64 loss: 0.6744256019592285
Batch 60/64 loss: 0.05618143081665039
Batch 61/64 loss: 0.12639331817626953
Batch 62/64 loss: -0.06822729110717773
Batch 63/64 loss: -0.2688326835632324
Batch 64/64 loss: -3.096937656402588
Epoch 90  Train loss: 0.08965214187023686  Val loss: -0.027447035222528726
Epoch 91
-------------------------------
Batch 1/64 loss: 0.13576507568359375
Batch 2/64 loss: 0.2872629165649414
Batch 3/64 loss: 0.6886487007141113
Batch 4/64 loss: 0.5558733940124512
Batch 5/64 loss: 0.062283992767333984
Batch 6/64 loss: -0.06585311889648438
Batch 7/64 loss: 0.03514719009399414
Batch 8/64 loss: 0.06310319900512695
Batch 9/64 loss: -0.1338939666748047
Batch 10/64 loss: -0.2620673179626465
Batch 11/64 loss: 0.2440938949584961
Batch 12/64 loss: 0.43282556533813477
Batch 13/64 loss: 0.09929656982421875
Batch 14/64 loss: 0.3120431900024414
Batch 15/64 loss: 0.12992000579833984
Batch 16/64 loss: 0.44548749923706055
Batch 17/64 loss: 0.2667522430419922
Batch 18/64 loss: 0.3938741683959961
Batch 19/64 loss: 0.4850926399230957
Batch 20/64 loss: -0.11053609848022461
Batch 21/64 loss: 0.0927577018737793
Batch 22/64 loss: 0.16825199127197266
Batch 23/64 loss: -0.16428470611572266
Batch 24/64 loss: 0.17632293701171875
Batch 25/64 loss: 0.29382944107055664
Batch 26/64 loss: 0.29207801818847656
Batch 27/64 loss: -0.2663450241088867
Batch 28/64 loss: 0.10777711868286133
Batch 29/64 loss: 0.31792211532592773
Batch 30/64 loss: -0.4463815689086914
Batch 31/64 loss: 0.2826504707336426
Batch 32/64 loss: -0.0420069694519043
Batch 33/64 loss: 0.13608503341674805
Batch 34/64 loss: 0.06871223449707031
Batch 35/64 loss: 0.49454355239868164
Batch 36/64 loss: 0.23580217361450195
Batch 37/64 loss: 0.27444982528686523
Batch 38/64 loss: 0.28831005096435547
Batch 39/64 loss: 0.05433940887451172
Batch 40/64 loss: -0.025264263153076172
Batch 41/64 loss: -0.085205078125
Batch 42/64 loss: -0.10666131973266602
Batch 43/64 loss: -0.16206073760986328
Batch 44/64 loss: -0.02704620361328125
Batch 45/64 loss: -0.14349985122680664
Batch 46/64 loss: -0.27028322219848633
Batch 47/64 loss: 0.07858085632324219
Batch 48/64 loss: -0.14046430587768555
Batch 49/64 loss: 0.1742539405822754
Batch 50/64 loss: 0.05484342575073242
Batch 51/64 loss: 0.11358833312988281
Batch 52/64 loss: -0.27176570892333984
Batch 53/64 loss: 0.4357295036315918
Batch 54/64 loss: 0.34680652618408203
Batch 55/64 loss: 0.5331978797912598
Batch 56/64 loss: 0.17680788040161133
Batch 57/64 loss: 0.4409065246582031
Batch 58/64 loss: -0.061484336853027344
Batch 59/64 loss: 0.3469815254211426
Batch 60/64 loss: -0.2010202407836914
Batch 61/64 loss: 0.20409870147705078
Batch 62/64 loss: -0.030629634857177734
Batch 63/64 loss: 0.19867753982543945
Batch 64/64 loss: -3.9521565437316895
Epoch 91  Train loss: 0.07913574704936907  Val loss: -0.05710671283944776
Epoch 92
-------------------------------
Batch 1/64 loss: 0.27628183364868164
Batch 2/64 loss: 0.2744312286376953
Batch 3/64 loss: -0.43134212493896484
Batch 4/64 loss: 0.03345203399658203
Batch 5/64 loss: 0.3888535499572754
Batch 6/64 loss: 0.02546548843383789
Batch 7/64 loss: -0.3491940498352051
Batch 8/64 loss: 0.18923187255859375
Batch 9/64 loss: 0.020925045013427734
Batch 10/64 loss: 0.03468608856201172
Batch 11/64 loss: 0.3885054588317871
Batch 12/64 loss: -0.19835519790649414
Batch 13/64 loss: 0.6457109451293945
Batch 14/64 loss: -0.19311761856079102
Batch 15/64 loss: -0.012274742126464844
Batch 16/64 loss: -0.3067770004272461
Batch 17/64 loss: 0.17666006088256836
Batch 18/64 loss: 0.01536703109741211
Batch 19/64 loss: 0.41959714889526367
Batch 20/64 loss: 0.28199291229248047
Batch 21/64 loss: 0.4227924346923828
Batch 22/64 loss: 0.2470541000366211
Batch 23/64 loss: 0.33484888076782227
Batch 24/64 loss: -0.3641090393066406
Batch 25/64 loss: 0.6069397926330566
Batch 26/64 loss: -0.1185760498046875
Batch 27/64 loss: 0.07428598403930664
Batch 28/64 loss: -0.08136653900146484
Batch 29/64 loss: -0.10940361022949219
Batch 30/64 loss: -0.2951517105102539
Batch 31/64 loss: 0.13924026489257812
Batch 32/64 loss: 0.4876275062561035
Batch 33/64 loss: 0.07571125030517578
Batch 34/64 loss: 0.04515695571899414
Batch 35/64 loss: 0.18901729583740234
Batch 36/64 loss: 0.6610608100891113
Batch 37/64 loss: 0.24584674835205078
Batch 38/64 loss: -0.026268959045410156
Batch 39/64 loss: -0.027902603149414062
Batch 40/64 loss: -0.1577305793762207
Batch 41/64 loss: -0.17430543899536133
Batch 42/64 loss: -0.29334211349487305
Batch 43/64 loss: 0.0448298454284668
Batch 44/64 loss: 0.38135337829589844
Batch 45/64 loss: 0.2516517639160156
Batch 46/64 loss: -0.06501531600952148
Batch 47/64 loss: -0.10460090637207031
Batch 48/64 loss: 0.3418097496032715
Batch 49/64 loss: 0.41031408309936523
Batch 50/64 loss: 0.18926477432250977
Batch 51/64 loss: 0.20864582061767578
Batch 52/64 loss: -0.1651315689086914
Batch 53/64 loss: -0.010090827941894531
Batch 54/64 loss: -0.16882991790771484
Batch 55/64 loss: -0.05651569366455078
Batch 56/64 loss: -0.30709075927734375
Batch 57/64 loss: 0.28637170791625977
Batch 58/64 loss: 0.2752199172973633
Batch 59/64 loss: 0.0423741340637207
Batch 60/64 loss: 0.18410015106201172
Batch 61/64 loss: 0.1563868522644043
Batch 62/64 loss: 0.0690774917602539
Batch 63/64 loss: 0.13963890075683594
Batch 64/64 loss: -3.2645583152770996
Epoch 92  Train loss: 0.050460708842558016  Val loss: 0.0017890799086528136
Epoch 93
-------------------------------
Batch 1/64 loss: -0.2903738021850586
Batch 2/64 loss: 0.13884782791137695
Batch 3/64 loss: 0.40103912353515625
Batch 4/64 loss: -0.11666154861450195
Batch 5/64 loss: 0.19104814529418945
Batch 6/64 loss: 0.35338640213012695
Batch 7/64 loss: 0.06915569305419922
Batch 8/64 loss: -0.3486161231994629
Batch 9/64 loss: 0.32697248458862305
Batch 10/64 loss: -0.18104839324951172
Batch 11/64 loss: 0.18326616287231445
Batch 12/64 loss: 0.46221256256103516
Batch 13/64 loss: 0.20122718811035156
Batch 14/64 loss: 0.39960432052612305
Batch 15/64 loss: 0.21065568923950195
Batch 16/64 loss: -0.03682231903076172
Batch 17/64 loss: -0.20392990112304688
Batch 18/64 loss: 0.1745009422302246
Batch 19/64 loss: -0.03490304946899414
Batch 20/64 loss: -0.3780660629272461
Batch 21/64 loss: -0.2042684555053711
Batch 22/64 loss: 0.4340343475341797
Batch 23/64 loss: 0.044893741607666016
Batch 24/64 loss: 0.35344934463500977
Batch 25/64 loss: -0.05769491195678711
Batch 26/64 loss: 0.8897056579589844
Batch 27/64 loss: -0.10548257827758789
Batch 28/64 loss: -0.4264826774597168
Batch 29/64 loss: 0.16216087341308594
Batch 30/64 loss: -0.09233999252319336
Batch 31/64 loss: -0.05716419219970703
Batch 32/64 loss: 1.0976486206054688
Batch 33/64 loss: -0.27056312561035156
Batch 34/64 loss: 0.28351879119873047
Batch 35/64 loss: 0.36757993698120117
Batch 36/64 loss: -0.12718963623046875
Batch 37/64 loss: 0.016902923583984375
Batch 38/64 loss: 0.3404970169067383
Batch 39/64 loss: 0.26674842834472656
Batch 40/64 loss: 0.15078306198120117
Batch 41/64 loss: 0.022458553314208984
Batch 42/64 loss: -0.11149740219116211
Batch 43/64 loss: -0.06030988693237305
Batch 44/64 loss: -0.007386207580566406
Batch 45/64 loss: 0.05658531188964844
Batch 46/64 loss: 0.25005054473876953
Batch 47/64 loss: 0.2943596839904785
Batch 48/64 loss: 0.21142816543579102
Batch 49/64 loss: 0.17535400390625
Batch 50/64 loss: -0.051756858825683594
Batch 51/64 loss: 0.18538188934326172
Batch 52/64 loss: 0.25250720977783203
Batch 53/64 loss: 0.1303262710571289
Batch 54/64 loss: -0.06661510467529297
Batch 55/64 loss: 0.17040061950683594
Batch 56/64 loss: -0.16478776931762695
Batch 57/64 loss: 0.26352548599243164
Batch 58/64 loss: -0.019449234008789062
Batch 59/64 loss: -0.3203892707824707
Batch 60/64 loss: 1.27201509475708
Batch 61/64 loss: -0.15389442443847656
Batch 62/64 loss: -0.44551897048950195
Batch 63/64 loss: 0.00881052017211914
Batch 64/64 loss: -3.2227649688720703
Epoch 93  Train loss: 0.0637295218075023  Val loss: -0.13912912742378786
Saving best model, epoch: 93
Epoch 94
-------------------------------
Batch 1/64 loss: 0.4985847473144531
Batch 2/64 loss: 0.1766963005065918
Batch 3/64 loss: 0.26996946334838867
Batch 4/64 loss: 0.23902177810668945
Batch 5/64 loss: 0.0008459091186523438
Batch 6/64 loss: 0.23117589950561523
Batch 7/64 loss: -0.27414655685424805
Batch 8/64 loss: -0.10596895217895508
Batch 9/64 loss: 0.17525482177734375
Batch 10/64 loss: -0.27784109115600586
Batch 11/64 loss: 0.20227861404418945
Batch 12/64 loss: -0.12700366973876953
Batch 13/64 loss: 0.4153165817260742
Batch 14/64 loss: 0.2704739570617676
Batch 15/64 loss: -0.02806997299194336
Batch 16/64 loss: 0.28455638885498047
Batch 17/64 loss: 0.4523777961730957
Batch 18/64 loss: 0.2272353172302246
Batch 19/64 loss: 0.24758148193359375
Batch 20/64 loss: -0.05528736114501953
Batch 21/64 loss: -0.05784320831298828
Batch 22/64 loss: 0.6995348930358887
Batch 23/64 loss: -0.08854341506958008
Batch 24/64 loss: 0.26592302322387695
Batch 25/64 loss: -0.0965280532836914
Batch 26/64 loss: -0.03137636184692383
Batch 27/64 loss: -0.06540632247924805
Batch 28/64 loss: -0.19176340103149414
Batch 29/64 loss: 0.24006271362304688
Batch 30/64 loss: -0.27890920639038086
Batch 31/64 loss: 0.016633987426757812
Batch 32/64 loss: -0.1860027313232422
Batch 33/64 loss: 0.045495033264160156
Batch 34/64 loss: -0.043406009674072266
Batch 35/64 loss: -0.10903167724609375
Batch 36/64 loss: 0.22609376907348633
Batch 37/64 loss: -0.23335838317871094
Batch 38/64 loss: 0.07048988342285156
Batch 39/64 loss: -0.1079854965209961
Batch 40/64 loss: 0.4150357246398926
Batch 41/64 loss: 0.13726520538330078
Batch 42/64 loss: -0.10437154769897461
Batch 43/64 loss: -0.20537614822387695
Batch 44/64 loss: -0.4133329391479492
Batch 45/64 loss: 0.11929988861083984
Batch 46/64 loss: -0.0982050895690918
Batch 47/64 loss: -0.2781505584716797
Batch 48/64 loss: 0.6862754821777344
Batch 49/64 loss: -0.0723423957824707
Batch 50/64 loss: 0.1027674674987793
Batch 51/64 loss: -0.015475273132324219
Batch 52/64 loss: -0.15155506134033203
Batch 53/64 loss: -0.14583253860473633
Batch 54/64 loss: 0.7951803207397461
Batch 55/64 loss: -0.1879439353942871
Batch 56/64 loss: -0.15758180618286133
Batch 57/64 loss: -0.10862207412719727
Batch 58/64 loss: -0.045228004455566406
Batch 59/64 loss: 0.054912567138671875
Batch 60/64 loss: 0.010004520416259766
Batch 61/64 loss: 0.7079362869262695
Batch 62/64 loss: 0.17926645278930664
Batch 63/64 loss: -0.2848935127258301
Batch 64/64 loss: -4.233304500579834
Epoch 94  Train loss: 0.01037153169220569  Val loss: -0.12161804146783049
Epoch 95
-------------------------------
Batch 1/64 loss: -0.0742502212524414
Batch 2/64 loss: -0.08691740036010742
Batch 3/64 loss: 0.938103199005127
Batch 4/64 loss: 0.42505359649658203
Batch 5/64 loss: -0.37194156646728516
Batch 6/64 loss: -0.17226457595825195
Batch 7/64 loss: 0.013165473937988281
Batch 8/64 loss: -0.17701053619384766
Batch 9/64 loss: 0.010718822479248047
Batch 10/64 loss: 0.0815591812133789
Batch 11/64 loss: 0.4058213233947754
Batch 12/64 loss: 0.37154579162597656
Batch 13/64 loss: 0.09488439559936523
Batch 14/64 loss: 0.11154556274414062
Batch 15/64 loss: 0.4529452323913574
Batch 16/64 loss: -0.12091493606567383
Batch 17/64 loss: -0.11052131652832031
Batch 18/64 loss: 0.24361371994018555
Batch 19/64 loss: -0.11631298065185547
Batch 20/64 loss: -0.1235051155090332
Batch 21/64 loss: -0.05225515365600586
Batch 22/64 loss: -0.027267932891845703
Batch 23/64 loss: 0.24512147903442383
Batch 24/64 loss: 0.15024423599243164
Batch 25/64 loss: 0.41916418075561523
Batch 26/64 loss: 0.4322690963745117
Batch 27/64 loss: -0.07329559326171875
Batch 28/64 loss: -0.06195783615112305
Batch 29/64 loss: -0.3524136543273926
Batch 30/64 loss: 1.2576384544372559
Batch 31/64 loss: -0.28126001358032227
Batch 32/64 loss: -0.2546253204345703
Batch 33/64 loss: -0.005019664764404297
Batch 34/64 loss: -0.16347837448120117
Batch 35/64 loss: 0.2490558624267578
Batch 36/64 loss: -0.11986207962036133
Batch 37/64 loss: -0.2206563949584961
Batch 38/64 loss: -0.1060953140258789
Batch 39/64 loss: 0.5545496940612793
Batch 40/64 loss: -0.011595726013183594
Batch 41/64 loss: 0.09160327911376953
Batch 42/64 loss: -0.23773813247680664
Batch 43/64 loss: -0.08922004699707031
Batch 44/64 loss: 0.006491661071777344
Batch 45/64 loss: -0.23620843887329102
Batch 46/64 loss: 0.7076468467712402
Batch 47/64 loss: -0.20550251007080078
Batch 48/64 loss: -0.40616512298583984
Batch 49/64 loss: 0.21132469177246094
Batch 50/64 loss: 0.0968012809753418
Batch 51/64 loss: 0.20815372467041016
Batch 52/64 loss: 0.13655424118041992
Batch 53/64 loss: -0.011159420013427734
Batch 54/64 loss: -0.04877328872680664
Batch 55/64 loss: 0.08611011505126953
Batch 56/64 loss: -0.4087209701538086
Batch 57/64 loss: 0.17225933074951172
Batch 58/64 loss: 0.3055410385131836
Batch 59/64 loss: 0.05867767333984375
Batch 60/64 loss: 0.08617830276489258
Batch 61/64 loss: -0.5081191062927246
Batch 62/64 loss: -0.27808189392089844
Batch 63/64 loss: -0.07588577270507812
Batch 64/64 loss: -3.716996669769287
Epoch 95  Train loss: 0.0038838835323558134  Val loss: -0.08624375071312554
Epoch 96
-------------------------------
Batch 1/64 loss: 0.4683241844177246
Batch 2/64 loss: 0.26923513412475586
Batch 3/64 loss: 0.48499345779418945
Batch 4/64 loss: 0.8430032730102539
Batch 5/64 loss: -0.0834970474243164
Batch 6/64 loss: 0.19767284393310547
Batch 7/64 loss: 0.2244586944580078
Batch 8/64 loss: -0.10447406768798828
Batch 9/64 loss: -0.03976583480834961
Batch 10/64 loss: -0.24689579010009766
Batch 11/64 loss: 0.08427047729492188
Batch 12/64 loss: 0.18616104125976562
Batch 13/64 loss: -0.12799644470214844
Batch 14/64 loss: -0.12199163436889648
Batch 15/64 loss: -0.07268953323364258
Batch 16/64 loss: -0.03286170959472656
Batch 17/64 loss: -0.06323814392089844
Batch 18/64 loss: -0.21549606323242188
Batch 19/64 loss: 0.9081220626831055
Batch 20/64 loss: -0.13398408889770508
Batch 21/64 loss: 0.13073492050170898
Batch 22/64 loss: 0.19195127487182617
Batch 23/64 loss: 0.29207611083984375
Batch 24/64 loss: -0.241424560546875
Batch 25/64 loss: -0.3216729164123535
Batch 26/64 loss: 0.09699010848999023
Batch 27/64 loss: -0.06275463104248047
Batch 28/64 loss: 0.41109371185302734
Batch 29/64 loss: 0.3367648124694824
Batch 30/64 loss: 0.1042928695678711
Batch 31/64 loss: 0.3090400695800781
Batch 32/64 loss: -0.28246164321899414
Batch 33/64 loss: -0.3902587890625
Batch 34/64 loss: -0.33521604537963867
Batch 35/64 loss: 0.35085487365722656
Batch 36/64 loss: 0.029782772064208984
Batch 37/64 loss: -0.1318964958190918
Batch 38/64 loss: 0.05266237258911133
Batch 39/64 loss: -0.14244604110717773
Batch 40/64 loss: -0.303497314453125
Batch 41/64 loss: -0.2611045837402344
Batch 42/64 loss: 0.14713478088378906
Batch 43/64 loss: 0.0545659065246582
Batch 44/64 loss: 0.10898923873901367
Batch 45/64 loss: 0.003269672393798828
Batch 46/64 loss: -0.22880077362060547
Batch 47/64 loss: -0.10681819915771484
Batch 48/64 loss: -0.08435726165771484
Batch 49/64 loss: 0.29540395736694336
Batch 50/64 loss: -0.12365579605102539
Batch 51/64 loss: -0.1700267791748047
Batch 52/64 loss: 0.05880403518676758
Batch 53/64 loss: -0.27733945846557617
Batch 54/64 loss: 0.17752695083618164
Batch 55/64 loss: -0.14307737350463867
Batch 56/64 loss: -0.06362247467041016
Batch 57/64 loss: 0.08112668991088867
Batch 58/64 loss: 0.3460226058959961
Batch 59/64 loss: -0.15872669219970703
Batch 60/64 loss: -0.4656658172607422
Batch 61/64 loss: 0.16833877563476562
Batch 62/64 loss: 0.18090152740478516
Batch 63/64 loss: 0.010907173156738281
Batch 64/64 loss: -3.3121113777160645
Epoch 96  Train loss: -0.006530527975044998  Val loss: -0.13326207059355535
Epoch 97
-------------------------------
Batch 1/64 loss: 0.10076522827148438
Batch 2/64 loss: -0.1232757568359375
Batch 3/64 loss: 0.29199934005737305
Batch 4/64 loss: -0.1333608627319336
Batch 5/64 loss: -0.06670713424682617
Batch 6/64 loss: -0.12066650390625
Batch 7/64 loss: -0.026352882385253906
Batch 8/64 loss: -0.14664649963378906
Batch 9/64 loss: 0.08058404922485352
Batch 10/64 loss: 0.047629356384277344
Batch 11/64 loss: -0.059990882873535156
Batch 12/64 loss: -0.15857458114624023
Batch 13/64 loss: -0.4747905731201172
Batch 14/64 loss: -0.20408010482788086
Batch 15/64 loss: -0.25390625
Batch 16/64 loss: -0.1791524887084961
Batch 17/64 loss: 0.6263322830200195
Batch 18/64 loss: 0.8751530647277832
Batch 19/64 loss: 0.08709716796875
Batch 20/64 loss: -0.19451665878295898
Batch 21/64 loss: -0.08377599716186523
Batch 22/64 loss: -0.0707855224609375
Batch 23/64 loss: -0.17906618118286133
Batch 24/64 loss: -0.20244169235229492
Batch 25/64 loss: 0.08415603637695312
Batch 26/64 loss: -0.36481475830078125
Batch 27/64 loss: 0.01531982421875
Batch 28/64 loss: 0.37539100646972656
Batch 29/64 loss: -0.19507837295532227
Batch 30/64 loss: -0.24323463439941406
Batch 31/64 loss: -0.0798187255859375
Batch 32/64 loss: 0.08549070358276367
Batch 33/64 loss: -0.37668323516845703
Batch 34/64 loss: 0.16086530685424805
Batch 35/64 loss: -0.1807718276977539
Batch 36/64 loss: -0.11494684219360352
Batch 37/64 loss: 0.23001670837402344
Batch 38/64 loss: 0.10504865646362305
Batch 39/64 loss: 0.4471864700317383
Batch 40/64 loss: 0.10422754287719727
Batch 41/64 loss: -0.047907352447509766
Batch 42/64 loss: 0.14663267135620117
Batch 43/64 loss: -0.29378843307495117
Batch 44/64 loss: -0.4597749710083008
Batch 45/64 loss: -0.4546966552734375
Batch 46/64 loss: 0.18546342849731445
Batch 47/64 loss: 0.5862178802490234
Batch 48/64 loss: -0.02843475341796875
Batch 49/64 loss: 0.03021240234375
Batch 50/64 loss: -0.31720685958862305
Batch 51/64 loss: -0.4031538963317871
Batch 52/64 loss: 0.025785446166992188
Batch 53/64 loss: -0.000102996826171875
Batch 54/64 loss: -0.23011255264282227
Batch 55/64 loss: -0.21672487258911133
Batch 56/64 loss: 0.26465320587158203
Batch 57/64 loss: -0.2199082374572754
Batch 58/64 loss: -0.13280630111694336
Batch 59/64 loss: 0.35112524032592773
Batch 60/64 loss: 0.7288017272949219
Batch 61/64 loss: -0.09163141250610352
Batch 62/64 loss: 0.8044214248657227
Batch 63/64 loss: 0.07384872436523438
Batch 64/64 loss: -4.343652725219727
Epoch 97  Train loss: -0.05447847702923943  Val loss: -0.15493593510893203
Saving best model, epoch: 97
Epoch 98
-------------------------------
Batch 1/64 loss: 0.06194877624511719
Batch 2/64 loss: 0.05472230911254883
Batch 3/64 loss: -0.14231395721435547
Batch 4/64 loss: -0.036760807037353516
Batch 5/64 loss: 0.15950632095336914
Batch 6/64 loss: 0.4368405342102051
Batch 7/64 loss: -0.07035160064697266
Batch 8/64 loss: 0.07299184799194336
Batch 9/64 loss: -0.08554220199584961
Batch 10/64 loss: 0.12572813034057617
Batch 11/64 loss: -0.21584415435791016
Batch 12/64 loss: -0.013283252716064453
Batch 13/64 loss: -0.20814275741577148
Batch 14/64 loss: 0.2756190299987793
Batch 15/64 loss: -0.15515470504760742
Batch 16/64 loss: -0.05441093444824219
Batch 17/64 loss: 0.5184063911437988
Batch 18/64 loss: 0.015671253204345703
Batch 19/64 loss: -0.03968334197998047
Batch 20/64 loss: -0.0074710845947265625
Batch 21/64 loss: -0.0009551048278808594
Batch 22/64 loss: -0.22200918197631836
Batch 23/64 loss: -0.0012388229370117188
Batch 24/64 loss: 0.10779666900634766
Batch 25/64 loss: -0.1638622283935547
Batch 26/64 loss: -0.39167213439941406
Batch 27/64 loss: -0.2574305534362793
Batch 28/64 loss: -0.36899471282958984
Batch 29/64 loss: 0.07983875274658203
Batch 30/64 loss: 0.011134147644042969
Batch 31/64 loss: -0.24990320205688477
Batch 32/64 loss: 0.9678301811218262
Batch 33/64 loss: 0.20987462997436523
Batch 34/64 loss: -0.06838321685791016
Batch 35/64 loss: 0.1216888427734375
Batch 36/64 loss: -0.07616519927978516
Batch 37/64 loss: -0.23090744018554688
Batch 38/64 loss: 1.0041561126708984
Batch 39/64 loss: 0.6051182746887207
Batch 40/64 loss: 0.2409677505493164
Batch 41/64 loss: 0.04548358917236328
Batch 42/64 loss: 0.18136978149414062
Batch 43/64 loss: -0.18301630020141602
Batch 44/64 loss: -0.004594326019287109
Batch 45/64 loss: 0.10828638076782227
Batch 46/64 loss: 0.32392311096191406
Batch 47/64 loss: -0.053822994232177734
Batch 48/64 loss: 0.026974201202392578
Batch 49/64 loss: 0.002102375030517578
Batch 50/64 loss: 0.0012803077697753906
Batch 51/64 loss: -0.11694574356079102
Batch 52/64 loss: 0.3931422233581543
Batch 53/64 loss: 0.25783872604370117
Batch 54/64 loss: -0.249908447265625
Batch 55/64 loss: -0.06567955017089844
Batch 56/64 loss: 0.6524453163146973
Batch 57/64 loss: 0.1681976318359375
Batch 58/64 loss: 0.0980825424194336
Batch 59/64 loss: -0.030166149139404297
Batch 60/64 loss: 0.16417455673217773
Batch 61/64 loss: -0.11809158325195312
Batch 62/64 loss: 0.14900636672973633
Batch 63/64 loss: 0.3261442184448242
Batch 64/64 loss: -3.6273107528686523
Epoch 98  Train loss: 0.021413373012168734  Val loss: -0.07156711591478065
Epoch 99
-------------------------------
Batch 1/64 loss: 0.14183378219604492
Batch 2/64 loss: -0.2662081718444824
Batch 3/64 loss: 0.38454437255859375
Batch 4/64 loss: -0.14159536361694336
Batch 5/64 loss: -0.43839311599731445
Batch 6/64 loss: -0.12965011596679688
Batch 7/64 loss: -0.13692760467529297
Batch 8/64 loss: -0.014894485473632812
Batch 9/64 loss: -0.2570371627807617
Batch 10/64 loss: 0.231597900390625
Batch 11/64 loss: -0.03971529006958008
Batch 12/64 loss: 0.002926349639892578
Batch 13/64 loss: -0.2274022102355957
Batch 14/64 loss: 0.03868722915649414
Batch 15/64 loss: -0.09995603561401367
Batch 16/64 loss: 0.11005926132202148
Batch 17/64 loss: -0.1654953956604004
Batch 18/64 loss: 0.14073801040649414
Batch 19/64 loss: -0.20125102996826172
Batch 20/64 loss: -0.020351409912109375
Batch 21/64 loss: -0.058269500732421875
Batch 22/64 loss: 0.6032252311706543
Batch 23/64 loss: 0.05255460739135742
Batch 24/64 loss: -0.060346126556396484
Batch 25/64 loss: -0.046772003173828125
Batch 26/64 loss: -0.22507190704345703
Batch 27/64 loss: -0.17553377151489258
Batch 28/64 loss: 0.40393733978271484
Batch 29/64 loss: -0.005397796630859375
Batch 30/64 loss: -0.3692755699157715
Batch 31/64 loss: -0.04273843765258789
Batch 32/64 loss: 0.1290302276611328
Batch 33/64 loss: -0.26853036880493164
Batch 34/64 loss: 0.38573408126831055
Batch 35/64 loss: 0.4124469757080078
Batch 36/64 loss: 0.12679195404052734
Batch 37/64 loss: 0.4632596969604492
Batch 38/64 loss: -0.24111509323120117
Batch 39/64 loss: -0.3777499198913574
Batch 40/64 loss: -0.28833436965942383
Batch 41/64 loss: 0.006072044372558594
Batch 42/64 loss: 0.0002498626708984375
Batch 43/64 loss: -0.2576632499694824
Batch 44/64 loss: 0.20677757263183594
Batch 45/64 loss: -0.2172102928161621
Batch 46/64 loss: 0.4385066032409668
Batch 47/64 loss: -0.18088483810424805
Batch 48/64 loss: 0.41573095321655273
Batch 49/64 loss: 0.35102319717407227
Batch 50/64 loss: -0.3403143882751465
Batch 51/64 loss: -0.18512678146362305
Batch 52/64 loss: 0.029376506805419922
Batch 53/64 loss: -0.006234169006347656
Batch 54/64 loss: 0.17888355255126953
Batch 55/64 loss: 0.33095645904541016
Batch 56/64 loss: -0.2839536666870117
Batch 57/64 loss: 0.009000778198242188
Batch 58/64 loss: 0.6589126586914062
Batch 59/64 loss: -0.10000991821289062
Batch 60/64 loss: 0.2370004653930664
Batch 61/64 loss: -0.2348775863647461
Batch 62/64 loss: -0.29655885696411133
Batch 63/64 loss: -0.14328241348266602
Batch 64/64 loss: -3.9384660720825195
Epoch 99  Train loss: -0.04718620075899012  Val loss: -0.1766203654181097
Saving best model, epoch: 99
Epoch 100
-------------------------------
Batch 1/64 loss: 0.08258962631225586
Batch 2/64 loss: -0.543799877166748
Batch 3/64 loss: -0.0923304557800293
Batch 4/64 loss: -0.10780191421508789
Batch 5/64 loss: 0.3014097213745117
Batch 6/64 loss: 0.03340339660644531
Batch 7/64 loss: -0.2115921974182129
Batch 8/64 loss: 0.0559382438659668
Batch 9/64 loss: 0.3159489631652832
Batch 10/64 loss: -0.06749391555786133
Batch 11/64 loss: -0.16692829132080078
Batch 12/64 loss: -0.44451045989990234
Batch 13/64 loss: -0.12232398986816406
Batch 14/64 loss: -0.020328521728515625
Batch 15/64 loss: -0.2242293357849121
Batch 16/64 loss: -0.37807178497314453
Batch 17/64 loss: -0.33112287521362305
Batch 18/64 loss: 0.7300562858581543
Batch 19/64 loss: 0.24556779861450195
Batch 20/64 loss: -0.47824573516845703
Batch 21/64 loss: 0.14505577087402344
Batch 22/64 loss: -0.10442924499511719
Batch 23/64 loss: -0.17721796035766602
Batch 24/64 loss: 0.07716560363769531
Batch 25/64 loss: -0.019227027893066406
Batch 26/64 loss: -0.47866392135620117
Batch 27/64 loss: 0.2834773063659668
Batch 28/64 loss: -0.11615228652954102
Batch 29/64 loss: -0.14196300506591797
Batch 30/64 loss: 0.11497354507446289
Batch 31/64 loss: 0.189453125
Batch 32/64 loss: 0.12392807006835938
Batch 33/64 loss: -0.25155067443847656
Batch 34/64 loss: 0.5386190414428711
Batch 35/64 loss: -0.2895851135253906
Batch 36/64 loss: -0.09556818008422852
Batch 37/64 loss: -0.20610666275024414
Batch 38/64 loss: 0.2661590576171875
Batch 39/64 loss: 0.02200603485107422
Batch 40/64 loss: -0.250246524810791
Batch 41/64 loss: 0.41201353073120117
Batch 42/64 loss: -0.24212932586669922
Batch 43/64 loss: 0.5433382987976074
Batch 44/64 loss: 0.15327882766723633
Batch 45/64 loss: -0.05744457244873047
Batch 46/64 loss: -0.10162544250488281
Batch 47/64 loss: -0.14766359329223633
Batch 48/64 loss: 0.3278326988220215
Batch 49/64 loss: 0.06523942947387695
Batch 50/64 loss: -0.16988801956176758
Batch 51/64 loss: -0.06824636459350586
Batch 52/64 loss: 0.14966249465942383
Batch 53/64 loss: -0.2895493507385254
Batch 54/64 loss: 0.520665168762207
Batch 55/64 loss: -0.014866352081298828
Batch 56/64 loss: -0.012266159057617188
Batch 57/64 loss: -0.2578768730163574
Batch 58/64 loss: -0.3177623748779297
Batch 59/64 loss: 0.14274072647094727
Batch 60/64 loss: -0.24306392669677734
Batch 61/64 loss: 0.03311729431152344
Batch 62/64 loss: 0.07525491714477539
Batch 63/64 loss: 0.20016860961914062
Batch 64/64 loss: -3.9673638343811035
Epoch 100  Train loss: -0.06381696626251819  Val loss: -0.17358509863365146
Epoch 101
-------------------------------
Batch 1/64 loss: 0.38901233673095703
Batch 2/64 loss: -0.3278064727783203
Batch 3/64 loss: -0.12923240661621094
Batch 4/64 loss: -0.2674131393432617
Batch 5/64 loss: -0.1154623031616211
Batch 6/64 loss: 0.3275899887084961
Batch 7/64 loss: -0.07007884979248047
Batch 8/64 loss: 0.0466303825378418
Batch 9/64 loss: -0.05048942565917969
Batch 10/64 loss: 0.3954887390136719
Batch 11/64 loss: -0.15277433395385742
Batch 12/64 loss: 0.7812113761901855
Batch 13/64 loss: -0.06200885772705078
Batch 14/64 loss: -0.18323850631713867
Batch 15/64 loss: -0.3243522644042969
Batch 16/64 loss: 0.3296847343444824
Batch 17/64 loss: 0.006832599639892578
Batch 18/64 loss: 0.29408979415893555
Batch 19/64 loss: -0.06631326675415039
Batch 20/64 loss: -0.13597440719604492
Batch 21/64 loss: -0.24248075485229492
Batch 22/64 loss: 0.24295520782470703
Batch 23/64 loss: -0.2432727813720703
Batch 24/64 loss: 0.18482494354248047
Batch 25/64 loss: 0.4346184730529785
Batch 26/64 loss: 0.20068597793579102
Batch 27/64 loss: -0.23619318008422852
Batch 28/64 loss: 0.3723478317260742
Batch 29/64 loss: -0.1197519302368164
Batch 30/64 loss: 0.061136722564697266
Batch 31/64 loss: 0.8172922134399414
Batch 32/64 loss: -0.11281251907348633
Batch 33/64 loss: -0.31447935104370117
Batch 34/64 loss: -0.04810190200805664
Batch 35/64 loss: -0.057064056396484375
Batch 36/64 loss: 0.33951616287231445
Batch 37/64 loss: -0.08771848678588867
Batch 38/64 loss: 0.016504764556884766
Batch 39/64 loss: 0.22852134704589844
Batch 40/64 loss: -0.15258216857910156
Batch 41/64 loss: -0.1944580078125
Batch 42/64 loss: 0.040328025817871094
Batch 43/64 loss: 0.22101593017578125
Batch 44/64 loss: -0.03470468521118164
Batch 45/64 loss: -0.0734105110168457
Batch 46/64 loss: -0.26671314239501953
Batch 47/64 loss: 0.1424574851989746
Batch 48/64 loss: -0.052496910095214844
Batch 49/64 loss: -0.30884361267089844
Batch 50/64 loss: 0.14456701278686523
Batch 51/64 loss: -0.047872066497802734
Batch 52/64 loss: -0.37510251998901367
Batch 53/64 loss: -0.09026002883911133
Batch 54/64 loss: 0.18642425537109375
Batch 55/64 loss: 0.11853837966918945
Batch 56/64 loss: 0.20586538314819336
Batch 57/64 loss: 0.09555196762084961
Batch 58/64 loss: 0.3234877586364746
Batch 59/64 loss: -0.11736297607421875
Batch 60/64 loss: -0.2917799949645996
Batch 61/64 loss: 0.06564474105834961
Batch 62/64 loss: -0.07881832122802734
Batch 63/64 loss: -0.25976085662841797
Batch 64/64 loss: -4.0120415687561035
Epoch 101  Train loss: -0.026468888451071346  Val loss: -0.23953290329766028
Saving best model, epoch: 101
Epoch 102
-------------------------------
Batch 1/64 loss: -0.20633649826049805
Batch 2/64 loss: 0.059571266174316406
Batch 3/64 loss: -0.3579983711242676
Batch 4/64 loss: -0.21286773681640625
Batch 5/64 loss: -0.31153154373168945
Batch 6/64 loss: -0.33594274520874023
Batch 7/64 loss: 0.06174468994140625
Batch 8/64 loss: 0.06227397918701172
Batch 9/64 loss: 0.2893848419189453
Batch 10/64 loss: -0.08789396286010742
Batch 11/64 loss: 0.1708998680114746
Batch 12/64 loss: -0.1241312026977539
Batch 13/64 loss: -0.18831920623779297
Batch 14/64 loss: -0.11984920501708984
Batch 15/64 loss: 0.11751270294189453
Batch 16/64 loss: -0.35390233993530273
Batch 17/64 loss: -0.3510403633117676
Batch 18/64 loss: 0.35204315185546875
Batch 19/64 loss: 0.061026573181152344
Batch 20/64 loss: -0.4522361755371094
Batch 21/64 loss: -0.17797422409057617
Batch 22/64 loss: -0.5731143951416016
Batch 23/64 loss: -0.057579994201660156
Batch 24/64 loss: -0.3831052780151367
Batch 25/64 loss: -0.04686546325683594
Batch 26/64 loss: -0.20242786407470703
Batch 27/64 loss: -0.258209228515625
Batch 28/64 loss: -0.21916437149047852
Batch 29/64 loss: -0.4247140884399414
Batch 30/64 loss: 0.06543159484863281
Batch 31/64 loss: 0.18019342422485352
Batch 32/64 loss: 0.42090892791748047
Batch 33/64 loss: 0.2454547882080078
Batch 34/64 loss: -0.2147970199584961
Batch 35/64 loss: -0.2509150505065918
Batch 36/64 loss: -0.05725669860839844
Batch 37/64 loss: 0.5420618057250977
Batch 38/64 loss: 0.20432424545288086
Batch 39/64 loss: -0.22905921936035156
Batch 40/64 loss: -0.24300384521484375
Batch 41/64 loss: -0.20634698867797852
Batch 42/64 loss: -0.1836223602294922
Batch 43/64 loss: -0.07408475875854492
Batch 44/64 loss: -0.1626424789428711
Batch 45/64 loss: -0.3910245895385742
Batch 46/64 loss: 0.12858104705810547
Batch 47/64 loss: 0.0320587158203125
Batch 48/64 loss: 0.014133930206298828
Batch 49/64 loss: -0.02037334442138672
Batch 50/64 loss: -0.1932659149169922
Batch 51/64 loss: -0.29166412353515625
Batch 52/64 loss: -0.07658624649047852
Batch 53/64 loss: -0.14864110946655273
Batch 54/64 loss: -0.14183998107910156
Batch 55/64 loss: 0.7992610931396484
Batch 56/64 loss: 0.08588314056396484
Batch 57/64 loss: 0.2015533447265625
Batch 58/64 loss: 0.10116386413574219
Batch 59/64 loss: 0.5384011268615723
Batch 60/64 loss: 0.027587890625
Batch 61/64 loss: 1.1483268737792969
Batch 62/64 loss: -0.07367753982543945
Batch 63/64 loss: -0.2078838348388672
Batch 64/64 loss: -3.1751327514648438
Epoch 102  Train loss: -0.07974048689299938  Val loss: -0.08536348637846328
Epoch 103
-------------------------------
Batch 1/64 loss: -0.0669546127319336
Batch 2/64 loss: 0.1514286994934082
Batch 3/64 loss: -0.16698789596557617
Batch 4/64 loss: 0.09000301361083984
Batch 5/64 loss: 0.08797931671142578
Batch 6/64 loss: -0.3827023506164551
Batch 7/64 loss: -0.11609983444213867
Batch 8/64 loss: 0.07530498504638672
Batch 9/64 loss: -0.04589033126831055
Batch 10/64 loss: -0.0999593734741211
Batch 11/64 loss: -0.06617879867553711
Batch 12/64 loss: -0.012300968170166016
Batch 13/64 loss: 0.08808755874633789
Batch 14/64 loss: 0.07840824127197266
Batch 15/64 loss: 0.16862964630126953
Batch 16/64 loss: 0.10236644744873047
Batch 17/64 loss: -0.22364330291748047
Batch 18/64 loss: -0.2978692054748535
Batch 19/64 loss: 0.14980554580688477
Batch 20/64 loss: 0.018594741821289062
Batch 21/64 loss: 0.038826942443847656
Batch 22/64 loss: 0.351992130279541
Batch 23/64 loss: -0.021164894104003906
Batch 24/64 loss: 0.203033447265625
Batch 25/64 loss: -0.16035938262939453
Batch 26/64 loss: -0.2856121063232422
Batch 27/64 loss: -0.33655643463134766
Batch 28/64 loss: 0.05230903625488281
Batch 29/64 loss: 0.15145015716552734
Batch 30/64 loss: -0.09537696838378906
Batch 31/64 loss: 0.4430727958679199
Batch 32/64 loss: -0.28814697265625
Batch 33/64 loss: -0.13709354400634766
Batch 34/64 loss: -0.1750168800354004
Batch 35/64 loss: 0.08266019821166992
Batch 36/64 loss: -0.010937213897705078
Batch 37/64 loss: 0.4392995834350586
Batch 38/64 loss: 0.03248119354248047
Batch 39/64 loss: -0.15119695663452148
Batch 40/64 loss: -0.23425054550170898
Batch 41/64 loss: -0.1075735092163086
Batch 42/64 loss: -0.12415933609008789
Batch 43/64 loss: -0.3446812629699707
Batch 44/64 loss: 0.16793441772460938
Batch 45/64 loss: 0.023568153381347656
Batch 46/64 loss: -0.23928546905517578
Batch 47/64 loss: -0.0741877555847168
Batch 48/64 loss: 0.40665674209594727
Batch 49/64 loss: -0.05051422119140625
Batch 50/64 loss: -0.1689748764038086
Batch 51/64 loss: 0.45186471939086914
Batch 52/64 loss: 0.6962680816650391
Batch 53/64 loss: 0.0860743522644043
Batch 54/64 loss: -0.014667987823486328
Batch 55/64 loss: 0.29944610595703125
Batch 56/64 loss: 0.027391433715820312
Batch 57/64 loss: 0.7992324829101562
Batch 58/64 loss: 0.1886286735534668
Batch 59/64 loss: 0.39424943923950195
Batch 60/64 loss: -0.23215770721435547
Batch 61/64 loss: 0.3327770233154297
Batch 62/64 loss: 0.13455438613891602
Batch 63/64 loss: 0.07878875732421875
Batch 64/64 loss: -3.8355965614318848
Epoch 103  Train loss: -0.011200465408026003  Val loss: -0.07491948760252229
Epoch 104
-------------------------------
Batch 1/64 loss: 0.04780769348144531
Batch 2/64 loss: -0.17340087890625
Batch 3/64 loss: 0.09865665435791016
Batch 4/64 loss: -0.4302377700805664
Batch 5/64 loss: 0.2100973129272461
Batch 6/64 loss: 0.4476194381713867
Batch 7/64 loss: -0.11508798599243164
Batch 8/64 loss: -0.21518564224243164
Batch 9/64 loss: -0.254030704498291
Batch 10/64 loss: -0.11844587326049805
Batch 11/64 loss: 0.025114059448242188
Batch 12/64 loss: -0.3027477264404297
Batch 13/64 loss: -0.05849456787109375
Batch 14/64 loss: 0.19733190536499023
Batch 15/64 loss: 0.9119009971618652
Batch 16/64 loss: 0.5508899688720703
Batch 17/64 loss: -0.1460108757019043
Batch 18/64 loss: 0.10114526748657227
Batch 19/64 loss: 0.029487133026123047
Batch 20/64 loss: -0.058269500732421875
Batch 21/64 loss: -0.27416086196899414
Batch 22/64 loss: -0.05102872848510742
Batch 23/64 loss: -0.20241022109985352
Batch 24/64 loss: 0.0698542594909668
Batch 25/64 loss: 0.4349651336669922
Batch 26/64 loss: 0.20814037322998047
Batch 27/64 loss: -0.21142578125
Batch 28/64 loss: -0.03674125671386719
Batch 29/64 loss: 0.47665882110595703
Batch 30/64 loss: -0.4572467803955078
Batch 31/64 loss: -0.09406280517578125
Batch 32/64 loss: -0.3825507164001465
Batch 33/64 loss: -0.22837400436401367
Batch 34/64 loss: 0.3245525360107422
Batch 35/64 loss: 0.10168170928955078
Batch 36/64 loss: 0.34095191955566406
Batch 37/64 loss: -0.01449441909790039
Batch 38/64 loss: -0.2032022476196289
Batch 39/64 loss: 0.14397907257080078
Batch 40/64 loss: 0.039310455322265625
Batch 41/64 loss: -0.09442806243896484
Batch 42/64 loss: 0.05774641036987305
Batch 43/64 loss: -0.2334904670715332
Batch 44/64 loss: 0.046173095703125
Batch 45/64 loss: 0.06714487075805664
Batch 46/64 loss: -0.1854405403137207
Batch 47/64 loss: -0.15621089935302734
Batch 48/64 loss: -0.3026895523071289
Batch 49/64 loss: -0.16449403762817383
Batch 50/64 loss: -0.08171224594116211
Batch 51/64 loss: -0.10149574279785156
Batch 52/64 loss: -0.025435447692871094
Batch 53/64 loss: 0.18332338333129883
Batch 54/64 loss: -0.1976475715637207
Batch 55/64 loss: -0.08443069458007812
Batch 56/64 loss: -0.39136505126953125
Batch 57/64 loss: 0.010594367980957031
Batch 58/64 loss: 0.12467193603515625
Batch 59/64 loss: -0.05148887634277344
Batch 60/64 loss: 0.06958198547363281
Batch 61/64 loss: 0.6857647895812988
Batch 62/64 loss: -0.3660883903503418
Batch 63/64 loss: 0.1756577491760254
Batch 64/64 loss: -2.7716851234436035
Epoch 104  Train loss: -0.037050783867929496  Val loss: -0.24753987420465529
Saving best model, epoch: 104
Epoch 105
-------------------------------
Batch 1/64 loss: -0.0782785415649414
Batch 2/64 loss: 0.2663106918334961
Batch 3/64 loss: -0.2550520896911621
Batch 4/64 loss: 0.34685182571411133
Batch 5/64 loss: -0.2576942443847656
Batch 6/64 loss: 0.025154590606689453
Batch 7/64 loss: -0.3736228942871094
Batch 8/64 loss: -0.3863978385925293
Batch 9/64 loss: -0.23165512084960938
Batch 10/64 loss: -0.2579360008239746
Batch 11/64 loss: -0.10640525817871094
Batch 12/64 loss: 0.1569690704345703
Batch 13/64 loss: 0.11478042602539062
Batch 14/64 loss: -0.2462940216064453
Batch 15/64 loss: -0.01162576675415039
Batch 16/64 loss: -0.1014399528503418
Batch 17/64 loss: 0.1746807098388672
Batch 18/64 loss: -0.11272525787353516
Batch 19/64 loss: -0.19988059997558594
Batch 20/64 loss: -0.15484094619750977
Batch 21/64 loss: -0.19345378875732422
Batch 22/64 loss: 0.042559146881103516
Batch 23/64 loss: 0.13617897033691406
Batch 24/64 loss: 0.4578285217285156
Batch 25/64 loss: -0.32790565490722656
Batch 26/64 loss: -0.004949092864990234
Batch 27/64 loss: -0.4279365539550781
Batch 28/64 loss: -0.277496337890625
Batch 29/64 loss: -0.2769598960876465
Batch 30/64 loss: -0.16309881210327148
Batch 31/64 loss: -0.18764734268188477
Batch 32/64 loss: 0.24857664108276367
Batch 33/64 loss: 0.0389094352722168
Batch 34/64 loss: -0.2311396598815918
Batch 35/64 loss: 0.12209415435791016
Batch 36/64 loss: -0.29955482482910156
Batch 37/64 loss: 0.4509735107421875
Batch 38/64 loss: -0.33005285263061523
Batch 39/64 loss: -0.15357542037963867
Batch 40/64 loss: -0.14960479736328125
Batch 41/64 loss: 0.17504358291625977
Batch 42/64 loss: -0.2893242835998535
Batch 43/64 loss: -0.21727657318115234
Batch 44/64 loss: -0.3452320098876953
Batch 45/64 loss: 0.20977115631103516
Batch 46/64 loss: -0.24289560317993164
Batch 47/64 loss: 0.10947799682617188
Batch 48/64 loss: -0.3321809768676758
Batch 49/64 loss: 0.07672977447509766
Batch 50/64 loss: -0.05229043960571289
Batch 51/64 loss: 0.11114072799682617
Batch 52/64 loss: -0.11600923538208008
Batch 53/64 loss: 0.48919057846069336
Batch 54/64 loss: 0.0023899078369140625
Batch 55/64 loss: 0.05954933166503906
Batch 56/64 loss: 0.6895027160644531
Batch 57/64 loss: -0.11097097396850586
Batch 58/64 loss: 0.4563407897949219
Batch 59/64 loss: 0.24094104766845703
Batch 60/64 loss: 0.30880308151245117
Batch 61/64 loss: 0.09006834030151367
Batch 62/64 loss: -0.18946552276611328
Batch 63/64 loss: 0.08608055114746094
Batch 64/64 loss: -3.941202163696289
Epoch 105  Train loss: -0.07783331029555376  Val loss: -0.15116591306076838
Epoch 106
-------------------------------
Batch 1/64 loss: 0.02930736541748047
Batch 2/64 loss: -0.0036635398864746094
Batch 3/64 loss: -0.3124575614929199
Batch 4/64 loss: -0.27782392501831055
Batch 5/64 loss: -0.3208789825439453
Batch 6/64 loss: 0.472261905670166
Batch 7/64 loss: -0.04106712341308594
Batch 8/64 loss: 0.724329948425293
Batch 9/64 loss: -0.15944862365722656
Batch 10/64 loss: -0.26349496841430664
Batch 11/64 loss: 0.05871725082397461
Batch 12/64 loss: 0.28390026092529297
Batch 13/64 loss: -0.1270756721496582
Batch 14/64 loss: -0.015385150909423828
Batch 15/64 loss: -0.08980369567871094
Batch 16/64 loss: -0.14694881439208984
Batch 17/64 loss: 0.12435340881347656
Batch 18/64 loss: 0.1960158348083496
Batch 19/64 loss: -0.1762995719909668
Batch 20/64 loss: -0.44402074813842773
Batch 21/64 loss: -0.35469675064086914
Batch 22/64 loss: -0.32366466522216797
Batch 23/64 loss: -0.09147119522094727
Batch 24/64 loss: -0.06971359252929688
Batch 25/64 loss: -0.04845571517944336
Batch 26/64 loss: -0.08075332641601562
Batch 27/64 loss: -0.23879003524780273
Batch 28/64 loss: -0.2593250274658203
Batch 29/64 loss: -0.34344053268432617
Batch 30/64 loss: -0.2494659423828125
Batch 31/64 loss: -0.3899674415588379
Batch 32/64 loss: -0.4015364646911621
Batch 33/64 loss: 0.0988149642944336
Batch 34/64 loss: -0.22743463516235352
Batch 35/64 loss: 0.07220268249511719
Batch 36/64 loss: 0.27704763412475586
Batch 37/64 loss: -0.25263261795043945
Batch 38/64 loss: 0.08854961395263672
Batch 39/64 loss: 0.11107873916625977
Batch 40/64 loss: 0.06361579895019531
Batch 41/64 loss: -0.22600364685058594
Batch 42/64 loss: 0.2794785499572754
Batch 43/64 loss: -0.27445077896118164
Batch 44/64 loss: -0.2823057174682617
Batch 45/64 loss: 0.16927433013916016
Batch 46/64 loss: -0.15402603149414062
Batch 47/64 loss: -0.17731571197509766
Batch 48/64 loss: 0.4772610664367676
Batch 49/64 loss: -0.27428102493286133
Batch 50/64 loss: -0.0067806243896484375
Batch 51/64 loss: -0.20329618453979492
Batch 52/64 loss: 0.276918888092041
Batch 53/64 loss: 0.45778656005859375
Batch 54/64 loss: 0.5257053375244141
Batch 55/64 loss: -0.11968326568603516
Batch 56/64 loss: -0.2440485954284668
Batch 57/64 loss: -0.11776447296142578
Batch 58/64 loss: -0.08860254287719727
Batch 59/64 loss: -0.11160659790039062
Batch 60/64 loss: 1.0048861503601074
Batch 61/64 loss: 0.29575014114379883
Batch 62/64 loss: -0.08393430709838867
Batch 63/64 loss: -0.31089258193969727
Batch 64/64 loss: -3.973008155822754
Epoch 106  Train loss: -0.0827797347424077  Val loss: -0.21049918997328715
Epoch 107
-------------------------------
Batch 1/64 loss: 0.13101625442504883
Batch 2/64 loss: -0.14977073669433594
Batch 3/64 loss: -0.3461151123046875
Batch 4/64 loss: -0.504359245300293
Batch 5/64 loss: 0.47973203659057617
Batch 6/64 loss: -0.21865320205688477
Batch 7/64 loss: 0.5274896621704102
Batch 8/64 loss: 0.29241514205932617
Batch 9/64 loss: -0.04975557327270508
Batch 10/64 loss: -0.17197132110595703
Batch 11/64 loss: 0.25045061111450195
Batch 12/64 loss: 0.2407383918762207
Batch 13/64 loss: -0.15375709533691406
Batch 14/64 loss: 0.17092180252075195
Batch 15/64 loss: -0.17206668853759766
Batch 16/64 loss: -0.27709007263183594
Batch 17/64 loss: 0.4152498245239258
Batch 18/64 loss: 0.19213294982910156
Batch 19/64 loss: 0.02199554443359375
Batch 20/64 loss: -0.0776362419128418
Batch 21/64 loss: -0.3742647171020508
Batch 22/64 loss: 0.0005145072937011719
Batch 23/64 loss: -0.24670791625976562
Batch 24/64 loss: 0.23201274871826172
Batch 25/64 loss: 0.011675357818603516
Batch 26/64 loss: -0.23232269287109375
Batch 27/64 loss: 0.3149566650390625
Batch 28/64 loss: 0.03567314147949219
Batch 29/64 loss: -0.4282264709472656
Batch 30/64 loss: -0.32528209686279297
Batch 31/64 loss: 0.12721538543701172
Batch 32/64 loss: -0.39118385314941406
Batch 33/64 loss: 0.2117476463317871
Batch 34/64 loss: 0.19640350341796875
Batch 35/64 loss: -0.3313126564025879
Batch 36/64 loss: -0.5145125389099121
Batch 37/64 loss: -0.42737483978271484
Batch 38/64 loss: -0.4339761734008789
Batch 39/64 loss: -0.3253045082092285
Batch 40/64 loss: -0.19910573959350586
Batch 41/64 loss: -0.7024173736572266
Batch 42/64 loss: -0.2581214904785156
Batch 43/64 loss: 0.023036479949951172
Batch 44/64 loss: 0.07948923110961914
Batch 45/64 loss: 0.0006837844848632812
Batch 46/64 loss: -0.1560802459716797
Batch 47/64 loss: 0.08932352066040039
Batch 48/64 loss: -0.016735076904296875
Batch 49/64 loss: -0.14063644409179688
Batch 50/64 loss: 0.03696393966674805
Batch 51/64 loss: -0.16317081451416016
Batch 52/64 loss: -0.032067298889160156
Batch 53/64 loss: 0.7785120010375977
Batch 54/64 loss: -0.09465932846069336
Batch 55/64 loss: -0.31734561920166016
Batch 56/64 loss: -0.020967483520507812
Batch 57/64 loss: -0.4666433334350586
Batch 58/64 loss: -0.22250604629516602
Batch 59/64 loss: 0.0882568359375
Batch 60/64 loss: -0.26273584365844727
Batch 61/64 loss: -0.38253211975097656
Batch 62/64 loss: -0.16688251495361328
Batch 63/64 loss: -0.050005435943603516
Batch 64/64 loss: -3.939189910888672
Epoch 107  Train loss: -0.12251045376646752  Val loss: -0.23956254257778942
Epoch 108
-------------------------------
Batch 1/64 loss: 0.2690243721008301
Batch 2/64 loss: -0.4367389678955078
Batch 3/64 loss: -0.05998706817626953
Batch 4/64 loss: 0.1893301010131836
Batch 5/64 loss: -0.012851238250732422
Batch 6/64 loss: 0.40691089630126953
Batch 7/64 loss: 0.2599821090698242
Batch 8/64 loss: -0.36963367462158203
Batch 9/64 loss: -0.1634202003479004
Batch 10/64 loss: 0.4394664764404297
Batch 11/64 loss: -0.11823892593383789
Batch 12/64 loss: -0.5211420059204102
Batch 13/64 loss: 0.2805323600769043
Batch 14/64 loss: -0.16182327270507812
Batch 15/64 loss: 0.1744976043701172
Batch 16/64 loss: -0.11347818374633789
Batch 17/64 loss: 0.5288176536560059
Batch 18/64 loss: -0.46798038482666016
Batch 19/64 loss: 0.5476446151733398
Batch 20/64 loss: 0.0038204193115234375
Batch 21/64 loss: 0.05681657791137695
Batch 22/64 loss: -0.13829421997070312
Batch 23/64 loss: -0.08653402328491211
Batch 24/64 loss: 0.6284265518188477
Batch 25/64 loss: -0.23028182983398438
Batch 26/64 loss: -0.20584344863891602
Batch 27/64 loss: -0.15137767791748047
Batch 28/64 loss: -0.2708754539489746
Batch 29/64 loss: -0.11970090866088867
Batch 30/64 loss: -0.13839340209960938
Batch 31/64 loss: -0.25493764877319336
Batch 32/64 loss: -0.14370250701904297
Batch 33/64 loss: 0.03893566131591797
Batch 34/64 loss: -0.3901176452636719
Batch 35/64 loss: -0.3406858444213867
Batch 36/64 loss: -0.2675004005432129
Batch 37/64 loss: 0.09143733978271484
Batch 38/64 loss: 0.14107561111450195
Batch 39/64 loss: -0.057534217834472656
Batch 40/64 loss: 0.14313650131225586
Batch 41/64 loss: -0.31821250915527344
Batch 42/64 loss: -0.0329127311706543
Batch 43/64 loss: -0.23554563522338867
Batch 44/64 loss: 0.17467546463012695
Batch 45/64 loss: -0.06553030014038086
Batch 46/64 loss: -0.20078229904174805
Batch 47/64 loss: 0.31174468994140625
Batch 48/64 loss: -0.09119701385498047
Batch 49/64 loss: -0.27899885177612305
Batch 50/64 loss: -0.23714828491210938
Batch 51/64 loss: -0.2249298095703125
Batch 52/64 loss: -0.2797889709472656
Batch 53/64 loss: 0.1782522201538086
Batch 54/64 loss: -0.3825826644897461
Batch 55/64 loss: 0.021294116973876953
Batch 56/64 loss: -0.3969998359680176
Batch 57/64 loss: -0.3499603271484375
Batch 58/64 loss: -0.3126358985900879
Batch 59/64 loss: -0.4131746292114258
Batch 60/64 loss: 0.11287736892700195
Batch 61/64 loss: 0.04718589782714844
Batch 62/64 loss: -0.07121038436889648
Batch 63/64 loss: 0.1930561065673828
Batch 64/64 loss: -3.483348846435547
Epoch 108  Train loss: -0.10174516416063496  Val loss: -0.24054065193097615
Epoch 109
-------------------------------
Batch 1/64 loss: -0.12278366088867188
Batch 2/64 loss: -0.4714975357055664
Batch 3/64 loss: -0.15685319900512695
Batch 4/64 loss: 0.05084800720214844
Batch 5/64 loss: 0.3419361114501953
Batch 6/64 loss: 0.4778718948364258
Batch 7/64 loss: -0.03625822067260742
Batch 8/64 loss: -0.2827596664428711
Batch 9/64 loss: 0.31641197204589844
Batch 10/64 loss: 0.008583545684814453
Batch 11/64 loss: -0.41379642486572266
Batch 12/64 loss: -0.02088451385498047
Batch 13/64 loss: -0.12810707092285156
Batch 14/64 loss: -0.14307785034179688
Batch 15/64 loss: -0.4762153625488281
Batch 16/64 loss: -0.18918323516845703
Batch 17/64 loss: 0.2890052795410156
Batch 18/64 loss: 0.029532432556152344
Batch 19/64 loss: -0.0807032585144043
Batch 20/64 loss: -0.29393482208251953
Batch 21/64 loss: -0.45650243759155273
Batch 22/64 loss: -0.36599111557006836
Batch 23/64 loss: -0.34851932525634766
Batch 24/64 loss: -0.17774534225463867
Batch 25/64 loss: 0.38439512252807617
Batch 26/64 loss: -0.25020503997802734
Batch 27/64 loss: 0.11425495147705078
Batch 28/64 loss: -0.08818674087524414
Batch 29/64 loss: -0.3885183334350586
Batch 30/64 loss: 0.38388776779174805
Batch 31/64 loss: -0.24784278869628906
Batch 32/64 loss: -0.4158902168273926
Batch 33/64 loss: 0.7018585205078125
Batch 34/64 loss: 0.12196016311645508
Batch 35/64 loss: 0.02800464630126953
Batch 36/64 loss: -0.06352901458740234
Batch 37/64 loss: 0.03803396224975586
Batch 38/64 loss: -0.45473718643188477
Batch 39/64 loss: 0.2209486961364746
Batch 40/64 loss: -0.3220639228820801
Batch 41/64 loss: 0.3923816680908203
Batch 42/64 loss: -0.24309682846069336
Batch 43/64 loss: -0.08066463470458984
Batch 44/64 loss: -0.1363520622253418
Batch 45/64 loss: -0.19324445724487305
Batch 46/64 loss: -0.07118654251098633
Batch 47/64 loss: -0.6042914390563965
Batch 48/64 loss: -0.15762567520141602
Batch 49/64 loss: 0.11995410919189453
Batch 50/64 loss: -0.1678147315979004
Batch 51/64 loss: -0.11748409271240234
Batch 52/64 loss: -0.45352602005004883
Batch 53/64 loss: -0.3163490295410156
Batch 54/64 loss: 0.10548782348632812
Batch 55/64 loss: -0.28231000900268555
Batch 56/64 loss: 0.04840660095214844
Batch 57/64 loss: -0.2207341194152832
Batch 58/64 loss: -0.19248056411743164
Batch 59/64 loss: 0.07358837127685547
Batch 60/64 loss: -0.15702199935913086
Batch 61/64 loss: -0.08551740646362305
Batch 62/64 loss: -0.22324371337890625
Batch 63/64 loss: 0.5477781295776367
Batch 64/64 loss: -3.181558132171631
Epoch 109  Train loss: -0.12062381856581744  Val loss: -0.1508323040205179
Epoch 110
-------------------------------
Batch 1/64 loss: -0.15648603439331055
Batch 2/64 loss: 0.0946054458618164
Batch 3/64 loss: 0.25988292694091797
Batch 4/64 loss: 0.04945039749145508
Batch 5/64 loss: -0.20605945587158203
Batch 6/64 loss: 0.011123180389404297
Batch 7/64 loss: 0.16126441955566406
Batch 8/64 loss: 0.11230325698852539
Batch 9/64 loss: 0.03683137893676758
Batch 10/64 loss: -0.1282343864440918
Batch 11/64 loss: 0.2397007942199707
Batch 12/64 loss: 0.006571769714355469
Batch 13/64 loss: -0.2747163772583008
Batch 14/64 loss: -0.33123302459716797
Batch 15/64 loss: -0.04548311233520508
Batch 16/64 loss: -0.13530206680297852
Batch 17/64 loss: -0.2537379264831543
Batch 18/64 loss: 0.35158586502075195
Batch 19/64 loss: -0.32381105422973633
Batch 20/64 loss: -0.3835458755493164
Batch 21/64 loss: -0.1187443733215332
Batch 22/64 loss: 0.0920553207397461
Batch 23/64 loss: 0.035474300384521484
Batch 24/64 loss: -0.32651662826538086
Batch 25/64 loss: -0.22513961791992188
Batch 26/64 loss: 0.4152030944824219
Batch 27/64 loss: -0.28429651260375977
Batch 28/64 loss: -0.20951032638549805
Batch 29/64 loss: -0.24296236038208008
Batch 30/64 loss: -0.21449708938598633
Batch 31/64 loss: -0.49454212188720703
Batch 32/64 loss: 0.558800220489502
Batch 33/64 loss: -0.32944250106811523
Batch 34/64 loss: -0.41597700119018555
Batch 35/64 loss: 0.05703163146972656
Batch 36/64 loss: -0.18226289749145508
Batch 37/64 loss: 0.6319394111633301
Batch 38/64 loss: -0.18867921829223633
Batch 39/64 loss: -0.019791603088378906
Batch 40/64 loss: 0.1626291275024414
Batch 41/64 loss: -0.33278894424438477
Batch 42/64 loss: -0.35469627380371094
Batch 43/64 loss: -0.06275606155395508
Batch 44/64 loss: -0.25545406341552734
Batch 45/64 loss: -0.13019704818725586
Batch 46/64 loss: -0.21127557754516602
Batch 47/64 loss: -0.5143470764160156
Batch 48/64 loss: 0.08094120025634766
Batch 49/64 loss: -0.40622901916503906
Batch 50/64 loss: -0.07712507247924805
Batch 51/64 loss: 0.4495830535888672
Batch 52/64 loss: -0.017529010772705078
Batch 53/64 loss: 0.11661863327026367
Batch 54/64 loss: 0.20960760116577148
Batch 55/64 loss: -0.43624305725097656
Batch 56/64 loss: 0.15249204635620117
Batch 57/64 loss: 0.026436805725097656
Batch 58/64 loss: -0.39151477813720703
Batch 59/64 loss: -0.16222524642944336
Batch 60/64 loss: -0.30891942977905273
Batch 61/64 loss: -0.0778188705444336
Batch 62/64 loss: 0.04853630065917969
Batch 63/64 loss: -0.09250164031982422
Batch 64/64 loss: -4.118598461151123
Epoch 110  Train loss: -0.12628821017695407  Val loss: -0.11067718820473582
Epoch 111
-------------------------------
Batch 1/64 loss: -0.12275218963623047
Batch 2/64 loss: -0.13360309600830078
Batch 3/64 loss: 0.12559938430786133
Batch 4/64 loss: -0.3495674133300781
Batch 5/64 loss: -0.18979167938232422
Batch 6/64 loss: -0.15427255630493164
Batch 7/64 loss: -0.3635592460632324
Batch 8/64 loss: 0.4497065544128418
Batch 9/64 loss: 0.09624528884887695
Batch 10/64 loss: -0.41893815994262695
Batch 11/64 loss: 0.12457418441772461
Batch 12/64 loss: -0.3410654067993164
Batch 13/64 loss: -0.36231279373168945
Batch 14/64 loss: -0.21194076538085938
Batch 15/64 loss: -0.2604951858520508
Batch 16/64 loss: 0.3379483222961426
Batch 17/64 loss: -0.19118595123291016
Batch 18/64 loss: 0.05653190612792969
Batch 19/64 loss: 0.6633701324462891
Batch 20/64 loss: 0.025721073150634766
Batch 21/64 loss: 0.4167518615722656
Batch 22/64 loss: -0.07302331924438477
Batch 23/64 loss: 0.0994405746459961
Batch 24/64 loss: 0.275087833404541
Batch 25/64 loss: 0.0014333724975585938
Batch 26/64 loss: 0.07250690460205078
Batch 27/64 loss: 0.2263355255126953
Batch 28/64 loss: -0.07638216018676758
Batch 29/64 loss: 0.0491175651550293
Batch 30/64 loss: -0.1604771614074707
Batch 31/64 loss: -0.13611555099487305
Batch 32/64 loss: -0.03463268280029297
Batch 33/64 loss: -0.18482065200805664
Batch 34/64 loss: -0.26432037353515625
Batch 35/64 loss: -0.1147909164428711
Batch 36/64 loss: -0.045955657958984375
Batch 37/64 loss: 0.0724191665649414
Batch 38/64 loss: 0.1581745147705078
Batch 39/64 loss: 0.23113584518432617
Batch 40/64 loss: -0.3106412887573242
Batch 41/64 loss: -0.25626087188720703
Batch 42/64 loss: 0.10119438171386719
Batch 43/64 loss: -0.1523146629333496
Batch 44/64 loss: -0.43587732315063477
Batch 45/64 loss: -0.01419210433959961
Batch 46/64 loss: -0.14242076873779297
Batch 47/64 loss: -0.4016103744506836
Batch 48/64 loss: -0.03691387176513672
Batch 49/64 loss: 0.2673759460449219
Batch 50/64 loss: -0.14853334426879883
Batch 51/64 loss: -0.46343278884887695
Batch 52/64 loss: 0.05016279220581055
Batch 53/64 loss: -0.3336811065673828
Batch 54/64 loss: -0.2778286933898926
Batch 55/64 loss: 0.07434272766113281
Batch 56/64 loss: 0.04513740539550781
Batch 57/64 loss: -0.2563762664794922
Batch 58/64 loss: -0.3023371696472168
Batch 59/64 loss: -0.502105712890625
Batch 60/64 loss: -0.32724618911743164
Batch 61/64 loss: 0.016439437866210938
Batch 62/64 loss: 0.5061874389648438
Batch 63/64 loss: -0.2740364074707031
Batch 64/64 loss: -3.8756160736083984
Epoch 111  Train loss: -0.1127777847589231  Val loss: -0.21659169410102555
Epoch 112
-------------------------------
Batch 1/64 loss: -0.3596987724304199
Batch 2/64 loss: -0.1775655746459961
Batch 3/64 loss: 0.002087116241455078
Batch 4/64 loss: 0.30981922149658203
Batch 5/64 loss: -0.021226882934570312
Batch 6/64 loss: -0.11820411682128906
Batch 7/64 loss: 0.22194480895996094
Batch 8/64 loss: 0.07632684707641602
Batch 9/64 loss: -0.19940853118896484
Batch 10/64 loss: -0.42944765090942383
Batch 11/64 loss: 0.4230809211730957
Batch 12/64 loss: 0.22150325775146484
Batch 13/64 loss: -0.35859060287475586
Batch 14/64 loss: -0.25124502182006836
Batch 15/64 loss: 0.0031981468200683594
Batch 16/64 loss: -0.19071578979492188
Batch 17/64 loss: -0.04337644577026367
Batch 18/64 loss: -0.2994346618652344
Batch 19/64 loss: -0.26341867446899414
Batch 20/64 loss: 0.28206586837768555
Batch 21/64 loss: -0.16149139404296875
Batch 22/64 loss: -0.22259044647216797
Batch 23/64 loss: 0.13100194931030273
Batch 24/64 loss: -0.04103803634643555
Batch 25/64 loss: -0.15221118927001953
Batch 26/64 loss: 0.015598773956298828
Batch 27/64 loss: 0.11126995086669922
Batch 28/64 loss: 0.04055356979370117
Batch 29/64 loss: -0.1203622817993164
Batch 30/64 loss: 0.15491342544555664
Batch 31/64 loss: -0.31908702850341797
Batch 32/64 loss: -0.546238899230957
Batch 33/64 loss: -0.21108198165893555
Batch 34/64 loss: 0.24631357192993164
Batch 35/64 loss: 0.20793867111206055
Batch 36/64 loss: 0.0010876655578613281
Batch 37/64 loss: -0.30948734283447266
Batch 38/64 loss: 0.050347328186035156
Batch 39/64 loss: -0.17608356475830078
Batch 40/64 loss: -0.019395828247070312
Batch 41/64 loss: -0.062399864196777344
Batch 42/64 loss: 0.4208993911743164
Batch 43/64 loss: 0.4386739730834961
Batch 44/64 loss: 0.04578542709350586
Batch 45/64 loss: -0.17306280136108398
Batch 46/64 loss: -0.15593290328979492
Batch 47/64 loss: 0.21582460403442383
Batch 48/64 loss: -0.40213584899902344
Batch 49/64 loss: -0.37682628631591797
Batch 50/64 loss: -0.27950239181518555
Batch 51/64 loss: -0.0963125228881836
Batch 52/64 loss: 0.6549654006958008
Batch 53/64 loss: -0.04680347442626953
Batch 54/64 loss: -0.019109725952148438
Batch 55/64 loss: -0.16679000854492188
Batch 56/64 loss: -0.41871213912963867
Batch 57/64 loss: -0.0770120620727539
Batch 58/64 loss: 0.19269275665283203
Batch 59/64 loss: -0.18938398361206055
Batch 60/64 loss: -0.059056758880615234
Batch 61/64 loss: -0.026848793029785156
Batch 62/64 loss: 0.13945865631103516
Batch 63/64 loss: -0.04124736785888672
Batch 64/64 loss: -3.651029586791992
Epoch 112  Train loss: -0.08962287902832031  Val loss: -0.24343185162626182
Epoch 113
-------------------------------
Batch 1/64 loss: -0.29294300079345703
Batch 2/64 loss: -0.08964776992797852
Batch 3/64 loss: -0.2917609214782715
Batch 4/64 loss: -0.12825870513916016
Batch 5/64 loss: -0.19146156311035156
Batch 6/64 loss: 0.45516443252563477
Batch 7/64 loss: -0.26605939865112305
Batch 8/64 loss: -0.08487367630004883
Batch 9/64 loss: 0.481447696685791
Batch 10/64 loss: -0.005444526672363281
Batch 11/64 loss: -0.045758724212646484
Batch 12/64 loss: 0.8319463729858398
Batch 13/64 loss: -0.06226921081542969
Batch 14/64 loss: 0.20115947723388672
Batch 15/64 loss: -0.04495859146118164
Batch 16/64 loss: 0.0006237030029296875
Batch 17/64 loss: 0.08448314666748047
Batch 18/64 loss: -0.3150601387023926
Batch 19/64 loss: -0.06268739700317383
Batch 20/64 loss: -0.24022197723388672
Batch 21/64 loss: 0.3078932762145996
Batch 22/64 loss: -0.15237140655517578
Batch 23/64 loss: 0.005058765411376953
Batch 24/64 loss: -0.36782312393188477
Batch 25/64 loss: 0.2753467559814453
Batch 26/64 loss: -0.06086254119873047
Batch 27/64 loss: -0.5108866691589355
Batch 28/64 loss: 0.30406761169433594
Batch 29/64 loss: -0.16848993301391602
Batch 30/64 loss: -0.31429481506347656
Batch 31/64 loss: -0.5936293601989746
Batch 32/64 loss: 0.05929708480834961
Batch 33/64 loss: -0.15141916275024414
Batch 34/64 loss: -0.4270763397216797
Batch 35/64 loss: 0.46874284744262695
Batch 36/64 loss: 0.14520883560180664
Batch 37/64 loss: -0.2590031623840332
Batch 38/64 loss: 0.13274574279785156
Batch 39/64 loss: -0.3798489570617676
Batch 40/64 loss: -0.3819003105163574
Batch 41/64 loss: -0.5850992202758789
Batch 42/64 loss: -0.04429149627685547
Batch 43/64 loss: -0.5027999877929688
Batch 44/64 loss: -0.17699718475341797
Batch 45/64 loss: -0.08477354049682617
Batch 46/64 loss: 0.23813819885253906
Batch 47/64 loss: -0.4483065605163574
Batch 48/64 loss: -0.3132333755493164
Batch 49/64 loss: 0.19899892807006836
Batch 50/64 loss: -0.08642244338989258
Batch 51/64 loss: -0.05000638961791992
Batch 52/64 loss: -0.02301645278930664
Batch 53/64 loss: -0.11303281784057617
Batch 54/64 loss: -0.17294979095458984
Batch 55/64 loss: -0.1173710823059082
Batch 56/64 loss: -0.026650428771972656
Batch 57/64 loss: 0.10141706466674805
Batch 58/64 loss: 0.13863372802734375
Batch 59/64 loss: 0.10293197631835938
Batch 60/64 loss: -0.09459590911865234
Batch 61/64 loss: 0.45041847229003906
Batch 62/64 loss: 0.010313034057617188
Batch 63/64 loss: -0.0005860328674316406
Batch 64/64 loss: -3.782684326171875
Epoch 113  Train loss: -0.10309208140653722  Val loss: -0.12979095826034284
Epoch 114
-------------------------------
Batch 1/64 loss: 0.43489837646484375
Batch 2/64 loss: 0.006389617919921875
Batch 3/64 loss: -0.10225152969360352
Batch 4/64 loss: 0.32657384872436523
Batch 5/64 loss: 0.01063394546508789
Batch 6/64 loss: 0.8687624931335449
Batch 7/64 loss: -0.3204331398010254
Batch 8/64 loss: -0.10678672790527344
Batch 9/64 loss: -0.35436248779296875
Batch 10/64 loss: -0.30254364013671875
Batch 11/64 loss: 0.1531367301940918
Batch 12/64 loss: -0.0032787322998046875
Batch 13/64 loss: -0.22025060653686523
Batch 14/64 loss: 0.07804298400878906
Batch 15/64 loss: -0.2452239990234375
Batch 16/64 loss: -0.12561941146850586
Batch 17/64 loss: -0.2956657409667969
Batch 18/64 loss: 0.18267440795898438
Batch 19/64 loss: -0.23465681076049805
Batch 20/64 loss: 0.11731672286987305
Batch 21/64 loss: 0.14258480072021484
Batch 22/64 loss: 0.00839090347290039
Batch 23/64 loss: 0.08058643341064453
Batch 24/64 loss: 0.39492082595825195
Batch 25/64 loss: -0.26383066177368164
Batch 26/64 loss: 0.4287381172180176
Batch 27/64 loss: -0.3592681884765625
Batch 28/64 loss: 0.06764554977416992
Batch 29/64 loss: -0.40304040908813477
Batch 30/64 loss: -0.4501481056213379
Batch 31/64 loss: 0.0411076545715332
Batch 32/64 loss: 0.025225162506103516
Batch 33/64 loss: 0.33721446990966797
Batch 34/64 loss: -0.1740555763244629
Batch 35/64 loss: -0.2396993637084961
Batch 36/64 loss: 0.06902027130126953
Batch 37/64 loss: -0.1994795799255371
Batch 38/64 loss: 0.12514781951904297
Batch 39/64 loss: -0.3529844284057617
Batch 40/64 loss: -0.3513069152832031
Batch 41/64 loss: -0.19566726684570312
Batch 42/64 loss: -0.39655208587646484
Batch 43/64 loss: 0.2517361640930176
Batch 44/64 loss: -0.11441373825073242
Batch 45/64 loss: 0.5489273071289062
Batch 46/64 loss: -0.24948501586914062
Batch 47/64 loss: -0.5573992729187012
Batch 48/64 loss: -0.2422943115234375
Batch 49/64 loss: -0.2880992889404297
Batch 50/64 loss: -0.08148574829101562
Batch 51/64 loss: -0.5218276977539062
Batch 52/64 loss: -0.10918426513671875
Batch 53/64 loss: -0.16725397109985352
Batch 54/64 loss: -0.2592911720275879
Batch 55/64 loss: -0.510216236114502
Batch 56/64 loss: 0.23356914520263672
Batch 57/64 loss: 0.11834239959716797
Batch 58/64 loss: -0.11329364776611328
Batch 59/64 loss: -0.4026956558227539
Batch 60/64 loss: -0.06619548797607422
Batch 61/64 loss: -0.19289207458496094
Batch 62/64 loss: 0.09409284591674805
Batch 63/64 loss: -0.12972593307495117
Batch 64/64 loss: -3.9905261993408203
Epoch 114  Train loss: -0.1184325423895144  Val loss: -0.22548651941043815
Epoch 115
-------------------------------
Batch 1/64 loss: -0.07514333724975586
Batch 2/64 loss: -0.47704458236694336
Batch 3/64 loss: -0.21454668045043945
Batch 4/64 loss: 0.011406898498535156
Batch 5/64 loss: 2.281189441680908
Batch 6/64 loss: -0.06804561614990234
Batch 7/64 loss: 0.12837934494018555
Batch 8/64 loss: -0.2229928970336914
Batch 9/64 loss: 0.20526981353759766
Batch 10/64 loss: -0.11139583587646484
Batch 11/64 loss: -0.10277175903320312
Batch 12/64 loss: -0.1625194549560547
Batch 13/64 loss: -0.3332853317260742
Batch 14/64 loss: 0.21850872039794922
Batch 15/64 loss: -0.28667211532592773
Batch 16/64 loss: 0.12279987335205078
Batch 17/64 loss: 0.37907838821411133
Batch 18/64 loss: 0.009174823760986328
Batch 19/64 loss: -0.24933147430419922
Batch 20/64 loss: -0.022947311401367188
Batch 21/64 loss: -0.05061817169189453
Batch 22/64 loss: 0.06414508819580078
Batch 23/64 loss: 0.3066096305847168
Batch 24/64 loss: -0.29706859588623047
Batch 25/64 loss: 0.09371757507324219
Batch 26/64 loss: -0.20849084854125977
Batch 27/64 loss: -0.16193056106567383
Batch 28/64 loss: 0.44400739669799805
Batch 29/64 loss: -0.27619504928588867
Batch 30/64 loss: -0.2239670753479004
Batch 31/64 loss: 0.029796600341796875
Batch 32/64 loss: -0.20177698135375977
Batch 33/64 loss: 0.46680593490600586
Batch 34/64 loss: -0.14165830612182617
Batch 35/64 loss: 0.025298595428466797
Batch 36/64 loss: -0.05537605285644531
Batch 37/64 loss: -0.308652400970459
Batch 38/64 loss: -0.03126382827758789
Batch 39/64 loss: 0.06201791763305664
Batch 40/64 loss: -0.05916118621826172
Batch 41/64 loss: -0.42458248138427734
Batch 42/64 loss: -0.34416770935058594
Batch 43/64 loss: 0.10355567932128906
Batch 44/64 loss: 0.09021425247192383
Batch 45/64 loss: -0.017641544342041016
Batch 46/64 loss: 0.09187507629394531
Batch 47/64 loss: 0.3782939910888672
Batch 48/64 loss: -0.3318314552307129
Batch 49/64 loss: 0.18083858489990234
Batch 50/64 loss: -0.04911518096923828
Batch 51/64 loss: 0.3045363426208496
Batch 52/64 loss: 0.05362653732299805
Batch 53/64 loss: -0.23195266723632812
Batch 54/64 loss: -0.40849876403808594
Batch 55/64 loss: 0.14347219467163086
Batch 56/64 loss: 0.17757606506347656
Batch 57/64 loss: 0.46808671951293945
Batch 58/64 loss: 0.6632752418518066
Batch 59/64 loss: -0.201385498046875
Batch 60/64 loss: -0.17569398880004883
Batch 61/64 loss: -0.22863435745239258
Batch 62/64 loss: -0.2569293975830078
Batch 63/64 loss: -0.3398103713989258
Batch 64/64 loss: -3.8444151878356934
Epoch 115  Train loss: -0.04286829069548962  Val loss: -0.19504177447446844
Epoch 116
-------------------------------
Batch 1/64 loss: -0.13444089889526367
Batch 2/64 loss: -0.1437993049621582
Batch 3/64 loss: -0.20081090927124023
Batch 4/64 loss: 0.06051826477050781
Batch 5/64 loss: 0.020525455474853516
Batch 6/64 loss: 0.011583328247070312
Batch 7/64 loss: 0.01674175262451172
Batch 8/64 loss: -0.1209568977355957
Batch 9/64 loss: -0.17686176300048828
Batch 10/64 loss: -0.28366756439208984
Batch 11/64 loss: -0.15582990646362305
Batch 12/64 loss: 0.05525970458984375
Batch 13/64 loss: -0.15257549285888672
Batch 14/64 loss: -0.21944379806518555
Batch 15/64 loss: 0.048563480377197266
Batch 16/64 loss: -0.3696603775024414
Batch 17/64 loss: 0.11859369277954102
Batch 18/64 loss: -0.15297222137451172
Batch 19/64 loss: -0.02010965347290039
Batch 20/64 loss: 0.0391077995300293
Batch 21/64 loss: -0.38428211212158203
Batch 22/64 loss: -0.33646392822265625
Batch 23/64 loss: -0.060454368591308594
Batch 24/64 loss: -0.38457727432250977
Batch 25/64 loss: -0.2106151580810547
Batch 26/64 loss: -0.28786516189575195
Batch 27/64 loss: -0.28131580352783203
Batch 28/64 loss: -0.4085979461669922
Batch 29/64 loss: 0.12704992294311523
Batch 30/64 loss: 0.05715608596801758
Batch 31/64 loss: -0.3054642677307129
Batch 32/64 loss: -0.029681682586669922
Batch 33/64 loss: -0.1602935791015625
Batch 34/64 loss: 0.25868892669677734
Batch 35/64 loss: -0.14067602157592773
Batch 36/64 loss: -0.2799034118652344
Batch 37/64 loss: -0.32729196548461914
Batch 38/64 loss: -0.1809682846069336
Batch 39/64 loss: 0.0635223388671875
Batch 40/64 loss: 0.027221202850341797
Batch 41/64 loss: -0.19443511962890625
Batch 42/64 loss: -0.35347557067871094
Batch 43/64 loss: -0.1309809684753418
Batch 44/64 loss: 0.020986557006835938
Batch 45/64 loss: 0.5487723350524902
Batch 46/64 loss: 0.021547794342041016
Batch 47/64 loss: -0.11580610275268555
Batch 48/64 loss: 0.25620365142822266
Batch 49/64 loss: -0.29517602920532227
Batch 50/64 loss: 1.1184287071228027
Batch 51/64 loss: -0.3912234306335449
Batch 52/64 loss: -0.04958391189575195
Batch 53/64 loss: 0.554041862487793
Batch 54/64 loss: -0.20720434188842773
Batch 55/64 loss: -0.05518770217895508
Batch 56/64 loss: -0.30443811416625977
Batch 57/64 loss: -0.12300395965576172
Batch 58/64 loss: -0.020159244537353516
Batch 59/64 loss: -0.10369157791137695
Batch 60/64 loss: 0.09734439849853516
Batch 61/64 loss: -0.23934364318847656
Batch 62/64 loss: -0.2576122283935547
Batch 63/64 loss: 0.20782232284545898
Batch 64/64 loss: -3.7401790618896484
Epoch 116  Train loss: -0.1227663750741996  Val loss: -0.23432797828490792
Epoch 117
-------------------------------
Batch 1/64 loss: -0.24875879287719727
Batch 2/64 loss: -0.34339141845703125
Batch 3/64 loss: -0.062436580657958984
Batch 4/64 loss: -0.1745157241821289
Batch 5/64 loss: 0.30916595458984375
Batch 6/64 loss: -0.4183368682861328
Batch 7/64 loss: -0.2733583450317383
Batch 8/64 loss: -0.19350719451904297
Batch 9/64 loss: -0.284149169921875
Batch 10/64 loss: 0.07560300827026367
Batch 11/64 loss: 0.12410163879394531
Batch 12/64 loss: -0.3340287208557129
Batch 13/64 loss: 0.5432596206665039
Batch 14/64 loss: 0.36591625213623047
Batch 15/64 loss: -0.17594146728515625
Batch 16/64 loss: 0.6180877685546875
Batch 17/64 loss: -0.09578084945678711
Batch 18/64 loss: -0.22949886322021484
Batch 19/64 loss: 0.19568347930908203
Batch 20/64 loss: -0.20956754684448242
Batch 21/64 loss: -0.3069000244140625
Batch 22/64 loss: 0.38999032974243164
Batch 23/64 loss: -0.0552825927734375
Batch 24/64 loss: 0.11843538284301758
Batch 25/64 loss: 0.2601737976074219
Batch 26/64 loss: -0.12515020370483398
Batch 27/64 loss: 0.10937356948852539
Batch 28/64 loss: 0.22961187362670898
Batch 29/64 loss: -0.05076265335083008
Batch 30/64 loss: 0.0953984260559082
Batch 31/64 loss: 0.2252507209777832
Batch 32/64 loss: -0.44396543502807617
Batch 33/64 loss: -0.1878337860107422
Batch 34/64 loss: -0.3752479553222656
Batch 35/64 loss: 0.15044355392456055
Batch 36/64 loss: -0.21465778350830078
Batch 37/64 loss: -0.19504117965698242
Batch 38/64 loss: 0.3516960144042969
Batch 39/64 loss: 0.06133604049682617
Batch 40/64 loss: -0.1650075912475586
Batch 41/64 loss: -0.13440847396850586
Batch 42/64 loss: 0.3229217529296875
Batch 43/64 loss: -0.2948484420776367
Batch 44/64 loss: -0.10616350173950195
Batch 45/64 loss: -0.041611671447753906
Batch 46/64 loss: 0.15138769149780273
Batch 47/64 loss: 0.09774494171142578
Batch 48/64 loss: -0.2037181854248047
Batch 49/64 loss: -0.6350650787353516
Batch 50/64 loss: 0.38135337829589844
Batch 51/64 loss: -0.0024576187133789062
Batch 52/64 loss: 0.20006942749023438
Batch 53/64 loss: -0.25347900390625
Batch 54/64 loss: -0.2112135887145996
Batch 55/64 loss: -0.6018023490905762
Batch 56/64 loss: -0.2896709442138672
Batch 57/64 loss: -0.37647390365600586
Batch 58/64 loss: -0.2152252197265625
Batch 59/64 loss: 0.23692083358764648
Batch 60/64 loss: 0.008005142211914062
Batch 61/64 loss: 0.31589698791503906
Batch 62/64 loss: -0.3057289123535156
Batch 63/64 loss: -0.26164960861206055
Batch 64/64 loss: -3.5031538009643555
Epoch 117  Train loss: -0.09076352960923138  Val loss: -0.1536673450797694
Epoch 118
-------------------------------
Batch 1/64 loss: -0.30623388290405273
Batch 2/64 loss: 0.09979629516601562
Batch 3/64 loss: -0.20164012908935547
Batch 4/64 loss: -0.2991361618041992
Batch 5/64 loss: -0.0013833045959472656
Batch 6/64 loss: -0.2147836685180664
Batch 7/64 loss: -0.41429948806762695
Batch 8/64 loss: -0.44492053985595703
Batch 9/64 loss: -0.2571592330932617
Batch 10/64 loss: -0.19310855865478516
Batch 11/64 loss: -0.23876667022705078
Batch 12/64 loss: 0.33916759490966797
Batch 13/64 loss: -0.33985042572021484
Batch 14/64 loss: 0.20249319076538086
Batch 15/64 loss: 0.5797696113586426
Batch 16/64 loss: 0.15014886856079102
Batch 17/64 loss: 1.088547706604004
Batch 18/64 loss: 0.043511390686035156
Batch 19/64 loss: -0.2990589141845703
Batch 20/64 loss: 0.190032958984375
Batch 21/64 loss: 0.07538175582885742
Batch 22/64 loss: -0.17238807678222656
Batch 23/64 loss: -0.020771503448486328
Batch 24/64 loss: -0.30957555770874023
Batch 25/64 loss: -0.3846879005432129
Batch 26/64 loss: 0.6971802711486816
Batch 27/64 loss: 0.37946319580078125
Batch 28/64 loss: -0.48267126083374023
Batch 29/64 loss: 0.2395458221435547
Batch 30/64 loss: -0.21557235717773438
Batch 31/64 loss: 0.12244606018066406
Batch 32/64 loss: -0.3172950744628906
Batch 33/64 loss: 0.6848540306091309
Batch 34/64 loss: 0.21869754791259766
Batch 35/64 loss: 0.3039107322692871
Batch 36/64 loss: 0.21851634979248047
Batch 37/64 loss: -0.0872812271118164
Batch 38/64 loss: -0.09030532836914062
Batch 39/64 loss: -0.36559581756591797
Batch 40/64 loss: 0.18974065780639648
Batch 41/64 loss: 0.23133373260498047
Batch 42/64 loss: -0.29787158966064453
Batch 43/64 loss: 0.17452764511108398
Batch 44/64 loss: 0.16379880905151367
Batch 45/64 loss: -0.13342809677124023
Batch 46/64 loss: 0.0736241340637207
Batch 47/64 loss: -0.37236785888671875
Batch 48/64 loss: -0.29514026641845703
Batch 49/64 loss: -0.0346064567565918
Batch 50/64 loss: -0.24187278747558594
Batch 51/64 loss: 0.3006319999694824
Batch 52/64 loss: -0.32912158966064453
Batch 53/64 loss: -0.04201650619506836
Batch 54/64 loss: -0.12888193130493164
Batch 55/64 loss: -0.16565465927124023
Batch 56/64 loss: -0.2715468406677246
Batch 57/64 loss: -0.14539670944213867
Batch 58/64 loss: -0.01024770736694336
Batch 59/64 loss: 0.007802486419677734
Batch 60/64 loss: -0.4204416275024414
Batch 61/64 loss: -0.2798624038696289
Batch 62/64 loss: 0.1420598030090332
Batch 63/64 loss: 0.8570685386657715
Batch 64/64 loss: -3.4291229248046875
Epoch 118  Train loss: -0.05682718613568474  Val loss: -0.345370839961206
Saving best model, epoch: 118
Epoch 119
-------------------------------
Batch 1/64 loss: -0.0517725944519043
Batch 2/64 loss: -0.5479564666748047
Batch 3/64 loss: -0.14620113372802734
Batch 4/64 loss: -0.09317493438720703
Batch 5/64 loss: -0.23049449920654297
Batch 6/64 loss: -0.14869022369384766
Batch 7/64 loss: -0.17260456085205078
Batch 8/64 loss: -0.03733539581298828
Batch 9/64 loss: -0.394012451171875
Batch 10/64 loss: -0.3057985305786133
Batch 11/64 loss: 0.09475040435791016
Batch 12/64 loss: -0.05698680877685547
Batch 13/64 loss: -0.14978885650634766
Batch 14/64 loss: 0.36031484603881836
Batch 15/64 loss: -0.19852447509765625
Batch 16/64 loss: -0.0319671630859375
Batch 17/64 loss: -0.25092029571533203
Batch 18/64 loss: -0.01938772201538086
Batch 19/64 loss: -0.32407665252685547
Batch 20/64 loss: -0.30388355255126953
Batch 21/64 loss: -0.24741649627685547
Batch 22/64 loss: -0.09436798095703125
Batch 23/64 loss: -0.17563962936401367
Batch 24/64 loss: 0.21453619003295898
Batch 25/64 loss: 0.042653560638427734
Batch 26/64 loss: 0.4507632255554199
Batch 27/64 loss: -0.18526935577392578
Batch 28/64 loss: 0.1936640739440918
Batch 29/64 loss: -0.04931497573852539
Batch 30/64 loss: -0.05814504623413086
Batch 31/64 loss: 0.20448541641235352
Batch 32/64 loss: -0.3507084846496582
Batch 33/64 loss: -0.5584254264831543
Batch 34/64 loss: -0.17251825332641602
Batch 35/64 loss: -0.00878286361694336
Batch 36/64 loss: 0.3149857521057129
Batch 37/64 loss: 0.17862844467163086
Batch 38/64 loss: -0.46044492721557617
Batch 39/64 loss: 0.27977561950683594
Batch 40/64 loss: 0.7102184295654297
Batch 41/64 loss: -0.5890989303588867
Batch 42/64 loss: -0.07491540908813477
Batch 43/64 loss: 0.337158203125
Batch 44/64 loss: 0.08909320831298828
Batch 45/64 loss: -0.2885885238647461
Batch 46/64 loss: -0.07040214538574219
Batch 47/64 loss: 0.027278423309326172
Batch 48/64 loss: -0.5649552345275879
Batch 49/64 loss: -0.47481679916381836
Batch 50/64 loss: 0.048168182373046875
Batch 51/64 loss: -0.2500185966491699
Batch 52/64 loss: -0.0012063980102539062
Batch 53/64 loss: 0.05753183364868164
Batch 54/64 loss: -0.3948822021484375
Batch 55/64 loss: -0.1643662452697754
Batch 56/64 loss: -0.40342092514038086
Batch 57/64 loss: 0.4913806915283203
Batch 58/64 loss: -0.28708410263061523
Batch 59/64 loss: -0.28868865966796875
Batch 60/64 loss: -0.29317378997802734
Batch 61/64 loss: -0.3709402084350586
Batch 62/64 loss: -0.1876668930053711
Batch 63/64 loss: -0.18543052673339844
Batch 64/64 loss: -3.3710098266601562
Epoch 119  Train loss: -0.14348448959051394  Val loss: -0.250466730176788
Epoch 120
-------------------------------
Batch 1/64 loss: -0.2528653144836426
Batch 2/64 loss: -0.4107508659362793
Batch 3/64 loss: 0.05753660202026367
Batch 4/64 loss: -0.015741348266601562
Batch 5/64 loss: -0.5122256278991699
Batch 6/64 loss: -0.37088632583618164
Batch 7/64 loss: -0.17383432388305664
Batch 8/64 loss: -0.2903714179992676
Batch 9/64 loss: 0.052442073822021484
Batch 10/64 loss: 0.116943359375
Batch 11/64 loss: -0.44142723083496094
Batch 12/64 loss: -0.14456653594970703
Batch 13/64 loss: -0.042829036712646484
Batch 14/64 loss: 0.4723515510559082
Batch 15/64 loss: -0.2260880470275879
Batch 16/64 loss: 0.08466911315917969
Batch 17/64 loss: -0.21608638763427734
Batch 18/64 loss: -0.2846393585205078
Batch 19/64 loss: 0.13243389129638672
Batch 20/64 loss: -0.4059004783630371
Batch 21/64 loss: -0.35735654830932617
Batch 22/64 loss: -0.3081021308898926
Batch 23/64 loss: -0.3559885025024414
Batch 24/64 loss: -0.026363849639892578
Batch 25/64 loss: -0.3721804618835449
Batch 26/64 loss: -0.3865065574645996
Batch 27/64 loss: 0.029514312744140625
Batch 28/64 loss: -0.1986541748046875
Batch 29/64 loss: 0.13524341583251953
Batch 30/64 loss: 0.20964717864990234
Batch 31/64 loss: -0.19121885299682617
Batch 32/64 loss: 0.1629505157470703
Batch 33/64 loss: 0.5218491554260254
Batch 34/64 loss: -0.32172346115112305
Batch 35/64 loss: -0.2608027458190918
Batch 36/64 loss: 0.35962533950805664
Batch 37/64 loss: -0.3264141082763672
Batch 38/64 loss: -0.36001110076904297
Batch 39/64 loss: -0.03432273864746094
Batch 40/64 loss: -0.2908148765563965
Batch 41/64 loss: -0.6117439270019531
Batch 42/64 loss: -0.43805551528930664
Batch 43/64 loss: -0.11091184616088867
Batch 44/64 loss: 0.07980966567993164
Batch 45/64 loss: -0.23687314987182617
Batch 46/64 loss: -0.11477422714233398
Batch 47/64 loss: 0.7962455749511719
Batch 48/64 loss: 0.1326751708984375
Batch 49/64 loss: -0.18221092224121094
Batch 50/64 loss: -0.12587785720825195
Batch 51/64 loss: 0.0666360855102539
Batch 52/64 loss: -0.19676589965820312
Batch 53/64 loss: -0.39653444290161133
Batch 54/64 loss: -0.5225510597229004
Batch 55/64 loss: 0.015845298767089844
Batch 56/64 loss: -0.30211687088012695
Batch 57/64 loss: 0.013834953308105469
Batch 58/64 loss: -0.1306910514831543
Batch 59/64 loss: -0.445767879486084
Batch 60/64 loss: -0.24994468688964844
Batch 61/64 loss: -0.2785677909851074
Batch 62/64 loss: 0.17183446884155273
Batch 63/64 loss: -0.01430368423461914
Batch 64/64 loss: -4.028712272644043
Epoch 120  Train loss: -0.17797348546046837  Val loss: -0.2823262296591428
Epoch 121
-------------------------------
Batch 1/64 loss: -0.3369560241699219
Batch 2/64 loss: -0.047034263610839844
Batch 3/64 loss: 0.3285360336303711
Batch 4/64 loss: -0.19136571884155273
Batch 5/64 loss: -0.31371355056762695
Batch 6/64 loss: -0.37767457962036133
Batch 7/64 loss: -0.5710177421569824
Batch 8/64 loss: -0.23041963577270508
Batch 9/64 loss: -0.34602975845336914
Batch 10/64 loss: -0.2064833641052246
Batch 11/64 loss: -0.08077383041381836
Batch 12/64 loss: 0.8922772407531738
Batch 13/64 loss: 0.05753278732299805
Batch 14/64 loss: 0.43422365188598633
Batch 15/64 loss: -0.14098787307739258
Batch 16/64 loss: -0.039545536041259766
Batch 17/64 loss: 0.12633228302001953
Batch 18/64 loss: 0.2822103500366211
Batch 19/64 loss: 0.09560108184814453
Batch 20/64 loss: -0.29178810119628906
Batch 21/64 loss: 0.0544428825378418
Batch 22/64 loss: -0.5030159950256348
Batch 23/64 loss: 0.13724708557128906
Batch 24/64 loss: -0.19675874710083008
Batch 25/64 loss: -0.5175161361694336
Batch 26/64 loss: -0.00043964385986328125
Batch 27/64 loss: -0.13406896591186523
Batch 28/64 loss: -0.47318458557128906
Batch 29/64 loss: 0.15913915634155273
Batch 30/64 loss: -0.20418214797973633
Batch 31/64 loss: -0.2101435661315918
Batch 32/64 loss: -0.10170173645019531
Batch 33/64 loss: -0.462277889251709
Batch 34/64 loss: -0.48809051513671875
Batch 35/64 loss: -0.40810203552246094
Batch 36/64 loss: 0.022807598114013672
Batch 37/64 loss: -0.4958462715148926
Batch 38/64 loss: -0.2827129364013672
Batch 39/64 loss: 0.27960824966430664
Batch 40/64 loss: -0.16412878036499023
Batch 41/64 loss: 0.1830606460571289
Batch 42/64 loss: -0.03322172164916992
Batch 43/64 loss: 0.07735443115234375
Batch 44/64 loss: -0.20859050750732422
Batch 45/64 loss: -0.3683309555053711
Batch 46/64 loss: -0.01981639862060547
Batch 47/64 loss: 0.1321268081665039
Batch 48/64 loss: -0.37636566162109375
Batch 49/64 loss: -0.32479286193847656
Batch 50/64 loss: -0.4597773551940918
Batch 51/64 loss: -0.31632566452026367
Batch 52/64 loss: -0.0036401748657226562
Batch 53/64 loss: 0.44856786727905273
Batch 54/64 loss: -0.26771116256713867
Batch 55/64 loss: 0.08009481430053711
Batch 56/64 loss: 0.023923397064208984
Batch 57/64 loss: -0.12566852569580078
Batch 58/64 loss: 0.40375614166259766
Batch 59/64 loss: -0.4384636878967285
Batch 60/64 loss: -0.09758234024047852
Batch 61/64 loss: -0.3953876495361328
Batch 62/64 loss: -0.012399673461914062
Batch 63/64 loss: -0.4203653335571289
Batch 64/64 loss: -3.7256155014038086
Epoch 121  Train loss: -0.16093754861869064  Val loss: -0.32380749679512993
Epoch 122
-------------------------------
Batch 1/64 loss: -0.5937724113464355
Batch 2/64 loss: -0.1764512062072754
Batch 3/64 loss: -0.1773052215576172
Batch 4/64 loss: -0.06383895874023438
Batch 5/64 loss: -0.2598090171813965
Batch 6/64 loss: 0.44376611709594727
Batch 7/64 loss: 0.5076322555541992
Batch 8/64 loss: -0.07324934005737305
Batch 9/64 loss: -0.319000244140625
Batch 10/64 loss: -0.2341451644897461
Batch 11/64 loss: -0.3586587905883789
Batch 12/64 loss: -0.09573554992675781
Batch 13/64 loss: 0.5752863883972168
Batch 14/64 loss: -0.0570225715637207
Batch 15/64 loss: -0.3533449172973633
Batch 16/64 loss: -0.004916667938232422
Batch 17/64 loss: 0.2142038345336914
Batch 18/64 loss: 0.2769908905029297
Batch 19/64 loss: -0.46028566360473633
Batch 20/64 loss: -0.3075737953186035
Batch 21/64 loss: -0.1532893180847168
Batch 22/64 loss: -0.0917506217956543
Batch 23/64 loss: 0.18008852005004883
Batch 24/64 loss: 0.1325225830078125
Batch 25/64 loss: -0.10247039794921875
Batch 26/64 loss: 0.7387828826904297
Batch 27/64 loss: -0.21654462814331055
Batch 28/64 loss: 0.12698078155517578
Batch 29/64 loss: -0.2771296501159668
Batch 30/64 loss: 0.14505720138549805
Batch 31/64 loss: 0.19399261474609375
Batch 32/64 loss: -0.24795913696289062
Batch 33/64 loss: -0.2675008773803711
Batch 34/64 loss: -0.24871253967285156
Batch 35/64 loss: 0.21324539184570312
Batch 36/64 loss: -0.14685726165771484
Batch 37/64 loss: -0.33838987350463867
Batch 38/64 loss: -0.2618699073791504
Batch 39/64 loss: -0.2683882713317871
Batch 40/64 loss: -0.017586708068847656
Batch 41/64 loss: 0.2891044616699219
Batch 42/64 loss: -0.15956878662109375
Batch 43/64 loss: -0.5382943153381348
Batch 44/64 loss: 0.3568844795227051
Batch 45/64 loss: -0.2770724296569824
Batch 46/64 loss: 0.4792475700378418
Batch 47/64 loss: 0.044991493225097656
Batch 48/64 loss: -0.16119766235351562
Batch 49/64 loss: -0.24207353591918945
Batch 50/64 loss: -0.3412604331970215
Batch 51/64 loss: -0.28815364837646484
Batch 52/64 loss: -0.3032417297363281
Batch 53/64 loss: -0.1620783805847168
Batch 54/64 loss: 0.03172636032104492
Batch 55/64 loss: 0.17383050918579102
Batch 56/64 loss: -0.2253103256225586
Batch 57/64 loss: -0.27373456954956055
Batch 58/64 loss: 0.6714191436767578
Batch 59/64 loss: 0.6920738220214844
Batch 60/64 loss: -0.23397207260131836
Batch 61/64 loss: -0.20286130905151367
Batch 62/64 loss: 0.07244014739990234
Batch 63/64 loss: -0.25665950775146484
Batch 64/64 loss: -4.004721641540527
Epoch 122  Train loss: -0.09854605805640128  Val loss: -0.21121117503372663
Epoch 123
-------------------------------
Batch 1/64 loss: 0.33037233352661133
Batch 2/64 loss: 0.1143803596496582
Batch 3/64 loss: 0.17525768280029297
Batch 4/64 loss: -0.41344642639160156
Batch 5/64 loss: -0.33696413040161133
Batch 6/64 loss: -0.41959285736083984
Batch 7/64 loss: 0.3512601852416992
Batch 8/64 loss: 0.45894289016723633
Batch 9/64 loss: -0.0759587287902832
Batch 10/64 loss: 0.2347583770751953
Batch 11/64 loss: -0.09793615341186523
Batch 12/64 loss: -0.01697397232055664
Batch 13/64 loss: 0.0682077407836914
Batch 14/64 loss: -0.2596726417541504
Batch 15/64 loss: -0.2166743278503418
Batch 16/64 loss: -0.3796834945678711
Batch 17/64 loss: -0.36255550384521484
Batch 18/64 loss: -0.3905520439147949
Batch 19/64 loss: -0.5504512786865234
Batch 20/64 loss: -0.4553341865539551
Batch 21/64 loss: -0.19525480270385742
Batch 22/64 loss: -0.28278493881225586
Batch 23/64 loss: 0.3606600761413574
Batch 24/64 loss: 0.10370445251464844
Batch 25/64 loss: -0.48232078552246094
Batch 26/64 loss: 0.32757139205932617
Batch 27/64 loss: -0.022727489471435547
Batch 28/64 loss: -0.32096290588378906
Batch 29/64 loss: -0.12078571319580078
Batch 30/64 loss: -0.34920740127563477
Batch 31/64 loss: -0.2821683883666992
Batch 32/64 loss: -0.2749457359313965
Batch 33/64 loss: 0.10101938247680664
Batch 34/64 loss: -0.36562538146972656
Batch 35/64 loss: 0.3406801223754883
Batch 36/64 loss: 0.35069990158081055
Batch 37/64 loss: -0.07325077056884766
Batch 38/64 loss: -0.21685123443603516
Batch 39/64 loss: -0.3017144203186035
Batch 40/64 loss: -0.15547657012939453
Batch 41/64 loss: -0.46241188049316406
Batch 42/64 loss: 0.08089923858642578
Batch 43/64 loss: -0.25286436080932617
Batch 44/64 loss: 0.16079473495483398
Batch 45/64 loss: 0.2120223045349121
Batch 46/64 loss: -0.16182947158813477
Batch 47/64 loss: 0.06363248825073242
Batch 48/64 loss: -0.1759052276611328
Batch 49/64 loss: -0.198486328125
Batch 50/64 loss: 0.10947799682617188
Batch 51/64 loss: -0.3485555648803711
Batch 52/64 loss: -0.046694278717041016
Batch 53/64 loss: -0.18079042434692383
Batch 54/64 loss: 0.021142959594726562
Batch 55/64 loss: -0.18874168395996094
Batch 56/64 loss: 1.1289854049682617
Batch 57/64 loss: -0.19383525848388672
Batch 58/64 loss: 0.2433938980102539
Batch 59/64 loss: -0.19744205474853516
Batch 60/64 loss: -0.6417322158813477
Batch 61/64 loss: -0.12282276153564453
Batch 62/64 loss: 0.011941909790039062
Batch 63/64 loss: -0.30113935470581055
Batch 64/64 loss: -3.85923433303833
Epoch 123  Train loss: -0.13235675400378658  Val loss: -0.21177197485854946
Epoch 124
-------------------------------
Batch 1/64 loss: -0.25977039337158203
Batch 2/64 loss: -0.2852802276611328
Batch 3/64 loss: 0.19180631637573242
Batch 4/64 loss: -0.30461645126342773
Batch 5/64 loss: -0.25814294815063477
Batch 6/64 loss: -0.4178776741027832
Batch 7/64 loss: -0.4061145782470703
Batch 8/64 loss: -0.3210153579711914
Batch 9/64 loss: 0.04164600372314453
Batch 10/64 loss: 0.008449554443359375
Batch 11/64 loss: -0.35625410079956055
Batch 12/64 loss: -0.04812192916870117
Batch 13/64 loss: 0.9562487602233887
Batch 14/64 loss: -0.15372037887573242
Batch 15/64 loss: -0.22466468811035156
Batch 16/64 loss: -0.7380967140197754
Batch 17/64 loss: -0.12750864028930664
Batch 18/64 loss: 0.36870288848876953
Batch 19/64 loss: 0.10603666305541992
Batch 20/64 loss: 0.03959512710571289
Batch 21/64 loss: 0.4758167266845703
Batch 22/64 loss: 0.09425878524780273
Batch 23/64 loss: -0.255584716796875
Batch 24/64 loss: 0.004908084869384766
Batch 25/64 loss: -0.38231611251831055
Batch 26/64 loss: -0.046178340911865234
Batch 27/64 loss: -0.20728445053100586
Batch 28/64 loss: -0.3632488250732422
Batch 29/64 loss: 0.17009353637695312
Batch 30/64 loss: 0.09022092819213867
Batch 31/64 loss: -0.150909423828125
Batch 32/64 loss: -0.1158304214477539
Batch 33/64 loss: 0.2814631462097168
Batch 34/64 loss: -0.10052013397216797
Batch 35/64 loss: -0.3773465156555176
Batch 36/64 loss: 0.24462413787841797
Batch 37/64 loss: 0.13677215576171875
Batch 38/64 loss: 0.35973215103149414
Batch 39/64 loss: -0.5255460739135742
Batch 40/64 loss: -0.5558037757873535
Batch 41/64 loss: -0.4169936180114746
Batch 42/64 loss: -0.29943418502807617
Batch 43/64 loss: -0.25026702880859375
Batch 44/64 loss: 0.38454151153564453
Batch 45/64 loss: -0.5588102340698242
Batch 46/64 loss: -0.3305845260620117
Batch 47/64 loss: -0.1835484504699707
Batch 48/64 loss: -0.26062917709350586
Batch 49/64 loss: -0.32869958877563477
Batch 50/64 loss: -0.18766069412231445
Batch 51/64 loss: 0.16130447387695312
Batch 52/64 loss: -0.4087405204772949
Batch 53/64 loss: 0.23998022079467773
Batch 54/64 loss: -0.3587827682495117
Batch 55/64 loss: 0.10069894790649414
Batch 56/64 loss: -0.4869394302368164
Batch 57/64 loss: 0.09674978256225586
Batch 58/64 loss: -0.15605449676513672
Batch 59/64 loss: -0.0005445480346679688
Batch 60/64 loss: -0.34456777572631836
Batch 61/64 loss: 0.03853034973144531
Batch 62/64 loss: -0.26565980911254883
Batch 63/64 loss: -0.23606204986572266
Batch 64/64 loss: -3.9044837951660156
Epoch 124  Train loss: -0.16301042145373776  Val loss: -0.2390602609955568
Epoch 125
-------------------------------
Batch 1/64 loss: -0.21683502197265625
Batch 2/64 loss: -0.13794183731079102
Batch 3/64 loss: 0.07301759719848633
Batch 4/64 loss: -0.3206944465637207
Batch 5/64 loss: -0.11899185180664062
Batch 6/64 loss: 0.4191441535949707
Batch 7/64 loss: -0.4136238098144531
Batch 8/64 loss: 0.04094982147216797
Batch 9/64 loss: -0.06454801559448242
Batch 10/64 loss: -0.4168391227722168
Batch 11/64 loss: -0.20035552978515625
Batch 12/64 loss: -0.3262910842895508
Batch 13/64 loss: 0.33998918533325195
Batch 14/64 loss: -0.1959385871887207
Batch 15/64 loss: -0.11482954025268555
Batch 16/64 loss: 0.7381343841552734
Batch 17/64 loss: -0.12436294555664062
Batch 18/64 loss: -0.3265061378479004
Batch 19/64 loss: 0.12916803359985352
Batch 20/64 loss: -0.4010944366455078
Batch 21/64 loss: -0.3113365173339844
Batch 22/64 loss: -0.2673807144165039
Batch 23/64 loss: -0.007880210876464844
Batch 24/64 loss: 0.3292074203491211
Batch 25/64 loss: -0.233612060546875
Batch 26/64 loss: -0.0776219367980957
Batch 27/64 loss: -0.16051292419433594
Batch 28/64 loss: -0.1342787742614746
Batch 29/64 loss: 0.2485184669494629
Batch 30/64 loss: 0.07484769821166992
Batch 31/64 loss: -0.13804864883422852
Batch 32/64 loss: -0.25697755813598633
Batch 33/64 loss: -0.2314319610595703
Batch 34/64 loss: -0.05227231979370117
Batch 35/64 loss: 0.014814376831054688
Batch 36/64 loss: -0.35582971572875977
Batch 37/64 loss: -0.5917820930480957
Batch 38/64 loss: -0.4730954170227051
Batch 39/64 loss: -0.3244156837463379
Batch 40/64 loss: -0.23808622360229492
Batch 41/64 loss: -0.3166179656982422
Batch 42/64 loss: 0.23750686645507812
Batch 43/64 loss: -0.057529449462890625
Batch 44/64 loss: -0.25006866455078125
Batch 45/64 loss: -0.15743589401245117
Batch 46/64 loss: 0.023953914642333984
Batch 47/64 loss: -0.20494365692138672
Batch 48/64 loss: -0.2265167236328125
Batch 49/64 loss: -0.02644062042236328
Batch 50/64 loss: -0.40267372131347656
Batch 51/64 loss: -0.20980119705200195
Batch 52/64 loss: -0.1325678825378418
Batch 53/64 loss: -0.4483766555786133
Batch 54/64 loss: -0.3379955291748047
Batch 55/64 loss: -0.03821849822998047
Batch 56/64 loss: -0.13564538955688477
Batch 57/64 loss: -0.0443572998046875
Batch 58/64 loss: -0.3035750389099121
Batch 59/64 loss: -0.45384788513183594
Batch 60/64 loss: 0.10276269912719727
Batch 61/64 loss: -0.20228910446166992
Batch 62/64 loss: 0.021995067596435547
Batch 63/64 loss: 0.04333686828613281
Batch 64/64 loss: -3.523189067840576
Epoch 125  Train loss: -0.17235076941695868  Val loss: -0.27363332112630206
Epoch 126
-------------------------------
Batch 1/64 loss: 0.08415508270263672
Batch 2/64 loss: -0.2727952003479004
Batch 3/64 loss: -0.35681581497192383
Batch 4/64 loss: -0.18562746047973633
Batch 5/64 loss: -0.16310930252075195
Batch 6/64 loss: -0.5102739334106445
Batch 7/64 loss: -0.324343204498291
Batch 8/64 loss: 0.07980775833129883
Batch 9/64 loss: 0.09995746612548828
Batch 10/64 loss: -0.022043704986572266
Batch 11/64 loss: -0.27497005462646484
Batch 12/64 loss: 0.23015451431274414
Batch 13/64 loss: -0.1577320098876953
Batch 14/64 loss: 0.02507305145263672
Batch 15/64 loss: 0.053577423095703125
Batch 16/64 loss: -0.20235586166381836
Batch 17/64 loss: 0.10489416122436523
Batch 18/64 loss: -0.18018770217895508
Batch 19/64 loss: -0.01716470718383789
Batch 20/64 loss: -0.24187564849853516
Batch 21/64 loss: -0.2867879867553711
Batch 22/64 loss: -0.40779876708984375
Batch 23/64 loss: -0.10189533233642578
Batch 24/64 loss: 0.035105228424072266
Batch 25/64 loss: 0.05702352523803711
Batch 26/64 loss: -0.3590550422668457
Batch 27/64 loss: -0.46635913848876953
Batch 28/64 loss: 0.08803415298461914
Batch 29/64 loss: -0.1907215118408203
Batch 30/64 loss: 0.007392406463623047
Batch 31/64 loss: -0.43331050872802734
Batch 32/64 loss: -0.35477685928344727
Batch 33/64 loss: -0.5302290916442871
Batch 34/64 loss: -0.46210432052612305
Batch 35/64 loss: -0.02146434783935547
Batch 36/64 loss: -0.31719255447387695
Batch 37/64 loss: 0.14904451370239258
Batch 38/64 loss: -0.2702493667602539
Batch 39/64 loss: 0.0516352653503418
Batch 40/64 loss: -0.19006061553955078
Batch 41/64 loss: -0.3683314323425293
Batch 42/64 loss: -0.07248544692993164
Batch 43/64 loss: -0.043004512786865234
Batch 44/64 loss: -0.4700798988342285
Batch 45/64 loss: -0.7364139556884766
Batch 46/64 loss: 0.2659158706665039
Batch 47/64 loss: -0.3159966468811035
Batch 48/64 loss: -0.19538068771362305
Batch 49/64 loss: -0.33942079544067383
Batch 50/64 loss: -0.37128162384033203
Batch 51/64 loss: -0.41336679458618164
Batch 52/64 loss: -0.38690996170043945
Batch 53/64 loss: -0.39599084854125977
Batch 54/64 loss: -0.41149425506591797
Batch 55/64 loss: 0.05144071578979492
Batch 56/64 loss: -0.47123193740844727
Batch 57/64 loss: -0.3987698554992676
Batch 58/64 loss: -0.21866512298583984
Batch 59/64 loss: -0.3969082832336426
Batch 60/64 loss: -0.5867528915405273
Batch 61/64 loss: -0.40494680404663086
Batch 62/64 loss: -0.4529414176940918
Batch 63/64 loss: -0.2679004669189453
Batch 64/64 loss: -3.86374568939209
Epoch 126  Train loss: -0.2593595579558728  Val loss: -0.3806038820456803
Saving best model, epoch: 126
Epoch 127
-------------------------------
Batch 1/64 loss: -0.16821861267089844
Batch 2/64 loss: -0.4593639373779297
Batch 3/64 loss: -0.45357608795166016
Batch 4/64 loss: -0.31139612197875977
Batch 5/64 loss: -0.21416807174682617
Batch 6/64 loss: -0.3589138984680176
Batch 7/64 loss: -0.006792545318603516
Batch 8/64 loss: 0.2733159065246582
Batch 9/64 loss: -0.14435577392578125
Batch 10/64 loss: -0.21991586685180664
Batch 11/64 loss: -0.4127187728881836
Batch 12/64 loss: -0.032047271728515625
Batch 13/64 loss: -0.11177301406860352
Batch 14/64 loss: -0.369204044342041
Batch 15/64 loss: -0.4860963821411133
Batch 16/64 loss: -0.055707454681396484
Batch 17/64 loss: -0.03935384750366211
Batch 18/64 loss: -0.733182430267334
Batch 19/64 loss: -0.4120016098022461
Batch 20/64 loss: -0.1926569938659668
Batch 21/64 loss: -0.6441307067871094
Batch 22/64 loss: -0.5505132675170898
Batch 23/64 loss: -0.443967342376709
Batch 24/64 loss: -0.09853887557983398
Batch 25/64 loss: -0.5426888465881348
Batch 26/64 loss: -0.20549869537353516
Batch 27/64 loss: 0.18925857543945312
Batch 28/64 loss: 0.09902477264404297
Batch 29/64 loss: -0.14530324935913086
Batch 30/64 loss: 0.3028583526611328
Batch 31/64 loss: -0.1588306427001953
Batch 32/64 loss: -0.2899298667907715
Batch 33/64 loss: 0.016103267669677734
Batch 34/64 loss: -0.028435707092285156
Batch 35/64 loss: -0.2363882064819336
Batch 36/64 loss: 0.2921266555786133
Batch 37/64 loss: 0.15694808959960938
Batch 38/64 loss: -0.5202479362487793
Batch 39/64 loss: 0.17108726501464844
Batch 40/64 loss: -0.10436630249023438
Batch 41/64 loss: -0.036275386810302734
Batch 42/64 loss: -0.45333099365234375
Batch 43/64 loss: -0.42018604278564453
Batch 44/64 loss: -0.07721376419067383
Batch 45/64 loss: 0.22155523300170898
Batch 46/64 loss: -0.32973289489746094
Batch 47/64 loss: -0.4252004623413086
Batch 48/64 loss: -0.3296346664428711
Batch 49/64 loss: -0.029433727264404297
Batch 50/64 loss: 0.1292285919189453
Batch 51/64 loss: -0.3047175407409668
Batch 52/64 loss: 0.7601580619812012
Batch 53/64 loss: -0.4482564926147461
Batch 54/64 loss: -0.34964561462402344
Batch 55/64 loss: 0.19625473022460938
Batch 56/64 loss: -0.5627102851867676
Batch 57/64 loss: -0.1148676872253418
Batch 58/64 loss: -0.11312246322631836
Batch 59/64 loss: -0.11345767974853516
Batch 60/64 loss: -0.3381824493408203
Batch 61/64 loss: 0.06297540664672852
Batch 62/64 loss: -0.6438817977905273
Batch 63/64 loss: -0.20855045318603516
Batch 64/64 loss: -4.217662811279297
Epoch 127  Train loss: -0.23123192132688036  Val loss: -0.2514281125412774
Epoch 128
-------------------------------
Batch 1/64 loss: 0.18098068237304688
Batch 2/64 loss: -0.19495153427124023
Batch 3/64 loss: -0.022713661193847656
Batch 4/64 loss: -0.4010443687438965
Batch 5/64 loss: -0.30354738235473633
Batch 6/64 loss: 0.1670074462890625
Batch 7/64 loss: -0.040490150451660156
Batch 8/64 loss: -0.41602563858032227
Batch 9/64 loss: -0.3192605972290039
Batch 10/64 loss: -0.20479774475097656
Batch 11/64 loss: -0.45777177810668945
Batch 12/64 loss: 0.1399393081665039
Batch 13/64 loss: -0.20678091049194336
Batch 14/64 loss: -0.3093562126159668
Batch 15/64 loss: -0.3327174186706543
Batch 16/64 loss: -0.6045947074890137
Batch 17/64 loss: 0.3631572723388672
Batch 18/64 loss: -0.020727157592773438
Batch 19/64 loss: -0.4379611015319824
Batch 20/64 loss: -0.03593301773071289
Batch 21/64 loss: -0.002826213836669922
Batch 22/64 loss: -0.05035400390625
Batch 23/64 loss: -0.11944580078125
Batch 24/64 loss: -0.19619035720825195
Batch 25/64 loss: -0.1102590560913086
Batch 26/64 loss: -0.16286134719848633
Batch 27/64 loss: -0.004670143127441406
Batch 28/64 loss: -0.3111262321472168
Batch 29/64 loss: 0.15166091918945312
Batch 30/64 loss: -0.2199082374572754
Batch 31/64 loss: 0.04734230041503906
Batch 32/64 loss: -0.13780498504638672
Batch 33/64 loss: -0.41388607025146484
Batch 34/64 loss: -0.1848282814025879
Batch 35/64 loss: -0.37522029876708984
Batch 36/64 loss: -0.18265342712402344
Batch 37/64 loss: -0.1975703239440918
Batch 38/64 loss: -0.3166694641113281
Batch 39/64 loss: 0.18881940841674805
Batch 40/64 loss: -0.43166065216064453
Batch 41/64 loss: -0.007855892181396484
Batch 42/64 loss: -0.33622217178344727
Batch 43/64 loss: -0.45368385314941406
Batch 44/64 loss: -0.525144100189209
Batch 45/64 loss: -0.32378530502319336
Batch 46/64 loss: 0.014292716979980469
Batch 47/64 loss: -0.13887977600097656
Batch 48/64 loss: 0.5505800247192383
Batch 49/64 loss: -0.3667874336242676
Batch 50/64 loss: -0.2593245506286621
Batch 51/64 loss: 0.21719741821289062
Batch 52/64 loss: -0.3577570915222168
Batch 53/64 loss: 0.05254793167114258
Batch 54/64 loss: -0.19375991821289062
Batch 55/64 loss: 0.08301162719726562
Batch 56/64 loss: 0.035597801208496094
Batch 57/64 loss: -0.3361701965332031
Batch 58/64 loss: -0.18444347381591797
Batch 59/64 loss: -0.3329200744628906
Batch 60/64 loss: -0.2630167007446289
Batch 61/64 loss: 0.06828832626342773
Batch 62/64 loss: 0.5418190956115723
Batch 63/64 loss: -0.3381514549255371
Batch 64/64 loss: -3.8612308502197266
Epoch 128  Train loss: -0.19197162553375843  Val loss: -0.29779675408327294
Epoch 129
-------------------------------
Batch 1/64 loss: 0.008315563201904297
Batch 2/64 loss: -0.14202213287353516
Batch 3/64 loss: -0.013512611389160156
Batch 4/64 loss: -0.04837512969970703
Batch 5/64 loss: 0.04471588134765625
Batch 6/64 loss: -0.2321176528930664
Batch 7/64 loss: -0.05164194107055664
Batch 8/64 loss: -0.41037607192993164
Batch 9/64 loss: 0.12546491622924805
Batch 10/64 loss: -0.20665740966796875
Batch 11/64 loss: 0.14124059677124023
Batch 12/64 loss: -0.23588275909423828
Batch 13/64 loss: -0.10899734497070312
Batch 14/64 loss: -0.012410640716552734
Batch 15/64 loss: -0.22138261795043945
Batch 16/64 loss: 0.17802715301513672
Batch 17/64 loss: 0.4201645851135254
Batch 18/64 loss: -0.1775832176208496
Batch 19/64 loss: -0.14865922927856445
Batch 20/64 loss: -0.06645679473876953
Batch 21/64 loss: -0.39690351486206055
Batch 22/64 loss: -0.3981318473815918
Batch 23/64 loss: -0.4436779022216797
Batch 24/64 loss: -0.37021636962890625
Batch 25/64 loss: -0.2309093475341797
Batch 26/64 loss: -0.1985030174255371
Batch 27/64 loss: -0.022485733032226562
Batch 28/64 loss: -0.29973745346069336
Batch 29/64 loss: -0.22838068008422852
Batch 30/64 loss: -0.42697715759277344
Batch 31/64 loss: -0.11477279663085938
Batch 32/64 loss: -0.523827075958252
Batch 33/64 loss: -0.5314984321594238
Batch 34/64 loss: -0.2695808410644531
Batch 35/64 loss: 0.04074382781982422
Batch 36/64 loss: -0.08796310424804688
Batch 37/64 loss: 0.9540634155273438
Batch 38/64 loss: 0.015233039855957031
Batch 39/64 loss: 0.23314332962036133
Batch 40/64 loss: -0.06646490097045898
Batch 41/64 loss: -0.22305583953857422
Batch 42/64 loss: -0.379971981048584
Batch 43/64 loss: -0.279050350189209
Batch 44/64 loss: -0.28966808319091797
Batch 45/64 loss: -0.3927030563354492
Batch 46/64 loss: -0.25051069259643555
Batch 47/64 loss: -0.12830591201782227
Batch 48/64 loss: -0.23218965530395508
Batch 49/64 loss: 0.030886173248291016
Batch 50/64 loss: -0.6461825370788574
Batch 51/64 loss: -0.6455583572387695
Batch 52/64 loss: 0.05766773223876953
Batch 53/64 loss: -0.37010812759399414
Batch 54/64 loss: -0.297238826751709
Batch 55/64 loss: -0.19942808151245117
Batch 56/64 loss: -0.10701227188110352
Batch 57/64 loss: 0.15156221389770508
Batch 58/64 loss: -0.20726299285888672
Batch 59/64 loss: -0.24555158615112305
Batch 60/64 loss: -0.4324021339416504
Batch 61/64 loss: -0.32711315155029297
Batch 62/64 loss: -0.35825634002685547
Batch 63/64 loss: -0.0948333740234375
Batch 64/64 loss: -4.034567832946777
Epoch 129  Train loss: -0.21046598471847236  Val loss: -0.36602823840793464
Epoch 130
-------------------------------
Batch 1/64 loss: -0.4550609588623047
Batch 2/64 loss: -0.17287921905517578
Batch 3/64 loss: -0.23894023895263672
Batch 4/64 loss: -0.6142807006835938
Batch 5/64 loss: -0.06244087219238281
Batch 6/64 loss: -0.29666709899902344
Batch 7/64 loss: 0.03680896759033203
Batch 8/64 loss: -0.3506612777709961
Batch 9/64 loss: -0.34623241424560547
Batch 10/64 loss: -0.1682133674621582
Batch 11/64 loss: -0.13283586502075195
Batch 12/64 loss: 0.18562030792236328
Batch 13/64 loss: -0.017178058624267578
Batch 14/64 loss: -0.19797039031982422
Batch 15/64 loss: -0.24094104766845703
Batch 16/64 loss: -0.26379871368408203
Batch 17/64 loss: -0.6195802688598633
Batch 18/64 loss: -0.12571001052856445
Batch 19/64 loss: -0.20569944381713867
Batch 20/64 loss: 0.06864356994628906
Batch 21/64 loss: -0.019340991973876953
Batch 22/64 loss: 0.5062856674194336
Batch 23/64 loss: -0.36534643173217773
Batch 24/64 loss: 0.1063079833984375
Batch 25/64 loss: 0.20167255401611328
Batch 26/64 loss: -0.11716318130493164
Batch 27/64 loss: -0.0450129508972168
Batch 28/64 loss: 0.11666107177734375
Batch 29/64 loss: -0.1060647964477539
Batch 30/64 loss: -0.23160076141357422
Batch 31/64 loss: 0.17207765579223633
Batch 32/64 loss: 0.04219245910644531
Batch 33/64 loss: -0.3142843246459961
Batch 34/64 loss: -0.3674631118774414
Batch 35/64 loss: -0.3130464553833008
Batch 36/64 loss: -0.36542177200317383
Batch 37/64 loss: -0.16826391220092773
Batch 38/64 loss: -0.3428349494934082
Batch 39/64 loss: 0.02007293701171875
Batch 40/64 loss: -0.14258670806884766
Batch 41/64 loss: -0.02001190185546875
Batch 42/64 loss: -0.1376643180847168
Batch 43/64 loss: -0.18329143524169922
Batch 44/64 loss: 0.056935787200927734
Batch 45/64 loss: -0.4943375587463379
Batch 46/64 loss: -0.1691737174987793
Batch 47/64 loss: -0.28685951232910156
Batch 48/64 loss: -0.45070314407348633
Batch 49/64 loss: -0.3097414970397949
Batch 50/64 loss: 0.055867671966552734
Batch 51/64 loss: -0.18219661712646484
Batch 52/64 loss: 0.09245061874389648
Batch 53/64 loss: -0.06601858139038086
Batch 54/64 loss: -0.40736865997314453
Batch 55/64 loss: -0.1265254020690918
Batch 56/64 loss: 0.12137937545776367
Batch 57/64 loss: -0.13872814178466797
Batch 58/64 loss: -0.44475221633911133
Batch 59/64 loss: -0.11916589736938477
Batch 60/64 loss: -0.27004003524780273
Batch 61/64 loss: -0.6439604759216309
Batch 62/64 loss: -0.20994091033935547
Batch 63/64 loss: -0.27880048751831055
Batch 64/64 loss: -3.8497471809387207
Epoch 130  Train loss: -0.2109981892155666  Val loss: -0.26797858955933873
Epoch 131
-------------------------------
Batch 1/64 loss: -0.2967057228088379
Batch 2/64 loss: 0.1729116439819336
Batch 3/64 loss: -0.07125568389892578
Batch 4/64 loss: -0.2048802375793457
Batch 5/64 loss: -0.2513613700866699
Batch 6/64 loss: -0.17466402053833008
Batch 7/64 loss: -0.21065378189086914
Batch 8/64 loss: -0.18710041046142578
Batch 9/64 loss: 0.36548328399658203
Batch 10/64 loss: -0.5315151214599609
Batch 11/64 loss: -0.3248910903930664
Batch 12/64 loss: -0.6124200820922852
Batch 13/64 loss: -0.1323866844177246
Batch 14/64 loss: 0.43525123596191406
Batch 15/64 loss: 0.024999141693115234
Batch 16/64 loss: -0.20187902450561523
Batch 17/64 loss: -0.25034570693969727
Batch 18/64 loss: 0.36217212677001953
Batch 19/64 loss: -0.2963733673095703
Batch 20/64 loss: 0.03370094299316406
Batch 21/64 loss: -0.025189876556396484
Batch 22/64 loss: -0.2703666687011719
Batch 23/64 loss: 0.028312206268310547
Batch 24/64 loss: -0.10653114318847656
Batch 25/64 loss: -0.4726138114929199
Batch 26/64 loss: -0.1631460189819336
Batch 27/64 loss: -0.3802981376647949
Batch 28/64 loss: -0.21403789520263672
Batch 29/64 loss: -0.6577835083007812
Batch 30/64 loss: -0.3558025360107422
Batch 31/64 loss: 0.08955717086791992
Batch 32/64 loss: -0.2286381721496582
Batch 33/64 loss: -0.05657005310058594
Batch 34/64 loss: 0.09361553192138672
Batch 35/64 loss: -0.40554141998291016
Batch 36/64 loss: -0.43279600143432617
Batch 37/64 loss: -0.4103407859802246
Batch 38/64 loss: -0.15130376815795898
Batch 39/64 loss: -0.47338438034057617
Batch 40/64 loss: -0.32981395721435547
Batch 41/64 loss: 0.13255834579467773
Batch 42/64 loss: -0.20391607284545898
Batch 43/64 loss: 0.03203582763671875
Batch 44/64 loss: -0.3636345863342285
Batch 45/64 loss: 0.2939925193786621
Batch 46/64 loss: 0.5664730072021484
Batch 47/64 loss: 0.12065935134887695
Batch 48/64 loss: -0.27584028244018555
Batch 49/64 loss: -0.4586162567138672
Batch 50/64 loss: 0.016961097717285156
Batch 51/64 loss: -0.2068495750427246
Batch 52/64 loss: -0.7317056655883789
Batch 53/64 loss: -0.46741485595703125
Batch 54/64 loss: -0.5754470825195312
Batch 55/64 loss: -0.323458194732666
Batch 56/64 loss: 0.18182659149169922
Batch 57/64 loss: -0.4998283386230469
Batch 58/64 loss: -0.29180192947387695
Batch 59/64 loss: -0.505795955657959
Batch 60/64 loss: -0.132171630859375
Batch 61/64 loss: -0.47165679931640625
Batch 62/64 loss: -0.5461487770080566
Batch 63/64 loss: -0.5358853340148926
Batch 64/64 loss: -4.1151909828186035
Epoch 131  Train loss: -0.24481011745976466  Val loss: -0.36721970862949016
Epoch 132
-------------------------------
Batch 1/64 loss: -0.15268754959106445
Batch 2/64 loss: -0.5416231155395508
Batch 3/64 loss: -0.28598880767822266
Batch 4/64 loss: -0.33191680908203125
Batch 5/64 loss: -0.025590896606445312
Batch 6/64 loss: -0.38297080993652344
Batch 7/64 loss: -0.04211997985839844
Batch 8/64 loss: 0.09951210021972656
Batch 9/64 loss: -0.3373255729675293
Batch 10/64 loss: -0.3526272773742676
Batch 11/64 loss: 0.12238550186157227
Batch 12/64 loss: -0.4979066848754883
Batch 13/64 loss: -0.4113292694091797
Batch 14/64 loss: 0.0666341781616211
Batch 15/64 loss: -0.42755651473999023
Batch 16/64 loss: -0.2603116035461426
Batch 17/64 loss: -0.5276904106140137
Batch 18/64 loss: 0.027374744415283203
Batch 19/64 loss: -0.42059803009033203
Batch 20/64 loss: -0.42014503479003906
Batch 21/64 loss: -0.23964166641235352
Batch 22/64 loss: -0.18845176696777344
Batch 23/64 loss: -0.39909982681274414
Batch 24/64 loss: 0.018593311309814453
Batch 25/64 loss: -0.3285393714904785
Batch 26/64 loss: -0.3243556022644043
Batch 27/64 loss: 0.15297174453735352
Batch 28/64 loss: -0.3693056106567383
Batch 29/64 loss: -0.3725290298461914
Batch 30/64 loss: -0.6648283004760742
Batch 31/64 loss: -0.24294471740722656
Batch 32/64 loss: -0.5531649589538574
Batch 33/64 loss: 0.3408060073852539
Batch 34/64 loss: -0.1725621223449707
Batch 35/64 loss: -0.3812522888183594
Batch 36/64 loss: 0.11263799667358398
Batch 37/64 loss: -0.10791349411010742
Batch 38/64 loss: -0.5805978775024414
Batch 39/64 loss: 0.01268768310546875
Batch 40/64 loss: -0.26255369186401367
Batch 41/64 loss: 0.10361289978027344
Batch 42/64 loss: -0.30818748474121094
Batch 43/64 loss: 0.182342529296875
Batch 44/64 loss: -0.26794958114624023
Batch 45/64 loss: 0.1471118927001953
Batch 46/64 loss: -0.25045013427734375
Batch 47/64 loss: -0.3799905776977539
Batch 48/64 loss: -0.2601919174194336
Batch 49/64 loss: -0.03055715560913086
Batch 50/64 loss: 0.18999195098876953
Batch 51/64 loss: -0.1334848403930664
Batch 52/64 loss: -0.26226234436035156
Batch 53/64 loss: -0.19184494018554688
Batch 54/64 loss: -0.25903940200805664
Batch 55/64 loss: -0.18393898010253906
Batch 56/64 loss: 0.0618739128112793
Batch 57/64 loss: -0.23523807525634766
Batch 58/64 loss: 0.19660329818725586
Batch 59/64 loss: -0.5113968849182129
Batch 60/64 loss: -0.1912403106689453
Batch 61/64 loss: -0.09344196319580078
Batch 62/64 loss: -0.19629955291748047
Batch 63/64 loss: -0.12351274490356445
Batch 64/64 loss: -4.211894989013672
Epoch 132  Train loss: -0.24798332663143383  Val loss: -0.25684975430727824
Epoch 133
-------------------------------
Batch 1/64 loss: -0.11540937423706055
Batch 2/64 loss: 0.2253890037536621
Batch 3/64 loss: -0.5416593551635742
Batch 4/64 loss: -0.102447509765625
Batch 5/64 loss: 0.04416704177856445
Batch 6/64 loss: 0.26525020599365234
Batch 7/64 loss: -0.28661680221557617
Batch 8/64 loss: -0.17664670944213867
Batch 9/64 loss: 0.14782381057739258
Batch 10/64 loss: -0.4666461944580078
Batch 11/64 loss: -0.3593788146972656
Batch 12/64 loss: -0.14641141891479492
Batch 13/64 loss: -0.08579444885253906
Batch 14/64 loss: -0.4591360092163086
Batch 15/64 loss: -0.3238387107849121
Batch 16/64 loss: -0.18334531784057617
Batch 17/64 loss: -0.13830852508544922
Batch 18/64 loss: -0.5467829704284668
Batch 19/64 loss: -0.37267541885375977
Batch 20/64 loss: 0.6643805503845215
Batch 21/64 loss: -0.04993629455566406
Batch 22/64 loss: -0.32703065872192383
Batch 23/64 loss: -0.2396225929260254
Batch 24/64 loss: -0.2080974578857422
Batch 25/64 loss: -0.0989837646484375
Batch 26/64 loss: -0.025163650512695312
Batch 27/64 loss: -0.1360936164855957
Batch 28/64 loss: -0.25415706634521484
Batch 29/64 loss: -0.36339807510375977
Batch 30/64 loss: 0.14435625076293945
Batch 31/64 loss: -0.09083890914916992
Batch 32/64 loss: -0.4904365539550781
Batch 33/64 loss: -0.21806001663208008
Batch 34/64 loss: -0.17282962799072266
Batch 35/64 loss: 0.16376638412475586
Batch 36/64 loss: -0.09005165100097656
Batch 37/64 loss: -0.07624626159667969
Batch 38/64 loss: 0.0792856216430664
Batch 39/64 loss: -0.25049591064453125
Batch 40/64 loss: -0.3338356018066406
Batch 41/64 loss: -0.5298013687133789
Batch 42/64 loss: -0.3228797912597656
Batch 43/64 loss: -0.5643954277038574
Batch 44/64 loss: 0.40692758560180664
Batch 45/64 loss: -0.2606234550476074
Batch 46/64 loss: -0.20682477951049805
Batch 47/64 loss: -0.30135583877563477
Batch 48/64 loss: -0.05847883224487305
Batch 49/64 loss: -0.3153042793273926
Batch 50/64 loss: 0.2624177932739258
Batch 51/64 loss: -0.5404467582702637
Batch 52/64 loss: -0.30372190475463867
Batch 53/64 loss: -0.09629964828491211
Batch 54/64 loss: -0.3197026252746582
Batch 55/64 loss: -0.1275634765625
Batch 56/64 loss: -0.6428747177124023
Batch 57/64 loss: -0.4759988784790039
Batch 58/64 loss: -0.22237110137939453
Batch 59/64 loss: 0.3362722396850586
Batch 60/64 loss: -0.2208247184753418
Batch 61/64 loss: 0.12673234939575195
Batch 62/64 loss: -0.41199398040771484
Batch 63/64 loss: 0.28296327590942383
Batch 64/64 loss: -3.3494858741760254
Epoch 133  Train loss: -0.204144614350562  Val loss: -0.0959461120395726
Epoch 134
-------------------------------
Batch 1/64 loss: -0.050386905670166016
Batch 2/64 loss: -0.31154727935791016
Batch 3/64 loss: -0.4902529716491699
Batch 4/64 loss: 0.05307340621948242
Batch 5/64 loss: -0.10477828979492188
Batch 6/64 loss: -0.14523935317993164
Batch 7/64 loss: -0.18879413604736328
Batch 8/64 loss: 0.4214606285095215
Batch 9/64 loss: 0.18435239791870117
Batch 10/64 loss: -0.4885711669921875
Batch 11/64 loss: -0.21778440475463867
Batch 12/64 loss: -0.4534573554992676
Batch 13/64 loss: 0.01625967025756836
Batch 14/64 loss: -0.45671510696411133
Batch 15/64 loss: -0.20995855331420898
Batch 16/64 loss: -0.07111167907714844
Batch 17/64 loss: 0.5756087303161621
Batch 18/64 loss: -0.48525333404541016
Batch 19/64 loss: -0.13479280471801758
Batch 20/64 loss: 0.27730274200439453
Batch 21/64 loss: -0.350466251373291
Batch 22/64 loss: -0.23087120056152344
Batch 23/64 loss: 0.5859394073486328
Batch 24/64 loss: -0.22463035583496094
Batch 25/64 loss: -0.15842390060424805
Batch 26/64 loss: -0.17773199081420898
Batch 27/64 loss: 0.012744903564453125
Batch 28/64 loss: -0.029145240783691406
Batch 29/64 loss: 0.3433346748352051
Batch 30/64 loss: -0.031557559967041016
Batch 31/64 loss: 0.28647708892822266
Batch 32/64 loss: -0.3541388511657715
Batch 33/64 loss: -0.40778160095214844
Batch 34/64 loss: -0.22001314163208008
Batch 35/64 loss: -0.5571370124816895
Batch 36/64 loss: -0.3969449996948242
Batch 37/64 loss: -0.16862916946411133
Batch 38/64 loss: -0.4060792922973633
Batch 39/64 loss: -0.22790861129760742
Batch 40/64 loss: -0.6526660919189453
Batch 41/64 loss: -0.1727771759033203
Batch 42/64 loss: -0.2675924301147461
Batch 43/64 loss: 0.017539501190185547
Batch 44/64 loss: 0.050896644592285156
Batch 45/64 loss: -0.14337539672851562
Batch 46/64 loss: -0.0902714729309082
Batch 47/64 loss: -0.40192699432373047
Batch 48/64 loss: -0.2778029441833496
Batch 49/64 loss: 0.009129524230957031
Batch 50/64 loss: -0.1278529167175293
Batch 51/64 loss: -0.1589803695678711
Batch 52/64 loss: 0.2701725959777832
Batch 53/64 loss: 0.04306221008300781
Batch 54/64 loss: -0.3564467430114746
Batch 55/64 loss: -0.10451364517211914
Batch 56/64 loss: -0.08469676971435547
Batch 57/64 loss: -0.5477066040039062
Batch 58/64 loss: -0.3493638038635254
Batch 59/64 loss: 0.4703998565673828
Batch 60/64 loss: -0.08272504806518555
Batch 61/64 loss: -0.23111867904663086
Batch 62/64 loss: 0.042469024658203125
Batch 63/64 loss: 0.5516366958618164
Batch 64/64 loss: -4.158365249633789
Epoch 134  Train loss: -0.16795033473594517  Val loss: -0.3174274614996107
Epoch 135
-------------------------------
Batch 1/64 loss: -0.2658271789550781
Batch 2/64 loss: -0.02135181427001953
Batch 3/64 loss: -0.24992990493774414
Batch 4/64 loss: -0.4035921096801758
Batch 5/64 loss: -0.22209596633911133
Batch 6/64 loss: -0.42358827590942383
Batch 7/64 loss: -0.4276599884033203
Batch 8/64 loss: -0.31748151779174805
Batch 9/64 loss: -0.5300817489624023
Batch 10/64 loss: -0.15937471389770508
Batch 11/64 loss: -0.30989885330200195
Batch 12/64 loss: 0.5010933876037598
Batch 13/64 loss: -0.4658021926879883
Batch 14/64 loss: 0.017647266387939453
Batch 15/64 loss: -0.5995535850524902
Batch 16/64 loss: 0.25450563430786133
Batch 17/64 loss: -0.34401512145996094
Batch 18/64 loss: -0.14414405822753906
Batch 19/64 loss: -0.08724308013916016
Batch 20/64 loss: -0.20588445663452148
Batch 21/64 loss: -0.3842201232910156
Batch 22/64 loss: -0.33075475692749023
Batch 23/64 loss: -0.3229961395263672
Batch 24/64 loss: -0.44301700592041016
Batch 25/64 loss: -0.1378931999206543
Batch 26/64 loss: -0.4613637924194336
Batch 27/64 loss: -0.378389835357666
Batch 28/64 loss: 0.23824214935302734
Batch 29/64 loss: -0.11530542373657227
Batch 30/64 loss: -0.16559934616088867
Batch 31/64 loss: 0.18833112716674805
Batch 32/64 loss: -0.02573680877685547
Batch 33/64 loss: 0.21425771713256836
Batch 34/64 loss: -0.2024073600769043
Batch 35/64 loss: -0.005647182464599609
Batch 36/64 loss: 0.6869511604309082
Batch 37/64 loss: -0.33490562438964844
Batch 38/64 loss: -0.45807600021362305
Batch 39/64 loss: -0.1934833526611328
Batch 40/64 loss: -0.37712669372558594
Batch 41/64 loss: -0.3624076843261719
Batch 42/64 loss: 0.046916961669921875
Batch 43/64 loss: -0.25716686248779297
Batch 44/64 loss: -0.2782869338989258
Batch 45/64 loss: -0.40004491806030273
Batch 46/64 loss: -0.39891958236694336
Batch 47/64 loss: -0.38372135162353516
Batch 48/64 loss: -0.2488865852355957
Batch 49/64 loss: -0.06647205352783203
Batch 50/64 loss: -0.01432037353515625
Batch 51/64 loss: -0.11736869812011719
Batch 52/64 loss: -0.0076141357421875
Batch 53/64 loss: 0.1754145622253418
Batch 54/64 loss: -0.23926830291748047
Batch 55/64 loss: -0.07477474212646484
Batch 56/64 loss: -0.045794010162353516
Batch 57/64 loss: -0.36240625381469727
Batch 58/64 loss: -0.2549452781677246
Batch 59/64 loss: -0.3783888816833496
Batch 60/64 loss: 0.21340274810791016
Batch 61/64 loss: -0.47983360290527344
Batch 62/64 loss: -0.3907642364501953
Batch 63/64 loss: -0.12757110595703125
Batch 64/64 loss: -3.9657983779907227
Epoch 135  Train loss: -0.23279982548133998  Val loss: -0.4075567370018189
Saving best model, epoch: 135
Epoch 136
-------------------------------
Batch 1/64 loss: 0.0014619827270507812
Batch 2/64 loss: -0.5297441482543945
Batch 3/64 loss: -0.21307134628295898
Batch 4/64 loss: -0.6692571640014648
Batch 5/64 loss: -0.3361330032348633
Batch 6/64 loss: -0.548924446105957
Batch 7/64 loss: -0.3022446632385254
Batch 8/64 loss: -0.10042905807495117
Batch 9/64 loss: 0.6210384368896484
Batch 10/64 loss: -0.5790777206420898
Batch 11/64 loss: 0.08626794815063477
Batch 12/64 loss: -0.26668787002563477
Batch 13/64 loss: -0.21599292755126953
Batch 14/64 loss: -0.5408992767333984
Batch 15/64 loss: -0.23785924911499023
Batch 16/64 loss: -0.5901937484741211
Batch 17/64 loss: -0.18516159057617188
Batch 18/64 loss: 0.12258100509643555
Batch 19/64 loss: -0.11815357208251953
Batch 20/64 loss: -0.2259364128112793
Batch 21/64 loss: -0.05776357650756836
Batch 22/64 loss: -0.42009925842285156
Batch 23/64 loss: -0.3868575096130371
Batch 24/64 loss: -0.276883602142334
Batch 25/64 loss: -0.4379100799560547
Batch 26/64 loss: -0.08085966110229492
Batch 27/64 loss: -0.26017189025878906
Batch 28/64 loss: 0.1907649040222168
Batch 29/64 loss: 0.04441404342651367
Batch 30/64 loss: -0.5003128051757812
Batch 31/64 loss: -0.21559381484985352
Batch 32/64 loss: -0.3567533493041992
Batch 33/64 loss: 0.028166770935058594
Batch 34/64 loss: -0.12998008728027344
Batch 35/64 loss: -0.1608734130859375
Batch 36/64 loss: -0.1585245132446289
Batch 37/64 loss: -0.5438199043273926
Batch 38/64 loss: -0.12925004959106445
Batch 39/64 loss: -0.47872352600097656
Batch 40/64 loss: 0.06699848175048828
Batch 41/64 loss: -0.09457159042358398
Batch 42/64 loss: 0.1634674072265625
Batch 43/64 loss: -0.12243032455444336
Batch 44/64 loss: -0.30048084259033203
Batch 45/64 loss: -0.002707958221435547
Batch 46/64 loss: -0.4498472213745117
Batch 47/64 loss: -0.15575695037841797
Batch 48/64 loss: -0.42110252380371094
Batch 49/64 loss: 0.10664892196655273
Batch 50/64 loss: -0.29810237884521484
Batch 51/64 loss: -0.03302764892578125
Batch 52/64 loss: -0.0814204216003418
Batch 53/64 loss: 0.05327939987182617
Batch 54/64 loss: 0.5184550285339355
Batch 55/64 loss: -0.49190855026245117
Batch 56/64 loss: -0.1597881317138672
Batch 57/64 loss: -0.6242742538452148
Batch 58/64 loss: 0.2039332389831543
Batch 59/64 loss: -0.19907855987548828
Batch 60/64 loss: -0.20152664184570312
Batch 61/64 loss: -0.28056764602661133
Batch 62/64 loss: -0.23259735107421875
Batch 63/64 loss: -0.13861560821533203
Batch 64/64 loss: -3.6181492805480957
Epoch 136  Train loss: -0.23604834874471028  Val loss: -0.34468187417361335
Epoch 137
-------------------------------
Batch 1/64 loss: 0.49327993392944336
Batch 2/64 loss: -0.43128061294555664
Batch 3/64 loss: 0.05165719985961914
Batch 4/64 loss: 0.21489524841308594
Batch 5/64 loss: -0.388948917388916
Batch 6/64 loss: -0.3654050827026367
Batch 7/64 loss: -0.4270763397216797
Batch 8/64 loss: -0.31642818450927734
Batch 9/64 loss: -0.8063559532165527
Batch 10/64 loss: -0.043379783630371094
Batch 11/64 loss: -0.0751490592956543
Batch 12/64 loss: -0.3027019500732422
Batch 13/64 loss: 0.0838627815246582
Batch 14/64 loss: -0.1181330680847168
Batch 15/64 loss: -0.006552696228027344
Batch 16/64 loss: -0.47248411178588867
Batch 17/64 loss: 0.09981060028076172
Batch 18/64 loss: -0.3612704277038574
Batch 19/64 loss: -0.28609514236450195
Batch 20/64 loss: -0.4681706428527832
Batch 21/64 loss: -0.3901810646057129
Batch 22/64 loss: -0.1985163688659668
Batch 23/64 loss: -0.04929971694946289
Batch 24/64 loss: -0.33585166931152344
Batch 25/64 loss: 0.1359391212463379
Batch 26/64 loss: -0.02988719940185547
Batch 27/64 loss: -0.40160608291625977
Batch 28/64 loss: -0.26822519302368164
Batch 29/64 loss: -0.6916103363037109
Batch 30/64 loss: -0.2386488914489746
Batch 31/64 loss: -0.30106449127197266
Batch 32/64 loss: 0.0325164794921875
Batch 33/64 loss: -0.32770633697509766
Batch 34/64 loss: -0.14266490936279297
Batch 35/64 loss: -0.45984840393066406
Batch 36/64 loss: -0.5022578239440918
Batch 37/64 loss: -0.5623245239257812
Batch 38/64 loss: -0.4413785934448242
Batch 39/64 loss: -0.10499954223632812
Batch 40/64 loss: 0.3708062171936035
Batch 41/64 loss: -0.6665544509887695
Batch 42/64 loss: -0.4343280792236328
Batch 43/64 loss: -0.43780040740966797
Batch 44/64 loss: -0.5562887191772461
Batch 45/64 loss: 0.13382577896118164
Batch 46/64 loss: 0.37717199325561523
Batch 47/64 loss: -0.4972519874572754
Batch 48/64 loss: -0.16159534454345703
Batch 49/64 loss: -0.2366633415222168
Batch 50/64 loss: -0.26217079162597656
Batch 51/64 loss: 0.40711307525634766
Batch 52/64 loss: -0.4040689468383789
Batch 53/64 loss: -0.29122066497802734
Batch 54/64 loss: -0.45063066482543945
Batch 55/64 loss: -0.44895362854003906
Batch 56/64 loss: -0.25529003143310547
Batch 57/64 loss: -0.2634611129760742
Batch 58/64 loss: -0.04206705093383789
Batch 59/64 loss: -0.6390290260314941
Batch 60/64 loss: -0.552642822265625
Batch 61/64 loss: -0.16132640838623047
Batch 62/64 loss: -0.3677496910095215
Batch 63/64 loss: -0.7142171859741211
Batch 64/64 loss: -4.344778537750244
Epoch 137  Train loss: -0.29829833647784065  Val loss: -0.3572908512915123
Epoch 138
-------------------------------
Batch 1/64 loss: -0.5602593421936035
Batch 2/64 loss: -0.5077724456787109
Batch 3/64 loss: 0.07294368743896484
Batch 4/64 loss: -0.5923914909362793
Batch 5/64 loss: -0.35498046875
Batch 6/64 loss: -0.49974870681762695
Batch 7/64 loss: -0.2721896171569824
Batch 8/64 loss: -0.2851071357727051
Batch 9/64 loss: -0.2681117057800293
Batch 10/64 loss: -0.06060028076171875
Batch 11/64 loss: -0.5546855926513672
Batch 12/64 loss: -0.5407185554504395
Batch 13/64 loss: -0.04421806335449219
Batch 14/64 loss: -0.5558714866638184
Batch 15/64 loss: -0.42078304290771484
Batch 16/64 loss: -0.09156417846679688
Batch 17/64 loss: -0.1788349151611328
Batch 18/64 loss: -0.18429946899414062
Batch 19/64 loss: -0.7016201019287109
Batch 20/64 loss: -0.08301687240600586
Batch 21/64 loss: -0.7197561264038086
Batch 22/64 loss: 0.33194589614868164
Batch 23/64 loss: -0.3021717071533203
Batch 24/64 loss: 0.14350652694702148
Batch 25/64 loss: -0.7297282218933105
Batch 26/64 loss: -0.3006563186645508
Batch 27/64 loss: -0.08595609664916992
Batch 28/64 loss: -0.47858428955078125
Batch 29/64 loss: 0.19602108001708984
Batch 30/64 loss: -0.292849063873291
Batch 31/64 loss: -0.30360937118530273
Batch 32/64 loss: -0.21318626403808594
Batch 33/64 loss: 0.08980226516723633
Batch 34/64 loss: 0.1722702980041504
Batch 35/64 loss: -0.2608489990234375
Batch 36/64 loss: -0.14018487930297852
Batch 37/64 loss: -0.5774269104003906
Batch 38/64 loss: 0.22780179977416992
Batch 39/64 loss: -0.4676523208618164
Batch 40/64 loss: -0.3018488883972168
Batch 41/64 loss: 0.4112825393676758
Batch 42/64 loss: 0.04388856887817383
Batch 43/64 loss: -0.1998915672302246
Batch 44/64 loss: -0.5582184791564941
Batch 45/64 loss: -0.23729610443115234
Batch 46/64 loss: -0.027605533599853516
Batch 47/64 loss: -0.18641948699951172
Batch 48/64 loss: 0.28758716583251953
Batch 49/64 loss: 0.05731344223022461
Batch 50/64 loss: -0.31804656982421875
Batch 51/64 loss: -0.38483762741088867
Batch 52/64 loss: 0.47303152084350586
Batch 53/64 loss: -0.22746896743774414
Batch 54/64 loss: -0.4060854911804199
Batch 55/64 loss: -0.1520524024963379
Batch 56/64 loss: 0.09484148025512695
Batch 57/64 loss: -0.4552607536315918
Batch 58/64 loss: 0.2669663429260254
Batch 59/64 loss: 0.013687610626220703
Batch 60/64 loss: -0.06433677673339844
Batch 61/64 loss: -0.0026683807373046875
Batch 62/64 loss: -0.33074331283569336
Batch 63/64 loss: -0.3611130714416504
Batch 64/64 loss: -4.111691474914551
Epoch 138  Train loss: -0.2516730327232211  Val loss: -0.33972361980844606
Epoch 139
-------------------------------
Batch 1/64 loss: -0.14468622207641602
Batch 2/64 loss: 0.08766365051269531
Batch 3/64 loss: -0.3385481834411621
Batch 4/64 loss: -0.0843653678894043
Batch 5/64 loss: -0.37754297256469727
Batch 6/64 loss: -0.4010334014892578
Batch 7/64 loss: -0.029448986053466797
Batch 8/64 loss: -0.10213804244995117
Batch 9/64 loss: -0.36573171615600586
Batch 10/64 loss: -0.2132430076599121
Batch 11/64 loss: -0.4361743927001953
Batch 12/64 loss: 0.29849863052368164
Batch 13/64 loss: -0.11581039428710938
Batch 14/64 loss: -0.33879852294921875
Batch 15/64 loss: -0.2521967887878418
Batch 16/64 loss: 0.6641526222229004
Batch 17/64 loss: -0.4320816993713379
Batch 18/64 loss: -0.6439809799194336
Batch 19/64 loss: -0.28492164611816406
Batch 20/64 loss: -0.2536025047302246
Batch 21/64 loss: 0.1216740608215332
Batch 22/64 loss: 0.4038248062133789
Batch 23/64 loss: -0.12305593490600586
Batch 24/64 loss: -0.4908428192138672
Batch 25/64 loss: -0.3141789436340332
Batch 26/64 loss: -0.5802159309387207
Batch 27/64 loss: -0.12907171249389648
Batch 28/64 loss: -0.31174707412719727
Batch 29/64 loss: -0.3379483222961426
Batch 30/64 loss: -0.21117067337036133
Batch 31/64 loss: -0.1064615249633789
Batch 32/64 loss: 0.12184906005859375
Batch 33/64 loss: -0.13261127471923828
Batch 34/64 loss: -0.15758562088012695
Batch 35/64 loss: -0.5785660743713379
Batch 36/64 loss: -0.7012333869934082
Batch 37/64 loss: -0.2628016471862793
Batch 38/64 loss: -0.24561738967895508
Batch 39/64 loss: -0.020443439483642578
Batch 40/64 loss: -0.16499567031860352
Batch 41/64 loss: -0.45168113708496094
Batch 42/64 loss: -0.09931612014770508
Batch 43/64 loss: -0.18629789352416992
Batch 44/64 loss: -0.37947940826416016
Batch 45/64 loss: 0.49110841751098633
Batch 46/64 loss: -0.4355173110961914
Batch 47/64 loss: -0.12229394912719727
Batch 48/64 loss: -0.4218025207519531
Batch 49/64 loss: -0.17586755752563477
Batch 50/64 loss: -0.5741047859191895
Batch 51/64 loss: -0.5317540168762207
Batch 52/64 loss: -0.5877294540405273
Batch 53/64 loss: -0.479764461517334
Batch 54/64 loss: -0.32956600189208984
Batch 55/64 loss: -0.19066905975341797
Batch 56/64 loss: -0.6832561492919922
Batch 57/64 loss: -0.33568286895751953
Batch 58/64 loss: -0.6661481857299805
Batch 59/64 loss: -0.21687793731689453
Batch 60/64 loss: 0.44936227798461914
Batch 61/64 loss: -0.23338556289672852
Batch 62/64 loss: -0.04955482482910156
Batch 63/64 loss: -0.4759387969970703
Batch 64/64 loss: -3.504016399383545
Epoch 139  Train loss: -0.2713634360070322  Val loss: -0.17830735301643713
Epoch 140
-------------------------------
Batch 1/64 loss: -0.17571067810058594
Batch 2/64 loss: -0.12664031982421875
Batch 3/64 loss: -0.4543037414550781
Batch 4/64 loss: -0.1359724998474121
Batch 5/64 loss: -0.2055206298828125
Batch 6/64 loss: -0.10594940185546875
Batch 7/64 loss: -0.03780508041381836
Batch 8/64 loss: -0.3808021545410156
Batch 9/64 loss: -0.2401723861694336
Batch 10/64 loss: -0.2852315902709961
Batch 11/64 loss: 0.4587726593017578
Batch 12/64 loss: -0.2270808219909668
Batch 13/64 loss: 0.313539981842041
Batch 14/64 loss: -0.4258890151977539
Batch 15/64 loss: -0.13682126998901367
Batch 16/64 loss: -0.08542013168334961
Batch 17/64 loss: -0.25983095169067383
Batch 18/64 loss: -0.30751895904541016
Batch 19/64 loss: 0.24177312850952148
Batch 20/64 loss: -0.45495033264160156
Batch 21/64 loss: -0.10125541687011719
Batch 22/64 loss: -0.1763930320739746
Batch 23/64 loss: 0.7639508247375488
Batch 24/64 loss: 0.018450260162353516
Batch 25/64 loss: -0.10666799545288086
Batch 26/64 loss: -0.14830350875854492
Batch 27/64 loss: -0.2573976516723633
Batch 28/64 loss: -0.03737592697143555
Batch 29/64 loss: -0.37845516204833984
Batch 30/64 loss: 0.0001239776611328125
Batch 31/64 loss: 0.4279308319091797
Batch 32/64 loss: -0.745976448059082
Batch 33/64 loss: -0.17705488204956055
Batch 34/64 loss: -0.11372041702270508
Batch 35/64 loss: 0.09890365600585938
Batch 36/64 loss: -0.3028864860534668
Batch 37/64 loss: -0.10531425476074219
Batch 38/64 loss: 0.011011123657226562
Batch 39/64 loss: -0.17492437362670898
Batch 40/64 loss: -0.22768449783325195
Batch 41/64 loss: -0.5435395240783691
Batch 42/64 loss: -0.17798709869384766
Batch 43/64 loss: -0.0939640998840332
Batch 44/64 loss: -0.33186817169189453
Batch 45/64 loss: -0.03800821304321289
Batch 46/64 loss: 0.2919125556945801
Batch 47/64 loss: -0.6098175048828125
Batch 48/64 loss: 0.13123703002929688
Batch 49/64 loss: 0.23835992813110352
Batch 50/64 loss: -0.4690208435058594
Batch 51/64 loss: 0.03931713104248047
Batch 52/64 loss: -0.10648393630981445
Batch 53/64 loss: -0.2906813621520996
Batch 54/64 loss: -0.17836332321166992
Batch 55/64 loss: -0.565155029296875
Batch 56/64 loss: -0.313079833984375
Batch 57/64 loss: -0.29233837127685547
Batch 58/64 loss: -0.2285146713256836
Batch 59/64 loss: -0.46086835861206055
Batch 60/64 loss: -0.3986549377441406
Batch 61/64 loss: 0.14008188247680664
Batch 62/64 loss: -0.28650808334350586
Batch 63/64 loss: 0.02353382110595703
Batch 64/64 loss: -3.6260337829589844
Epoch 140  Train loss: -0.18830603805242802  Val loss: -0.42708194378725034
Saving best model, epoch: 140
Epoch 141
-------------------------------
Batch 1/64 loss: -0.3245425224304199
Batch 2/64 loss: -0.09404230117797852
Batch 3/64 loss: -0.43932199478149414
Batch 4/64 loss: -0.18944692611694336
Batch 5/64 loss: 0.08487319946289062
Batch 6/64 loss: 0.3972043991088867
Batch 7/64 loss: 0.08166313171386719
Batch 8/64 loss: -0.43320608139038086
Batch 9/64 loss: -0.34548377990722656
Batch 10/64 loss: -0.39756345748901367
Batch 11/64 loss: -0.4142007827758789
Batch 12/64 loss: 0.07466793060302734
Batch 13/64 loss: 0.018299579620361328
Batch 14/64 loss: -0.4982032775878906
Batch 15/64 loss: -0.3010983467102051
Batch 16/64 loss: -0.5545616149902344
Batch 17/64 loss: -0.5240116119384766
Batch 18/64 loss: -0.2799229621887207
Batch 19/64 loss: -0.40684938430786133
Batch 20/64 loss: -0.06391191482543945
Batch 21/64 loss: -0.32180118560791016
Batch 22/64 loss: -0.46717262268066406
Batch 23/64 loss: -0.29486703872680664
Batch 24/64 loss: -0.0021276473999023438
Batch 25/64 loss: -0.5793132781982422
Batch 26/64 loss: -0.5686931610107422
Batch 27/64 loss: -0.12102460861206055
Batch 28/64 loss: -0.04501676559448242
Batch 29/64 loss: -0.15314006805419922
Batch 30/64 loss: -0.6577835083007812
Batch 31/64 loss: -0.050171852111816406
Batch 32/64 loss: -0.1358642578125
Batch 33/64 loss: -0.09922599792480469
Batch 34/64 loss: -0.18153619766235352
Batch 35/64 loss: -0.40637922286987305
Batch 36/64 loss: -0.04295635223388672
Batch 37/64 loss: -0.29549121856689453
Batch 38/64 loss: -0.32076120376586914
Batch 39/64 loss: 0.27706003189086914
Batch 40/64 loss: 0.13202953338623047
Batch 41/64 loss: -0.3799128532409668
Batch 42/64 loss: -0.26949644088745117
Batch 43/64 loss: -0.6354899406433105
Batch 44/64 loss: -0.2828950881958008
Batch 45/64 loss: -0.5020456314086914
Batch 46/64 loss: -0.35719776153564453
Batch 47/64 loss: -0.20815134048461914
Batch 48/64 loss: -0.7790570259094238
Batch 49/64 loss: -0.04978609085083008
Batch 50/64 loss: 0.019614696502685547
Batch 51/64 loss: -0.09387540817260742
Batch 52/64 loss: -0.2597999572753906
Batch 53/64 loss: 0.45552968978881836
Batch 54/64 loss: -0.07441329956054688
Batch 55/64 loss: -0.5915603637695312
Batch 56/64 loss: -0.2647128105163574
Batch 57/64 loss: -0.14584922790527344
Batch 58/64 loss: -0.31928300857543945
Batch 59/64 loss: -0.42878055572509766
Batch 60/64 loss: -0.17435979843139648
Batch 61/64 loss: -0.6290726661682129
Batch 62/64 loss: -0.07974576950073242
Batch 63/64 loss: -0.2644529342651367
Batch 64/64 loss: -3.935366153717041
Epoch 141  Train loss: -0.2856504085017186  Val loss: -0.3867377841595522
Epoch 142
-------------------------------
Batch 1/64 loss: -0.06791877746582031
Batch 2/64 loss: -0.6121735572814941
Batch 3/64 loss: -0.5376687049865723
Batch 4/64 loss: 0.3054518699645996
Batch 5/64 loss: -0.5379509925842285
Batch 6/64 loss: 0.23429536819458008
Batch 7/64 loss: -0.6223235130310059
Batch 8/64 loss: -0.46386289596557617
Batch 9/64 loss: -0.32057619094848633
Batch 10/64 loss: 0.27518796920776367
Batch 11/64 loss: -0.2516965866088867
Batch 12/64 loss: 0.04619264602661133
Batch 13/64 loss: -0.3804478645324707
Batch 14/64 loss: -0.5071115493774414
Batch 15/64 loss: -0.4068741798400879
Batch 16/64 loss: -0.20351743698120117
Batch 17/64 loss: -0.17136049270629883
Batch 18/64 loss: -0.37162160873413086
Batch 19/64 loss: -0.3388667106628418
Batch 20/64 loss: -0.07452964782714844
Batch 21/64 loss: -0.03762245178222656
Batch 22/64 loss: 0.0009565353393554688
Batch 23/64 loss: -0.16793298721313477
Batch 24/64 loss: -0.3150782585144043
Batch 25/64 loss: -0.3005948066711426
Batch 26/64 loss: -0.13305377960205078
Batch 27/64 loss: -0.18924617767333984
Batch 28/64 loss: -0.12021017074584961
Batch 29/64 loss: -0.32802867889404297
Batch 30/64 loss: -0.14739513397216797
Batch 31/64 loss: -0.3404254913330078
Batch 32/64 loss: 0.15692377090454102
Batch 33/64 loss: -0.5523333549499512
Batch 34/64 loss: -0.34835147857666016
Batch 35/64 loss: 0.2500019073486328
Batch 36/64 loss: -0.5334420204162598
Batch 37/64 loss: -0.5041427612304688
Batch 38/64 loss: -0.5717716217041016
Batch 39/64 loss: -0.1766681671142578
Batch 40/64 loss: -0.4458322525024414
Batch 41/64 loss: -0.35552978515625
Batch 42/64 loss: -0.29114818572998047
Batch 43/64 loss: -0.029349327087402344
Batch 44/64 loss: 0.09581947326660156
Batch 45/64 loss: 0.4912137985229492
Batch 46/64 loss: 0.1448211669921875
Batch 47/64 loss: -0.5461854934692383
Batch 48/64 loss: -0.1647944450378418
Batch 49/64 loss: -0.4393172264099121
Batch 50/64 loss: 0.19805097579956055
Batch 51/64 loss: -0.3607316017150879
Batch 52/64 loss: -0.46857213973999023
Batch 53/64 loss: -0.4817023277282715
Batch 54/64 loss: -0.13313007354736328
Batch 55/64 loss: -0.03576183319091797
Batch 56/64 loss: 0.15494489669799805
Batch 57/64 loss: 0.009611129760742188
Batch 58/64 loss: 0.04599952697753906
Batch 59/64 loss: -0.31038761138916016
Batch 60/64 loss: -0.09487342834472656
Batch 61/64 loss: 0.016340255737304688
Batch 62/64 loss: -0.38852977752685547
Batch 63/64 loss: -0.033808231353759766
Batch 64/64 loss: -4.288782596588135
Epoch 142  Train loss: -0.2510623913185269  Val loss: -0.21039649465239743
Epoch 143
-------------------------------
Batch 1/64 loss: -0.42846107482910156
Batch 2/64 loss: -0.2581629753112793
Batch 3/64 loss: 0.1539773941040039
Batch 4/64 loss: -0.35275840759277344
Batch 5/64 loss: -0.21346569061279297
Batch 6/64 loss: -0.49254894256591797
Batch 7/64 loss: 1.0109729766845703
Batch 8/64 loss: 0.4543313980102539
Batch 9/64 loss: -0.050189971923828125
Batch 10/64 loss: -0.10591316223144531
Batch 11/64 loss: 0.2371225357055664
Batch 12/64 loss: 0.4735407829284668
Batch 13/64 loss: 0.30501604080200195
Batch 14/64 loss: -0.13990449905395508
Batch 15/64 loss: 0.028937339782714844
Batch 16/64 loss: 0.3046903610229492
Batch 17/64 loss: -0.1942148208618164
Batch 18/64 loss: 0.5741486549377441
Batch 19/64 loss: -0.1960158348083496
Batch 20/64 loss: 0.4620985984802246
Batch 21/64 loss: 0.04719734191894531
Batch 22/64 loss: 0.26720237731933594
Batch 23/64 loss: 0.318880558013916
Batch 24/64 loss: -0.47760629653930664
Batch 25/64 loss: 0.0314483642578125
Batch 26/64 loss: 0.0712275505065918
Batch 27/64 loss: 0.44616222381591797
Batch 28/64 loss: 0.028462886810302734
Batch 29/64 loss: 0.27577638626098633
Batch 30/64 loss: -0.5170621871948242
Batch 31/64 loss: 0.1977977752685547
Batch 32/64 loss: -0.19972658157348633
Batch 33/64 loss: -0.17379331588745117
Batch 34/64 loss: -0.05933332443237305
Batch 35/64 loss: 0.3972916603088379
Batch 36/64 loss: -0.41190052032470703
Batch 37/64 loss: -0.4129648208618164
Batch 38/64 loss: -0.2566409111022949
Batch 39/64 loss: -0.13938522338867188
Batch 40/64 loss: -0.3719015121459961
Batch 41/64 loss: -0.2121419906616211
Batch 42/64 loss: -0.2066783905029297
Batch 43/64 loss: -0.16999292373657227
Batch 44/64 loss: -0.25083017349243164
Batch 45/64 loss: 0.16874980926513672
Batch 46/64 loss: -0.14271163940429688
Batch 47/64 loss: -0.24080801010131836
Batch 48/64 loss: 0.11282014846801758
Batch 49/64 loss: 0.005741119384765625
Batch 50/64 loss: 0.6757197380065918
Batch 51/64 loss: -0.09891939163208008
Batch 52/64 loss: 0.31801843643188477
Batch 53/64 loss: 0.015070915222167969
Batch 54/64 loss: 0.2310018539428711
Batch 55/64 loss: -0.14914226531982422
Batch 56/64 loss: -0.2456836700439453
Batch 57/64 loss: -0.20899009704589844
Batch 58/64 loss: -0.35596513748168945
Batch 59/64 loss: -0.036450862884521484
Batch 60/64 loss: 0.04524993896484375
Batch 61/64 loss: 0.29828834533691406
Batch 62/64 loss: -0.3825230598449707
Batch 63/64 loss: -0.13198328018188477
Batch 64/64 loss: -3.9565229415893555
Epoch 143  Train loss: -0.05168972015380859  Val loss: -0.23097487249734885
Epoch 144
-------------------------------
Batch 1/64 loss: -0.45979738235473633
Batch 2/64 loss: -0.1616377830505371
Batch 3/64 loss: 0.11390018463134766
Batch 4/64 loss: 0.02152252197265625
Batch 5/64 loss: -0.03258180618286133
Batch 6/64 loss: 0.4513559341430664
Batch 7/64 loss: -0.08412837982177734
Batch 8/64 loss: -0.5130400657653809
Batch 9/64 loss: -0.0717463493347168
Batch 10/64 loss: -0.42786312103271484
Batch 11/64 loss: -0.5205869674682617
Batch 12/64 loss: -0.27899885177612305
Batch 13/64 loss: -0.20180702209472656
Batch 14/64 loss: 0.09277820587158203
Batch 15/64 loss: 0.41640377044677734
Batch 16/64 loss: -0.20979690551757812
Batch 17/64 loss: -0.05410003662109375
Batch 18/64 loss: -0.6938252449035645
Batch 19/64 loss: -0.30449819564819336
Batch 20/64 loss: -0.06658124923706055
Batch 21/64 loss: 0.014604568481445312
Batch 22/64 loss: -0.41417932510375977
Batch 23/64 loss: 0.4163064956665039
Batch 24/64 loss: 0.26283693313598633
Batch 25/64 loss: 0.12584781646728516
Batch 26/64 loss: 0.06663942337036133
Batch 27/64 loss: -0.2932572364807129
Batch 28/64 loss: -0.15581607818603516
Batch 29/64 loss: -0.05005836486816406
Batch 30/64 loss: -0.17250633239746094
Batch 31/64 loss: -0.02250051498413086
Batch 32/64 loss: 0.3261752128601074
Batch 33/64 loss: 0.1336078643798828
Batch 34/64 loss: -0.11457252502441406
Batch 35/64 loss: 0.2518806457519531
Batch 36/64 loss: -0.5011239051818848
Batch 37/64 loss: 0.04393339157104492
Batch 38/64 loss: 0.01236104965209961
Batch 39/64 loss: -0.35121917724609375
Batch 40/64 loss: 0.22917890548706055
Batch 41/64 loss: 0.44287109375
Batch 42/64 loss: -0.1565542221069336
Batch 43/64 loss: 0.2014474868774414
Batch 44/64 loss: 0.6922698020935059
Batch 45/64 loss: -0.05504322052001953
Batch 46/64 loss: -0.1954798698425293
Batch 47/64 loss: -0.2543158531188965
Batch 48/64 loss: 0.03368091583251953
Batch 49/64 loss: -0.23286104202270508
Batch 50/64 loss: -0.15995502471923828
Batch 51/64 loss: -0.5238504409790039
Batch 52/64 loss: -0.3463006019592285
Batch 53/64 loss: 0.22173309326171875
Batch 54/64 loss: -0.630465030670166
Batch 55/64 loss: -0.22142696380615234
Batch 56/64 loss: -0.10081005096435547
Batch 57/64 loss: 0.10177278518676758
Batch 58/64 loss: 0.1571674346923828
Batch 59/64 loss: -0.20292139053344727
Batch 60/64 loss: -0.3671846389770508
Batch 61/64 loss: -0.43126964569091797
Batch 62/64 loss: -0.16454601287841797
Batch 63/64 loss: -0.1679062843322754
Batch 64/64 loss: -3.9335947036743164
Epoch 144  Train loss: -0.1331299389109892  Val loss: -0.3776533775722858
Epoch 145
-------------------------------
Batch 1/64 loss: -0.27336835861206055
Batch 2/64 loss: 0.2323627471923828
Batch 3/64 loss: -0.47739458084106445
Batch 4/64 loss: -0.3110041618347168
Batch 5/64 loss: -0.5306696891784668
Batch 6/64 loss: -0.1632676124572754
Batch 7/64 loss: -0.3524055480957031
Batch 8/64 loss: -0.6108918190002441
Batch 9/64 loss: -0.38340234756469727
Batch 10/64 loss: -0.2988247871398926
Batch 11/64 loss: 0.05110359191894531
Batch 12/64 loss: -0.3880615234375
Batch 13/64 loss: 0.03778266906738281
Batch 14/64 loss: -0.2846856117248535
Batch 15/64 loss: -0.3994565010070801
Batch 16/64 loss: -0.17892026901245117
Batch 17/64 loss: -0.1144399642944336
Batch 18/64 loss: -0.05115365982055664
Batch 19/64 loss: -0.18536663055419922
Batch 20/64 loss: -0.06863117218017578
Batch 21/64 loss: -0.11147499084472656
Batch 22/64 loss: -0.3954310417175293
Batch 23/64 loss: 0.19134140014648438
Batch 24/64 loss: -0.5307364463806152
Batch 25/64 loss: -0.15720891952514648
Batch 26/64 loss: -0.2090892791748047
Batch 27/64 loss: -0.14959192276000977
Batch 28/64 loss: -0.28073883056640625
Batch 29/64 loss: -0.09721660614013672
Batch 30/64 loss: 0.12276649475097656
Batch 31/64 loss: -0.023082256317138672
Batch 32/64 loss: -0.18081331253051758
Batch 33/64 loss: 0.22688913345336914
Batch 34/64 loss: -0.29274559020996094
Batch 35/64 loss: 0.10940790176391602
Batch 36/64 loss: 0.25301218032836914
Batch 37/64 loss: 0.05714702606201172
Batch 38/64 loss: -0.38661766052246094
Batch 39/64 loss: -0.45503902435302734
Batch 40/64 loss: -0.39618635177612305
Batch 41/64 loss: -0.5274734497070312
Batch 42/64 loss: -0.381680965423584
Batch 43/64 loss: -0.46632862091064453
Batch 44/64 loss: 0.06058788299560547
Batch 45/64 loss: -0.11812782287597656
Batch 46/64 loss: -0.03691577911376953
Batch 47/64 loss: -0.49941110610961914
Batch 48/64 loss: -0.31366539001464844
Batch 49/64 loss: -0.4591712951660156
Batch 50/64 loss: -0.01784038543701172
Batch 51/64 loss: 0.18105554580688477
Batch 52/64 loss: -0.0466156005859375
Batch 53/64 loss: 0.1257176399230957
Batch 54/64 loss: -0.3259916305541992
Batch 55/64 loss: -0.5496072769165039
Batch 56/64 loss: -0.5473904609680176
Batch 57/64 loss: -0.3212738037109375
Batch 58/64 loss: -0.08452892303466797
Batch 59/64 loss: -0.27089548110961914
Batch 60/64 loss: -0.2514920234680176
Batch 61/64 loss: -0.2400951385498047
Batch 62/64 loss: 0.06828880310058594
Batch 63/64 loss: -0.3188776969909668
Batch 64/64 loss: -4.127294540405273
Epoch 145  Train loss: -0.24930677974925322  Val loss: -0.337316558942762
Epoch 146
-------------------------------
Batch 1/64 loss: -0.12276744842529297
Batch 2/64 loss: -0.3932194709777832
Batch 3/64 loss: -0.31389856338500977
Batch 4/64 loss: 0.2687339782714844
Batch 5/64 loss: -0.6831207275390625
Batch 6/64 loss: -0.41866254806518555
Batch 7/64 loss: -0.2718319892883301
Batch 8/64 loss: -0.3847236633300781
Batch 9/64 loss: 0.2175760269165039
Batch 10/64 loss: -0.35265111923217773
Batch 11/64 loss: 0.07520341873168945
Batch 12/64 loss: -0.7481307983398438
Batch 13/64 loss: -0.26751708984375
Batch 14/64 loss: 0.021146297454833984
Batch 15/64 loss: 0.2178044319152832
Batch 16/64 loss: -0.26532793045043945
Batch 17/64 loss: 0.1239633560180664
Batch 18/64 loss: -0.17102527618408203
Batch 19/64 loss: -0.27652931213378906
Batch 20/64 loss: -0.3044090270996094
Batch 21/64 loss: -0.557685375213623
Batch 22/64 loss: -0.4187297821044922
Batch 23/64 loss: -0.3893275260925293
Batch 24/64 loss: -0.3460545539855957
Batch 25/64 loss: -0.4422588348388672
Batch 26/64 loss: -0.23562908172607422
Batch 27/64 loss: -0.22805213928222656
Batch 28/64 loss: 0.2997264862060547
Batch 29/64 loss: -0.3558011054992676
Batch 30/64 loss: 0.7652044296264648
Batch 31/64 loss: -0.36545801162719727
Batch 32/64 loss: -0.6139988899230957
Batch 33/64 loss: -0.32590389251708984
Batch 34/64 loss: -0.2774772644042969
Batch 35/64 loss: -0.23920917510986328
Batch 36/64 loss: 0.37224626541137695
Batch 37/64 loss: 0.12069845199584961
Batch 38/64 loss: -0.5330448150634766
Batch 39/64 loss: -0.1110529899597168
Batch 40/64 loss: -0.3951845169067383
Batch 41/64 loss: 0.11503934860229492
Batch 42/64 loss: -0.1157979965209961
Batch 43/64 loss: -0.1684861183166504
Batch 44/64 loss: -0.19569015502929688
Batch 45/64 loss: -0.4868597984313965
Batch 46/64 loss: -0.285214900970459
Batch 47/64 loss: 0.12687253952026367
Batch 48/64 loss: -0.15271234512329102
Batch 49/64 loss: 0.1046600341796875
Batch 50/64 loss: -0.2622847557067871
Batch 51/64 loss: -0.3088712692260742
Batch 52/64 loss: 0.00321197509765625
Batch 53/64 loss: -0.25679922103881836
Batch 54/64 loss: -0.06535625457763672
Batch 55/64 loss: -0.29532623291015625
Batch 56/64 loss: -0.6711544990539551
Batch 57/64 loss: -0.574028491973877
Batch 58/64 loss: -0.3751673698425293
Batch 59/64 loss: -0.6633028984069824
Batch 60/64 loss: -0.28371477127075195
Batch 61/64 loss: 0.04915618896484375
Batch 62/64 loss: -0.4190826416015625
Batch 63/64 loss: 0.16601037979125977
Batch 64/64 loss: -4.094882965087891
Epoch 146  Train loss: -0.2574500588809743  Val loss: -0.3873695425970857
Epoch 147
-------------------------------
Batch 1/64 loss: -0.5333986282348633
Batch 2/64 loss: -0.25919151306152344
Batch 3/64 loss: -0.30591297149658203
Batch 4/64 loss: 0.41881322860717773
Batch 5/64 loss: 0.1409296989440918
Batch 6/64 loss: -0.6157131195068359
Batch 7/64 loss: -0.15880155563354492
Batch 8/64 loss: -0.46663951873779297
Batch 9/64 loss: 0.011225223541259766
Batch 10/64 loss: 0.21083831787109375
Batch 11/64 loss: 0.08550214767456055
Batch 12/64 loss: -0.012194633483886719
Batch 13/64 loss: -0.2335071563720703
Batch 14/64 loss: -0.13973331451416016
Batch 15/64 loss: -0.32829999923706055
Batch 16/64 loss: -0.34282445907592773
Batch 17/64 loss: -0.12746238708496094
Batch 18/64 loss: 0.4242215156555176
Batch 19/64 loss: -0.1126251220703125
Batch 20/64 loss: -0.058188438415527344
Batch 21/64 loss: -0.23620271682739258
Batch 22/64 loss: -0.3903846740722656
Batch 23/64 loss: 0.21726512908935547
Batch 24/64 loss: 0.17832422256469727
Batch 25/64 loss: 0.10408306121826172
Batch 26/64 loss: -0.19141292572021484
Batch 27/64 loss: 0.5109167098999023
Batch 28/64 loss: -0.16809320449829102
Batch 29/64 loss: -0.37860822677612305
Batch 30/64 loss: -0.17544937133789062
Batch 31/64 loss: -0.36060619354248047
Batch 32/64 loss: 0.4555239677429199
Batch 33/64 loss: -0.0196990966796875
Batch 34/64 loss: -0.32455968856811523
Batch 35/64 loss: -0.2771415710449219
Batch 36/64 loss: -0.06743907928466797
Batch 37/64 loss: -0.11187219619750977
Batch 38/64 loss: -0.439115047454834
Batch 39/64 loss: 0.16254043579101562
Batch 40/64 loss: -0.19315481185913086
Batch 41/64 loss: -0.38190603256225586
Batch 42/64 loss: 0.24030017852783203
Batch 43/64 loss: -0.2252349853515625
Batch 44/64 loss: 0.2610945701599121
Batch 45/64 loss: -0.34073543548583984
Batch 46/64 loss: 0.07386922836303711
Batch 47/64 loss: -0.07451009750366211
Batch 48/64 loss: -0.4972257614135742
Batch 49/64 loss: 0.3481178283691406
Batch 50/64 loss: -0.4267845153808594
Batch 51/64 loss: -0.1907052993774414
Batch 52/64 loss: -0.3231387138366699
Batch 53/64 loss: -0.7283282279968262
Batch 54/64 loss: 0.2386002540588379
Batch 55/64 loss: -0.432340145111084
Batch 56/64 loss: -0.13814735412597656
Batch 57/64 loss: 0.064697265625
Batch 58/64 loss: -0.47602081298828125
Batch 59/64 loss: -0.3254876136779785
Batch 60/64 loss: -0.6129498481750488
Batch 61/64 loss: 0.1081857681274414
Batch 62/64 loss: -0.23447418212890625
Batch 63/64 loss: -0.12683868408203125
Batch 64/64 loss: -4.564357757568359
Epoch 147  Train loss: -0.18402006111893  Val loss: -0.35097456469978255
Epoch 148
-------------------------------
Batch 1/64 loss: -0.40871143341064453
Batch 2/64 loss: -0.37971067428588867
Batch 3/64 loss: -0.38705015182495117
Batch 4/64 loss: -0.40358877182006836
Batch 5/64 loss: -0.4283614158630371
Batch 6/64 loss: -0.23691415786743164
Batch 7/64 loss: -0.2060089111328125
Batch 8/64 loss: -0.35131072998046875
Batch 9/64 loss: -0.49526405334472656
Batch 10/64 loss: -0.4981231689453125
Batch 11/64 loss: 0.10857677459716797
Batch 12/64 loss: 0.15024328231811523
Batch 13/64 loss: 0.21044588088989258
Batch 14/64 loss: -0.046579837799072266
Batch 15/64 loss: 0.0751047134399414
Batch 16/64 loss: -0.4122200012207031
Batch 17/64 loss: -0.07619380950927734
Batch 18/64 loss: -0.30129432678222656
Batch 19/64 loss: -0.28915929794311523
Batch 20/64 loss: -0.2696995735168457
Batch 21/64 loss: -0.13231372833251953
Batch 22/64 loss: 0.35184383392333984
Batch 23/64 loss: -0.055891990661621094
Batch 24/64 loss: -0.36317968368530273
Batch 25/64 loss: 0.0798788070678711
Batch 26/64 loss: -0.2786135673522949
Batch 27/64 loss: 0.022249698638916016
Batch 28/64 loss: -0.29149484634399414
Batch 29/64 loss: -0.0012969970703125
Batch 30/64 loss: -0.28590822219848633
Batch 31/64 loss: 0.24801301956176758
Batch 32/64 loss: 0.007785320281982422
Batch 33/64 loss: -0.2667970657348633
Batch 34/64 loss: -0.2768087387084961
Batch 35/64 loss: -0.32765913009643555
Batch 36/64 loss: -0.1598963737487793
Batch 37/64 loss: 0.10759973526000977
Batch 38/64 loss: -0.45389795303344727
Batch 39/64 loss: -0.43511152267456055
Batch 40/64 loss: 0.41809940338134766
Batch 41/64 loss: -0.07331037521362305
Batch 42/64 loss: 0.2988886833190918
Batch 43/64 loss: -0.48406982421875
Batch 44/64 loss: 0.13974285125732422
Batch 45/64 loss: -0.33411121368408203
Batch 46/64 loss: -0.2889833450317383
Batch 47/64 loss: -0.31987810134887695
Batch 48/64 loss: -0.5473775863647461
Batch 49/64 loss: -0.3079519271850586
Batch 50/64 loss: -0.66021728515625
Batch 51/64 loss: 0.15701532363891602
Batch 52/64 loss: -0.2901191711425781
Batch 53/64 loss: -0.14738845825195312
Batch 54/64 loss: 0.3111734390258789
Batch 55/64 loss: -0.2911338806152344
Batch 56/64 loss: 0.08444356918334961
Batch 57/64 loss: -0.5451807975769043
Batch 58/64 loss: -0.4001455307006836
Batch 59/64 loss: -0.17015647888183594
Batch 60/64 loss: -0.4580345153808594
Batch 61/64 loss: -0.5190591812133789
Batch 62/64 loss: -0.215362548828125
Batch 63/64 loss: -0.7556009292602539
Batch 64/64 loss: -3.668148994445801
Epoch 148  Train loss: -0.2401121363920324  Val loss: -0.40657267619654075
Epoch 149
-------------------------------
Batch 1/64 loss: -0.1872258186340332
Batch 2/64 loss: -0.37929487228393555
Batch 3/64 loss: 0.10722541809082031
Batch 4/64 loss: 0.3147716522216797
Batch 5/64 loss: -0.23849821090698242
Batch 6/64 loss: 0.044589996337890625
Batch 7/64 loss: -0.2373504638671875
Batch 8/64 loss: 0.007984161376953125
Batch 9/64 loss: -0.39513349533081055
Batch 10/64 loss: -0.07316255569458008
Batch 11/64 loss: -0.29300546646118164
Batch 12/64 loss: -0.13234901428222656
Batch 13/64 loss: -0.17193126678466797
Batch 14/64 loss: -0.06308889389038086
Batch 15/64 loss: -0.41355466842651367
Batch 16/64 loss: -0.37578678131103516
Batch 17/64 loss: -0.17508411407470703
Batch 18/64 loss: -0.4620790481567383
Batch 19/64 loss: -0.10676813125610352
Batch 20/64 loss: -0.4642214775085449
Batch 21/64 loss: 0.025794029235839844
Batch 22/64 loss: -0.36176633834838867
Batch 23/64 loss: -0.38684511184692383
Batch 24/64 loss: -0.5835037231445312
Batch 25/64 loss: -0.06093406677246094
Batch 26/64 loss: -0.1713700294494629
Batch 27/64 loss: -0.17552804946899414
Batch 28/64 loss: -0.3390798568725586
Batch 29/64 loss: 0.02023458480834961
Batch 30/64 loss: -0.22284269332885742
Batch 31/64 loss: 0.0862574577331543
Batch 32/64 loss: -0.3460350036621094
Batch 33/64 loss: -0.3822169303894043
Batch 34/64 loss: -0.35206031799316406
Batch 35/64 loss: -0.4821052551269531
Batch 36/64 loss: -0.2173480987548828
Batch 37/64 loss: -0.09522867202758789
Batch 38/64 loss: 0.035637855529785156
Batch 39/64 loss: 0.013824939727783203
Batch 40/64 loss: -0.14128828048706055
Batch 41/64 loss: -0.44862794876098633
Batch 42/64 loss: -0.07987260818481445
Batch 43/64 loss: -0.1659560203552246
Batch 44/64 loss: -0.4428424835205078
Batch 45/64 loss: -0.16190433502197266
Batch 46/64 loss: -0.41445350646972656
Batch 47/64 loss: -0.23139619827270508
Batch 48/64 loss: -0.2352595329284668
Batch 49/64 loss: -0.43549203872680664
Batch 50/64 loss: -0.09166193008422852
Batch 51/64 loss: -0.2713189125061035
Batch 52/64 loss: -0.48113298416137695
Batch 53/64 loss: -0.3128533363342285
Batch 54/64 loss: -0.4463677406311035
Batch 55/64 loss: -0.34917449951171875
Batch 56/64 loss: -0.24041271209716797
Batch 57/64 loss: -0.22905492782592773
Batch 58/64 loss: -0.30847740173339844
Batch 59/64 loss: -0.1025238037109375
Batch 60/64 loss: -0.414461612701416
Batch 61/64 loss: 0.15801429748535156
Batch 62/64 loss: -0.2138214111328125
Batch 63/64 loss: -0.6551022529602051
Batch 64/64 loss: -3.7347798347473145
Epoch 149  Train loss: -0.2698918492186303  Val loss: -0.36489501114153783
Epoch 150
-------------------------------
Batch 1/64 loss: 0.24771356582641602
Batch 2/64 loss: -0.09654045104980469
Batch 3/64 loss: -0.5680780410766602
Batch 4/64 loss: -0.2497234344482422
Batch 5/64 loss: -0.24743270874023438
Batch 6/64 loss: -0.28566503524780273
Batch 7/64 loss: -0.0357661247253418
Batch 8/64 loss: -0.16641616821289062
Batch 9/64 loss: -0.12720680236816406
Batch 10/64 loss: -0.4325900077819824
Batch 11/64 loss: -0.045137882232666016
Batch 12/64 loss: -0.5089807510375977
Batch 13/64 loss: -0.4192056655883789
Batch 14/64 loss: -0.33597326278686523
Batch 15/64 loss: -0.367128849029541
Batch 16/64 loss: -0.03015279769897461
Batch 17/64 loss: -0.05837821960449219
Batch 18/64 loss: -0.5585198402404785
Batch 19/64 loss: -0.28039979934692383
Batch 20/64 loss: -0.6460838317871094
Batch 21/64 loss: -0.4115257263183594
Batch 22/64 loss: -0.25769758224487305
Batch 23/64 loss: -0.1632523536682129
Batch 24/64 loss: 0.23802852630615234
Batch 25/64 loss: 0.28345775604248047
Batch 26/64 loss: -0.2606987953186035
Batch 27/64 loss: 0.169952392578125
Batch 28/64 loss: 0.0355830192565918
Batch 29/64 loss: 0.20660972595214844
Batch 30/64 loss: -0.3916645050048828
Batch 31/64 loss: -0.29911327362060547
Batch 32/64 loss: -0.25562238693237305
Batch 33/64 loss: -0.25473594665527344
Batch 34/64 loss: -0.3368515968322754
Batch 35/64 loss: -0.32779884338378906
Batch 36/64 loss: -0.37563228607177734
Batch 37/64 loss: -0.45943450927734375
Batch 38/64 loss: 0.3310279846191406
Batch 39/64 loss: -0.5033140182495117
Batch 40/64 loss: -0.2113947868347168
Batch 41/64 loss: -0.4511127471923828
Batch 42/64 loss: -0.3654303550720215
Batch 43/64 loss: -0.44728660583496094
Batch 44/64 loss: -0.4065427780151367
Batch 45/64 loss: -0.4887118339538574
Batch 46/64 loss: -0.1863856315612793
Batch 47/64 loss: -0.32927799224853516
Batch 48/64 loss: -0.22979736328125
Batch 49/64 loss: -0.5939774513244629
Batch 50/64 loss: 0.46260547637939453
Batch 51/64 loss: -0.4147982597351074
Batch 52/64 loss: -0.2958974838256836
Batch 53/64 loss: -0.3825960159301758
Batch 54/64 loss: -0.11895179748535156
Batch 55/64 loss: -0.41214466094970703
Batch 56/64 loss: -0.29750633239746094
Batch 57/64 loss: -0.3812141418457031
Batch 58/64 loss: 0.034552574157714844
Batch 59/64 loss: -0.24090242385864258
Batch 60/64 loss: -0.5324206352233887
Batch 61/64 loss: 0.0037374496459960938
Batch 62/64 loss: -0.13297080993652344
Batch 63/64 loss: -0.38582801818847656
Batch 64/64 loss: -3.490755081176758
Epoch 150  Train loss: -0.2771241954728669  Val loss: -0.37505821673730805
Epoch 151
-------------------------------
Batch 1/64 loss: 0.25117063522338867
Batch 2/64 loss: -0.32047557830810547
Batch 3/64 loss: -0.1618633270263672
Batch 4/64 loss: -0.6054792404174805
Batch 5/64 loss: -0.48170948028564453
Batch 6/64 loss: 0.431699275970459
Batch 7/64 loss: 0.022780418395996094
Batch 8/64 loss: -0.0885167121887207
Batch 9/64 loss: -0.31719207763671875
Batch 10/64 loss: -0.29461240768432617
Batch 11/64 loss: -0.10939979553222656
Batch 12/64 loss: -0.12508249282836914
Batch 13/64 loss: -0.02943134307861328
Batch 14/64 loss: -0.060535430908203125
Batch 15/64 loss: -0.6071743965148926
Batch 16/64 loss: -0.1474924087524414
Batch 17/64 loss: -0.7276825904846191
Batch 18/64 loss: -0.26353025436401367
Batch 19/64 loss: -0.2418060302734375
Batch 20/64 loss: -0.502955436706543
Batch 21/64 loss: -0.2990598678588867
Batch 22/64 loss: -0.7242507934570312
Batch 23/64 loss: -0.3929319381713867
Batch 24/64 loss: -0.3735818862915039
Batch 25/64 loss: -0.29880619049072266
Batch 26/64 loss: 0.13980436325073242
Batch 27/64 loss: -0.21643400192260742
Batch 28/64 loss: -0.42504310607910156
Batch 29/64 loss: -0.45662975311279297
Batch 30/64 loss: -0.5607271194458008
Batch 31/64 loss: 0.19248294830322266
Batch 32/64 loss: -0.23256444931030273
Batch 33/64 loss: -0.2873044013977051
Batch 34/64 loss: -0.11584854125976562
Batch 35/64 loss: -0.42948436737060547
Batch 36/64 loss: -0.40290164947509766
Batch 37/64 loss: -0.3477482795715332
Batch 38/64 loss: -0.1507415771484375
Batch 39/64 loss: -0.2930107116699219
Batch 40/64 loss: -0.3528170585632324
Batch 41/64 loss: -0.6061348915100098
Batch 42/64 loss: -0.3007044792175293
Batch 43/64 loss: -0.3585481643676758
Batch 44/64 loss: -0.3687000274658203
Batch 45/64 loss: 1.0487995147705078
Batch 46/64 loss: 0.16794729232788086
Batch 47/64 loss: -0.13038921356201172
Batch 48/64 loss: -0.4010148048400879
Batch 49/64 loss: -0.05988025665283203
Batch 50/64 loss: -0.44106245040893555
Batch 51/64 loss: -0.4315657615661621
Batch 52/64 loss: -0.24332761764526367
Batch 53/64 loss: -0.024192333221435547
Batch 54/64 loss: -0.40523862838745117
Batch 55/64 loss: 0.05190849304199219
Batch 56/64 loss: 0.24409723281860352
Batch 57/64 loss: -0.568082332611084
Batch 58/64 loss: -0.4300417900085449
Batch 59/64 loss: -0.6425848007202148
Batch 60/64 loss: -0.06789827346801758
Batch 61/64 loss: -0.06915616989135742
Batch 62/64 loss: -0.618861198425293
Batch 63/64 loss: -0.22569799423217773
Batch 64/64 loss: -4.267512321472168
Epoch 151  Train loss: -0.2900054894241632  Val loss: -0.4411956813327226
Saving best model, epoch: 151
Epoch 152
-------------------------------
Batch 1/64 loss: -0.5036768913269043
Batch 2/64 loss: -0.2760047912597656
Batch 3/64 loss: -0.1223912239074707
Batch 4/64 loss: -0.4553947448730469
Batch 5/64 loss: -0.5838265419006348
Batch 6/64 loss: -0.17498302459716797
Batch 7/64 loss: 0.07303380966186523
Batch 8/64 loss: -0.004519939422607422
Batch 9/64 loss: 0.1615009307861328
Batch 10/64 loss: -0.6267986297607422
Batch 11/64 loss: -0.18922996520996094
Batch 12/64 loss: -0.37732744216918945
Batch 13/64 loss: -0.3558330535888672
Batch 14/64 loss: -0.3375511169433594
Batch 15/64 loss: -0.03592109680175781
Batch 16/64 loss: -0.34422826766967773
Batch 17/64 loss: -0.14953136444091797
Batch 18/64 loss: -0.0966787338256836
Batch 19/64 loss: -0.46788549423217773
Batch 20/64 loss: -0.31826162338256836
Batch 21/64 loss: -0.4953608512878418
Batch 22/64 loss: -0.08654928207397461
Batch 23/64 loss: -0.47849035263061523
Batch 24/64 loss: -0.35066652297973633
Batch 25/64 loss: -0.39218759536743164
Batch 26/64 loss: 0.1100916862487793
Batch 27/64 loss: 0.009869098663330078
Batch 28/64 loss: -0.11858177185058594
Batch 29/64 loss: -0.1724562644958496
Batch 30/64 loss: 0.5334744453430176
Batch 31/64 loss: -0.2215876579284668
Batch 32/64 loss: -0.16588640213012695
Batch 33/64 loss: -0.4411163330078125
Batch 34/64 loss: -0.6117734909057617
Batch 35/64 loss: 0.09751224517822266
Batch 36/64 loss: -0.3495926856994629
Batch 37/64 loss: -0.4100165367126465
Batch 38/64 loss: -0.23781633377075195
Batch 39/64 loss: -0.2651691436767578
Batch 40/64 loss: 0.06267881393432617
Batch 41/64 loss: 0.020004749298095703
Batch 42/64 loss: -0.16740036010742188
Batch 43/64 loss: -0.32410526275634766
Batch 44/64 loss: -0.36434078216552734
Batch 45/64 loss: -0.3213353157043457
Batch 46/64 loss: 0.2330636978149414
Batch 47/64 loss: -0.313870906829834
Batch 48/64 loss: -0.1880803108215332
Batch 49/64 loss: -0.15186357498168945
Batch 50/64 loss: 0.14505815505981445
Batch 51/64 loss: -0.058582305908203125
Batch 52/64 loss: -0.011707782745361328
Batch 53/64 loss: 0.09980392456054688
Batch 54/64 loss: -0.6553559303283691
Batch 55/64 loss: -0.4839916229248047
Batch 56/64 loss: 0.06644916534423828
Batch 57/64 loss: 0.15700101852416992
Batch 58/64 loss: 0.18195009231567383
Batch 59/64 loss: -0.0640869140625
Batch 60/64 loss: -0.11600589752197266
Batch 61/64 loss: -0.21045255661010742
Batch 62/64 loss: -0.3003048896789551
Batch 63/64 loss: -0.11967897415161133
Batch 64/64 loss: -3.9757351875305176
Epoch 152  Train loss: -0.23684342141244927  Val loss: -0.47577322471592437
Saving best model, epoch: 152
Epoch 153
-------------------------------
Batch 1/64 loss: -0.36844968795776367
Batch 2/64 loss: -0.41058921813964844
Batch 3/64 loss: -0.4049859046936035
Batch 4/64 loss: -0.05098581314086914
Batch 5/64 loss: -0.020243167877197266
Batch 6/64 loss: -0.6340775489807129
Batch 7/64 loss: 0.0959625244140625
Batch 8/64 loss: -0.20930862426757812
Batch 9/64 loss: -0.4208216667175293
Batch 10/64 loss: -0.15550899505615234
Batch 11/64 loss: -0.5653009414672852
Batch 12/64 loss: -0.6032576560974121
Batch 13/64 loss: -0.018770217895507812
Batch 14/64 loss: -0.3842177391052246
Batch 15/64 loss: -0.4595785140991211
Batch 16/64 loss: -0.5658473968505859
Batch 17/64 loss: -0.15631961822509766
Batch 18/64 loss: 0.03353691101074219
Batch 19/64 loss: -0.49814653396606445
Batch 20/64 loss: 0.09312963485717773
Batch 21/64 loss: -0.5767440795898438
Batch 22/64 loss: -0.0778350830078125
Batch 23/64 loss: -0.012648582458496094
Batch 24/64 loss: -0.3148670196533203
Batch 25/64 loss: -0.28867626190185547
Batch 26/64 loss: -0.08228445053100586
Batch 27/64 loss: -0.4850578308105469
Batch 28/64 loss: -0.0368499755859375
Batch 29/64 loss: -0.5171451568603516
Batch 30/64 loss: -0.5491600036621094
Batch 31/64 loss: -0.018863201141357422
Batch 32/64 loss: -0.09788370132446289
Batch 33/64 loss: 0.03672933578491211
Batch 34/64 loss: 0.5377473831176758
Batch 35/64 loss: -0.40706729888916016
Batch 36/64 loss: -0.3916049003601074
Batch 37/64 loss: -0.22200632095336914
Batch 38/64 loss: -0.21332168579101562
Batch 39/64 loss: -0.5013761520385742
Batch 40/64 loss: -0.5036678314208984
Batch 41/64 loss: -0.30832624435424805
Batch 42/64 loss: 0.2718954086303711
Batch 43/64 loss: -0.2738051414489746
Batch 44/64 loss: -0.5644941329956055
Batch 45/64 loss: -0.17859554290771484
Batch 46/64 loss: -0.19668292999267578
Batch 47/64 loss: -0.2521381378173828
Batch 48/64 loss: 0.14570379257202148
Batch 49/64 loss: -0.4157266616821289
Batch 50/64 loss: -0.28495073318481445
Batch 51/64 loss: -0.9238195419311523
Batch 52/64 loss: -0.5039939880371094
Batch 53/64 loss: 0.0863490104675293
Batch 54/64 loss: -0.46192359924316406
Batch 55/64 loss: -0.0579071044921875
Batch 56/64 loss: -0.1662755012512207
Batch 57/64 loss: -0.08237361907958984
Batch 58/64 loss: 0.0015778541564941406
Batch 59/64 loss: -0.6634588241577148
Batch 60/64 loss: -0.38043642044067383
Batch 61/64 loss: -0.028503894805908203
Batch 62/64 loss: -0.09499025344848633
Batch 63/64 loss: 0.027271747589111328
Batch 64/64 loss: -3.936934471130371
Epoch 153  Train loss: -0.2930928361182119  Val loss: -0.32487596597048835
Epoch 154
-------------------------------
Batch 1/64 loss: -0.4818897247314453
Batch 2/64 loss: -0.6560888290405273
Batch 3/64 loss: -0.3185253143310547
Batch 4/64 loss: -0.1976156234741211
Batch 5/64 loss: 0.11527395248413086
Batch 6/64 loss: -0.5001592636108398
Batch 7/64 loss: -0.16765165328979492
Batch 8/64 loss: -0.4689154624938965
Batch 9/64 loss: 0.1632404327392578
Batch 10/64 loss: -0.10982942581176758
Batch 11/64 loss: -0.31844663619995117
Batch 12/64 loss: -0.5316991806030273
Batch 13/64 loss: -0.36278438568115234
Batch 14/64 loss: -0.4235086441040039
Batch 15/64 loss: -0.08697032928466797
Batch 16/64 loss: 0.09812402725219727
Batch 17/64 loss: 0.05478668212890625
Batch 18/64 loss: -0.5182352066040039
Batch 19/64 loss: -0.5701923370361328
Batch 20/64 loss: -0.7395210266113281
Batch 21/64 loss: -0.5928130149841309
Batch 22/64 loss: -0.07921075820922852
Batch 23/64 loss: -0.3599863052368164
Batch 24/64 loss: -0.48070859909057617
Batch 25/64 loss: -0.40189170837402344
Batch 26/64 loss: 0.13367366790771484
Batch 27/64 loss: -0.4476461410522461
Batch 28/64 loss: 0.22621631622314453
Batch 29/64 loss: -0.4767131805419922
Batch 30/64 loss: -0.18843650817871094
Batch 31/64 loss: 0.3247098922729492
Batch 32/64 loss: -0.21117687225341797
Batch 33/64 loss: -0.26322460174560547
Batch 34/64 loss: -0.39440441131591797
Batch 35/64 loss: -0.2423558235168457
Batch 36/64 loss: 0.09609508514404297
Batch 37/64 loss: -0.6868953704833984
Batch 38/64 loss: 0.36822986602783203
Batch 39/64 loss: -0.29837989807128906
Batch 40/64 loss: 0.2221212387084961
Batch 41/64 loss: -0.46575021743774414
Batch 42/64 loss: -0.6562714576721191
Batch 43/64 loss: -0.3367161750793457
Batch 44/64 loss: -0.3666224479675293
Batch 45/64 loss: -0.2976341247558594
Batch 46/64 loss: -0.6639671325683594
Batch 47/64 loss: 0.025684833526611328
Batch 48/64 loss: -0.28586578369140625
Batch 49/64 loss: -0.21186542510986328
Batch 50/64 loss: -0.4775094985961914
Batch 51/64 loss: 0.10872602462768555
Batch 52/64 loss: -0.26694536209106445
Batch 53/64 loss: -0.21210098266601562
Batch 54/64 loss: -0.3136763572692871
Batch 55/64 loss: -0.3510928153991699
Batch 56/64 loss: -0.08362054824829102
Batch 57/64 loss: -0.5819602012634277
Batch 58/64 loss: -0.41210174560546875
Batch 59/64 loss: 0.20406675338745117
Batch 60/64 loss: -0.40140724182128906
Batch 61/64 loss: -0.14328527450561523
Batch 62/64 loss: -0.40424442291259766
Batch 63/64 loss: -0.4495406150817871
Batch 64/64 loss: -4.246483325958252
Epoch 154  Train loss: -0.31375635745478614  Val loss: -0.4166585522418989
Epoch 155
-------------------------------
Batch 1/64 loss: -0.18098974227905273
Batch 2/64 loss: 0.04779338836669922
Batch 3/64 loss: -0.3529086112976074
Batch 4/64 loss: -0.47830772399902344
Batch 5/64 loss: -0.19098472595214844
Batch 6/64 loss: -0.9024181365966797
Batch 7/64 loss: -0.8110775947570801
Batch 8/64 loss: -0.609705924987793
Batch 9/64 loss: -0.2031397819519043
Batch 10/64 loss: -0.5423951148986816
Batch 11/64 loss: -0.5066580772399902
Batch 12/64 loss: -0.19691181182861328
Batch 13/64 loss: -0.22933197021484375
Batch 14/64 loss: -0.23601007461547852
Batch 15/64 loss: -0.49080371856689453
Batch 16/64 loss: -0.5467739105224609
Batch 17/64 loss: -0.10544967651367188
Batch 18/64 loss: -0.42702388763427734
Batch 19/64 loss: -0.332977294921875
Batch 20/64 loss: -0.5597128868103027
Batch 21/64 loss: -0.33496999740600586
Batch 22/64 loss: -0.2720303535461426
Batch 23/64 loss: 0.10369682312011719
Batch 24/64 loss: 0.23768186569213867
Batch 25/64 loss: -0.1229705810546875
Batch 26/64 loss: -0.18737411499023438
Batch 27/64 loss: -0.43724584579467773
Batch 28/64 loss: -0.4093308448791504
Batch 29/64 loss: 0.30144262313842773
Batch 30/64 loss: -0.5826535224914551
Batch 31/64 loss: -0.4034743309020996
Batch 32/64 loss: -0.23901033401489258
Batch 33/64 loss: -0.42609453201293945
Batch 34/64 loss: -0.08456897735595703
Batch 35/64 loss: 0.16391706466674805
Batch 36/64 loss: -0.11329221725463867
Batch 37/64 loss: -0.2736973762512207
Batch 38/64 loss: 0.1051187515258789
Batch 39/64 loss: -0.5551967620849609
Batch 40/64 loss: -0.2038125991821289
Batch 41/64 loss: 0.5466527938842773
Batch 42/64 loss: -0.2031564712524414
Batch 43/64 loss: -0.28787899017333984
Batch 44/64 loss: 0.06647539138793945
Batch 45/64 loss: -0.35964536666870117
Batch 46/64 loss: -0.23744726181030273
Batch 47/64 loss: 0.07331466674804688
Batch 48/64 loss: -0.18523931503295898
Batch 49/64 loss: -0.5572619438171387
Batch 50/64 loss: -0.4282546043395996
Batch 51/64 loss: -0.3618617057800293
Batch 52/64 loss: -0.28617382049560547
Batch 53/64 loss: -0.1745295524597168
Batch 54/64 loss: -0.2898716926574707
Batch 55/64 loss: -0.07645320892333984
Batch 56/64 loss: 0.5450019836425781
Batch 57/64 loss: -0.5968446731567383
Batch 58/64 loss: -0.26358747482299805
Batch 59/64 loss: -0.5157055854797363
Batch 60/64 loss: -0.1203312873840332
Batch 61/64 loss: -0.2987699508666992
Batch 62/64 loss: -0.28885841369628906
Batch 63/64 loss: -0.37422990798950195
Batch 64/64 loss: -3.968381881713867
Epoch 155  Train loss: -0.30181326024672567  Val loss: -0.3523595882035613
Epoch 156
-------------------------------
Batch 1/64 loss: -0.21650409698486328
Batch 2/64 loss: -0.2857246398925781
Batch 3/64 loss: -0.20324182510375977
Batch 4/64 loss: 0.1565384864807129
Batch 5/64 loss: -0.3497591018676758
Batch 6/64 loss: -0.4464292526245117
Batch 7/64 loss: 0.04005861282348633
Batch 8/64 loss: -0.5301566123962402
Batch 9/64 loss: -0.010724067687988281
Batch 10/64 loss: 0.37473201751708984
Batch 11/64 loss: -0.23963594436645508
Batch 12/64 loss: -0.3799571990966797
Batch 13/64 loss: -0.5638885498046875
Batch 14/64 loss: -0.21462249755859375
Batch 15/64 loss: -0.3910388946533203
Batch 16/64 loss: -0.37330198287963867
Batch 17/64 loss: 0.2570180892944336
Batch 18/64 loss: -0.49264001846313477
Batch 19/64 loss: -0.6541681289672852
Batch 20/64 loss: -0.15637683868408203
Batch 21/64 loss: -0.2352447509765625
Batch 22/64 loss: -0.22713375091552734
Batch 23/64 loss: 0.27355289459228516
Batch 24/64 loss: -0.39806509017944336
Batch 25/64 loss: -0.0360407829284668
Batch 26/64 loss: -0.31118249893188477
Batch 27/64 loss: -0.14261913299560547
Batch 28/64 loss: -0.20835256576538086
Batch 29/64 loss: 0.16493463516235352
Batch 30/64 loss: -0.26886844635009766
Batch 31/64 loss: -0.2451624870300293
Batch 32/64 loss: -0.26108551025390625
Batch 33/64 loss: -0.3106880187988281
Batch 34/64 loss: -0.22806692123413086
Batch 35/64 loss: -0.0421748161315918
Batch 36/64 loss: -0.0693502426147461
Batch 37/64 loss: -0.25295591354370117
Batch 38/64 loss: 0.07914066314697266
Batch 39/64 loss: -0.10837793350219727
Batch 40/64 loss: -0.12900257110595703
Batch 41/64 loss: -0.31475353240966797
Batch 42/64 loss: -0.2493271827697754
Batch 43/64 loss: -0.07982730865478516
Batch 44/64 loss: -0.043752193450927734
Batch 45/64 loss: -0.3197464942932129
Batch 46/64 loss: -0.3776717185974121
Batch 47/64 loss: 0.01673269271850586
Batch 48/64 loss: -0.22550296783447266
Batch 49/64 loss: -0.07255744934082031
Batch 50/64 loss: -0.22289657592773438
Batch 51/64 loss: -0.21769475936889648
Batch 52/64 loss: -0.26209163665771484
Batch 53/64 loss: -0.012127876281738281
Batch 54/64 loss: -0.4357185363769531
Batch 55/64 loss: -0.5280189514160156
Batch 56/64 loss: -0.18615961074829102
Batch 57/64 loss: 0.30008840560913086
Batch 58/64 loss: -0.15519189834594727
Batch 59/64 loss: -0.2904520034790039
Batch 60/64 loss: -0.5282034873962402
Batch 61/64 loss: -0.32730960845947266
Batch 62/64 loss: -0.38933658599853516
Batch 63/64 loss: 0.033050537109375
Batch 64/64 loss: -3.9769234657287598
Epoch 156  Train loss: -0.24325846316767674  Val loss: -0.3437517303781411
Epoch 157
-------------------------------
Batch 1/64 loss: -0.1489400863647461
Batch 2/64 loss: -0.41699838638305664
Batch 3/64 loss: -0.4695134162902832
Batch 4/64 loss: -0.4043607711791992
Batch 5/64 loss: -0.48724937438964844
Batch 6/64 loss: -0.49704456329345703
Batch 7/64 loss: -0.25864744186401367
Batch 8/64 loss: -0.30705928802490234
Batch 9/64 loss: 0.1410503387451172
Batch 10/64 loss: 0.014833450317382812
Batch 11/64 loss: -0.6467657089233398
Batch 12/64 loss: -0.4725351333618164
Batch 13/64 loss: -0.1578664779663086
Batch 14/64 loss: -0.4617762565612793
Batch 15/64 loss: -0.3109879493713379
Batch 16/64 loss: -0.2962985038757324
Batch 17/64 loss: -0.45464611053466797
Batch 18/64 loss: 0.4693751335144043
Batch 19/64 loss: 0.09322214126586914
Batch 20/64 loss: -0.2942347526550293
Batch 21/64 loss: -0.260831356048584
Batch 22/64 loss: 0.2775592803955078
Batch 23/64 loss: 0.38324832916259766
Batch 24/64 loss: -0.42607975006103516
Batch 25/64 loss: -0.27179813385009766
Batch 26/64 loss: -0.1726212501525879
Batch 27/64 loss: -0.39662599563598633
Batch 28/64 loss: -0.511756420135498
Batch 29/64 loss: -0.43517494201660156
Batch 30/64 loss: -0.31311702728271484
Batch 31/64 loss: -0.3699150085449219
Batch 32/64 loss: -0.5216007232666016
Batch 33/64 loss: -0.6657576560974121
Batch 34/64 loss: -0.3751797676086426
Batch 35/64 loss: -0.34128618240356445
Batch 36/64 loss: -0.2969355583190918
Batch 37/64 loss: -0.0831599235534668
Batch 38/64 loss: -0.5497231483459473
Batch 39/64 loss: -0.4389066696166992
Batch 40/64 loss: -0.037885189056396484
Batch 41/64 loss: -0.3749523162841797
Batch 42/64 loss: -0.18657398223876953
Batch 43/64 loss: -0.15777254104614258
Batch 44/64 loss: -0.21278858184814453
Batch 45/64 loss: -0.2721123695373535
Batch 46/64 loss: -0.01608133316040039
Batch 47/64 loss: -0.271975040435791
Batch 48/64 loss: 0.007934093475341797
Batch 49/64 loss: -0.13800668716430664
Batch 50/64 loss: 0.6412215232849121
Batch 51/64 loss: -0.4267568588256836
Batch 52/64 loss: -0.18676185607910156
Batch 53/64 loss: -0.16448450088500977
Batch 54/64 loss: -0.3933844566345215
Batch 55/64 loss: -0.24299860000610352
Batch 56/64 loss: -0.11654424667358398
Batch 57/64 loss: -0.41950511932373047
Batch 58/64 loss: -0.3081059455871582
Batch 59/64 loss: 0.0029282569885253906
Batch 60/64 loss: 0.043440818786621094
Batch 61/64 loss: -0.5294094085693359
Batch 62/64 loss: -0.13620471954345703
Batch 63/64 loss: -0.2133035659790039
Batch 64/64 loss: -4.288425922393799
Epoch 157  Train loss: -0.2896079549602434  Val loss: -0.3821714080076447
Epoch 158
-------------------------------
Batch 1/64 loss: -0.5341463088989258
Batch 2/64 loss: -0.3368053436279297
Batch 3/64 loss: -0.3623476028442383
Batch 4/64 loss: -0.11947202682495117
Batch 5/64 loss: -0.4805135726928711
Batch 6/64 loss: 0.23073053359985352
Batch 7/64 loss: -0.47064733505249023
Batch 8/64 loss: -0.619504451751709
Batch 9/64 loss: -0.14461326599121094
Batch 10/64 loss: -0.3238039016723633
Batch 11/64 loss: -0.417630672454834
Batch 12/64 loss: -0.2497115135192871
Batch 13/64 loss: -0.07810592651367188
Batch 14/64 loss: -0.02123546600341797
Batch 15/64 loss: -0.4154949188232422
Batch 16/64 loss: -0.39867639541625977
Batch 17/64 loss: -0.5370297431945801
Batch 18/64 loss: -0.28202342987060547
Batch 19/64 loss: -0.6238517761230469
Batch 20/64 loss: 0.1338057518005371
Batch 21/64 loss: -0.21572637557983398
Batch 22/64 loss: -0.6056795120239258
Batch 23/64 loss: -0.10306787490844727
Batch 24/64 loss: -0.502741813659668
Batch 25/64 loss: -0.6504545211791992
Batch 26/64 loss: -0.04450416564941406
Batch 27/64 loss: -0.19688653945922852
Batch 28/64 loss: -0.17558717727661133
Batch 29/64 loss: -0.20483922958374023
Batch 30/64 loss: -0.3310837745666504
Batch 31/64 loss: -0.15503358840942383
Batch 32/64 loss: -0.5968594551086426
Batch 33/64 loss: -0.4788179397583008
Batch 34/64 loss: -0.23910903930664062
Batch 35/64 loss: -0.385587215423584
Batch 36/64 loss: -0.5809898376464844
Batch 37/64 loss: -0.41910457611083984
Batch 38/64 loss: 0.148162841796875
Batch 39/64 loss: -0.629734992980957
Batch 40/64 loss: -0.2812485694885254
Batch 41/64 loss: -0.2155900001525879
Batch 42/64 loss: -0.2779264450073242
Batch 43/64 loss: -0.26957273483276367
Batch 44/64 loss: 0.661931037902832
Batch 45/64 loss: 0.020320892333984375
Batch 46/64 loss: -0.37716054916381836
Batch 47/64 loss: -0.34876251220703125
Batch 48/64 loss: -0.5579872131347656
Batch 49/64 loss: -0.24590063095092773
Batch 50/64 loss: -0.3187284469604492
Batch 51/64 loss: 0.019539833068847656
Batch 52/64 loss: -0.4911656379699707
Batch 53/64 loss: -0.4330563545227051
Batch 54/64 loss: -0.1858220100402832
Batch 55/64 loss: -0.016968727111816406
Batch 56/64 loss: -0.13490867614746094
Batch 57/64 loss: 0.06932973861694336
Batch 58/64 loss: -0.1331653594970703
Batch 59/64 loss: -0.17482852935791016
Batch 60/64 loss: -0.033935546875
Batch 61/64 loss: -0.017629146575927734
Batch 62/64 loss: -0.4987010955810547
Batch 63/64 loss: -0.21573495864868164
Batch 64/64 loss: -3.945545196533203
Epoch 158  Train loss: -0.3111459245868758  Val loss: -0.367572273175741
Epoch 159
-------------------------------
Batch 1/64 loss: -0.5966148376464844
Batch 2/64 loss: -0.5310282707214355
Batch 3/64 loss: -0.24828624725341797
Batch 4/64 loss: -0.3164191246032715
Batch 5/64 loss: 0.13891983032226562
Batch 6/64 loss: -0.3831148147583008
Batch 7/64 loss: -0.45174169540405273
Batch 8/64 loss: -0.5346779823303223
Batch 9/64 loss: -0.23670625686645508
Batch 10/64 loss: 0.021631240844726562
Batch 11/64 loss: 0.06878376007080078
Batch 12/64 loss: -0.28803110122680664
Batch 13/64 loss: -0.10419464111328125
Batch 14/64 loss: 0.08445596694946289
Batch 15/64 loss: -0.007198333740234375
Batch 16/64 loss: -0.3822627067565918
Batch 17/64 loss: -0.3548555374145508
Batch 18/64 loss: -0.13872051239013672
Batch 19/64 loss: -0.06775617599487305
Batch 20/64 loss: 0.02372121810913086
Batch 21/64 loss: -0.5245809555053711
Batch 22/64 loss: -0.38395214080810547
Batch 23/64 loss: -0.3435516357421875
Batch 24/64 loss: -0.1594529151916504
Batch 25/64 loss: 0.02566814422607422
Batch 26/64 loss: 0.1579298973083496
Batch 27/64 loss: -0.7043924331665039
Batch 28/64 loss: -0.5785336494445801
Batch 29/64 loss: -0.3397188186645508
Batch 30/64 loss: -0.5126662254333496
Batch 31/64 loss: -0.2717909812927246
Batch 32/64 loss: -0.36953115463256836
Batch 33/64 loss: -0.6504101753234863
Batch 34/64 loss: 0.14182662963867188
Batch 35/64 loss: -0.4909634590148926
Batch 36/64 loss: -0.45136451721191406
Batch 37/64 loss: -0.008630752563476562
Batch 38/64 loss: -0.14228153228759766
Batch 39/64 loss: -0.33179235458374023
Batch 40/64 loss: -0.22887659072875977
Batch 41/64 loss: -0.2554202079772949
Batch 42/64 loss: -0.2470550537109375
Batch 43/64 loss: -0.4398927688598633
Batch 44/64 loss: -0.4216136932373047
Batch 45/64 loss: -0.015822887420654297
Batch 46/64 loss: 0.13496923446655273
Batch 47/64 loss: -0.30425071716308594
Batch 48/64 loss: -0.7680048942565918
Batch 49/64 loss: -0.13294363021850586
Batch 50/64 loss: 0.445101261138916
Batch 51/64 loss: 0.029614925384521484
Batch 52/64 loss: -0.3525724411010742
Batch 53/64 loss: 0.09992265701293945
Batch 54/64 loss: -0.4491410255432129
Batch 55/64 loss: -0.26621150970458984
Batch 56/64 loss: 0.31632423400878906
Batch 57/64 loss: -0.6632547378540039
Batch 58/64 loss: 0.0025949478149414062
Batch 59/64 loss: -0.684455394744873
Batch 60/64 loss: -0.4717574119567871
Batch 61/64 loss: -0.18121099472045898
Batch 62/64 loss: -0.4245109558105469
Batch 63/64 loss: -0.44957494735717773
Batch 64/64 loss: -3.9125399589538574
Epoch 159  Train loss: -0.29654482860191195  Val loss: -0.2720387383424949
Epoch 160
-------------------------------
Batch 1/64 loss: -0.61932373046875
Batch 2/64 loss: -0.005066871643066406
Batch 3/64 loss: -0.4222755432128906
Batch 4/64 loss: -0.21416854858398438
Batch 5/64 loss: -0.4976191520690918
Batch 6/64 loss: -0.4814300537109375
Batch 7/64 loss: -0.26781225204467773
Batch 8/64 loss: -0.43513059616088867
Batch 9/64 loss: -0.2064204216003418
Batch 10/64 loss: -0.13332223892211914
Batch 11/64 loss: 0.012439727783203125
Batch 12/64 loss: -0.15891790390014648
Batch 13/64 loss: -0.2822237014770508
Batch 14/64 loss: -0.20680761337280273
Batch 15/64 loss: -0.5237650871276855
Batch 16/64 loss: 0.35672426223754883
Batch 17/64 loss: -0.2008070945739746
Batch 18/64 loss: -0.6855936050415039
Batch 19/64 loss: -0.5332765579223633
Batch 20/64 loss: 0.006290435791015625
Batch 21/64 loss: -0.5603809356689453
Batch 22/64 loss: -0.6692953109741211
Batch 23/64 loss: -0.2391366958618164
Batch 24/64 loss: -0.27336931228637695
Batch 25/64 loss: -0.2771873474121094
Batch 26/64 loss: 0.16959905624389648
Batch 27/64 loss: -0.2914919853210449
Batch 28/64 loss: -0.5575723648071289
Batch 29/64 loss: -0.46231698989868164
Batch 30/64 loss: -0.3299241065979004
Batch 31/64 loss: -0.5888409614562988
Batch 32/64 loss: -0.4711318016052246
Batch 33/64 loss: -0.36850643157958984
Batch 34/64 loss: -0.3246898651123047
Batch 35/64 loss: -0.6216707229614258
Batch 36/64 loss: -0.411038875579834
Batch 37/64 loss: -0.46057558059692383
Batch 38/64 loss: -0.052336692810058594
Batch 39/64 loss: -0.6500544548034668
Batch 40/64 loss: -0.5618171691894531
Batch 41/64 loss: -0.3133230209350586
Batch 42/64 loss: -0.21406936645507812
Batch 43/64 loss: -0.6449542045593262
Batch 44/64 loss: -0.4817848205566406
Batch 45/64 loss: 0.13036489486694336
Batch 46/64 loss: 0.29100799560546875
Batch 47/64 loss: -0.4267420768737793
Batch 48/64 loss: -0.37657880783081055
Batch 49/64 loss: 0.12848711013793945
Batch 50/64 loss: -0.6118812561035156
Batch 51/64 loss: -0.5249309539794922
Batch 52/64 loss: -0.5608649253845215
Batch 53/64 loss: -0.19199800491333008
Batch 54/64 loss: -0.5123071670532227
Batch 55/64 loss: -0.3340320587158203
Batch 56/64 loss: -0.09947538375854492
Batch 57/64 loss: -0.31166791915893555
Batch 58/64 loss: -0.11125659942626953
Batch 59/64 loss: 0.19554853439331055
Batch 60/64 loss: 1.4883966445922852
Batch 61/64 loss: 0.22313499450683594
Batch 62/64 loss: -0.459317684173584
Batch 63/64 loss: 0.18078899383544922
Batch 64/64 loss: -3.5521368980407715
Epoch 160  Train loss: -0.30904788783952303  Val loss: -0.15841850464286672
Epoch 161
-------------------------------
Batch 1/64 loss: 0.22849321365356445
Batch 2/64 loss: -0.7065525054931641
Batch 3/64 loss: -0.4325404167175293
Batch 4/64 loss: -0.5263333320617676
Batch 5/64 loss: 0.15891075134277344
Batch 6/64 loss: -0.38057470321655273
Batch 7/64 loss: 0.22284460067749023
Batch 8/64 loss: 8.821487426757812e-05
Batch 9/64 loss: 0.09166097640991211
Batch 10/64 loss: -0.15300607681274414
Batch 11/64 loss: -0.26695728302001953
Batch 12/64 loss: -0.5460748672485352
Batch 13/64 loss: -0.2756948471069336
Batch 14/64 loss: -0.1358966827392578
Batch 15/64 loss: -0.09389400482177734
Batch 16/64 loss: -0.3036818504333496
Batch 17/64 loss: -0.43473339080810547
Batch 18/64 loss: -0.24987268447875977
Batch 19/64 loss: -0.5588288307189941
Batch 20/64 loss: -0.4082155227661133
Batch 21/64 loss: -0.26364994049072266
Batch 22/64 loss: -0.5664215087890625
Batch 23/64 loss: -0.05385398864746094
Batch 24/64 loss: -0.21469926834106445
Batch 25/64 loss: -0.41437816619873047
Batch 26/64 loss: -0.32058143615722656
Batch 27/64 loss: -0.020178794860839844
Batch 28/64 loss: -0.08310317993164062
Batch 29/64 loss: -0.5243887901306152
Batch 30/64 loss: -0.5591106414794922
Batch 31/64 loss: -0.3555469512939453
Batch 32/64 loss: -0.4836406707763672
Batch 33/64 loss: -0.28165674209594727
Batch 34/64 loss: -0.5371484756469727
Batch 35/64 loss: -0.4441404342651367
Batch 36/64 loss: -0.5113587379455566
Batch 37/64 loss: -0.36334991455078125
Batch 38/64 loss: -0.3256406784057617
Batch 39/64 loss: -0.2934393882751465
Batch 40/64 loss: -0.2392582893371582
Batch 41/64 loss: 0.45810699462890625
Batch 42/64 loss: -0.5347604751586914
Batch 43/64 loss: -0.4603137969970703
Batch 44/64 loss: -0.4079751968383789
Batch 45/64 loss: -0.2849545478820801
Batch 46/64 loss: -0.10248374938964844
Batch 47/64 loss: -0.5257492065429688
Batch 48/64 loss: 0.009047508239746094
Batch 49/64 loss: -0.3721799850463867
Batch 50/64 loss: -0.14561176300048828
Batch 51/64 loss: -0.039612770080566406
Batch 52/64 loss: -0.05108308792114258
Batch 53/64 loss: 0.5027828216552734
Batch 54/64 loss: -0.1279592514038086
Batch 55/64 loss: 0.526308536529541
Batch 56/64 loss: -0.5045151710510254
Batch 57/64 loss: -0.27538394927978516
Batch 58/64 loss: -0.13744592666625977
Batch 59/64 loss: -0.3305683135986328
Batch 60/64 loss: -0.3947277069091797
Batch 61/64 loss: -0.5354456901550293
Batch 62/64 loss: 0.0164337158203125
Batch 63/64 loss: -0.3153200149536133
Batch 64/64 loss: -4.32819128036499
Epoch 161  Train loss: -0.29656407412360697  Val loss: -0.30301681990476
Epoch 162
-------------------------------
Batch 1/64 loss: -0.2774772644042969
Batch 2/64 loss: -0.3392910957336426
Batch 3/64 loss: -0.40180206298828125
Batch 4/64 loss: -0.23528718948364258
Batch 5/64 loss: -0.5486693382263184
Batch 6/64 loss: 0.20995283126831055
Batch 7/64 loss: -0.5984268188476562
Batch 8/64 loss: -0.22453022003173828
Batch 9/64 loss: -0.13669443130493164
Batch 10/64 loss: 0.12432670593261719
Batch 11/64 loss: -0.3902568817138672
Batch 12/64 loss: -0.4373030662536621
Batch 13/64 loss: 0.01837635040283203
Batch 14/64 loss: -0.20834589004516602
Batch 15/64 loss: 0.01813793182373047
Batch 16/64 loss: -0.43221139907836914
Batch 17/64 loss: -0.1241312026977539
Batch 18/64 loss: -0.2605900764465332
Batch 19/64 loss: -0.23139190673828125
Batch 20/64 loss: 0.18886947631835938
Batch 21/64 loss: -0.4092369079589844
Batch 22/64 loss: -0.3614945411682129
Batch 23/64 loss: -0.03500080108642578
Batch 24/64 loss: -0.3811683654785156
Batch 25/64 loss: -0.5191512107849121
Batch 26/64 loss: -0.24842405319213867
Batch 27/64 loss: -0.7403445243835449
Batch 28/64 loss: -0.4649672508239746
Batch 29/64 loss: -0.2542581558227539
Batch 30/64 loss: -0.3976902961730957
Batch 31/64 loss: -0.4040493965148926
Batch 32/64 loss: -0.4295692443847656
Batch 33/64 loss: -0.4274263381958008
Batch 34/64 loss: -0.5652303695678711
Batch 35/64 loss: -0.03263139724731445
Batch 36/64 loss: -0.3297247886657715
Batch 37/64 loss: -0.4260110855102539
Batch 38/64 loss: 0.2850494384765625
Batch 39/64 loss: -0.03630876541137695
Batch 40/64 loss: -0.40447044372558594
Batch 41/64 loss: -0.0517277717590332
Batch 42/64 loss: -0.08806324005126953
Batch 43/64 loss: -0.2539987564086914
Batch 44/64 loss: 0.32110071182250977
Batch 45/64 loss: -0.003788471221923828
Batch 46/64 loss: -0.14893102645874023
Batch 47/64 loss: -0.29462385177612305
Batch 48/64 loss: -0.06672191619873047
Batch 49/64 loss: -0.14795970916748047
Batch 50/64 loss: -0.14498615264892578
Batch 51/64 loss: -0.49078893661499023
Batch 52/64 loss: -0.3811802864074707
Batch 53/64 loss: -0.0279541015625
Batch 54/64 loss: 0.09006595611572266
Batch 55/64 loss: -0.33686304092407227
Batch 56/64 loss: -0.19159555435180664
Batch 57/64 loss: -0.47481250762939453
Batch 58/64 loss: -0.24555015563964844
Batch 59/64 loss: -0.21289539337158203
Batch 60/64 loss: 0.7135009765625
Batch 61/64 loss: -0.48538827896118164
Batch 62/64 loss: -0.29663991928100586
Batch 63/64 loss: -0.22047710418701172
Batch 64/64 loss: -3.923048973083496
Epoch 162  Train loss: -0.27061049891453165  Val loss: -0.3377568353082716
Epoch 163
-------------------------------
Batch 1/64 loss: -0.20206022262573242
Batch 2/64 loss: -0.3780040740966797
Batch 3/64 loss: 0.0696249008178711
Batch 4/64 loss: 0.07641792297363281
Batch 5/64 loss: 0.026572704315185547
Batch 6/64 loss: -0.5370669364929199
Batch 7/64 loss: -0.6074895858764648
Batch 8/64 loss: -0.2667675018310547
Batch 9/64 loss: -0.15323734283447266
Batch 10/64 loss: -0.6165590286254883
Batch 11/64 loss: -0.11901378631591797
Batch 12/64 loss: -0.371978759765625
Batch 13/64 loss: 0.4265456199645996
Batch 14/64 loss: -0.24093151092529297
Batch 15/64 loss: -0.09156608581542969
Batch 16/64 loss: -0.13941383361816406
Batch 17/64 loss: -0.3425922393798828
Batch 18/64 loss: -0.5914487838745117
Batch 19/64 loss: -0.587162971496582
Batch 20/64 loss: 0.19483232498168945
Batch 21/64 loss: -0.3367481231689453
Batch 22/64 loss: -0.3292675018310547
Batch 23/64 loss: -0.438751220703125
Batch 24/64 loss: 0.21095657348632812
Batch 25/64 loss: -0.49428844451904297
Batch 26/64 loss: -0.09221601486206055
Batch 27/64 loss: -0.24621152877807617
Batch 28/64 loss: -0.6339478492736816
Batch 29/64 loss: -0.3310122489929199
Batch 30/64 loss: -0.18709468841552734
Batch 31/64 loss: -0.21145963668823242
Batch 32/64 loss: -0.08042240142822266
Batch 33/64 loss: -0.45059776306152344
Batch 34/64 loss: -0.1140890121459961
Batch 35/64 loss: -0.4947824478149414
Batch 36/64 loss: 0.15082406997680664
Batch 37/64 loss: -0.22901391983032227
Batch 38/64 loss: -0.26659202575683594
Batch 39/64 loss: -0.5116939544677734
Batch 40/64 loss: -0.1278371810913086
Batch 41/64 loss: -0.36850595474243164
Batch 42/64 loss: -0.45808887481689453
Batch 43/64 loss: -0.2524452209472656
Batch 44/64 loss: -0.30615711212158203
Batch 45/64 loss: -0.3606834411621094
Batch 46/64 loss: -0.46929454803466797
Batch 47/64 loss: -0.4602961540222168
Batch 48/64 loss: -0.2029256820678711
Batch 49/64 loss: -0.20511484146118164
Batch 50/64 loss: -0.013191699981689453
Batch 51/64 loss: -0.4533510208129883
Batch 52/64 loss: -0.3937101364135742
Batch 53/64 loss: -0.4536900520324707
Batch 54/64 loss: -0.3072018623352051
Batch 55/64 loss: -0.12463712692260742
Batch 56/64 loss: -0.06011629104614258
Batch 57/64 loss: -0.2502098083496094
Batch 58/64 loss: -0.4309520721435547
Batch 59/64 loss: -0.4854698181152344
Batch 60/64 loss: -0.35376787185668945
Batch 61/64 loss: -0.5945882797241211
Batch 62/64 loss: -0.4180278778076172
Batch 63/64 loss: -0.34029674530029297
Batch 64/64 loss: -4.15809965133667
Epoch 163  Train loss: -0.32230339985267786  Val loss: -0.4501921794668506
Epoch 164
-------------------------------
Batch 1/64 loss: 0.44228124618530273
Batch 2/64 loss: 0.24706697463989258
Batch 3/64 loss: 0.2865900993347168
Batch 4/64 loss: -0.3874797821044922
Batch 5/64 loss: 0.13146114349365234
Batch 6/64 loss: 0.01565694808959961
Batch 7/64 loss: -0.3054618835449219
Batch 8/64 loss: -0.6473598480224609
Batch 9/64 loss: -0.47161006927490234
Batch 10/64 loss: -0.43628740310668945
Batch 11/64 loss: -0.0469508171081543
Batch 12/64 loss: -0.20163631439208984
Batch 13/64 loss: -0.44243526458740234
Batch 14/64 loss: -0.3763566017150879
Batch 15/64 loss: -0.03728055953979492
Batch 16/64 loss: -0.32962846755981445
Batch 17/64 loss: -0.5433101654052734
Batch 18/64 loss: -0.2128143310546875
Batch 19/64 loss: -0.19675731658935547
Batch 20/64 loss: -0.26968812942504883
Batch 21/64 loss: -0.7212972640991211
Batch 22/64 loss: -0.19614887237548828
Batch 23/64 loss: -0.41024351119995117
Batch 24/64 loss: -0.6124272346496582
Batch 25/64 loss: 0.3126192092895508
Batch 26/64 loss: -0.23926591873168945
Batch 27/64 loss: -0.01474761962890625
Batch 28/64 loss: -0.40384483337402344
Batch 29/64 loss: 0.26340484619140625
Batch 30/64 loss: -0.5113325119018555
Batch 31/64 loss: 0.07187509536743164
Batch 32/64 loss: -0.6394014358520508
Batch 33/64 loss: 0.17654895782470703
Batch 34/64 loss: -0.2795438766479492
Batch 35/64 loss: -0.36296653747558594
Batch 36/64 loss: -0.23856544494628906
Batch 37/64 loss: -0.24497509002685547
Batch 38/64 loss: -0.3536081314086914
Batch 39/64 loss: -0.31645822525024414
Batch 40/64 loss: -0.3732466697692871
Batch 41/64 loss: -0.38037729263305664
Batch 42/64 loss: -0.22990131378173828
Batch 43/64 loss: -0.2984304428100586
Batch 44/64 loss: -0.16370105743408203
Batch 45/64 loss: -0.5120711326599121
Batch 46/64 loss: 0.13919305801391602
Batch 47/64 loss: -0.2022690773010254
Batch 48/64 loss: -0.011382579803466797
Batch 49/64 loss: -0.2701282501220703
Batch 50/64 loss: -0.38783979415893555
Batch 51/64 loss: -0.4690675735473633
Batch 52/64 loss: -0.5281147956848145
Batch 53/64 loss: -0.40523195266723633
Batch 54/64 loss: -0.127532958984375
Batch 55/64 loss: -0.43705129623413086
Batch 56/64 loss: -0.5276021957397461
Batch 57/64 loss: -0.144683837890625
Batch 58/64 loss: -0.24601507186889648
Batch 59/64 loss: -0.3036031723022461
Batch 60/64 loss: -0.6617803573608398
Batch 61/64 loss: -0.26735544204711914
Batch 62/64 loss: -0.46360301971435547
Batch 63/64 loss: -0.3516225814819336
Batch 64/64 loss: -4.083138465881348
Epoch 164  Train loss: -0.30099061330159504  Val loss: -0.4211613894328219
Epoch 165
-------------------------------
Batch 1/64 loss: -0.4655733108520508
Batch 2/64 loss: -0.5175619125366211
Batch 3/64 loss: -0.1523604393005371
Batch 4/64 loss: -0.4824552536010742
Batch 5/64 loss: -0.33719444274902344
Batch 6/64 loss: -0.2591714859008789
Batch 7/64 loss: -0.1834731101989746
Batch 8/64 loss: -0.3141212463378906
Batch 9/64 loss: -0.07430839538574219
Batch 10/64 loss: -0.2076096534729004
Batch 11/64 loss: 0.0031981468200683594
Batch 12/64 loss: 0.16239213943481445
Batch 13/64 loss: -0.18063116073608398
Batch 14/64 loss: -0.20116662979125977
Batch 15/64 loss: -0.6323757171630859
Batch 16/64 loss: -0.3070945739746094
Batch 17/64 loss: -0.6454906463623047
Batch 18/64 loss: -0.34594011306762695
Batch 19/64 loss: -0.022250652313232422
Batch 20/64 loss: -0.2960176467895508
Batch 21/64 loss: -0.00939178466796875
Batch 22/64 loss: -0.40353870391845703
Batch 23/64 loss: 0.3191361427307129
Batch 24/64 loss: -0.40803003311157227
Batch 25/64 loss: -0.3429689407348633
Batch 26/64 loss: -0.4436960220336914
Batch 27/64 loss: -0.20514297485351562
Batch 28/64 loss: -0.08400726318359375
Batch 29/64 loss: -0.40375804901123047
Batch 30/64 loss: -0.6167473793029785
Batch 31/64 loss: 0.032483577728271484
Batch 32/64 loss: -0.5875244140625
Batch 33/64 loss: -0.5600724220275879
Batch 34/64 loss: -0.44734621047973633
Batch 35/64 loss: -0.2522730827331543
Batch 36/64 loss: 0.07417726516723633
Batch 37/64 loss: 0.021719932556152344
Batch 38/64 loss: 0.006064414978027344
Batch 39/64 loss: -0.6588501930236816
Batch 40/64 loss: -0.1680598258972168
Batch 41/64 loss: -0.47335004806518555
Batch 42/64 loss: -0.3562145233154297
Batch 43/64 loss: -0.47444629669189453
Batch 44/64 loss: -0.5252447128295898
Batch 45/64 loss: -0.09361839294433594
Batch 46/64 loss: -0.6479034423828125
Batch 47/64 loss: 0.8361091613769531
Batch 48/64 loss: -0.32987546920776367
Batch 49/64 loss: -0.6309523582458496
Batch 50/64 loss: 0.1027536392211914
Batch 51/64 loss: -0.006488800048828125
Batch 52/64 loss: -0.4356846809387207
Batch 53/64 loss: -0.26672983169555664
Batch 54/64 loss: -0.24075078964233398
Batch 55/64 loss: -0.4548501968383789
Batch 56/64 loss: -0.5605697631835938
Batch 57/64 loss: -0.6334757804870605
Batch 58/64 loss: 0.09808731079101562
Batch 59/64 loss: -0.36086273193359375
Batch 60/64 loss: -0.18993759155273438
Batch 61/64 loss: -0.18692255020141602
Batch 62/64 loss: -0.2142167091369629
Batch 63/64 loss: -0.50531005859375
Batch 64/64 loss: -4.353663444519043
Epoch 165  Train loss: -0.3201997532564051  Val loss: -0.4485444006641296
Epoch 166
-------------------------------
Batch 1/64 loss: -0.06884622573852539
Batch 2/64 loss: -0.2974972724914551
Batch 3/64 loss: -0.6144132614135742
Batch 4/64 loss: -0.32033205032348633
Batch 5/64 loss: -0.7882776260375977
Batch 6/64 loss: -0.1240682601928711
Batch 7/64 loss: -0.302490234375
Batch 8/64 loss: -0.1452794075012207
Batch 9/64 loss: -0.06932210922241211
Batch 10/64 loss: 0.1586461067199707
Batch 11/64 loss: -0.285003662109375
Batch 12/64 loss: -0.5380024909973145
Batch 13/64 loss: -0.17897701263427734
Batch 14/64 loss: -0.5655813217163086
Batch 15/64 loss: -0.22571849822998047
Batch 16/64 loss: -0.19173431396484375
Batch 17/64 loss: -0.10961532592773438
Batch 18/64 loss: -0.21207189559936523
Batch 19/64 loss: -0.2340850830078125
Batch 20/64 loss: 1.3600678443908691
Batch 21/64 loss: -0.49121713638305664
Batch 22/64 loss: -0.38891029357910156
Batch 23/64 loss: 0.09164667129516602
Batch 24/64 loss: -0.40227508544921875
Batch 25/64 loss: -0.21616458892822266
Batch 26/64 loss: -0.3543891906738281
Batch 27/64 loss: 0.4047689437866211
Batch 28/64 loss: -0.21829986572265625
Batch 29/64 loss: -0.37158679962158203
Batch 30/64 loss: -0.09845256805419922
Batch 31/64 loss: -0.29728174209594727
Batch 32/64 loss: -0.24259090423583984
Batch 33/64 loss: 1.1231093406677246
Batch 34/64 loss: -0.22951745986938477
Batch 35/64 loss: -0.002704620361328125
Batch 36/64 loss: -0.4106006622314453
Batch 37/64 loss: -0.38874101638793945
Batch 38/64 loss: -0.510277271270752
Batch 39/64 loss: 0.06698751449584961
Batch 40/64 loss: -0.5974698066711426
Batch 41/64 loss: -0.05554676055908203
Batch 42/64 loss: -0.35088348388671875
Batch 43/64 loss: -0.34973955154418945
Batch 44/64 loss: -0.4733004570007324
Batch 45/64 loss: -0.1139059066772461
Batch 46/64 loss: -0.5069527626037598
Batch 47/64 loss: -0.31499195098876953
Batch 48/64 loss: 0.26915979385375977
Batch 49/64 loss: -0.10903692245483398
Batch 50/64 loss: 0.20136404037475586
Batch 51/64 loss: -0.3889636993408203
Batch 52/64 loss: -0.1448383331298828
Batch 53/64 loss: -0.43660593032836914
Batch 54/64 loss: -0.5732669830322266
Batch 55/64 loss: -0.32926273345947266
Batch 56/64 loss: -0.014606475830078125
Batch 57/64 loss: -0.3554811477661133
Batch 58/64 loss: -0.7353096008300781
Batch 59/64 loss: -0.3361387252807617
Batch 60/64 loss: -0.47204065322875977
Batch 61/64 loss: -0.3858060836791992
Batch 62/64 loss: -0.33422183990478516
Batch 63/64 loss: -0.6057391166687012
Batch 64/64 loss: -4.315195083618164
Epoch 166  Train loss: -0.27355419981713386  Val loss: -0.3978001571602838
Epoch 167
-------------------------------
Batch 1/64 loss: -0.46483659744262695
Batch 2/64 loss: -0.5184421539306641
Batch 3/64 loss: -0.362185001373291
Batch 4/64 loss: -0.43784093856811523
Batch 5/64 loss: -0.3522047996520996
Batch 6/64 loss: -0.38126277923583984
Batch 7/64 loss: -0.17118310928344727
Batch 8/64 loss: -0.12073469161987305
Batch 9/64 loss: -0.7024950981140137
Batch 10/64 loss: -0.5462026596069336
Batch 11/64 loss: -0.6890087127685547
Batch 12/64 loss: -0.22521257400512695
Batch 13/64 loss: -0.7018475532531738
Batch 14/64 loss: -0.4713115692138672
Batch 15/64 loss: 0.1072397232055664
Batch 16/64 loss: 0.5512118339538574
Batch 17/64 loss: -0.2472224235534668
Batch 18/64 loss: 0.039536476135253906
Batch 19/64 loss: -0.1202692985534668
Batch 20/64 loss: -0.3188462257385254
Batch 21/64 loss: -0.16223526000976562
Batch 22/64 loss: -0.1499958038330078
Batch 23/64 loss: -0.03602457046508789
Batch 24/64 loss: -0.4447469711303711
Batch 25/64 loss: -0.39957523345947266
Batch 26/64 loss: -0.09743070602416992
Batch 27/64 loss: -0.38542747497558594
Batch 28/64 loss: -0.5496702194213867
Batch 29/64 loss: -0.7202262878417969
Batch 30/64 loss: -0.5555224418640137
Batch 31/64 loss: -0.39337587356567383
Batch 32/64 loss: -0.2179403305053711
Batch 33/64 loss: -0.6075291633605957
Batch 34/64 loss: 0.6403279304504395
Batch 35/64 loss: -0.44335269927978516
Batch 36/64 loss: -0.383026123046875
Batch 37/64 loss: -0.43787717819213867
Batch 38/64 loss: -0.48627281188964844
Batch 39/64 loss: -0.02192211151123047
Batch 40/64 loss: -0.37844276428222656
Batch 41/64 loss: -0.30946826934814453
Batch 42/64 loss: -0.13281679153442383
Batch 43/64 loss: -0.14623403549194336
Batch 44/64 loss: -0.34075355529785156
Batch 45/64 loss: -0.1004171371459961
Batch 46/64 loss: 0.1767101287841797
Batch 47/64 loss: 0.01216745376586914
Batch 48/64 loss: -0.13856792449951172
Batch 49/64 loss: -0.3781619071960449
Batch 50/64 loss: -0.5425801277160645
Batch 51/64 loss: 0.07130765914916992
Batch 52/64 loss: -0.028171539306640625
Batch 53/64 loss: -0.45421600341796875
Batch 54/64 loss: -0.43190765380859375
Batch 55/64 loss: -0.4193539619445801
Batch 56/64 loss: -0.38687562942504883
Batch 57/64 loss: 0.7811031341552734
Batch 58/64 loss: -0.18240880966186523
Batch 59/64 loss: -0.5160598754882812
Batch 60/64 loss: -0.09287071228027344
Batch 61/64 loss: -0.5365829467773438
Batch 62/64 loss: 0.03490924835205078
Batch 63/64 loss: -0.08281183242797852
Batch 64/64 loss: -4.315854072570801
Epoch 167  Train loss: -0.3097150727814319  Val loss: -0.4048530604831132
Epoch 168
-------------------------------
Batch 1/64 loss: 0.05166053771972656
Batch 2/64 loss: -0.2383742332458496
Batch 3/64 loss: -0.46969127655029297
Batch 4/64 loss: 0.06028938293457031
Batch 5/64 loss: -0.3922238349914551
Batch 6/64 loss: 0.1136331558227539
Batch 7/64 loss: -0.2526721954345703
Batch 8/64 loss: -0.35990095138549805
Batch 9/64 loss: -0.4747328758239746
Batch 10/64 loss: -0.4317588806152344
Batch 11/64 loss: -0.429110050201416
Batch 12/64 loss: -0.4950985908508301
Batch 13/64 loss: -0.4855794906616211
Batch 14/64 loss: -0.38781261444091797
Batch 15/64 loss: -0.2776451110839844
Batch 16/64 loss: 0.05093860626220703
Batch 17/64 loss: -0.15510845184326172
Batch 18/64 loss: -0.21842432022094727
Batch 19/64 loss: -0.028810977935791016
Batch 20/64 loss: -0.5766816139221191
Batch 21/64 loss: 0.1730504035949707
Batch 22/64 loss: -0.5808525085449219
Batch 23/64 loss: -0.33363914489746094
Batch 24/64 loss: -0.12002277374267578
Batch 25/64 loss: -0.6113681793212891
Batch 26/64 loss: -0.20099878311157227
Batch 27/64 loss: -0.2807316780090332
Batch 28/64 loss: 0.13509607315063477
Batch 29/64 loss: 0.3910822868347168
Batch 30/64 loss: 0.3214263916015625
Batch 31/64 loss: 0.02554798126220703
Batch 32/64 loss: -0.2965536117553711
Batch 33/64 loss: -0.31597042083740234
Batch 34/64 loss: 0.31823158264160156
Batch 35/64 loss: -0.6090664863586426
Batch 36/64 loss: 0.09499931335449219
Batch 37/64 loss: 0.2221665382385254
Batch 38/64 loss: -0.24663114547729492
Batch 39/64 loss: -0.11300182342529297
Batch 40/64 loss: -0.5700244903564453
Batch 41/64 loss: -0.6657061576843262
Batch 42/64 loss: -0.2148585319519043
Batch 43/64 loss: -0.2876167297363281
Batch 44/64 loss: -0.5213546752929688
Batch 45/64 loss: -0.3400130271911621
Batch 46/64 loss: -0.6731481552124023
Batch 47/64 loss: -0.21311283111572266
Batch 48/64 loss: -0.33951759338378906
Batch 49/64 loss: -0.22218751907348633
Batch 50/64 loss: -0.2056903839111328
Batch 51/64 loss: -0.1431894302368164
Batch 52/64 loss: -0.2823820114135742
Batch 53/64 loss: -0.5632381439208984
Batch 54/64 loss: -0.39539098739624023
Batch 55/64 loss: -0.5969772338867188
Batch 56/64 loss: -0.6353240013122559
Batch 57/64 loss: -0.1412215232849121
Batch 58/64 loss: -0.49544286727905273
Batch 59/64 loss: 0.3316178321838379
Batch 60/64 loss: 0.31507253646850586
Batch 61/64 loss: -0.22928142547607422
Batch 62/64 loss: 0.13954877853393555
Batch 63/64 loss: -0.23319673538208008
Batch 64/64 loss: -3.9957408905029297
Epoch 168  Train loss: -0.27613773719937196  Val loss: -0.3977509855814406
Epoch 169
-------------------------------
Batch 1/64 loss: -0.4308309555053711
Batch 2/64 loss: -0.587928295135498
Batch 3/64 loss: -0.21093082427978516
Batch 4/64 loss: 0.09893035888671875
Batch 5/64 loss: -0.23029661178588867
Batch 6/64 loss: -0.45757102966308594
Batch 7/64 loss: -0.23505258560180664
Batch 8/64 loss: -0.5442423820495605
Batch 9/64 loss: -0.37006139755249023
Batch 10/64 loss: -0.288851261138916
Batch 11/64 loss: -0.16573047637939453
Batch 12/64 loss: -0.2082080841064453
Batch 13/64 loss: -0.8105854988098145
Batch 14/64 loss: -0.2625570297241211
Batch 15/64 loss: -0.37377166748046875
Batch 16/64 loss: -0.26865530014038086
Batch 17/64 loss: -0.5920548439025879
Batch 18/64 loss: 0.08180570602416992
Batch 19/64 loss: -0.391448974609375
Batch 20/64 loss: -0.6250905990600586
Batch 21/64 loss: -0.34491825103759766
Batch 22/64 loss: -0.770233154296875
Batch 23/64 loss: -0.30469703674316406
Batch 24/64 loss: -0.15807342529296875
Batch 25/64 loss: -0.1357560157775879
Batch 26/64 loss: -0.6068973541259766
Batch 27/64 loss: -0.22011852264404297
Batch 28/64 loss: 0.13093280792236328
Batch 29/64 loss: -0.28140926361083984
Batch 30/64 loss: -0.16971158981323242
Batch 31/64 loss: -0.3982677459716797
Batch 32/64 loss: -0.40238094329833984
Batch 33/64 loss: -0.28900718688964844
Batch 34/64 loss: -0.48026466369628906
Batch 35/64 loss: -0.5942015647888184
Batch 36/64 loss: 0.20508384704589844
Batch 37/64 loss: 0.7251186370849609
Batch 38/64 loss: -0.6329002380371094
Batch 39/64 loss: -0.5082874298095703
Batch 40/64 loss: -0.3099493980407715
Batch 41/64 loss: -0.7607064247131348
Batch 42/64 loss: -0.06745433807373047
Batch 43/64 loss: -0.13257837295532227
Batch 44/64 loss: -0.1944141387939453
Batch 45/64 loss: -0.3180427551269531
Batch 46/64 loss: -0.4292144775390625
Batch 47/64 loss: 0.051599979400634766
Batch 48/64 loss: 0.1270003318786621
Batch 49/64 loss: 0.2115488052368164
Batch 50/64 loss: -0.1660165786743164
Batch 51/64 loss: -0.6299710273742676
Batch 52/64 loss: -0.11492204666137695
Batch 53/64 loss: -0.5049057006835938
Batch 54/64 loss: -0.3703951835632324
Batch 55/64 loss: -0.10576677322387695
Batch 56/64 loss: 0.04612159729003906
Batch 57/64 loss: 0.18508005142211914
Batch 58/64 loss: -0.35436105728149414
Batch 59/64 loss: 0.5667743682861328
Batch 60/64 loss: -0.29056549072265625
Batch 61/64 loss: 0.11389827728271484
Batch 62/64 loss: -0.1892690658569336
Batch 63/64 loss: 0.360288143157959
Batch 64/64 loss: -4.072278022766113
Epoch 169  Train loss: -0.28924785314821727  Val loss: -0.22539488802251129
Epoch 170
-------------------------------
Batch 1/64 loss: -0.4724259376525879
Batch 2/64 loss: -0.34609222412109375
Batch 3/64 loss: -0.31812334060668945
Batch 4/64 loss: 0.9431900978088379
Batch 5/64 loss: -0.11794805526733398
Batch 6/64 loss: 0.13004016876220703
Batch 7/64 loss: -0.09031295776367188
Batch 8/64 loss: 0.005711078643798828
Batch 9/64 loss: 0.0909876823425293
Batch 10/64 loss: -0.08521842956542969
Batch 11/64 loss: -0.01674365997314453
Batch 12/64 loss: 0.022388458251953125
Batch 13/64 loss: -0.3812227249145508
Batch 14/64 loss: -0.22409725189208984
Batch 15/64 loss: -0.2493724822998047
Batch 16/64 loss: -0.2023472785949707
Batch 17/64 loss: -0.49657440185546875
Batch 18/64 loss: -0.09463310241699219
Batch 19/64 loss: 0.05486440658569336
Batch 20/64 loss: -0.3699946403503418
Batch 21/64 loss: -0.16787481307983398
Batch 22/64 loss: -0.5125164985656738
Batch 23/64 loss: 0.09208297729492188
Batch 24/64 loss: -0.20178604125976562
Batch 25/64 loss: -0.28864288330078125
Batch 26/64 loss: -0.39324283599853516
Batch 27/64 loss: -0.08292675018310547
Batch 28/64 loss: 0.01391458511352539
Batch 29/64 loss: -0.492037296295166
Batch 30/64 loss: -0.08499622344970703
Batch 31/64 loss: 0.2658071517944336
Batch 32/64 loss: -0.47348880767822266
Batch 33/64 loss: -0.16290712356567383
Batch 34/64 loss: 0.011637687683105469
Batch 35/64 loss: -0.3922276496887207
Batch 36/64 loss: -0.11049509048461914
Batch 37/64 loss: -0.013800621032714844
Batch 38/64 loss: -0.03400135040283203
Batch 39/64 loss: -0.2691020965576172
Batch 40/64 loss: -0.10585498809814453
Batch 41/64 loss: 0.11904478073120117
Batch 42/64 loss: -0.35237836837768555
Batch 43/64 loss: -0.12236642837524414
Batch 44/64 loss: -0.15042829513549805
Batch 45/64 loss: 0.08913707733154297
Batch 46/64 loss: -0.4905538558959961
Batch 47/64 loss: -0.38199710845947266
Batch 48/64 loss: -0.3357119560241699
Batch 49/64 loss: -0.18743562698364258
Batch 50/64 loss: 0.03865385055541992
Batch 51/64 loss: -0.5285711288452148
Batch 52/64 loss: -0.7733283042907715
Batch 53/64 loss: 0.10165739059448242
Batch 54/64 loss: -0.11502647399902344
Batch 55/64 loss: -0.2705698013305664
Batch 56/64 loss: -0.45220327377319336
Batch 57/64 loss: -2.4318695068359375e-05
Batch 58/64 loss: -0.3101634979248047
Batch 59/64 loss: -0.6777915954589844
Batch 60/64 loss: -0.6396722793579102
Batch 61/64 loss: 0.07717704772949219
Batch 62/64 loss: -0.31122684478759766
Batch 63/64 loss: -0.20619630813598633
Batch 64/64 loss: -4.126433372497559
Epoch 170  Train loss: -0.22894405664182177  Val loss: -0.4291538749773478
Epoch 171
-------------------------------
Batch 1/64 loss: -0.30254030227661133
Batch 2/64 loss: -0.33296775817871094
Batch 3/64 loss: -0.3288888931274414
Batch 4/64 loss: 0.0049762725830078125
Batch 5/64 loss: -0.2991056442260742
Batch 6/64 loss: 0.28246116638183594
Batch 7/64 loss: 0.1029348373413086
Batch 8/64 loss: 0.20455026626586914
Batch 9/64 loss: -0.5567879676818848
Batch 10/64 loss: -0.3445267677307129
Batch 11/64 loss: -0.3547511100769043
Batch 12/64 loss: -0.020544052124023438
Batch 13/64 loss: -0.1928234100341797
Batch 14/64 loss: 0.06975698471069336
Batch 15/64 loss: -0.03663921356201172
Batch 16/64 loss: -0.47342634201049805
Batch 17/64 loss: -0.09451961517333984
Batch 18/64 loss: -0.28732776641845703
Batch 19/64 loss: 0.1300044059753418
Batch 20/64 loss: -0.46019411087036133
Batch 21/64 loss: -0.18796300888061523
Batch 22/64 loss: -0.1514449119567871
Batch 23/64 loss: -0.39341115951538086
Batch 24/64 loss: -0.17992544174194336
Batch 25/64 loss: -0.3693556785583496
Batch 26/64 loss: -0.4416685104370117
Batch 27/64 loss: -0.25006580352783203
Batch 28/64 loss: -0.299405574798584
Batch 29/64 loss: -0.4235343933105469
Batch 30/64 loss: -0.32701873779296875
Batch 31/64 loss: -0.6518149375915527
Batch 32/64 loss: -0.5236687660217285
Batch 33/64 loss: -0.16665077209472656
Batch 34/64 loss: -0.38556814193725586
Batch 35/64 loss: -0.21653413772583008
Batch 36/64 loss: -0.4113593101501465
Batch 37/64 loss: -0.3284611701965332
Batch 38/64 loss: -0.48311901092529297
Batch 39/64 loss: -0.22941255569458008
Batch 40/64 loss: -0.5714654922485352
Batch 41/64 loss: -0.573491096496582
Batch 42/64 loss: -0.27516603469848633
Batch 43/64 loss: -0.30768871307373047
Batch 44/64 loss: -0.692835807800293
Batch 45/64 loss: -0.37437868118286133
Batch 46/64 loss: -0.09045886993408203
Batch 47/64 loss: -0.23027753829956055
Batch 48/64 loss: 0.006916999816894531
Batch 49/64 loss: -0.1504526138305664
Batch 50/64 loss: -0.23977375030517578
Batch 51/64 loss: -0.38501739501953125
Batch 52/64 loss: -0.4564690589904785
Batch 53/64 loss: -0.088104248046875
Batch 54/64 loss: -0.6247291564941406
Batch 55/64 loss: 0.09820413589477539
Batch 56/64 loss: -0.6257596015930176
Batch 57/64 loss: -0.6251397132873535
Batch 58/64 loss: -0.25365400314331055
Batch 59/64 loss: -0.5341653823852539
Batch 60/64 loss: -0.8277578353881836
Batch 61/64 loss: -0.060877323150634766
Batch 62/64 loss: -0.42325401306152344
Batch 63/64 loss: -0.26978302001953125
Batch 64/64 loss: -4.1339802742004395
Epoch 171  Train loss: -0.33547928567026175  Val loss: -0.3728949818824165
Epoch 172
-------------------------------
Batch 1/64 loss: -0.5756664276123047
Batch 2/64 loss: -0.19024944305419922
Batch 3/64 loss: -0.30934906005859375
Batch 4/64 loss: -0.5472884178161621
Batch 5/64 loss: 0.10557746887207031
Batch 6/64 loss: -0.007981300354003906
Batch 7/64 loss: 0.6701450347900391
Batch 8/64 loss: -0.49636173248291016
Batch 9/64 loss: -0.9321107864379883
Batch 10/64 loss: -0.5817852020263672
Batch 11/64 loss: -0.13231611251831055
Batch 12/64 loss: -0.5131959915161133
Batch 13/64 loss: -0.2620272636413574
Batch 14/64 loss: -0.6236371994018555
Batch 15/64 loss: -0.5195684432983398
Batch 16/64 loss: -0.13199090957641602
Batch 17/64 loss: -0.5720577239990234
Batch 18/64 loss: -0.2264266014099121
Batch 19/64 loss: -0.3401775360107422
Batch 20/64 loss: -0.45368528366088867
Batch 21/64 loss: -0.38732099533081055
Batch 22/64 loss: -0.4914674758911133
Batch 23/64 loss: -0.19129610061645508
Batch 24/64 loss: -0.16047906875610352
Batch 25/64 loss: -0.37024593353271484
Batch 26/64 loss: -0.30748414993286133
Batch 27/64 loss: -0.2769966125488281
Batch 28/64 loss: 1.8585453033447266
Batch 29/64 loss: -0.1926407814025879
Batch 30/64 loss: -0.5328340530395508
Batch 31/64 loss: -0.04197883605957031
Batch 32/64 loss: -0.09090423583984375
Batch 33/64 loss: -0.10037899017333984
Batch 34/64 loss: -0.04634809494018555
Batch 35/64 loss: -0.4372529983520508
Batch 36/64 loss: -0.18431520462036133
Batch 37/64 loss: 0.00011157989501953125
Batch 38/64 loss: -0.4398837089538574
Batch 39/64 loss: -0.38964176177978516
Batch 40/64 loss: -0.15956878662109375
Batch 41/64 loss: -0.26512908935546875
Batch 42/64 loss: -0.24714231491088867
Batch 43/64 loss: -0.13407182693481445
Batch 44/64 loss: -0.37554407119750977
Batch 45/64 loss: -0.1436138153076172
Batch 46/64 loss: -0.03586292266845703
Batch 47/64 loss: -0.5134825706481934
Batch 48/64 loss: -0.2943587303161621
Batch 49/64 loss: -0.42981958389282227
Batch 50/64 loss: 0.763730525970459
Batch 51/64 loss: -0.1739811897277832
Batch 52/64 loss: 0.6033749580383301
Batch 53/64 loss: -0.27823543548583984
Batch 54/64 loss: -0.17423772811889648
Batch 55/64 loss: -0.1879734992980957
Batch 56/64 loss: -0.025719642639160156
Batch 57/64 loss: -0.12322998046875
Batch 58/64 loss: 0.3724541664123535
Batch 59/64 loss: -0.4974174499511719
Batch 60/64 loss: -0.31919431686401367
Batch 61/64 loss: -0.33600568771362305
Batch 62/64 loss: 0.008517742156982422
Batch 63/64 loss: -0.44260406494140625
Batch 64/64 loss: -4.093287467956543
Epoch 172  Train loss: -0.24944385827756396  Val loss: -0.2865725120727959
Epoch 173
-------------------------------
Batch 1/64 loss: -0.020180702209472656
Batch 2/64 loss: -0.40854406356811523
Batch 3/64 loss: -0.5643863677978516
Batch 4/64 loss: -0.5075387954711914
Batch 5/64 loss: -0.3666810989379883
Batch 6/64 loss: -0.20249080657958984
Batch 7/64 loss: -0.21680068969726562
Batch 8/64 loss: -0.36496782302856445
Batch 9/64 loss: -0.4274430274963379
Batch 10/64 loss: -0.19655752182006836
Batch 11/64 loss: -0.009089469909667969
Batch 12/64 loss: -0.5188131332397461
Batch 13/64 loss: -0.39499616622924805
Batch 14/64 loss: -0.21989679336547852
Batch 15/64 loss: -0.36849451065063477
Batch 16/64 loss: -0.34708356857299805
Batch 17/64 loss: -0.3360610008239746
Batch 18/64 loss: -0.17137575149536133
Batch 19/64 loss: -0.03862476348876953
Batch 20/64 loss: -0.5315265655517578
Batch 21/64 loss: 0.5175247192382812
Batch 22/64 loss: 0.4103403091430664
Batch 23/64 loss: -0.3833770751953125
Batch 24/64 loss: -0.5279345512390137
Batch 25/64 loss: -0.31047773361206055
Batch 26/64 loss: -0.3025946617126465
Batch 27/64 loss: -0.354306697845459
Batch 28/64 loss: -0.4731712341308594
Batch 29/64 loss: -0.2908339500427246
Batch 30/64 loss: -0.261594295501709
Batch 31/64 loss: -0.11202859878540039
Batch 32/64 loss: -0.4560837745666504
Batch 33/64 loss: -0.3428177833557129
Batch 34/64 loss: -0.16042423248291016
Batch 35/64 loss: -0.7328281402587891
Batch 36/64 loss: -0.374509334564209
Batch 37/64 loss: 0.08354854583740234
Batch 38/64 loss: -0.28449249267578125
Batch 39/64 loss: -0.30394411087036133
Batch 40/64 loss: -0.33347463607788086
Batch 41/64 loss: -0.653322696685791
Batch 42/64 loss: -0.17118549346923828
Batch 43/64 loss: -0.6764440536499023
Batch 44/64 loss: -0.4481353759765625
Batch 45/64 loss: -0.7299580574035645
Batch 46/64 loss: 0.4284219741821289
Batch 47/64 loss: -0.03402519226074219
Batch 48/64 loss: -0.5311808586120605
Batch 49/64 loss: -0.49028825759887695
Batch 50/64 loss: -0.35671043395996094
Batch 51/64 loss: 0.08726072311401367
Batch 52/64 loss: -0.6170411109924316
Batch 53/64 loss: -0.1588740348815918
Batch 54/64 loss: -0.13110589981079102
Batch 55/64 loss: 0.0009317398071289062
Batch 56/64 loss: 0.17030668258666992
Batch 57/64 loss: -0.3737058639526367
Batch 58/64 loss: -0.4471144676208496
Batch 59/64 loss: -0.3574962615966797
Batch 60/64 loss: -0.06631994247436523
Batch 61/64 loss: -0.4539337158203125
Batch 62/64 loss: -0.4030609130859375
Batch 63/64 loss: -0.7231869697570801
Batch 64/64 loss: -4.396897315979004
Epoch 173  Train loss: -0.33943331475351374  Val loss: -0.4774805835841857
Saving best model, epoch: 173
Epoch 174
-------------------------------
Batch 1/64 loss: -0.060103416442871094
Batch 2/64 loss: -0.11647510528564453
Batch 3/64 loss: -0.4808688163757324
Batch 4/64 loss: -0.5573763847351074
Batch 5/64 loss: -0.262723445892334
Batch 6/64 loss: -0.4718360900878906
Batch 7/64 loss: -0.4465060234069824
Batch 8/64 loss: -0.41487693786621094
Batch 9/64 loss: 0.023342609405517578
Batch 10/64 loss: -0.4128284454345703
Batch 11/64 loss: -0.5330591201782227
Batch 12/64 loss: -0.42095947265625
Batch 13/64 loss: -0.3692755699157715
Batch 14/64 loss: 0.1355743408203125
Batch 15/64 loss: -0.03491640090942383
Batch 16/64 loss: -0.7764840126037598
Batch 17/64 loss: -0.42015886306762695
Batch 18/64 loss: -0.38819265365600586
Batch 19/64 loss: -0.12282514572143555
Batch 20/64 loss: 0.06032609939575195
Batch 21/64 loss: -0.49823474884033203
Batch 22/64 loss: -0.40419960021972656
Batch 23/64 loss: -0.11794900894165039
Batch 24/64 loss: -0.030858993530273438
Batch 25/64 loss: 0.047154903411865234
Batch 26/64 loss: -0.3721189498901367
Batch 27/64 loss: -0.3584733009338379
Batch 28/64 loss: -0.06281042098999023
Batch 29/64 loss: -0.41123247146606445
Batch 30/64 loss: -0.2541661262512207
Batch 31/64 loss: -0.4283757209777832
Batch 32/64 loss: -0.26430416107177734
Batch 33/64 loss: -0.3191957473754883
Batch 34/64 loss: -0.603370189666748
Batch 35/64 loss: 0.30408525466918945
Batch 36/64 loss: -0.23520278930664062
Batch 37/64 loss: -0.707862377166748
Batch 38/64 loss: -0.4354567527770996
Batch 39/64 loss: -0.2175278663635254
Batch 40/64 loss: -0.0751791000366211
Batch 41/64 loss: 0.0641622543334961
Batch 42/64 loss: -0.32178497314453125
Batch 43/64 loss: -0.15043306350708008
Batch 44/64 loss: -0.5293974876403809
Batch 45/64 loss: -0.21365976333618164
Batch 46/64 loss: -0.5237798690795898
Batch 47/64 loss: -0.6294455528259277
Batch 48/64 loss: -0.5596199035644531
Batch 49/64 loss: -0.5915250778198242
Batch 50/64 loss: 0.08956003189086914
Batch 51/64 loss: -0.47601985931396484
Batch 52/64 loss: -0.39462947845458984
Batch 53/64 loss: -0.7264070510864258
Batch 54/64 loss: -0.5001773834228516
Batch 55/64 loss: -0.3367023468017578
Batch 56/64 loss: -0.10776329040527344
Batch 57/64 loss: -0.22181463241577148
Batch 58/64 loss: -0.4638080596923828
Batch 59/64 loss: -0.42449092864990234
Batch 60/64 loss: -0.28148508071899414
Batch 61/64 loss: -0.16946697235107422
Batch 62/64 loss: -0.47588157653808594
Batch 63/64 loss: -0.8899307250976562
Batch 64/64 loss: -3.3596324920654297
Epoch 174  Train loss: -0.35874080283969056  Val loss: -0.4857890597733435
Saving best model, epoch: 174
Epoch 175
-------------------------------
Batch 1/64 loss: -0.2659144401550293
Batch 2/64 loss: -0.48499488830566406
Batch 3/64 loss: -0.7681188583374023
Batch 4/64 loss: -0.07895517349243164
Batch 5/64 loss: -0.4639549255371094
Batch 6/64 loss: -0.2773246765136719
Batch 7/64 loss: 0.28992795944213867
Batch 8/64 loss: -0.39467620849609375
Batch 9/64 loss: -0.1565704345703125
Batch 10/64 loss: 0.5896353721618652
Batch 11/64 loss: -0.05899667739868164
Batch 12/64 loss: -0.7017498016357422
Batch 13/64 loss: -0.4581179618835449
Batch 14/64 loss: -0.5203256607055664
Batch 15/64 loss: -0.39426422119140625
Batch 16/64 loss: -0.26031970977783203
Batch 17/64 loss: -0.19980430603027344
Batch 18/64 loss: 0.007936477661132812
Batch 19/64 loss: -0.5527596473693848
Batch 20/64 loss: -0.3459763526916504
Batch 21/64 loss: -0.37178707122802734
Batch 22/64 loss: -0.18148326873779297
Batch 23/64 loss: 0.05857276916503906
Batch 24/64 loss: 0.11842870712280273
Batch 25/64 loss: -0.3430361747741699
Batch 26/64 loss: 0.115875244140625
Batch 27/64 loss: -0.39240550994873047
Batch 28/64 loss: -0.25844478607177734
Batch 29/64 loss: -0.18899011611938477
Batch 30/64 loss: -0.3432798385620117
Batch 31/64 loss: -0.21610450744628906
Batch 32/64 loss: -0.5734157562255859
Batch 33/64 loss: -0.705143928527832
Batch 34/64 loss: -0.03983449935913086
Batch 35/64 loss: 0.08777570724487305
Batch 36/64 loss: -0.5433716773986816
Batch 37/64 loss: -0.14520502090454102
Batch 38/64 loss: -0.37634849548339844
Batch 39/64 loss: -0.044480323791503906
Batch 40/64 loss: -0.1470327377319336
Batch 41/64 loss: -0.5080294609069824
Batch 42/64 loss: -0.6760663986206055
Batch 43/64 loss: -0.3307065963745117
Batch 44/64 loss: -0.2617940902709961
Batch 45/64 loss: -0.4818120002746582
Batch 46/64 loss: -0.7084145545959473
Batch 47/64 loss: -0.533052921295166
Batch 48/64 loss: -0.1559429168701172
Batch 49/64 loss: -0.3360142707824707
Batch 50/64 loss: -0.34617090225219727
Batch 51/64 loss: -0.4513216018676758
Batch 52/64 loss: -0.6405138969421387
Batch 53/64 loss: -0.29897069931030273
Batch 54/64 loss: 0.025223255157470703
Batch 55/64 loss: -0.4196333885192871
Batch 56/64 loss: 0.23198699951171875
Batch 57/64 loss: -0.6864142417907715
Batch 58/64 loss: -0.5303010940551758
Batch 59/64 loss: -0.16492700576782227
Batch 60/64 loss: -0.46615076065063477
Batch 61/64 loss: -0.1408543586730957
Batch 62/64 loss: -0.4766550064086914
Batch 63/64 loss: -0.42618608474731445
Batch 64/64 loss: -4.155959129333496
Epoch 175  Train loss: -0.34328983157288795  Val loss: -0.2214726189157807
Epoch 176
-------------------------------
Batch 1/64 loss: 0.14681339263916016
Batch 2/64 loss: -0.17471599578857422
Batch 3/64 loss: -0.2049698829650879
Batch 4/64 loss: -0.6087841987609863
Batch 5/64 loss: 0.2509593963623047
Batch 6/64 loss: -0.137115478515625
Batch 7/64 loss: -0.6002874374389648
Batch 8/64 loss: -0.3135490417480469
Batch 9/64 loss: -0.7357516288757324
Batch 10/64 loss: 0.0012865066528320312
Batch 11/64 loss: -0.45554065704345703
Batch 12/64 loss: 0.15272998809814453
Batch 13/64 loss: -0.3179917335510254
Batch 14/64 loss: -0.5297222137451172
Batch 15/64 loss: -0.11253643035888672
Batch 16/64 loss: -0.45589542388916016
Batch 17/64 loss: -0.16144132614135742
Batch 18/64 loss: -0.4386768341064453
Batch 19/64 loss: -0.4461355209350586
Batch 20/64 loss: 0.14584112167358398
Batch 21/64 loss: -0.3208599090576172
Batch 22/64 loss: -0.2337327003479004
Batch 23/64 loss: -0.1826496124267578
Batch 24/64 loss: -0.48816490173339844
Batch 25/64 loss: -0.2732667922973633
Batch 26/64 loss: -0.6771450042724609
Batch 27/64 loss: -0.2786684036254883
Batch 28/64 loss: -0.1987624168395996
Batch 29/64 loss: -0.2773432731628418
Batch 30/64 loss: -0.3172144889831543
Batch 31/64 loss: 0.15692567825317383
Batch 32/64 loss: -0.4233694076538086
Batch 33/64 loss: -0.3622932434082031
Batch 34/64 loss: -0.17275476455688477
Batch 35/64 loss: -0.10201120376586914
Batch 36/64 loss: -0.36972618103027344
Batch 37/64 loss: -0.16926240921020508
Batch 38/64 loss: -0.1543259620666504
Batch 39/64 loss: -0.1004171371459961
Batch 40/64 loss: -0.1913747787475586
Batch 41/64 loss: -0.3990135192871094
Batch 42/64 loss: -0.16197586059570312
Batch 43/64 loss: -0.3941617012023926
Batch 44/64 loss: -0.16366004943847656
Batch 45/64 loss: -0.40613269805908203
Batch 46/64 loss: -0.3767051696777344
Batch 47/64 loss: -0.3169846534729004
Batch 48/64 loss: -0.24249267578125
Batch 49/64 loss: -0.5777101516723633
Batch 50/64 loss: -0.2796778678894043
Batch 51/64 loss: -0.21320390701293945
Batch 52/64 loss: 0.0366058349609375
Batch 53/64 loss: -0.5907807350158691
Batch 54/64 loss: -0.29472923278808594
Batch 55/64 loss: -0.6050457954406738
Batch 56/64 loss: 0.009726524353027344
Batch 57/64 loss: -0.3547825813293457
Batch 58/64 loss: -0.2031097412109375
Batch 59/64 loss: -0.4290313720703125
Batch 60/64 loss: -0.27342700958251953
Batch 61/64 loss: -0.5210108757019043
Batch 62/64 loss: -0.35997819900512695
Batch 63/64 loss: 0.016550064086914062
Batch 64/64 loss: -3.76163911819458
Epoch 176  Train loss: -0.31457036897247914  Val loss: -0.32727412587588595
Epoch 177
-------------------------------
Batch 1/64 loss: -0.4919443130493164
Batch 2/64 loss: -0.10290384292602539
Batch 3/64 loss: -0.2695188522338867
Batch 4/64 loss: -0.2873964309692383
Batch 5/64 loss: -0.19719791412353516
Batch 6/64 loss: -0.16208696365356445
Batch 7/64 loss: 0.11636161804199219
Batch 8/64 loss: -0.3925337791442871
Batch 9/64 loss: -0.5616359710693359
Batch 10/64 loss: -0.3339252471923828
Batch 11/64 loss: -0.23185062408447266
Batch 12/64 loss: -0.5761160850524902
Batch 13/64 loss: -0.08494997024536133
Batch 14/64 loss: 0.12167596817016602
Batch 15/64 loss: -0.26937341690063477
Batch 16/64 loss: -0.34000730514526367
Batch 17/64 loss: -0.3145937919616699
Batch 18/64 loss: -0.3319272994995117
Batch 19/64 loss: -0.39214086532592773
Batch 20/64 loss: -0.48456764221191406
Batch 21/64 loss: -0.06755495071411133
Batch 22/64 loss: -0.6499738693237305
Batch 23/64 loss: -0.4999270439147949
Batch 24/64 loss: -0.4673027992248535
Batch 25/64 loss: -0.1835641860961914
Batch 26/64 loss: -0.5860462188720703
Batch 27/64 loss: -0.08116626739501953
Batch 28/64 loss: -0.23353099822998047
Batch 29/64 loss: -0.3406658172607422
Batch 30/64 loss: -0.20806074142456055
Batch 31/64 loss: 0.09979724884033203
Batch 32/64 loss: -0.19093036651611328
Batch 33/64 loss: -0.22150421142578125
Batch 34/64 loss: -0.09950065612792969
Batch 35/64 loss: -0.28350400924682617
Batch 36/64 loss: -0.5462284088134766
Batch 37/64 loss: -0.19105148315429688
Batch 38/64 loss: -0.45900917053222656
Batch 39/64 loss: -0.569572925567627
Batch 40/64 loss: -0.47016000747680664
Batch 41/64 loss: -0.41982507705688477
Batch 42/64 loss: 0.011408329010009766
Batch 43/64 loss: -0.5350179672241211
Batch 44/64 loss: -0.6017303466796875
Batch 45/64 loss: -0.3426322937011719
Batch 46/64 loss: -0.5574378967285156
Batch 47/64 loss: -0.09285402297973633
Batch 48/64 loss: 0.4507308006286621
Batch 49/64 loss: -0.17262792587280273
Batch 50/64 loss: 0.06285285949707031
Batch 51/64 loss: -0.6322231292724609
Batch 52/64 loss: -0.47454071044921875
Batch 53/64 loss: -0.31262636184692383
Batch 54/64 loss: -0.5077528953552246
Batch 55/64 loss: -0.4010629653930664
Batch 56/64 loss: -0.43976688385009766
Batch 57/64 loss: -0.6038403511047363
Batch 58/64 loss: -0.4408555030822754
Batch 59/64 loss: -0.7712874412536621
Batch 60/64 loss: -0.6028537750244141
Batch 61/64 loss: -0.05079936981201172
Batch 62/64 loss: -0.351137638092041
Batch 63/64 loss: 0.01396799087524414
Batch 64/64 loss: -3.743269443511963
Epoch 177  Train loss: -0.3516149689169491  Val loss: -0.42827697963649053
Epoch 178
-------------------------------
Batch 1/64 loss: -0.13450288772583008
Batch 2/64 loss: -0.5724520683288574
Batch 3/64 loss: -0.4508633613586426
Batch 4/64 loss: 0.1696758270263672
Batch 5/64 loss: -0.6876430511474609
Batch 6/64 loss: -0.37840795516967773
Batch 7/64 loss: 0.26007509231567383
Batch 8/64 loss: -0.8197689056396484
Batch 9/64 loss: -0.2118535041809082
Batch 10/64 loss: -0.30524730682373047
Batch 11/64 loss: -0.33411312103271484
Batch 12/64 loss: -0.5338668823242188
Batch 13/64 loss: -0.058600425720214844
Batch 14/64 loss: -0.09083986282348633
Batch 15/64 loss: -0.34652233123779297
Batch 16/64 loss: -0.48053884506225586
Batch 17/64 loss: 0.1837148666381836
Batch 18/64 loss: -0.8201017379760742
Batch 19/64 loss: -0.5634260177612305
Batch 20/64 loss: -0.16777849197387695
Batch 21/64 loss: -0.6529459953308105
Batch 22/64 loss: -0.6367855072021484
Batch 23/64 loss: -0.31842613220214844
Batch 24/64 loss: -0.46442317962646484
Batch 25/64 loss: -0.3751044273376465
Batch 26/64 loss: -0.3757948875427246
Batch 27/64 loss: -0.538175106048584
Batch 28/64 loss: -0.655731201171875
Batch 29/64 loss: 0.16285324096679688
Batch 30/64 loss: -0.4883995056152344
Batch 31/64 loss: -0.4200878143310547
Batch 32/64 loss: -0.42328357696533203
Batch 33/64 loss: -0.5920448303222656
Batch 34/64 loss: 0.06255674362182617
Batch 35/64 loss: -0.2075958251953125
Batch 36/64 loss: -0.3321561813354492
Batch 37/64 loss: -0.05514335632324219
Batch 38/64 loss: -0.4054727554321289
Batch 39/64 loss: -0.24177312850952148
Batch 40/64 loss: -0.024844646453857422
Batch 41/64 loss: -0.14365148544311523
Batch 42/64 loss: -0.7067966461181641
Batch 43/64 loss: -0.24315547943115234
Batch 44/64 loss: -0.42908716201782227
Batch 45/64 loss: -0.6659145355224609
Batch 46/64 loss: -0.057570457458496094
Batch 47/64 loss: 0.11181068420410156
Batch 48/64 loss: -0.46596240997314453
Batch 49/64 loss: -0.35535573959350586
Batch 50/64 loss: 0.11696720123291016
Batch 51/64 loss: -0.2651939392089844
Batch 52/64 loss: -0.31778478622436523
Batch 53/64 loss: -0.15969133377075195
Batch 54/64 loss: -0.39974355697631836
Batch 55/64 loss: -0.35193586349487305
Batch 56/64 loss: -0.4650449752807617
Batch 57/64 loss: -0.2858905792236328
Batch 58/64 loss: -0.3476686477661133
Batch 59/64 loss: -0.6084394454956055
Batch 60/64 loss: -0.30834388732910156
Batch 61/64 loss: -0.3022928237915039
Batch 62/64 loss: -0.11832332611083984
Batch 63/64 loss: -0.6446685791015625
Batch 64/64 loss: -3.863800525665283
Epoch 178  Train loss: -0.37078317193424  Val loss: -0.2866593324851334
Epoch 179
-------------------------------
Batch 1/64 loss: -0.4114341735839844
Batch 2/64 loss: -0.40532779693603516
Batch 3/64 loss: -0.34943389892578125
Batch 4/64 loss: 0.3921065330505371
Batch 5/64 loss: -0.04499530792236328
Batch 6/64 loss: -0.36284637451171875
Batch 7/64 loss: -0.31842994689941406
Batch 8/64 loss: -0.21484041213989258
Batch 9/64 loss: -0.1296243667602539
Batch 10/64 loss: -0.049437522888183594
Batch 11/64 loss: -0.47320556640625
Batch 12/64 loss: -0.25922298431396484
Batch 13/64 loss: -0.2118701934814453
Batch 14/64 loss: -0.14467525482177734
Batch 15/64 loss: -0.6397905349731445
Batch 16/64 loss: -0.6364951133728027
Batch 17/64 loss: -0.8363313674926758
Batch 18/64 loss: -0.5850672721862793
Batch 19/64 loss: -0.032002925872802734
Batch 20/64 loss: -0.42565107345581055
Batch 21/64 loss: -0.5774064064025879
Batch 22/64 loss: -0.08133077621459961
Batch 23/64 loss: -0.5058789253234863
Batch 24/64 loss: -0.6420650482177734
Batch 25/64 loss: -0.42473793029785156
Batch 26/64 loss: -0.20879030227661133
Batch 27/64 loss: -0.32470130920410156
Batch 28/64 loss: -0.3568906784057617
Batch 29/64 loss: -0.23256969451904297
Batch 30/64 loss: 0.34772825241088867
Batch 31/64 loss: -0.37042665481567383
Batch 32/64 loss: -0.6468896865844727
Batch 33/64 loss: -0.4812192916870117
Batch 34/64 loss: -0.48757457733154297
Batch 35/64 loss: -0.07063007354736328
Batch 36/64 loss: 0.13262367248535156
Batch 37/64 loss: -0.36690330505371094
Batch 38/64 loss: -0.07979774475097656
Batch 39/64 loss: 0.4644503593444824
Batch 40/64 loss: 0.21899032592773438
Batch 41/64 loss: -0.5757293701171875
Batch 42/64 loss: -0.21450471878051758
Batch 43/64 loss: -0.17651653289794922
Batch 44/64 loss: 0.1539154052734375
Batch 45/64 loss: -0.5180110931396484
Batch 46/64 loss: -0.8816766738891602
Batch 47/64 loss: -0.035977840423583984
Batch 48/64 loss: -0.35743188858032227
Batch 49/64 loss: -0.1422595977783203
Batch 50/64 loss: -0.2208843231201172
Batch 51/64 loss: -0.675051212310791
Batch 52/64 loss: -0.1880021095275879
Batch 53/64 loss: -0.22645139694213867
Batch 54/64 loss: -0.06957626342773438
Batch 55/64 loss: -0.3264799118041992
Batch 56/64 loss: -0.12035799026489258
Batch 57/64 loss: -0.27357912063598633
Batch 58/64 loss: -0.4166994094848633
Batch 59/64 loss: -0.0557560920715332
Batch 60/64 loss: -0.46584558486938477
Batch 61/64 loss: -0.09157180786132812
Batch 62/64 loss: -0.7515993118286133
Batch 63/64 loss: -0.3384838104248047
Batch 64/64 loss: -3.9608216285705566
Epoch 179  Train loss: -0.32583125058342427  Val loss: -0.46801305554576755
Epoch 180
-------------------------------
Batch 1/64 loss: -0.14052248001098633
Batch 2/64 loss: -0.3066539764404297
Batch 3/64 loss: -0.46334123611450195
Batch 4/64 loss: -0.5211372375488281
Batch 5/64 loss: -0.07099056243896484
Batch 6/64 loss: 0.14369773864746094
Batch 7/64 loss: -0.33757495880126953
Batch 8/64 loss: -0.3843226432800293
Batch 9/64 loss: -0.6107378005981445
Batch 10/64 loss: -0.42530012130737305
Batch 11/64 loss: -0.6238327026367188
Batch 12/64 loss: -0.3274970054626465
Batch 13/64 loss: -0.03487873077392578
Batch 14/64 loss: -0.6025323867797852
Batch 15/64 loss: -0.23944997787475586
Batch 16/64 loss: -0.03099346160888672
Batch 17/64 loss: -0.2994217872619629
Batch 18/64 loss: -0.3988981246948242
Batch 19/64 loss: -0.061303138732910156
Batch 20/64 loss: -0.28033876419067383
Batch 21/64 loss: -0.41496706008911133
Batch 22/64 loss: -0.20956039428710938
Batch 23/64 loss: -0.28145551681518555
Batch 24/64 loss: -0.40552759170532227
Batch 25/64 loss: -0.606898307800293
Batch 26/64 loss: -0.42015695571899414
Batch 27/64 loss: -0.09866619110107422
Batch 28/64 loss: -0.2905268669128418
Batch 29/64 loss: -0.39609336853027344
Batch 30/64 loss: -0.48052072525024414
Batch 31/64 loss: -0.5484347343444824
Batch 32/64 loss: -0.8658876419067383
Batch 33/64 loss: -0.459597110748291
Batch 34/64 loss: -0.5315890312194824
Batch 35/64 loss: -0.24370670318603516
Batch 36/64 loss: -0.06909036636352539
Batch 37/64 loss: -0.5175237655639648
Batch 38/64 loss: 0.10496282577514648
Batch 39/64 loss: -0.4112863540649414
Batch 40/64 loss: 0.08289384841918945
Batch 41/64 loss: -0.4784979820251465
Batch 42/64 loss: -0.35370969772338867
Batch 43/64 loss: -0.19109249114990234
Batch 44/64 loss: -0.27848005294799805
Batch 45/64 loss: -0.7082610130310059
Batch 46/64 loss: -0.15295791625976562
Batch 47/64 loss: -0.47899818420410156
Batch 48/64 loss: -0.45403337478637695
Batch 49/64 loss: -0.21701955795288086
Batch 50/64 loss: -0.7325000762939453
Batch 51/64 loss: 0.09283161163330078
Batch 52/64 loss: -0.16146421432495117
Batch 53/64 loss: 0.11736583709716797
Batch 54/64 loss: -0.24962091445922852
Batch 55/64 loss: -0.5843997001647949
Batch 56/64 loss: -0.30301427841186523
Batch 57/64 loss: -0.409207820892334
Batch 58/64 loss: -0.4366927146911621
Batch 59/64 loss: -0.3688364028930664
Batch 60/64 loss: -0.1325225830078125
Batch 61/64 loss: -0.3431520462036133
Batch 62/64 loss: -0.317840576171875
Batch 63/64 loss: -0.6144256591796875
Batch 64/64 loss: -4.409003734588623
Epoch 180  Train loss: -0.37871284671858246  Val loss: -0.4195010391707273
Epoch 181
-------------------------------
Batch 1/64 loss: -0.6283869743347168
Batch 2/64 loss: 0.05240297317504883
Batch 3/64 loss: -0.15473270416259766
Batch 4/64 loss: -0.23052978515625
Batch 5/64 loss: -0.5356941223144531
Batch 6/64 loss: -0.5204610824584961
Batch 7/64 loss: -0.0858926773071289
Batch 8/64 loss: -0.2099475860595703
Batch 9/64 loss: -0.4430980682373047
Batch 10/64 loss: -0.2781815528869629
Batch 11/64 loss: -0.42934274673461914
Batch 12/64 loss: -0.5803465843200684
Batch 13/64 loss: -0.48325443267822266
Batch 14/64 loss: -0.3531684875488281
Batch 15/64 loss: -0.4848346710205078
Batch 16/64 loss: -0.5871801376342773
Batch 17/64 loss: -0.35831212997436523
Batch 18/64 loss: -0.5903539657592773
Batch 19/64 loss: -0.058185577392578125
Batch 20/64 loss: -0.31574058532714844
Batch 21/64 loss: -0.11492443084716797
Batch 22/64 loss: 0.12691688537597656
Batch 23/64 loss: -0.5324888229370117
Batch 24/64 loss: -0.10746526718139648
Batch 25/64 loss: -0.3358163833618164
Batch 26/64 loss: -0.0938711166381836
Batch 27/64 loss: -0.25209808349609375
Batch 28/64 loss: -0.318448543548584
Batch 29/64 loss: -0.4974370002746582
Batch 30/64 loss: -0.5782427787780762
Batch 31/64 loss: -0.21062612533569336
Batch 32/64 loss: -0.47293949127197266
Batch 33/64 loss: -0.30271100997924805
Batch 34/64 loss: 0.10859870910644531
Batch 35/64 loss: -0.3891448974609375
Batch 36/64 loss: 0.24705791473388672
Batch 37/64 loss: -0.5012736320495605
Batch 38/64 loss: -0.16701602935791016
Batch 39/64 loss: -0.14322471618652344
Batch 40/64 loss: -0.39337635040283203
Batch 41/64 loss: -0.03461122512817383
Batch 42/64 loss: -0.5574073791503906
Batch 43/64 loss: -0.25673437118530273
Batch 44/64 loss: -0.2217082977294922
Batch 45/64 loss: -0.4548344612121582
Batch 46/64 loss: -0.6706695556640625
Batch 47/64 loss: -0.45948028564453125
Batch 48/64 loss: -0.7519950866699219
Batch 49/64 loss: -0.21423768997192383
Batch 50/64 loss: -0.4804043769836426
Batch 51/64 loss: -0.16987037658691406
Batch 52/64 loss: -0.3948974609375
Batch 53/64 loss: -0.5223574638366699
Batch 54/64 loss: -0.3568439483642578
Batch 55/64 loss: -0.29444074630737305
Batch 56/64 loss: -0.13666296005249023
Batch 57/64 loss: -0.4286785125732422
Batch 58/64 loss: -0.40416526794433594
Batch 59/64 loss: -0.7238006591796875
Batch 60/64 loss: 0.11077213287353516
Batch 61/64 loss: -0.5627236366271973
Batch 62/64 loss: 0.029841899871826172
Batch 63/64 loss: -0.27945375442504883
Batch 64/64 loss: -4.308232307434082
Epoch 181  Train loss: -0.37129897697299136  Val loss: -0.5519121438776914
Saving best model, epoch: 181
Epoch 182
-------------------------------
Batch 1/64 loss: -0.30594825744628906
Batch 2/64 loss: -0.4242362976074219
Batch 3/64 loss: -0.6862363815307617
Batch 4/64 loss: 0.04746866226196289
Batch 5/64 loss: -0.31435346603393555
Batch 6/64 loss: -0.43670129776000977
Batch 7/64 loss: -0.6164779663085938
Batch 8/64 loss: -0.5051007270812988
Batch 9/64 loss: -0.3308286666870117
Batch 10/64 loss: -0.7558326721191406
Batch 11/64 loss: 0.22527170181274414
Batch 12/64 loss: -0.5663714408874512
Batch 13/64 loss: -0.5402674674987793
Batch 14/64 loss: -0.36002254486083984
Batch 15/64 loss: 0.034311771392822266
Batch 16/64 loss: -0.5120987892150879
Batch 17/64 loss: -0.5219459533691406
Batch 18/64 loss: -0.5125970840454102
Batch 19/64 loss: -0.6525259017944336
Batch 20/64 loss: -0.46585512161254883
Batch 21/64 loss: -0.5259075164794922
Batch 22/64 loss: -0.38205528259277344
Batch 23/64 loss: -0.4879264831542969
Batch 24/64 loss: 0.006545543670654297
Batch 25/64 loss: -0.2856326103210449
Batch 26/64 loss: -0.887423038482666
Batch 27/64 loss: 0.008549690246582031
Batch 28/64 loss: -0.6776866912841797
Batch 29/64 loss: -0.5420541763305664
Batch 30/64 loss: -0.46781396865844727
Batch 31/64 loss: -0.48644542694091797
Batch 32/64 loss: -0.4544839859008789
Batch 33/64 loss: -0.2476191520690918
Batch 34/64 loss: -0.19845151901245117
Batch 35/64 loss: 0.05422544479370117
Batch 36/64 loss: -0.2673969268798828
Batch 37/64 loss: -0.6486101150512695
Batch 38/64 loss: -0.3317899703979492
Batch 39/64 loss: -0.30912113189697266
Batch 40/64 loss: -0.5483684539794922
Batch 41/64 loss: -0.48735475540161133
Batch 42/64 loss: -0.5665750503540039
Batch 43/64 loss: -0.2444138526916504
Batch 44/64 loss: -0.521812915802002
Batch 45/64 loss: -0.3139982223510742
Batch 46/64 loss: -0.49685096740722656
Batch 47/64 loss: 0.43622779846191406
Batch 48/64 loss: -0.6067943572998047
Batch 49/64 loss: -0.1487588882446289
Batch 50/64 loss: -0.2531251907348633
Batch 51/64 loss: -0.5140056610107422
Batch 52/64 loss: -0.5800585746765137
Batch 53/64 loss: -0.17176008224487305
Batch 54/64 loss: -0.23106145858764648
Batch 55/64 loss: -0.033113956451416016
Batch 56/64 loss: -0.21653270721435547
Batch 57/64 loss: -0.07010650634765625
Batch 58/64 loss: -0.20944976806640625
Batch 59/64 loss: -0.3543891906738281
Batch 60/64 loss: -0.12911319732666016
Batch 61/64 loss: -0.5662341117858887
Batch 62/64 loss: -0.30803680419921875
Batch 63/64 loss: -0.7259674072265625
Batch 64/64 loss: -4.191664695739746
Epoch 182  Train loss: -0.4131270277733896  Val loss: -0.5251396152981368
Epoch 183
-------------------------------
Batch 1/64 loss: -0.5117850303649902
Batch 2/64 loss: 0.6005253791809082
Batch 3/64 loss: -0.8623261451721191
Batch 4/64 loss: -0.4938030242919922
Batch 5/64 loss: -0.5912175178527832
Batch 6/64 loss: -0.29324769973754883
Batch 7/64 loss: -0.01566791534423828
Batch 8/64 loss: -0.6834688186645508
Batch 9/64 loss: -0.3064727783203125
Batch 10/64 loss: -0.48575735092163086
Batch 11/64 loss: -0.6072053909301758
Batch 12/64 loss: -0.2801237106323242
Batch 13/64 loss: -0.7751560211181641
Batch 14/64 loss: -0.28918981552124023
Batch 15/64 loss: -0.2752513885498047
Batch 16/64 loss: -0.17131757736206055
Batch 17/64 loss: -0.5940103530883789
Batch 18/64 loss: -0.3406658172607422
Batch 19/64 loss: 0.14224672317504883
Batch 20/64 loss: -0.3932809829711914
Batch 21/64 loss: -0.6515102386474609
Batch 22/64 loss: -0.5592823028564453
Batch 23/64 loss: -0.19657659530639648
Batch 24/64 loss: -0.7786283493041992
Batch 25/64 loss: -0.5531034469604492
Batch 26/64 loss: -0.03916597366333008
Batch 27/64 loss: -0.44133663177490234
Batch 28/64 loss: -0.7305669784545898
Batch 29/64 loss: -0.2117147445678711
Batch 30/64 loss: -0.26838016510009766
Batch 31/64 loss: -0.25847864151000977
Batch 32/64 loss: -0.49661779403686523
Batch 33/64 loss: -0.22421741485595703
Batch 34/64 loss: -0.684443473815918
Batch 35/64 loss: -0.5994305610656738
Batch 36/64 loss: -0.06828022003173828
Batch 37/64 loss: -0.6334409713745117
Batch 38/64 loss: -0.3281369209289551
Batch 39/64 loss: 0.14078521728515625
Batch 40/64 loss: -0.5840539932250977
Batch 41/64 loss: -0.6931838989257812
Batch 42/64 loss: -0.20068740844726562
Batch 43/64 loss: 0.0932321548461914
Batch 44/64 loss: -0.15072154998779297
Batch 45/64 loss: -0.29360008239746094
Batch 46/64 loss: -0.6212315559387207
Batch 47/64 loss: -0.5868587493896484
Batch 48/64 loss: -0.4379863739013672
Batch 49/64 loss: -0.658348560333252
Batch 50/64 loss: -0.42055702209472656
Batch 51/64 loss: -0.5665273666381836
Batch 52/64 loss: -0.6234440803527832
Batch 53/64 loss: -0.7815742492675781
Batch 54/64 loss: -0.20202970504760742
Batch 55/64 loss: -0.28803110122680664
Batch 56/64 loss: -0.5427470207214355
Batch 57/64 loss: -0.26841306686401367
Batch 58/64 loss: -0.07463979721069336
Batch 59/64 loss: -0.8563728332519531
Batch 60/64 loss: -0.3919963836669922
Batch 61/64 loss: -0.10863304138183594
Batch 62/64 loss: -0.23586797714233398
Batch 63/64 loss: -0.4729785919189453
Batch 64/64 loss: -3.9845008850097656
Epoch 183  Train loss: -0.43553457821116726  Val loss: -0.3475370505421432
Epoch 184
-------------------------------
Batch 1/64 loss: -0.418365478515625
Batch 2/64 loss: 0.29047536849975586
Batch 3/64 loss: -0.3503999710083008
Batch 4/64 loss: -0.40089988708496094
Batch 5/64 loss: -0.5791034698486328
Batch 6/64 loss: -0.41631555557250977
Batch 7/64 loss: -0.17316770553588867
Batch 8/64 loss: -0.5370082855224609
Batch 9/64 loss: -0.29776620864868164
Batch 10/64 loss: -0.511256217956543
Batch 11/64 loss: -0.6847262382507324
Batch 12/64 loss: -0.16160869598388672
Batch 13/64 loss: -0.2742452621459961
Batch 14/64 loss: -0.4361906051635742
Batch 15/64 loss: 0.07181787490844727
Batch 16/64 loss: 0.27185630798339844
Batch 17/64 loss: -0.5008449554443359
Batch 18/64 loss: -0.12450313568115234
Batch 19/64 loss: -0.6083488464355469
Batch 20/64 loss: -0.8716526031494141
Batch 21/64 loss: -0.04349660873413086
Batch 22/64 loss: -0.6798624992370605
Batch 23/64 loss: -0.43178224563598633
Batch 24/64 loss: -0.49056005477905273
Batch 25/64 loss: 0.336606502532959
Batch 26/64 loss: -0.3063960075378418
Batch 27/64 loss: -0.6165685653686523
Batch 28/64 loss: -0.21053075790405273
Batch 29/64 loss: -0.6016507148742676
Batch 30/64 loss: -0.7116575241088867
Batch 31/64 loss: -0.20474481582641602
Batch 32/64 loss: -0.12462091445922852
Batch 33/64 loss: -0.2786121368408203
Batch 34/64 loss: -0.05898094177246094
Batch 35/64 loss: -0.5120019912719727
Batch 36/64 loss: -0.21878862380981445
Batch 37/64 loss: -0.4415898323059082
Batch 38/64 loss: -0.15160083770751953
Batch 39/64 loss: -0.48850154876708984
Batch 40/64 loss: 0.034637451171875
Batch 41/64 loss: -0.4663548469543457
Batch 42/64 loss: -0.47130870819091797
Batch 43/64 loss: -0.14257335662841797
Batch 44/64 loss: -0.33855247497558594
Batch 45/64 loss: -0.4953947067260742
Batch 46/64 loss: -0.41579198837280273
Batch 47/64 loss: -0.12433719635009766
Batch 48/64 loss: -0.5410375595092773
Batch 49/64 loss: -0.4733552932739258
Batch 50/64 loss: -0.5208277702331543
Batch 51/64 loss: -0.5040268898010254
Batch 52/64 loss: -0.3252854347229004
Batch 53/64 loss: 0.12204980850219727
Batch 54/64 loss: -0.3026723861694336
Batch 55/64 loss: -0.14126157760620117
Batch 56/64 loss: -0.5312347412109375
Batch 57/64 loss: -0.5592164993286133
Batch 58/64 loss: -0.1413588523864746
Batch 59/64 loss: -0.5878386497497559
Batch 60/64 loss: -0.3118104934692383
Batch 61/64 loss: -0.6756000518798828
Batch 62/64 loss: -0.3644223213195801
Batch 63/64 loss: -0.6281485557556152
Batch 64/64 loss: -4.463493824005127
Epoch 184  Train loss: -0.3953088180691588  Val loss: -0.4423286726384638
Epoch 185
-------------------------------
Batch 1/64 loss: 0.04181623458862305
Batch 2/64 loss: -0.5106163024902344
Batch 3/64 loss: -0.8160572052001953
Batch 4/64 loss: -0.541069507598877
Batch 5/64 loss: 0.4968123435974121
Batch 6/64 loss: -0.5251703262329102
Batch 7/64 loss: -0.36347389221191406
Batch 8/64 loss: -0.6374216079711914
Batch 9/64 loss: -0.07055377960205078
Batch 10/64 loss: 0.14232444763183594
Batch 11/64 loss: -0.3998546600341797
Batch 12/64 loss: -0.3650956153869629
Batch 13/64 loss: -0.4811887741088867
Batch 14/64 loss: -0.7073783874511719
Batch 15/64 loss: -0.4175605773925781
Batch 16/64 loss: -0.43974781036376953
Batch 17/64 loss: -0.33007097244262695
Batch 18/64 loss: -0.4297161102294922
Batch 19/64 loss: -0.5078158378601074
Batch 20/64 loss: -0.28570079803466797
Batch 21/64 loss: -0.784980297088623
Batch 22/64 loss: -0.6111726760864258
Batch 23/64 loss: 0.09862947463989258
Batch 24/64 loss: -0.764735221862793
Batch 25/64 loss: -0.5471401214599609
Batch 26/64 loss: -0.8178796768188477
Batch 27/64 loss: -0.18926715850830078
Batch 28/64 loss: -0.20430755615234375
Batch 29/64 loss: 0.1361074447631836
Batch 30/64 loss: -0.7952337265014648
Batch 31/64 loss: -0.25008487701416016
Batch 32/64 loss: -0.38106822967529297
Batch 33/64 loss: -0.40454864501953125
Batch 34/64 loss: 0.07175207138061523
Batch 35/64 loss: -0.3407902717590332
Batch 36/64 loss: -0.3323092460632324
Batch 37/64 loss: 0.07935476303100586
Batch 38/64 loss: 0.3233451843261719
Batch 39/64 loss: -0.01009511947631836
Batch 40/64 loss: 0.4131464958190918
Batch 41/64 loss: -0.26940011978149414
Batch 42/64 loss: -0.6244807243347168
Batch 43/64 loss: -0.280057430267334
Batch 44/64 loss: -0.21664905548095703
Batch 45/64 loss: -0.6241354942321777
Batch 46/64 loss: -0.5570821762084961
Batch 47/64 loss: -0.18276405334472656
Batch 48/64 loss: -0.48796939849853516
Batch 49/64 loss: -0.18108367919921875
Batch 50/64 loss: -0.28612613677978516
Batch 51/64 loss: -0.4629182815551758
Batch 52/64 loss: -0.012570381164550781
Batch 53/64 loss: -0.5070919990539551
Batch 54/64 loss: -0.5281171798706055
Batch 55/64 loss: 0.032320499420166016
Batch 56/64 loss: -0.22568893432617188
Batch 57/64 loss: -0.06682538986206055
Batch 58/64 loss: -0.5480384826660156
Batch 59/64 loss: -0.4000082015991211
Batch 60/64 loss: -0.6813721656799316
Batch 61/64 loss: -0.540074348449707
Batch 62/64 loss: -0.4558134078979492
Batch 63/64 loss: -0.10939216613769531
Batch 64/64 loss: -4.532352447509766
Epoch 185  Train loss: -0.377622267779182  Val loss: -0.5613943014767572
Saving best model, epoch: 185
Epoch 186
-------------------------------
Batch 1/64 loss: -0.6945853233337402
Batch 2/64 loss: -0.3536796569824219
Batch 3/64 loss: -0.42548227310180664
Batch 4/64 loss: -0.46971940994262695
Batch 5/64 loss: -0.1113739013671875
Batch 6/64 loss: -0.44833803176879883
Batch 7/64 loss: -0.5888762474060059
Batch 8/64 loss: -0.430267333984375
Batch 9/64 loss: -0.5872621536254883
Batch 10/64 loss: -0.8188629150390625
Batch 11/64 loss: -0.28276777267456055
Batch 12/64 loss: -0.660184383392334
Batch 13/64 loss: -0.37218475341796875
Batch 14/64 loss: -0.5012025833129883
Batch 15/64 loss: -0.5915145874023438
Batch 16/64 loss: -0.6859140396118164
Batch 17/64 loss: -0.4211721420288086
Batch 18/64 loss: -0.06177663803100586
Batch 19/64 loss: 0.04014873504638672
Batch 20/64 loss: -0.47353601455688477
Batch 21/64 loss: -0.6408190727233887
Batch 22/64 loss: 0.41302013397216797
Batch 23/64 loss: -0.4930109977722168
Batch 24/64 loss: -0.3786582946777344
Batch 25/64 loss: -0.49152278900146484
Batch 26/64 loss: -0.008677005767822266
Batch 27/64 loss: -0.29419374465942383
Batch 28/64 loss: -0.4814414978027344
Batch 29/64 loss: -0.5554723739624023
Batch 30/64 loss: -0.15334033966064453
Batch 31/64 loss: 0.49930334091186523
Batch 32/64 loss: -0.6879291534423828
Batch 33/64 loss: -0.10566186904907227
Batch 34/64 loss: -0.7072210311889648
Batch 35/64 loss: -0.30008840560913086
Batch 36/64 loss: -0.49790287017822266
Batch 37/64 loss: -0.5516533851623535
Batch 38/64 loss: -0.3422813415527344
Batch 39/64 loss: -0.2701454162597656
Batch 40/64 loss: -0.4465351104736328
Batch 41/64 loss: -0.7111926078796387
Batch 42/64 loss: -0.5795440673828125
Batch 43/64 loss: -0.49527645111083984
Batch 44/64 loss: -0.2674441337585449
Batch 45/64 loss: -0.22609758377075195
Batch 46/64 loss: -0.519801139831543
Batch 47/64 loss: -0.5375938415527344
Batch 48/64 loss: 0.09282445907592773
Batch 49/64 loss: -0.30112266540527344
Batch 50/64 loss: -0.4697737693786621
Batch 51/64 loss: 0.19204473495483398
Batch 52/64 loss: -0.428375244140625
Batch 53/64 loss: 0.19552278518676758
Batch 54/64 loss: -0.585698127746582
Batch 55/64 loss: -0.20542621612548828
Batch 56/64 loss: -0.4006333351135254
Batch 57/64 loss: -0.6235761642456055
Batch 58/64 loss: -0.770655632019043
Batch 59/64 loss: -0.37314319610595703
Batch 60/64 loss: -0.5436544418334961
Batch 61/64 loss: -0.5082006454467773
Batch 62/64 loss: -0.32646942138671875
Batch 63/64 loss: 0.17824554443359375
Batch 64/64 loss: -4.520262241363525
Epoch 186  Train loss: -0.4241258116329418  Val loss: -0.4846980956821507
Epoch 187
-------------------------------
Batch 1/64 loss: -0.26839351654052734
Batch 2/64 loss: -0.5142173767089844
Batch 3/64 loss: -0.5873613357543945
Batch 4/64 loss: -0.4703235626220703
Batch 5/64 loss: -0.5829720497131348
Batch 6/64 loss: -0.5680027008056641
Batch 7/64 loss: -0.3349189758300781
Batch 8/64 loss: -0.8474664688110352
Batch 9/64 loss: -0.4581270217895508
Batch 10/64 loss: -0.5072736740112305
Batch 11/64 loss: -0.4908447265625
Batch 12/64 loss: -0.2897367477416992
Batch 13/64 loss: -0.5996332168579102
Batch 14/64 loss: -0.4310736656188965
Batch 15/64 loss: -0.49060487747192383
Batch 16/64 loss: -0.19523048400878906
Batch 17/64 loss: -0.41443395614624023
Batch 18/64 loss: -0.4880828857421875
Batch 19/64 loss: -0.5455541610717773
Batch 20/64 loss: -0.49863672256469727
Batch 21/64 loss: -0.5505847930908203
Batch 22/64 loss: -0.5458011627197266
Batch 23/64 loss: -0.42115211486816406
Batch 24/64 loss: -0.12057781219482422
Batch 25/64 loss: -0.27956342697143555
Batch 26/64 loss: -0.49270153045654297
Batch 27/64 loss: -0.5906820297241211
Batch 28/64 loss: -0.29428577423095703
Batch 29/64 loss: -0.09776687622070312
Batch 30/64 loss: -0.5200920104980469
Batch 31/64 loss: -0.06177568435668945
Batch 32/64 loss: -0.5321083068847656
Batch 33/64 loss: -0.36899757385253906
Batch 34/64 loss: 0.3135967254638672
Batch 35/64 loss: 0.31020116806030273
Batch 36/64 loss: -0.2762451171875
Batch 37/64 loss: -0.38713550567626953
Batch 38/64 loss: 0.14275217056274414
Batch 39/64 loss: -0.4389967918395996
Batch 40/64 loss: -0.3238983154296875
Batch 41/64 loss: -0.3712143898010254
Batch 42/64 loss: -0.29818058013916016
Batch 43/64 loss: -0.7321085929870605
Batch 44/64 loss: -0.49903297424316406
Batch 45/64 loss: -0.24969196319580078
Batch 46/64 loss: -0.19099044799804688
Batch 47/64 loss: -0.45563650131225586
Batch 48/64 loss: -0.6705226898193359
Batch 49/64 loss: -0.5350131988525391
Batch 50/64 loss: -0.15161418914794922
Batch 51/64 loss: -0.03024768829345703
Batch 52/64 loss: -0.47075891494750977
Batch 53/64 loss: -0.40557384490966797
Batch 54/64 loss: -0.20331239700317383
Batch 55/64 loss: -0.673985481262207
Batch 56/64 loss: -0.31786394119262695
Batch 57/64 loss: -0.28801822662353516
Batch 58/64 loss: -0.2520437240600586
Batch 59/64 loss: -0.3056039810180664
Batch 60/64 loss: 0.05102396011352539
Batch 61/64 loss: -0.24078941345214844
Batch 62/64 loss: -0.012200355529785156
Batch 63/64 loss: -0.5389018058776855
Batch 64/64 loss: -3.925248622894287
Epoch 187  Train loss: -0.4063516972111721  Val loss: -0.40830618737079843
Epoch 188
-------------------------------
Batch 1/64 loss: -0.40317583084106445
Batch 2/64 loss: -0.36732006072998047
Batch 3/64 loss: -0.40616321563720703
Batch 4/64 loss: 0.42212581634521484
Batch 5/64 loss: -0.00739288330078125
Batch 6/64 loss: -0.31719255447387695
Batch 7/64 loss: -0.30332422256469727
Batch 8/64 loss: -0.7570972442626953
Batch 9/64 loss: -0.4481215476989746
Batch 10/64 loss: -0.645721435546875
Batch 11/64 loss: -0.18668651580810547
Batch 12/64 loss: -0.3004746437072754
Batch 13/64 loss: 0.10596513748168945
Batch 14/64 loss: 0.07351350784301758
Batch 15/64 loss: -0.6879549026489258
Batch 16/64 loss: -0.5064268112182617
Batch 17/64 loss: -0.46271610260009766
Batch 18/64 loss: -0.40158939361572266
Batch 19/64 loss: 0.14549589157104492
Batch 20/64 loss: -0.5989723205566406
Batch 21/64 loss: -0.6158952713012695
Batch 22/64 loss: -0.1966238021850586
Batch 23/64 loss: -0.22006845474243164
Batch 24/64 loss: -0.16889142990112305
Batch 25/64 loss: -0.18271684646606445
Batch 26/64 loss: -0.3129849433898926
Batch 27/64 loss: -0.5029144287109375
Batch 28/64 loss: -0.1466660499572754
Batch 29/64 loss: -0.4159579277038574
Batch 30/64 loss: -0.3596992492675781
Batch 31/64 loss: -0.4781761169433594
Batch 32/64 loss: -0.6471290588378906
Batch 33/64 loss: -0.21079730987548828
Batch 34/64 loss: 0.23012447357177734
Batch 35/64 loss: -0.673771858215332
Batch 36/64 loss: -0.5480532646179199
Batch 37/64 loss: -0.29894399642944336
Batch 38/64 loss: -0.10499763488769531
Batch 39/64 loss: -0.6063461303710938
Batch 40/64 loss: -0.6605844497680664
Batch 41/64 loss: -0.5557947158813477
Batch 42/64 loss: -0.6221456527709961
Batch 43/64 loss: -0.6545600891113281
Batch 44/64 loss: -0.8693399429321289
Batch 45/64 loss: -0.1554584503173828
Batch 46/64 loss: -0.6582498550415039
Batch 47/64 loss: 0.36542558670043945
Batch 48/64 loss: -0.5767817497253418
Batch 49/64 loss: -0.4030184745788574
Batch 50/64 loss: -0.08988714218139648
Batch 51/64 loss: -0.6614127159118652
Batch 52/64 loss: -0.22436237335205078
Batch 53/64 loss: -0.20375919342041016
Batch 54/64 loss: -0.37614011764526367
Batch 55/64 loss: -0.2753567695617676
Batch 56/64 loss: -0.5478081703186035
Batch 57/64 loss: -0.3208351135253906
Batch 58/64 loss: -0.47223567962646484
Batch 59/64 loss: 0.1826643943786621
Batch 60/64 loss: -0.018421649932861328
Batch 61/64 loss: -0.56390380859375
Batch 62/64 loss: -0.3280062675476074
Batch 63/64 loss: -0.5472011566162109
Batch 64/64 loss: -3.801455020904541
Epoch 188  Train loss: -0.38591377968881646  Val loss: -0.33666276440177995
Epoch 189
-------------------------------
Batch 1/64 loss: -0.22183895111083984
Batch 2/64 loss: 0.12078142166137695
Batch 3/64 loss: -0.47283220291137695
Batch 4/64 loss: -0.5289831161499023
Batch 5/64 loss: 0.005213737487792969
Batch 6/64 loss: -0.2928352355957031
Batch 7/64 loss: -0.6969308853149414
Batch 8/64 loss: -0.12066841125488281
Batch 9/64 loss: 0.26668500900268555
Batch 10/64 loss: -0.7450666427612305
Batch 11/64 loss: -0.27880382537841797
Batch 12/64 loss: -0.7006473541259766
Batch 13/64 loss: -0.6120700836181641
Batch 14/64 loss: 0.12545156478881836
Batch 15/64 loss: -0.5949859619140625
Batch 16/64 loss: -0.12737560272216797
Batch 17/64 loss: -0.5341649055480957
Batch 18/64 loss: -0.6296396255493164
Batch 19/64 loss: 0.018028736114501953
Batch 20/64 loss: -0.6611132621765137
Batch 21/64 loss: -0.03666830062866211
Batch 22/64 loss: -0.6236562728881836
Batch 23/64 loss: -0.7206149101257324
Batch 24/64 loss: -0.004845142364501953
Batch 25/64 loss: -0.3907008171081543
Batch 26/64 loss: -0.734644889831543
Batch 27/64 loss: -0.20020771026611328
Batch 28/64 loss: -0.04486083984375
Batch 29/64 loss: -0.4990205764770508
Batch 30/64 loss: -0.6629533767700195
Batch 31/64 loss: -0.523106575012207
Batch 32/64 loss: -0.5136570930480957
Batch 33/64 loss: -0.2224431037902832
Batch 34/64 loss: -0.22247552871704102
Batch 35/64 loss: -0.26595020294189453
Batch 36/64 loss: 0.23652172088623047
Batch 37/64 loss: 0.06287670135498047
Batch 38/64 loss: -0.3662252426147461
Batch 39/64 loss: -0.34587526321411133
Batch 40/64 loss: -0.362760066986084
Batch 41/64 loss: -0.24232244491577148
Batch 42/64 loss: 0.009681224822998047
Batch 43/64 loss: -0.29522037506103516
Batch 44/64 loss: 0.05132913589477539
Batch 45/64 loss: -0.26195812225341797
Batch 46/64 loss: -0.29889965057373047
Batch 47/64 loss: -0.28076696395874023
Batch 48/64 loss: -0.23790788650512695
Batch 49/64 loss: -0.49897146224975586
Batch 50/64 loss: -0.47388172149658203
Batch 51/64 loss: -0.28939199447631836
Batch 52/64 loss: -0.6013526916503906
Batch 53/64 loss: -0.3360576629638672
Batch 54/64 loss: -0.1873645782470703
Batch 55/64 loss: -0.35961055755615234
Batch 56/64 loss: -0.2602882385253906
Batch 57/64 loss: -0.6695866584777832
Batch 58/64 loss: -0.5655703544616699
Batch 59/64 loss: -0.18469858169555664
Batch 60/64 loss: -0.07824420928955078
Batch 61/64 loss: -0.25609350204467773
Batch 62/64 loss: -0.4908437728881836
Batch 63/64 loss: -0.3956794738769531
Batch 64/64 loss: -4.06110143661499
Epoch 189  Train loss: -0.36662885815489527  Val loss: -0.5251426303509584
Epoch 190
-------------------------------
Batch 1/64 loss: -0.3609175682067871
Batch 2/64 loss: -0.519498348236084
Batch 3/64 loss: 0.050989627838134766
Batch 4/64 loss: -0.13667011260986328
Batch 5/64 loss: -0.14098358154296875
Batch 6/64 loss: -0.29163026809692383
Batch 7/64 loss: -0.3009481430053711
Batch 8/64 loss: -0.4747791290283203
Batch 9/64 loss: -0.17589092254638672
Batch 10/64 loss: -0.39847373962402344
Batch 11/64 loss: -0.5468940734863281
Batch 12/64 loss: -0.595646858215332
Batch 13/64 loss: -0.5626497268676758
Batch 14/64 loss: -0.34673357009887695
Batch 15/64 loss: -0.2947072982788086
Batch 16/64 loss: -0.6559762954711914
Batch 17/64 loss: -0.5361533164978027
Batch 18/64 loss: -0.5286126136779785
Batch 19/64 loss: -0.0733647346496582
Batch 20/64 loss: -0.30586957931518555
Batch 21/64 loss: 0.23489093780517578
Batch 22/64 loss: -0.3373861312866211
Batch 23/64 loss: -0.024679183959960938
Batch 24/64 loss: -0.34790992736816406
Batch 25/64 loss: -0.42509889602661133
Batch 26/64 loss: -0.467348575592041
Batch 27/64 loss: -0.05965137481689453
Batch 28/64 loss: -0.3010563850402832
Batch 29/64 loss: -0.32703447341918945
Batch 30/64 loss: -0.41500234603881836
Batch 31/64 loss: -0.46781158447265625
Batch 32/64 loss: -0.6930074691772461
Batch 33/64 loss: -0.17229652404785156
Batch 34/64 loss: -0.6324625015258789
Batch 35/64 loss: -0.359130859375
Batch 36/64 loss: -0.2123250961303711
Batch 37/64 loss: -0.0712747573852539
Batch 38/64 loss: -0.2342829704284668
Batch 39/64 loss: -0.4149436950683594
Batch 40/64 loss: -0.7211112976074219
Batch 41/64 loss: -0.6187171936035156
Batch 42/64 loss: -0.5527167320251465
Batch 43/64 loss: -0.6801824569702148
Batch 44/64 loss: -0.7433195114135742
Batch 45/64 loss: -0.4394359588623047
Batch 46/64 loss: -0.5383920669555664
Batch 47/64 loss: -0.4916362762451172
Batch 48/64 loss: -0.04867124557495117
Batch 49/64 loss: 0.12775611877441406
Batch 50/64 loss: -0.3655571937561035
Batch 51/64 loss: -0.15150117874145508
Batch 52/64 loss: -0.2440638542175293
Batch 53/64 loss: -0.5398879051208496
Batch 54/64 loss: -0.6153421401977539
Batch 55/64 loss: -0.15918827056884766
Batch 56/64 loss: -0.7620511054992676
Batch 57/64 loss: -0.3869791030883789
Batch 58/64 loss: -0.4800987243652344
Batch 59/64 loss: -0.17787742614746094
Batch 60/64 loss: -0.07305622100830078
Batch 61/64 loss: -0.17125463485717773
Batch 62/64 loss: -0.1712665557861328
Batch 63/64 loss: 0.08735942840576172
Batch 64/64 loss: -4.259260177612305
Epoch 190  Train loss: -0.3927036659390319  Val loss: -0.44529429170274243
Epoch 191
-------------------------------
Batch 1/64 loss: -0.21285772323608398
Batch 2/64 loss: -0.0319218635559082
Batch 3/64 loss: -0.3270883560180664
Batch 4/64 loss: -0.029449939727783203
Batch 5/64 loss: -0.20381832122802734
Batch 6/64 loss: -0.6054272651672363
Batch 7/64 loss: -0.1488327980041504
Batch 8/64 loss: -0.6638040542602539
Batch 9/64 loss: -0.38860559463500977
Batch 10/64 loss: -0.38028860092163086
Batch 11/64 loss: -0.3444032669067383
Batch 12/64 loss: -0.7572517395019531
Batch 13/64 loss: -0.5513725280761719
Batch 14/64 loss: -0.36371755599975586
Batch 15/64 loss: 0.13721752166748047
Batch 16/64 loss: -0.436765193939209
Batch 17/64 loss: -0.6798439025878906
Batch 18/64 loss: -0.6579146385192871
Batch 19/64 loss: -0.3178224563598633
Batch 20/64 loss: -0.5839905738830566
Batch 21/64 loss: -0.25005674362182617
Batch 22/64 loss: -0.18967342376708984
Batch 23/64 loss: -0.6991786956787109
Batch 24/64 loss: 0.39518308639526367
Batch 25/64 loss: -0.3509044647216797
Batch 26/64 loss: -0.5271854400634766
Batch 27/64 loss: -0.9075603485107422
Batch 28/64 loss: -0.727536678314209
Batch 29/64 loss: -0.6782569885253906
Batch 30/64 loss: -0.08855533599853516
Batch 31/64 loss: -0.2604646682739258
Batch 32/64 loss: -0.34145402908325195
Batch 33/64 loss: -0.664980411529541
Batch 34/64 loss: 0.1662898063659668
Batch 35/64 loss: -0.3370065689086914
Batch 36/64 loss: -0.2931194305419922
Batch 37/64 loss: -0.4739956855773926
Batch 38/64 loss: 0.6537842750549316
Batch 39/64 loss: -0.5538864135742188
Batch 40/64 loss: -0.3200387954711914
Batch 41/64 loss: -0.12137603759765625
Batch 42/64 loss: -0.6423635482788086
Batch 43/64 loss: -0.6336240768432617
Batch 44/64 loss: -0.40747880935668945
Batch 45/64 loss: -0.20347976684570312
Batch 46/64 loss: -0.03482532501220703
Batch 47/64 loss: -0.5374660491943359
Batch 48/64 loss: 0.045565128326416016
Batch 49/64 loss: -0.6329984664916992
Batch 50/64 loss: -0.6869478225708008
Batch 51/64 loss: -0.19031095504760742
Batch 52/64 loss: -0.2133784294128418
Batch 53/64 loss: -0.4580955505371094
Batch 54/64 loss: -0.4775576591491699
Batch 55/64 loss: -0.45812416076660156
Batch 56/64 loss: -0.4388151168823242
Batch 57/64 loss: -0.30818605422973633
Batch 58/64 loss: -0.3643307685852051
Batch 59/64 loss: -0.02287769317626953
Batch 60/64 loss: -0.28217649459838867
Batch 61/64 loss: -0.3633236885070801
Batch 62/64 loss: -0.40375232696533203
Batch 63/64 loss: 0.12858295440673828
Batch 64/64 loss: -4.119458198547363
Epoch 191  Train loss: -0.3889174928852156  Val loss: -0.5508691060174372
Epoch 192
-------------------------------
Batch 1/64 loss: -0.6685185432434082
Batch 2/64 loss: -0.67974853515625
Batch 3/64 loss: -0.46701717376708984
Batch 4/64 loss: -0.6472640037536621
Batch 5/64 loss: -0.20716190338134766
Batch 6/64 loss: -0.09340524673461914
Batch 7/64 loss: -0.3715500831604004
Batch 8/64 loss: 0.11191463470458984
Batch 9/64 loss: -0.26525402069091797
Batch 10/64 loss: -0.33944177627563477
Batch 11/64 loss: -0.21530723571777344
Batch 12/64 loss: -0.43195533752441406
Batch 13/64 loss: -0.3674635887145996
Batch 14/64 loss: -0.30907773971557617
Batch 15/64 loss: -0.8079605102539062
Batch 16/64 loss: -0.30402517318725586
Batch 17/64 loss: -0.5738487243652344
Batch 18/64 loss: -0.7931199073791504
Batch 19/64 loss: -0.5912470817565918
Batch 20/64 loss: -0.5911979675292969
Batch 21/64 loss: -0.26409101486206055
Batch 22/64 loss: -0.7178354263305664
Batch 23/64 loss: -0.37099552154541016
Batch 24/64 loss: -0.6860866546630859
Batch 25/64 loss: -0.6778326034545898
Batch 26/64 loss: -0.3064689636230469
Batch 27/64 loss: -0.5109739303588867
Batch 28/64 loss: -0.3094305992126465
Batch 29/64 loss: -0.21182632446289062
Batch 30/64 loss: -0.7799978256225586
Batch 31/64 loss: -0.004836082458496094
Batch 32/64 loss: -0.11825180053710938
Batch 33/64 loss: -0.5423712730407715
Batch 34/64 loss: -0.0869593620300293
Batch 35/64 loss: 0.08954477310180664
Batch 36/64 loss: -0.5377893447875977
Batch 37/64 loss: -0.47649669647216797
Batch 38/64 loss: -0.4298267364501953
Batch 39/64 loss: -0.24410247802734375
Batch 40/64 loss: -0.34537839889526367
Batch 41/64 loss: -0.4100027084350586
Batch 42/64 loss: -0.29671382904052734
Batch 43/64 loss: -0.08251714706420898
Batch 44/64 loss: 0.41498279571533203
Batch 45/64 loss: 0.004572868347167969
Batch 46/64 loss: -0.4501643180847168
Batch 47/64 loss: -0.26714372634887695
Batch 48/64 loss: -0.2481517791748047
Batch 49/64 loss: -0.636723518371582
Batch 50/64 loss: -0.17608165740966797
Batch 51/64 loss: -0.5580906867980957
Batch 52/64 loss: -0.21964406967163086
Batch 53/64 loss: -0.24983978271484375
Batch 54/64 loss: -0.790003776550293
Batch 55/64 loss: -0.6313953399658203
Batch 56/64 loss: -0.20998573303222656
Batch 57/64 loss: -0.5510244369506836
Batch 58/64 loss: -0.5418825149536133
Batch 59/64 loss: -0.12175607681274414
Batch 60/64 loss: -0.5885610580444336
Batch 61/64 loss: -0.7097177505493164
Batch 62/64 loss: -0.13833856582641602
Batch 63/64 loss: 0.3005228042602539
Batch 64/64 loss: -3.741276264190674
Epoch 192  Train loss: -0.4095415433247884  Val loss: -0.3800885701916881
Epoch 193
-------------------------------
Batch 1/64 loss: 0.111053466796875
Batch 2/64 loss: 0.46132659912109375
Batch 3/64 loss: -0.32215023040771484
Batch 4/64 loss: -0.06763744354248047
Batch 5/64 loss: -0.3777151107788086
Batch 6/64 loss: -0.15171527862548828
Batch 7/64 loss: -0.5381007194519043
Batch 8/64 loss: 0.10917139053344727
Batch 9/64 loss: 0.6692538261413574
Batch 10/64 loss: -0.5497112274169922
Batch 11/64 loss: -0.5993871688842773
Batch 12/64 loss: 0.2189044952392578
Batch 13/64 loss: -0.21367645263671875
Batch 14/64 loss: -0.5084829330444336
Batch 15/64 loss: 0.1579427719116211
Batch 16/64 loss: -0.2453904151916504
Batch 17/64 loss: -0.3820004463195801
Batch 18/64 loss: 0.28679561614990234
Batch 19/64 loss: -0.1746973991394043
Batch 20/64 loss: -0.5478668212890625
Batch 21/64 loss: 0.004723072052001953
Batch 22/64 loss: -0.15335464477539062
Batch 23/64 loss: -0.5801944732666016
Batch 24/64 loss: -0.05145263671875
Batch 25/64 loss: -0.3078188896179199
Batch 26/64 loss: -0.05981922149658203
Batch 27/64 loss: -0.32244110107421875
Batch 28/64 loss: -0.553253173828125
Batch 29/64 loss: -0.38086652755737305
Batch 30/64 loss: -0.5439958572387695
Batch 31/64 loss: -0.023987293243408203
Batch 32/64 loss: 0.13684558868408203
Batch 33/64 loss: -0.3546462059020996
Batch 34/64 loss: 0.030364513397216797
Batch 35/64 loss: -0.07201671600341797
Batch 36/64 loss: -0.32512903213500977
Batch 37/64 loss: -0.613987922668457
Batch 38/64 loss: -0.2738609313964844
Batch 39/64 loss: -0.08877372741699219
Batch 40/64 loss: -0.47860097885131836
Batch 41/64 loss: -0.6012811660766602
Batch 42/64 loss: -0.24030399322509766
Batch 43/64 loss: -0.7120962142944336
Batch 44/64 loss: -0.12531423568725586
Batch 45/64 loss: -0.5316619873046875
Batch 46/64 loss: -0.4632701873779297
Batch 47/64 loss: -0.25586795806884766
Batch 48/64 loss: -0.10015344619750977
Batch 49/64 loss: -0.5252828598022461
Batch 50/64 loss: -0.473783016204834
Batch 51/64 loss: -0.1889033317565918
Batch 52/64 loss: -0.12093877792358398
Batch 53/64 loss: -0.599524974822998
Batch 54/64 loss: -0.42679452896118164
Batch 55/64 loss: -0.5845017433166504
Batch 56/64 loss: -0.3923606872558594
Batch 57/64 loss: -0.2861614227294922
Batch 58/64 loss: -0.40537118911743164
Batch 59/64 loss: -0.29449987411499023
Batch 60/64 loss: -0.5342340469360352
Batch 61/64 loss: -0.1869206428527832
Batch 62/64 loss: -0.2419896125793457
Batch 63/64 loss: -0.5388727188110352
Batch 64/64 loss: -3.86403226852417
Epoch 193  Train loss: -0.3043837248110304  Val loss: -0.4106514003268632
Epoch 194
-------------------------------
Batch 1/64 loss: -0.5197544097900391
Batch 2/64 loss: -0.621638298034668
Batch 3/64 loss: -0.0389552116394043
Batch 4/64 loss: -0.49388647079467773
Batch 5/64 loss: -0.5394177436828613
Batch 6/64 loss: -0.5248432159423828
Batch 7/64 loss: -0.08458280563354492
Batch 8/64 loss: -0.8460693359375
Batch 9/64 loss: -0.6584434509277344
Batch 10/64 loss: -0.4024825096130371
Batch 11/64 loss: -0.7472877502441406
Batch 12/64 loss: 0.2306051254272461
Batch 13/64 loss: -0.6960077285766602
Batch 14/64 loss: -0.11587285995483398
Batch 15/64 loss: -0.29808616638183594
Batch 16/64 loss: 0.20843791961669922
Batch 17/64 loss: 0.003368854522705078
Batch 18/64 loss: -0.5863423347473145
Batch 19/64 loss: -0.09531354904174805
Batch 20/64 loss: -0.4662437438964844
Batch 21/64 loss: -0.4740266799926758
Batch 22/64 loss: -0.22003507614135742
Batch 23/64 loss: -0.37598609924316406
Batch 24/64 loss: -0.5804963111877441
Batch 25/64 loss: -0.12639570236206055
Batch 26/64 loss: -0.49079227447509766
Batch 27/64 loss: -0.7447729110717773
Batch 28/64 loss: -0.24202775955200195
Batch 29/64 loss: -0.41090965270996094
Batch 30/64 loss: -0.4435291290283203
Batch 31/64 loss: 0.43160438537597656
Batch 32/64 loss: -0.5311031341552734
Batch 33/64 loss: -0.11603784561157227
Batch 34/64 loss: -0.3785839080810547
Batch 35/64 loss: -0.11628913879394531
Batch 36/64 loss: -0.4268965721130371
Batch 37/64 loss: 0.481290340423584
Batch 38/64 loss: -0.45145702362060547
Batch 39/64 loss: -0.6227335929870605
Batch 40/64 loss: -0.7774372100830078
Batch 41/64 loss: -0.29353952407836914
Batch 42/64 loss: -0.4819822311401367
Batch 43/64 loss: -0.4025759696960449
Batch 44/64 loss: -0.15282487869262695
Batch 45/64 loss: -0.09032154083251953
Batch 46/64 loss: -0.43173980712890625
Batch 47/64 loss: -0.5423541069030762
Batch 48/64 loss: -0.4306955337524414
Batch 49/64 loss: -0.27727174758911133
Batch 50/64 loss: 0.041167259216308594
Batch 51/64 loss: -0.09100723266601562
Batch 52/64 loss: -0.058574676513671875
Batch 53/64 loss: -0.5494747161865234
Batch 54/64 loss: -0.5464577674865723
Batch 55/64 loss: -0.33034753799438477
Batch 56/64 loss: -0.363616943359375
Batch 57/64 loss: -0.2242732048034668
Batch 58/64 loss: -0.5194916725158691
Batch 59/64 loss: 0.29250288009643555
Batch 60/64 loss: -0.26827383041381836
Batch 61/64 loss: -0.29427433013916016
Batch 62/64 loss: -0.4436206817626953
Batch 63/64 loss: 0.19658279418945312
Batch 64/64 loss: -4.085748672485352
Epoch 194  Train loss: -0.36448952917959176  Val loss: -0.4028747532375899
Epoch 195
-------------------------------
Batch 1/64 loss: -0.6382293701171875
Batch 2/64 loss: -0.505950927734375
Batch 3/64 loss: -0.7294540405273438
Batch 4/64 loss: -0.5378384590148926
Batch 5/64 loss: -0.5112910270690918
Batch 6/64 loss: -0.07431554794311523
Batch 7/64 loss: -0.7632293701171875
Batch 8/64 loss: -0.26343441009521484
Batch 9/64 loss: -0.624293327331543
Batch 10/64 loss: -0.17277956008911133
Batch 11/64 loss: 0.07634258270263672
Batch 12/64 loss: -0.587468147277832
Batch 13/64 loss: -0.6767177581787109
Batch 14/64 loss: -0.32115936279296875
Batch 15/64 loss: -0.1068425178527832
Batch 16/64 loss: -0.07045507431030273
Batch 17/64 loss: -0.6601052284240723
Batch 18/64 loss: -0.20761394500732422
Batch 19/64 loss: -0.039136409759521484
Batch 20/64 loss: -0.3281707763671875
Batch 21/64 loss: 0.45539331436157227
Batch 22/64 loss: -0.014858722686767578
Batch 23/64 loss: -0.1442422866821289
Batch 24/64 loss: -0.422943115234375
Batch 25/64 loss: -0.14534997940063477
Batch 26/64 loss: -0.3834681510925293
Batch 27/64 loss: -0.21913671493530273
Batch 28/64 loss: -0.848238468170166
Batch 29/64 loss: -0.550529956817627
Batch 30/64 loss: -0.34206104278564453
Batch 31/64 loss: 0.015160560607910156
Batch 32/64 loss: -0.14201974868774414
Batch 33/64 loss: 0.12407779693603516
Batch 34/64 loss: -0.36091089248657227
Batch 35/64 loss: -0.5832538604736328
Batch 36/64 loss: -0.4113492965698242
Batch 37/64 loss: -0.11820793151855469
Batch 38/64 loss: -0.5201830863952637
Batch 39/64 loss: -0.5918245315551758
Batch 40/64 loss: -0.6659278869628906
Batch 41/64 loss: -0.5187606811523438
Batch 42/64 loss: -0.0653071403503418
Batch 43/64 loss: -0.45648193359375
Batch 44/64 loss: 0.000110626220703125
Batch 45/64 loss: -0.3635263442993164
Batch 46/64 loss: -0.27536487579345703
Batch 47/64 loss: -0.39728784561157227
Batch 48/64 loss: -0.661323070526123
Batch 49/64 loss: -0.5158705711364746
Batch 50/64 loss: -0.15941715240478516
Batch 51/64 loss: -0.2849273681640625
Batch 52/64 loss: 0.33431529998779297
Batch 53/64 loss: -0.3887672424316406
Batch 54/64 loss: -0.6562037467956543
Batch 55/64 loss: -0.6399693489074707
Batch 56/64 loss: -0.6705760955810547
Batch 57/64 loss: -0.07145214080810547
Batch 58/64 loss: -0.5637869834899902
Batch 59/64 loss: -0.23363924026489258
Batch 60/64 loss: -0.0713491439819336
Batch 61/64 loss: -0.4828805923461914
Batch 62/64 loss: -0.35405397415161133
Batch 63/64 loss: -0.4977998733520508
Batch 64/64 loss: -4.411074638366699
Epoch 195  Train loss: -0.3908179152245615  Val loss: -0.4527656843572138
Epoch 196
-------------------------------
Batch 1/64 loss: -0.5567746162414551
Batch 2/64 loss: -0.336637020111084
Batch 3/64 loss: -0.6037111282348633
Batch 4/64 loss: -0.6057949066162109
Batch 5/64 loss: -0.6272974014282227
Batch 6/64 loss: -0.17985153198242188
Batch 7/64 loss: -0.30826711654663086
Batch 8/64 loss: -0.28513288497924805
Batch 9/64 loss: 0.6623916625976562
Batch 10/64 loss: -0.24376296997070312
Batch 11/64 loss: -0.4049057960510254
Batch 12/64 loss: -0.607823371887207
Batch 13/64 loss: -0.6610374450683594
Batch 14/64 loss: -0.7208356857299805
Batch 15/64 loss: -0.40056276321411133
Batch 16/64 loss: -0.1405954360961914
Batch 17/64 loss: -0.44979429244995117
Batch 18/64 loss: -0.3453683853149414
Batch 19/64 loss: -0.43995189666748047
Batch 20/64 loss: -0.1898646354675293
Batch 21/64 loss: -0.6010403633117676
Batch 22/64 loss: -0.31476593017578125
Batch 23/64 loss: -0.4472541809082031
Batch 24/64 loss: -0.3210477828979492
Batch 25/64 loss: -0.6119632720947266
Batch 26/64 loss: -0.22961854934692383
Batch 27/64 loss: -0.7010526657104492
Batch 28/64 loss: -0.2280268669128418
Batch 29/64 loss: -0.5510568618774414
Batch 30/64 loss: -0.6082906723022461
Batch 31/64 loss: -0.3710899353027344
Batch 32/64 loss: -0.39140892028808594
Batch 33/64 loss: -0.675112247467041
Batch 34/64 loss: -0.1660165786743164
Batch 35/64 loss: -0.22664451599121094
Batch 36/64 loss: -0.4828939437866211
Batch 37/64 loss: -0.6497287750244141
Batch 38/64 loss: -0.03299903869628906
Batch 39/64 loss: -0.6142668724060059
Batch 40/64 loss: -0.2833280563354492
Batch 41/64 loss: -0.7073917388916016
Batch 42/64 loss: -0.0768122673034668
Batch 43/64 loss: -0.6719207763671875
Batch 44/64 loss: -0.37146711349487305
Batch 45/64 loss: -0.6630620956420898
Batch 46/64 loss: 0.07037544250488281
Batch 47/64 loss: -0.5404853820800781
Batch 48/64 loss: -0.6160917282104492
Batch 49/64 loss: -0.7045373916625977
Batch 50/64 loss: -0.5266227722167969
Batch 51/64 loss: -0.7349257469177246
Batch 52/64 loss: -0.6403017044067383
Batch 53/64 loss: -0.5740933418273926
Batch 54/64 loss: -0.4039583206176758
Batch 55/64 loss: -0.7310285568237305
Batch 56/64 loss: -0.41367149353027344
Batch 57/64 loss: -0.6145944595336914
Batch 58/64 loss: -0.5447216033935547
Batch 59/64 loss: -0.6744422912597656
Batch 60/64 loss: -0.39374351501464844
Batch 61/64 loss: -0.06965970993041992
Batch 62/64 loss: 0.017357826232910156
Batch 63/64 loss: -0.25260114669799805
Batch 64/64 loss: -4.495380878448486
Epoch 196  Train loss: -0.4731469677943809  Val loss: -0.5116508326579615
Epoch 197
-------------------------------
Batch 1/64 loss: -0.23497867584228516
Batch 2/64 loss: -0.05395364761352539
Batch 3/64 loss: -0.34198522567749023
Batch 4/64 loss: -0.41770315170288086
Batch 5/64 loss: -0.33852243423461914
Batch 6/64 loss: -0.3856163024902344
Batch 7/64 loss: -0.49697303771972656
Batch 8/64 loss: -0.7733163833618164
Batch 9/64 loss: -0.47516584396362305
Batch 10/64 loss: -0.4275836944580078
Batch 11/64 loss: -0.12680387496948242
Batch 12/64 loss: -0.5431351661682129
Batch 13/64 loss: -0.5457949638366699
Batch 14/64 loss: -0.19570112228393555
Batch 15/64 loss: -0.6671924591064453
Batch 16/64 loss: -0.46192121505737305
Batch 17/64 loss: -0.5088601112365723
Batch 18/64 loss: -0.7164697647094727
Batch 19/64 loss: -0.33292484283447266
Batch 20/64 loss: -0.6056604385375977
Batch 21/64 loss: -0.28519630432128906
Batch 22/64 loss: -0.49137163162231445
Batch 23/64 loss: -0.8869748115539551
Batch 24/64 loss: -0.6581211090087891
Batch 25/64 loss: -0.771723747253418
Batch 26/64 loss: -0.4220004081726074
Batch 27/64 loss: -0.3665900230407715
Batch 28/64 loss: -0.32753467559814453
Batch 29/64 loss: -0.43096923828125
Batch 30/64 loss: -0.3226628303527832
Batch 31/64 loss: -0.44262170791625977
Batch 32/64 loss: -0.5272483825683594
Batch 33/64 loss: -0.5252456665039062
Batch 34/64 loss: -0.2954573631286621
Batch 35/64 loss: -0.7909283638000488
Batch 36/64 loss: -0.3784523010253906
Batch 37/64 loss: -0.6009812355041504
Batch 38/64 loss: -0.3097681999206543
Batch 39/64 loss: -0.26212215423583984
Batch 40/64 loss: -0.5100469589233398
Batch 41/64 loss: -0.3533048629760742
Batch 42/64 loss: -0.9065485000610352
Batch 43/64 loss: -0.19094085693359375
Batch 44/64 loss: -0.6170611381530762
Batch 45/64 loss: -0.19658422470092773
Batch 46/64 loss: -0.4766077995300293
Batch 47/64 loss: -0.2001643180847168
Batch 48/64 loss: -0.5126118659973145
Batch 49/64 loss: -0.2991147041320801
Batch 50/64 loss: -0.0120697021484375
Batch 51/64 loss: -0.3829936981201172
Batch 52/64 loss: -0.7243614196777344
Batch 53/64 loss: -0.7440128326416016
Batch 54/64 loss: 0.36277008056640625
Batch 55/64 loss: -0.029989242553710938
Batch 56/64 loss: -0.16148090362548828
Batch 57/64 loss: -0.22220563888549805
Batch 58/64 loss: -0.4135732650756836
Batch 59/64 loss: 0.0339655876159668
Batch 60/64 loss: -0.27857255935668945
Batch 61/64 loss: -0.4070925712585449
Batch 62/64 loss: -0.5950937271118164
Batch 63/64 loss: -0.39977121353149414
Batch 64/64 loss: -4.271426200866699
Epoch 197  Train loss: -0.45783950581270105  Val loss: -0.4104716507429929
Epoch 198
-------------------------------
Batch 1/64 loss: -0.5588045120239258
Batch 2/64 loss: -0.33931446075439453
Batch 3/64 loss: -0.45761775970458984
Batch 4/64 loss: -0.6479578018188477
Batch 5/64 loss: 0.33578062057495117
Batch 6/64 loss: -0.46476125717163086
Batch 7/64 loss: -0.24619340896606445
Batch 8/64 loss: -0.1910533905029297
Batch 9/64 loss: -0.8682279586791992
Batch 10/64 loss: -0.4429354667663574
Batch 11/64 loss: -0.32325077056884766
Batch 12/64 loss: -0.1830763816833496
Batch 13/64 loss: -0.4100532531738281
Batch 14/64 loss: -0.6331844329833984
Batch 15/64 loss: -0.25684213638305664
Batch 16/64 loss: -0.5337915420532227
Batch 17/64 loss: -0.48406171798706055
Batch 18/64 loss: -0.6122350692749023
Batch 19/64 loss: -0.6382460594177246
Batch 20/64 loss: -0.5781378746032715
Batch 21/64 loss: -0.2824077606201172
Batch 22/64 loss: -0.26320838928222656
Batch 23/64 loss: -0.6350822448730469
Batch 24/64 loss: -0.23146915435791016
Batch 25/64 loss: 0.18799209594726562
Batch 26/64 loss: -0.24395418167114258
Batch 27/64 loss: -0.5578398704528809
Batch 28/64 loss: -0.28563690185546875
Batch 29/64 loss: -0.509465217590332
Batch 30/64 loss: -0.3001370429992676
Batch 31/64 loss: -0.5039768218994141
Batch 32/64 loss: -0.5354828834533691
Batch 33/64 loss: -0.5184354782104492
Batch 34/64 loss: -0.25437164306640625
Batch 35/64 loss: -0.41832447052001953
Batch 36/64 loss: -0.36609649658203125
Batch 37/64 loss: -0.5157628059387207
Batch 38/64 loss: -0.4436626434326172
Batch 39/64 loss: -0.631141185760498
Batch 40/64 loss: -0.14880752563476562
Batch 41/64 loss: -0.4282712936401367
Batch 42/64 loss: -0.06809711456298828
Batch 43/64 loss: -0.3091239929199219
Batch 44/64 loss: -0.5837478637695312
Batch 45/64 loss: -0.06455039978027344
Batch 46/64 loss: 0.004097938537597656
Batch 47/64 loss: -0.6083889007568359
Batch 48/64 loss: -0.5101866722106934
Batch 49/64 loss: -0.5408430099487305
Batch 50/64 loss: -0.5611681938171387
Batch 51/64 loss: -0.025932788848876953
Batch 52/64 loss: 0.011521339416503906
Batch 53/64 loss: 0.08905363082885742
Batch 54/64 loss: -0.24666643142700195
Batch 55/64 loss: -0.5028982162475586
Batch 56/64 loss: -0.2286238670349121
Batch 57/64 loss: -0.1483144760131836
Batch 58/64 loss: -0.0457310676574707
Batch 59/64 loss: -0.06954479217529297
Batch 60/64 loss: -0.6340599060058594
Batch 61/64 loss: -0.4096336364746094
Batch 62/64 loss: -0.09897041320800781
Batch 63/64 loss: -0.5034208297729492
Batch 64/64 loss: -4.356427192687988
Epoch 198  Train loss: -0.4037969664031384  Val loss: -0.4038131556560084
Epoch 199
-------------------------------
Batch 1/64 loss: -0.3761558532714844
Batch 2/64 loss: -0.3952031135559082
Batch 3/64 loss: -0.5151844024658203
Batch 4/64 loss: -0.2512211799621582
Batch 5/64 loss: -0.5637989044189453
Batch 6/64 loss: -0.2611250877380371
Batch 7/64 loss: -0.2700333595275879
Batch 8/64 loss: -0.778864860534668
Batch 9/64 loss: -0.32809019088745117
Batch 10/64 loss: -0.47425365447998047
Batch 11/64 loss: -0.3505082130432129
Batch 12/64 loss: -0.4096851348876953
Batch 13/64 loss: -0.15654420852661133
Batch 14/64 loss: 0.20009899139404297
Batch 15/64 loss: -0.26973676681518555
Batch 16/64 loss: -0.3699474334716797
Batch 17/64 loss: -0.5928187370300293
Batch 18/64 loss: -0.24598407745361328
Batch 19/64 loss: -0.6862850189208984
Batch 20/64 loss: -0.7011332511901855
Batch 21/64 loss: -0.8031935691833496
Batch 22/64 loss: -0.427431583404541
Batch 23/64 loss: -0.3855018615722656
Batch 24/64 loss: -0.05323505401611328
Batch 25/64 loss: -0.6318035125732422
Batch 26/64 loss: -0.3410367965698242
Batch 27/64 loss: 0.17130613327026367
Batch 28/64 loss: -0.2142949104309082
Batch 29/64 loss: -0.15981674194335938
Batch 30/64 loss: -0.40673208236694336
Batch 31/64 loss: -0.16602230072021484
Batch 32/64 loss: -0.19449710845947266
Batch 33/64 loss: -0.5460152626037598
Batch 34/64 loss: -0.4999666213989258
Batch 35/64 loss: -0.5185689926147461
Batch 36/64 loss: -0.4308052062988281
Batch 37/64 loss: -0.5967311859130859
Batch 38/64 loss: 0.24404430389404297
Batch 39/64 loss: -0.3794288635253906
Batch 40/64 loss: -0.4642658233642578
Batch 41/64 loss: -0.5950851440429688
Batch 42/64 loss: -0.08586645126342773
Batch 43/64 loss: -0.3425607681274414
Batch 44/64 loss: -0.2440333366394043
Batch 45/64 loss: -0.24475860595703125
Batch 46/64 loss: -0.2643275260925293
Batch 47/64 loss: -0.2358260154724121
Batch 48/64 loss: -0.3574790954589844
Batch 49/64 loss: -0.7032070159912109
Batch 50/64 loss: -0.6830272674560547
Batch 51/64 loss: -0.1409010887145996
Batch 52/64 loss: -0.08829212188720703
Batch 53/64 loss: -0.5987663269042969
Batch 54/64 loss: -0.4289703369140625
Batch 55/64 loss: -0.5780344009399414
Batch 56/64 loss: -0.7423028945922852
Batch 57/64 loss: -0.5104022026062012
Batch 58/64 loss: -0.3080120086669922
Batch 59/64 loss: -0.41558170318603516
Batch 60/64 loss: -0.8123517036437988
Batch 61/64 loss: -0.5826625823974609
Batch 62/64 loss: -0.22838544845581055
Batch 63/64 loss: -0.09535360336303711
Batch 64/64 loss: -3.9655261039733887
Epoch 199  Train loss: -0.42134594075820025  Val loss: -0.48759035720038657
Epoch 200
-------------------------------
Batch 1/64 loss: -0.011427879333496094
Batch 2/64 loss: -0.5315799713134766
Batch 3/64 loss: -0.03644704818725586
Batch 4/64 loss: -0.20864629745483398
Batch 5/64 loss: -0.22182846069335938
Batch 6/64 loss: -0.4312715530395508
Batch 7/64 loss: -0.5233736038208008
Batch 8/64 loss: -0.39751243591308594
Batch 9/64 loss: -0.20571041107177734
Batch 10/64 loss: 0.11272144317626953
Batch 11/64 loss: -0.8358144760131836
Batch 12/64 loss: -0.3217935562133789
Batch 13/64 loss: -0.7325935363769531
Batch 14/64 loss: -0.8272867202758789
Batch 15/64 loss: -0.5356893539428711
Batch 16/64 loss: -0.34899330139160156
Batch 17/64 loss: -0.18169832229614258
Batch 18/64 loss: -0.645329475402832
Batch 19/64 loss: -0.8637266159057617
Batch 20/64 loss: -0.5159726142883301
Batch 21/64 loss: -0.45990943908691406
Batch 22/64 loss: -0.17053508758544922
Batch 23/64 loss: -0.17282581329345703
Batch 24/64 loss: -0.439267635345459
Batch 25/64 loss: -0.667363166809082
Batch 26/64 loss: 0.455233097076416
Batch 27/64 loss: -0.4852161407470703
Batch 28/64 loss: -0.5984477996826172
Batch 29/64 loss: -0.40102386474609375
Batch 30/64 loss: -0.28919219970703125
Batch 31/64 loss: 0.021866798400878906
Batch 32/64 loss: -0.4757728576660156
Batch 33/64 loss: -0.7939081192016602
Batch 34/64 loss: -0.29850292205810547
Batch 35/64 loss: -0.49658870697021484
Batch 36/64 loss: -0.6404132843017578
Batch 37/64 loss: 0.3079695701599121
Batch 38/64 loss: -0.3922710418701172
Batch 39/64 loss: -0.1334218978881836
Batch 40/64 loss: -0.5075006484985352
Batch 41/64 loss: 0.006203174591064453
Batch 42/64 loss: -0.23954391479492188
Batch 43/64 loss: -0.5806074142456055
Batch 44/64 loss: -0.5167703628540039
Batch 45/64 loss: -0.4569711685180664
Batch 46/64 loss: -0.6019992828369141
Batch 47/64 loss: -0.5310029983520508
Batch 48/64 loss: 0.12804031372070312
Batch 49/64 loss: -0.29328012466430664
Batch 50/64 loss: -0.5428981781005859
Batch 51/64 loss: -0.4781031608581543
Batch 52/64 loss: -0.3141798973083496
Batch 53/64 loss: -0.3672657012939453
Batch 54/64 loss: -0.7184104919433594
Batch 55/64 loss: -0.7435698509216309
Batch 56/64 loss: -0.5246987342834473
Batch 57/64 loss: -0.4887504577636719
Batch 58/64 loss: -0.4765458106994629
Batch 59/64 loss: -0.20929956436157227
Batch 60/64 loss: -0.40416622161865234
Batch 61/64 loss: 0.44443225860595703
Batch 62/64 loss: -0.5677475929260254
Batch 63/64 loss: -0.15758895874023438
Batch 64/64 loss: -4.323232650756836
Epoch 200  Train loss: -0.42005041533825443  Val loss: -0.5593998178173996
Epoch 201
-------------------------------
Batch 1/64 loss: -0.4307560920715332
Batch 2/64 loss: -0.4741072654724121
Batch 3/64 loss: -0.3471379280090332
Batch 4/64 loss: -0.5067391395568848
Batch 5/64 loss: -0.531245231628418
Batch 6/64 loss: -0.4405694007873535
Batch 7/64 loss: -0.44970178604125977
Batch 8/64 loss: -0.4651470184326172
Batch 9/64 loss: -0.2461833953857422
Batch 10/64 loss: -0.3274965286254883
Batch 11/64 loss: -0.36440277099609375
Batch 12/64 loss: -0.2514052391052246
Batch 13/64 loss: -0.14447021484375
Batch 14/64 loss: 0.1297588348388672
Batch 15/64 loss: -0.3445301055908203
Batch 16/64 loss: -0.4352703094482422
Batch 17/64 loss: 0.34943437576293945
Batch 18/64 loss: -0.3943648338317871
Batch 19/64 loss: 0.14100885391235352
Batch 20/64 loss: -0.6456995010375977
Batch 21/64 loss: -0.3876628875732422
Batch 22/64 loss: -0.015969276428222656
Batch 23/64 loss: -0.7123279571533203
Batch 24/64 loss: -0.12160730361938477
Batch 25/64 loss: -0.7082796096801758
Batch 26/64 loss: -0.5275468826293945
Batch 27/64 loss: -0.5202546119689941
Batch 28/64 loss: -0.3333096504211426
Batch 29/64 loss: -0.4421977996826172
Batch 30/64 loss: -0.1594843864440918
Batch 31/64 loss: -0.27639293670654297
Batch 32/64 loss: -0.6809349060058594
Batch 33/64 loss: -0.6311254501342773
Batch 34/64 loss: -0.10000896453857422
Batch 35/64 loss: -0.516815185546875
Batch 36/64 loss: -0.6032977104187012
Batch 37/64 loss: -0.4533252716064453
Batch 38/64 loss: -0.6653070449829102
Batch 39/64 loss: -0.6163277626037598
Batch 40/64 loss: -0.6952304840087891
Batch 41/64 loss: -0.04168081283569336
Batch 42/64 loss: -0.11508893966674805
Batch 43/64 loss: -0.6008963584899902
Batch 44/64 loss: -0.3103060722351074
Batch 45/64 loss: -0.3489260673522949
Batch 46/64 loss: 0.07619524002075195
Batch 47/64 loss: -0.47277355194091797
Batch 48/64 loss: -0.6870136260986328
Batch 49/64 loss: -0.6809959411621094
Batch 50/64 loss: -0.13385486602783203
Batch 51/64 loss: 0.02249765396118164
Batch 52/64 loss: -0.4014735221862793
Batch 53/64 loss: -0.2199230194091797
Batch 54/64 loss: -0.6768412590026855
Batch 55/64 loss: -0.34081459045410156
Batch 56/64 loss: -0.5164422988891602
Batch 57/64 loss: -0.2603030204772949
Batch 58/64 loss: -0.12244987487792969
Batch 59/64 loss: -0.1306748390197754
Batch 60/64 loss: -0.5873384475708008
Batch 61/64 loss: -0.32291555404663086
Batch 62/64 loss: -0.5887317657470703
Batch 63/64 loss: -0.4985179901123047
Batch 64/64 loss: -4.324407577514648
Epoch 201  Train loss: -0.41645499584721585  Val loss: -0.5224210863670533
Epoch 202
-------------------------------
Batch 1/64 loss: -0.3841533660888672
Batch 2/64 loss: -0.40161800384521484
Batch 3/64 loss: -0.0916142463684082
Batch 4/64 loss: -0.5237603187561035
Batch 5/64 loss: -0.2037334442138672
Batch 6/64 loss: -0.5787510871887207
Batch 7/64 loss: -0.3792295455932617
Batch 8/64 loss: -0.33998632431030273
Batch 9/64 loss: 0.08289766311645508
Batch 10/64 loss: -0.4163665771484375
Batch 11/64 loss: -0.2877163887023926
Batch 12/64 loss: -0.6102008819580078
Batch 13/64 loss: -0.27985620498657227
Batch 14/64 loss: -0.3934760093688965
Batch 15/64 loss: -0.2995014190673828
Batch 16/64 loss: -0.6699256896972656
Batch 17/64 loss: -0.8239717483520508
Batch 18/64 loss: -0.3297443389892578
Batch 19/64 loss: -0.8056840896606445
Batch 20/64 loss: -0.4325580596923828
Batch 21/64 loss: -0.47022247314453125
Batch 22/64 loss: -0.5442681312561035
Batch 23/64 loss: -0.20615386962890625
Batch 24/64 loss: -0.6772069931030273
Batch 25/64 loss: -0.47138309478759766
Batch 26/64 loss: -0.1462574005126953
Batch 27/64 loss: -0.36857128143310547
Batch 28/64 loss: -0.2716403007507324
Batch 29/64 loss: -0.3342552185058594
Batch 30/64 loss: -0.12171602249145508
Batch 31/64 loss: -0.47180843353271484
Batch 32/64 loss: -0.6049885749816895
Batch 33/64 loss: -0.6279439926147461
Batch 34/64 loss: -0.3947172164916992
Batch 35/64 loss: -0.4502410888671875
Batch 36/64 loss: -0.6798839569091797
Batch 37/64 loss: -0.5277891159057617
Batch 38/64 loss: -0.29379701614379883
Batch 39/64 loss: -0.10913896560668945
Batch 40/64 loss: -0.3284626007080078
Batch 41/64 loss: -0.7411069869995117
Batch 42/64 loss: -0.3691716194152832
Batch 43/64 loss: -0.2378692626953125
Batch 44/64 loss: -0.14786195755004883
Batch 45/64 loss: -0.11369180679321289
Batch 46/64 loss: -0.5123834609985352
Batch 47/64 loss: -0.6860857009887695
Batch 48/64 loss: -0.3577723503112793
Batch 49/64 loss: -0.21932411193847656
Batch 50/64 loss: -0.036101341247558594
Batch 51/64 loss: -0.7600469589233398
Batch 52/64 loss: -0.40823841094970703
Batch 53/64 loss: -0.38477230072021484
Batch 54/64 loss: -0.6466622352600098
Batch 55/64 loss: 0.212799072265625
Batch 56/64 loss: -0.2851839065551758
Batch 57/64 loss: -0.32837581634521484
Batch 58/64 loss: -0.6130447387695312
Batch 59/64 loss: -0.28660058975219727
Batch 60/64 loss: -0.2093791961669922
Batch 61/64 loss: -0.15377426147460938
Batch 62/64 loss: 0.09259414672851562
Batch 63/64 loss: -0.34496593475341797
Batch 64/64 loss: -3.8653197288513184
Epoch 202  Train loss: -0.41890831928627165  Val loss: -0.42520082611398596
Epoch 203
-------------------------------
Batch 1/64 loss: -0.5436315536499023
Batch 2/64 loss: -0.41649341583251953
Batch 3/64 loss: -0.5610032081604004
Batch 4/64 loss: -0.2765846252441406
Batch 5/64 loss: -0.12360095977783203
Batch 6/64 loss: -0.3038749694824219
Batch 7/64 loss: -0.16057157516479492
Batch 8/64 loss: -0.38903284072875977
Batch 9/64 loss: -0.1687455177307129
Batch 10/64 loss: -0.6828136444091797
Batch 11/64 loss: -0.8071298599243164
Batch 12/64 loss: -0.4164409637451172
Batch 13/64 loss: -0.4670839309692383
Batch 14/64 loss: 0.10245943069458008
Batch 15/64 loss: -0.4679126739501953
Batch 16/64 loss: -0.1755232810974121
Batch 17/64 loss: -0.5367393493652344
Batch 18/64 loss: -0.7977352142333984
Batch 19/64 loss: -0.7038955688476562
Batch 20/64 loss: -0.19466733932495117
Batch 21/64 loss: -0.2727513313293457
Batch 22/64 loss: -0.2075815200805664
Batch 23/64 loss: -0.16444969177246094
Batch 24/64 loss: -0.4920830726623535
Batch 25/64 loss: -0.7178163528442383
Batch 26/64 loss: -0.4658088684082031
Batch 27/64 loss: -0.20000457763671875
Batch 28/64 loss: -0.4552431106567383
Batch 29/64 loss: -0.34821557998657227
Batch 30/64 loss: -0.21331024169921875
Batch 31/64 loss: -0.3274803161621094
Batch 32/64 loss: -0.5605401992797852
Batch 33/64 loss: -0.35506153106689453
Batch 34/64 loss: -0.7130336761474609
Batch 35/64 loss: -0.19977760314941406
Batch 36/64 loss: -0.5433530807495117
Batch 37/64 loss: -0.505338191986084
Batch 38/64 loss: -0.9126386642456055
Batch 39/64 loss: -0.4983229637145996
Batch 40/64 loss: -0.2174854278564453
Batch 41/64 loss: -0.7204036712646484
Batch 42/64 loss: -0.2733011245727539
Batch 43/64 loss: -0.24217557907104492
Batch 44/64 loss: -0.22401094436645508
Batch 45/64 loss: -0.6693763732910156
Batch 46/64 loss: 0.09180593490600586
Batch 47/64 loss: -0.6434192657470703
Batch 48/64 loss: -0.5095071792602539
Batch 49/64 loss: -0.6071786880493164
Batch 50/64 loss: -0.703831672668457
Batch 51/64 loss: 0.6221723556518555
Batch 52/64 loss: -0.5299777984619141
Batch 53/64 loss: -0.023887157440185547
Batch 54/64 loss: -0.1825876235961914
Batch 55/64 loss: -0.07992076873779297
Batch 56/64 loss: -0.1770615577697754
Batch 57/64 loss: 0.07705974578857422
Batch 58/64 loss: -0.5368680953979492
Batch 59/64 loss: -0.16167402267456055
Batch 60/64 loss: 0.1080923080444336
Batch 61/64 loss: -0.4718337059020996
Batch 62/64 loss: -0.31477785110473633
Batch 63/64 loss: 0.04796266555786133
Batch 64/64 loss: -4.3414835929870605
Epoch 203  Train loss: -0.40536674237718767  Val loss: -0.4507957143881886
Epoch 204
-------------------------------
Batch 1/64 loss: 0.03326606750488281
Batch 2/64 loss: -0.4312119483947754
Batch 3/64 loss: -0.4538121223449707
Batch 4/64 loss: -0.4655637741088867
Batch 5/64 loss: -0.6147975921630859
Batch 6/64 loss: 0.014070510864257812
Batch 7/64 loss: -0.30129432678222656
Batch 8/64 loss: -0.28818798065185547
Batch 9/64 loss: -0.24642658233642578
Batch 10/64 loss: 0.18117237091064453
Batch 11/64 loss: -0.01668548583984375
Batch 12/64 loss: -0.35173559188842773
Batch 13/64 loss: -0.42383861541748047
Batch 14/64 loss: 0.5164170265197754
Batch 15/64 loss: -0.08131027221679688
Batch 16/64 loss: -0.346005916595459
Batch 17/64 loss: 0.017429351806640625
Batch 18/64 loss: -0.06370067596435547
Batch 19/64 loss: -0.2654995918273926
Batch 20/64 loss: -0.4778437614440918
Batch 21/64 loss: -0.1332836151123047
Batch 22/64 loss: -0.38584470748901367
Batch 23/64 loss: -0.29826784133911133
Batch 24/64 loss: -0.5304336547851562
Batch 25/64 loss: -0.21364641189575195
Batch 26/64 loss: -0.46991729736328125
Batch 27/64 loss: -0.5411114692687988
Batch 28/64 loss: -0.5852141380310059
Batch 29/64 loss: -0.45012712478637695
Batch 30/64 loss: -0.4023776054382324
Batch 31/64 loss: -0.3589363098144531
Batch 32/64 loss: -0.6799030303955078
Batch 33/64 loss: -0.5832319259643555
Batch 34/64 loss: -0.5045833587646484
Batch 35/64 loss: -0.49344491958618164
Batch 36/64 loss: -0.37929773330688477
Batch 37/64 loss: -0.6386141777038574
Batch 38/64 loss: -0.09702491760253906
Batch 39/64 loss: -0.1058950424194336
Batch 40/64 loss: -0.4514765739440918
Batch 41/64 loss: -0.33362483978271484
Batch 42/64 loss: -0.5152506828308105
Batch 43/64 loss: -0.41041994094848633
Batch 44/64 loss: -0.34821176528930664
Batch 45/64 loss: -0.512850284576416
Batch 46/64 loss: -0.7560176849365234
Batch 47/64 loss: -0.12591934204101562
Batch 48/64 loss: -0.08730125427246094
Batch 49/64 loss: 0.10376882553100586
Batch 50/64 loss: -0.5654449462890625
Batch 51/64 loss: -0.4737548828125
Batch 52/64 loss: -0.4301624298095703
Batch 53/64 loss: 0.043387413024902344
Batch 54/64 loss: -0.7006559371948242
Batch 55/64 loss: -0.3173336982727051
Batch 56/64 loss: -0.7153234481811523
Batch 57/64 loss: -0.27275562286376953
Batch 58/64 loss: -0.27190065383911133
Batch 59/64 loss: -0.14199018478393555
Batch 60/64 loss: -0.6429290771484375
Batch 61/64 loss: -0.5643987655639648
Batch 62/64 loss: -0.29412221908569336
Batch 63/64 loss: -0.1546335220336914
Batch 64/64 loss: -4.064698219299316
Epoch 204  Train loss: -0.37497348037420536  Val loss: -0.34732299490073293
Epoch 205
-------------------------------
Batch 1/64 loss: -0.6225929260253906
Batch 2/64 loss: -0.38157224655151367
Batch 3/64 loss: -0.4735736846923828
Batch 4/64 loss: 0.2692403793334961
Batch 5/64 loss: -0.1961827278137207
Batch 6/64 loss: -0.38685131072998047
Batch 7/64 loss: -0.3322715759277344
Batch 8/64 loss: -0.7601542472839355
Batch 9/64 loss: -0.5932474136352539
Batch 10/64 loss: -0.09590578079223633
Batch 11/64 loss: -0.47405385971069336
Batch 12/64 loss: -0.33602046966552734
Batch 13/64 loss: -0.09085273742675781
Batch 14/64 loss: -0.3295321464538574
Batch 15/64 loss: -0.07408857345581055
Batch 16/64 loss: -0.8579635620117188
Batch 17/64 loss: 0.021918296813964844
Batch 18/64 loss: -0.24801874160766602
Batch 19/64 loss: -0.4884519577026367
Batch 20/64 loss: -0.3544921875
Batch 21/64 loss: 0.03310060501098633
Batch 22/64 loss: -0.3881864547729492
Batch 23/64 loss: -0.7073783874511719
Batch 24/64 loss: -0.4255218505859375
Batch 25/64 loss: -0.37003564834594727
Batch 26/64 loss: -0.49903011322021484
Batch 27/64 loss: -0.6040940284729004
Batch 28/64 loss: -0.7430257797241211
Batch 29/64 loss: -0.5583686828613281
Batch 30/64 loss: -0.5116000175476074
Batch 31/64 loss: -0.17458248138427734
Batch 32/64 loss: -0.5184206962585449
Batch 33/64 loss: -0.5950994491577148
Batch 34/64 loss: -0.33861255645751953
Batch 35/64 loss: -0.27228212356567383
Batch 36/64 loss: -0.18339109420776367
Batch 37/64 loss: -0.6904788017272949
Batch 38/64 loss: -0.5458040237426758
Batch 39/64 loss: -0.21718072891235352
Batch 40/64 loss: -0.34476280212402344
Batch 41/64 loss: -0.5700483322143555
Batch 42/64 loss: -0.6298565864562988
Batch 43/64 loss: -0.6560049057006836
Batch 44/64 loss: -0.5469889640808105
Batch 45/64 loss: -0.12187719345092773
Batch 46/64 loss: -0.37077903747558594
Batch 47/64 loss: -0.5174851417541504
Batch 48/64 loss: -0.4785032272338867
Batch 49/64 loss: -0.5945768356323242
Batch 50/64 loss: -0.5641136169433594
Batch 51/64 loss: 0.5930571556091309
Batch 52/64 loss: -0.49097776412963867
Batch 53/64 loss: -0.5911436080932617
Batch 54/64 loss: -0.6730079650878906
Batch 55/64 loss: -0.2997922897338867
Batch 56/64 loss: -0.07490015029907227
Batch 57/64 loss: -0.4086265563964844
Batch 58/64 loss: 0.04739952087402344
Batch 59/64 loss: -0.7864818572998047
Batch 60/64 loss: -0.4060959815979004
Batch 61/64 loss: -0.5881519317626953
Batch 62/64 loss: -0.564699649810791
Batch 63/64 loss: -0.6508975028991699
Batch 64/64 loss: -3.8897080421447754
Epoch 205  Train loss: -0.4442549967298321  Val loss: -0.48852560036780496
Epoch 206
-------------------------------
Batch 1/64 loss: -0.17926406860351562
Batch 2/64 loss: -0.4089217185974121
Batch 3/64 loss: -0.7830162048339844
Batch 4/64 loss: -0.9044952392578125
Batch 5/64 loss: -0.35701608657836914
Batch 6/64 loss: -0.3862571716308594
Batch 7/64 loss: -0.8387289047241211
Batch 8/64 loss: -0.3064613342285156
Batch 9/64 loss: -0.693120002746582
Batch 10/64 loss: -0.573822021484375
Batch 11/64 loss: -0.08260726928710938
Batch 12/64 loss: -0.13788509368896484
Batch 13/64 loss: -0.5712709426879883
Batch 14/64 loss: -0.4464378356933594
Batch 15/64 loss: -0.011652469635009766
Batch 16/64 loss: -0.10870075225830078
Batch 17/64 loss: -0.5153083801269531
Batch 18/64 loss: -0.3633232116699219
Batch 19/64 loss: -0.43119001388549805
Batch 20/64 loss: -0.6016840934753418
Batch 21/64 loss: -0.518946647644043
Batch 22/64 loss: -0.2644968032836914
Batch 23/64 loss: -0.31368017196655273
Batch 24/64 loss: -0.18965625762939453
Batch 25/64 loss: 0.13036203384399414
Batch 26/64 loss: 0.03901863098144531
Batch 27/64 loss: -0.052697181701660156
Batch 28/64 loss: -0.21867656707763672
Batch 29/64 loss: -0.20836591720581055
Batch 30/64 loss: -0.21101665496826172
Batch 31/64 loss: -0.2508726119995117
Batch 32/64 loss: -0.43630409240722656
Batch 33/64 loss: -0.1752309799194336
Batch 34/64 loss: -0.5507087707519531
Batch 35/64 loss: -0.6652488708496094
Batch 36/64 loss: 0.1421070098876953
Batch 37/64 loss: -0.5583286285400391
Batch 38/64 loss: -0.2949662208557129
Batch 39/64 loss: -0.6207399368286133
Batch 40/64 loss: -0.47318220138549805
Batch 41/64 loss: -0.26703929901123047
Batch 42/64 loss: 0.5392098426818848
Batch 43/64 loss: -0.7684001922607422
Batch 44/64 loss: -0.39891624450683594
Batch 45/64 loss: -0.48253631591796875
Batch 46/64 loss: -0.8538303375244141
Batch 47/64 loss: -0.7760496139526367
Batch 48/64 loss: -0.7064352035522461
Batch 49/64 loss: -0.049569129943847656
Batch 50/64 loss: -0.40535545349121094
Batch 51/64 loss: -0.41722583770751953
Batch 52/64 loss: -0.1782670021057129
Batch 53/64 loss: -0.3330879211425781
Batch 54/64 loss: -0.644777774810791
Batch 55/64 loss: -0.6346554756164551
Batch 56/64 loss: -0.037509918212890625
Batch 57/64 loss: -0.6862411499023438
Batch 58/64 loss: -0.3876934051513672
Batch 59/64 loss: -0.5125441551208496
Batch 60/64 loss: -0.5102243423461914
Batch 61/64 loss: -0.42926645278930664
Batch 62/64 loss: -0.48394155502319336
Batch 63/64 loss: -0.4362626075744629
Batch 64/64 loss: -4.279410362243652
Epoch 206  Train loss: -0.43079170152252794  Val loss: -0.5429038477107831
Epoch 207
-------------------------------
Batch 1/64 loss: -0.29483842849731445
Batch 2/64 loss: -0.3515477180480957
Batch 3/64 loss: -0.7040395736694336
Batch 4/64 loss: -0.29929542541503906
Batch 5/64 loss: -0.5770597457885742
Batch 6/64 loss: -0.6419253349304199
Batch 7/64 loss: -0.44980430603027344
Batch 8/64 loss: -0.4016704559326172
Batch 9/64 loss: -0.4419112205505371
Batch 10/64 loss: -0.6550869941711426
Batch 11/64 loss: -0.5201888084411621
Batch 12/64 loss: -0.460141658782959
Batch 13/64 loss: 0.047283172607421875
Batch 14/64 loss: -0.07484054565429688
Batch 15/64 loss: -0.07411813735961914
Batch 16/64 loss: -0.42445945739746094
Batch 17/64 loss: -0.47600650787353516
Batch 18/64 loss: -0.7192392349243164
Batch 19/64 loss: -0.43703556060791016
Batch 20/64 loss: -0.33231210708618164
Batch 21/64 loss: -0.4442920684814453
Batch 22/64 loss: -0.4799232482910156
Batch 23/64 loss: -0.8711347579956055
Batch 24/64 loss: -0.6896424293518066
Batch 25/64 loss: -0.6120162010192871
Batch 26/64 loss: -0.42094945907592773
Batch 27/64 loss: -0.8170499801635742
Batch 28/64 loss: -0.6142139434814453
Batch 29/64 loss: -0.5770606994628906
Batch 30/64 loss: -0.20064258575439453
Batch 31/64 loss: -0.24651622772216797
Batch 32/64 loss: -0.5309724807739258
Batch 33/64 loss: -0.018066883087158203
Batch 34/64 loss: -0.3899078369140625
Batch 35/64 loss: 0.13797998428344727
Batch 36/64 loss: -0.5468111038208008
Batch 37/64 loss: -0.17781686782836914
Batch 38/64 loss: -0.29570722579956055
Batch 39/64 loss: -0.4421067237854004
Batch 40/64 loss: -0.2668423652648926
Batch 41/64 loss: -0.689849853515625
Batch 42/64 loss: -0.5044989585876465
Batch 43/64 loss: -0.3337268829345703
Batch 44/64 loss: -0.5885710716247559
Batch 45/64 loss: -0.8186540603637695
Batch 46/64 loss: -0.44292640686035156
Batch 47/64 loss: -0.3492131233215332
Batch 48/64 loss: -0.271728515625
Batch 49/64 loss: -0.2441082000732422
Batch 50/64 loss: -0.48479175567626953
Batch 51/64 loss: -0.3786740303039551
Batch 52/64 loss: -0.2346024513244629
Batch 53/64 loss: -0.3891892433166504
Batch 54/64 loss: -0.6761093139648438
Batch 55/64 loss: -0.33607053756713867
Batch 56/64 loss: -0.4598875045776367
Batch 57/64 loss: -0.6777286529541016
Batch 58/64 loss: -0.5726232528686523
Batch 59/64 loss: -0.8310661315917969
Batch 60/64 loss: -0.6174125671386719
Batch 61/64 loss: -0.22846269607543945
Batch 62/64 loss: -0.6087455749511719
Batch 63/64 loss: -0.3053302764892578
Batch 64/64 loss: -4.3315653800964355
Epoch 207  Train loss: -0.48760119606466856  Val loss: -0.4481714255211689
Epoch 208
-------------------------------
Batch 1/64 loss: -0.4314398765563965
Batch 2/64 loss: -0.7565784454345703
Batch 3/64 loss: -0.34155941009521484
Batch 4/64 loss: -0.20076656341552734
Batch 5/64 loss: 0.475006103515625
Batch 6/64 loss: -0.34193897247314453
Batch 7/64 loss: -0.64013671875
Batch 8/64 loss: -0.5037651062011719
Batch 9/64 loss: -0.8246259689331055
Batch 10/64 loss: -0.531947135925293
Batch 11/64 loss: -0.21105194091796875
Batch 12/64 loss: -0.6846866607666016
Batch 13/64 loss: -0.5382900238037109
Batch 14/64 loss: -0.7616519927978516
Batch 15/64 loss: -0.2963695526123047
Batch 16/64 loss: 0.19176101684570312
Batch 17/64 loss: -0.6332197189331055
Batch 18/64 loss: -0.6783819198608398
Batch 19/64 loss: -0.8321409225463867
Batch 20/64 loss: -0.4174613952636719
Batch 21/64 loss: -0.6946225166320801
Batch 22/64 loss: -0.1159520149230957
Batch 23/64 loss: -0.2476186752319336
Batch 24/64 loss: -0.44381141662597656
Batch 25/64 loss: -0.3052940368652344
Batch 26/64 loss: -0.2587890625
Batch 27/64 loss: 0.22032403945922852
Batch 28/64 loss: -0.2641615867614746
Batch 29/64 loss: -0.7697381973266602
Batch 30/64 loss: -0.20115375518798828
Batch 31/64 loss: -0.3960380554199219
Batch 32/64 loss: -0.2705039978027344
Batch 33/64 loss: -0.3368997573852539
Batch 34/64 loss: -0.5530719757080078
Batch 35/64 loss: -0.16240930557250977
Batch 36/64 loss: -0.29108142852783203
Batch 37/64 loss: -0.3329348564147949
Batch 38/64 loss: -0.3397188186645508
Batch 39/64 loss: -0.2771282196044922
Batch 40/64 loss: 0.1638188362121582
Batch 41/64 loss: -0.37818241119384766
Batch 42/64 loss: -0.6847434043884277
Batch 43/64 loss: -0.7011723518371582
Batch 44/64 loss: -0.3172740936279297
Batch 45/64 loss: -0.5393276214599609
Batch 46/64 loss: 0.5663681030273438
Batch 47/64 loss: -0.5138583183288574
Batch 48/64 loss: -0.41087770462036133
Batch 49/64 loss: -0.32845640182495117
Batch 50/64 loss: -0.06649971008300781
Batch 51/64 loss: -0.08174943923950195
Batch 52/64 loss: -0.2004566192626953
Batch 53/64 loss: -0.4194679260253906
Batch 54/64 loss: -0.40360164642333984
Batch 55/64 loss: -0.4005160331726074
Batch 56/64 loss: -0.19467830657958984
Batch 57/64 loss: -0.0981287956237793
Batch 58/64 loss: 0.5500373840332031
Batch 59/64 loss: -0.49329280853271484
Batch 60/64 loss: 0.06309080123901367
Batch 61/64 loss: 0.15250921249389648
Batch 62/64 loss: -0.4967060089111328
Batch 63/64 loss: -0.39162731170654297
Batch 64/64 loss: -4.341663837432861
Epoch 208  Train loss: -0.37460218504363413  Val loss: -0.4625367626701434
Epoch 209
-------------------------------
Batch 1/64 loss: -0.11486673355102539
Batch 2/64 loss: -0.7044816017150879
Batch 3/64 loss: -0.6788301467895508
Batch 4/64 loss: -0.6168088912963867
Batch 5/64 loss: -0.0459895133972168
Batch 6/64 loss: -0.24955987930297852
Batch 7/64 loss: -0.2810344696044922
Batch 8/64 loss: -0.35324764251708984
Batch 9/64 loss: -0.2197737693786621
Batch 10/64 loss: -0.36087751388549805
Batch 11/64 loss: -0.3831334114074707
Batch 12/64 loss: -0.17160797119140625
Batch 13/64 loss: -0.6291966438293457
Batch 14/64 loss: -0.19078922271728516
Batch 15/64 loss: -0.6707210540771484
Batch 16/64 loss: -0.24978399276733398
Batch 17/64 loss: -0.3394045829772949
Batch 18/64 loss: -0.7154006958007812
Batch 19/64 loss: 0.40505409240722656
Batch 20/64 loss: -0.5571861267089844
Batch 21/64 loss: 0.261260986328125
Batch 22/64 loss: -0.2760162353515625
Batch 23/64 loss: -0.41950035095214844
Batch 24/64 loss: -0.6438727378845215
Batch 25/64 loss: -0.4042668342590332
Batch 26/64 loss: -0.6514673233032227
Batch 27/64 loss: -0.8090476989746094
Batch 28/64 loss: -0.3931283950805664
Batch 29/64 loss: -0.6529626846313477
Batch 30/64 loss: -0.4752645492553711
Batch 31/64 loss: -0.31545591354370117
Batch 32/64 loss: -0.5905895233154297
Batch 33/64 loss: -0.6490230560302734
Batch 34/64 loss: -0.5091309547424316
Batch 35/64 loss: 0.02676105499267578
Batch 36/64 loss: -0.052220821380615234
Batch 37/64 loss: -0.21761417388916016
Batch 38/64 loss: -0.5328140258789062
Batch 39/64 loss: -0.10757160186767578
Batch 40/64 loss: -0.4416217803955078
Batch 41/64 loss: -0.37774181365966797
Batch 42/64 loss: -0.12407207489013672
Batch 43/64 loss: -0.0521397590637207
Batch 44/64 loss: 0.3056020736694336
Batch 45/64 loss: 1.055051326751709
Batch 46/64 loss: -0.6046571731567383
Batch 47/64 loss: -0.781519889831543
Batch 48/64 loss: -0.3732733726501465
Batch 49/64 loss: -0.2745213508605957
Batch 50/64 loss: -0.37178850173950195
Batch 51/64 loss: -0.40592193603515625
Batch 52/64 loss: -0.40502071380615234
Batch 53/64 loss: -0.15325450897216797
Batch 54/64 loss: -0.26908397674560547
Batch 55/64 loss: -0.005712985992431641
Batch 56/64 loss: -0.8624472618103027
Batch 57/64 loss: -0.3625297546386719
Batch 58/64 loss: -0.584662914276123
Batch 59/64 loss: -0.2676992416381836
Batch 60/64 loss: -0.2872285842895508
Batch 61/64 loss: -0.6620540618896484
Batch 62/64 loss: -0.14806413650512695
Batch 63/64 loss: -0.1951603889465332
Batch 64/64 loss: -3.971468925476074
Epoch 209  Train loss: -0.3791010052550073  Val loss: -0.4304154648403941
Epoch 210
-------------------------------
Batch 1/64 loss: -0.23497915267944336
Batch 2/64 loss: -0.41555118560791016
Batch 3/64 loss: -0.5955572128295898
Batch 4/64 loss: -0.20260858535766602
Batch 5/64 loss: -0.25902605056762695
Batch 6/64 loss: -0.30936145782470703
Batch 7/64 loss: -0.5639529228210449
Batch 8/64 loss: -0.3422727584838867
Batch 9/64 loss: -0.22084474563598633
Batch 10/64 loss: -0.2890024185180664
Batch 11/64 loss: 0.2383270263671875
Batch 12/64 loss: -0.4154806137084961
Batch 13/64 loss: -0.6020169258117676
Batch 14/64 loss: -0.2813882827758789
Batch 15/64 loss: -0.2023611068725586
Batch 16/64 loss: -0.48670387268066406
Batch 17/64 loss: -0.11884069442749023
Batch 18/64 loss: -0.30124807357788086
Batch 19/64 loss: 0.034990787506103516
Batch 20/64 loss: -0.3455076217651367
Batch 21/64 loss: -0.3596949577331543
Batch 22/64 loss: -0.07934284210205078
Batch 23/64 loss: -0.2648954391479492
Batch 24/64 loss: -0.6361355781555176
Batch 25/64 loss: -0.504847526550293
Batch 26/64 loss: -0.3187899589538574
Batch 27/64 loss: -0.2568521499633789
Batch 28/64 loss: -0.6766600608825684
Batch 29/64 loss: -0.7660064697265625
Batch 30/64 loss: -0.8433332443237305
Batch 31/64 loss: -0.665806770324707
Batch 32/64 loss: -0.4751453399658203
Batch 33/64 loss: -0.3802037239074707
Batch 34/64 loss: -0.5852108001708984
Batch 35/64 loss: -0.5406055450439453
Batch 36/64 loss: -0.7317109107971191
Batch 37/64 loss: -0.2897348403930664
Batch 38/64 loss: -0.6178841590881348
Batch 39/64 loss: -0.15788936614990234
Batch 40/64 loss: -0.14350318908691406
Batch 41/64 loss: -0.43604469299316406
Batch 42/64 loss: -0.5736098289489746
Batch 43/64 loss: -0.09789466857910156
Batch 44/64 loss: -0.5379767417907715
Batch 45/64 loss: -0.3810262680053711
Batch 46/64 loss: -0.06018352508544922
Batch 47/64 loss: -0.2589550018310547
Batch 48/64 loss: -0.6294393539428711
Batch 49/64 loss: -0.2991318702697754
Batch 50/64 loss: -0.31295108795166016
Batch 51/64 loss: -0.2852749824523926
Batch 52/64 loss: -0.40913820266723633
Batch 53/64 loss: -0.45548343658447266
Batch 54/64 loss: -0.38063907623291016
Batch 55/64 loss: -0.43233489990234375
Batch 56/64 loss: 0.02364826202392578
Batch 57/64 loss: -0.5806193351745605
Batch 58/64 loss: -0.35415124893188477
Batch 59/64 loss: -0.5025463104248047
Batch 60/64 loss: -0.5751800537109375
Batch 61/64 loss: -0.46358776092529297
Batch 62/64 loss: -0.5156188011169434
Batch 63/64 loss: -0.41106176376342773
Batch 64/64 loss: -4.519289493560791
Epoch 210  Train loss: -0.4317852001564175  Val loss: -0.5041624901630625
Epoch 211
-------------------------------
Batch 1/64 loss: -0.06324958801269531
Batch 2/64 loss: -0.48453617095947266
Batch 3/64 loss: -0.5709552764892578
Batch 4/64 loss: -0.8836870193481445
Batch 5/64 loss: -0.11415958404541016
Batch 6/64 loss: -0.6648740768432617
Batch 7/64 loss: -0.7184567451477051
Batch 8/64 loss: -0.6265730857849121
Batch 9/64 loss: -0.6995258331298828
Batch 10/64 loss: -0.2804536819458008
Batch 11/64 loss: -0.865086555480957
Batch 12/64 loss: -0.49806737899780273
Batch 13/64 loss: -0.6584463119506836
Batch 14/64 loss: -0.3738884925842285
Batch 15/64 loss: -0.7059602737426758
Batch 16/64 loss: -0.06923294067382812
Batch 17/64 loss: -0.4553260803222656
Batch 18/64 loss: -0.4739370346069336
Batch 19/64 loss: -0.23344993591308594
Batch 20/64 loss: -0.3935098648071289
Batch 21/64 loss: -0.287900447845459
Batch 22/64 loss: -0.6091842651367188
Batch 23/64 loss: -0.5866584777832031
Batch 24/64 loss: -0.6689882278442383
Batch 25/64 loss: -0.7038593292236328
Batch 26/64 loss: -0.5014972686767578
Batch 27/64 loss: -0.5201244354248047
Batch 28/64 loss: -0.8061895370483398
Batch 29/64 loss: -0.7545256614685059
Batch 30/64 loss: -0.5888886451721191
Batch 31/64 loss: -0.46614646911621094
Batch 32/64 loss: -0.40097808837890625
Batch 33/64 loss: -0.4997105598449707
Batch 34/64 loss: 0.19057130813598633
Batch 35/64 loss: -0.5381441116333008
Batch 36/64 loss: -0.6890363693237305
Batch 37/64 loss: -0.39868974685668945
Batch 38/64 loss: -0.30479955673217773
Batch 39/64 loss: -0.3539700508117676
Batch 40/64 loss: -0.7597322463989258
Batch 41/64 loss: -0.20824480056762695
Batch 42/64 loss: -0.7324929237365723
Batch 43/64 loss: -0.4271845817565918
Batch 44/64 loss: -0.05985307693481445
Batch 45/64 loss: 0.04413318634033203
Batch 46/64 loss: -0.5972490310668945
Batch 47/64 loss: -0.7347593307495117
Batch 48/64 loss: -0.5627288818359375
Batch 49/64 loss: -0.7866697311401367
Batch 50/64 loss: -0.2729654312133789
Batch 51/64 loss: -0.5126967430114746
Batch 52/64 loss: -0.6763696670532227
Batch 53/64 loss: -0.4578733444213867
Batch 54/64 loss: -0.3468470573425293
Batch 55/64 loss: -0.24422788619995117
Batch 56/64 loss: -0.46016502380371094
Batch 57/64 loss: -0.5975275039672852
Batch 58/64 loss: -0.16388320922851562
Batch 59/64 loss: -0.5613360404968262
Batch 60/64 loss: -0.49883413314819336
Batch 61/64 loss: -0.19335317611694336
Batch 62/64 loss: -0.2653069496154785
Batch 63/64 loss: -0.5562305450439453
Batch 64/64 loss: -3.3107337951660156
Epoch 211  Train loss: -0.5088242250330308  Val loss: -0.6441831424883551
Saving best model, epoch: 211
Epoch 212
-------------------------------
Batch 1/64 loss: -0.6221599578857422
Batch 2/64 loss: -0.0782470703125
Batch 3/64 loss: -0.42063093185424805
Batch 4/64 loss: -0.7719097137451172
Batch 5/64 loss: -0.7768402099609375
Batch 6/64 loss: 0.0123748779296875
Batch 7/64 loss: -0.5766029357910156
Batch 8/64 loss: -0.6463661193847656
Batch 9/64 loss: -0.026790618896484375
Batch 10/64 loss: -0.21475887298583984
Batch 11/64 loss: -0.7567434310913086
Batch 12/64 loss: -0.6270880699157715
Batch 13/64 loss: -0.45284318923950195
Batch 14/64 loss: -0.38724756240844727
Batch 15/64 loss: -0.3573322296142578
Batch 16/64 loss: 0.030536651611328125
Batch 17/64 loss: -0.4912147521972656
Batch 18/64 loss: 0.05990409851074219
Batch 19/64 loss: -0.712010383605957
Batch 20/64 loss: -0.6546201705932617
Batch 21/64 loss: -0.5420727729797363
Batch 22/64 loss: -0.6232948303222656
Batch 23/64 loss: -0.5079364776611328
Batch 24/64 loss: -0.4529581069946289
Batch 25/64 loss: -0.18567895889282227
Batch 26/64 loss: -0.4941391944885254
Batch 27/64 loss: -0.7040519714355469
Batch 28/64 loss: -0.35566091537475586
Batch 29/64 loss: -0.4949178695678711
Batch 30/64 loss: -0.6636943817138672
Batch 31/64 loss: -0.2654547691345215
Batch 32/64 loss: -0.6395645141601562
Batch 33/64 loss: -0.2842864990234375
Batch 34/64 loss: -0.5466732978820801
Batch 35/64 loss: -0.8277840614318848
Batch 36/64 loss: -0.5639801025390625
Batch 37/64 loss: -0.5000381469726562
Batch 38/64 loss: -0.26772499084472656
Batch 39/64 loss: -0.6959409713745117
Batch 40/64 loss: -0.3836193084716797
Batch 41/64 loss: -0.21456003189086914
Batch 42/64 loss: -0.29230499267578125
Batch 43/64 loss: -0.8823814392089844
Batch 44/64 loss: -0.19725656509399414
Batch 45/64 loss: -0.7471828460693359
Batch 46/64 loss: -0.47908544540405273
Batch 47/64 loss: -0.5691690444946289
Batch 48/64 loss: -0.11015892028808594
Batch 49/64 loss: -0.46689558029174805
Batch 50/64 loss: -0.36816978454589844
Batch 51/64 loss: -0.2553691864013672
Batch 52/64 loss: -0.588932991027832
Batch 53/64 loss: -0.7044782638549805
Batch 54/64 loss: -0.8229894638061523
Batch 55/64 loss: -0.4068031311035156
Batch 56/64 loss: -0.46976375579833984
Batch 57/64 loss: -0.788543701171875
Batch 58/64 loss: -0.45197582244873047
Batch 59/64 loss: -0.8208436965942383
Batch 60/64 loss: -0.8376741409301758
Batch 61/64 loss: -0.5271315574645996
Batch 62/64 loss: -0.3319830894470215
Batch 63/64 loss: -0.5460104942321777
Batch 64/64 loss: -3.7931008338928223
Epoch 212  Train loss: -0.5206988596448712  Val loss: -0.494307947322675
Epoch 213
-------------------------------
Batch 1/64 loss: -0.6465163230895996
Batch 2/64 loss: -0.3970317840576172
Batch 3/64 loss: -0.5520133972167969
Batch 4/64 loss: -0.5378851890563965
Batch 5/64 loss: 0.1396627426147461
Batch 6/64 loss: -0.19451141357421875
Batch 7/64 loss: -0.1675853729248047
Batch 8/64 loss: -0.5175342559814453
Batch 9/64 loss: -0.591578483581543
Batch 10/64 loss: -0.1058359146118164
Batch 11/64 loss: -0.4672684669494629
Batch 12/64 loss: -0.29065847396850586
Batch 13/64 loss: -0.4482231140136719
Batch 14/64 loss: -0.4406156539916992
Batch 15/64 loss: -0.4081707000732422
Batch 16/64 loss: -0.22704267501831055
Batch 17/64 loss: 0.20975971221923828
Batch 18/64 loss: -0.5931310653686523
Batch 19/64 loss: -0.1261754035949707
Batch 20/64 loss: 0.09171056747436523
Batch 21/64 loss: -0.6488418579101562
Batch 22/64 loss: -0.2236652374267578
Batch 23/64 loss: -0.612518310546875
Batch 24/64 loss: -0.761690616607666
Batch 25/64 loss: -0.32001733779907227
Batch 26/64 loss: -0.8393163681030273
Batch 27/64 loss: -0.5210881233215332
Batch 28/64 loss: -0.3442354202270508
Batch 29/64 loss: 0.19872140884399414
Batch 30/64 loss: -0.6493759155273438
Batch 31/64 loss: -0.31715869903564453
Batch 32/64 loss: -0.0878438949584961
Batch 33/64 loss: -0.24460744857788086
Batch 34/64 loss: -0.3008708953857422
Batch 35/64 loss: -0.6395721435546875
Batch 36/64 loss: -0.6177902221679688
Batch 37/64 loss: -0.4517807960510254
Batch 38/64 loss: -0.6177325248718262
Batch 39/64 loss: -0.677215576171875
Batch 40/64 loss: -0.15925884246826172
Batch 41/64 loss: -0.6744575500488281
Batch 42/64 loss: -0.6455607414245605
Batch 43/64 loss: -0.2214350700378418
Batch 44/64 loss: -0.5976448059082031
Batch 45/64 loss: -0.5700469017028809
Batch 46/64 loss: -0.03287458419799805
Batch 47/64 loss: -0.4657292366027832
Batch 48/64 loss: -0.30561208724975586
Batch 49/64 loss: -0.615382194519043
Batch 50/64 loss: -0.2955803871154785
Batch 51/64 loss: -0.30632781982421875
Batch 52/64 loss: -0.05288219451904297
Batch 53/64 loss: -0.4653172492980957
Batch 54/64 loss: -0.1270136833190918
Batch 55/64 loss: -0.4500551223754883
Batch 56/64 loss: -0.8404445648193359
Batch 57/64 loss: -0.4407501220703125
Batch 58/64 loss: -0.19805479049682617
Batch 59/64 loss: 0.048661231994628906
Batch 60/64 loss: -0.5720906257629395
Batch 61/64 loss: -0.23011016845703125
Batch 62/64 loss: -0.3921217918395996
Batch 63/64 loss: -0.29772186279296875
Batch 64/64 loss: -4.242757797241211
Epoch 213  Train loss: -0.4241117514815985  Val loss: -0.5336432178405552
Epoch 214
-------------------------------
Batch 1/64 loss: -0.3767671585083008
Batch 2/64 loss: -0.5018463134765625
Batch 3/64 loss: -0.7133998870849609
Batch 4/64 loss: -0.6119976043701172
Batch 5/64 loss: -0.07645893096923828
Batch 6/64 loss: -0.0724644660949707
Batch 7/64 loss: -0.5153951644897461
Batch 8/64 loss: -0.44974374771118164
Batch 9/64 loss: -0.540865421295166
Batch 10/64 loss: -0.32912731170654297
Batch 11/64 loss: -0.32036733627319336
Batch 12/64 loss: -0.3016810417175293
Batch 13/64 loss: -0.7624759674072266
Batch 14/64 loss: -0.41789913177490234
Batch 15/64 loss: -0.3562765121459961
Batch 16/64 loss: -0.6087541580200195
Batch 17/64 loss: -0.6101865768432617
Batch 18/64 loss: -0.5402274131774902
Batch 19/64 loss: -0.6798877716064453
Batch 20/64 loss: -0.46039295196533203
Batch 21/64 loss: -0.6478080749511719
Batch 22/64 loss: -0.3417038917541504
Batch 23/64 loss: 0.2632155418395996
Batch 24/64 loss: -0.8764410018920898
Batch 25/64 loss: 0.11400699615478516
Batch 26/64 loss: -0.5630278587341309
Batch 27/64 loss: -0.47403812408447266
Batch 28/64 loss: -0.4963264465332031
Batch 29/64 loss: -0.38373470306396484
Batch 30/64 loss: -0.08330583572387695
Batch 31/64 loss: -0.5776958465576172
Batch 32/64 loss: -0.7645120620727539
Batch 33/64 loss: -0.3486366271972656
Batch 34/64 loss: -0.5879874229431152
Batch 35/64 loss: 0.10195589065551758
Batch 36/64 loss: -0.605473518371582
Batch 37/64 loss: -0.8112754821777344
Batch 38/64 loss: -0.7729902267456055
Batch 39/64 loss: -0.41173744201660156
Batch 40/64 loss: -0.33254432678222656
Batch 41/64 loss: -0.47148609161376953
Batch 42/64 loss: -0.06387662887573242
Batch 43/64 loss: -0.10900545120239258
Batch 44/64 loss: -0.052881717681884766
Batch 45/64 loss: -0.7277836799621582
Batch 46/64 loss: -0.05659961700439453
Batch 47/64 loss: 0.0038499832153320312
Batch 48/64 loss: -0.36989307403564453
Batch 49/64 loss: -0.41915225982666016
Batch 50/64 loss: -0.48142290115356445
Batch 51/64 loss: 0.1428823471069336
Batch 52/64 loss: -0.030390262603759766
Batch 53/64 loss: -0.18280267715454102
Batch 54/64 loss: -0.5879201889038086
Batch 55/64 loss: -0.49123334884643555
Batch 56/64 loss: 0.28383302688598633
Batch 57/64 loss: -0.13003063201904297
Batch 58/64 loss: -0.32834482192993164
Batch 59/64 loss: -0.6120095252990723
Batch 60/64 loss: -0.3590579032897949
Batch 61/64 loss: -0.6487727165222168
Batch 62/64 loss: -0.6825389862060547
Batch 63/64 loss: -0.22364044189453125
Batch 64/64 loss: -4.7267303466796875
Epoch 214  Train loss: -0.4390525593477137  Val loss: -0.5795829484552862
Epoch 215
-------------------------------
Batch 1/64 loss: -0.24088144302368164
Batch 2/64 loss: -0.5953764915466309
Batch 3/64 loss: -0.5780329704284668
Batch 4/64 loss: -0.7077827453613281
Batch 5/64 loss: -0.503624439239502
Batch 6/64 loss: -0.6686086654663086
Batch 7/64 loss: -0.40259456634521484
Batch 8/64 loss: -0.6154947280883789
Batch 9/64 loss: -0.5519571304321289
Batch 10/64 loss: -0.6553459167480469
Batch 11/64 loss: -0.3599224090576172
Batch 12/64 loss: -0.12341737747192383
Batch 13/64 loss: -0.3184037208557129
Batch 14/64 loss: -0.5318517684936523
Batch 15/64 loss: 0.0015807151794433594
Batch 16/64 loss: -0.04781627655029297
Batch 17/64 loss: -0.6076884269714355
Batch 18/64 loss: -0.06054496765136719
Batch 19/64 loss: -0.5707893371582031
Batch 20/64 loss: -0.5994176864624023
Batch 21/64 loss: -0.3654012680053711
Batch 22/64 loss: 0.08871698379516602
Batch 23/64 loss: -0.2888011932373047
Batch 24/64 loss: -0.7819461822509766
Batch 25/64 loss: -0.28807735443115234
Batch 26/64 loss: -0.7038521766662598
Batch 27/64 loss: -0.4457683563232422
Batch 28/64 loss: -0.16727447509765625
Batch 29/64 loss: -0.5181474685668945
Batch 30/64 loss: -0.5531978607177734
Batch 31/64 loss: -0.25931358337402344
Batch 32/64 loss: -0.2970290184020996
Batch 33/64 loss: -0.6177444458007812
Batch 34/64 loss: -0.4631052017211914
Batch 35/64 loss: -0.4707508087158203
Batch 36/64 loss: -0.7714214324951172
Batch 37/64 loss: -0.3419814109802246
Batch 38/64 loss: -0.3751535415649414
Batch 39/64 loss: -0.15172958374023438
Batch 40/64 loss: -0.48909473419189453
Batch 41/64 loss: 0.28449344635009766
Batch 42/64 loss: -0.47414064407348633
Batch 43/64 loss: -0.42435121536254883
Batch 44/64 loss: -0.6052751541137695
Batch 45/64 loss: -0.5912933349609375
Batch 46/64 loss: -0.18878650665283203
Batch 47/64 loss: -0.5755925178527832
Batch 48/64 loss: -0.4911689758300781
Batch 49/64 loss: -0.7675004005432129
Batch 50/64 loss: -0.41469383239746094
Batch 51/64 loss: -0.2664780616760254
Batch 52/64 loss: -0.6813726425170898
Batch 53/64 loss: -0.7994513511657715
Batch 54/64 loss: -0.1568460464477539
Batch 55/64 loss: 0.016733169555664062
Batch 56/64 loss: -0.5800561904907227
Batch 57/64 loss: -0.34966373443603516
Batch 58/64 loss: -0.6667146682739258
Batch 59/64 loss: -0.04671764373779297
Batch 60/64 loss: -0.29789257049560547
Batch 61/64 loss: -0.4067268371582031
Batch 62/64 loss: -0.020782947540283203
Batch 63/64 loss: -0.6296472549438477
Batch 64/64 loss: -3.1078391075134277
Epoch 215  Train loss: -0.4464917444715313  Val loss: -0.6106448091592166
Epoch 216
-------------------------------
Batch 1/64 loss: -0.4927096366882324
Batch 2/64 loss: -0.721372127532959
Batch 3/64 loss: 0.05043649673461914
Batch 4/64 loss: -0.3505520820617676
Batch 5/64 loss: -0.49305057525634766
Batch 6/64 loss: -0.41245126724243164
Batch 7/64 loss: -0.5601263046264648
Batch 8/64 loss: -0.3812098503112793
Batch 9/64 loss: -0.6180133819580078
Batch 10/64 loss: -0.46753358840942383
Batch 11/64 loss: -0.321868896484375
Batch 12/64 loss: -0.6543335914611816
Batch 13/64 loss: -0.45215940475463867
Batch 14/64 loss: -0.1357588768005371
Batch 15/64 loss: -0.44747495651245117
Batch 16/64 loss: -0.45405006408691406
Batch 17/64 loss: -0.39920663833618164
Batch 18/64 loss: 0.0811147689819336
Batch 19/64 loss: -0.8070735931396484
Batch 20/64 loss: -0.7290325164794922
Batch 21/64 loss: -0.1934065818786621
Batch 22/64 loss: -0.4598560333251953
Batch 23/64 loss: -0.6008768081665039
Batch 24/64 loss: -0.5431337356567383
Batch 25/64 loss: -0.13934993743896484
Batch 26/64 loss: -0.678715705871582
Batch 27/64 loss: -0.4426851272583008
Batch 28/64 loss: -0.6523771286010742
Batch 29/64 loss: -0.6707944869995117
Batch 30/64 loss: -0.7117948532104492
Batch 31/64 loss: -0.5640878677368164
Batch 32/64 loss: -0.657379150390625
Batch 33/64 loss: -0.7046632766723633
Batch 34/64 loss: -0.7990007400512695
Batch 35/64 loss: 0.2281026840209961
Batch 36/64 loss: -0.6813926696777344
Batch 37/64 loss: -0.5781183242797852
Batch 38/64 loss: -0.371368408203125
Batch 39/64 loss: -0.5737571716308594
Batch 40/64 loss: -0.4215860366821289
Batch 41/64 loss: -0.3331785202026367
Batch 42/64 loss: -0.4418144226074219
Batch 43/64 loss: -0.7826604843139648
Batch 44/64 loss: -0.7428150177001953
Batch 45/64 loss: -0.35828685760498047
Batch 46/64 loss: -0.3314948081970215
Batch 47/64 loss: -0.6068339347839355
Batch 48/64 loss: -0.6565427780151367
Batch 49/64 loss: -0.27594947814941406
Batch 50/64 loss: 0.027677536010742188
Batch 51/64 loss: -0.05963706970214844
Batch 52/64 loss: -0.7112531661987305
Batch 53/64 loss: -0.24028539657592773
Batch 54/64 loss: -0.07561922073364258
Batch 55/64 loss: -0.3012094497680664
Batch 56/64 loss: -0.6125082969665527
Batch 57/64 loss: -0.6491737365722656
Batch 58/64 loss: -0.4329648017883301
Batch 59/64 loss: -0.4579739570617676
Batch 60/64 loss: -0.46054506301879883
Batch 61/64 loss: 0.12158536911010742
Batch 62/64 loss: 0.2682619094848633
Batch 63/64 loss: -0.4333658218383789
Batch 64/64 loss: -4.519591808319092
Epoch 216  Train loss: -0.4850031179540298  Val loss: -0.5402994844102368
Epoch 217
-------------------------------
Batch 1/64 loss: -0.16844892501831055
Batch 2/64 loss: -0.8545284271240234
Batch 3/64 loss: -0.8176746368408203
Batch 4/64 loss: -0.6532368659973145
Batch 5/64 loss: -0.567997932434082
Batch 6/64 loss: -0.6357240676879883
Batch 7/64 loss: -0.26168394088745117
Batch 8/64 loss: -0.42850303649902344
Batch 9/64 loss: -0.346529483795166
Batch 10/64 loss: -0.47256898880004883
Batch 11/64 loss: -0.7401237487792969
Batch 12/64 loss: -0.6221237182617188
Batch 13/64 loss: -0.2736501693725586
Batch 14/64 loss: -0.4370856285095215
Batch 15/64 loss: -0.8019981384277344
Batch 16/64 loss: -0.6486597061157227
Batch 17/64 loss: -0.32430601119995117
Batch 18/64 loss: -0.8801078796386719
Batch 19/64 loss: -0.6117830276489258
Batch 20/64 loss: -0.5899152755737305
Batch 21/64 loss: -0.120361328125
Batch 22/64 loss: -0.058045387268066406
Batch 23/64 loss: 0.029792308807373047
Batch 24/64 loss: -0.4795503616333008
Batch 25/64 loss: -0.7754964828491211
Batch 26/64 loss: -0.7158517837524414
Batch 27/64 loss: -0.28449487686157227
Batch 28/64 loss: -0.48465538024902344
Batch 29/64 loss: -0.42485952377319336
Batch 30/64 loss: -0.6130313873291016
Batch 31/64 loss: -0.6003007888793945
Batch 32/64 loss: -0.5437889099121094
Batch 33/64 loss: -0.6320090293884277
Batch 34/64 loss: -0.6694884300231934
Batch 35/64 loss: -0.47970056533813477
Batch 36/64 loss: -0.3759193420410156
Batch 37/64 loss: -0.39550113677978516
Batch 38/64 loss: -0.5210075378417969
Batch 39/64 loss: -0.6128759384155273
Batch 40/64 loss: -0.45476722717285156
Batch 41/64 loss: -0.5967135429382324
Batch 42/64 loss: -0.43489789962768555
Batch 43/64 loss: 0.04046154022216797
Batch 44/64 loss: -0.5881633758544922
Batch 45/64 loss: -0.7178707122802734
Batch 46/64 loss: -0.02109813690185547
Batch 47/64 loss: -0.4511079788208008
Batch 48/64 loss: 1.1357603073120117
Batch 49/64 loss: -0.20770835876464844
Batch 50/64 loss: -0.5857143402099609
Batch 51/64 loss: -0.5444912910461426
Batch 52/64 loss: -0.6850476264953613
Batch 53/64 loss: -0.7245121002197266
Batch 54/64 loss: -0.3158998489379883
Batch 55/64 loss: -0.6560077667236328
Batch 56/64 loss: -0.4484381675720215
Batch 57/64 loss: 0.12078428268432617
Batch 58/64 loss: -0.5423736572265625
Batch 59/64 loss: -0.4687309265136719
Batch 60/64 loss: -0.30033302307128906
Batch 61/64 loss: -0.5583405494689941
Batch 62/64 loss: -0.34406423568725586
Batch 63/64 loss: -0.7001876831054688
Batch 64/64 loss: -4.241316795349121
Epoch 217  Train loss: -0.5039097318462297  Val loss: -0.5779827288335951
Epoch 218
-------------------------------
Batch 1/64 loss: -0.42520570755004883
Batch 2/64 loss: -0.3958911895751953
Batch 3/64 loss: -0.26032495498657227
Batch 4/64 loss: -0.6588954925537109
Batch 5/64 loss: -0.25679588317871094
Batch 6/64 loss: -0.3856639862060547
Batch 7/64 loss: 0.5093235969543457
Batch 8/64 loss: -0.2736988067626953
Batch 9/64 loss: 0.04333925247192383
Batch 10/64 loss: 0.03471231460571289
Batch 11/64 loss: -0.45299243927001953
Batch 12/64 loss: 0.05390214920043945
Batch 13/64 loss: -0.5753002166748047
Batch 14/64 loss: -0.5216646194458008
Batch 15/64 loss: -0.6532883644104004
Batch 16/64 loss: -0.6869244575500488
Batch 17/64 loss: -0.611781120300293
Batch 18/64 loss: -0.7507572174072266
Batch 19/64 loss: -0.5825424194335938
Batch 20/64 loss: -0.4137115478515625
Batch 21/64 loss: -0.2560262680053711
Batch 22/64 loss: -0.7120051383972168
Batch 23/64 loss: 0.01261281967163086
Batch 24/64 loss: -0.3589944839477539
Batch 25/64 loss: -0.8539624214172363
Batch 26/64 loss: -0.5838494300842285
Batch 27/64 loss: -0.5190896987915039
Batch 28/64 loss: 0.03379201889038086
Batch 29/64 loss: -0.5318098068237305
Batch 30/64 loss: -0.2040729522705078
Batch 31/64 loss: -0.7112898826599121
Batch 32/64 loss: -0.6080946922302246
Batch 33/64 loss: -0.2099771499633789
Batch 34/64 loss: -0.6303586959838867
Batch 35/64 loss: -0.6505470275878906
Batch 36/64 loss: -0.8129425048828125
Batch 37/64 loss: -0.8191909790039062
Batch 38/64 loss: -0.4931979179382324
Batch 39/64 loss: -0.5029816627502441
Batch 40/64 loss: -0.42244863510131836
Batch 41/64 loss: -0.6146154403686523
Batch 42/64 loss: -0.5454216003417969
Batch 43/64 loss: -0.5706591606140137
Batch 44/64 loss: -0.20320558547973633
Batch 45/64 loss: -0.5663375854492188
Batch 46/64 loss: -0.2995872497558594
Batch 47/64 loss: -0.7511405944824219
Batch 48/64 loss: -0.5415945053100586
Batch 49/64 loss: -0.31981849670410156
Batch 50/64 loss: -0.6844511032104492
Batch 51/64 loss: -0.25475215911865234
Batch 52/64 loss: -0.46778297424316406
Batch 53/64 loss: -0.270263671875
Batch 54/64 loss: -0.38951921463012695
Batch 55/64 loss: -0.5264606475830078
Batch 56/64 loss: -0.7614860534667969
Batch 57/64 loss: -0.3890199661254883
Batch 58/64 loss: -0.9450531005859375
Batch 59/64 loss: 0.12235546112060547
Batch 60/64 loss: -0.5109810829162598
Batch 61/64 loss: -0.8290843963623047
Batch 62/64 loss: -0.4722938537597656
Batch 63/64 loss: -0.5757389068603516
Batch 64/64 loss: -3.7748708724975586
Epoch 218  Train loss: -0.49092801037956685  Val loss: -0.6361491803041438
Epoch 219
-------------------------------
Batch 1/64 loss: -0.2248687744140625
Batch 2/64 loss: -0.5209217071533203
Batch 3/64 loss: -0.4008784294128418
Batch 4/64 loss: -0.7783823013305664
Batch 5/64 loss: -0.09395885467529297
Batch 6/64 loss: -0.41715526580810547
Batch 7/64 loss: -0.4643592834472656
Batch 8/64 loss: -0.40189647674560547
Batch 9/64 loss: 0.45900917053222656
Batch 10/64 loss: -0.8293180465698242
Batch 11/64 loss: -0.2951068878173828
Batch 12/64 loss: -0.8541164398193359
Batch 13/64 loss: -0.4119234085083008
Batch 14/64 loss: -0.4683561325073242
Batch 15/64 loss: -0.5257210731506348
Batch 16/64 loss: 0.35697174072265625
Batch 17/64 loss: -0.6200017929077148
Batch 18/64 loss: -0.6094045639038086
Batch 19/64 loss: -0.3134908676147461
Batch 20/64 loss: -0.08095598220825195
Batch 21/64 loss: -0.5371551513671875
Batch 22/64 loss: -0.2191929817199707
Batch 23/64 loss: -0.4078245162963867
Batch 24/64 loss: -0.46686840057373047
Batch 25/64 loss: 0.4343414306640625
Batch 26/64 loss: -0.9141340255737305
Batch 27/64 loss: -0.8164224624633789
Batch 28/64 loss: -0.6504964828491211
Batch 29/64 loss: -0.35182762145996094
Batch 30/64 loss: -0.534907341003418
Batch 31/64 loss: -0.4354095458984375
Batch 32/64 loss: -0.5358200073242188
Batch 33/64 loss: -0.6831197738647461
Batch 34/64 loss: -0.46335601806640625
Batch 35/64 loss: -0.7821321487426758
Batch 36/64 loss: -0.8348274230957031
Batch 37/64 loss: -0.2034435272216797
Batch 38/64 loss: -0.015613555908203125
Batch 39/64 loss: -0.30598926544189453
Batch 40/64 loss: -0.5647687911987305
Batch 41/64 loss: -0.43648815155029297
Batch 42/64 loss: -0.37035083770751953
Batch 43/64 loss: -0.5585527420043945
Batch 44/64 loss: -0.39476680755615234
Batch 45/64 loss: 0.08355140686035156
Batch 46/64 loss: -0.4209127426147461
Batch 47/64 loss: -0.23624229431152344
Batch 48/64 loss: -0.059978485107421875
Batch 49/64 loss: -0.5575046539306641
Batch 50/64 loss: -0.26842784881591797
Batch 51/64 loss: 0.8837699890136719
Batch 52/64 loss: -0.2731666564941406
Batch 53/64 loss: -0.33243560791015625
Batch 54/64 loss: -0.42870521545410156
Batch 55/64 loss: -0.4457721710205078
Batch 56/64 loss: -0.6234874725341797
Batch 57/64 loss: -0.6236085891723633
Batch 58/64 loss: -0.18188714981079102
Batch 59/64 loss: -0.24157428741455078
Batch 60/64 loss: -0.6547832489013672
Batch 61/64 loss: -0.6120648384094238
Batch 62/64 loss: -0.569979190826416
Batch 63/64 loss: -0.5269908905029297
Batch 64/64 loss: -4.042567729949951
Epoch 219  Train loss: -0.43397783952600816  Val loss: -0.5079020077420264
Epoch 220
-------------------------------
Batch 1/64 loss: -0.7146415710449219
Batch 2/64 loss: -0.7399196624755859
Batch 3/64 loss: -0.42914867401123047
Batch 4/64 loss: -0.6808624267578125
Batch 5/64 loss: -0.288330078125
Batch 6/64 loss: -0.49649715423583984
Batch 7/64 loss: -0.4240703582763672
Batch 8/64 loss: -0.479156494140625
Batch 9/64 loss: -0.5543766021728516
Batch 10/64 loss: -0.4107475280761719
Batch 11/64 loss: -0.6426734924316406
Batch 12/64 loss: 0.16936206817626953
Batch 13/64 loss: -0.23742008209228516
Batch 14/64 loss: -0.33156728744506836
Batch 15/64 loss: -0.30471372604370117
Batch 16/64 loss: -0.4916372299194336
Batch 17/64 loss: -0.4952840805053711
Batch 18/64 loss: -0.718475341796875
Batch 19/64 loss: -0.14775419235229492
Batch 20/64 loss: -0.5363626480102539
Batch 21/64 loss: -0.47863101959228516
Batch 22/64 loss: -0.6752719879150391
Batch 23/64 loss: -0.4685525894165039
Batch 24/64 loss: -0.5221385955810547
Batch 25/64 loss: -0.39486122131347656
Batch 26/64 loss: -0.29958534240722656
Batch 27/64 loss: -0.08045530319213867
Batch 28/64 loss: -0.23466968536376953
Batch 29/64 loss: -0.2164163589477539
Batch 30/64 loss: -0.04393768310546875
Batch 31/64 loss: -0.26543235778808594
Batch 32/64 loss: -0.5562019348144531
Batch 33/64 loss: -0.05822467803955078
Batch 34/64 loss: -0.12447214126586914
Batch 35/64 loss: -0.7755889892578125
Batch 36/64 loss: -0.3663821220397949
Batch 37/64 loss: -0.23783588409423828
Batch 38/64 loss: -0.5021438598632812
Batch 39/64 loss: -0.4598417282104492
Batch 40/64 loss: -0.47278404235839844
Batch 41/64 loss: -0.6688318252563477
Batch 42/64 loss: -0.43341922760009766
Batch 43/64 loss: -0.39649200439453125
Batch 44/64 loss: 0.23583126068115234
Batch 45/64 loss: -0.5989170074462891
Batch 46/64 loss: -0.4856696128845215
Batch 47/64 loss: -0.6937875747680664
Batch 48/64 loss: 0.15599584579467773
Batch 49/64 loss: -0.6445598602294922
Batch 50/64 loss: -0.5452766418457031
Batch 51/64 loss: -0.8835201263427734
Batch 52/64 loss: -0.7606821060180664
Batch 53/64 loss: -0.7697944641113281
Batch 54/64 loss: -0.17397594451904297
Batch 55/64 loss: -0.3236579895019531
Batch 56/64 loss: -0.3561711311340332
Batch 57/64 loss: -0.9899454116821289
Batch 58/64 loss: -0.5910429954528809
Batch 59/64 loss: -0.38976001739501953
Batch 60/64 loss: -0.3818655014038086
Batch 61/64 loss: -0.3069939613342285
Batch 62/64 loss: -0.7996625900268555
Batch 63/64 loss: -0.5965800285339355
Batch 64/64 loss: -3.7545857429504395
Epoch 220  Train loss: -0.47690074210073435  Val loss: -0.5792403073654961
Epoch 221
-------------------------------
Batch 1/64 loss: -0.10400056838989258
Batch 2/64 loss: -0.7969226837158203
Batch 3/64 loss: -0.5566139221191406
Batch 4/64 loss: -0.730562686920166
Batch 5/64 loss: -0.2751307487487793
Batch 6/64 loss: -0.21457290649414062
Batch 7/64 loss: -0.906743049621582
Batch 8/64 loss: -0.2159428596496582
Batch 9/64 loss: -0.19754600524902344
Batch 10/64 loss: -0.893733024597168
Batch 11/64 loss: -0.6600780487060547
Batch 12/64 loss: -0.6666908264160156
Batch 13/64 loss: -0.7445478439331055
Batch 14/64 loss: -0.9009685516357422
Batch 15/64 loss: -0.6358737945556641
Batch 16/64 loss: -0.6962652206420898
Batch 17/64 loss: -0.6048669815063477
Batch 18/64 loss: -0.8367223739624023
Batch 19/64 loss: -0.8229150772094727
Batch 20/64 loss: -0.37827491760253906
Batch 21/64 loss: -0.26383399963378906
Batch 22/64 loss: -0.6330280303955078
Batch 23/64 loss: -0.5312695503234863
Batch 24/64 loss: -0.6018257141113281
Batch 25/64 loss: -0.8841524124145508
Batch 26/64 loss: 0.16945552825927734
Batch 27/64 loss: -0.6283054351806641
Batch 28/64 loss: -0.49791812896728516
Batch 29/64 loss: -0.8586788177490234
Batch 30/64 loss: -0.5276432037353516
Batch 31/64 loss: -0.6732320785522461
Batch 32/64 loss: -0.49278831481933594
Batch 33/64 loss: -0.4652891159057617
Batch 34/64 loss: -0.6047639846801758
Batch 35/64 loss: -0.013571739196777344
Batch 36/64 loss: -0.3415074348449707
Batch 37/64 loss: -0.44142723083496094
Batch 38/64 loss: -0.5113487243652344
Batch 39/64 loss: -0.18651390075683594
Batch 40/64 loss: -0.4551377296447754
Batch 41/64 loss: -0.06880807876586914
Batch 42/64 loss: -0.16571998596191406
Batch 43/64 loss: -0.19819164276123047
Batch 44/64 loss: -0.17790603637695312
Batch 45/64 loss: -0.26285648345947266
Batch 46/64 loss: -0.5904865264892578
Batch 47/64 loss: -0.30357789993286133
Batch 48/64 loss: -0.4209451675415039
Batch 49/64 loss: -0.38529062271118164
Batch 50/64 loss: -0.7486600875854492
Batch 51/64 loss: -0.2402644157409668
Batch 52/64 loss: -0.6135292053222656
Batch 53/64 loss: -0.4743380546569824
Batch 54/64 loss: -0.41505908966064453
Batch 55/64 loss: -0.7143888473510742
Batch 56/64 loss: -0.7902145385742188
Batch 57/64 loss: -0.6380147933959961
Batch 58/64 loss: -0.3670682907104492
Batch 59/64 loss: 0.09576702117919922
Batch 60/64 loss: 0.18607282638549805
Batch 61/64 loss: -0.3459663391113281
Batch 62/64 loss: -0.31462764739990234
Batch 63/64 loss: -0.7642583847045898
Batch 64/64 loss: -3.767381191253662
Epoch 221  Train loss: -0.5149116908802706  Val loss: -0.6001991586586863
Epoch 222
-------------------------------
Batch 1/64 loss: -0.5018296241760254
Batch 2/64 loss: -0.24991130828857422
Batch 3/64 loss: -0.43325042724609375
Batch 4/64 loss: -0.4265317916870117
Batch 5/64 loss: -0.5281515121459961
Batch 6/64 loss: -0.42639970779418945
Batch 7/64 loss: -0.7800941467285156
Batch 8/64 loss: -0.7182168960571289
Batch 9/64 loss: -0.8819179534912109
Batch 10/64 loss: -0.48111486434936523
Batch 11/64 loss: -0.19353818893432617
Batch 12/64 loss: -0.5853137969970703
Batch 13/64 loss: -0.5699319839477539
Batch 14/64 loss: -0.7468719482421875
Batch 15/64 loss: -0.7028098106384277
Batch 16/64 loss: -0.1315460205078125
Batch 17/64 loss: -0.7114477157592773
Batch 18/64 loss: -0.06460094451904297
Batch 19/64 loss: -0.4577445983886719
Batch 20/64 loss: -0.4059324264526367
Batch 21/64 loss: -0.5823526382446289
Batch 22/64 loss: -0.043460845947265625
Batch 23/64 loss: -0.18145132064819336
Batch 24/64 loss: -0.034542083740234375
Batch 25/64 loss: -0.45751190185546875
Batch 26/64 loss: -0.43527984619140625
Batch 27/64 loss: 0.1908245086669922
Batch 28/64 loss: -0.8516616821289062
Batch 29/64 loss: -0.6119241714477539
Batch 30/64 loss: -0.5086145401000977
Batch 31/64 loss: -0.8312063217163086
Batch 32/64 loss: -0.48845720291137695
Batch 33/64 loss: -0.7889270782470703
Batch 34/64 loss: -0.36157941818237305
Batch 35/64 loss: 0.10467529296875
Batch 36/64 loss: -0.30037832260131836
Batch 37/64 loss: -0.5533981323242188
Batch 38/64 loss: -0.7616777420043945
Batch 39/64 loss: -0.7312536239624023
Batch 40/64 loss: -0.08648109436035156
Batch 41/64 loss: 0.7381858825683594
Batch 42/64 loss: 0.09142160415649414
Batch 43/64 loss: -0.7200827598571777
Batch 44/64 loss: -0.4550328254699707
Batch 45/64 loss: -0.25125789642333984
Batch 46/64 loss: -0.48447561264038086
Batch 47/64 loss: -0.2581601142883301
Batch 48/64 loss: -0.6365532875061035
Batch 49/64 loss: -0.7325649261474609
Batch 50/64 loss: -0.5585789680480957
Batch 51/64 loss: -0.2634577751159668
Batch 52/64 loss: -0.6216115951538086
Batch 53/64 loss: -0.5346617698669434
Batch 54/64 loss: -0.4002971649169922
Batch 55/64 loss: -0.6085653305053711
Batch 56/64 loss: 0.12317800521850586
Batch 57/64 loss: -0.6950807571411133
Batch 58/64 loss: -0.46478748321533203
Batch 59/64 loss: -0.9378995895385742
Batch 60/64 loss: -0.79815673828125
Batch 61/64 loss: -0.6058292388916016
Batch 62/64 loss: -0.5211324691772461
Batch 63/64 loss: -0.6890506744384766
Batch 64/64 loss: -4.317502498626709
Epoch 222  Train loss: -0.4993630147447773  Val loss: -0.6405128400350354
Epoch 223
-------------------------------
Batch 1/64 loss: -0.5441141128540039
Batch 2/64 loss: -0.8502445220947266
Batch 3/64 loss: -0.6427001953125
Batch 4/64 loss: -0.5799708366394043
Batch 5/64 loss: -0.5785536766052246
Batch 6/64 loss: -0.7443037033081055
Batch 7/64 loss: -0.6902055740356445
Batch 8/64 loss: -0.4479484558105469
Batch 9/64 loss: -0.612879753112793
Batch 10/64 loss: -0.21983957290649414
Batch 11/64 loss: -0.43616294860839844
Batch 12/64 loss: -0.6725387573242188
Batch 13/64 loss: -0.385561466217041
Batch 14/64 loss: -0.6432085037231445
Batch 15/64 loss: -0.31415271759033203
Batch 16/64 loss: -0.9048614501953125
Batch 17/64 loss: -0.23687314987182617
Batch 18/64 loss: -0.5307521820068359
Batch 19/64 loss: -0.4980325698852539
Batch 20/64 loss: -0.32433414459228516
Batch 21/64 loss: -0.20568275451660156
Batch 22/64 loss: -0.574183464050293
Batch 23/64 loss: -0.22774410247802734
Batch 24/64 loss: -0.76007080078125
Batch 25/64 loss: -0.6519813537597656
Batch 26/64 loss: -0.7283143997192383
Batch 27/64 loss: 0.10799217224121094
Batch 28/64 loss: -0.40041494369506836
Batch 29/64 loss: -0.4387359619140625
Batch 30/64 loss: -0.34693479537963867
Batch 31/64 loss: -0.08609867095947266
Batch 32/64 loss: -0.32230663299560547
Batch 33/64 loss: -0.2914462089538574
Batch 34/64 loss: -0.433474063873291
Batch 35/64 loss: -0.6468095779418945
Batch 36/64 loss: -0.36571264266967773
Batch 37/64 loss: -0.29195547103881836
Batch 38/64 loss: -0.262453556060791
Batch 39/64 loss: -0.6648435592651367
Batch 40/64 loss: -0.4432029724121094
Batch 41/64 loss: -0.40101146697998047
Batch 42/64 loss: -0.4557676315307617
Batch 43/64 loss: -0.9056320190429688
Batch 44/64 loss: -0.4443483352661133
Batch 45/64 loss: -0.7127046585083008
Batch 46/64 loss: -0.4477386474609375
Batch 47/64 loss: -0.8378000259399414
Batch 48/64 loss: -0.6129350662231445
Batch 49/64 loss: -0.5753259658813477
Batch 50/64 loss: -0.49416017532348633
Batch 51/64 loss: 0.09440326690673828
Batch 52/64 loss: -0.6296758651733398
Batch 53/64 loss: -0.5294303894042969
Batch 54/64 loss: -0.36653947830200195
Batch 55/64 loss: 0.39083290100097656
Batch 56/64 loss: -0.2419114112854004
Batch 57/64 loss: -0.44353437423706055
Batch 58/64 loss: -0.520416259765625
Batch 59/64 loss: -1.0165958404541016
Batch 60/64 loss: -0.32874536514282227
Batch 61/64 loss: -1.0678987503051758
Batch 62/64 loss: -0.6267623901367188
Batch 63/64 loss: -0.20960426330566406
Batch 64/64 loss: -3.920948028564453
Epoch 223  Train loss: -0.521029416252585  Val loss: -0.6382180308967931
Epoch 224
-------------------------------
Batch 1/64 loss: -0.7584781646728516
Batch 2/64 loss: -0.8629436492919922
Batch 3/64 loss: -0.6776766777038574
Batch 4/64 loss: -0.30649232864379883
Batch 5/64 loss: -0.7756538391113281
Batch 6/64 loss: -0.5246152877807617
Batch 7/64 loss: -0.5798015594482422
Batch 8/64 loss: -0.5623054504394531
Batch 9/64 loss: 0.17421579360961914
Batch 10/64 loss: -0.5536222457885742
Batch 11/64 loss: -0.772221565246582
Batch 12/64 loss: -0.5219907760620117
Batch 13/64 loss: -0.6029071807861328
Batch 14/64 loss: -0.44829559326171875
Batch 15/64 loss: -0.616884708404541
Batch 16/64 loss: -0.5387988090515137
Batch 17/64 loss: -0.7676973342895508
Batch 18/64 loss: -0.8103694915771484
Batch 19/64 loss: -0.3224625587463379
Batch 20/64 loss: -0.6495914459228516
Batch 21/64 loss: -0.7086143493652344
Batch 22/64 loss: -0.7211875915527344
Batch 23/64 loss: -0.019135475158691406
Batch 24/64 loss: -0.043233394622802734
Batch 25/64 loss: -0.5488443374633789
Batch 26/64 loss: -0.600924015045166
Batch 27/64 loss: -0.5662236213684082
Batch 28/64 loss: -0.5992188453674316
Batch 29/64 loss: -0.3335075378417969
Batch 30/64 loss: 0.554896354675293
Batch 31/64 loss: -0.6986541748046875
Batch 32/64 loss: -0.7517309188842773
Batch 33/64 loss: -0.26742982864379883
Batch 34/64 loss: -0.587672233581543
Batch 35/64 loss: -0.47910594940185547
Batch 36/64 loss: -0.7744722366333008
Batch 37/64 loss: 0.03352785110473633
Batch 38/64 loss: -0.39375877380371094
Batch 39/64 loss: -0.4235048294067383
Batch 40/64 loss: -0.06250953674316406
Batch 41/64 loss: -0.5221986770629883
Batch 42/64 loss: -0.5440802574157715
Batch 43/64 loss: 0.022390365600585938
Batch 44/64 loss: -0.6539764404296875
Batch 45/64 loss: -0.3094487190246582
Batch 46/64 loss: -0.5995521545410156
Batch 47/64 loss: -0.8427581787109375
Batch 48/64 loss: -0.07088613510131836
Batch 49/64 loss: -0.030682086944580078
Batch 50/64 loss: -0.276547908782959
Batch 51/64 loss: -0.6318140029907227
Batch 52/64 loss: -0.3938145637512207
Batch 53/64 loss: -0.5195455551147461
Batch 54/64 loss: -0.2376565933227539
Batch 55/64 loss: -0.09185457229614258
Batch 56/64 loss: -0.17029571533203125
Batch 57/64 loss: -0.8440780639648438
Batch 58/64 loss: -0.2607421875
Batch 59/64 loss: -0.21187734603881836
Batch 60/64 loss: 0.32557201385498047
Batch 61/64 loss: -0.2229466438293457
Batch 62/64 loss: -0.6344685554504395
Batch 63/64 loss: -0.8343114852905273
Batch 64/64 loss: -3.463559627532959
Epoch 224  Train loss: -0.4803629725587134  Val loss: -0.602702976502094
Epoch 225
-------------------------------
Batch 1/64 loss: -0.19226551055908203
Batch 2/64 loss: -0.33568811416625977
Batch 3/64 loss: -0.33136463165283203
Batch 4/64 loss: -0.3080458641052246
Batch 5/64 loss: -0.5800280570983887
Batch 6/64 loss: -0.6801071166992188
Batch 7/64 loss: -0.3346891403198242
Batch 8/64 loss: -0.40479516983032227
Batch 9/64 loss: -0.7640285491943359
Batch 10/64 loss: -0.8293046951293945
Batch 11/64 loss: -0.26161766052246094
Batch 12/64 loss: -0.5657734870910645
Batch 13/64 loss: -0.4092388153076172
Batch 14/64 loss: -0.6460838317871094
Batch 15/64 loss: -0.7023305892944336
Batch 16/64 loss: -0.5435075759887695
Batch 17/64 loss: -0.48087310791015625
Batch 18/64 loss: -0.14748430252075195
Batch 19/64 loss: -0.39644432067871094
Batch 20/64 loss: -0.5687661170959473
Batch 21/64 loss: -0.581756591796875
Batch 22/64 loss: -0.909663200378418
Batch 23/64 loss: -0.3895092010498047
Batch 24/64 loss: -0.04714155197143555
Batch 25/64 loss: -0.3557882308959961
Batch 26/64 loss: -0.6690998077392578
Batch 27/64 loss: -0.610931396484375
Batch 28/64 loss: -0.8432397842407227
Batch 29/64 loss: -0.7180862426757812
Batch 30/64 loss: -0.3504524230957031
Batch 31/64 loss: -0.6968326568603516
Batch 32/64 loss: -0.48571062088012695
Batch 33/64 loss: -0.7040166854858398
Batch 34/64 loss: -0.6957721710205078
Batch 35/64 loss: -0.6425037384033203
Batch 36/64 loss: 0.03285980224609375
Batch 37/64 loss: -0.285552978515625
Batch 38/64 loss: -0.6069250106811523
Batch 39/64 loss: -0.4941596984863281
Batch 40/64 loss: -0.12613773345947266
Batch 41/64 loss: -0.2582516670227051
Batch 42/64 loss: -0.5198416709899902
Batch 43/64 loss: -0.66357421875
Batch 44/64 loss: -0.25089311599731445
Batch 45/64 loss: -0.4763054847717285
Batch 46/64 loss: -0.5993971824645996
Batch 47/64 loss: -0.12260723114013672
Batch 48/64 loss: -0.4262666702270508
Batch 49/64 loss: -0.47658729553222656
Batch 50/64 loss: -0.35257387161254883
Batch 51/64 loss: 0.28424739837646484
Batch 52/64 loss: -0.27953624725341797
Batch 53/64 loss: -0.2762022018432617
Batch 54/64 loss: -0.6302013397216797
Batch 55/64 loss: -0.3374648094177246
Batch 56/64 loss: -0.5045895576477051
Batch 57/64 loss: -0.6121597290039062
Batch 58/64 loss: 0.0002980232238769531
Batch 59/64 loss: -0.26417016983032227
Batch 60/64 loss: -0.23385906219482422
Batch 61/64 loss: -0.488558292388916
Batch 62/64 loss: -0.3869171142578125
Batch 63/64 loss: 0.11844730377197266
Batch 64/64 loss: -4.005619049072266
Epoch 225  Train loss: -0.4772397658404182  Val loss: -0.552561795998275
Epoch 226
-------------------------------
Batch 1/64 loss: -0.31464672088623047
Batch 2/64 loss: -0.5820932388305664
Batch 3/64 loss: -0.6325206756591797
Batch 4/64 loss: -0.8576302528381348
Batch 5/64 loss: -0.714838981628418
Batch 6/64 loss: -0.4741549491882324
Batch 7/64 loss: -0.4514131546020508
Batch 8/64 loss: -0.23070192337036133
Batch 9/64 loss: -0.654388427734375
Batch 10/64 loss: -0.8561496734619141
Batch 11/64 loss: -0.1979990005493164
Batch 12/64 loss: -0.342529296875
Batch 13/64 loss: -0.6728887557983398
Batch 14/64 loss: -0.17252588272094727
Batch 15/64 loss: -0.5706090927124023
Batch 16/64 loss: -0.6697778701782227
Batch 17/64 loss: -0.027610301971435547
Batch 18/64 loss: -0.34528446197509766
Batch 19/64 loss: -0.3074989318847656
Batch 20/64 loss: -0.19480133056640625
Batch 21/64 loss: -0.5959033966064453
Batch 22/64 loss: -0.13469362258911133
Batch 23/64 loss: -0.5980386734008789
Batch 24/64 loss: -0.6165761947631836
Batch 25/64 loss: -0.6082344055175781
Batch 26/64 loss: -0.8000717163085938
Batch 27/64 loss: -0.691770076751709
Batch 28/64 loss: -0.6508240699768066
Batch 29/64 loss: -0.7497291564941406
Batch 30/64 loss: -0.6020126342773438
Batch 31/64 loss: -0.5833992958068848
Batch 32/64 loss: -0.4212174415588379
Batch 33/64 loss: -0.17727279663085938
Batch 34/64 loss: -0.5222387313842773
Batch 35/64 loss: -0.1198420524597168
Batch 36/64 loss: -0.618321418762207
Batch 37/64 loss: -0.32943105697631836
Batch 38/64 loss: -0.5328021049499512
Batch 39/64 loss: -0.22483444213867188
Batch 40/64 loss: -0.2805023193359375
Batch 41/64 loss: -0.42584753036499023
Batch 42/64 loss: -0.25158119201660156
Batch 43/64 loss: -0.9305553436279297
Batch 44/64 loss: -0.28671932220458984
Batch 45/64 loss: -0.3882942199707031
Batch 46/64 loss: -0.2144312858581543
Batch 47/64 loss: -0.6370582580566406
Batch 48/64 loss: -0.5857434272766113
Batch 49/64 loss: -0.4434843063354492
Batch 50/64 loss: -0.3519768714904785
Batch 51/64 loss: -0.2348175048828125
Batch 52/64 loss: -0.6764307022094727
Batch 53/64 loss: -0.5867757797241211
Batch 54/64 loss: -0.26490020751953125
Batch 55/64 loss: -0.5071625709533691
Batch 56/64 loss: -0.49851036071777344
Batch 57/64 loss: -0.5956926345825195
Batch 58/64 loss: -0.7292633056640625
Batch 59/64 loss: -0.5570087432861328
Batch 60/64 loss: -0.2085871696472168
Batch 61/64 loss: -0.6460180282592773
Batch 62/64 loss: -0.27419233322143555
Batch 63/64 loss: -0.5272464752197266
Batch 64/64 loss: -4.222512722015381
Epoch 226  Train loss: -0.5194817356034821  Val loss: -0.4705148546146773
Epoch 227
-------------------------------
Batch 1/64 loss: -0.2849717140197754
Batch 2/64 loss: -0.749481201171875
Batch 3/64 loss: -0.6799225807189941
Batch 4/64 loss: -0.5195407867431641
Batch 5/64 loss: -0.756688117980957
Batch 6/64 loss: -0.9222936630249023
Batch 7/64 loss: -0.24053192138671875
Batch 8/64 loss: -0.5080151557922363
Batch 9/64 loss: -0.5171241760253906
Batch 10/64 loss: -0.4868464469909668
Batch 11/64 loss: -0.6954193115234375
Batch 12/64 loss: -0.7774324417114258
Batch 13/64 loss: -0.2386646270751953
Batch 14/64 loss: -0.6515007019042969
Batch 15/64 loss: 0.4795408248901367
Batch 16/64 loss: -0.47395896911621094
Batch 17/64 loss: -0.43109989166259766
Batch 18/64 loss: -0.7296304702758789
Batch 19/64 loss: -0.3599529266357422
Batch 20/64 loss: -0.2832150459289551
Batch 21/64 loss: -0.05727338790893555
Batch 22/64 loss: -0.5222043991088867
Batch 23/64 loss: -0.006969928741455078
Batch 24/64 loss: -0.5601844787597656
Batch 25/64 loss: 0.21045351028442383
Batch 26/64 loss: -0.7841672897338867
Batch 27/64 loss: -0.446441650390625
Batch 28/64 loss: -0.23668193817138672
Batch 29/64 loss: -0.4740781784057617
Batch 30/64 loss: -0.19544649124145508
Batch 31/64 loss: -0.5065059661865234
Batch 32/64 loss: -0.3408365249633789
Batch 33/64 loss: -0.34688472747802734
Batch 34/64 loss: 0.06410074234008789
Batch 35/64 loss: -0.37946510314941406
Batch 36/64 loss: -0.3388795852661133
Batch 37/64 loss: -0.4726748466491699
Batch 38/64 loss: 0.02769947052001953
Batch 39/64 loss: 0.15163850784301758
Batch 40/64 loss: -0.6205978393554688
Batch 41/64 loss: -0.3863663673400879
Batch 42/64 loss: -0.5498600006103516
Batch 43/64 loss: -0.35634422302246094
Batch 44/64 loss: -0.28238391876220703
Batch 45/64 loss: -0.5663518905639648
Batch 46/64 loss: -0.4042625427246094
Batch 47/64 loss: -0.694068431854248
Batch 48/64 loss: -0.05994367599487305
Batch 49/64 loss: -0.6221756935119629
Batch 50/64 loss: -0.18901586532592773
Batch 51/64 loss: -0.4844813346862793
Batch 52/64 loss: -0.6701421737670898
Batch 53/64 loss: -0.3463602066040039
Batch 54/64 loss: -0.6674938201904297
Batch 55/64 loss: -0.04872322082519531
Batch 56/64 loss: -0.18497180938720703
Batch 57/64 loss: -0.2537412643432617
Batch 58/64 loss: 0.24060487747192383
Batch 59/64 loss: -0.6021218299865723
Batch 60/64 loss: -0.3554816246032715
Batch 61/64 loss: -0.8320736885070801
Batch 62/64 loss: -0.5423741340637207
Batch 63/64 loss: -0.9969730377197266
Batch 64/64 loss: -4.340139389038086
Epoch 227  Train loss: -0.45133113487094056  Val loss: -0.5734533460689164
Epoch 228
-------------------------------
Batch 1/64 loss: -0.5640358924865723
Batch 2/64 loss: -0.6546916961669922
Batch 3/64 loss: -0.29124021530151367
Batch 4/64 loss: -0.5851364135742188
Batch 5/64 loss: -0.39166831970214844
Batch 6/64 loss: -0.7081642150878906
Batch 7/64 loss: -0.23564863204956055
Batch 8/64 loss: -0.4476504325866699
Batch 9/64 loss: -0.3484668731689453
Batch 10/64 loss: -0.39121007919311523
Batch 11/64 loss: 0.8532280921936035
Batch 12/64 loss: -0.3574790954589844
Batch 13/64 loss: -0.5845751762390137
Batch 14/64 loss: -0.6218986511230469
Batch 15/64 loss: -0.4833087921142578
Batch 16/64 loss: -0.7693424224853516
Batch 17/64 loss: -0.5994138717651367
Batch 18/64 loss: -0.4605550765991211
Batch 19/64 loss: -0.4020366668701172
Batch 20/64 loss: -0.4802875518798828
Batch 21/64 loss: -0.5269293785095215
Batch 22/64 loss: -0.4140758514404297
Batch 23/64 loss: -0.3464164733886719
Batch 24/64 loss: -0.2860383987426758
Batch 25/64 loss: -0.42377138137817383
Batch 26/64 loss: -0.373504638671875
Batch 27/64 loss: -0.324432373046875
Batch 28/64 loss: -0.7431774139404297
Batch 29/64 loss: -0.4273524284362793
Batch 30/64 loss: 0.3703174591064453
Batch 31/64 loss: -0.6364660263061523
Batch 32/64 loss: -0.7323856353759766
Batch 33/64 loss: -0.7237973213195801
Batch 34/64 loss: -0.6416950225830078
Batch 35/64 loss: -0.1970229148864746
Batch 36/64 loss: -0.8936266899108887
Batch 37/64 loss: -0.4094095230102539
Batch 38/64 loss: -0.7208690643310547
Batch 39/64 loss: -0.7088894844055176
Batch 40/64 loss: -0.45749473571777344
Batch 41/64 loss: -0.6830687522888184
Batch 42/64 loss: -0.7009744644165039
Batch 43/64 loss: -0.29933643341064453
Batch 44/64 loss: -0.009815216064453125
Batch 45/64 loss: -0.3635740280151367
Batch 46/64 loss: -0.6215982437133789
Batch 47/64 loss: -0.8274803161621094
Batch 48/64 loss: -0.741729736328125
Batch 49/64 loss: -0.2579774856567383
Batch 50/64 loss: -0.3831462860107422
Batch 51/64 loss: -0.7809591293334961
Batch 52/64 loss: -0.5235939025878906
Batch 53/64 loss: -0.9473915100097656
Batch 54/64 loss: -0.48665952682495117
Batch 55/64 loss: -0.3095841407775879
Batch 56/64 loss: -0.4064640998840332
Batch 57/64 loss: -0.6615452766418457
Batch 58/64 loss: -0.4960169792175293
Batch 59/64 loss: -0.6738862991333008
Batch 60/64 loss: -0.40375852584838867
Batch 61/64 loss: -0.7574281692504883
Batch 62/64 loss: -0.806239128112793
Batch 63/64 loss: 0.5288195610046387
Batch 64/64 loss: -4.386524677276611
Epoch 228  Train loss: -0.5183360137191473  Val loss: -0.5996698857992375
Epoch 229
-------------------------------
Batch 1/64 loss: -0.08409595489501953
Batch 2/64 loss: -0.7945594787597656
Batch 3/64 loss: -0.7523984909057617
Batch 4/64 loss: -0.6228013038635254
Batch 5/64 loss: -0.755615234375
Batch 6/64 loss: -0.08068323135375977
Batch 7/64 loss: -0.5553603172302246
Batch 8/64 loss: -0.3276500701904297
Batch 9/64 loss: -0.3048739433288574
Batch 10/64 loss: -0.4064450263977051
Batch 11/64 loss: -0.045470237731933594
Batch 12/64 loss: -0.6846485137939453
Batch 13/64 loss: -0.026305198669433594
Batch 14/64 loss: 0.07599115371704102
Batch 15/64 loss: 0.022788524627685547
Batch 16/64 loss: -0.5271530151367188
Batch 17/64 loss: -0.3700861930847168
Batch 18/64 loss: -0.6086587905883789
Batch 19/64 loss: -0.38326549530029297
Batch 20/64 loss: -0.8363237380981445
Batch 21/64 loss: -0.07584381103515625
Batch 22/64 loss: -0.648341178894043
Batch 23/64 loss: -0.6995944976806641
Batch 24/64 loss: -0.5590057373046875
Batch 25/64 loss: -0.41640806198120117
Batch 26/64 loss: -0.7631158828735352
Batch 27/64 loss: -0.2948312759399414
Batch 28/64 loss: -0.938990592956543
Batch 29/64 loss: -0.15074920654296875
Batch 30/64 loss: -0.559910774230957
Batch 31/64 loss: -0.06370162963867188
Batch 32/64 loss: -0.7439589500427246
Batch 33/64 loss: -0.12154483795166016
Batch 34/64 loss: -0.5490851402282715
Batch 35/64 loss: -0.40993499755859375
Batch 36/64 loss: 0.07453298568725586
Batch 37/64 loss: -0.4770808219909668
Batch 38/64 loss: -0.641204833984375
Batch 39/64 loss: -0.8554830551147461
Batch 40/64 loss: -0.6153173446655273
Batch 41/64 loss: -0.6224536895751953
Batch 42/64 loss: -0.18757057189941406
Batch 43/64 loss: -0.7968254089355469
Batch 44/64 loss: -0.4176931381225586
Batch 45/64 loss: -0.35429859161376953
Batch 46/64 loss: -0.6402664184570312
Batch 47/64 loss: -0.7051486968994141
Batch 48/64 loss: -0.4738287925720215
Batch 49/64 loss: -0.6316089630126953
Batch 50/64 loss: -0.8875064849853516
Batch 51/64 loss: -0.31720733642578125
Batch 52/64 loss: -0.9210672378540039
Batch 53/64 loss: -0.9244871139526367
Batch 54/64 loss: -0.059828758239746094
Batch 55/64 loss: -0.35033464431762695
Batch 56/64 loss: -0.5144166946411133
Batch 57/64 loss: -0.4541921615600586
Batch 58/64 loss: -0.5998811721801758
Batch 59/64 loss: -0.6862983703613281
Batch 60/64 loss: 0.13748550415039062
Batch 61/64 loss: -0.4974994659423828
Batch 62/64 loss: -0.4564247131347656
Batch 63/64 loss: -0.7134494781494141
Batch 64/64 loss: -4.295938014984131
Epoch 229  Train loss: -0.5156696487875546  Val loss: -0.6949463414982012
Saving best model, epoch: 229
Epoch 230
-------------------------------
Batch 1/64 loss: -0.47208309173583984
Batch 2/64 loss: -0.6445159912109375
Batch 3/64 loss: -0.5272188186645508
Batch 4/64 loss: -0.4166450500488281
Batch 5/64 loss: -0.49349403381347656
Batch 6/64 loss: -0.6121006011962891
Batch 7/64 loss: -0.9396486282348633
Batch 8/64 loss: -0.4371500015258789
Batch 9/64 loss: -0.7887086868286133
Batch 10/64 loss: -0.4398174285888672
Batch 11/64 loss: -0.5074987411499023
Batch 12/64 loss: -0.770848274230957
Batch 13/64 loss: -0.46651649475097656
Batch 14/64 loss: -0.6663322448730469
Batch 15/64 loss: -0.5681953430175781
Batch 16/64 loss: -0.8279895782470703
Batch 17/64 loss: -0.2087078094482422
Batch 18/64 loss: -0.37649011611938477
Batch 19/64 loss: -0.9226770401000977
Batch 20/64 loss: -0.2794971466064453
Batch 21/64 loss: -0.7550449371337891
Batch 22/64 loss: -0.3825082778930664
Batch 23/64 loss: -0.3126106262207031
Batch 24/64 loss: -0.4082450866699219
Batch 25/64 loss: -0.8128223419189453
Batch 26/64 loss: -0.2933039665222168
Batch 27/64 loss: -0.27446842193603516
Batch 28/64 loss: 0.36791181564331055
Batch 29/64 loss: -0.2512507438659668
Batch 30/64 loss: -0.6893997192382812
Batch 31/64 loss: 0.23523664474487305
Batch 32/64 loss: -0.6852750778198242
Batch 33/64 loss: -0.7107257843017578
Batch 34/64 loss: -0.6681833267211914
Batch 35/64 loss: -0.34156036376953125
Batch 36/64 loss: -0.45281362533569336
Batch 37/64 loss: -0.36250877380371094
Batch 38/64 loss: -0.34466123580932617
Batch 39/64 loss: -0.6401338577270508
Batch 40/64 loss: -0.3205890655517578
Batch 41/64 loss: -0.5256795883178711
Batch 42/64 loss: -0.4418601989746094
Batch 43/64 loss: -0.6149187088012695
Batch 44/64 loss: -0.5417575836181641
Batch 45/64 loss: -0.3373680114746094
Batch 46/64 loss: -0.03666257858276367
Batch 47/64 loss: -0.7890338897705078
Batch 48/64 loss: -0.4792184829711914
Batch 49/64 loss: -0.4626779556274414
Batch 50/64 loss: -0.33833789825439453
Batch 51/64 loss: -0.4283714294433594
Batch 52/64 loss: -0.5738649368286133
Batch 53/64 loss: -0.14368343353271484
Batch 54/64 loss: -0.7266387939453125
Batch 55/64 loss: -0.5464744567871094
Batch 56/64 loss: -0.16836214065551758
Batch 57/64 loss: -0.21184825897216797
Batch 58/64 loss: -0.29584503173828125
Batch 59/64 loss: -0.9082889556884766
Batch 60/64 loss: -0.4080672264099121
Batch 61/64 loss: 0.25699806213378906
Batch 62/64 loss: -0.6001176834106445
Batch 63/64 loss: -0.5157451629638672
Batch 64/64 loss: -4.6016340255737305
Epoch 230  Train loss: -0.513853204016592  Val loss: -0.5254733947544163
Epoch 231
-------------------------------
Batch 1/64 loss: -0.5560479164123535
Batch 2/64 loss: -0.4095573425292969
Batch 3/64 loss: -0.5637912750244141
Batch 4/64 loss: -0.5475673675537109
Batch 5/64 loss: -0.4474015235900879
Batch 6/64 loss: -0.5613002777099609
Batch 7/64 loss: -0.5269250869750977
Batch 8/64 loss: -0.6828241348266602
Batch 9/64 loss: -0.5391921997070312
Batch 10/64 loss: -0.22436809539794922
Batch 11/64 loss: -0.25765037536621094
Batch 12/64 loss: -0.13782835006713867
Batch 13/64 loss: -0.5050420761108398
Batch 14/64 loss: -0.557551383972168
Batch 15/64 loss: -0.42367124557495117
Batch 16/64 loss: -0.5291271209716797
Batch 17/64 loss: -0.6654739379882812
Batch 18/64 loss: -0.6068744659423828
Batch 19/64 loss: -0.7524166107177734
Batch 20/64 loss: -0.3087148666381836
Batch 21/64 loss: -0.4137134552001953
Batch 22/64 loss: -0.9519510269165039
Batch 23/64 loss: -0.5904922485351562
Batch 24/64 loss: -1.0848875045776367
Batch 25/64 loss: -0.6147861480712891
Batch 26/64 loss: -0.13232040405273438
Batch 27/64 loss: -0.5874576568603516
Batch 28/64 loss: -0.642789363861084
Batch 29/64 loss: -0.29308366775512695
Batch 30/64 loss: -0.6890535354614258
Batch 31/64 loss: -0.5831966400146484
Batch 32/64 loss: -0.6154489517211914
Batch 33/64 loss: -0.4186258316040039
Batch 34/64 loss: -0.5362749099731445
Batch 35/64 loss: -0.6825399398803711
Batch 36/64 loss: -0.14959287643432617
Batch 37/64 loss: -0.7530899047851562
Batch 38/64 loss: -0.6577425003051758
Batch 39/64 loss: -0.9042801856994629
Batch 40/64 loss: -0.33341312408447266
Batch 41/64 loss: -0.4447898864746094
Batch 42/64 loss: -0.5907402038574219
Batch 43/64 loss: -0.5256328582763672
Batch 44/64 loss: 0.1617717742919922
Batch 45/64 loss: -0.2812948226928711
Batch 46/64 loss: -0.7346715927124023
Batch 47/64 loss: -0.10703039169311523
Batch 48/64 loss: -0.23482370376586914
Batch 49/64 loss: -0.43970203399658203
Batch 50/64 loss: -0.21116161346435547
Batch 51/64 loss: -0.8921833038330078
Batch 52/64 loss: -0.6609897613525391
Batch 53/64 loss: -0.7432003021240234
Batch 54/64 loss: -0.867828369140625
Batch 55/64 loss: -0.6447019577026367
Batch 56/64 loss: -0.2795596122741699
Batch 57/64 loss: -0.15065574645996094
Batch 58/64 loss: -0.1507549285888672
Batch 59/64 loss: -0.5035066604614258
Batch 60/64 loss: -0.2643294334411621
Batch 61/64 loss: -0.38452911376953125
Batch 62/64 loss: -0.4445686340332031
Batch 63/64 loss: -0.571629524230957
Batch 64/64 loss: -4.429529190063477
Epoch 231  Train loss: -0.5447329427681717  Val loss: -0.6122217473295546
Epoch 232
-------------------------------
Batch 1/64 loss: -0.08515453338623047
Batch 2/64 loss: -0.26474428176879883
Batch 3/64 loss: -0.7092428207397461
Batch 4/64 loss: -0.5223073959350586
Batch 5/64 loss: -0.2386302947998047
Batch 6/64 loss: 0.12194156646728516
Batch 7/64 loss: -0.633671760559082
Batch 8/64 loss: -0.04985618591308594
Batch 9/64 loss: -0.5762720108032227
Batch 10/64 loss: -0.27774906158447266
Batch 11/64 loss: -0.5652303695678711
Batch 12/64 loss: -0.7314729690551758
Batch 13/64 loss: -0.28286218643188477
Batch 14/64 loss: -0.4969043731689453
Batch 15/64 loss: -0.4235544204711914
Batch 16/64 loss: -0.12458229064941406
Batch 17/64 loss: -0.7677803039550781
Batch 18/64 loss: -0.5389904975891113
Batch 19/64 loss: -0.7687168121337891
Batch 20/64 loss: -0.36495065689086914
Batch 21/64 loss: -0.5424966812133789
Batch 22/64 loss: -0.6864109039306641
Batch 23/64 loss: -0.3959646224975586
Batch 24/64 loss: -0.23479509353637695
Batch 25/64 loss: -0.2774953842163086
Batch 26/64 loss: -0.3783555030822754
Batch 27/64 loss: -0.279632568359375
Batch 28/64 loss: -0.7192001342773438
Batch 29/64 loss: -0.2560606002807617
Batch 30/64 loss: -0.18198156356811523
Batch 31/64 loss: -0.4768519401550293
Batch 32/64 loss: -0.2874932289123535
Batch 33/64 loss: -0.3219141960144043
Batch 34/64 loss: -0.4336824417114258
Batch 35/64 loss: 0.0550074577331543
Batch 36/64 loss: -0.6950645446777344
Batch 37/64 loss: -0.7955417633056641
Batch 38/64 loss: -0.2694683074951172
Batch 39/64 loss: -0.3182802200317383
Batch 40/64 loss: -0.5009355545043945
Batch 41/64 loss: -0.51629638671875
Batch 42/64 loss: 0.14124822616577148
Batch 43/64 loss: -0.5150270462036133
Batch 44/64 loss: -0.6823453903198242
Batch 45/64 loss: -0.7916069030761719
Batch 46/64 loss: -0.728337287902832
Batch 47/64 loss: -0.5918817520141602
Batch 48/64 loss: -0.9566564559936523
Batch 49/64 loss: -0.28321313858032227
Batch 50/64 loss: -0.5107660293579102
Batch 51/64 loss: -0.41030168533325195
Batch 52/64 loss: -0.8338842391967773
Batch 53/64 loss: -0.9009265899658203
Batch 54/64 loss: -0.7647571563720703
Batch 55/64 loss: -0.703526496887207
Batch 56/64 loss: -0.9501466751098633
Batch 57/64 loss: -0.44411182403564453
Batch 58/64 loss: -0.2967801094055176
Batch 59/64 loss: -0.4988088607788086
Batch 60/64 loss: -0.6283831596374512
Batch 61/64 loss: -0.4100189208984375
Batch 62/64 loss: -0.4299430847167969
Batch 63/64 loss: -0.042729854583740234
Batch 64/64 loss: -4.346778869628906
Epoch 232  Train loss: -0.506770736095952  Val loss: -0.6595436305934211
Epoch 233
-------------------------------
Batch 1/64 loss: -0.5075936317443848
Batch 2/64 loss: -0.1670055389404297
Batch 3/64 loss: -0.3325214385986328
Batch 4/64 loss: -0.7024731636047363
Batch 5/64 loss: -0.7858438491821289
Batch 6/64 loss: -0.01676034927368164
Batch 7/64 loss: -0.65924072265625
Batch 8/64 loss: -0.4433164596557617
Batch 9/64 loss: -0.839777946472168
Batch 10/64 loss: -0.701202392578125
Batch 11/64 loss: -0.8236818313598633
Batch 12/64 loss: -0.20411968231201172
Batch 13/64 loss: -0.5951042175292969
Batch 14/64 loss: -0.5643405914306641
Batch 15/64 loss: -0.8037967681884766
Batch 16/64 loss: -0.6427507400512695
Batch 17/64 loss: -0.3636512756347656
Batch 18/64 loss: -0.7941122055053711
Batch 19/64 loss: -0.1235055923461914
Batch 20/64 loss: -0.3503913879394531
Batch 21/64 loss: -0.5643196105957031
Batch 22/64 loss: -0.9381551742553711
Batch 23/64 loss: -0.5830621719360352
Batch 24/64 loss: -0.5360822677612305
Batch 25/64 loss: -0.6197471618652344
Batch 26/64 loss: -0.2422184944152832
Batch 27/64 loss: -0.7090082168579102
Batch 28/64 loss: -0.14296627044677734
Batch 29/64 loss: -0.18244552612304688
Batch 30/64 loss: -0.20648670196533203
Batch 31/64 loss: -0.30840063095092773
Batch 32/64 loss: -0.3599252700805664
Batch 33/64 loss: -0.35494041442871094
Batch 34/64 loss: -0.5982980728149414
Batch 35/64 loss: -0.6417675018310547
Batch 36/64 loss: -0.4626493453979492
Batch 37/64 loss: -0.6285820007324219
Batch 38/64 loss: -0.30353736877441406
Batch 39/64 loss: -0.23332548141479492
Batch 40/64 loss: -0.361295223236084
Batch 41/64 loss: -0.5584158897399902
Batch 42/64 loss: -0.2002415657043457
Batch 43/64 loss: -0.33504724502563477
Batch 44/64 loss: -0.5568399429321289
Batch 45/64 loss: -0.5022125244140625
Batch 46/64 loss: -0.6666946411132812
Batch 47/64 loss: -0.7087802886962891
Batch 48/64 loss: -0.6663198471069336
Batch 49/64 loss: -0.42665624618530273
Batch 50/64 loss: -0.5707874298095703
Batch 51/64 loss: -0.5298476219177246
Batch 52/64 loss: -0.2915802001953125
Batch 53/64 loss: -0.349884033203125
Batch 54/64 loss: -0.4419698715209961
Batch 55/64 loss: -0.8288936614990234
Batch 56/64 loss: -0.8733177185058594
Batch 57/64 loss: -0.3575582504272461
Batch 58/64 loss: -0.6159229278564453
Batch 59/64 loss: -0.20093297958374023
Batch 60/64 loss: -0.4137411117553711
Batch 61/64 loss: -0.4569821357727051
Batch 62/64 loss: -0.5324230194091797
Batch 63/64 loss: -0.6009674072265625
Batch 64/64 loss: -4.456568241119385
Epoch 233  Train loss: -0.5400289479423972  Val loss: -0.533783208053956
Epoch 234
-------------------------------
Batch 1/64 loss: -0.612919807434082
Batch 2/64 loss: -0.4274892807006836
Batch 3/64 loss: 0.10874414443969727
Batch 4/64 loss: -0.2905607223510742
Batch 5/64 loss: -0.6546649932861328
Batch 6/64 loss: -0.43410682678222656
Batch 7/64 loss: 0.11468696594238281
Batch 8/64 loss: -0.5848603248596191
Batch 9/64 loss: -0.3632059097290039
Batch 10/64 loss: -0.5066862106323242
Batch 11/64 loss: -0.9501857757568359
Batch 12/64 loss: -0.6234407424926758
Batch 13/64 loss: -0.8053455352783203
Batch 14/64 loss: -0.5514535903930664
Batch 15/64 loss: -0.20634937286376953
Batch 16/64 loss: -0.6736316680908203
Batch 17/64 loss: -0.9005270004272461
Batch 18/64 loss: -0.3599724769592285
Batch 19/64 loss: -0.7146234512329102
Batch 20/64 loss: -0.6575727462768555
Batch 21/64 loss: -0.2990121841430664
Batch 22/64 loss: -0.33347463607788086
Batch 23/64 loss: -0.7124309539794922
Batch 24/64 loss: -0.5651884078979492
Batch 25/64 loss: -0.008873462677001953
Batch 26/64 loss: -0.47919750213623047
Batch 27/64 loss: -0.34824180603027344
Batch 28/64 loss: -0.08605766296386719
Batch 29/64 loss: -0.3072481155395508
Batch 30/64 loss: -0.6868991851806641
Batch 31/64 loss: -0.4949789047241211
Batch 32/64 loss: -0.3109121322631836
Batch 33/64 loss: -0.42597246170043945
Batch 34/64 loss: -0.48883628845214844
Batch 35/64 loss: -0.5306901931762695
Batch 36/64 loss: -0.6628904342651367
Batch 37/64 loss: -0.6813879013061523
Batch 38/64 loss: -0.7069387435913086
Batch 39/64 loss: -0.4338979721069336
Batch 40/64 loss: -0.6187992095947266
Batch 41/64 loss: -0.6432723999023438
Batch 42/64 loss: -0.37735795974731445
Batch 43/64 loss: -0.3614358901977539
Batch 44/64 loss: -0.6247444152832031
Batch 45/64 loss: -0.4245128631591797
Batch 46/64 loss: -0.15720701217651367
Batch 47/64 loss: -0.5475931167602539
Batch 48/64 loss: -0.5080747604370117
Batch 49/64 loss: -0.14767026901245117
Batch 50/64 loss: -0.7359037399291992
Batch 51/64 loss: -0.4071497917175293
Batch 52/64 loss: 0.0771636962890625
Batch 53/64 loss: -0.8784923553466797
Batch 54/64 loss: -0.6138095855712891
Batch 55/64 loss: -0.5899629592895508
Batch 56/64 loss: -0.6138973236083984
Batch 57/64 loss: -0.33305788040161133
Batch 58/64 loss: 0.07651615142822266
Batch 59/64 loss: -0.517362117767334
Batch 60/64 loss: -0.5278854370117188
Batch 61/64 loss: -0.14942312240600586
Batch 62/64 loss: -0.5543031692504883
Batch 63/64 loss: -0.3699469566345215
Batch 64/64 loss: -4.206717491149902
Epoch 234  Train loss: -0.5076159196741441  Val loss: -0.5954964365746147
Epoch 235
-------------------------------
Batch 1/64 loss: -0.7530937194824219
Batch 2/64 loss: 0.03639936447143555
Batch 3/64 loss: -0.6230173110961914
Batch 4/64 loss: -0.8458261489868164
Batch 5/64 loss: -0.5712823867797852
Batch 6/64 loss: -0.7094612121582031
Batch 7/64 loss: -0.5897836685180664
Batch 8/64 loss: -0.7219715118408203
Batch 9/64 loss: -0.7515249252319336
Batch 10/64 loss: -0.1851663589477539
Batch 11/64 loss: -0.42552900314331055
Batch 12/64 loss: -0.6459059715270996
Batch 13/64 loss: -0.6866655349731445
Batch 14/64 loss: -0.8858842849731445
Batch 15/64 loss: -0.6725625991821289
Batch 16/64 loss: -0.8036174774169922
Batch 17/64 loss: -0.4716815948486328
Batch 18/64 loss: -0.4972653388977051
Batch 19/64 loss: -0.28316259384155273
Batch 20/64 loss: -0.1525130271911621
Batch 21/64 loss: -0.41139888763427734
Batch 22/64 loss: -0.7225332260131836
Batch 23/64 loss: -0.8068485260009766
Batch 24/64 loss: -0.5628414154052734
Batch 25/64 loss: -0.7040414810180664
Batch 26/64 loss: -0.4061737060546875
Batch 27/64 loss: -0.6621875762939453
Batch 28/64 loss: -0.30080747604370117
Batch 29/64 loss: -0.15372419357299805
Batch 30/64 loss: -0.680394172668457
Batch 31/64 loss: -0.6982021331787109
Batch 32/64 loss: -0.45807647705078125
Batch 33/64 loss: -0.25624656677246094
Batch 34/64 loss: -0.3974113464355469
Batch 35/64 loss: -0.7979373931884766
Batch 36/64 loss: -0.27899885177612305
Batch 37/64 loss: -0.6632566452026367
Batch 38/64 loss: -0.5496559143066406
Batch 39/64 loss: -0.26740312576293945
Batch 40/64 loss: -0.43996620178222656
Batch 41/64 loss: -0.8134326934814453
Batch 42/64 loss: -0.6622400283813477
Batch 43/64 loss: -0.2890019416809082
Batch 44/64 loss: -0.8834323883056641
Batch 45/64 loss: -0.7086114883422852
Batch 46/64 loss: -0.5034680366516113
Batch 47/64 loss: 0.09756231307983398
Batch 48/64 loss: -0.24602031707763672
Batch 49/64 loss: -0.6442289352416992
Batch 50/64 loss: -0.5418171882629395
Batch 51/64 loss: -0.4898681640625
Batch 52/64 loss: -0.6713762283325195
Batch 53/64 loss: -0.9282236099243164
Batch 54/64 loss: -0.40877246856689453
Batch 55/64 loss: -0.6505222320556641
Batch 56/64 loss: -0.13032293319702148
Batch 57/64 loss: -0.2985343933105469
Batch 58/64 loss: -0.13078689575195312
Batch 59/64 loss: -0.8308000564575195
Batch 60/64 loss: -0.7093148231506348
Batch 61/64 loss: -0.5680675506591797
Batch 62/64 loss: -0.27813148498535156
Batch 63/64 loss: -0.29378509521484375
Batch 64/64 loss: -4.399479389190674
Epoch 235  Train loss: -0.5700459106295717  Val loss: -0.546527180884712
Epoch 236
-------------------------------
Batch 1/64 loss: -0.6762781143188477
Batch 2/64 loss: -0.6718559265136719
Batch 3/64 loss: -0.4463920593261719
Batch 4/64 loss: -0.14793014526367188
Batch 5/64 loss: -0.8342647552490234
Batch 6/64 loss: -0.5298972129821777
Batch 7/64 loss: -0.35529518127441406
Batch 8/64 loss: -0.36133861541748047
Batch 9/64 loss: -0.050940513610839844
Batch 10/64 loss: -0.6235713958740234
Batch 11/64 loss: -0.5175457000732422
Batch 12/64 loss: -0.5445079803466797
Batch 13/64 loss: -0.8192548751831055
Batch 14/64 loss: -0.6189966201782227
Batch 15/64 loss: -0.44011402130126953
Batch 16/64 loss: -0.4538908004760742
Batch 17/64 loss: -0.721405029296875
Batch 18/64 loss: -0.1378612518310547
Batch 19/64 loss: -0.7286930084228516
Batch 20/64 loss: -0.9014177322387695
Batch 21/64 loss: -0.48614978790283203
Batch 22/64 loss: -0.5643434524536133
Batch 23/64 loss: -0.5965156555175781
Batch 24/64 loss: -0.1472148895263672
Batch 25/64 loss: -0.7617826461791992
Batch 26/64 loss: -0.7319650650024414
Batch 27/64 loss: -0.23195838928222656
Batch 28/64 loss: -0.318572998046875
Batch 29/64 loss: 0.717139720916748
Batch 30/64 loss: 0.05255746841430664
Batch 31/64 loss: -0.682154655456543
Batch 32/64 loss: -0.5261516571044922
Batch 33/64 loss: -0.4916839599609375
Batch 34/64 loss: -0.44756174087524414
Batch 35/64 loss: -0.7567386627197266
Batch 36/64 loss: -0.44707679748535156
Batch 37/64 loss: -0.6271858215332031
Batch 38/64 loss: -0.8606634140014648
Batch 39/64 loss: -0.705143928527832
Batch 40/64 loss: -0.8791780471801758
Batch 41/64 loss: -0.27260398864746094
Batch 42/64 loss: -0.25724267959594727
Batch 43/64 loss: -0.5425558090209961
Batch 44/64 loss: -0.18470001220703125
Batch 45/64 loss: -0.654578685760498
Batch 46/64 loss: -0.8482856750488281
Batch 47/64 loss: -0.35451602935791016
Batch 48/64 loss: -0.4400649070739746
Batch 49/64 loss: -0.5329208374023438
Batch 50/64 loss: 0.6092896461486816
Batch 51/64 loss: -0.5395164489746094
Batch 52/64 loss: -0.6416702270507812
Batch 53/64 loss: -0.6612491607666016
Batch 54/64 loss: -0.4620246887207031
Batch 55/64 loss: -0.5547657012939453
Batch 56/64 loss: -0.5377445220947266
Batch 57/64 loss: -0.5958552360534668
Batch 58/64 loss: -0.8003215789794922
Batch 59/64 loss: -0.6398129463195801
Batch 60/64 loss: -0.5966796875
Batch 61/64 loss: -0.3584260940551758
Batch 62/64 loss: -0.6648263931274414
Batch 63/64 loss: -0.7073802947998047
Batch 64/64 loss: -4.32955265045166
Epoch 236  Train loss: -0.5421084198297239  Val loss: -0.6551970517922103
Epoch 237
-------------------------------
Batch 1/64 loss: -0.5347013473510742
Batch 2/64 loss: -0.8418073654174805
Batch 3/64 loss: -0.38805294036865234
Batch 4/64 loss: -0.5477199554443359
Batch 5/64 loss: -0.5030431747436523
Batch 6/64 loss: -0.6729440689086914
Batch 7/64 loss: -0.6019935607910156
Batch 8/64 loss: -0.6645421981811523
Batch 9/64 loss: -0.010764122009277344
Batch 10/64 loss: -0.5290079116821289
Batch 11/64 loss: -0.3102912902832031
Batch 12/64 loss: -0.41500043869018555
Batch 13/64 loss: -0.47736454010009766
Batch 14/64 loss: -0.5661125183105469
Batch 15/64 loss: -0.3699803352355957
Batch 16/64 loss: -0.4892425537109375
Batch 17/64 loss: -0.5069332122802734
Batch 18/64 loss: -0.5497684478759766
Batch 19/64 loss: -0.6579928398132324
Batch 20/64 loss: -0.553248405456543
Batch 21/64 loss: -0.5901970863342285
Batch 22/64 loss: -0.32193613052368164
Batch 23/64 loss: -0.4682445526123047
Batch 24/64 loss: -0.7000722885131836
Batch 25/64 loss: -0.5044326782226562
Batch 26/64 loss: -0.7328386306762695
Batch 27/64 loss: -0.681361198425293
Batch 28/64 loss: -0.6970481872558594
Batch 29/64 loss: -0.8170261383056641
Batch 30/64 loss: -0.2758369445800781
Batch 31/64 loss: -0.7281103134155273
Batch 32/64 loss: -0.6466817855834961
Batch 33/64 loss: -0.8158035278320312
Batch 34/64 loss: -0.36342811584472656
Batch 35/64 loss: -0.5364055633544922
Batch 36/64 loss: -0.9017019271850586
Batch 37/64 loss: -0.17880868911743164
Batch 38/64 loss: -0.49936866760253906
Batch 39/64 loss: -1.0903711318969727
Batch 40/64 loss: -0.5384702682495117
Batch 41/64 loss: -0.49442481994628906
Batch 42/64 loss: -0.7155971527099609
Batch 43/64 loss: -0.040345191955566406
Batch 44/64 loss: -0.61309814453125
Batch 45/64 loss: -0.11868667602539062
Batch 46/64 loss: -0.5115156173706055
Batch 47/64 loss: -0.04751777648925781
Batch 48/64 loss: -0.7382516860961914
Batch 49/64 loss: -0.7164649963378906
Batch 50/64 loss: -0.9154119491577148
Batch 51/64 loss: -0.5774922370910645
Batch 52/64 loss: 0.13300514221191406
Batch 53/64 loss: -0.5709333419799805
Batch 54/64 loss: -0.4628114700317383
Batch 55/64 loss: -0.40136003494262695
Batch 56/64 loss: -0.41880130767822266
Batch 57/64 loss: -0.6619324684143066
Batch 58/64 loss: -0.1730356216430664
Batch 59/64 loss: -0.6100664138793945
Batch 60/64 loss: -0.1957569122314453
Batch 61/64 loss: -0.10465240478515625
Batch 62/64 loss: -0.3888845443725586
Batch 63/64 loss: -0.7233209609985352
Batch 64/64 loss: -4.113253116607666
Epoch 237  Train loss: -0.5557796309976016  Val loss: -0.6461390727983717
Epoch 238
-------------------------------
Batch 1/64 loss: -0.8634195327758789
Batch 2/64 loss: -0.5729074478149414
Batch 3/64 loss: -0.6614246368408203
Batch 4/64 loss: -0.7456669807434082
Batch 5/64 loss: -0.6744794845581055
Batch 6/64 loss: -0.35968589782714844
Batch 7/64 loss: -0.4359703063964844
Batch 8/64 loss: -0.43485450744628906
Batch 9/64 loss: -0.4736003875732422
Batch 10/64 loss: -0.5829362869262695
Batch 11/64 loss: -0.359952449798584
Batch 12/64 loss: -0.280428409576416
Batch 13/64 loss: -0.3976621627807617
Batch 14/64 loss: -0.7532577514648438
Batch 15/64 loss: -0.5933880805969238
Batch 16/64 loss: -0.6006488800048828
Batch 17/64 loss: -0.5541753768920898
Batch 18/64 loss: -0.5329432487487793
Batch 19/64 loss: -0.6026902198791504
Batch 20/64 loss: -0.7677679061889648
Batch 21/64 loss: -0.8387136459350586
Batch 22/64 loss: -0.7091970443725586
Batch 23/64 loss: -0.22780275344848633
Batch 24/64 loss: -0.17876529693603516
Batch 25/64 loss: -0.2110309600830078
Batch 26/64 loss: -0.5768461227416992
Batch 27/64 loss: -0.6274957656860352
Batch 28/64 loss: -0.8337764739990234
Batch 29/64 loss: -0.7185869216918945
Batch 30/64 loss: -0.6325206756591797
Batch 31/64 loss: -0.5662736892700195
Batch 32/64 loss: -0.8244142532348633
Batch 33/64 loss: -0.39202404022216797
Batch 34/64 loss: -0.9755659103393555
Batch 35/64 loss: -0.6458454132080078
Batch 36/64 loss: -0.42530059814453125
Batch 37/64 loss: -0.4767441749572754
Batch 38/64 loss: -0.2760500907897949
Batch 39/64 loss: -0.7637128829956055
Batch 40/64 loss: -0.5655069351196289
Batch 41/64 loss: -0.8427753448486328
Batch 42/64 loss: -0.8369607925415039
Batch 43/64 loss: -0.7061023712158203
Batch 44/64 loss: -0.7585887908935547
Batch 45/64 loss: 0.07361841201782227
Batch 46/64 loss: -0.6720399856567383
Batch 47/64 loss: -0.626767635345459
Batch 48/64 loss: -0.7502326965332031
Batch 49/64 loss: -0.6623067855834961
Batch 50/64 loss: 0.10514259338378906
Batch 51/64 loss: -0.3840818405151367
Batch 52/64 loss: -0.603785514831543
Batch 53/64 loss: 0.06344175338745117
Batch 54/64 loss: -0.5429801940917969
Batch 55/64 loss: -0.40767335891723633
Batch 56/64 loss: -0.8292551040649414
Batch 57/64 loss: -0.35847043991088867
Batch 58/64 loss: -0.6103515625
Batch 59/64 loss: -0.8281822204589844
Batch 60/64 loss: -0.6005287170410156
Batch 61/64 loss: -0.5154800415039062
Batch 62/64 loss: -0.7319955825805664
Batch 63/64 loss: -0.48848390579223633
Batch 64/64 loss: -3.849393844604492
Epoch 238  Train loss: -0.5978967704024969  Val loss: -0.6573705378266954
Epoch 239
-------------------------------
Batch 1/64 loss: -0.6974258422851562
Batch 2/64 loss: -0.42600536346435547
Batch 3/64 loss: -0.14605045318603516
Batch 4/64 loss: -0.17596817016601562
Batch 5/64 loss: -0.7838497161865234
Batch 6/64 loss: -0.7481069564819336
Batch 7/64 loss: -0.5040855407714844
Batch 8/64 loss: -0.8199806213378906
Batch 9/64 loss: -0.4470033645629883
Batch 10/64 loss: -0.8618001937866211
Batch 11/64 loss: -0.665341854095459
Batch 12/64 loss: -0.0758519172668457
Batch 13/64 loss: -0.4005279541015625
Batch 14/64 loss: -0.6921186447143555
Batch 15/64 loss: -0.3935670852661133
Batch 16/64 loss: -0.7637004852294922
Batch 17/64 loss: -0.6786518096923828
Batch 18/64 loss: -0.4071812629699707
Batch 19/64 loss: -0.5935707092285156
Batch 20/64 loss: -0.30147457122802734
Batch 21/64 loss: -0.45748043060302734
Batch 22/64 loss: -0.48661136627197266
Batch 23/64 loss: -0.4745931625366211
Batch 24/64 loss: -0.3173980712890625
Batch 25/64 loss: -0.5831012725830078
Batch 26/64 loss: -0.5968542098999023
Batch 27/64 loss: -0.277005672454834
Batch 28/64 loss: -0.6472148895263672
Batch 29/64 loss: -0.4867238998413086
Batch 30/64 loss: -0.6451940536499023
Batch 31/64 loss: -0.864720344543457
Batch 32/64 loss: -0.29952287673950195
Batch 33/64 loss: -0.7299003601074219
Batch 34/64 loss: -0.8927164077758789
Batch 35/64 loss: -0.47899627685546875
Batch 36/64 loss: -0.46112537384033203
Batch 37/64 loss: -0.6451244354248047
Batch 38/64 loss: -0.6376867294311523
Batch 39/64 loss: -0.7010526657104492
Batch 40/64 loss: -0.8884592056274414
Batch 41/64 loss: -0.7517242431640625
Batch 42/64 loss: -0.48347997665405273
Batch 43/64 loss: -0.5858240127563477
Batch 44/64 loss: -0.5071792602539062
Batch 45/64 loss: -0.5513219833374023
Batch 46/64 loss: -0.5999622344970703
Batch 47/64 loss: -0.7464456558227539
Batch 48/64 loss: -0.7753067016601562
Batch 49/64 loss: -1.0142841339111328
Batch 50/64 loss: -0.39041709899902344
Batch 51/64 loss: -0.535456657409668
Batch 52/64 loss: -0.7679738998413086
Batch 53/64 loss: -0.21981048583984375
Batch 54/64 loss: -0.6091208457946777
Batch 55/64 loss: -0.4316864013671875
Batch 56/64 loss: -0.5966076850891113
Batch 57/64 loss: -0.5577726364135742
Batch 58/64 loss: -0.7443151473999023
Batch 59/64 loss: -0.17807245254516602
Batch 60/64 loss: -0.677884578704834
Batch 61/64 loss: -0.44678449630737305
Batch 62/64 loss: -0.18056583404541016
Batch 63/64 loss: -0.6427602767944336
Batch 64/64 loss: -4.216711521148682
Epoch 239  Train loss: -0.6009574048659381  Val loss: -0.7331635648851952
Saving best model, epoch: 239
Epoch 240
-------------------------------
Batch 1/64 loss: -0.8335475921630859
Batch 2/64 loss: -0.7005710601806641
Batch 3/64 loss: -0.4964723587036133
Batch 4/64 loss: -0.29715824127197266
Batch 5/64 loss: -0.40505313873291016
Batch 6/64 loss: -0.5597953796386719
Batch 7/64 loss: 0.0704045295715332
Batch 8/64 loss: -0.8762979507446289
Batch 9/64 loss: -0.24232149124145508
Batch 10/64 loss: -0.9041881561279297
Batch 11/64 loss: -0.5866508483886719
Batch 12/64 loss: -0.3750648498535156
Batch 13/64 loss: -0.15214967727661133
Batch 14/64 loss: -0.4108762741088867
Batch 15/64 loss: -0.2873978614807129
Batch 16/64 loss: -0.6805286407470703
Batch 17/64 loss: -0.23643922805786133
Batch 18/64 loss: -0.8453884124755859
Batch 19/64 loss: -0.7992849349975586
Batch 20/64 loss: -0.34426116943359375
Batch 21/64 loss: -0.6470475196838379
Batch 22/64 loss: -0.48395395278930664
Batch 23/64 loss: -0.16177749633789062
Batch 24/64 loss: -0.6620807647705078
Batch 25/64 loss: -0.7293906211853027
Batch 26/64 loss: -0.8889217376708984
Batch 27/64 loss: -0.20088958740234375
Batch 28/64 loss: -0.6962671279907227
Batch 29/64 loss: -0.61639404296875
Batch 30/64 loss: -0.510899543762207
Batch 31/64 loss: -0.2472219467163086
Batch 32/64 loss: -0.9497966766357422
Batch 33/64 loss: -0.8296871185302734
Batch 34/64 loss: -0.6784019470214844
Batch 35/64 loss: -0.7673254013061523
Batch 36/64 loss: -0.2275409698486328
Batch 37/64 loss: -0.8261032104492188
Batch 38/64 loss: -0.6323957443237305
Batch 39/64 loss: -0.719050407409668
Batch 40/64 loss: -0.2853560447692871
Batch 41/64 loss: -0.5781192779541016
Batch 42/64 loss: -0.7221736907958984
Batch 43/64 loss: -0.3643302917480469
Batch 44/64 loss: -0.9306173324584961
Batch 45/64 loss: -0.591130256652832
Batch 46/64 loss: 0.15586137771606445
Batch 47/64 loss: -0.5204582214355469
Batch 48/64 loss: -0.5951871871948242
Batch 49/64 loss: -0.369417667388916
Batch 50/64 loss: -0.7048325538635254
Batch 51/64 loss: -0.908721923828125
Batch 52/64 loss: -0.30660438537597656
Batch 53/64 loss: -0.7426776885986328
Batch 54/64 loss: -0.33553600311279297
Batch 55/64 loss: -0.5982036590576172
Batch 56/64 loss: -0.13321208953857422
Batch 57/64 loss: -0.565770149230957
Batch 58/64 loss: -0.7956686019897461
Batch 59/64 loss: -0.6577997207641602
Batch 60/64 loss: -0.5198650360107422
Batch 61/64 loss: -0.3817863464355469
Batch 62/64 loss: -0.6020660400390625
Batch 63/64 loss: -0.13076448440551758
Batch 64/64 loss: -4.505351543426514
Epoch 240  Train loss: -0.5804178742801442  Val loss: -0.6800971801338327
Epoch 241
-------------------------------
Batch 1/64 loss: -0.6685247421264648
Batch 2/64 loss: -0.6100559234619141
Batch 3/64 loss: -0.2680187225341797
Batch 4/64 loss: -0.677088737487793
Batch 5/64 loss: -0.7479209899902344
Batch 6/64 loss: -0.7789134979248047
Batch 7/64 loss: -0.6843767166137695
Batch 8/64 loss: -0.6137161254882812
Batch 9/64 loss: -0.5788278579711914
Batch 10/64 loss: -0.5538425445556641
Batch 11/64 loss: -0.4035482406616211
Batch 12/64 loss: -0.6290903091430664
Batch 13/64 loss: -0.6905879974365234
Batch 14/64 loss: -0.2994232177734375
Batch 15/64 loss: -0.01889801025390625
Batch 16/64 loss: -0.6786861419677734
Batch 17/64 loss: -0.2063007354736328
Batch 18/64 loss: -0.0048732757568359375
Batch 19/64 loss: -0.5737791061401367
Batch 20/64 loss: -0.24047565460205078
Batch 21/64 loss: -0.632145881652832
Batch 22/64 loss: -0.4472379684448242
Batch 23/64 loss: -0.3796052932739258
Batch 24/64 loss: -0.6018142700195312
Batch 25/64 loss: 0.3114128112792969
Batch 26/64 loss: -0.6561141014099121
Batch 27/64 loss: -0.6777782440185547
Batch 28/64 loss: -0.895263671875
Batch 29/64 loss: -0.7454853057861328
Batch 30/64 loss: -0.4140472412109375
Batch 31/64 loss: -0.6429424285888672
Batch 32/64 loss: -0.3734416961669922
Batch 33/64 loss: -0.8734760284423828
Batch 34/64 loss: -0.5372352600097656
Batch 35/64 loss: -0.7057418823242188
Batch 36/64 loss: -0.9656887054443359
Batch 37/64 loss: -0.7878284454345703
Batch 38/64 loss: -0.30394649505615234
Batch 39/64 loss: -0.13323497772216797
Batch 40/64 loss: -0.5727448463439941
Batch 41/64 loss: -0.4223313331604004
Batch 42/64 loss: -0.3046236038208008
Batch 43/64 loss: -0.7926044464111328
Batch 44/64 loss: -0.14068222045898438
Batch 45/64 loss: -0.5601301193237305
Batch 46/64 loss: -0.5275764465332031
Batch 47/64 loss: -0.7837133407592773
Batch 48/64 loss: -0.29794931411743164
Batch 49/64 loss: -0.4743194580078125
Batch 50/64 loss: -0.5592312812805176
Batch 51/64 loss: -0.3655524253845215
Batch 52/64 loss: -0.5154380798339844
Batch 53/64 loss: -0.5010919570922852
Batch 54/64 loss: -0.26775312423706055
Batch 55/64 loss: -0.5242304801940918
Batch 56/64 loss: -0.2884249687194824
Batch 57/64 loss: -0.63275146484375
Batch 58/64 loss: -0.49916648864746094
Batch 59/64 loss: -0.23149633407592773
Batch 60/64 loss: -0.550471305847168
Batch 61/64 loss: -0.3508319854736328
Batch 62/64 loss: -0.6009750366210938
Batch 63/64 loss: -0.5040431022644043
Batch 64/64 loss: -4.577948093414307
Epoch 241  Train loss: -0.5504338488859288  Val loss: -0.6268816551392021
Epoch 242
-------------------------------
Batch 1/64 loss: -0.8449735641479492
Batch 2/64 loss: -0.739964485168457
Batch 3/64 loss: -0.29839658737182617
Batch 4/64 loss: -0.5989894866943359
Batch 5/64 loss: -0.7038679122924805
Batch 6/64 loss: -0.36425065994262695
Batch 7/64 loss: -0.6924428939819336
Batch 8/64 loss: -0.8465633392333984
Batch 9/64 loss: -0.8598852157592773
Batch 10/64 loss: -0.4892854690551758
Batch 11/64 loss: -0.6960067749023438
Batch 12/64 loss: -0.4283409118652344
Batch 13/64 loss: -0.20712661743164062
Batch 14/64 loss: -0.3538856506347656
Batch 15/64 loss: -0.7135801315307617
Batch 16/64 loss: -0.7601337432861328
Batch 17/64 loss: -0.33220672607421875
Batch 18/64 loss: -0.7294139862060547
Batch 19/64 loss: -0.5963716506958008
Batch 20/64 loss: -0.7376327514648438
Batch 21/64 loss: -0.6136846542358398
Batch 22/64 loss: -0.8440608978271484
Batch 23/64 loss: -0.7634525299072266
Batch 24/64 loss: -0.48694705963134766
Batch 25/64 loss: -0.5769228935241699
Batch 26/64 loss: -0.5459446907043457
Batch 27/64 loss: -0.6113467216491699
Batch 28/64 loss: 0.01125478744506836
Batch 29/64 loss: -0.5194048881530762
Batch 30/64 loss: -0.7522573471069336
Batch 31/64 loss: -0.3952312469482422
Batch 32/64 loss: -0.6394824981689453
Batch 33/64 loss: -0.1728658676147461
Batch 34/64 loss: -0.5279502868652344
Batch 35/64 loss: -0.19801759719848633
Batch 36/64 loss: -0.6065244674682617
Batch 37/64 loss: -0.7851028442382812
Batch 38/64 loss: -0.6959199905395508
Batch 39/64 loss: -0.06357002258300781
Batch 40/64 loss: -0.23738956451416016
Batch 41/64 loss: -0.43986940383911133
Batch 42/64 loss: -0.7016630172729492
Batch 43/64 loss: -0.5559258460998535
Batch 44/64 loss: -0.596674919128418
Batch 45/64 loss: -0.7237462997436523
Batch 46/64 loss: -0.7382593154907227
Batch 47/64 loss: -0.6114263534545898
Batch 48/64 loss: -0.5128068923950195
Batch 49/64 loss: 0.09826040267944336
Batch 50/64 loss: -0.1697840690612793
Batch 51/64 loss: -0.7181577682495117
Batch 52/64 loss: -0.3005814552307129
Batch 53/64 loss: -0.6406412124633789
Batch 54/64 loss: -0.14843082427978516
Batch 55/64 loss: -0.7357382774353027
Batch 56/64 loss: -0.9020853042602539
Batch 57/64 loss: -0.07189607620239258
Batch 58/64 loss: -0.4884500503540039
Batch 59/64 loss: -0.05755758285522461
Batch 60/64 loss: -0.44756460189819336
Batch 61/64 loss: -0.4459953308105469
Batch 62/64 loss: -0.4072532653808594
Batch 63/64 loss: -0.7410545349121094
Batch 64/64 loss: -4.288599967956543
Epoch 242  Train loss: -0.5692845774631874  Val loss: -0.45858252089457824
Epoch 243
-------------------------------
Batch 1/64 loss: -0.4743013381958008
Batch 2/64 loss: -0.3825855255126953
Batch 3/64 loss: -0.20228195190429688
Batch 4/64 loss: -0.08401966094970703
Batch 5/64 loss: -0.4162445068359375
Batch 6/64 loss: -0.4588627815246582
Batch 7/64 loss: -0.4233059883117676
Batch 8/64 loss: -0.5125021934509277
Batch 9/64 loss: -0.7340335845947266
Batch 10/64 loss: -0.537043571472168
Batch 11/64 loss: -0.4296560287475586
Batch 12/64 loss: -0.7051534652709961
Batch 13/64 loss: -0.561643123626709
Batch 14/64 loss: -0.7294826507568359
Batch 15/64 loss: -0.6868829727172852
Batch 16/64 loss: -0.326479434967041
Batch 17/64 loss: -0.5561466217041016
Batch 18/64 loss: -0.8327789306640625
Batch 19/64 loss: -0.6165866851806641
Batch 20/64 loss: -0.5252885818481445
Batch 21/64 loss: -0.3038330078125
Batch 22/64 loss: -0.8373498916625977
Batch 23/64 loss: -0.5359821319580078
Batch 24/64 loss: -0.6686105728149414
Batch 25/64 loss: -0.3958740234375
Batch 26/64 loss: 0.0822610855102539
Batch 27/64 loss: -0.6265525817871094
Batch 28/64 loss: -0.692021369934082
Batch 29/64 loss: -0.241058349609375
Batch 30/64 loss: -0.6147918701171875
Batch 31/64 loss: -0.6621246337890625
Batch 32/64 loss: -0.6344203948974609
Batch 33/64 loss: -0.4856147766113281
Batch 34/64 loss: -0.44951343536376953
Batch 35/64 loss: 0.04025840759277344
Batch 36/64 loss: -0.5207319259643555
Batch 37/64 loss: -0.5062351226806641
Batch 38/64 loss: -0.15533828735351562
Batch 39/64 loss: 0.10039424896240234
Batch 40/64 loss: -0.8383359909057617
Batch 41/64 loss: -0.6718406677246094
Batch 42/64 loss: -0.5491909980773926
Batch 43/64 loss: -0.2762002944946289
Batch 44/64 loss: -0.5594758987426758
Batch 45/64 loss: -0.3240928649902344
Batch 46/64 loss: -0.7462654113769531
Batch 47/64 loss: -0.6420221328735352
Batch 48/64 loss: 0.12401199340820312
Batch 49/64 loss: -0.21433448791503906
Batch 50/64 loss: -0.5237655639648438
Batch 51/64 loss: 0.6113991737365723
Batch 52/64 loss: 0.6158428192138672
Batch 53/64 loss: -0.4875612258911133
Batch 54/64 loss: -0.8256826400756836
Batch 55/64 loss: -0.5046329498291016
Batch 56/64 loss: -0.2452254295349121
Batch 57/64 loss: -0.46717405319213867
Batch 58/64 loss: -0.1988825798034668
Batch 59/64 loss: -0.46856021881103516
Batch 60/64 loss: -0.5986537933349609
Batch 61/64 loss: 0.14875078201293945
Batch 62/64 loss: -0.5369167327880859
Batch 63/64 loss: -0.7842512130737305
Batch 64/64 loss: -4.620326042175293
Epoch 243  Train loss: -0.4820505590999828  Val loss: -0.4866495820664868
Epoch 244
-------------------------------
Batch 1/64 loss: -0.6816635131835938
Batch 2/64 loss: -0.6337709426879883
Batch 3/64 loss: -0.7091503143310547
Batch 4/64 loss: -0.4563937187194824
Batch 5/64 loss: -0.46213722229003906
Batch 6/64 loss: -0.5185136795043945
Batch 7/64 loss: -0.6750144958496094
Batch 8/64 loss: -0.6585812568664551
Batch 9/64 loss: -0.3908553123474121
Batch 10/64 loss: -0.8953104019165039
Batch 11/64 loss: 1.109309196472168
Batch 12/64 loss: 0.12936115264892578
Batch 13/64 loss: -0.4777555465698242
Batch 14/64 loss: -0.5931282043457031
Batch 15/64 loss: -0.5546779632568359
Batch 16/64 loss: -0.548832893371582
Batch 17/64 loss: -0.4678077697753906
Batch 18/64 loss: -0.22580718994140625
Batch 19/64 loss: -0.2792816162109375
Batch 20/64 loss: -0.6974191665649414
Batch 21/64 loss: -0.39655494689941406
Batch 22/64 loss: 0.07689714431762695
Batch 23/64 loss: -0.6128215789794922
Batch 24/64 loss: -0.4965057373046875
Batch 25/64 loss: -0.1630096435546875
Batch 26/64 loss: -0.41272640228271484
Batch 27/64 loss: -0.03957939147949219
Batch 28/64 loss: -0.49729061126708984
Batch 29/64 loss: -0.39290571212768555
Batch 30/64 loss: -0.3545417785644531
Batch 31/64 loss: -0.6657638549804688
Batch 32/64 loss: 0.4175381660461426
Batch 33/64 loss: -0.7692012786865234
Batch 34/64 loss: -0.7593593597412109
Batch 35/64 loss: -0.20205926895141602
Batch 36/64 loss: -0.6695461273193359
Batch 37/64 loss: -0.5840702056884766
Batch 38/64 loss: -0.4790172576904297
Batch 39/64 loss: -0.5913686752319336
Batch 40/64 loss: -0.2744269371032715
Batch 41/64 loss: -0.3815879821777344
Batch 42/64 loss: -0.013407707214355469
Batch 43/64 loss: 0.07105731964111328
Batch 44/64 loss: -0.6516070365905762
Batch 45/64 loss: -0.5196084976196289
Batch 46/64 loss: -0.456113338470459
Batch 47/64 loss: -0.7079367637634277
Batch 48/64 loss: -0.39435434341430664
Batch 49/64 loss: -0.40871286392211914
Batch 50/64 loss: -0.6366186141967773
Batch 51/64 loss: -0.5972347259521484
Batch 52/64 loss: -0.8469343185424805
Batch 53/64 loss: -0.3402595520019531
Batch 54/64 loss: -0.7784299850463867
Batch 55/64 loss: -0.21401023864746094
Batch 56/64 loss: 0.09743547439575195
Batch 57/64 loss: -0.4510641098022461
Batch 58/64 loss: -0.15093135833740234
Batch 59/64 loss: -0.5970182418823242
Batch 60/64 loss: -0.22360706329345703
Batch 61/64 loss: -0.5529074668884277
Batch 62/64 loss: -0.24213314056396484
Batch 63/64 loss: -0.21100568771362305
Batch 64/64 loss: -4.00811767578125
Epoch 244  Train loss: -0.4512442794500613  Val loss: -0.6121972467481476
Epoch 245
-------------------------------
Batch 1/64 loss: -0.6664190292358398
Batch 2/64 loss: -0.7721376419067383
Batch 3/64 loss: -0.6510705947875977
Batch 4/64 loss: -0.7635135650634766
Batch 5/64 loss: -0.5618057250976562
Batch 6/64 loss: -0.43365955352783203
Batch 7/64 loss: 1.7427515983581543
Batch 8/64 loss: -0.2576913833618164
Batch 9/64 loss: -0.017198562622070312
Batch 10/64 loss: -0.3699202537536621
Batch 11/64 loss: -0.10745429992675781
Batch 12/64 loss: -0.30509233474731445
Batch 13/64 loss: -0.5391101837158203
Batch 14/64 loss: -0.2855796813964844
Batch 15/64 loss: -0.3065071105957031
Batch 16/64 loss: -0.026453018188476562
Batch 17/64 loss: -0.011953353881835938
Batch 18/64 loss: -0.32753610610961914
Batch 19/64 loss: -0.1692352294921875
Batch 20/64 loss: -0.3922233581542969
Batch 21/64 loss: -0.6772336959838867
Batch 22/64 loss: -0.3667583465576172
Batch 23/64 loss: -0.3186511993408203
Batch 24/64 loss: -0.2495412826538086
Batch 25/64 loss: -0.37297773361206055
Batch 26/64 loss: -0.42458152770996094
Batch 27/64 loss: 0.907038688659668
Batch 28/64 loss: -0.4314413070678711
Batch 29/64 loss: -0.5698556900024414
Batch 30/64 loss: -0.36037540435791016
Batch 31/64 loss: -0.5529718399047852
Batch 32/64 loss: -0.4519467353820801
Batch 33/64 loss: -0.49018144607543945
Batch 34/64 loss: -0.05602836608886719
Batch 35/64 loss: -0.7120823860168457
Batch 36/64 loss: -0.31836891174316406
Batch 37/64 loss: -0.18976688385009766
Batch 38/64 loss: 0.07996320724487305
Batch 39/64 loss: -0.4871363639831543
Batch 40/64 loss: -0.6695632934570312
Batch 41/64 loss: -0.4404788017272949
Batch 42/64 loss: -0.47183942794799805
Batch 43/64 loss: -0.5359702110290527
Batch 44/64 loss: -0.25954198837280273
Batch 45/64 loss: -0.38201475143432617
Batch 46/64 loss: -0.4838714599609375
Batch 47/64 loss: -0.7561559677124023
Batch 48/64 loss: 0.23192977905273438
Batch 49/64 loss: -0.23373031616210938
Batch 50/64 loss: -0.09661054611206055
Batch 51/64 loss: -0.4654855728149414
Batch 52/64 loss: 0.011259078979492188
Batch 53/64 loss: -0.4520292282104492
Batch 54/64 loss: -0.488889217376709
Batch 55/64 loss: -0.5865554809570312
Batch 56/64 loss: -0.31984996795654297
Batch 57/64 loss: -0.6023273468017578
Batch 58/64 loss: -0.32523012161254883
Batch 59/64 loss: -0.4876542091369629
Batch 60/64 loss: -0.3018498420715332
Batch 61/64 loss: -0.7559137344360352
Batch 62/64 loss: -0.7050514221191406
Batch 63/64 loss: -0.6339359283447266
Batch 64/64 loss: -4.545224189758301
Epoch 245  Train loss: -0.3903527016733207  Val loss: -0.5763030429066661
Epoch 246
-------------------------------
Batch 1/64 loss: -0.5821418762207031
Batch 2/64 loss: -0.33158016204833984
Batch 3/64 loss: -0.8133745193481445
Batch 4/64 loss: 0.9266119003295898
Batch 5/64 loss: -0.42298221588134766
Batch 6/64 loss: -0.3660316467285156
Batch 7/64 loss: -0.4166855812072754
Batch 8/64 loss: -0.06050395965576172
Batch 9/64 loss: 0.0321192741394043
Batch 10/64 loss: -0.021683692932128906
Batch 11/64 loss: -0.5287551879882812
Batch 12/64 loss: -0.36667442321777344
Batch 13/64 loss: -0.4902076721191406
Batch 14/64 loss: -0.4042978286743164
Batch 15/64 loss: -0.4824190139770508
Batch 16/64 loss: -0.7930831909179688
Batch 17/64 loss: -0.14833402633666992
Batch 18/64 loss: -0.6941814422607422
Batch 19/64 loss: -0.9256267547607422
Batch 20/64 loss: -0.04892587661743164
Batch 21/64 loss: -0.8152122497558594
Batch 22/64 loss: -0.734562873840332
Batch 23/64 loss: -0.45350170135498047
Batch 24/64 loss: -0.5838847160339355
Batch 25/64 loss: -0.32266998291015625
Batch 26/64 loss: -0.5763015747070312
Batch 27/64 loss: -0.38277530670166016
Batch 28/64 loss: -0.45551490783691406
Batch 29/64 loss: -0.8726863861083984
Batch 30/64 loss: -0.6965408325195312
Batch 31/64 loss: -0.446075439453125
Batch 32/64 loss: -0.13688373565673828
Batch 33/64 loss: -0.1770763397216797
Batch 34/64 loss: -0.47545814514160156
Batch 35/64 loss: -0.42765235900878906
Batch 36/64 loss: -0.6626834869384766
Batch 37/64 loss: -0.006868839263916016
Batch 38/64 loss: -0.6252326965332031
Batch 39/64 loss: 0.09355020523071289
Batch 40/64 loss: -0.08411264419555664
Batch 41/64 loss: -0.5955619812011719
Batch 42/64 loss: -0.6983146667480469
Batch 43/64 loss: -0.4147219657897949
Batch 44/64 loss: -0.6174883842468262
Batch 45/64 loss: -0.5964393615722656
Batch 46/64 loss: -0.4320077896118164
Batch 47/64 loss: -0.19555902481079102
Batch 48/64 loss: 0.0656118392944336
Batch 49/64 loss: -0.6400909423828125
Batch 50/64 loss: -0.5939550399780273
Batch 51/64 loss: -0.5567264556884766
Batch 52/64 loss: -0.7916479110717773
Batch 53/64 loss: 0.2217087745666504
Batch 54/64 loss: -0.6902666091918945
Batch 55/64 loss: -0.6924581527709961
Batch 56/64 loss: -0.7756252288818359
Batch 57/64 loss: -0.3508129119873047
Batch 58/64 loss: -0.625216007232666
Batch 59/64 loss: -0.9392070770263672
Batch 60/64 loss: -0.5074605941772461
Batch 61/64 loss: -0.6804742813110352
Batch 62/64 loss: -0.45293426513671875
Batch 63/64 loss: -0.8299837112426758
Batch 64/64 loss: -3.824702739715576
Epoch 246  Train loss: -0.4868872268527162  Val loss: -0.643705427032156
Epoch 247
-------------------------------
Batch 1/64 loss: -0.756805419921875
Batch 2/64 loss: -0.3571305274963379
Batch 3/64 loss: -0.5546903610229492
Batch 4/64 loss: -0.3872504234313965
Batch 5/64 loss: -0.6108131408691406
Batch 6/64 loss: -0.49431657791137695
Batch 7/64 loss: -0.44263553619384766
Batch 8/64 loss: -0.06026935577392578
Batch 9/64 loss: -0.6580295562744141
Batch 10/64 loss: -0.49520111083984375
Batch 11/64 loss: -0.5288996696472168
Batch 12/64 loss: -0.8756027221679688
Batch 13/64 loss: -0.33240795135498047
Batch 14/64 loss: -0.3463592529296875
Batch 15/64 loss: -0.06400012969970703
Batch 16/64 loss: -0.4960036277770996
Batch 17/64 loss: -0.5605478286743164
Batch 18/64 loss: -0.3953437805175781
Batch 19/64 loss: -0.6595010757446289
Batch 20/64 loss: -0.8793182373046875
Batch 21/64 loss: -0.4668850898742676
Batch 22/64 loss: -0.8187055587768555
Batch 23/64 loss: -0.27616214752197266
Batch 24/64 loss: -0.540583610534668
Batch 25/64 loss: -0.9326343536376953
Batch 26/64 loss: -0.6097588539123535
Batch 27/64 loss: -0.6145362854003906
Batch 28/64 loss: -0.7527866363525391
Batch 29/64 loss: -0.5581631660461426
Batch 30/64 loss: -0.6532497406005859
Batch 31/64 loss: -0.8233413696289062
Batch 32/64 loss: -0.4239482879638672
Batch 33/64 loss: -0.5109043121337891
Batch 34/64 loss: -0.8014717102050781
Batch 35/64 loss: -0.5458803176879883
Batch 36/64 loss: -0.3104076385498047
Batch 37/64 loss: -0.6086387634277344
Batch 38/64 loss: -0.2104663848876953
Batch 39/64 loss: -0.39162778854370117
Batch 40/64 loss: -0.25566577911376953
Batch 41/64 loss: -0.6827716827392578
Batch 42/64 loss: -0.5911149978637695
Batch 43/64 loss: -0.3145899772644043
Batch 44/64 loss: -0.5899238586425781
Batch 45/64 loss: -0.5070705413818359
Batch 46/64 loss: -0.5357494354248047
Batch 47/64 loss: -0.7252302169799805
Batch 48/64 loss: -0.22498798370361328
Batch 49/64 loss: -0.2456207275390625
Batch 50/64 loss: -0.6356573104858398
Batch 51/64 loss: -0.21834039688110352
Batch 52/64 loss: -0.4381752014160156
Batch 53/64 loss: -0.6018710136413574
Batch 54/64 loss: -0.40022754669189453
Batch 55/64 loss: -0.553926944732666
Batch 56/64 loss: -0.38005828857421875
Batch 57/64 loss: -0.3007049560546875
Batch 58/64 loss: -0.48502159118652344
Batch 59/64 loss: -0.22022056579589844
Batch 60/64 loss: -0.47983837127685547
Batch 61/64 loss: -0.5797748565673828
Batch 62/64 loss: -0.80059814453125
Batch 63/64 loss: -0.49730968475341797
Batch 64/64 loss: -4.507404327392578
Epoch 247  Train loss: -0.5560828489415786  Val loss: -0.6535850603555896
Epoch 248
-------------------------------
Batch 1/64 loss: -0.6796102523803711
Batch 2/64 loss: -0.6419677734375
Batch 3/64 loss: -0.7455959320068359
Batch 4/64 loss: -0.7003822326660156
Batch 5/64 loss: -0.33887672424316406
Batch 6/64 loss: -0.5414924621582031
Batch 7/64 loss: -0.5220479965209961
Batch 8/64 loss: -0.3185758590698242
Batch 9/64 loss: -0.46222925186157227
Batch 10/64 loss: -0.33086156845092773
Batch 11/64 loss: -0.2319936752319336
Batch 12/64 loss: -0.7503604888916016
Batch 13/64 loss: -0.5362100601196289
Batch 14/64 loss: -0.6960124969482422
Batch 15/64 loss: -0.4075288772583008
Batch 16/64 loss: -0.612006664276123
Batch 17/64 loss: -0.5603609085083008
Batch 18/64 loss: -0.6030721664428711
Batch 19/64 loss: -0.46975088119506836
Batch 20/64 loss: -0.6801047325134277
Batch 21/64 loss: -0.8054194450378418
Batch 22/64 loss: -0.596954345703125
Batch 23/64 loss: -0.5117053985595703
Batch 24/64 loss: -0.5115795135498047
Batch 25/64 loss: -0.6953449249267578
Batch 26/64 loss: -0.4185667037963867
Batch 27/64 loss: -0.5424661636352539
Batch 28/64 loss: -0.4680290222167969
Batch 29/64 loss: -0.38551807403564453
Batch 30/64 loss: -0.5014781951904297
Batch 31/64 loss: 0.10304975509643555
Batch 32/64 loss: -0.24640178680419922
Batch 33/64 loss: -0.592155933380127
Batch 34/64 loss: -0.18583393096923828
Batch 35/64 loss: -0.03289318084716797
Batch 36/64 loss: -0.5154705047607422
Batch 37/64 loss: -0.42927074432373047
Batch 38/64 loss: -0.5876331329345703
Batch 39/64 loss: -0.7697439193725586
Batch 40/64 loss: -0.6537942886352539
Batch 41/64 loss: -0.43657493591308594
Batch 42/64 loss: -0.4296412467956543
Batch 43/64 loss: -0.40425872802734375
Batch 44/64 loss: -0.6052656173706055
Batch 45/64 loss: -0.7106332778930664
Batch 46/64 loss: -0.17076683044433594
Batch 47/64 loss: -0.543182373046875
Batch 48/64 loss: -0.4661712646484375
Batch 49/64 loss: -0.34449291229248047
Batch 50/64 loss: -0.6151704788208008
Batch 51/64 loss: -0.09353828430175781
Batch 52/64 loss: -0.28774213790893555
Batch 53/64 loss: -0.6996355056762695
Batch 54/64 loss: -0.5248489379882812
Batch 55/64 loss: -0.6968164443969727
Batch 56/64 loss: -0.2539701461791992
Batch 57/64 loss: -0.6209983825683594
Batch 58/64 loss: -0.5564150810241699
Batch 59/64 loss: -0.6680798530578613
Batch 60/64 loss: -0.5301780700683594
Batch 61/64 loss: -0.5795631408691406
Batch 62/64 loss: -0.1699361801147461
Batch 63/64 loss: -0.6762819290161133
Batch 64/64 loss: -4.334778785705566
Epoch 248  Train loss: -0.5413568047916189  Val loss: -0.689664139370738
Epoch 249
-------------------------------
Batch 1/64 loss: -0.24328899383544922
Batch 2/64 loss: -0.1332392692565918
Batch 3/64 loss: -0.7046670913696289
Batch 4/64 loss: -0.49799203872680664
Batch 5/64 loss: 0.08710193634033203
Batch 6/64 loss: -0.0531764030456543
Batch 7/64 loss: -0.5306711196899414
Batch 8/64 loss: -0.33698463439941406
Batch 9/64 loss: -0.6179046630859375
Batch 10/64 loss: -0.7620716094970703
Batch 11/64 loss: -0.707244873046875
Batch 12/64 loss: -0.7203540802001953
Batch 13/64 loss: -0.5917329788208008
Batch 14/64 loss: -0.7006826400756836
Batch 15/64 loss: -0.7559728622436523
Batch 16/64 loss: -0.43052148818969727
Batch 17/64 loss: -0.5870895385742188
Batch 18/64 loss: -0.3363614082336426
Batch 19/64 loss: -0.6620140075683594
Batch 20/64 loss: -0.14733123779296875
Batch 21/64 loss: -0.509307861328125
Batch 22/64 loss: -0.42618751525878906
Batch 23/64 loss: -0.708033561706543
Batch 24/64 loss: -0.6221723556518555
Batch 25/64 loss: -0.6807107925415039
Batch 26/64 loss: -0.6590909957885742
Batch 27/64 loss: -0.25289440155029297
Batch 28/64 loss: -0.7568225860595703
Batch 29/64 loss: -0.6417398452758789
Batch 30/64 loss: -0.7097005844116211
Batch 31/64 loss: -0.7223148345947266
Batch 32/64 loss: -0.43856048583984375
Batch 33/64 loss: -0.416043758392334
Batch 34/64 loss: -0.7756118774414062
Batch 35/64 loss: 0.46121644973754883
Batch 36/64 loss: -0.44713687896728516
Batch 37/64 loss: -0.38477468490600586
Batch 38/64 loss: -0.49643516540527344
Batch 39/64 loss: -0.4320082664489746
Batch 40/64 loss: -0.6743040084838867
Batch 41/64 loss: -0.6863117218017578
Batch 42/64 loss: -0.4845590591430664
Batch 43/64 loss: -0.5580706596374512
Batch 44/64 loss: -0.5011758804321289
Batch 45/64 loss: -0.5563602447509766
Batch 46/64 loss: -0.6652088165283203
Batch 47/64 loss: -0.6275138854980469
Batch 48/64 loss: -0.17228174209594727
Batch 49/64 loss: -0.7534217834472656
Batch 50/64 loss: -0.47783756256103516
Batch 51/64 loss: -0.3936910629272461
Batch 52/64 loss: -0.7290000915527344
Batch 53/64 loss: -0.5277996063232422
Batch 54/64 loss: 0.39964914321899414
Batch 55/64 loss: -0.3578939437866211
Batch 56/64 loss: -0.15461969375610352
Batch 57/64 loss: -0.3742241859436035
Batch 58/64 loss: -0.7526302337646484
Batch 59/64 loss: -0.6207523345947266
Batch 60/64 loss: -0.19538021087646484
Batch 61/64 loss: -0.629002571105957
Batch 62/64 loss: -0.5836138725280762
Batch 63/64 loss: -0.11307287216186523
Batch 64/64 loss: -4.0732316970825195
Epoch 249  Train loss: -0.5222671321794099  Val loss: -0.580795222541311
Epoch 250
-------------------------------
Batch 1/64 loss: -0.34668636322021484
Batch 2/64 loss: -0.7833871841430664
Batch 3/64 loss: -0.3833775520324707
Batch 4/64 loss: -0.6212081909179688
Batch 5/64 loss: -0.6904630661010742
Batch 6/64 loss: -0.7221145629882812
Batch 7/64 loss: -0.8396215438842773
Batch 8/64 loss: 0.03350400924682617
Batch 9/64 loss: -0.39191293716430664
Batch 10/64 loss: -0.6845722198486328
Batch 11/64 loss: -0.8131170272827148
Batch 12/64 loss: -0.17684459686279297
Batch 13/64 loss: -0.5217628479003906
Batch 14/64 loss: -1.0852031707763672
Batch 15/64 loss: -0.7933340072631836
Batch 16/64 loss: 0.010342121124267578
Batch 17/64 loss: -0.2905998229980469
Batch 18/64 loss: -0.3493928909301758
Batch 19/64 loss: -0.48522377014160156
Batch 20/64 loss: -0.06698417663574219
Batch 21/64 loss: -0.15980052947998047
Batch 22/64 loss: -0.5155735015869141
Batch 23/64 loss: -0.8478832244873047
Batch 24/64 loss: -0.5048761367797852
Batch 25/64 loss: -0.41740894317626953
Batch 26/64 loss: -0.798029899597168
Batch 27/64 loss: -0.20866918563842773
Batch 28/64 loss: -0.33716678619384766
Batch 29/64 loss: -0.7974491119384766
Batch 30/64 loss: -0.7555027008056641
Batch 31/64 loss: -0.5420827865600586
Batch 32/64 loss: -0.48902320861816406
Batch 33/64 loss: -0.5055027008056641
Batch 34/64 loss: -0.08888578414916992
Batch 35/64 loss: -0.750758171081543
Batch 36/64 loss: -0.6571798324584961
Batch 37/64 loss: -0.3459792137145996
Batch 38/64 loss: -0.4796280860900879
Batch 39/64 loss: -0.8178701400756836
Batch 40/64 loss: -0.7802963256835938
Batch 41/64 loss: 0.0393681526184082
Batch 42/64 loss: -0.20661211013793945
Batch 43/64 loss: -0.5839591026306152
Batch 44/64 loss: -0.8014917373657227
Batch 45/64 loss: -0.4012460708618164
Batch 46/64 loss: -0.6703076362609863
Batch 47/64 loss: -0.06419897079467773
Batch 48/64 loss: -0.5194387435913086
Batch 49/64 loss: -0.48011255264282227
Batch 50/64 loss: -0.44286251068115234
Batch 51/64 loss: -0.7091751098632812
Batch 52/64 loss: -0.7037830352783203
Batch 53/64 loss: -0.7382078170776367
Batch 54/64 loss: -0.22934961318969727
Batch 55/64 loss: -0.6404447555541992
Batch 56/64 loss: -0.6674108505249023
Batch 57/64 loss: -0.7535810470581055
Batch 58/64 loss: -0.42070865631103516
Batch 59/64 loss: -0.6189908981323242
Batch 60/64 loss: -0.7192325592041016
Batch 61/64 loss: -0.26392221450805664
Batch 62/64 loss: -0.48232555389404297
Batch 63/64 loss: -0.7502880096435547
Batch 64/64 loss: -4.397670745849609
Epoch 250  Train loss: -0.5635774201037838  Val loss: -0.7178848764740724
Epoch 251
-------------------------------
Batch 1/64 loss: -0.49707603454589844
Batch 2/64 loss: -0.9387607574462891
Batch 3/64 loss: -0.4281930923461914
Batch 4/64 loss: -0.7659769058227539
Batch 5/64 loss: -0.650660514831543
Batch 6/64 loss: -0.25165367126464844
Batch 7/64 loss: -0.38872432708740234
Batch 8/64 loss: -0.5536856651306152
Batch 9/64 loss: -0.7324047088623047
Batch 10/64 loss: -0.35538482666015625
Batch 11/64 loss: -0.6842803955078125
Batch 12/64 loss: -0.7894525527954102
Batch 13/64 loss: -0.5601167678833008
Batch 14/64 loss: -1.0607337951660156
Batch 15/64 loss: -0.6518964767456055
Batch 16/64 loss: -0.4909639358520508
Batch 17/64 loss: -0.8799376487731934
Batch 18/64 loss: -0.25732851028442383
Batch 19/64 loss: -0.46319007873535156
Batch 20/64 loss: -0.4746055603027344
Batch 21/64 loss: -0.11471939086914062
Batch 22/64 loss: -0.317965030670166
Batch 23/64 loss: -0.4331626892089844
Batch 24/64 loss: -0.8703031539916992
Batch 25/64 loss: -0.05840635299682617
Batch 26/64 loss: -0.7649760246276855
Batch 27/64 loss: -0.6802091598510742
Batch 28/64 loss: -0.5574250221252441
Batch 29/64 loss: -0.7051792144775391
Batch 30/64 loss: -0.6170644760131836
Batch 31/64 loss: -0.3220858573913574
Batch 32/64 loss: -0.5341482162475586
Batch 33/64 loss: -0.6944408416748047
Batch 34/64 loss: -0.6633501052856445
Batch 35/64 loss: -0.9905824661254883
Batch 36/64 loss: -0.22420644760131836
Batch 37/64 loss: -0.14488887786865234
Batch 38/64 loss: -1.0577821731567383
Batch 39/64 loss: -0.45079994201660156
Batch 40/64 loss: -0.66015625
Batch 41/64 loss: -0.8831882476806641
Batch 42/64 loss: -0.4205746650695801
Batch 43/64 loss: -0.4227323532104492
Batch 44/64 loss: -0.48592376708984375
Batch 45/64 loss: -0.873410701751709
Batch 46/64 loss: -0.8126764297485352
Batch 47/64 loss: -0.8059778213500977
Batch 48/64 loss: -0.6663980484008789
Batch 49/64 loss: -0.6608819961547852
Batch 50/64 loss: -0.6282224655151367
Batch 51/64 loss: -0.5594348907470703
Batch 52/64 loss: -0.5817060470581055
Batch 53/64 loss: -0.7360744476318359
Batch 54/64 loss: -0.21196937561035156
Batch 55/64 loss: -0.4776487350463867
Batch 56/64 loss: -0.7799129486083984
Batch 57/64 loss: -0.5698823928833008
Batch 58/64 loss: -0.7272138595581055
Batch 59/64 loss: -0.41057872772216797
Batch 60/64 loss: -0.6125078201293945
Batch 61/64 loss: -0.41999006271362305
Batch 62/64 loss: -0.7183494567871094
Batch 63/64 loss: -0.3546171188354492
Batch 64/64 loss: -3.975101947784424
Epoch 251  Train loss: -0.6202051256217208  Val loss: -0.6682449550563118
Epoch 252
-------------------------------
Batch 1/64 loss: -0.4958305358886719
Batch 2/64 loss: -0.6553564071655273
Batch 3/64 loss: -0.7484817504882812
Batch 4/64 loss: -0.32109880447387695
Batch 5/64 loss: -0.4130983352661133
Batch 6/64 loss: -0.6335287094116211
Batch 7/64 loss: -0.9101495742797852
Batch 8/64 loss: -0.6115083694458008
Batch 9/64 loss: -0.4632105827331543
Batch 10/64 loss: -0.14290857315063477
Batch 11/64 loss: -0.6194171905517578
Batch 12/64 loss: -0.4620542526245117
Batch 13/64 loss: -0.6524343490600586
Batch 14/64 loss: -0.7730340957641602
Batch 15/64 loss: -0.3606119155883789
Batch 16/64 loss: -0.7500009536743164
Batch 17/64 loss: -0.315676212310791
Batch 18/64 loss: -0.3549461364746094
Batch 19/64 loss: -0.8029851913452148
Batch 20/64 loss: -0.5689926147460938
Batch 21/64 loss: -0.4402732849121094
Batch 22/64 loss: -0.3433046340942383
Batch 23/64 loss: -0.5729465484619141
Batch 24/64 loss: -0.5937423706054688
Batch 25/64 loss: -0.8348560333251953
Batch 26/64 loss: -0.5940141677856445
Batch 27/64 loss: -0.9511966705322266
Batch 28/64 loss: -0.507598876953125
Batch 29/64 loss: -0.4130220413208008
Batch 30/64 loss: -0.44914817810058594
Batch 31/64 loss: -0.8861732482910156
Batch 32/64 loss: -0.4929356575012207
Batch 33/64 loss: -0.7466001510620117
Batch 34/64 loss: -0.7682104110717773
Batch 35/64 loss: -0.0703740119934082
Batch 36/64 loss: -0.5763974189758301
Batch 37/64 loss: -0.9044284820556641
Batch 38/64 loss: -0.9193363189697266
Batch 39/64 loss: -0.7832698822021484
Batch 40/64 loss: -0.6359720230102539
Batch 41/64 loss: -0.7884759902954102
Batch 42/64 loss: -0.8015012741088867
Batch 43/64 loss: -0.4043416976928711
Batch 44/64 loss: -0.4305000305175781
Batch 45/64 loss: -0.6978282928466797
Batch 46/64 loss: -1.031259536743164
Batch 47/64 loss: -0.6214075088500977
Batch 48/64 loss: -0.9860248565673828
Batch 49/64 loss: -0.5689182281494141
Batch 50/64 loss: -0.06625986099243164
Batch 51/64 loss: -0.9435234069824219
Batch 52/64 loss: -0.32709407806396484
Batch 53/64 loss: -0.5667190551757812
Batch 54/64 loss: -0.6431765556335449
Batch 55/64 loss: -0.8913116455078125
Batch 56/64 loss: -0.3262033462524414
Batch 57/64 loss: -0.8853054046630859
Batch 58/64 loss: -0.2095952033996582
Batch 59/64 loss: -0.5032153129577637
Batch 60/64 loss: -0.7754344940185547
Batch 61/64 loss: -0.42566490173339844
Batch 62/64 loss: -0.45883846282958984
Batch 63/64 loss: -0.715916633605957
Batch 64/64 loss: -4.573835849761963
Epoch 252  Train loss: -0.6437336098914053  Val loss: -0.7131001187354019
Epoch 253
-------------------------------
Batch 1/64 loss: -0.8629741668701172
Batch 2/64 loss: -0.8078594207763672
Batch 3/64 loss: -0.5098190307617188
Batch 4/64 loss: -0.31744384765625
Batch 5/64 loss: -0.7845993041992188
Batch 6/64 loss: -0.7143449783325195
Batch 7/64 loss: -0.9987668991088867
Batch 8/64 loss: -0.570012092590332
Batch 9/64 loss: -0.35552406311035156
Batch 10/64 loss: -0.4027233123779297
Batch 11/64 loss: -0.6832790374755859
Batch 12/64 loss: -0.8267812728881836
Batch 13/64 loss: -0.28102779388427734
Batch 14/64 loss: -0.3671417236328125
Batch 15/64 loss: -0.6031808853149414
Batch 16/64 loss: -0.541041374206543
Batch 17/64 loss: -0.7669229507446289
Batch 18/64 loss: -0.5957803726196289
Batch 19/64 loss: -0.9033994674682617
Batch 20/64 loss: -0.9776287078857422
Batch 21/64 loss: -0.6351189613342285
Batch 22/64 loss: -0.5289459228515625
Batch 23/64 loss: -0.9522504806518555
Batch 24/64 loss: -0.5452394485473633
Batch 25/64 loss: -0.6092338562011719
Batch 26/64 loss: -0.6096782684326172
Batch 27/64 loss: -0.8849706649780273
Batch 28/64 loss: -0.6897192001342773
Batch 29/64 loss: -0.6335697174072266
Batch 30/64 loss: -0.12752008438110352
Batch 31/64 loss: -0.5264987945556641
Batch 32/64 loss: -0.5881376266479492
Batch 33/64 loss: -0.6506214141845703
Batch 34/64 loss: -1.0461111068725586
Batch 35/64 loss: -0.6759271621704102
Batch 36/64 loss: -0.3341512680053711
Batch 37/64 loss: -0.44652557373046875
Batch 38/64 loss: -0.6881341934204102
Batch 39/64 loss: -0.4323863983154297
Batch 40/64 loss: -0.6293811798095703
Batch 41/64 loss: -0.6329526901245117
Batch 42/64 loss: -0.6724691390991211
Batch 43/64 loss: -0.4217071533203125
Batch 44/64 loss: -0.5489988327026367
Batch 45/64 loss: -0.3170490264892578
Batch 46/64 loss: -0.6508502960205078
Batch 47/64 loss: -0.9233989715576172
Batch 48/64 loss: 0.035025596618652344
Batch 49/64 loss: -0.4276752471923828
Batch 50/64 loss: -0.9133510589599609
Batch 51/64 loss: -0.09834957122802734
Batch 52/64 loss: -0.5864953994750977
Batch 53/64 loss: -0.5562267303466797
Batch 54/64 loss: -0.8563909530639648
Batch 55/64 loss: -0.7206230163574219
Batch 56/64 loss: -0.6596260070800781
Batch 57/64 loss: -0.9458599090576172
Batch 58/64 loss: -0.46861743927001953
Batch 59/64 loss: -0.49866390228271484
Batch 60/64 loss: -0.07981061935424805
Batch 61/64 loss: -0.5815095901489258
Batch 62/64 loss: -0.06576871871948242
Batch 63/64 loss: -0.8028602600097656
Batch 64/64 loss: -4.738996505737305
Epoch 253  Train loss: -0.6439662484561696  Val loss: -0.7609678314313856
Saving best model, epoch: 253
Epoch 254
-------------------------------
Batch 1/64 loss: -0.9540777206420898
Batch 2/64 loss: -0.22022438049316406
Batch 3/64 loss: -0.40322256088256836
Batch 4/64 loss: -0.7778739929199219
Batch 5/64 loss: -0.5458688735961914
Batch 6/64 loss: -0.3836555480957031
Batch 7/64 loss: -0.09312152862548828
Batch 8/64 loss: -0.19249868392944336
Batch 9/64 loss: -0.711756706237793
Batch 10/64 loss: -0.8680486679077148
Batch 11/64 loss: -0.5447463989257812
Batch 12/64 loss: -0.6931304931640625
Batch 13/64 loss: -0.5004911422729492
Batch 14/64 loss: -0.3349723815917969
Batch 15/64 loss: -0.999908447265625
Batch 16/64 loss: -0.8350849151611328
Batch 17/64 loss: -0.8941059112548828
Batch 18/64 loss: -0.7450351715087891
Batch 19/64 loss: -0.774724006652832
Batch 20/64 loss: -0.8372621536254883
Batch 21/64 loss: -0.952392578125
Batch 22/64 loss: -0.1673598289489746
Batch 23/64 loss: -0.2400522232055664
Batch 24/64 loss: -0.5265345573425293
Batch 25/64 loss: -0.4830813407897949
Batch 26/64 loss: -0.737309455871582
Batch 27/64 loss: -0.43291378021240234
Batch 28/64 loss: -0.34091949462890625
Batch 29/64 loss: -0.48398780822753906
Batch 30/64 loss: -0.3502960205078125
Batch 31/64 loss: -0.9624977111816406
Batch 32/64 loss: -0.49582672119140625
Batch 33/64 loss: -0.28762292861938477
Batch 34/64 loss: -0.7132177352905273
Batch 35/64 loss: -0.8587479591369629
Batch 36/64 loss: -0.7970733642578125
Batch 37/64 loss: -0.17266178131103516
Batch 38/64 loss: -0.505922794342041
Batch 39/64 loss: -0.31230831146240234
Batch 40/64 loss: -0.5003085136413574
Batch 41/64 loss: -0.5040416717529297
Batch 42/64 loss: -0.8685312271118164
Batch 43/64 loss: -0.5351896286010742
Batch 44/64 loss: -0.2428269386291504
Batch 45/64 loss: -0.32465553283691406
Batch 46/64 loss: -0.29996299743652344
Batch 47/64 loss: -0.6808328628540039
Batch 48/64 loss: -0.3619065284729004
Batch 49/64 loss: -1.1153244972229004
Batch 50/64 loss: -1.021388053894043
Batch 51/64 loss: -0.9001054763793945
Batch 52/64 loss: -0.44136524200439453
Batch 53/64 loss: -0.6488122940063477
Batch 54/64 loss: -0.9886789321899414
Batch 55/64 loss: -0.3118882179260254
Batch 56/64 loss: -0.5873255729675293
Batch 57/64 loss: -0.8658514022827148
Batch 58/64 loss: -0.7178964614868164
Batch 59/64 loss: -0.4721055030822754
Batch 60/64 loss: -0.5641441345214844
Batch 61/64 loss: -0.40744781494140625
Batch 62/64 loss: -0.8295211791992188
Batch 63/64 loss: -0.5957422256469727
Batch 64/64 loss: -4.360732078552246
Epoch 254  Train loss: -0.6303833419201421  Val loss: -0.6974123925277868
Epoch 255
-------------------------------
Batch 1/64 loss: -0.9975624084472656
Batch 2/64 loss: -0.3627643585205078
Batch 3/64 loss: -0.5141181945800781
Batch 4/64 loss: -0.3041067123413086
Batch 5/64 loss: -0.5998382568359375
Batch 6/64 loss: -0.32298946380615234
Batch 7/64 loss: -0.8068389892578125
Batch 8/64 loss: -0.4403200149536133
Batch 9/64 loss: -0.30100154876708984
Batch 10/64 loss: -0.7134933471679688
Batch 11/64 loss: -0.5634689331054688
Batch 12/64 loss: -0.2487645149230957
Batch 13/64 loss: -0.7765464782714844
Batch 14/64 loss: -0.9379205703735352
Batch 15/64 loss: -0.8401689529418945
Batch 16/64 loss: -0.5399293899536133
Batch 17/64 loss: -0.5104341506958008
Batch 18/64 loss: -0.6767902374267578
Batch 19/64 loss: -0.7033224105834961
Batch 20/64 loss: -0.5899896621704102
Batch 21/64 loss: -0.7909278869628906
Batch 22/64 loss: -0.6076240539550781
Batch 23/64 loss: -0.8177223205566406
Batch 24/64 loss: -0.6318521499633789
Batch 25/64 loss: -0.5448646545410156
Batch 26/64 loss: -0.7698287963867188
Batch 27/64 loss: -0.5116157531738281
Batch 28/64 loss: -0.6356000900268555
Batch 29/64 loss: -0.6341867446899414
Batch 30/64 loss: 0.17041349411010742
Batch 31/64 loss: -0.3287773132324219
Batch 32/64 loss: -0.33022069931030273
Batch 33/64 loss: -0.32577991485595703
Batch 34/64 loss: -0.9767637252807617
Batch 35/64 loss: -0.3537163734436035
Batch 36/64 loss: -0.6437282562255859
Batch 37/64 loss: -0.9444761276245117
Batch 38/64 loss: -0.6193323135375977
Batch 39/64 loss: -0.30254030227661133
Batch 40/64 loss: -0.46718311309814453
Batch 41/64 loss: -0.40970611572265625
Batch 42/64 loss: -0.42807435989379883
Batch 43/64 loss: -0.6696157455444336
Batch 44/64 loss: 0.25665950775146484
Batch 45/64 loss: 0.08220815658569336
Batch 46/64 loss: -0.19885921478271484
Batch 47/64 loss: -0.6754684448242188
Batch 48/64 loss: -0.49962902069091797
Batch 49/64 loss: -0.5998210906982422
Batch 50/64 loss: -0.3636493682861328
Batch 51/64 loss: -0.5958795547485352
Batch 52/64 loss: -0.6969833374023438
Batch 53/64 loss: -0.804530143737793
Batch 54/64 loss: -0.5878057479858398
Batch 55/64 loss: -0.9515781402587891
Batch 56/64 loss: -0.6052255630493164
Batch 57/64 loss: -0.7253952026367188
Batch 58/64 loss: -0.06518411636352539
Batch 59/64 loss: -0.44158267974853516
Batch 60/64 loss: -0.9348258972167969
Batch 61/64 loss: -0.8145055770874023
Batch 62/64 loss: -0.5295515060424805
Batch 63/64 loss: -0.5910015106201172
Batch 64/64 loss: -4.338301658630371
Epoch 255  Train loss: -0.5948302212883444  Val loss: -0.6887580897799882
Epoch 256
-------------------------------
Batch 1/64 loss: -0.7354040145874023
Batch 2/64 loss: -0.8078222274780273
Batch 3/64 loss: -0.7213258743286133
Batch 4/64 loss: -0.416287899017334
Batch 5/64 loss: -0.8589487075805664
Batch 6/64 loss: -0.6221432685852051
Batch 7/64 loss: -0.13553476333618164
Batch 8/64 loss: -0.6607646942138672
Batch 9/64 loss: -0.6978940963745117
Batch 10/64 loss: -0.01219797134399414
Batch 11/64 loss: -0.49036693572998047
Batch 12/64 loss: -0.6777257919311523
Batch 13/64 loss: -0.7077541351318359
Batch 14/64 loss: -0.1676774024963379
Batch 15/64 loss: -0.705510139465332
Batch 16/64 loss: -0.6701703071594238
Batch 17/64 loss: -0.783717155456543
Batch 18/64 loss: -0.5863242149353027
Batch 19/64 loss: -0.713496208190918
Batch 20/64 loss: -0.5958805084228516
Batch 21/64 loss: -0.5196075439453125
Batch 22/64 loss: -0.6359858512878418
Batch 23/64 loss: -0.17133760452270508
Batch 24/64 loss: -0.45813608169555664
Batch 25/64 loss: -0.7271838188171387
Batch 26/64 loss: -0.5259242057800293
Batch 27/64 loss: -0.6981201171875
Batch 28/64 loss: -0.732818603515625
Batch 29/64 loss: -0.8784341812133789
Batch 30/64 loss: -0.674372673034668
Batch 31/64 loss: -0.5466442108154297
Batch 32/64 loss: -0.6521816253662109
Batch 33/64 loss: -0.5460405349731445
Batch 34/64 loss: -0.4093008041381836
Batch 35/64 loss: -0.5492877960205078
Batch 36/64 loss: -0.7496633529663086
Batch 37/64 loss: -0.14380264282226562
Batch 38/64 loss: -0.5326423645019531
Batch 39/64 loss: -0.7017688751220703
Batch 40/64 loss: -0.46279335021972656
Batch 41/64 loss: -0.7601642608642578
Batch 42/64 loss: -0.5894851684570312
Batch 43/64 loss: -0.6400976181030273
Batch 44/64 loss: -0.28192806243896484
Batch 45/64 loss: -0.5332927703857422
Batch 46/64 loss: -0.07871437072753906
Batch 47/64 loss: -0.40349578857421875
Batch 48/64 loss: -0.6405763626098633
Batch 49/64 loss: -0.8697280883789062
Batch 50/64 loss: -0.4931044578552246
Batch 51/64 loss: -0.8197164535522461
Batch 52/64 loss: -0.8167924880981445
Batch 53/64 loss: -0.11008501052856445
Batch 54/64 loss: -0.4883732795715332
Batch 55/64 loss: -0.7755537033081055
Batch 56/64 loss: -0.28920793533325195
Batch 57/64 loss: -0.7752456665039062
Batch 58/64 loss: -0.2587885856628418
Batch 59/64 loss: -0.30324220657348633
Batch 60/64 loss: -0.7027969360351562
Batch 61/64 loss: -0.7181520462036133
Batch 62/64 loss: -0.8784723281860352
Batch 63/64 loss: -0.5757322311401367
Batch 64/64 loss: -4.507744789123535
Epoch 256  Train loss: -0.6159458347395355  Val loss: -0.7186354542106288
Epoch 257
-------------------------------
Batch 1/64 loss: -0.5055446624755859
Batch 2/64 loss: -0.8051309585571289
Batch 3/64 loss: -0.510169506072998
Batch 4/64 loss: -0.6038436889648438
Batch 5/64 loss: -0.6954975128173828
Batch 6/64 loss: -0.7735624313354492
Batch 7/64 loss: -0.8654384613037109
Batch 8/64 loss: -0.5980472564697266
Batch 9/64 loss: -0.27014827728271484
Batch 10/64 loss: -0.3995199203491211
Batch 11/64 loss: -0.8185234069824219
Batch 12/64 loss: -0.5388655662536621
Batch 13/64 loss: -0.3029603958129883
Batch 14/64 loss: 0.04746437072753906
Batch 15/64 loss: -0.4881453514099121
Batch 16/64 loss: -0.9015369415283203
Batch 17/64 loss: -0.631401538848877
Batch 18/64 loss: -0.8589687347412109
Batch 19/64 loss: -0.6389932632446289
Batch 20/64 loss: -0.5139555931091309
Batch 21/64 loss: -0.6164369583129883
Batch 22/64 loss: -0.6899533271789551
Batch 23/64 loss: -0.740203857421875
Batch 24/64 loss: -0.2575831413269043
Batch 25/64 loss: -0.6619892120361328
Batch 26/64 loss: -0.6863470077514648
Batch 27/64 loss: -0.6709718704223633
Batch 28/64 loss: -0.3514389991760254
Batch 29/64 loss: -0.6467385292053223
Batch 30/64 loss: -0.22454023361206055
Batch 31/64 loss: -0.9839153289794922
Batch 32/64 loss: -0.43828296661376953
Batch 33/64 loss: -0.8064393997192383
Batch 34/64 loss: -0.7809076309204102
Batch 35/64 loss: -0.31792783737182617
Batch 36/64 loss: -0.6096582412719727
Batch 37/64 loss: -0.4879641532897949
Batch 38/64 loss: -0.8279590606689453
Batch 39/64 loss: -0.7265653610229492
Batch 40/64 loss: -0.759272575378418
Batch 41/64 loss: -0.4194364547729492
Batch 42/64 loss: -0.5763101577758789
Batch 43/64 loss: -0.6470508575439453
Batch 44/64 loss: -0.7741355895996094
Batch 45/64 loss: -0.6454563140869141
Batch 46/64 loss: -0.39882707595825195
Batch 47/64 loss: -0.8881816864013672
Batch 48/64 loss: -0.42465972900390625
Batch 49/64 loss: -0.6302194595336914
Batch 50/64 loss: -0.33258914947509766
Batch 51/64 loss: -0.45659446716308594
Batch 52/64 loss: -0.37860631942749023
Batch 53/64 loss: -0.39936161041259766
Batch 54/64 loss: -0.23775625228881836
Batch 55/64 loss: -0.4388608932495117
Batch 56/64 loss: -0.23099327087402344
Batch 57/64 loss: -0.2652397155761719
Batch 58/64 loss: 0.07326793670654297
Batch 59/64 loss: 0.051555633544921875
Batch 60/64 loss: -0.7428207397460938
Batch 61/64 loss: -0.45035505294799805
Batch 62/64 loss: -0.4930391311645508
Batch 63/64 loss: -0.728358268737793
Batch 64/64 loss: -4.664258003234863
Epoch 257  Train loss: -0.5943546182969037  Val loss: -0.7182800450275854
Epoch 258
-------------------------------
Batch 1/64 loss: -0.6743221282958984
Batch 2/64 loss: -0.6859188079833984
Batch 3/64 loss: 0.022498130798339844
Batch 4/64 loss: -0.6074137687683105
Batch 5/64 loss: -0.33566856384277344
Batch 6/64 loss: -0.5973553657531738
Batch 7/64 loss: -0.3638787269592285
Batch 8/64 loss: -0.1333470344543457
Batch 9/64 loss: -0.7750749588012695
Batch 10/64 loss: -0.4674997329711914
Batch 11/64 loss: -0.6441287994384766
Batch 12/64 loss: -0.6294994354248047
Batch 13/64 loss: -0.19693851470947266
Batch 14/64 loss: -0.917445182800293
Batch 15/64 loss: -0.34772539138793945
Batch 16/64 loss: -0.3587522506713867
Batch 17/64 loss: -0.7578067779541016
Batch 18/64 loss: -0.8043756484985352
Batch 19/64 loss: -0.9010648727416992
Batch 20/64 loss: -0.6911983489990234
Batch 21/64 loss: -0.33965158462524414
Batch 22/64 loss: -0.459442138671875
Batch 23/64 loss: -0.5193929672241211
Batch 24/64 loss: -0.4200429916381836
Batch 25/64 loss: -0.41405344009399414
Batch 26/64 loss: 0.13798809051513672
Batch 27/64 loss: -0.7348432540893555
Batch 28/64 loss: -0.7088909149169922
Batch 29/64 loss: -0.4424161911010742
Batch 30/64 loss: -0.5490732192993164
Batch 31/64 loss: -0.05634641647338867
Batch 32/64 loss: -0.5988168716430664
Batch 33/64 loss: -0.5549259185791016
Batch 34/64 loss: -0.19096660614013672
Batch 35/64 loss: -0.6776828765869141
Batch 36/64 loss: -0.3990592956542969
Batch 37/64 loss: -0.4725961685180664
Batch 38/64 loss: -0.7284622192382812
Batch 39/64 loss: -0.33229541778564453
Batch 40/64 loss: -0.8764247894287109
Batch 41/64 loss: -0.8569250106811523
Batch 42/64 loss: -0.7427101135253906
Batch 43/64 loss: -0.9162826538085938
Batch 44/64 loss: -0.7636823654174805
Batch 45/64 loss: -0.14963436126708984
Batch 46/64 loss: -0.7652549743652344
Batch 47/64 loss: -0.7678985595703125
Batch 48/64 loss: -0.7347192764282227
Batch 49/64 loss: -0.26192140579223633
Batch 50/64 loss: -0.22055530548095703
Batch 51/64 loss: -0.5218582153320312
Batch 52/64 loss: -0.9914693832397461
Batch 53/64 loss: -0.6452417373657227
Batch 54/64 loss: 0.03008747100830078
Batch 55/64 loss: -0.7691860198974609
Batch 56/64 loss: -0.20736932754516602
Batch 57/64 loss: -0.676060676574707
Batch 58/64 loss: -0.912236213684082
Batch 59/64 loss: -0.30603933334350586
Batch 60/64 loss: -0.1957530975341797
Batch 61/64 loss: -0.7098197937011719
Batch 62/64 loss: -0.6270885467529297
Batch 63/64 loss: -0.45868778228759766
Batch 64/64 loss: -4.596041679382324
Epoch 258  Train loss: -0.5775944990270278  Val loss: -0.6482153168248966
Epoch 259
-------------------------------
Batch 1/64 loss: -0.802093505859375
Batch 2/64 loss: -0.8241891860961914
Batch 3/64 loss: -0.30889892578125
Batch 4/64 loss: -0.5479540824890137
Batch 5/64 loss: -0.5566978454589844
Batch 6/64 loss: -0.45816612243652344
Batch 7/64 loss: -0.4997901916503906
Batch 8/64 loss: -0.5769262313842773
Batch 9/64 loss: -0.3269367218017578
Batch 10/64 loss: -0.3538074493408203
Batch 11/64 loss: -0.612027645111084
Batch 12/64 loss: -0.4835224151611328
Batch 13/64 loss: -0.09159278869628906
Batch 14/64 loss: -0.5810108184814453
Batch 15/64 loss: -0.7398672103881836
Batch 16/64 loss: -0.7058887481689453
Batch 17/64 loss: -0.5663661956787109
Batch 18/64 loss: -0.6893291473388672
Batch 19/64 loss: -0.8232145309448242
Batch 20/64 loss: -0.60003662109375
Batch 21/64 loss: -0.6975126266479492
Batch 22/64 loss: -0.49282073974609375
Batch 23/64 loss: -0.552922248840332
Batch 24/64 loss: -0.5532779693603516
Batch 25/64 loss: -0.6772584915161133
Batch 26/64 loss: -0.23323345184326172
Batch 27/64 loss: -0.6613988876342773
Batch 28/64 loss: -0.533421516418457
Batch 29/64 loss: -0.20410442352294922
Batch 30/64 loss: -0.5384054183959961
Batch 31/64 loss: -0.12047624588012695
Batch 32/64 loss: -0.6516952514648438
Batch 33/64 loss: -0.95068359375
Batch 34/64 loss: -0.4380040168762207
Batch 35/64 loss: -0.4967231750488281
Batch 36/64 loss: -0.7573232650756836
Batch 37/64 loss: -0.9179458618164062
Batch 38/64 loss: -0.3692502975463867
Batch 39/64 loss: -0.5046300888061523
Batch 40/64 loss: -0.7791013717651367
Batch 41/64 loss: -0.6804900169372559
Batch 42/64 loss: -0.7424402236938477
Batch 43/64 loss: -0.5740137100219727
Batch 44/64 loss: -0.3824014663696289
Batch 45/64 loss: -0.45713138580322266
Batch 46/64 loss: -0.11342763900756836
Batch 47/64 loss: -0.5609893798828125
Batch 48/64 loss: -0.8769741058349609
Batch 49/64 loss: -0.5419368743896484
Batch 50/64 loss: -0.8698081970214844
Batch 51/64 loss: -0.8018112182617188
Batch 52/64 loss: -0.4238400459289551
Batch 53/64 loss: -0.3198375701904297
Batch 54/64 loss: -0.7397232055664062
Batch 55/64 loss: -0.7068042755126953
Batch 56/64 loss: -0.676722526550293
Batch 57/64 loss: -0.7271347045898438
Batch 58/64 loss: -0.33040475845336914
Batch 59/64 loss: -0.7344455718994141
Batch 60/64 loss: -0.860844612121582
Batch 61/64 loss: -0.5218706130981445
Batch 62/64 loss: -0.5254120826721191
Batch 63/64 loss: -0.5147905349731445
Batch 64/64 loss: -4.0938825607299805
Epoch 259  Train loss: -0.6122693641513002  Val loss: -0.6499304361769423
Epoch 260
-------------------------------
Batch 1/64 loss: -0.5049896240234375
Batch 2/64 loss: -0.6988582611083984
Batch 3/64 loss: -0.7040987014770508
Batch 4/64 loss: -0.3911457061767578
Batch 5/64 loss: -0.5650749206542969
Batch 6/64 loss: -0.8263044357299805
Batch 7/64 loss: -0.774439811706543
Batch 8/64 loss: -0.7731418609619141
Batch 9/64 loss: 0.007243633270263672
Batch 10/64 loss: -0.7805614471435547
Batch 11/64 loss: -0.36840152740478516
Batch 12/64 loss: -0.5226001739501953
Batch 13/64 loss: -0.8283462524414062
Batch 14/64 loss: -0.2477269172668457
Batch 15/64 loss: -0.8859100341796875
Batch 16/64 loss: -0.22078180313110352
Batch 17/64 loss: 0.28069496154785156
Batch 18/64 loss: -0.5833206176757812
Batch 19/64 loss: -0.3484182357788086
Batch 20/64 loss: -0.056514739990234375
Batch 21/64 loss: -0.5017566680908203
Batch 22/64 loss: -0.6390547752380371
Batch 23/64 loss: -0.8755874633789062
Batch 24/64 loss: -0.5437545776367188
Batch 25/64 loss: -0.5204391479492188
Batch 26/64 loss: -0.7775506973266602
Batch 27/64 loss: -0.7723789215087891
Batch 28/64 loss: -0.5706691741943359
Batch 29/64 loss: -0.11578893661499023
Batch 30/64 loss: -0.16323423385620117
Batch 31/64 loss: -0.8643369674682617
Batch 32/64 loss: -0.5109472274780273
Batch 33/64 loss: -0.3760490417480469
Batch 34/64 loss: -0.6422567367553711
Batch 35/64 loss: -0.09825515747070312
Batch 36/64 loss: -0.41666126251220703
Batch 37/64 loss: -0.8858232498168945
Batch 38/64 loss: -0.7382454872131348
Batch 39/64 loss: -0.9471511840820312
Batch 40/64 loss: -0.7374939918518066
Batch 41/64 loss: -0.5155267715454102
Batch 42/64 loss: -0.07042121887207031
Batch 43/64 loss: -0.27762317657470703
Batch 44/64 loss: -0.7993059158325195
Batch 45/64 loss: -0.6576118469238281
Batch 46/64 loss: -0.7300691604614258
Batch 47/64 loss: -0.6910057067871094
Batch 48/64 loss: -0.5046706199645996
Batch 49/64 loss: -0.6747951507568359
Batch 50/64 loss: -0.5335903167724609
Batch 51/64 loss: -0.6460156440734863
Batch 52/64 loss: -0.8297643661499023
Batch 53/64 loss: -0.7437801361083984
Batch 54/64 loss: -0.4841289520263672
Batch 55/64 loss: -0.08607864379882812
Batch 56/64 loss: -0.7903852462768555
Batch 57/64 loss: -0.7006416320800781
Batch 58/64 loss: -0.6379919052124023
Batch 59/64 loss: -0.7250490188598633
Batch 60/64 loss: -0.6466398239135742
Batch 61/64 loss: -0.16443824768066406
Batch 62/64 loss: -0.8949995040893555
Batch 63/64 loss: -0.3502388000488281
Batch 64/64 loss: -4.423969268798828
Epoch 260  Train loss: -0.5954961589738434  Val loss: -0.6754054957648733
Epoch 261
-------------------------------
Batch 1/64 loss: 0.1109013557434082
Batch 2/64 loss: -0.5928106307983398
Batch 3/64 loss: -0.7396183013916016
Batch 4/64 loss: -0.6214170455932617
Batch 5/64 loss: -0.11345195770263672
Batch 6/64 loss: -0.9454011917114258
Batch 7/64 loss: -0.3513936996459961
Batch 8/64 loss: -0.5958738327026367
Batch 9/64 loss: -0.6507987976074219
Batch 10/64 loss: -0.6755743026733398
Batch 11/64 loss: -0.8110795021057129
Batch 12/64 loss: -0.8562736511230469
Batch 13/64 loss: -0.8214330673217773
Batch 14/64 loss: -0.6763648986816406
Batch 15/64 loss: -0.3682699203491211
Batch 16/64 loss: -0.0139923095703125
Batch 17/64 loss: -0.037271976470947266
Batch 18/64 loss: -0.516139030456543
Batch 19/64 loss: -0.3893117904663086
Batch 20/64 loss: -0.5954523086547852
Batch 21/64 loss: -0.7815713882446289
Batch 22/64 loss: -0.4017753601074219
Batch 23/64 loss: -0.5712852478027344
Batch 24/64 loss: -0.7986593246459961
Batch 25/64 loss: -0.8115549087524414
Batch 26/64 loss: -0.5751829147338867
Batch 27/64 loss: -0.4878559112548828
Batch 28/64 loss: -0.3317413330078125
Batch 29/64 loss: -0.5853986740112305
Batch 30/64 loss: -0.5813474655151367
Batch 31/64 loss: -0.2927379608154297
Batch 32/64 loss: -0.4072732925415039
Batch 33/64 loss: -0.8052587509155273
Batch 34/64 loss: 0.1016225814819336
Batch 35/64 loss: -0.638275146484375
Batch 36/64 loss: -0.5872106552124023
Batch 37/64 loss: -0.6197795867919922
Batch 38/64 loss: -0.5827035903930664
Batch 39/64 loss: -0.6939563751220703
Batch 40/64 loss: -0.3587212562561035
Batch 41/64 loss: -0.060529232025146484
Batch 42/64 loss: -0.5082974433898926
Batch 43/64 loss: -0.27512359619140625
Batch 44/64 loss: -0.47669029235839844
Batch 45/64 loss: -0.5113420486450195
Batch 46/64 loss: -0.16272830963134766
Batch 47/64 loss: -0.6819419860839844
Batch 48/64 loss: -0.5249052047729492
Batch 49/64 loss: -0.3413729667663574
Batch 50/64 loss: -0.7151803970336914
Batch 51/64 loss: -0.38923025131225586
Batch 52/64 loss: -0.4691495895385742
Batch 53/64 loss: -0.5235567092895508
Batch 54/64 loss: -0.41736459732055664
Batch 55/64 loss: 0.03972196578979492
Batch 56/64 loss: -0.33754873275756836
Batch 57/64 loss: -0.4945945739746094
Batch 58/64 loss: -0.41155004501342773
Batch 59/64 loss: -0.5597610473632812
Batch 60/64 loss: -0.6603059768676758
Batch 61/64 loss: -0.0770115852355957
Batch 62/64 loss: -0.619478702545166
Batch 63/64 loss: -0.45534706115722656
Batch 64/64 loss: -4.590793132781982
Epoch 261  Train loss: -0.535656104368322  Val loss: -0.6195192763076205
Epoch 262
-------------------------------
Batch 1/64 loss: -0.26812744140625
Batch 2/64 loss: 0.055002689361572266
Batch 3/64 loss: -0.14220857620239258
Batch 4/64 loss: -0.7564525604248047
Batch 5/64 loss: -0.5182733535766602
Batch 6/64 loss: -0.8002243041992188
Batch 7/64 loss: -0.8152885437011719
Batch 8/64 loss: -0.8552770614624023
Batch 9/64 loss: -0.7247810363769531
Batch 10/64 loss: -0.6838340759277344
Batch 11/64 loss: -0.1555924415588379
Batch 12/64 loss: -0.6971673965454102
Batch 13/64 loss: -0.30521726608276367
Batch 14/64 loss: -0.8017282485961914
Batch 15/64 loss: 0.027548789978027344
Batch 16/64 loss: -0.9265623092651367
Batch 17/64 loss: -0.12364578247070312
Batch 18/64 loss: -0.49799537658691406
Batch 19/64 loss: -0.6791791915893555
Batch 20/64 loss: -0.5603036880493164
Batch 21/64 loss: -0.5503559112548828
Batch 22/64 loss: -0.7503089904785156
Batch 23/64 loss: -0.6064386367797852
Batch 24/64 loss: -0.7249317169189453
Batch 25/64 loss: -0.7261724472045898
Batch 26/64 loss: -0.4575538635253906
Batch 27/64 loss: -0.7036571502685547
Batch 28/64 loss: -0.8274393081665039
Batch 29/64 loss: -0.610567569732666
Batch 30/64 loss: -0.7140998840332031
Batch 31/64 loss: 0.2867617607116699
Batch 32/64 loss: -0.8124961853027344
Batch 33/64 loss: -0.4894747734069824
Batch 34/64 loss: -0.7541351318359375
Batch 35/64 loss: -0.8084478378295898
Batch 36/64 loss: -0.5659260749816895
Batch 37/64 loss: -0.818115234375
Batch 38/64 loss: -0.42409658432006836
Batch 39/64 loss: -0.48903846740722656
Batch 40/64 loss: -0.43840789794921875
Batch 41/64 loss: -0.5110588073730469
Batch 42/64 loss: -0.5760555267333984
Batch 43/64 loss: -0.17938852310180664
Batch 44/64 loss: -0.5270833969116211
Batch 45/64 loss: -0.4683198928833008
Batch 46/64 loss: -0.46802568435668945
Batch 47/64 loss: -0.2619161605834961
Batch 48/64 loss: -0.5352044105529785
Batch 49/64 loss: -0.4945049285888672
Batch 50/64 loss: 0.7769069671630859
Batch 51/64 loss: -0.4004945755004883
Batch 52/64 loss: -0.6652383804321289
Batch 53/64 loss: -0.43071937561035156
Batch 54/64 loss: -0.4813423156738281
Batch 55/64 loss: 0.24826574325561523
Batch 56/64 loss: -0.6265716552734375
Batch 57/64 loss: 0.2884654998779297
Batch 58/64 loss: -0.3153200149536133
Batch 59/64 loss: -0.5234689712524414
Batch 60/64 loss: 0.2640223503112793
Batch 61/64 loss: -0.10199260711669922
Batch 62/64 loss: -0.47576045989990234
Batch 63/64 loss: -0.28673553466796875
Batch 64/64 loss: -4.617631912231445
Epoch 262  Train loss: -0.5086897831337125  Val loss: -0.35001438835642185
Epoch 263
-------------------------------
Batch 1/64 loss: -0.7438883781433105
Batch 2/64 loss: -0.16888666152954102
Batch 3/64 loss: -0.3463282585144043
Batch 4/64 loss: -0.1666545867919922
Batch 5/64 loss: -0.2275533676147461
Batch 6/64 loss: -0.4002671241760254
Batch 7/64 loss: -0.2610964775085449
Batch 8/64 loss: -0.21035385131835938
Batch 9/64 loss: -0.5191831588745117
Batch 10/64 loss: -0.4173297882080078
Batch 11/64 loss: -0.2940349578857422
Batch 12/64 loss: -0.5821514129638672
Batch 13/64 loss: -0.2875537872314453
Batch 14/64 loss: -0.7682285308837891
Batch 15/64 loss: -0.6613531112670898
Batch 16/64 loss: -0.1568756103515625
Batch 17/64 loss: -0.12582635879516602
Batch 18/64 loss: -0.09045124053955078
Batch 19/64 loss: -0.7773604393005371
Batch 20/64 loss: -0.09344863891601562
Batch 21/64 loss: -0.4552450180053711
Batch 22/64 loss: -0.36744022369384766
Batch 23/64 loss: -0.6119623184204102
Batch 24/64 loss: 0.013953685760498047
Batch 25/64 loss: -0.5907220840454102
Batch 26/64 loss: -0.6017971038818359
Batch 27/64 loss: -0.8189716339111328
Batch 28/64 loss: -0.5266075134277344
Batch 29/64 loss: -0.32417869567871094
Batch 30/64 loss: -0.7457771301269531
Batch 31/64 loss: -0.7482919692993164
Batch 32/64 loss: -0.546867847442627
Batch 33/64 loss: -0.6971635818481445
Batch 34/64 loss: -0.7419133186340332
Batch 35/64 loss: -0.6460990905761719
Batch 36/64 loss: -0.5965852737426758
Batch 37/64 loss: -0.4723219871520996
Batch 38/64 loss: -0.37048959732055664
Batch 39/64 loss: -0.03608560562133789
Batch 40/64 loss: -0.6293611526489258
Batch 41/64 loss: -0.878870964050293
Batch 42/64 loss: -0.48772335052490234
Batch 43/64 loss: -0.604217529296875
Batch 44/64 loss: -0.7404251098632812
Batch 45/64 loss: -0.4662036895751953
Batch 46/64 loss: -0.5707426071166992
Batch 47/64 loss: -0.27546072006225586
Batch 48/64 loss: -0.15111446380615234
Batch 49/64 loss: -0.5030059814453125
Batch 50/64 loss: -0.7482070922851562
Batch 51/64 loss: -0.5153059959411621
Batch 52/64 loss: -0.5059347152709961
Batch 53/64 loss: -0.4900646209716797
Batch 54/64 loss: -0.4906148910522461
Batch 55/64 loss: -0.5504684448242188
Batch 56/64 loss: -0.7054882049560547
Batch 57/64 loss: -0.7440919876098633
Batch 58/64 loss: -0.04726696014404297
Batch 59/64 loss: -0.7377729415893555
Batch 60/64 loss: -0.5947732925415039
Batch 61/64 loss: -0.5797080993652344
Batch 62/64 loss: -0.6561994552612305
Batch 63/64 loss: -0.5204362869262695
Batch 64/64 loss: -4.389064788818359
Epoch 263  Train loss: -0.5281356736725452  Val loss: -0.6882719321758887
Epoch 264
-------------------------------
Batch 1/64 loss: -0.6630630493164062
Batch 2/64 loss: -0.5276327133178711
Batch 3/64 loss: -0.42575740814208984
Batch 4/64 loss: -0.8377170562744141
Batch 5/64 loss: -0.32362937927246094
Batch 6/64 loss: -0.5834922790527344
Batch 7/64 loss: -0.6638088226318359
Batch 8/64 loss: -0.4117298126220703
Batch 9/64 loss: -0.4293708801269531
Batch 10/64 loss: -0.3942127227783203
Batch 11/64 loss: -0.33884716033935547
Batch 12/64 loss: -0.7158002853393555
Batch 13/64 loss: -0.5577011108398438
Batch 14/64 loss: -0.6986293792724609
Batch 15/64 loss: -0.5238747596740723
Batch 16/64 loss: -0.7897253036499023
Batch 17/64 loss: -0.5426340103149414
Batch 18/64 loss: -0.48465728759765625
Batch 19/64 loss: -0.4614238739013672
Batch 20/64 loss: -0.7279958724975586
Batch 21/64 loss: -0.6992092132568359
Batch 22/64 loss: -0.4778566360473633
Batch 23/64 loss: -0.7950115203857422
Batch 24/64 loss: -0.6876010894775391
Batch 25/64 loss: -0.48334550857543945
Batch 26/64 loss: -0.5578947067260742
Batch 27/64 loss: -0.9070281982421875
Batch 28/64 loss: -0.5950374603271484
Batch 29/64 loss: 0.0023603439331054688
Batch 30/64 loss: -0.6598119735717773
Batch 31/64 loss: -0.7714414596557617
Batch 32/64 loss: -0.5719962120056152
Batch 33/64 loss: -0.7153940200805664
Batch 34/64 loss: -0.6323156356811523
Batch 35/64 loss: -0.5597324371337891
Batch 36/64 loss: -0.5298137664794922
Batch 37/64 loss: -0.5030860900878906
Batch 38/64 loss: -0.4731893539428711
Batch 39/64 loss: -0.6163730621337891
Batch 40/64 loss: -0.4093923568725586
Batch 41/64 loss: -0.5359063148498535
Batch 42/64 loss: -0.5092635154724121
Batch 43/64 loss: -0.7859325408935547
Batch 44/64 loss: -0.3661656379699707
Batch 45/64 loss: -0.21285152435302734
Batch 46/64 loss: -0.5224485397338867
Batch 47/64 loss: -0.28641700744628906
Batch 48/64 loss: -0.5592880249023438
Batch 49/64 loss: -0.547825813293457
Batch 50/64 loss: -0.7706451416015625
Batch 51/64 loss: -0.49530935287475586
Batch 52/64 loss: -0.7848386764526367
Batch 53/64 loss: -0.6568484306335449
Batch 54/64 loss: -0.8813905715942383
Batch 55/64 loss: -0.470977783203125
Batch 56/64 loss: -0.4052119255065918
Batch 57/64 loss: -1.0103302001953125
Batch 58/64 loss: -0.6792020797729492
Batch 59/64 loss: -0.6533360481262207
Batch 60/64 loss: -0.8085479736328125
Batch 61/64 loss: -0.5412073135375977
Batch 62/64 loss: -0.5499186515808105
Batch 63/64 loss: -0.6586761474609375
Batch 64/64 loss: -4.28530740737915
Epoch 264  Train loss: -0.621982639911128  Val loss: -0.7152126613761142
Epoch 265
-------------------------------
Batch 1/64 loss: -0.6043386459350586
Batch 2/64 loss: -0.2541036605834961
Batch 3/64 loss: -0.7033843994140625
Batch 4/64 loss: -0.6804523468017578
Batch 5/64 loss: -0.2646598815917969
Batch 6/64 loss: -0.2472095489501953
Batch 7/64 loss: -0.9726800918579102
Batch 8/64 loss: -0.5496273040771484
Batch 9/64 loss: -0.49007558822631836
Batch 10/64 loss: -0.27964210510253906
Batch 11/64 loss: -0.5041108131408691
Batch 12/64 loss: -0.6988558769226074
Batch 13/64 loss: -0.8301348686218262
Batch 14/64 loss: -0.5393276214599609
Batch 15/64 loss: -0.8134860992431641
Batch 16/64 loss: -0.24464941024780273
Batch 17/64 loss: -1.0401153564453125
Batch 18/64 loss: -0.7123956680297852
Batch 19/64 loss: -0.6110715866088867
Batch 20/64 loss: -0.8543987274169922
Batch 21/64 loss: -0.40468931198120117
Batch 22/64 loss: -0.5914831161499023
Batch 23/64 loss: -0.8294458389282227
Batch 24/64 loss: -0.5861897468566895
Batch 25/64 loss: -0.6309528350830078
Batch 26/64 loss: -0.5399017333984375
Batch 27/64 loss: -0.18964290618896484
Batch 28/64 loss: -0.27065467834472656
Batch 29/64 loss: -0.6526451110839844
Batch 30/64 loss: -0.4203648567199707
Batch 31/64 loss: -0.5374784469604492
Batch 32/64 loss: -0.6891145706176758
Batch 33/64 loss: -0.9701347351074219
Batch 34/64 loss: -0.8638639450073242
Batch 35/64 loss: -0.8108978271484375
Batch 36/64 loss: -0.5921001434326172
Batch 37/64 loss: -0.6986827850341797
Batch 38/64 loss: -0.6922931671142578
Batch 39/64 loss: -0.4877967834472656
Batch 40/64 loss: -0.9558925628662109
Batch 41/64 loss: -0.32808780670166016
Batch 42/64 loss: -0.6779508590698242
Batch 43/64 loss: -0.19032573699951172
Batch 44/64 loss: -0.8716583251953125
Batch 45/64 loss: -0.45438194274902344
Batch 46/64 loss: -0.5366611480712891
Batch 47/64 loss: -0.6035118103027344
Batch 48/64 loss: -0.3423933982849121
Batch 49/64 loss: -0.4996318817138672
Batch 50/64 loss: 0.08424854278564453
Batch 51/64 loss: -1.0147466659545898
Batch 52/64 loss: -0.8115558624267578
Batch 53/64 loss: -0.5993762016296387
Batch 54/64 loss: -0.21540451049804688
Batch 55/64 loss: -0.5139064788818359
Batch 56/64 loss: -0.6377010345458984
Batch 57/64 loss: -0.734126091003418
Batch 58/64 loss: -0.8165712356567383
Batch 59/64 loss: -0.7665691375732422
Batch 60/64 loss: -0.415402889251709
Batch 61/64 loss: -0.4879951477050781
Batch 62/64 loss: -0.21989774703979492
Batch 63/64 loss: -0.3670158386230469
Batch 64/64 loss: -4.49278450012207
Epoch 265  Train loss: -0.6227318632836435  Val loss: -0.7047056611051264
Epoch 266
-------------------------------
Batch 1/64 loss: -0.9770479202270508
Batch 2/64 loss: -0.43821287155151367
Batch 3/64 loss: -0.7240285873413086
Batch 4/64 loss: -0.6728572845458984
Batch 5/64 loss: -0.533473014831543
Batch 6/64 loss: -0.24359464645385742
Batch 7/64 loss: -0.7940349578857422
Batch 8/64 loss: 0.008231163024902344
Batch 9/64 loss: -0.7605218887329102
Batch 10/64 loss: -0.6264219284057617
Batch 11/64 loss: -0.4082489013671875
Batch 12/64 loss: -0.3169536590576172
Batch 13/64 loss: -0.2547111511230469
Batch 14/64 loss: -0.6511936187744141
Batch 15/64 loss: -0.19059467315673828
Batch 16/64 loss: -0.7764148712158203
Batch 17/64 loss: -0.35994482040405273
Batch 18/64 loss: -0.6777229309082031
Batch 19/64 loss: -0.9589872360229492
Batch 20/64 loss: 0.4725027084350586
Batch 21/64 loss: -0.6992263793945312
Batch 22/64 loss: -0.5567026138305664
Batch 23/64 loss: -0.4277763366699219
Batch 24/64 loss: -0.7511672973632812
Batch 25/64 loss: -0.5216102600097656
Batch 26/64 loss: -0.61419677734375
Batch 27/64 loss: -0.3887653350830078
Batch 28/64 loss: -0.6667346954345703
Batch 29/64 loss: -0.6056151390075684
Batch 30/64 loss: -0.15987110137939453
Batch 31/64 loss: -0.5228347778320312
Batch 32/64 loss: -0.2797527313232422
Batch 33/64 loss: -0.40682029724121094
Batch 34/64 loss: -0.8858709335327148
Batch 35/64 loss: -0.6714839935302734
Batch 36/64 loss: -0.5843544006347656
Batch 37/64 loss: -0.25114917755126953
Batch 38/64 loss: -0.47747373580932617
Batch 39/64 loss: -0.7291765213012695
Batch 40/64 loss: -0.21144962310791016
Batch 41/64 loss: -0.6674385070800781
Batch 42/64 loss: -0.7462368011474609
Batch 43/64 loss: -0.5613274574279785
Batch 44/64 loss: -0.5616445541381836
Batch 45/64 loss: -0.7613096237182617
Batch 46/64 loss: -0.5534343719482422
Batch 47/64 loss: -0.3558187484741211
Batch 48/64 loss: -0.12244606018066406
Batch 49/64 loss: -0.6079616546630859
Batch 50/64 loss: -0.6044998168945312
Batch 51/64 loss: -0.59197998046875
Batch 52/64 loss: -0.5996904373168945
Batch 53/64 loss: -0.5618038177490234
Batch 54/64 loss: -0.5270600318908691
Batch 55/64 loss: -0.5883612632751465
Batch 56/64 loss: -0.4674997329711914
Batch 57/64 loss: -0.6496067047119141
Batch 58/64 loss: 0.6201772689819336
Batch 59/64 loss: -0.5763282775878906
Batch 60/64 loss: -0.4679555892944336
Batch 61/64 loss: -0.5967807769775391
Batch 62/64 loss: -0.10381364822387695
Batch 63/64 loss: -0.5045347213745117
Batch 64/64 loss: -3.999142646789551
Epoch 266  Train loss: -0.5404388315537396  Val loss: -0.5020970216731435
Epoch 267
-------------------------------
Batch 1/64 loss: -0.41303396224975586
Batch 2/64 loss: -0.6605663299560547
Batch 3/64 loss: -0.7904958724975586
Batch 4/64 loss: -0.49086952209472656
Batch 5/64 loss: -0.38666534423828125
Batch 6/64 loss: -0.24862003326416016
Batch 7/64 loss: -0.5622673034667969
Batch 8/64 loss: -0.5671682357788086
Batch 9/64 loss: 0.04668712615966797
Batch 10/64 loss: -0.23002099990844727
Batch 11/64 loss: 0.14204978942871094
Batch 12/64 loss: 0.07856464385986328
Batch 13/64 loss: -0.26706886291503906
Batch 14/64 loss: -0.8832521438598633
Batch 15/64 loss: -0.6068563461303711
Batch 16/64 loss: -0.8434600830078125
Batch 17/64 loss: -0.5236845016479492
Batch 18/64 loss: -0.6966648101806641
Batch 19/64 loss: -0.7046871185302734
Batch 20/64 loss: -0.5469555854797363
Batch 21/64 loss: -0.2455754280090332
Batch 22/64 loss: -0.6974172592163086
Batch 23/64 loss: -0.661067008972168
Batch 24/64 loss: -0.9135837554931641
Batch 25/64 loss: -0.7089762687683105
Batch 26/64 loss: -0.07420492172241211
Batch 27/64 loss: -0.5984277725219727
Batch 28/64 loss: -0.5418643951416016
Batch 29/64 loss: -0.8931474685668945
Batch 30/64 loss: -0.385162353515625
Batch 31/64 loss: -0.381223201751709
Batch 32/64 loss: -0.75830078125
Batch 33/64 loss: -0.4845614433288574
Batch 34/64 loss: -0.4014420509338379
Batch 35/64 loss: -0.6736159324645996
Batch 36/64 loss: -0.8814339637756348
Batch 37/64 loss: -0.42164039611816406
Batch 38/64 loss: -0.6309452056884766
Batch 39/64 loss: -0.5788326263427734
Batch 40/64 loss: -0.7855625152587891
Batch 41/64 loss: -0.509765625
Batch 42/64 loss: -0.6596174240112305
Batch 43/64 loss: -0.25222349166870117
Batch 44/64 loss: -0.8184432983398438
Batch 45/64 loss: -0.5726995468139648
Batch 46/64 loss: -0.23965883255004883
Batch 47/64 loss: -0.28520822525024414
Batch 48/64 loss: -0.7130546569824219
Batch 49/64 loss: -0.6006755828857422
Batch 50/64 loss: -0.6607685089111328
Batch 51/64 loss: -0.5611910820007324
Batch 52/64 loss: -0.5724878311157227
Batch 53/64 loss: -0.6000962257385254
Batch 54/64 loss: -0.72412109375
Batch 55/64 loss: -0.6442956924438477
Batch 56/64 loss: -0.8400230407714844
Batch 57/64 loss: -0.3188161849975586
Batch 58/64 loss: -0.20386028289794922
Batch 59/64 loss: -0.7193326950073242
Batch 60/64 loss: -0.6156940460205078
Batch 61/64 loss: -0.8717184066772461
Batch 62/64 loss: -0.5263524055480957
Batch 63/64 loss: -1.037710189819336
Batch 64/64 loss: -4.372464179992676
Epoch 267  Train loss: -0.5913597293928557  Val loss: -0.6401852479915029
Epoch 268
-------------------------------
Batch 1/64 loss: -0.4805116653442383
Batch 2/64 loss: -0.59417724609375
Batch 3/64 loss: -0.8799343109130859
Batch 4/64 loss: -0.22092676162719727
Batch 5/64 loss: -0.2560758590698242
Batch 6/64 loss: -0.7550525665283203
Batch 7/64 loss: -0.750391960144043
Batch 8/64 loss: -0.5887718200683594
Batch 9/64 loss: -0.8168134689331055
Batch 10/64 loss: -0.936248779296875
Batch 11/64 loss: -0.6358203887939453
Batch 12/64 loss: -0.9421606063842773
Batch 13/64 loss: -0.7837848663330078
Batch 14/64 loss: -0.901392936706543
Batch 15/64 loss: -0.8262796401977539
Batch 16/64 loss: -0.650299072265625
Batch 17/64 loss: -0.3194432258605957
Batch 18/64 loss: -0.6260547637939453
Batch 19/64 loss: -0.7279348373413086
Batch 20/64 loss: -0.15876150131225586
Batch 21/64 loss: -0.8563947677612305
Batch 22/64 loss: -0.8221025466918945
Batch 23/64 loss: -0.29929351806640625
Batch 24/64 loss: -0.13160133361816406
Batch 25/64 loss: -0.5487937927246094
Batch 26/64 loss: -0.7528390884399414
Batch 27/64 loss: -0.42882728576660156
Batch 28/64 loss: -0.3869647979736328
Batch 29/64 loss: -0.5792903900146484
Batch 30/64 loss: -0.7653665542602539
Batch 31/64 loss: -0.5118484497070312
Batch 32/64 loss: -0.5822267532348633
Batch 33/64 loss: -0.7389011383056641
Batch 34/64 loss: -0.7266578674316406
Batch 35/64 loss: -0.8209524154663086
Batch 36/64 loss: -0.5790653228759766
Batch 37/64 loss: -0.8711996078491211
Batch 38/64 loss: 0.037273406982421875
Batch 39/64 loss: -0.49455928802490234
Batch 40/64 loss: -0.6442689895629883
Batch 41/64 loss: -0.6613655090332031
Batch 42/64 loss: -0.20763111114501953
Batch 43/64 loss: -0.36358642578125
Batch 44/64 loss: -0.8899822235107422
Batch 45/64 loss: -0.4307575225830078
Batch 46/64 loss: -0.7686700820922852
Batch 47/64 loss: -0.7344555854797363
Batch 48/64 loss: -0.7741680145263672
Batch 49/64 loss: -0.7975339889526367
Batch 50/64 loss: -0.5944867134094238
Batch 51/64 loss: -0.46378040313720703
Batch 52/64 loss: -0.4572019577026367
Batch 53/64 loss: -0.7465963363647461
Batch 54/64 loss: 0.11530590057373047
Batch 55/64 loss: -0.7463064193725586
Batch 56/64 loss: -0.24616384506225586
Batch 57/64 loss: -0.7778234481811523
Batch 58/64 loss: -0.6022491455078125
Batch 59/64 loss: -0.3135690689086914
Batch 60/64 loss: -0.7395572662353516
Batch 61/64 loss: -0.4970674514770508
Batch 62/64 loss: -0.19112634658813477
Batch 63/64 loss: -0.726588249206543
Batch 64/64 loss: -3.977921962738037
Epoch 268  Train loss: -0.6267218739378686  Val loss: -0.6503901268608382
Epoch 269
-------------------------------
Batch 1/64 loss: -0.4389948844909668
Batch 2/64 loss: -0.8373966217041016
Batch 3/64 loss: -0.582423210144043
Batch 4/64 loss: -0.25452136993408203
Batch 5/64 loss: -0.8973379135131836
Batch 6/64 loss: -0.9491510391235352
Batch 7/64 loss: -0.3203878402709961
Batch 8/64 loss: 0.08090686798095703
Batch 9/64 loss: -0.5372962951660156
Batch 10/64 loss: -0.6270513534545898
Batch 11/64 loss: -0.8156623840332031
Batch 12/64 loss: -0.6635532379150391
Batch 13/64 loss: -0.760655403137207
Batch 14/64 loss: -0.8594522476196289
Batch 15/64 loss: -0.6388669013977051
Batch 16/64 loss: -0.6432666778564453
Batch 17/64 loss: -0.8178958892822266
Batch 18/64 loss: 0.055774688720703125
Batch 19/64 loss: -0.46895551681518555
Batch 20/64 loss: -0.5665555000305176
Batch 21/64 loss: -0.7907772064208984
Batch 22/64 loss: -0.8368625640869141
Batch 23/64 loss: -0.5248546600341797
Batch 24/64 loss: -0.8555707931518555
Batch 25/64 loss: -0.5047292709350586
Batch 26/64 loss: -0.34209489822387695
Batch 27/64 loss: -0.7002391815185547
Batch 28/64 loss: -0.4548988342285156
Batch 29/64 loss: -0.6480541229248047
Batch 30/64 loss: -0.5585737228393555
Batch 31/64 loss: -0.8138256072998047
Batch 32/64 loss: -0.6197748184204102
Batch 33/64 loss: -0.31666088104248047
Batch 34/64 loss: -0.4888143539428711
Batch 35/64 loss: -0.6302413940429688
Batch 36/64 loss: -0.2564105987548828
Batch 37/64 loss: -0.49684810638427734
Batch 38/64 loss: -0.284329891204834
Batch 39/64 loss: -0.6485633850097656
Batch 40/64 loss: -0.41182756423950195
Batch 41/64 loss: -0.6941919326782227
Batch 42/64 loss: 0.11557674407958984
Batch 43/64 loss: -0.0762624740600586
Batch 44/64 loss: -0.12413835525512695
Batch 45/64 loss: -0.3850126266479492
Batch 46/64 loss: -0.4253683090209961
Batch 47/64 loss: -0.2986445426940918
Batch 48/64 loss: -0.7080163955688477
Batch 49/64 loss: -0.2513427734375
Batch 50/64 loss: -0.8487939834594727
Batch 51/64 loss: -0.6528482437133789
Batch 52/64 loss: -0.38089990615844727
Batch 53/64 loss: -0.7137737274169922
Batch 54/64 loss: -0.29688024520874023
Batch 55/64 loss: -0.7335968017578125
Batch 56/64 loss: -0.3875856399536133
Batch 57/64 loss: -0.6212701797485352
Batch 58/64 loss: -0.3377876281738281
Batch 59/64 loss: -0.22396421432495117
Batch 60/64 loss: -0.3546786308288574
Batch 61/64 loss: -0.3458595275878906
Batch 62/64 loss: -0.7217369079589844
Batch 63/64 loss: -0.637420654296875
Batch 64/64 loss: -4.826557159423828
Epoch 269  Train loss: -0.5717821083816828  Val loss: -0.6573913285822394
Epoch 270
-------------------------------
Batch 1/64 loss: -0.7164163589477539
Batch 2/64 loss: -0.5453329086303711
Batch 3/64 loss: -0.8909263610839844
Batch 4/64 loss: -0.6042108535766602
Batch 5/64 loss: -0.793391227722168
Batch 6/64 loss: -0.3883213996887207
Batch 7/64 loss: -0.55963134765625
Batch 8/64 loss: -0.4367237091064453
Batch 9/64 loss: -0.41281795501708984
Batch 10/64 loss: -0.40022897720336914
Batch 11/64 loss: -0.27462291717529297
Batch 12/64 loss: -0.4724130630493164
Batch 13/64 loss: -0.23160552978515625
Batch 14/64 loss: -0.6467933654785156
Batch 15/64 loss: -0.5558328628540039
Batch 16/64 loss: -0.6687374114990234
Batch 17/64 loss: -0.4228334426879883
Batch 18/64 loss: -0.4538564682006836
Batch 19/64 loss: -0.5866885185241699
Batch 20/64 loss: -0.7080812454223633
Batch 21/64 loss: -0.44724464416503906
Batch 22/64 loss: -0.3346433639526367
Batch 23/64 loss: -0.1810626983642578
Batch 24/64 loss: -0.9228315353393555
Batch 25/64 loss: -0.6599197387695312
Batch 26/64 loss: -0.693507194519043
Batch 27/64 loss: -0.4814901351928711
Batch 28/64 loss: -0.3939065933227539
Batch 29/64 loss: -0.5903463363647461
Batch 30/64 loss: -0.8155355453491211
Batch 31/64 loss: -0.7311172485351562
Batch 32/64 loss: -0.6534490585327148
Batch 33/64 loss: -0.5162239074707031
Batch 34/64 loss: -0.8212966918945312
Batch 35/64 loss: -0.5944552421569824
Batch 36/64 loss: -0.7052946090698242
Batch 37/64 loss: -0.7144222259521484
Batch 38/64 loss: -0.7622385025024414
Batch 39/64 loss: -0.34656333923339844
Batch 40/64 loss: -0.714381217956543
Batch 41/64 loss: -0.49637508392333984
Batch 42/64 loss: -0.29383373260498047
Batch 43/64 loss: -0.4996681213378906
Batch 44/64 loss: 0.6240396499633789
Batch 45/64 loss: -0.7983531951904297
Batch 46/64 loss: -0.24413585662841797
Batch 47/64 loss: -0.7632246017456055
Batch 48/64 loss: -0.5432424545288086
Batch 49/64 loss: -0.39846086502075195
Batch 50/64 loss: -0.25720787048339844
Batch 51/64 loss: -0.04812479019165039
Batch 52/64 loss: -0.40873003005981445
Batch 53/64 loss: -0.6229696273803711
Batch 54/64 loss: -0.516728401184082
Batch 55/64 loss: -0.8742408752441406
Batch 56/64 loss: -0.795384407043457
Batch 57/64 loss: -0.4606161117553711
Batch 58/64 loss: -0.6073436737060547
Batch 59/64 loss: -0.520805835723877
Batch 60/64 loss: -0.32392263412475586
Batch 61/64 loss: -0.45099544525146484
Batch 62/64 loss: -0.28186702728271484
Batch 63/64 loss: -0.3394432067871094
Batch 64/64 loss: -4.068112373352051
Epoch 270  Train loss: -0.5619155173208199  Val loss: -0.562081510668358
Epoch 271
-------------------------------
Batch 1/64 loss: -0.575782299041748
Batch 2/64 loss: -0.637934684753418
Batch 3/64 loss: -0.5181159973144531
Batch 4/64 loss: -0.46782588958740234
Batch 5/64 loss: -0.6487150192260742
Batch 6/64 loss: -0.7505717277526855
Batch 7/64 loss: -0.45417308807373047
Batch 8/64 loss: -0.5566520690917969
Batch 9/64 loss: -0.6136865615844727
Batch 10/64 loss: -0.660405158996582
Batch 11/64 loss: -0.19641685485839844
Batch 12/64 loss: -0.6171245574951172
Batch 13/64 loss: -0.5609045028686523
Batch 14/64 loss: -0.5685606002807617
Batch 15/64 loss: -0.6497640609741211
Batch 16/64 loss: -1.0019645690917969
Batch 17/64 loss: -0.5919818878173828
Batch 18/64 loss: -0.5498142242431641
Batch 19/64 loss: -0.43848228454589844
Batch 20/64 loss: -0.4316444396972656
Batch 21/64 loss: -0.5336980819702148
Batch 22/64 loss: -0.8789787292480469
Batch 23/64 loss: -0.6042661666870117
Batch 24/64 loss: -0.7819805145263672
Batch 25/64 loss: -0.5612363815307617
Batch 26/64 loss: -0.6167497634887695
Batch 27/64 loss: -0.5481729507446289
Batch 28/64 loss: -0.3604917526245117
Batch 29/64 loss: -0.3460049629211426
Batch 30/64 loss: -0.27999114990234375
Batch 31/64 loss: -0.3582572937011719
Batch 32/64 loss: -0.9956779479980469
Batch 33/64 loss: -0.716303825378418
Batch 34/64 loss: -0.8427362442016602
Batch 35/64 loss: -0.4860105514526367
Batch 36/64 loss: -0.26406288146972656
Batch 37/64 loss: -0.6016898155212402
Batch 38/64 loss: -0.8664150238037109
Batch 39/64 loss: -0.5622930526733398
Batch 40/64 loss: -0.2120833396911621
Batch 41/64 loss: -0.42711734771728516
Batch 42/64 loss: -0.45233917236328125
Batch 43/64 loss: -0.584376335144043
Batch 44/64 loss: -0.6726512908935547
Batch 45/64 loss: -0.3283576965332031
Batch 46/64 loss: -0.5895118713378906
Batch 47/64 loss: -0.5954122543334961
Batch 48/64 loss: -0.035119056701660156
Batch 49/64 loss: -0.7502841949462891
Batch 50/64 loss: -0.5995206832885742
Batch 51/64 loss: -0.4683842658996582
Batch 52/64 loss: -0.7868232727050781
Batch 53/64 loss: -0.041399478912353516
Batch 54/64 loss: -0.9571542739868164
Batch 55/64 loss: -0.41994619369506836
Batch 56/64 loss: -0.5000014305114746
Batch 57/64 loss: -0.6237449645996094
Batch 58/64 loss: -0.8318405151367188
Batch 59/64 loss: -0.8749990463256836
Batch 60/64 loss: -0.24248695373535156
Batch 61/64 loss: -0.27149391174316406
Batch 62/64 loss: -1.033534049987793
Batch 63/64 loss: -0.7041816711425781
Batch 64/64 loss: -4.301060199737549
Epoch 271  Train loss: -0.6105740547180176  Val loss: -0.707555816755262
Epoch 272
-------------------------------
Batch 1/64 loss: -0.8012971878051758
Batch 2/64 loss: -0.6993122100830078
Batch 3/64 loss: -0.40643739700317383
Batch 4/64 loss: -0.6326279640197754
Batch 5/64 loss: -0.6010174751281738
Batch 6/64 loss: -0.8176107406616211
Batch 7/64 loss: -0.2258462905883789
Batch 8/64 loss: -0.5227184295654297
Batch 9/64 loss: -0.9603567123413086
Batch 10/64 loss: -0.5703535079956055
Batch 11/64 loss: -0.8642787933349609
Batch 12/64 loss: 0.2554187774658203
Batch 13/64 loss: -0.6654434204101562
Batch 14/64 loss: -0.6680135726928711
Batch 15/64 loss: -0.6683015823364258
Batch 16/64 loss: -0.1511850357055664
Batch 17/64 loss: -0.5615110397338867
Batch 18/64 loss: -0.920013427734375
Batch 19/64 loss: -0.12173986434936523
Batch 20/64 loss: -0.6745510101318359
Batch 21/64 loss: -0.6040892601013184
Batch 22/64 loss: -0.8344078063964844
Batch 23/64 loss: -0.6547174453735352
Batch 24/64 loss: -0.6091485023498535
Batch 25/64 loss: -0.3995351791381836
Batch 26/64 loss: -0.8187837600708008
Batch 27/64 loss: -0.9842424392700195
Batch 28/64 loss: -0.49352455139160156
Batch 29/64 loss: -0.41427040100097656
Batch 30/64 loss: -0.4805030822753906
Batch 31/64 loss: -0.43648862838745117
Batch 32/64 loss: -0.547515869140625
Batch 33/64 loss: -0.5542736053466797
Batch 34/64 loss: -0.8283510208129883
Batch 35/64 loss: -0.7032432556152344
Batch 36/64 loss: -0.7149267196655273
Batch 37/64 loss: -0.6957521438598633
Batch 38/64 loss: -0.8883819580078125
Batch 39/64 loss: -0.7869739532470703
Batch 40/64 loss: -0.4114542007446289
Batch 41/64 loss: -0.8509616851806641
Batch 42/64 loss: -0.6103734970092773
Batch 43/64 loss: -0.7810335159301758
Batch 44/64 loss: 0.3689093589782715
Batch 45/64 loss: 0.1374664306640625
Batch 46/64 loss: -0.8045949935913086
Batch 47/64 loss: -0.7894191741943359
Batch 48/64 loss: -0.4944887161254883
Batch 49/64 loss: -0.6074943542480469
Batch 50/64 loss: -0.7601079940795898
Batch 51/64 loss: -0.6488590240478516
Batch 52/64 loss: -0.5625429153442383
Batch 53/64 loss: -0.8219509124755859
Batch 54/64 loss: -0.6480932235717773
Batch 55/64 loss: -0.16507911682128906
Batch 56/64 loss: -1.007979393005371
Batch 57/64 loss: -0.6752996444702148
Batch 58/64 loss: -0.5621013641357422
Batch 59/64 loss: -0.6016998291015625
Batch 60/64 loss: -0.6243085861206055
Batch 61/64 loss: 0.05146026611328125
Batch 62/64 loss: -0.8046703338623047
Batch 63/64 loss: -1.0068578720092773
Batch 64/64 loss: -4.611861228942871
Epoch 272  Train loss: -0.6410471635706284  Val loss: -0.741518302471777
Epoch 273
-------------------------------
Batch 1/64 loss: -0.6588869094848633
Batch 2/64 loss: -0.3651266098022461
Batch 3/64 loss: -0.38816070556640625
Batch 4/64 loss: -0.19765758514404297
Batch 5/64 loss: -0.580134391784668
Batch 6/64 loss: -0.5585012435913086
Batch 7/64 loss: -0.39519500732421875
Batch 8/64 loss: -0.5760374069213867
Batch 9/64 loss: -0.7560291290283203
Batch 10/64 loss: -0.7599306106567383
Batch 11/64 loss: -0.9178285598754883
Batch 12/64 loss: -0.8106002807617188
Batch 13/64 loss: -0.3530416488647461
Batch 14/64 loss: -0.3569450378417969
Batch 15/64 loss: -0.7300033569335938
Batch 16/64 loss: -0.3855924606323242
Batch 17/64 loss: -0.5024490356445312
Batch 18/64 loss: -0.6194334030151367
Batch 19/64 loss: -0.22975730895996094
Batch 20/64 loss: -0.4906597137451172
Batch 21/64 loss: -0.45372486114501953
Batch 22/64 loss: -0.4735736846923828
Batch 23/64 loss: -0.5157737731933594
Batch 24/64 loss: -0.4589376449584961
Batch 25/64 loss: -0.7220773696899414
Batch 26/64 loss: -0.5531768798828125
Batch 27/64 loss: -0.6395835876464844
Batch 28/64 loss: -0.21151161193847656
Batch 29/64 loss: -0.23217487335205078
Batch 30/64 loss: -0.3130226135253906
Batch 31/64 loss: -0.0993642807006836
Batch 32/64 loss: -0.38625526428222656
Batch 33/64 loss: -0.21292781829833984
Batch 34/64 loss: -0.5220670700073242
Batch 35/64 loss: -0.48519229888916016
Batch 36/64 loss: -0.1410226821899414
Batch 37/64 loss: -0.5096025466918945
Batch 38/64 loss: -0.7918605804443359
Batch 39/64 loss: -0.557164192199707
Batch 40/64 loss: 0.27713489532470703
Batch 41/64 loss: 0.05279970169067383
Batch 42/64 loss: -0.5385255813598633
Batch 43/64 loss: -0.4138631820678711
Batch 44/64 loss: -0.6796731948852539
Batch 45/64 loss: 0.07503938674926758
Batch 46/64 loss: -0.6936960220336914
Batch 47/64 loss: -0.6060523986816406
Batch 48/64 loss: -0.5975933074951172
Batch 49/64 loss: -0.13285541534423828
Batch 50/64 loss: -0.6719207763671875
Batch 51/64 loss: -0.6716976165771484
Batch 52/64 loss: -0.4334268569946289
Batch 53/64 loss: -0.7378787994384766
Batch 54/64 loss: -0.81207275390625
Batch 55/64 loss: -0.4657621383666992
Batch 56/64 loss: -0.6400690078735352
Batch 57/64 loss: -0.721613883972168
Batch 58/64 loss: -0.6512985229492188
Batch 59/64 loss: -0.5490589141845703
Batch 60/64 loss: -0.7981643676757812
Batch 61/64 loss: -0.6671056747436523
Batch 62/64 loss: -0.6136598587036133
Batch 63/64 loss: -0.3516397476196289
Batch 64/64 loss: -4.466808319091797
Epoch 273  Train loss: -0.5380979500564874  Val loss: -0.6045214466212951
Epoch 274
-------------------------------
Batch 1/64 loss: -0.44699668884277344
Batch 2/64 loss: -0.12972116470336914
Batch 3/64 loss: -0.616213321685791
Batch 4/64 loss: -0.30668163299560547
Batch 5/64 loss: -0.645637035369873
Batch 6/64 loss: -0.279388427734375
Batch 7/64 loss: -0.4026470184326172
Batch 8/64 loss: -0.2665872573852539
Batch 9/64 loss: -0.6696767807006836
Batch 10/64 loss: -0.7810478210449219
Batch 11/64 loss: -0.42726802825927734
Batch 12/64 loss: -0.7715654373168945
Batch 13/64 loss: -0.5757646560668945
Batch 14/64 loss: -0.36716699600219727
Batch 15/64 loss: -0.5834665298461914
Batch 16/64 loss: -0.529139518737793
Batch 17/64 loss: -0.7948799133300781
Batch 18/64 loss: -0.6589937210083008
Batch 19/64 loss: -0.8709011077880859
Batch 20/64 loss: -0.16612720489501953
Batch 21/64 loss: -0.4879589080810547
Batch 22/64 loss: -0.8604488372802734
Batch 23/64 loss: -0.10610628128051758
Batch 24/64 loss: -0.693812370300293
Batch 25/64 loss: 0.0010528564453125
Batch 26/64 loss: -0.5422391891479492
Batch 27/64 loss: -0.6234531402587891
Batch 28/64 loss: 0.31906557083129883
Batch 29/64 loss: -0.5522327423095703
Batch 30/64 loss: -0.6790637969970703
Batch 31/64 loss: -0.9961881637573242
Batch 32/64 loss: -0.18703365325927734
Batch 33/64 loss: -0.7570991516113281
Batch 34/64 loss: -0.6445064544677734
Batch 35/64 loss: -0.7345256805419922
Batch 36/64 loss: -0.5774106979370117
Batch 37/64 loss: -0.6558933258056641
Batch 38/64 loss: -0.819972038269043
Batch 39/64 loss: -0.7080621719360352
Batch 40/64 loss: -0.6529569625854492
Batch 41/64 loss: -0.22988557815551758
Batch 42/64 loss: -0.1642627716064453
Batch 43/64 loss: -0.6204118728637695
Batch 44/64 loss: -0.7006149291992188
Batch 45/64 loss: -0.6138067245483398
Batch 46/64 loss: -0.6130514144897461
Batch 47/64 loss: -0.07661104202270508
Batch 48/64 loss: -0.40875816345214844
Batch 49/64 loss: -0.5753922462463379
Batch 50/64 loss: -0.7932882308959961
Batch 51/64 loss: -0.5160064697265625
Batch 52/64 loss: -0.5066318511962891
Batch 53/64 loss: -0.3607025146484375
Batch 54/64 loss: -0.7421998977661133
Batch 55/64 loss: -0.5018339157104492
Batch 56/64 loss: -0.5536088943481445
Batch 57/64 loss: -0.6819047927856445
Batch 58/64 loss: -0.23293781280517578
Batch 59/64 loss: -0.0054798126220703125
Batch 60/64 loss: -1.0242500305175781
Batch 61/64 loss: -0.9601755142211914
Batch 62/64 loss: -0.7259044647216797
Batch 63/64 loss: -0.3552722930908203
Batch 64/64 loss: -4.471728324890137
Epoch 274  Train loss: -0.5735765157961378  Val loss: -0.7523951579615012
Epoch 275
-------------------------------
Batch 1/64 loss: -0.7170314788818359
Batch 2/64 loss: -0.6601343154907227
Batch 3/64 loss: -0.7364435195922852
Batch 4/64 loss: -0.631464958190918
Batch 5/64 loss: -1.0193853378295898
Batch 6/64 loss: -0.9487733840942383
Batch 7/64 loss: -0.6166372299194336
Batch 8/64 loss: -0.7939786911010742
Batch 9/64 loss: -0.5156707763671875
Batch 10/64 loss: -0.621002197265625
Batch 11/64 loss: -0.8374176025390625
Batch 12/64 loss: -0.8338375091552734
Batch 13/64 loss: -0.6503372192382812
Batch 14/64 loss: -0.43277931213378906
Batch 15/64 loss: -0.8313760757446289
Batch 16/64 loss: -0.330902099609375
Batch 17/64 loss: -0.3630495071411133
Batch 18/64 loss: -0.735896110534668
Batch 19/64 loss: -0.15809154510498047
Batch 20/64 loss: -0.6835231781005859
Batch 21/64 loss: -0.6385021209716797
Batch 22/64 loss: -0.5256576538085938
Batch 23/64 loss: -0.735865592956543
Batch 24/64 loss: -0.27203941345214844
Batch 25/64 loss: -0.25434017181396484
Batch 26/64 loss: -0.7036037445068359
Batch 27/64 loss: -0.5735244750976562
Batch 28/64 loss: -0.7908382415771484
Batch 29/64 loss: -0.6987228393554688
Batch 30/64 loss: -0.4013023376464844
Batch 31/64 loss: -0.6624894142150879
Batch 32/64 loss: -0.7987947463989258
Batch 33/64 loss: -0.5554437637329102
Batch 34/64 loss: -0.09880924224853516
Batch 35/64 loss: -0.8005485534667969
Batch 36/64 loss: -0.015961170196533203
Batch 37/64 loss: -0.6164250373840332
Batch 38/64 loss: -0.3582115173339844
Batch 39/64 loss: -0.3850059509277344
Batch 40/64 loss: -0.6388521194458008
Batch 41/64 loss: -0.49334239959716797
Batch 42/64 loss: -0.6341991424560547
Batch 43/64 loss: 0.14355707168579102
Batch 44/64 loss: -0.7586259841918945
Batch 45/64 loss: -0.7875766754150391
Batch 46/64 loss: -0.626643180847168
Batch 47/64 loss: -1.0058698654174805
Batch 48/64 loss: -0.936223030090332
Batch 49/64 loss: -0.6304492950439453
Batch 50/64 loss: -0.33773040771484375
Batch 51/64 loss: -0.7285652160644531
Batch 52/64 loss: -0.49826908111572266
Batch 53/64 loss: -0.6706695556640625
Batch 54/64 loss: -0.6577801704406738
Batch 55/64 loss: -0.44449520111083984
Batch 56/64 loss: -0.5718746185302734
Batch 57/64 loss: -0.33511924743652344
Batch 58/64 loss: -0.5558414459228516
Batch 59/64 loss: -0.7030563354492188
Batch 60/64 loss: -0.8567209243774414
Batch 61/64 loss: -0.8965673446655273
Batch 62/64 loss: -0.7275848388671875
Batch 63/64 loss: 0.2104659080505371
Batch 64/64 loss: -3.9961814880371094
Epoch 275  Train loss: -0.6296939176671645  Val loss: -0.6135720452901834
Epoch 276
-------------------------------
Batch 1/64 loss: -0.2409687042236328
Batch 2/64 loss: -0.729161262512207
Batch 3/64 loss: -0.36857032775878906
Batch 4/64 loss: -0.7184371948242188
Batch 5/64 loss: -0.8153305053710938
Batch 6/64 loss: -0.7767343521118164
Batch 7/64 loss: -0.15901803970336914
Batch 8/64 loss: -0.6812095642089844
Batch 9/64 loss: -0.7552919387817383
Batch 10/64 loss: -0.6752033233642578
Batch 11/64 loss: -0.9124221801757812
Batch 12/64 loss: -0.6982927322387695
Batch 13/64 loss: -0.6790037155151367
Batch 14/64 loss: -0.7446804046630859
Batch 15/64 loss: -0.6660747528076172
Batch 16/64 loss: -0.5506715774536133
Batch 17/64 loss: -0.44025611877441406
Batch 18/64 loss: -0.12540817260742188
Batch 19/64 loss: -0.5902996063232422
Batch 20/64 loss: -0.6019234657287598
Batch 21/64 loss: -0.7929906845092773
Batch 22/64 loss: -0.3058357238769531
Batch 23/64 loss: -0.8697910308837891
Batch 24/64 loss: -0.7417640686035156
Batch 25/64 loss: -0.4787445068359375
Batch 26/64 loss: -0.6937713623046875
Batch 27/64 loss: 0.1273059844970703
Batch 28/64 loss: -0.9869117736816406
Batch 29/64 loss: -0.15865325927734375
Batch 30/64 loss: -0.6652460098266602
Batch 31/64 loss: -0.6542778015136719
Batch 32/64 loss: -0.853907585144043
Batch 33/64 loss: -0.8053045272827148
Batch 34/64 loss: -0.3513450622558594
Batch 35/64 loss: -0.6877355575561523
Batch 36/64 loss: -0.8481807708740234
Batch 37/64 loss: -0.39660167694091797
Batch 38/64 loss: -0.5123577117919922
Batch 39/64 loss: -0.7846698760986328
Batch 40/64 loss: -0.724329948425293
Batch 41/64 loss: -0.5004415512084961
Batch 42/64 loss: -0.5203170776367188
Batch 43/64 loss: 0.5068526268005371
Batch 44/64 loss: -0.4061298370361328
Batch 45/64 loss: -0.818608283996582
Batch 46/64 loss: -0.572169303894043
Batch 47/64 loss: 0.26661014556884766
Batch 48/64 loss: -0.5919947624206543
Batch 49/64 loss: -0.11588096618652344
Batch 50/64 loss: -0.30353450775146484
Batch 51/64 loss: -0.4289264678955078
Batch 52/64 loss: -0.0008459091186523438
Batch 53/64 loss: -0.5262346267700195
Batch 54/64 loss: -0.4898557662963867
Batch 55/64 loss: -0.8087663650512695
Batch 56/64 loss: -0.7033214569091797
Batch 57/64 loss: -0.516265869140625
Batch 58/64 loss: -0.7783956527709961
Batch 59/64 loss: -0.35666799545288086
Batch 60/64 loss: -0.7374892234802246
Batch 61/64 loss: -0.7126555442810059
Batch 62/64 loss: -0.49952125549316406
Batch 63/64 loss: -0.1942768096923828
Batch 64/64 loss: -4.632447242736816
Epoch 276  Train loss: -0.5866234162274528  Val loss: -0.6290655889871604
Epoch 277
-------------------------------
Batch 1/64 loss: -0.8346519470214844
Batch 2/64 loss: -0.6012954711914062
Batch 3/64 loss: -0.8794794082641602
Batch 4/64 loss: -0.656003475189209
Batch 5/64 loss: -0.724489688873291
Batch 6/64 loss: -0.6081681251525879
Batch 7/64 loss: -0.7018527984619141
Batch 8/64 loss: -0.4364290237426758
Batch 9/64 loss: -0.666015625
Batch 10/64 loss: -0.03738975524902344
Batch 11/64 loss: -0.3681344985961914
Batch 12/64 loss: -0.7296304702758789
Batch 13/64 loss: -0.2748699188232422
Batch 14/64 loss: -0.5174198150634766
Batch 15/64 loss: -0.8752279281616211
Batch 16/64 loss: -0.5424423217773438
Batch 17/64 loss: -0.3696427345275879
Batch 18/64 loss: 0.0015530586242675781
Batch 19/64 loss: -0.45662689208984375
Batch 20/64 loss: -0.7646312713623047
Batch 21/64 loss: -0.4826169013977051
Batch 22/64 loss: -0.80487060546875
Batch 23/64 loss: -0.7973670959472656
Batch 24/64 loss: -0.38620948791503906
Batch 25/64 loss: -0.6569366455078125
Batch 26/64 loss: -0.9783296585083008
Batch 27/64 loss: -0.7334976196289062
Batch 28/64 loss: -0.8213729858398438
Batch 29/64 loss: -0.6916122436523438
Batch 30/64 loss: -0.8079824447631836
Batch 31/64 loss: -0.25176191329956055
Batch 32/64 loss: -0.5096206665039062
Batch 33/64 loss: -0.6866350173950195
Batch 34/64 loss: -0.7961130142211914
Batch 35/64 loss: -0.5251264572143555
Batch 36/64 loss: -0.7729244232177734
Batch 37/64 loss: -0.6053609848022461
Batch 38/64 loss: -0.7682390213012695
Batch 39/64 loss: -0.6434216499328613
Batch 40/64 loss: -0.6339397430419922
Batch 41/64 loss: -0.6013379096984863
Batch 42/64 loss: -0.8378686904907227
Batch 43/64 loss: -0.7246255874633789
Batch 44/64 loss: -0.9185791015625
Batch 45/64 loss: -0.6277217864990234
Batch 46/64 loss: -0.26249217987060547
Batch 47/64 loss: -0.36022377014160156
Batch 48/64 loss: -0.6445999145507812
Batch 49/64 loss: -0.7107505798339844
Batch 50/64 loss: -0.6460227966308594
Batch 51/64 loss: -0.46953773498535156
Batch 52/64 loss: -0.6496601104736328
Batch 53/64 loss: -0.780797004699707
Batch 54/64 loss: -0.6252727508544922
Batch 55/64 loss: -0.24207162857055664
Batch 56/64 loss: -0.34585094451904297
Batch 57/64 loss: -0.9910602569580078
Batch 58/64 loss: -0.6174383163452148
Batch 59/64 loss: -0.5843601226806641
Batch 60/64 loss: -0.7515592575073242
Batch 61/64 loss: -0.33336353302001953
Batch 62/64 loss: -0.15189266204833984
Batch 63/64 loss: -0.5782594680786133
Batch 64/64 loss: -4.079102993011475
Epoch 277  Train loss: -0.6417483928156834  Val loss: -0.5680434105732187
Epoch 278
-------------------------------
Batch 1/64 loss: -0.8785629272460938
Batch 2/64 loss: -0.8498334884643555
Batch 3/64 loss: -0.6255273818969727
Batch 4/64 loss: -0.10406303405761719
Batch 5/64 loss: -0.32150840759277344
Batch 6/64 loss: -0.8314704895019531
Batch 7/64 loss: -0.7900457382202148
Batch 8/64 loss: -0.8446826934814453
Batch 9/64 loss: -0.746638298034668
Batch 10/64 loss: -0.2961997985839844
Batch 11/64 loss: -0.6802206039428711
Batch 12/64 loss: -0.566929817199707
Batch 13/64 loss: -0.18857336044311523
Batch 14/64 loss: -0.7396669387817383
Batch 15/64 loss: -0.8950958251953125
Batch 16/64 loss: -0.5628566741943359
Batch 17/64 loss: -0.9512519836425781
Batch 18/64 loss: -0.8741416931152344
Batch 19/64 loss: -0.49166297912597656
Batch 20/64 loss: -0.6371936798095703
Batch 21/64 loss: -0.43841981887817383
Batch 22/64 loss: -0.7516613006591797
Batch 23/64 loss: -0.1888742446899414
Batch 24/64 loss: -0.5003681182861328
Batch 25/64 loss: -0.5906190872192383
Batch 26/64 loss: -0.3090653419494629
Batch 27/64 loss: -0.46157169342041016
Batch 28/64 loss: -1.0718803405761719
Batch 29/64 loss: -0.5288305282592773
Batch 30/64 loss: -0.7262630462646484
Batch 31/64 loss: -0.8582859039306641
Batch 32/64 loss: -0.40430259704589844
Batch 33/64 loss: -0.3956718444824219
Batch 34/64 loss: -0.31339406967163086
Batch 35/64 loss: -0.25166845321655273
Batch 36/64 loss: -0.4347848892211914
Batch 37/64 loss: -0.49004364013671875
Batch 38/64 loss: -0.3228759765625
Batch 39/64 loss: -0.5712404251098633
Batch 40/64 loss: -0.4187326431274414
Batch 41/64 loss: -0.6239748001098633
Batch 42/64 loss: -0.42858028411865234
Batch 43/64 loss: -0.515955924987793
Batch 44/64 loss: -0.5019121170043945
Batch 45/64 loss: -0.5692253112792969
Batch 46/64 loss: -0.44835758209228516
Batch 47/64 loss: -0.4950399398803711
Batch 48/64 loss: -0.6723432540893555
Batch 49/64 loss: -0.3367609977722168
Batch 50/64 loss: -0.4999351501464844
Batch 51/64 loss: -0.3290133476257324
Batch 52/64 loss: -0.427093505859375
Batch 53/64 loss: -0.8322687149047852
Batch 54/64 loss: -0.21429777145385742
Batch 55/64 loss: -0.5513172149658203
Batch 56/64 loss: -0.7891788482666016
Batch 57/64 loss: -0.19472265243530273
Batch 58/64 loss: -0.6481180191040039
Batch 59/64 loss: -0.8084125518798828
Batch 60/64 loss: -0.5885825157165527
Batch 61/64 loss: -0.7882528305053711
Batch 62/64 loss: -0.6298942565917969
Batch 63/64 loss: -0.7933645248413086
Batch 64/64 loss: -4.5204267501831055
Epoch 278  Train loss: -0.6114761090746113  Val loss: -0.7120985444059077
Epoch 279
-------------------------------
Batch 1/64 loss: -0.6786422729492188
Batch 2/64 loss: -0.6769857406616211
Batch 3/64 loss: -0.704960823059082
Batch 4/64 loss: -0.8696308135986328
Batch 5/64 loss: -0.5025138854980469
Batch 6/64 loss: -0.5650672912597656
Batch 7/64 loss: -0.3926057815551758
Batch 8/64 loss: -0.5846109390258789
Batch 9/64 loss: -0.3628530502319336
Batch 10/64 loss: -0.8262491226196289
Batch 11/64 loss: -0.6300191879272461
Batch 12/64 loss: -0.6056909561157227
Batch 13/64 loss: -0.7332658767700195
Batch 14/64 loss: -0.8512716293334961
Batch 15/64 loss: -0.1494283676147461
Batch 16/64 loss: -0.8327045440673828
Batch 17/64 loss: -0.7621479034423828
Batch 18/64 loss: -0.5681896209716797
Batch 19/64 loss: -0.6267242431640625
Batch 20/64 loss: -0.7558498382568359
Batch 21/64 loss: -0.8297243118286133
Batch 22/64 loss: -0.6781940460205078
Batch 23/64 loss: -0.06951475143432617
Batch 24/64 loss: -0.7298345565795898
Batch 25/64 loss: -0.7981653213500977
Batch 26/64 loss: -0.4516258239746094
Batch 27/64 loss: -0.565582275390625
Batch 28/64 loss: -0.5900859832763672
Batch 29/64 loss: -0.14710044860839844
Batch 30/64 loss: 0.0017018318176269531
Batch 31/64 loss: -0.44969940185546875
Batch 32/64 loss: -0.8942394256591797
Batch 33/64 loss: -0.5989789962768555
Batch 34/64 loss: -0.5913276672363281
Batch 35/64 loss: -0.5971803665161133
Batch 36/64 loss: -0.4675912857055664
Batch 37/64 loss: -0.8021163940429688
Batch 38/64 loss: -0.5076541900634766
Batch 39/64 loss: -0.6153793334960938
Batch 40/64 loss: -0.42079734802246094
Batch 41/64 loss: -0.6830124855041504
Batch 42/64 loss: -0.7085342407226562
Batch 43/64 loss: -0.4197654724121094
Batch 44/64 loss: -0.619781494140625
Batch 45/64 loss: -0.7336606979370117
Batch 46/64 loss: -0.9564456939697266
Batch 47/64 loss: -0.7708330154418945
Batch 48/64 loss: -0.8091716766357422
Batch 49/64 loss: -0.5835161209106445
Batch 50/64 loss: -0.7190713882446289
Batch 51/64 loss: -0.657045841217041
Batch 52/64 loss: -0.7588434219360352
Batch 53/64 loss: -0.750788688659668
Batch 54/64 loss: -0.7278909683227539
Batch 55/64 loss: -0.7351770401000977
Batch 56/64 loss: -0.4757356643676758
Batch 57/64 loss: -0.5986833572387695
Batch 58/64 loss: -0.3181877136230469
Batch 59/64 loss: 0.39521312713623047
Batch 60/64 loss: -0.16704511642456055
Batch 61/64 loss: -1.0622577667236328
Batch 62/64 loss: -0.578129768371582
Batch 63/64 loss: -0.524960994720459
Batch 64/64 loss: -4.506096839904785
Epoch 279  Train loss: -0.6403984331617169  Val loss: -0.6692904311766739
Epoch 280
-------------------------------
Batch 1/64 loss: -0.679316520690918
Batch 2/64 loss: -0.5058960914611816
Batch 3/64 loss: -0.7793521881103516
Batch 4/64 loss: -0.8095026016235352
Batch 5/64 loss: -0.5223789215087891
Batch 6/64 loss: -0.5220890045166016
Batch 7/64 loss: -0.8096046447753906
Batch 8/64 loss: -0.6694602966308594
Batch 9/64 loss: -0.5902795791625977
Batch 10/64 loss: -0.8028316497802734
Batch 11/64 loss: -0.532526969909668
Batch 12/64 loss: -0.6157569885253906
Batch 13/64 loss: -1.0124101638793945
Batch 14/64 loss: -0.7457723617553711
Batch 15/64 loss: -0.9317970275878906
Batch 16/64 loss: -0.3720741271972656
Batch 17/64 loss: -0.5609283447265625
Batch 18/64 loss: -0.4746394157409668
Batch 19/64 loss: -0.6216306686401367
Batch 20/64 loss: -0.5804033279418945
Batch 21/64 loss: -0.7520570755004883
Batch 22/64 loss: -0.7970905303955078
Batch 23/64 loss: -0.6172809600830078
Batch 24/64 loss: -0.8026285171508789
Batch 25/64 loss: -0.5181550979614258
Batch 26/64 loss: -0.2401123046875
Batch 27/64 loss: -0.8440732955932617
Batch 28/64 loss: -0.44750165939331055
Batch 29/64 loss: -0.3370232582092285
Batch 30/64 loss: -0.9464931488037109
Batch 31/64 loss: -0.1283869743347168
Batch 32/64 loss: 0.061563968658447266
Batch 33/64 loss: -0.2563457489013672
Batch 34/64 loss: -0.5242104530334473
Batch 35/64 loss: -0.32460594177246094
Batch 36/64 loss: -0.4725370407104492
Batch 37/64 loss: -0.3167538642883301
Batch 38/64 loss: -0.32601165771484375
Batch 39/64 loss: -0.30109405517578125
Batch 40/64 loss: -0.7245540618896484
Batch 41/64 loss: -0.7483987808227539
Batch 42/64 loss: -0.8324089050292969
Batch 43/64 loss: -0.45622825622558594
Batch 44/64 loss: -0.5739436149597168
Batch 45/64 loss: -0.7499818801879883
Batch 46/64 loss: -0.7018918991088867
Batch 47/64 loss: -0.8954801559448242
Batch 48/64 loss: -0.5559616088867188
Batch 49/64 loss: -0.6699380874633789
Batch 50/64 loss: -0.7268619537353516
Batch 51/64 loss: -0.4754648208618164
Batch 52/64 loss: -0.8858680725097656
Batch 53/64 loss: -0.9734926223754883
Batch 54/64 loss: -0.6850986480712891
Batch 55/64 loss: -0.6583318710327148
Batch 56/64 loss: -0.4389476776123047
Batch 57/64 loss: -0.3486928939819336
Batch 58/64 loss: -0.9005880355834961
Batch 59/64 loss: -0.3510551452636719
Batch 60/64 loss: -0.9219703674316406
Batch 61/64 loss: -0.6722126007080078
Batch 62/64 loss: -0.8716592788696289
Batch 63/64 loss: -0.2535228729248047
Batch 64/64 loss: -4.567378044128418
Epoch 280  Train loss: -0.6514123318242092  Val loss: -0.6001774764962212
Epoch 281
-------------------------------
Batch 1/64 loss: -0.46341419219970703
Batch 2/64 loss: -0.686070442199707
Batch 3/64 loss: -0.7388696670532227
Batch 4/64 loss: -0.42497920989990234
Batch 5/64 loss: -0.3175015449523926
Batch 6/64 loss: -0.617457389831543
Batch 7/64 loss: -0.4021167755126953
Batch 8/64 loss: -0.34579038619995117
Batch 9/64 loss: -0.2491922378540039
Batch 10/64 loss: -0.37238597869873047
Batch 11/64 loss: -0.3424415588378906
Batch 12/64 loss: -0.4230613708496094
Batch 13/64 loss: -0.5615673065185547
Batch 14/64 loss: -0.636988639831543
Batch 15/64 loss: -0.07257938385009766
Batch 16/64 loss: -0.7448720932006836
Batch 17/64 loss: -0.8669023513793945
Batch 18/64 loss: -0.36450767517089844
Batch 19/64 loss: -0.711277961730957
Batch 20/64 loss: 0.06670999526977539
Batch 21/64 loss: -0.10619544982910156
Batch 22/64 loss: -0.5449542999267578
Batch 23/64 loss: -0.6054306030273438
Batch 24/64 loss: -0.6828536987304688
Batch 25/64 loss: -0.742889404296875
Batch 26/64 loss: -0.42693233489990234
Batch 27/64 loss: -0.3449263572692871
Batch 28/64 loss: -0.45331621170043945
Batch 29/64 loss: -0.59613037109375
Batch 30/64 loss: -0.8719015121459961
Batch 31/64 loss: -0.6397724151611328
Batch 32/64 loss: -0.8041610717773438
Batch 33/64 loss: -0.6269512176513672
Batch 34/64 loss: -0.5777173042297363
Batch 35/64 loss: -0.5271472930908203
Batch 36/64 loss: -0.5005531311035156
Batch 37/64 loss: -0.569551944732666
Batch 38/64 loss: -0.7701282501220703
Batch 39/64 loss: -0.7785282135009766
Batch 40/64 loss: -0.9378385543823242
Batch 41/64 loss: -0.5380783081054688
Batch 42/64 loss: -0.6768312454223633
Batch 43/64 loss: -0.14925432205200195
Batch 44/64 loss: -0.7663602828979492
Batch 45/64 loss: -0.5139150619506836
Batch 46/64 loss: -0.7073097229003906
Batch 47/64 loss: -0.5757284164428711
Batch 48/64 loss: -0.8306493759155273
Batch 49/64 loss: -0.5082263946533203
Batch 50/64 loss: -0.4298853874206543
Batch 51/64 loss: -0.634028434753418
Batch 52/64 loss: -0.556854248046875
Batch 53/64 loss: -0.9376974105834961
Batch 54/64 loss: -0.622706413269043
Batch 55/64 loss: -0.7650747299194336
Batch 56/64 loss: -0.8558111190795898
Batch 57/64 loss: -0.6085672378540039
Batch 58/64 loss: 0.2893495559692383
Batch 59/64 loss: -0.8733339309692383
Batch 60/64 loss: -0.46024036407470703
Batch 61/64 loss: 0.005612373352050781
Batch 62/64 loss: -0.8697729110717773
Batch 63/64 loss: -0.6013240814208984
Batch 64/64 loss: -4.453535079956055
Epoch 281  Train loss: -0.5946664249195772  Val loss: -0.6429660049910398
Epoch 282
-------------------------------
Batch 1/64 loss: -0.43082332611083984
Batch 2/64 loss: -0.6245498657226562
Batch 3/64 loss: -0.23822402954101562
Batch 4/64 loss: -0.800349235534668
Batch 5/64 loss: 0.12499856948852539
Batch 6/64 loss: -0.11819744110107422
Batch 7/64 loss: -0.5590591430664062
Batch 8/64 loss: -0.5884847640991211
Batch 9/64 loss: -0.9946994781494141
Batch 10/64 loss: -0.28127193450927734
Batch 11/64 loss: -0.9220952987670898
Batch 12/64 loss: -0.5851821899414062
Batch 13/64 loss: -0.7969036102294922
Batch 14/64 loss: -0.7879018783569336
Batch 15/64 loss: -1.0494375228881836
Batch 16/64 loss: -0.26900386810302734
Batch 17/64 loss: -0.6427011489868164
Batch 18/64 loss: -0.6436371803283691
Batch 19/64 loss: -0.270963191986084
Batch 20/64 loss: -0.7741994857788086
Batch 21/64 loss: -0.5501937866210938
Batch 22/64 loss: -0.5512299537658691
Batch 23/64 loss: -0.6365718841552734
Batch 24/64 loss: -0.22023439407348633
Batch 25/64 loss: -0.5951175689697266
Batch 26/64 loss: -0.6007137298583984
Batch 27/64 loss: -0.37929201126098633
Batch 28/64 loss: -0.8443698883056641
Batch 29/64 loss: -0.2644662857055664
Batch 30/64 loss: -0.0714874267578125
Batch 31/64 loss: -0.5396556854248047
Batch 32/64 loss: -0.614501953125
Batch 33/64 loss: -0.694483757019043
Batch 34/64 loss: -0.8218564987182617
Batch 35/64 loss: -0.685297966003418
Batch 36/64 loss: -0.5114669799804688
Batch 37/64 loss: -0.3644232749938965
Batch 38/64 loss: -0.44731712341308594
Batch 39/64 loss: -0.9107131958007812
Batch 40/64 loss: -0.4047403335571289
Batch 41/64 loss: -0.3919496536254883
Batch 42/64 loss: -0.6206827163696289
Batch 43/64 loss: -0.10941410064697266
Batch 44/64 loss: -0.730224609375
Batch 45/64 loss: -0.6659250259399414
Batch 46/64 loss: -0.2049880027770996
Batch 47/64 loss: -0.3176870346069336
Batch 48/64 loss: -0.4886808395385742
Batch 49/64 loss: -0.4810514450073242
Batch 50/64 loss: -0.5198888778686523
Batch 51/64 loss: -0.661674976348877
Batch 52/64 loss: -0.38973140716552734
Batch 53/64 loss: -0.45226192474365234
Batch 54/64 loss: 0.0808401107788086
Batch 55/64 loss: 0.10031366348266602
Batch 56/64 loss: -0.6885552406311035
Batch 57/64 loss: -0.8148250579833984
Batch 58/64 loss: -0.6666078567504883
Batch 59/64 loss: -0.7875185012817383
Batch 60/64 loss: -0.44755125045776367
Batch 61/64 loss: -0.5158500671386719
Batch 62/64 loss: -0.7211456298828125
Batch 63/64 loss: -0.7275476455688477
Batch 64/64 loss: -4.578886032104492
Epoch 282  Train loss: -0.5743936052509383  Val loss: -0.5779988790295788
Epoch 283
-------------------------------
Batch 1/64 loss: -0.5475625991821289
Batch 2/64 loss: -0.43256664276123047
Batch 3/64 loss: -0.2930278778076172
Batch 4/64 loss: -0.49016380310058594
Batch 5/64 loss: 0.22507238388061523
Batch 6/64 loss: -0.6479873657226562
Batch 7/64 loss: -0.4098472595214844
Batch 8/64 loss: -0.6312713623046875
Batch 9/64 loss: -0.914942741394043
Batch 10/64 loss: -0.45290660858154297
Batch 11/64 loss: -0.6575746536254883
Batch 12/64 loss: -0.7003164291381836
Batch 13/64 loss: -0.8197488784790039
Batch 14/64 loss: -0.7471485137939453
Batch 15/64 loss: -0.5736832618713379
Batch 16/64 loss: -0.5578031539916992
Batch 17/64 loss: -0.7139930725097656
Batch 18/64 loss: -0.6397476196289062
Batch 19/64 loss: -0.8620719909667969
Batch 20/64 loss: -0.7538948059082031
Batch 21/64 loss: -0.46448802947998047
Batch 22/64 loss: -0.7533903121948242
Batch 23/64 loss: -0.9523420333862305
Batch 24/64 loss: -0.4958467483520508
Batch 25/64 loss: -0.7056236267089844
Batch 26/64 loss: -0.7012166976928711
Batch 27/64 loss: -0.6169776916503906
Batch 28/64 loss: -0.40750551223754883
Batch 29/64 loss: -0.5049266815185547
Batch 30/64 loss: -0.8133697509765625
Batch 31/64 loss: -0.6402158737182617
Batch 32/64 loss: -0.5705060958862305
Batch 33/64 loss: -0.5925455093383789
Batch 34/64 loss: 0.11218500137329102
Batch 35/64 loss: -0.45206737518310547
Batch 36/64 loss: -0.2647972106933594
Batch 37/64 loss: -0.8017282485961914
Batch 38/64 loss: -0.8244199752807617
Batch 39/64 loss: -0.7764368057250977
Batch 40/64 loss: -1.0292472839355469
Batch 41/64 loss: -0.6915035247802734
Batch 42/64 loss: -0.6846303939819336
Batch 43/64 loss: -0.7176809310913086
Batch 44/64 loss: -0.9526195526123047
Batch 45/64 loss: -0.608184814453125
Batch 46/64 loss: -0.28371334075927734
Batch 47/64 loss: -0.9194583892822266
Batch 48/64 loss: -0.5958423614501953
Batch 49/64 loss: -0.9363374710083008
Batch 50/64 loss: -0.8827400207519531
Batch 51/64 loss: -0.3525099754333496
Batch 52/64 loss: -0.5428304672241211
Batch 53/64 loss: -0.7044315338134766
Batch 54/64 loss: -0.7624101638793945
Batch 55/64 loss: -0.744044303894043
Batch 56/64 loss: -0.8305625915527344
Batch 57/64 loss: -0.8166179656982422
Batch 58/64 loss: -0.34130859375
Batch 59/64 loss: -0.5086469650268555
Batch 60/64 loss: -0.2659425735473633
Batch 61/64 loss: -0.8383493423461914
Batch 62/64 loss: -0.7135763168334961
Batch 63/64 loss: -0.5428962707519531
Batch 64/64 loss: -4.608546257019043
Epoch 283  Train loss: -0.6677631415572821  Val loss: -0.7656331930783197
Saving best model, epoch: 283
Epoch 284
-------------------------------
Batch 1/64 loss: -0.7982025146484375
Batch 2/64 loss: -0.7071571350097656
Batch 3/64 loss: -0.8757495880126953
Batch 4/64 loss: 0.1712808609008789
Batch 5/64 loss: -0.6663007736206055
Batch 6/64 loss: -0.5053339004516602
Batch 7/64 loss: -0.7212629318237305
Batch 8/64 loss: -0.6334676742553711
Batch 9/64 loss: -0.32033348083496094
Batch 10/64 loss: -0.6845560073852539
Batch 11/64 loss: -0.7028408050537109
Batch 12/64 loss: -0.7913074493408203
Batch 13/64 loss: -0.7096261978149414
Batch 14/64 loss: -0.7369852066040039
Batch 15/64 loss: -0.5443849563598633
Batch 16/64 loss: -0.7939958572387695
Batch 17/64 loss: -0.7660684585571289
Batch 18/64 loss: -0.8642892837524414
Batch 19/64 loss: -0.78253173828125
Batch 20/64 loss: -0.7498836517333984
Batch 21/64 loss: -0.30681896209716797
Batch 22/64 loss: -0.9588832855224609
Batch 23/64 loss: -0.6389579772949219
Batch 24/64 loss: -0.7847061157226562
Batch 25/64 loss: -0.42322778701782227
Batch 26/64 loss: -0.44399595260620117
Batch 27/64 loss: -0.9438180923461914
Batch 28/64 loss: -0.8336801528930664
Batch 29/64 loss: -0.5488729476928711
Batch 30/64 loss: -0.8841519355773926
Batch 31/64 loss: -0.25947141647338867
Batch 32/64 loss: -0.1159048080444336
Batch 33/64 loss: -0.8217411041259766
Batch 34/64 loss: -0.512364387512207
Batch 35/64 loss: -0.7897205352783203
Batch 36/64 loss: -0.6984190940856934
Batch 37/64 loss: -0.6788463592529297
Batch 38/64 loss: -0.7931098937988281
Batch 39/64 loss: -0.8042030334472656
Batch 40/64 loss: -0.8288192749023438
Batch 41/64 loss: -0.6916408538818359
Batch 42/64 loss: -0.37078189849853516
Batch 43/64 loss: -0.618290901184082
Batch 44/64 loss: -0.5057229995727539
Batch 45/64 loss: -0.6010417938232422
Batch 46/64 loss: -0.7430963516235352
Batch 47/64 loss: -0.6313199996948242
Batch 48/64 loss: -0.7016191482543945
Batch 49/64 loss: -0.3257932662963867
Batch 50/64 loss: -0.999659538269043
Batch 51/64 loss: -0.944554328918457
Batch 52/64 loss: -0.21599483489990234
Batch 53/64 loss: -0.5798110961914062
Batch 54/64 loss: -0.7770490646362305
Batch 55/64 loss: -0.43546056747436523
Batch 56/64 loss: -0.3679027557373047
Batch 57/64 loss: -0.5455875396728516
Batch 58/64 loss: -0.9187612533569336
Batch 59/64 loss: -0.3048067092895508
Batch 60/64 loss: -0.029392719268798828
Batch 61/64 loss: -0.30788469314575195
Batch 62/64 loss: -0.3784303665161133
Batch 63/64 loss: -0.5346775054931641
Batch 64/64 loss: -4.773978233337402
Epoch 284  Train loss: -0.664509391784668  Val loss: -0.6668983275947702
Epoch 285
-------------------------------
Batch 1/64 loss: -0.8226280212402344
Batch 2/64 loss: -0.4879727363586426
Batch 3/64 loss: -0.732874870300293
Batch 4/64 loss: -0.7179670333862305
Batch 5/64 loss: -0.24215221405029297
Batch 6/64 loss: -0.942234992980957
Batch 7/64 loss: -0.16689395904541016
Batch 8/64 loss: -0.6102533340454102
Batch 9/64 loss: -0.3256206512451172
Batch 10/64 loss: -0.6206483840942383
Batch 11/64 loss: -0.21502971649169922
Batch 12/64 loss: -0.8293247222900391
Batch 13/64 loss: -0.5872516632080078
Batch 14/64 loss: -0.3111085891723633
Batch 15/64 loss: -0.5593786239624023
Batch 16/64 loss: -0.5397272109985352
Batch 17/64 loss: -0.1613459587097168
Batch 18/64 loss: -0.4528841972351074
Batch 19/64 loss: -0.42803192138671875
Batch 20/64 loss: -0.54022216796875
Batch 21/64 loss: -0.8994894027709961
Batch 22/64 loss: -0.7529811859130859
Batch 23/64 loss: -0.7079019546508789
Batch 24/64 loss: -0.8165645599365234
Batch 25/64 loss: -0.5986833572387695
Batch 26/64 loss: -0.4412875175476074
Batch 27/64 loss: -1.0170650482177734
Batch 28/64 loss: -0.5437383651733398
Batch 29/64 loss: -0.6553964614868164
Batch 30/64 loss: -0.518150806427002
Batch 31/64 loss: -0.8759450912475586
Batch 32/64 loss: -0.7121505737304688
Batch 33/64 loss: -0.37012767791748047
Batch 34/64 loss: -0.08704090118408203
Batch 35/64 loss: -0.5529050827026367
Batch 36/64 loss: -0.582524299621582
Batch 37/64 loss: -0.5585851669311523
Batch 38/64 loss: -0.7285995483398438
Batch 39/64 loss: -0.9269170761108398
Batch 40/64 loss: -0.5396890640258789
Batch 41/64 loss: -0.6350212097167969
Batch 42/64 loss: -0.13057947158813477
Batch 43/64 loss: -0.650975227355957
Batch 44/64 loss: -0.5098037719726562
Batch 45/64 loss: -0.713627815246582
Batch 46/64 loss: -0.6463508605957031
Batch 47/64 loss: -0.8289394378662109
Batch 48/64 loss: -0.956578254699707
Batch 49/64 loss: -0.13151073455810547
Batch 50/64 loss: -0.33025074005126953
Batch 51/64 loss: -0.5594549179077148
Batch 52/64 loss: -0.20438480377197266
Batch 53/64 loss: 0.01553964614868164
Batch 54/64 loss: -0.6618404388427734
Batch 55/64 loss: -0.6492109298706055
Batch 56/64 loss: -0.41123390197753906
Batch 57/64 loss: -0.5531787872314453
Batch 58/64 loss: -0.49469470977783203
Batch 59/64 loss: -0.5228328704833984
Batch 60/64 loss: -0.23528718948364258
Batch 61/64 loss: -0.5990848541259766
Batch 62/64 loss: -0.5812778472900391
Batch 63/64 loss: -0.5034770965576172
Batch 64/64 loss: -4.465872764587402
Epoch 285  Train loss: -0.5964353710997339  Val loss: -0.5905102601985341
Epoch 286
-------------------------------
Batch 1/64 loss: -0.7241086959838867
Batch 2/64 loss: -0.9435796737670898
Batch 3/64 loss: -0.6151247024536133
Batch 4/64 loss: -0.5206203460693359
Batch 5/64 loss: -0.6071567535400391
Batch 6/64 loss: 1.2329111099243164
Batch 7/64 loss: -0.5366506576538086
Batch 8/64 loss: -0.6615438461303711
Batch 9/64 loss: -0.6161594390869141
Batch 10/64 loss: -0.7050237655639648
Batch 11/64 loss: -0.7581682205200195
Batch 12/64 loss: -0.9256792068481445
Batch 13/64 loss: -0.8360357284545898
Batch 14/64 loss: -0.31961536407470703
Batch 15/64 loss: -0.7743844985961914
Batch 16/64 loss: -0.7440376281738281
Batch 17/64 loss: -0.34703540802001953
Batch 18/64 loss: -0.8406143188476562
Batch 19/64 loss: -0.7900047302246094
Batch 20/64 loss: -0.1340932846069336
Batch 21/64 loss: -0.2580118179321289
Batch 22/64 loss: 0.5425930023193359
Batch 23/64 loss: -0.07224369049072266
Batch 24/64 loss: -0.7561006546020508
Batch 25/64 loss: -0.38710689544677734
Batch 26/64 loss: -0.4625568389892578
Batch 27/64 loss: -0.036884307861328125
Batch 28/64 loss: -0.5685949325561523
Batch 29/64 loss: -0.026409149169921875
Batch 30/64 loss: -0.3027324676513672
Batch 31/64 loss: -0.387237548828125
Batch 32/64 loss: -0.7595634460449219
Batch 33/64 loss: -0.44350719451904297
Batch 34/64 loss: -0.5518321990966797
Batch 35/64 loss: -0.5067100524902344
Batch 36/64 loss: -0.6244926452636719
Batch 37/64 loss: -0.12896251678466797
Batch 38/64 loss: -0.5906977653503418
Batch 39/64 loss: -0.4297676086425781
Batch 40/64 loss: 0.38002681732177734
Batch 41/64 loss: -0.46608686447143555
Batch 42/64 loss: -0.4609956741333008
Batch 43/64 loss: -0.3797597885131836
Batch 44/64 loss: -0.25931739807128906
Batch 45/64 loss: -0.7685260772705078
Batch 46/64 loss: -0.5013909339904785
Batch 47/64 loss: -0.6120262145996094
Batch 48/64 loss: -0.6646175384521484
Batch 49/64 loss: -0.6732745170593262
Batch 50/64 loss: -0.4650120735168457
Batch 51/64 loss: -0.42003726959228516
Batch 52/64 loss: -0.4496631622314453
Batch 53/64 loss: -0.39290285110473633
Batch 54/64 loss: -0.707066535949707
Batch 55/64 loss: -0.37993431091308594
Batch 56/64 loss: -0.6120729446411133
Batch 57/64 loss: -0.7604513168334961
Batch 58/64 loss: -0.3244485855102539
Batch 59/64 loss: -0.6114907264709473
Batch 60/64 loss: -0.8625469207763672
Batch 61/64 loss: -0.8523588180541992
Batch 62/64 loss: -0.5015020370483398
Batch 63/64 loss: -0.5460820198059082
Batch 64/64 loss: -4.5155839920043945
Epoch 286  Train loss: -0.5269924650005265  Val loss: -0.6403818556533236
Epoch 287
-------------------------------
Batch 1/64 loss: -0.6051206588745117
Batch 2/64 loss: -0.4907503128051758
Batch 3/64 loss: -0.5475349426269531
Batch 4/64 loss: -0.22974014282226562
Batch 5/64 loss: -0.5270495414733887
Batch 6/64 loss: -0.6190028190612793
Batch 7/64 loss: -1.069437026977539
Batch 8/64 loss: -0.5973587036132812
Batch 9/64 loss: -0.3778262138366699
Batch 10/64 loss: -0.3865218162536621
Batch 11/64 loss: -0.3942852020263672
Batch 12/64 loss: -0.10308694839477539
Batch 13/64 loss: -0.34595680236816406
Batch 14/64 loss: -0.0113372802734375
Batch 15/64 loss: -0.7404284477233887
Batch 16/64 loss: -0.3000016212463379
Batch 17/64 loss: -0.6447792053222656
Batch 18/64 loss: -0.07404899597167969
Batch 19/64 loss: -0.8205499649047852
Batch 20/64 loss: -0.8844699859619141
Batch 21/64 loss: -0.4451766014099121
Batch 22/64 loss: -0.9725332260131836
Batch 23/64 loss: -0.5624933242797852
Batch 24/64 loss: -0.21819353103637695
Batch 25/64 loss: -0.7462029457092285
Batch 26/64 loss: -0.8620462417602539
Batch 27/64 loss: -0.8529510498046875
Batch 28/64 loss: -0.38755035400390625
Batch 29/64 loss: -0.8883237838745117
Batch 30/64 loss: -0.08237934112548828
Batch 31/64 loss: -0.5232439041137695
Batch 32/64 loss: -0.7921772003173828
Batch 33/64 loss: -0.8101949691772461
Batch 34/64 loss: -0.6158685684204102
Batch 35/64 loss: -0.45456600189208984
Batch 36/64 loss: -1.0389652252197266
Batch 37/64 loss: -0.622622013092041
Batch 38/64 loss: -0.6037387847900391
Batch 39/64 loss: -0.3952064514160156
Batch 40/64 loss: -0.2469649314880371
Batch 41/64 loss: 0.01241302490234375
Batch 42/64 loss: -0.3338909149169922
Batch 43/64 loss: -0.7933239936828613
Batch 44/64 loss: -0.7694797515869141
Batch 45/64 loss: -0.6511955261230469
Batch 46/64 loss: -0.6814031600952148
Batch 47/64 loss: -0.7889242172241211
Batch 48/64 loss: -0.6172580718994141
Batch 49/64 loss: -0.6162309646606445
Batch 50/64 loss: -0.6098651885986328
Batch 51/64 loss: -0.4617495536804199
Batch 52/64 loss: -0.7426328659057617
Batch 53/64 loss: -0.566868782043457
Batch 54/64 loss: -0.5149049758911133
Batch 55/64 loss: -0.9293603897094727
Batch 56/64 loss: -0.8991279602050781
Batch 57/64 loss: -0.49295902252197266
Batch 58/64 loss: -0.8696889877319336
Batch 59/64 loss: -0.6273155212402344
Batch 60/64 loss: -0.7455101013183594
Batch 61/64 loss: -1.0742988586425781
Batch 62/64 loss: -0.7400798797607422
Batch 63/64 loss: -0.5376701354980469
Batch 64/64 loss: -4.454711437225342
Epoch 287  Train loss: -0.6318908934499703  Val loss: -0.7053795254107603
Epoch 288
-------------------------------
Batch 1/64 loss: -0.8656597137451172
Batch 2/64 loss: -0.21300506591796875
Batch 3/64 loss: -0.6653242111206055
Batch 4/64 loss: -0.8826990127563477
Batch 5/64 loss: -0.6427879333496094
Batch 6/64 loss: 0.08551979064941406
Batch 7/64 loss: -0.14151382446289062
Batch 8/64 loss: -0.7005939483642578
Batch 9/64 loss: -0.6171712875366211
Batch 10/64 loss: -0.44226741790771484
Batch 11/64 loss: -0.8637924194335938
Batch 12/64 loss: -0.958073616027832
Batch 13/64 loss: -0.5669832229614258
Batch 14/64 loss: -0.5508699417114258
Batch 15/64 loss: -0.3418407440185547
Batch 16/64 loss: -0.8964166641235352
Batch 17/64 loss: -0.7193288803100586
Batch 18/64 loss: -0.6760110855102539
Batch 19/64 loss: -0.4964628219604492
Batch 20/64 loss: -0.6516752243041992
Batch 21/64 loss: -0.3082547187805176
Batch 22/64 loss: -0.6058683395385742
Batch 23/64 loss: -0.57354736328125
Batch 24/64 loss: -0.471527099609375
Batch 25/64 loss: -0.4353785514831543
Batch 26/64 loss: -0.5181465148925781
Batch 27/64 loss: -0.7045903205871582
Batch 28/64 loss: -0.3489694595336914
Batch 29/64 loss: 0.09746122360229492
Batch 30/64 loss: -0.5673022270202637
Batch 31/64 loss: -0.6622447967529297
Batch 32/64 loss: -0.9028139114379883
Batch 33/64 loss: -0.7800512313842773
Batch 34/64 loss: -0.17225122451782227
Batch 35/64 loss: -0.4973897933959961
Batch 36/64 loss: -0.5698776245117188
Batch 37/64 loss: -0.3080778121948242
Batch 38/64 loss: -0.6291790008544922
Batch 39/64 loss: -0.6299896240234375
Batch 40/64 loss: -0.9066438674926758
Batch 41/64 loss: -0.34447383880615234
Batch 42/64 loss: -0.6338748931884766
Batch 43/64 loss: -0.5560121536254883
Batch 44/64 loss: -0.8876981735229492
Batch 45/64 loss: -0.2810330390930176
Batch 46/64 loss: -0.7176809310913086
Batch 47/64 loss: -0.6017370223999023
Batch 48/64 loss: -0.9104843139648438
Batch 49/64 loss: -0.4380912780761719
Batch 50/64 loss: -0.7594490051269531
Batch 51/64 loss: -0.709010124206543
Batch 52/64 loss: -0.790135383605957
Batch 53/64 loss: -0.8401508331298828
Batch 54/64 loss: -0.8170986175537109
Batch 55/64 loss: -0.8357315063476562
Batch 56/64 loss: -0.11970806121826172
Batch 57/64 loss: -0.7731170654296875
Batch 58/64 loss: -0.6759557723999023
Batch 59/64 loss: -0.271484375
Batch 60/64 loss: -0.6826953887939453
Batch 61/64 loss: -0.2567272186279297
Batch 62/64 loss: -0.0592193603515625
Batch 63/64 loss: -0.3353853225708008
Batch 64/64 loss: -4.131783485412598
Epoch 288  Train loss: -0.6070178948196711  Val loss: -0.42056404192423086
Epoch 289
-------------------------------
Batch 1/64 loss: -0.6900119781494141
Batch 2/64 loss: -0.5781106948852539
Batch 3/64 loss: -0.8103733062744141
Batch 4/64 loss: -0.6296787261962891
Batch 5/64 loss: -0.06208944320678711
Batch 6/64 loss: -0.5118703842163086
Batch 7/64 loss: 0.1100616455078125
Batch 8/64 loss: -0.722783088684082
Batch 9/64 loss: -0.8867950439453125
Batch 10/64 loss: -0.41129302978515625
Batch 11/64 loss: -0.36932849884033203
Batch 12/64 loss: -0.4262399673461914
Batch 13/64 loss: -0.6218013763427734
Batch 14/64 loss: -0.25086021423339844
Batch 15/64 loss: -0.1550140380859375
Batch 16/64 loss: -0.5516023635864258
Batch 17/64 loss: -0.37203216552734375
Batch 18/64 loss: -0.5547580718994141
Batch 19/64 loss: -0.42078542709350586
Batch 20/64 loss: -0.20507478713989258
Batch 21/64 loss: -0.07107830047607422
Batch 22/64 loss: -0.6849184036254883
Batch 23/64 loss: -0.6435270309448242
Batch 24/64 loss: -0.639552116394043
Batch 25/64 loss: -0.12040567398071289
Batch 26/64 loss: -0.6058149337768555
Batch 27/64 loss: -0.9800958633422852
Batch 28/64 loss: -0.5989875793457031
Batch 29/64 loss: -0.2648305892944336
Batch 30/64 loss: -0.6613330841064453
Batch 31/64 loss: -0.3331923484802246
Batch 32/64 loss: -0.5885987281799316
Batch 33/64 loss: -0.21001815795898438
Batch 34/64 loss: -0.6488633155822754
Batch 35/64 loss: -0.32010316848754883
Batch 36/64 loss: -0.6685075759887695
Batch 37/64 loss: -0.6285943984985352
Batch 38/64 loss: -0.3068552017211914
Batch 39/64 loss: -0.8157329559326172
Batch 40/64 loss: -0.785792350769043
Batch 41/64 loss: -0.6977777481079102
Batch 42/64 loss: -0.7103891372680664
Batch 43/64 loss: -0.5682945251464844
Batch 44/64 loss: -0.6079225540161133
Batch 45/64 loss: -0.6588859558105469
Batch 46/64 loss: -0.5703811645507812
Batch 47/64 loss: -0.26341867446899414
Batch 48/64 loss: -0.2203359603881836
Batch 49/64 loss: -0.8756742477416992
Batch 50/64 loss: -0.521571159362793
Batch 51/64 loss: -0.2032022476196289
Batch 52/64 loss: -0.02338886260986328
Batch 53/64 loss: -0.22670841217041016
Batch 54/64 loss: -0.7413768768310547
Batch 55/64 loss: -0.5839204788208008
Batch 56/64 loss: -0.7253036499023438
Batch 57/64 loss: -0.7819337844848633
Batch 58/64 loss: -0.671208381652832
Batch 59/64 loss: -0.7169246673583984
Batch 60/64 loss: -0.7759122848510742
Batch 61/64 loss: -0.5308971405029297
Batch 62/64 loss: -0.694493293762207
Batch 63/64 loss: -0.2608928680419922
Batch 64/64 loss: -4.5188446044921875
Epoch 289  Train loss: -0.560269651225969  Val loss: -0.6523544731009048
Epoch 290
-------------------------------
Batch 1/64 loss: -0.719207763671875
Batch 2/64 loss: -0.7124128341674805
Batch 3/64 loss: -0.599574089050293
Batch 4/64 loss: -0.6135587692260742
Batch 5/64 loss: -0.7147960662841797
Batch 6/64 loss: -0.6635255813598633
Batch 7/64 loss: -0.2916221618652344
Batch 8/64 loss: -0.15897750854492188
Batch 9/64 loss: -0.5209512710571289
Batch 10/64 loss: -0.1848926544189453
Batch 11/64 loss: -0.7453145980834961
Batch 12/64 loss: -0.9724769592285156
Batch 13/64 loss: -0.7302026748657227
Batch 14/64 loss: -0.666440486907959
Batch 15/64 loss: -0.7923178672790527
Batch 16/64 loss: -0.8129959106445312
Batch 17/64 loss: -0.5899825096130371
Batch 18/64 loss: -0.6431412696838379
Batch 19/64 loss: 0.022369861602783203
Batch 20/64 loss: -0.1525564193725586
Batch 21/64 loss: -0.055640220642089844
Batch 22/64 loss: -0.842010498046875
Batch 23/64 loss: -0.7400922775268555
Batch 24/64 loss: -0.31992340087890625
Batch 25/64 loss: -0.7384786605834961
Batch 26/64 loss: -0.6826128959655762
Batch 27/64 loss: -0.33226871490478516
Batch 28/64 loss: -0.7243137359619141
Batch 29/64 loss: -0.3646059036254883
Batch 30/64 loss: -0.49829673767089844
Batch 31/64 loss: -0.7788257598876953
Batch 32/64 loss: -0.5570826530456543
Batch 33/64 loss: -0.38048267364501953
Batch 34/64 loss: -0.49040985107421875
Batch 35/64 loss: -0.5239958763122559
Batch 36/64 loss: -0.6621847152709961
Batch 37/64 loss: -0.4447135925292969
Batch 38/64 loss: -0.27862548828125
Batch 39/64 loss: -0.6550130844116211
Batch 40/64 loss: -0.5604348182678223
Batch 41/64 loss: -0.7926225662231445
Batch 42/64 loss: -0.5959086418151855
Batch 43/64 loss: -0.28429508209228516
Batch 44/64 loss: -0.5725307464599609
Batch 45/64 loss: -0.8143825531005859
Batch 46/64 loss: -0.6007137298583984
Batch 47/64 loss: -0.5606532096862793
Batch 48/64 loss: -0.7453908920288086
Batch 49/64 loss: -0.5335683822631836
Batch 50/64 loss: -0.7764596939086914
Batch 51/64 loss: -0.5710248947143555
Batch 52/64 loss: -0.8454504013061523
Batch 53/64 loss: -0.9921026229858398
Batch 54/64 loss: -0.47924327850341797
Batch 55/64 loss: -0.5875492095947266
Batch 56/64 loss: -0.6878538131713867
Batch 57/64 loss: -0.8504734039306641
Batch 58/64 loss: -0.725616455078125
Batch 59/64 loss: -0.41950130462646484
Batch 60/64 loss: -0.42643022537231445
Batch 61/64 loss: -0.2856898307800293
Batch 62/64 loss: -0.6355571746826172
Batch 63/64 loss: -0.5870962142944336
Batch 64/64 loss: -4.625250816345215
Epoch 290  Train loss: -0.6232414357802447  Val loss: -0.6318503835356932
Epoch 291
-------------------------------
Batch 1/64 loss: -0.7784585952758789
Batch 2/64 loss: -0.4779939651489258
Batch 3/64 loss: -0.5820627212524414
Batch 4/64 loss: -1.0431222915649414
Batch 5/64 loss: -0.4953460693359375
Batch 6/64 loss: -0.6211233139038086
Batch 7/64 loss: -0.5979890823364258
Batch 8/64 loss: -0.759556770324707
Batch 9/64 loss: -0.8493452072143555
Batch 10/64 loss: -0.9815273284912109
Batch 11/64 loss: -0.896245002746582
Batch 12/64 loss: -0.771453857421875
Batch 13/64 loss: -0.7816457748413086
Batch 14/64 loss: -0.44953250885009766
Batch 15/64 loss: -0.47773075103759766
Batch 16/64 loss: -0.12801265716552734
Batch 17/64 loss: -0.2937436103820801
Batch 18/64 loss: -0.509772777557373
Batch 19/64 loss: -0.9153165817260742
Batch 20/64 loss: -0.6896514892578125
Batch 21/64 loss: -0.3465394973754883
Batch 22/64 loss: -0.5879287719726562
Batch 23/64 loss: -0.7347564697265625
Batch 24/64 loss: -0.8942384719848633
Batch 25/64 loss: -0.6449704170227051
Batch 26/64 loss: -0.29930877685546875
Batch 27/64 loss: -0.7774724960327148
Batch 28/64 loss: -0.9296979904174805
Batch 29/64 loss: -0.8052682876586914
Batch 30/64 loss: -0.9360103607177734
Batch 31/64 loss: -0.0894460678100586
Batch 32/64 loss: -0.8869953155517578
Batch 33/64 loss: -0.8801126480102539
Batch 34/64 loss: -0.7820606231689453
Batch 35/64 loss: -0.27816057205200195
Batch 36/64 loss: -0.8207387924194336
Batch 37/64 loss: -0.5471115112304688
Batch 38/64 loss: -0.601374626159668
Batch 39/64 loss: -0.22203683853149414
Batch 40/64 loss: -0.44949913024902344
Batch 41/64 loss: -0.7973461151123047
Batch 42/64 loss: -0.2809286117553711
Batch 43/64 loss: -0.32466697692871094
Batch 44/64 loss: -0.09344911575317383
Batch 45/64 loss: -0.7974328994750977
Batch 46/64 loss: -0.3199300765991211
Batch 47/64 loss: -0.6702661514282227
Batch 48/64 loss: -0.48278284072875977
Batch 49/64 loss: -0.42138195037841797
Batch 50/64 loss: -0.6507024765014648
Batch 51/64 loss: 0.2646465301513672
Batch 52/64 loss: -0.6488666534423828
Batch 53/64 loss: -0.6259784698486328
Batch 54/64 loss: -0.416715145111084
Batch 55/64 loss: -0.5793924331665039
Batch 56/64 loss: -0.7233810424804688
Batch 57/64 loss: -0.59844970703125
Batch 58/64 loss: -0.0036458969116210938
Batch 59/64 loss: -0.7418088912963867
Batch 60/64 loss: -0.16849184036254883
Batch 61/64 loss: -0.7460346221923828
Batch 62/64 loss: -0.31520509719848633
Batch 63/64 loss: -0.48444366455078125
Batch 64/64 loss: -4.760857582092285
Epoch 291  Train loss: -0.624480868320839  Val loss: -0.7489465274351979
Epoch 292
-------------------------------
Batch 1/64 loss: -0.6353549957275391
Batch 2/64 loss: -0.9944486618041992
Batch 3/64 loss: -0.8090724945068359
Batch 4/64 loss: -0.7455711364746094
Batch 5/64 loss: -0.026542186737060547
Batch 6/64 loss: -0.8217172622680664
Batch 7/64 loss: -0.6651296615600586
Batch 8/64 loss: -0.8229227066040039
Batch 9/64 loss: -0.3200235366821289
Batch 10/64 loss: -0.39939117431640625
Batch 11/64 loss: -0.9226589202880859
Batch 12/64 loss: -0.4656391143798828
Batch 13/64 loss: -0.9010496139526367
Batch 14/64 loss: -0.27251338958740234
Batch 15/64 loss: -0.6501293182373047
Batch 16/64 loss: -0.48456287384033203
Batch 17/64 loss: -0.828923225402832
Batch 18/64 loss: -0.8535146713256836
Batch 19/64 loss: -0.7467622756958008
Batch 20/64 loss: -0.9308576583862305
Batch 21/64 loss: -0.4917774200439453
Batch 22/64 loss: -0.8405380249023438
Batch 23/64 loss: -0.5890817642211914
Batch 24/64 loss: -0.9250564575195312
Batch 25/64 loss: -0.8112640380859375
Batch 26/64 loss: -0.7390298843383789
Batch 27/64 loss: -0.9604682922363281
Batch 28/64 loss: -0.8357181549072266
Batch 29/64 loss: -0.34006738662719727
Batch 30/64 loss: -0.4559807777404785
Batch 31/64 loss: -0.6945428848266602
Batch 32/64 loss: -0.9837093353271484
Batch 33/64 loss: -0.6705732345581055
Batch 34/64 loss: -0.6958723068237305
Batch 35/64 loss: -0.33393001556396484
Batch 36/64 loss: -0.26278114318847656
Batch 37/64 loss: -0.668461799621582
Batch 38/64 loss: -0.35336875915527344
Batch 39/64 loss: -0.7575578689575195
Batch 40/64 loss: -0.9172248840332031
Batch 41/64 loss: -0.8086643218994141
Batch 42/64 loss: -0.42702484130859375
Batch 43/64 loss: -0.5410194396972656
Batch 44/64 loss: -0.7816457748413086
Batch 45/64 loss: -0.15174198150634766
Batch 46/64 loss: -0.6502599716186523
Batch 47/64 loss: -0.7350788116455078
Batch 48/64 loss: -0.8178234100341797
Batch 49/64 loss: -0.5952243804931641
Batch 50/64 loss: -0.6435661315917969
Batch 51/64 loss: -0.8562726974487305
Batch 52/64 loss: -0.6950817108154297
Batch 53/64 loss: -1.0560855865478516
Batch 54/64 loss: -0.2160177230834961
Batch 55/64 loss: -0.8022994995117188
Batch 56/64 loss: -0.666813850402832
Batch 57/64 loss: -0.6957883834838867
Batch 58/64 loss: -0.5075397491455078
Batch 59/64 loss: -0.693608283996582
Batch 60/64 loss: -0.8170671463012695
Batch 61/64 loss: -0.3987770080566406
Batch 62/64 loss: -0.28955936431884766
Batch 63/64 loss: -0.5074224472045898
Batch 64/64 loss: -4.203609466552734
Epoch 292  Train loss: -0.6922490811815449  Val loss: -0.25988655483599793
Epoch 293
-------------------------------
Batch 1/64 loss: -0.34141063690185547
Batch 2/64 loss: -0.3772916793823242
Batch 3/64 loss: -0.4889860153198242
Batch 4/64 loss: -0.39469051361083984
Batch 5/64 loss: -0.6810264587402344
Batch 6/64 loss: -0.6321916580200195
Batch 7/64 loss: -0.5458889007568359
Batch 8/64 loss: -0.28557395935058594
Batch 9/64 loss: -0.7027587890625
Batch 10/64 loss: -0.6239147186279297
Batch 11/64 loss: -0.8105020523071289
Batch 12/64 loss: -0.3085317611694336
Batch 13/64 loss: -0.11089038848876953
Batch 14/64 loss: -0.7577123641967773
Batch 15/64 loss: -0.5589303970336914
Batch 16/64 loss: -0.6283016204833984
Batch 17/64 loss: -0.39917469024658203
Batch 18/64 loss: -0.9091405868530273
Batch 19/64 loss: -0.5648117065429688
Batch 20/64 loss: -0.8687944412231445
Batch 21/64 loss: -0.9941120147705078
Batch 22/64 loss: -0.6709518432617188
Batch 23/64 loss: -0.6025238037109375
Batch 24/64 loss: -0.4238734245300293
Batch 25/64 loss: -0.6758794784545898
Batch 26/64 loss: -0.7252607345581055
Batch 27/64 loss: -0.6671152114868164
Batch 28/64 loss: -0.6710319519042969
Batch 29/64 loss: -0.6491050720214844
Batch 30/64 loss: -0.5879878997802734
Batch 31/64 loss: -0.71429443359375
Batch 32/64 loss: -0.19801902770996094
Batch 33/64 loss: -0.5581150054931641
Batch 34/64 loss: -0.25974130630493164
Batch 35/64 loss: -0.6405439376831055
Batch 36/64 loss: -0.5798845291137695
Batch 37/64 loss: -0.5780506134033203
Batch 38/64 loss: -0.6770057678222656
Batch 39/64 loss: -0.39603710174560547
Batch 40/64 loss: -0.47983360290527344
Batch 41/64 loss: -0.5809640884399414
Batch 42/64 loss: -0.8771047592163086
Batch 43/64 loss: -0.7573823928833008
Batch 44/64 loss: -0.5358552932739258
Batch 45/64 loss: -0.8542509078979492
Batch 46/64 loss: -0.77154541015625
Batch 47/64 loss: -0.6792745590209961
Batch 48/64 loss: -0.7578954696655273
Batch 49/64 loss: -0.7820415496826172
Batch 50/64 loss: -0.9494247436523438
Batch 51/64 loss: -0.8484230041503906
Batch 52/64 loss: -0.3848390579223633
Batch 53/64 loss: -0.47241783142089844
Batch 54/64 loss: -0.592320442199707
Batch 55/64 loss: -0.5173807144165039
Batch 56/64 loss: -0.2865414619445801
Batch 57/64 loss: -0.6773233413696289
Batch 58/64 loss: -0.46173572540283203
Batch 59/64 loss: -0.6964578628540039
Batch 60/64 loss: -0.4946928024291992
Batch 61/64 loss: -0.6524944305419922
Batch 62/64 loss: 0.27257347106933594
Batch 63/64 loss: -0.929133415222168
Batch 64/64 loss: -4.289802074432373
Epoch 293  Train loss: -0.6312810579935709  Val loss: -0.5299950825799372
Epoch 294
-------------------------------
Batch 1/64 loss: -0.2619614601135254
Batch 2/64 loss: -0.32944297790527344
Batch 3/64 loss: -0.5030355453491211
Batch 4/64 loss: -1.0030498504638672
Batch 5/64 loss: -0.6960067749023438
Batch 6/64 loss: -0.22944355010986328
Batch 7/64 loss: -0.7329196929931641
Batch 8/64 loss: -0.5229911804199219
Batch 9/64 loss: -0.8542881011962891
Batch 10/64 loss: -0.7519702911376953
Batch 11/64 loss: -0.7286005020141602
Batch 12/64 loss: -0.835627555847168
Batch 13/64 loss: -0.4212503433227539
Batch 14/64 loss: -0.678746223449707
Batch 15/64 loss: -0.7578878402709961
Batch 16/64 loss: -0.8843498229980469
Batch 17/64 loss: -0.11261844635009766
Batch 18/64 loss: -0.7958097457885742
Batch 19/64 loss: -0.4317646026611328
Batch 20/64 loss: -0.6357107162475586
Batch 21/64 loss: -0.7386703491210938
Batch 22/64 loss: -0.7556619644165039
Batch 23/64 loss: -0.16976261138916016
Batch 24/64 loss: -0.5426826477050781
Batch 25/64 loss: -0.64739990234375
Batch 26/64 loss: -0.8971052169799805
Batch 27/64 loss: -0.7343482971191406
Batch 28/64 loss: 0.3138418197631836
Batch 29/64 loss: -1.1403799057006836
Batch 30/64 loss: -0.4642496109008789
Batch 31/64 loss: -0.806370735168457
Batch 32/64 loss: -0.6608657836914062
Batch 33/64 loss: -0.8926219940185547
Batch 34/64 loss: -0.6446342468261719
Batch 35/64 loss: -0.8294830322265625
Batch 36/64 loss: -0.3625774383544922
Batch 37/64 loss: -0.7166652679443359
Batch 38/64 loss: -1.0114812850952148
Batch 39/64 loss: -0.3464527130126953
Batch 40/64 loss: -0.6815853118896484
Batch 41/64 loss: -0.6181707382202148
Batch 42/64 loss: -0.5124979019165039
Batch 43/64 loss: -0.7446489334106445
Batch 44/64 loss: -0.2972450256347656
Batch 45/64 loss: 0.04952239990234375
Batch 46/64 loss: -0.8149862289428711
Batch 47/64 loss: -0.4488410949707031
Batch 48/64 loss: -0.5210542678833008
Batch 49/64 loss: -0.4344930648803711
Batch 50/64 loss: -1.123250961303711
Batch 51/64 loss: -0.052725791931152344
Batch 52/64 loss: -0.5578184127807617
Batch 53/64 loss: -0.540125846862793
Batch 54/64 loss: -0.4252605438232422
Batch 55/64 loss: -0.994293212890625
Batch 56/64 loss: -0.34135913848876953
Batch 57/64 loss: -0.7766532897949219
Batch 58/64 loss: -0.5363502502441406
Batch 59/64 loss: -0.6494951248168945
Batch 60/64 loss: -0.6638994216918945
Batch 61/64 loss: -0.33911705017089844
Batch 62/64 loss: -0.39339542388916016
Batch 63/64 loss: -0.661433219909668
Batch 64/64 loss: -4.6795735359191895
Epoch 294  Train loss: -0.6400612457125795  Val loss: -0.6311747101983664
Epoch 295
-------------------------------
Batch 1/64 loss: -0.5982208251953125
Batch 2/64 loss: -0.3388519287109375
Batch 3/64 loss: -0.39896202087402344
Batch 4/64 loss: -0.7146005630493164
Batch 5/64 loss: -0.7535581588745117
Batch 6/64 loss: -0.4436812400817871
Batch 7/64 loss: -0.369903564453125
Batch 8/64 loss: -0.2282695770263672
Batch 9/64 loss: -0.7120332717895508
Batch 10/64 loss: -0.6306533813476562
Batch 11/64 loss: -0.7223272323608398
Batch 12/64 loss: -0.636286735534668
Batch 13/64 loss: -0.673151969909668
Batch 14/64 loss: -0.46894168853759766
Batch 15/64 loss: -0.5533828735351562
Batch 16/64 loss: -0.7360191345214844
Batch 17/64 loss: -0.587367057800293
Batch 18/64 loss: -0.515571117401123
Batch 19/64 loss: 0.18286800384521484
Batch 20/64 loss: -0.03956031799316406
Batch 21/64 loss: -0.7260799407958984
Batch 22/64 loss: -0.573204517364502
Batch 23/64 loss: -0.8327198028564453
Batch 24/64 loss: -0.709315299987793
Batch 25/64 loss: -0.7406253814697266
Batch 26/64 loss: -0.6538152694702148
Batch 27/64 loss: -0.8038120269775391
Batch 28/64 loss: -0.6667051315307617
Batch 29/64 loss: -0.05742359161376953
Batch 30/64 loss: -0.3077983856201172
Batch 31/64 loss: -0.8344430923461914
Batch 32/64 loss: -0.6451444625854492
Batch 33/64 loss: -0.7970485687255859
Batch 34/64 loss: -0.3802661895751953
Batch 35/64 loss: -0.8159894943237305
Batch 36/64 loss: -0.7274332046508789
Batch 37/64 loss: -0.7414407730102539
Batch 38/64 loss: -0.98162841796875
Batch 39/64 loss: -0.6794443130493164
Batch 40/64 loss: -0.5991487503051758
Batch 41/64 loss: -0.9441699981689453
Batch 42/64 loss: -0.7366962432861328
Batch 43/64 loss: -0.7188806533813477
Batch 44/64 loss: -0.2630491256713867
Batch 45/64 loss: -0.610313892364502
Batch 46/64 loss: -0.3494701385498047
Batch 47/64 loss: -0.8684711456298828
Batch 48/64 loss: -0.685154914855957
Batch 49/64 loss: -0.7110509872436523
Batch 50/64 loss: -0.5303144454956055
Batch 51/64 loss: -0.9737701416015625
Batch 52/64 loss: -0.7734622955322266
Batch 53/64 loss: -0.4758739471435547
Batch 54/64 loss: -0.5577187538146973
Batch 55/64 loss: -0.6190156936645508
Batch 56/64 loss: -0.5619831085205078
Batch 57/64 loss: -0.8278388977050781
Batch 58/64 loss: -0.5986471176147461
Batch 59/64 loss: -0.7954959869384766
Batch 60/64 loss: -0.49857616424560547
Batch 61/64 loss: -0.12217140197753906
Batch 62/64 loss: -0.5746784210205078
Batch 63/64 loss: -0.6645956039428711
Batch 64/64 loss: -4.563969612121582
Epoch 295  Train loss: -0.644648432264141  Val loss: -0.6127778672680413
Epoch 296
-------------------------------
Batch 1/64 loss: -0.6601314544677734
Batch 2/64 loss: -0.4442634582519531
Batch 3/64 loss: -0.48326587677001953
Batch 4/64 loss: -0.4929838180541992
Batch 5/64 loss: -0.587895393371582
Batch 6/64 loss: -0.7431268692016602
Batch 7/64 loss: -0.6366233825683594
Batch 8/64 loss: -0.39579200744628906
Batch 9/64 loss: -0.6286029815673828
Batch 10/64 loss: -0.6908121109008789
Batch 11/64 loss: -0.6046352386474609
Batch 12/64 loss: -0.7960996627807617
Batch 13/64 loss: -0.9157981872558594
Batch 14/64 loss: -0.7867755889892578
Batch 15/64 loss: -0.5928678512573242
Batch 16/64 loss: -0.9244651794433594
Batch 17/64 loss: -0.4825286865234375
Batch 18/64 loss: -0.39977359771728516
Batch 19/64 loss: -0.0059032440185546875
Batch 20/64 loss: -0.7780780792236328
Batch 21/64 loss: -0.41447925567626953
Batch 22/64 loss: -0.5667533874511719
Batch 23/64 loss: -0.832122802734375
Batch 24/64 loss: -0.8044509887695312
Batch 25/64 loss: -0.6850366592407227
Batch 26/64 loss: 0.05898714065551758
Batch 27/64 loss: -0.8170356750488281
Batch 28/64 loss: -0.39450645446777344
Batch 29/64 loss: -0.6549654006958008
Batch 30/64 loss: -0.6154422760009766
Batch 31/64 loss: -0.5732460021972656
Batch 32/64 loss: -0.4727363586425781
Batch 33/64 loss: -0.8338851928710938
Batch 34/64 loss: -0.5812253952026367
Batch 35/64 loss: -0.4255342483520508
Batch 36/64 loss: -0.6118526458740234
Batch 37/64 loss: -0.6157445907592773
Batch 38/64 loss: -0.9458990097045898
Batch 39/64 loss: -0.2049417495727539
Batch 40/64 loss: -0.9025421142578125
Batch 41/64 loss: -0.47743797302246094
Batch 42/64 loss: -0.8726911544799805
Batch 43/64 loss: 0.08117818832397461
Batch 44/64 loss: -0.35486698150634766
Batch 45/64 loss: -0.7940254211425781
Batch 46/64 loss: -0.48446178436279297
Batch 47/64 loss: -0.34648990631103516
Batch 48/64 loss: -0.5944671630859375
Batch 49/64 loss: -0.6052727699279785
Batch 50/64 loss: -0.3814868927001953
Batch 51/64 loss: -0.7493686676025391
Batch 52/64 loss: -0.5092802047729492
Batch 53/64 loss: -0.9933929443359375
Batch 54/64 loss: -0.7349586486816406
Batch 55/64 loss: -0.36469173431396484
Batch 56/64 loss: -0.6231327056884766
Batch 57/64 loss: -0.7322378158569336
Batch 58/64 loss: -0.37111663818359375
Batch 59/64 loss: -0.880706787109375
Batch 60/64 loss: -0.4577455520629883
Batch 61/64 loss: -0.7040224075317383
Batch 62/64 loss: -0.7146501541137695
Batch 63/64 loss: -0.5597119331359863
Batch 64/64 loss: -4.383977890014648
Epoch 296  Train loss: -0.6346173679127413  Val loss: -0.6716067258435017
Epoch 297
-------------------------------
Batch 1/64 loss: -0.8894796371459961
Batch 2/64 loss: -0.7261075973510742
Batch 3/64 loss: -0.6357278823852539
Batch 4/64 loss: -0.5595006942749023
Batch 5/64 loss: -0.5923843383789062
Batch 6/64 loss: -0.8460760116577148
Batch 7/64 loss: -0.6260910034179688
Batch 8/64 loss: -0.5595579147338867
Batch 9/64 loss: -0.7861824035644531
Batch 10/64 loss: -0.7316093444824219
Batch 11/64 loss: -0.687525749206543
Batch 12/64 loss: -0.7040519714355469
Batch 13/64 loss: -0.7515430450439453
Batch 14/64 loss: -0.595952033996582
Batch 15/64 loss: -0.3654289245605469
Batch 16/64 loss: -0.8168573379516602
Batch 17/64 loss: -0.04275846481323242
Batch 18/64 loss: -0.7816219329833984
Batch 19/64 loss: -0.26894664764404297
Batch 20/64 loss: -0.6542625427246094
Batch 21/64 loss: -0.23146295547485352
Batch 22/64 loss: -0.6874542236328125
Batch 23/64 loss: -0.5016307830810547
Batch 24/64 loss: -0.5346918106079102
Batch 25/64 loss: -0.7056283950805664
Batch 26/64 loss: -0.09972238540649414
Batch 27/64 loss: -0.7364463806152344
Batch 28/64 loss: -0.6005039215087891
Batch 29/64 loss: -0.7570629119873047
Batch 30/64 loss: -0.666386604309082
Batch 31/64 loss: -0.7533636093139648
Batch 32/64 loss: -0.4209308624267578
Batch 33/64 loss: -0.5247983932495117
Batch 34/64 loss: -0.33135318756103516
Batch 35/64 loss: -0.6855764389038086
Batch 36/64 loss: -0.39484214782714844
Batch 37/64 loss: -0.5614309310913086
Batch 38/64 loss: -0.9141454696655273
Batch 39/64 loss: -0.8677778244018555
Batch 40/64 loss: -0.41685009002685547
Batch 41/64 loss: -0.31561851501464844
Batch 42/64 loss: -0.2220296859741211
Batch 43/64 loss: -0.43663787841796875
Batch 44/64 loss: -0.40475940704345703
Batch 45/64 loss: -0.5675582885742188
Batch 46/64 loss: -0.2144021987915039
Batch 47/64 loss: -0.4628772735595703
Batch 48/64 loss: -0.553065299987793
Batch 49/64 loss: -0.7236785888671875
Batch 50/64 loss: -0.7506313323974609
Batch 51/64 loss: -0.43976879119873047
Batch 52/64 loss: -0.6015729904174805
Batch 53/64 loss: -0.4994945526123047
Batch 54/64 loss: -0.8938360214233398
Batch 55/64 loss: -0.5853557586669922
Batch 56/64 loss: -0.6640224456787109
Batch 57/64 loss: -0.8722381591796875
Batch 58/64 loss: -0.604914665222168
Batch 59/64 loss: -0.8796815872192383
Batch 60/64 loss: -0.8220634460449219
Batch 61/64 loss: -0.7084646224975586
Batch 62/64 loss: -0.7825603485107422
Batch 63/64 loss: 0.1115565299987793
Batch 64/64 loss: -4.4107465744018555
Epoch 297  Train loss: -0.6308307460710114  Val loss: -0.7550259881822514
Epoch 298
-------------------------------
Batch 1/64 loss: -0.639307975769043
Batch 2/64 loss: -0.531764030456543
Batch 3/64 loss: -0.8040456771850586
Batch 4/64 loss: -0.39136791229248047
Batch 5/64 loss: -0.5648889541625977
Batch 6/64 loss: -0.41966962814331055
Batch 7/64 loss: -0.7693195343017578
Batch 8/64 loss: -0.7286272048950195
Batch 9/64 loss: -0.8419322967529297
Batch 10/64 loss: -0.7133255004882812
Batch 11/64 loss: -0.39525842666625977
Batch 12/64 loss: -0.30175304412841797
Batch 13/64 loss: -0.6992015838623047
Batch 14/64 loss: -0.8397970199584961
Batch 15/64 loss: -0.7673187255859375
Batch 16/64 loss: -0.6196832656860352
Batch 17/64 loss: -0.7654561996459961
Batch 18/64 loss: -0.7109861373901367
Batch 19/64 loss: -0.5533103942871094
Batch 20/64 loss: -0.31874608993530273
Batch 21/64 loss: -0.6641998291015625
Batch 22/64 loss: -0.6306209564208984
Batch 23/64 loss: -0.6227235794067383
Batch 24/64 loss: -0.5215849876403809
Batch 25/64 loss: -0.8658924102783203
Batch 26/64 loss: -0.9590682983398438
Batch 27/64 loss: -0.6186456680297852
Batch 28/64 loss: -0.44425249099731445
Batch 29/64 loss: -0.24364757537841797
Batch 30/64 loss: -0.9603137969970703
Batch 31/64 loss: -0.6638078689575195
Batch 32/64 loss: -0.8680849075317383
Batch 33/64 loss: -0.4202995300292969
Batch 34/64 loss: -0.6914005279541016
Batch 35/64 loss: -0.5792579650878906
Batch 36/64 loss: -0.30921316146850586
Batch 37/64 loss: -0.5682449340820312
Batch 38/64 loss: -0.6583127975463867
Batch 39/64 loss: -0.7073545455932617
Batch 40/64 loss: -0.04175758361816406
Batch 41/64 loss: -0.6413021087646484
Batch 42/64 loss: -0.36757469177246094
Batch 43/64 loss: -0.6455879211425781
Batch 44/64 loss: 0.39673423767089844
Batch 45/64 loss: -0.6526651382446289
Batch 46/64 loss: -0.6354789733886719
Batch 47/64 loss: -0.6620721817016602
Batch 48/64 loss: -0.8110771179199219
Batch 49/64 loss: -0.7928199768066406
Batch 50/64 loss: -0.8484621047973633
Batch 51/64 loss: -0.8248977661132812
Batch 52/64 loss: -0.2195873260498047
Batch 53/64 loss: -0.9241008758544922
Batch 54/64 loss: -0.9170732498168945
Batch 55/64 loss: -0.8323812484741211
Batch 56/64 loss: -0.7498579025268555
Batch 57/64 loss: -0.8857908248901367
Batch 58/64 loss: -0.5062170028686523
Batch 59/64 loss: -0.4215574264526367
Batch 60/64 loss: -0.5290184020996094
Batch 61/64 loss: -0.35564422607421875
Batch 62/64 loss: -0.6539068222045898
Batch 63/64 loss: -0.5983285903930664
Batch 64/64 loss: -3.1426916122436523
Epoch 298  Train loss: -0.6407863429948395  Val loss: -0.39449345696832716
Epoch 299
-------------------------------
Batch 1/64 loss: -0.6745643615722656
Batch 2/64 loss: -0.7080497741699219
Batch 3/64 loss: -0.5754270553588867
Batch 4/64 loss: -0.5404114723205566
Batch 5/64 loss: -0.7934436798095703
Batch 6/64 loss: -0.5624876022338867
Batch 7/64 loss: -0.13319969177246094
Batch 8/64 loss: -0.7017507553100586
Batch 9/64 loss: -0.5581340789794922
Batch 10/64 loss: -0.5536708831787109
Batch 11/64 loss: -0.7462024688720703
Batch 12/64 loss: -0.5317230224609375
Batch 13/64 loss: 0.2209177017211914
Batch 14/64 loss: -0.37999820709228516
Batch 15/64 loss: -0.9610328674316406
Batch 16/64 loss: -0.34562206268310547
Batch 17/64 loss: -0.5089435577392578
Batch 18/64 loss: -0.5679216384887695
Batch 19/64 loss: 0.07853221893310547
Batch 20/64 loss: -0.5111150741577148
Batch 21/64 loss: -0.0984344482421875
Batch 22/64 loss: -0.6775226593017578
Batch 23/64 loss: -0.6368637084960938
Batch 24/64 loss: -0.36105823516845703
Batch 25/64 loss: -0.5338850021362305
Batch 26/64 loss: -0.3334388732910156
Batch 27/64 loss: -0.5313444137573242
Batch 28/64 loss: -0.5408506393432617
Batch 29/64 loss: -0.7015771865844727
Batch 30/64 loss: 0.2487773895263672
Batch 31/64 loss: -0.46405696868896484
Batch 32/64 loss: -0.6680221557617188
Batch 33/64 loss: -0.614903450012207
Batch 34/64 loss: -0.9979162216186523
Batch 35/64 loss: -0.46376991271972656
Batch 36/64 loss: -0.4053020477294922
Batch 37/64 loss: -0.19133949279785156
Batch 38/64 loss: -0.5282564163208008
Batch 39/64 loss: -0.11028766632080078
Batch 40/64 loss: -0.484529972076416
Batch 41/64 loss: -0.5798397064208984
Batch 42/64 loss: -0.6618270874023438
Batch 43/64 loss: -0.15552091598510742
Batch 44/64 loss: -0.5712747573852539
Batch 45/64 loss: -0.5479440689086914
Batch 46/64 loss: -0.9622182846069336
Batch 47/64 loss: -0.6874408721923828
Batch 48/64 loss: 0.059205055236816406
Batch 49/64 loss: -0.8342390060424805
Batch 50/64 loss: -0.3164229393005371
Batch 51/64 loss: -0.0016984939575195312
Batch 52/64 loss: -0.3471083641052246
Batch 53/64 loss: -0.6372537612915039
Batch 54/64 loss: -0.562434196472168
Batch 55/64 loss: -0.04674100875854492
Batch 56/64 loss: -0.3382244110107422
Batch 57/64 loss: -0.7972288131713867
Batch 58/64 loss: -0.10121297836303711
Batch 59/64 loss: -0.7516984939575195
Batch 60/64 loss: -0.47597408294677734
Batch 61/64 loss: -0.5336189270019531
Batch 62/64 loss: 0.10784149169921875
Batch 63/64 loss: -0.5525660514831543
Batch 64/64 loss: -4.517051696777344
Epoch 299  Train loss: -0.5150127036898744  Val loss: -0.6267767575188601
Epoch 300
-------------------------------
Batch 1/64 loss: -0.767613410949707
Batch 2/64 loss: -0.9888467788696289
Batch 3/64 loss: -0.27562713623046875
Batch 4/64 loss: -0.6359338760375977
Batch 5/64 loss: -0.5724902153015137
Batch 6/64 loss: -0.3459348678588867
Batch 7/64 loss: -0.23565053939819336
Batch 8/64 loss: -0.72021484375
Batch 9/64 loss: -0.3224043846130371
Batch 10/64 loss: -0.33490848541259766
Batch 11/64 loss: -0.1337885856628418
Batch 12/64 loss: -0.6601572036743164
Batch 13/64 loss: -0.40753698348999023
Batch 14/64 loss: -0.9042549133300781
Batch 15/64 loss: -0.4117097854614258
Batch 16/64 loss: -0.4954872131347656
Batch 17/64 loss: -0.6720943450927734
Batch 18/64 loss: -0.5638389587402344
Batch 19/64 loss: -0.9896278381347656
Batch 20/64 loss: -0.7856855392456055
Batch 21/64 loss: -0.47949695587158203
Batch 22/64 loss: -0.46947479248046875
Batch 23/64 loss: -0.6032247543334961
Batch 24/64 loss: -0.4024658203125
Batch 25/64 loss: -0.7672567367553711
Batch 26/64 loss: -0.3715782165527344
Batch 27/64 loss: -0.6752729415893555
Batch 28/64 loss: -0.48100996017456055
Batch 29/64 loss: -0.06549263000488281
Batch 30/64 loss: -0.08115959167480469
Batch 31/64 loss: -0.5350079536437988
Batch 32/64 loss: -0.4585580825805664
Batch 33/64 loss: -0.21712875366210938
Batch 34/64 loss: -0.8999013900756836
Batch 35/64 loss: -0.6774039268493652
Batch 36/64 loss: -0.7418813705444336
Batch 37/64 loss: -0.42408275604248047
Batch 38/64 loss: -0.42571115493774414
Batch 39/64 loss: -0.32489585876464844
Batch 40/64 loss: -0.4139423370361328
Batch 41/64 loss: -0.42085790634155273
Batch 42/64 loss: -0.7138099670410156
Batch 43/64 loss: -0.7493438720703125
Batch 44/64 loss: 0.11575889587402344
Batch 45/64 loss: -0.5985012054443359
Batch 46/64 loss: -0.7717580795288086
Batch 47/64 loss: -0.5712184906005859
Batch 48/64 loss: -0.5737504959106445
Batch 49/64 loss: -0.5889768600463867
Batch 50/64 loss: -0.6661710739135742
Batch 51/64 loss: -0.8564653396606445
Batch 52/64 loss: -0.8285350799560547
Batch 53/64 loss: -0.41046142578125
Batch 54/64 loss: -0.7839765548706055
Batch 55/64 loss: -0.7612380981445312
Batch 56/64 loss: -0.7797079086303711
Batch 57/64 loss: -0.7979307174682617
Batch 58/64 loss: -0.9548587799072266
Batch 59/64 loss: -0.49866580963134766
Batch 60/64 loss: -0.6744508743286133
Batch 61/64 loss: -0.5389585494995117
Batch 62/64 loss: -0.7047996520996094
Batch 63/64 loss: -0.8715219497680664
Batch 64/64 loss: -4.6997551918029785
Epoch 300  Train loss: -0.6159022443434772  Val loss: -0.7568845060682788
Epoch 301
-------------------------------
Batch 1/64 loss: -0.677098274230957
Batch 2/64 loss: -0.36523962020874023
Batch 3/64 loss: -0.6894512176513672
Batch 4/64 loss: -0.8289403915405273
Batch 5/64 loss: -0.8025178909301758
Batch 6/64 loss: -0.6444177627563477
Batch 7/64 loss: -0.42720508575439453
Batch 8/64 loss: -0.12516164779663086
Batch 9/64 loss: -0.5732431411743164
Batch 10/64 loss: -0.7967090606689453
Batch 11/64 loss: -0.28176116943359375
Batch 12/64 loss: -0.3214445114135742
Batch 13/64 loss: -0.5733547210693359
Batch 14/64 loss: -0.9348506927490234
Batch 15/64 loss: -0.5979022979736328
Batch 16/64 loss: -0.5116167068481445
Batch 17/64 loss: -0.8329620361328125
Batch 18/64 loss: -0.6836051940917969
Batch 19/64 loss: -0.7649164199829102
Batch 20/64 loss: -0.6099100112915039
Batch 21/64 loss: -0.3165454864501953
Batch 22/64 loss: -0.639012336730957
Batch 23/64 loss: -0.825526237487793
Batch 24/64 loss: -0.6408147811889648
Batch 25/64 loss: -0.7495183944702148
Batch 26/64 loss: -0.6605796813964844
Batch 27/64 loss: -0.6509299278259277
Batch 28/64 loss: -0.46903038024902344
Batch 29/64 loss: -0.3739776611328125
Batch 30/64 loss: -0.6297454833984375
Batch 31/64 loss: -0.9795303344726562
Batch 32/64 loss: -0.8030376434326172
Batch 33/64 loss: -0.2329397201538086
Batch 34/64 loss: -0.33234357833862305
Batch 35/64 loss: -0.3988370895385742
Batch 36/64 loss: -0.6964225769042969
Batch 37/64 loss: -0.7559986114501953
Batch 38/64 loss: -0.9883337020874023
Batch 39/64 loss: -0.49338626861572266
Batch 40/64 loss: -0.4543781280517578
Batch 41/64 loss: -0.12589693069458008
Batch 42/64 loss: -0.2761363983154297
Batch 43/64 loss: -0.8825998306274414
Batch 44/64 loss: -0.38327789306640625
Batch 45/64 loss: -0.8589639663696289
Batch 46/64 loss: -0.26462650299072266
Batch 47/64 loss: -0.817387580871582
Batch 48/64 loss: -0.10432052612304688
Batch 49/64 loss: -0.2593097686767578
Batch 50/64 loss: -0.5362358093261719
Batch 51/64 loss: -0.8184585571289062
Batch 52/64 loss: -0.5762691497802734
Batch 53/64 loss: -0.7743387222290039
Batch 54/64 loss: -0.6187505722045898
Batch 55/64 loss: -0.2377619743347168
Batch 56/64 loss: -0.5435171127319336
Batch 57/64 loss: -0.7558717727661133
Batch 58/64 loss: -0.4290428161621094
Batch 59/64 loss: -0.8182439804077148
Batch 60/64 loss: -0.8423395156860352
Batch 61/64 loss: -0.5867853164672852
Batch 62/64 loss: -0.6574029922485352
Batch 63/64 loss: -0.672612190246582
Batch 64/64 loss: -4.414864540100098
Epoch 301  Train loss: -0.6319136339075425  Val loss: -0.6737366509191769
Epoch 302
-------------------------------
Batch 1/64 loss: -0.724370002746582
Batch 2/64 loss: -0.45450782775878906
Batch 3/64 loss: -0.040865421295166016
Batch 4/64 loss: -0.5247831344604492
Batch 5/64 loss: -0.6666889190673828
Batch 6/64 loss: -1.0146856307983398
Batch 7/64 loss: -0.2818779945373535
Batch 8/64 loss: -0.35085010528564453
Batch 9/64 loss: -0.7839431762695312
Batch 10/64 loss: -0.034857749938964844
Batch 11/64 loss: -0.8621969223022461
Batch 12/64 loss: -0.05085945129394531
Batch 13/64 loss: -0.8445339202880859
Batch 14/64 loss: -0.15506982803344727
Batch 15/64 loss: -0.5959129333496094
Batch 16/64 loss: -0.3512535095214844
Batch 17/64 loss: -0.7941303253173828
Batch 18/64 loss: -0.47380828857421875
Batch 19/64 loss: -0.381986141204834
Batch 20/64 loss: -0.8544216156005859
Batch 21/64 loss: -0.856475830078125
Batch 22/64 loss: -0.8408241271972656
Batch 23/64 loss: -0.52435302734375
Batch 24/64 loss: -0.5980730056762695
Batch 25/64 loss: -0.3063082695007324
Batch 26/64 loss: -0.6512966156005859
Batch 27/64 loss: -0.3832836151123047
Batch 28/64 loss: -0.6342325210571289
Batch 29/64 loss: -0.7755441665649414
Batch 30/64 loss: -0.8270645141601562
Batch 31/64 loss: -0.6886301040649414
Batch 32/64 loss: -0.41795969009399414
Batch 33/64 loss: -0.29747724533081055
Batch 34/64 loss: -0.5309481620788574
Batch 35/64 loss: -0.7085433006286621
Batch 36/64 loss: -0.6542825698852539
Batch 37/64 loss: -0.2617464065551758
Batch 38/64 loss: -0.4777870178222656
Batch 39/64 loss: -0.39827585220336914
Batch 40/64 loss: -0.8830709457397461
Batch 41/64 loss: -0.5747518539428711
Batch 42/64 loss: -0.735015869140625
Batch 43/64 loss: -0.8689327239990234
Batch 44/64 loss: -0.7674150466918945
Batch 45/64 loss: -0.3100895881652832
Batch 46/64 loss: -0.9561119079589844
Batch 47/64 loss: -0.1435680389404297
Batch 48/64 loss: -0.6110401153564453
Batch 49/64 loss: -0.6606788635253906
Batch 50/64 loss: -0.6476621627807617
Batch 51/64 loss: -0.5625438690185547
Batch 52/64 loss: -0.8505401611328125
Batch 53/64 loss: -0.7123794555664062
Batch 54/64 loss: -0.505033016204834
Batch 55/64 loss: -0.6393108367919922
Batch 56/64 loss: -0.5531473159790039
Batch 57/64 loss: -0.842505931854248
Batch 58/64 loss: -0.7236251831054688
Batch 59/64 loss: -0.6522741317749023
Batch 60/64 loss: -0.9049472808837891
Batch 61/64 loss: -0.2828078269958496
Batch 62/64 loss: -0.9964094161987305
Batch 63/64 loss: -0.9146709442138672
Batch 64/64 loss: -4.356790542602539
Epoch 302  Train loss: -0.6375032836315678  Val loss: -0.6771252950032552
Epoch 303
-------------------------------
Batch 1/64 loss: 0.4128446578979492
Batch 2/64 loss: -0.7254257202148438
Batch 3/64 loss: -0.7425174713134766
Batch 4/64 loss: -0.5202813148498535
Batch 5/64 loss: -0.6771888732910156
Batch 6/64 loss: -0.8673219680786133
Batch 7/64 loss: -0.8358278274536133
Batch 8/64 loss: -0.5808658599853516
Batch 9/64 loss: -0.8952856063842773
Batch 10/64 loss: -0.8003578186035156
Batch 11/64 loss: -0.5609827041625977
Batch 12/64 loss: -0.5244951248168945
Batch 13/64 loss: -0.8835115432739258
Batch 14/64 loss: -0.4504547119140625
Batch 15/64 loss: -0.8164911270141602
Batch 16/64 loss: -0.6790552139282227
Batch 17/64 loss: -0.6119203567504883
Batch 18/64 loss: -0.5069212913513184
Batch 19/64 loss: -0.5849218368530273
Batch 20/64 loss: -1.0082168579101562
Batch 21/64 loss: -0.2711796760559082
Batch 22/64 loss: -0.5438709259033203
Batch 23/64 loss: -0.5490250587463379
Batch 24/64 loss: -0.610260009765625
Batch 25/64 loss: -0.33385181427001953
Batch 26/64 loss: 0.17307662963867188
Batch 27/64 loss: -0.4002676010131836
Batch 28/64 loss: -0.4358692169189453
Batch 29/64 loss: -0.7156391143798828
Batch 30/64 loss: -0.7604207992553711
Batch 31/64 loss: -0.3679771423339844
Batch 32/64 loss: -0.7278814315795898
Batch 33/64 loss: -0.7410249710083008
Batch 34/64 loss: -0.824397087097168
Batch 35/64 loss: -1.0716142654418945
Batch 36/64 loss: -0.7344694137573242
Batch 37/64 loss: -0.7223062515258789
Batch 38/64 loss: -0.45742034912109375
Batch 39/64 loss: -1.0344324111938477
Batch 40/64 loss: -0.5825061798095703
Batch 41/64 loss: -0.02370166778564453
Batch 42/64 loss: -0.652003288269043
Batch 43/64 loss: -0.9341945648193359
Batch 44/64 loss: -0.8527402877807617
Batch 45/64 loss: -1.0291976928710938
Batch 46/64 loss: -0.7013359069824219
Batch 47/64 loss: -0.6902828216552734
Batch 48/64 loss: -0.5387039184570312
Batch 49/64 loss: -0.7088127136230469
Batch 50/64 loss: -0.6181964874267578
Batch 51/64 loss: -0.21850872039794922
Batch 52/64 loss: -0.5400476455688477
Batch 53/64 loss: -0.5271091461181641
Batch 54/64 loss: -0.8985204696655273
Batch 55/64 loss: -0.7447052001953125
Batch 56/64 loss: -1.0753698348999023
Batch 57/64 loss: -0.9095859527587891
Batch 58/64 loss: -0.7618799209594727
Batch 59/64 loss: 0.4340629577636719
Batch 60/64 loss: -0.708247184753418
Batch 61/64 loss: -0.883819580078125
Batch 62/64 loss: -0.646613597869873
Batch 63/64 loss: -0.23872900009155273
Batch 64/64 loss: -4.308578968048096
Epoch 303  Train loss: -0.663062157350428  Val loss: -0.6581514496164224
Epoch 304
-------------------------------
Batch 1/64 loss: -0.4181652069091797
Batch 2/64 loss: -0.6362218856811523
Batch 3/64 loss: -0.7970829010009766
Batch 4/64 loss: -0.7404270172119141
Batch 5/64 loss: -0.39304161071777344
Batch 6/64 loss: -0.9452066421508789
Batch 7/64 loss: -0.6202487945556641
Batch 8/64 loss: -0.5268125534057617
Batch 9/64 loss: -0.673797607421875
Batch 10/64 loss: -0.6187372207641602
Batch 11/64 loss: -0.6616077423095703
Batch 12/64 loss: -0.5846118927001953
Batch 13/64 loss: -0.4872918128967285
Batch 14/64 loss: -0.6978874206542969
Batch 15/64 loss: -0.8517942428588867
Batch 16/64 loss: -0.7465000152587891
Batch 17/64 loss: -0.8735532760620117
Batch 18/64 loss: -0.7485466003417969
Batch 19/64 loss: 0.052903175354003906
Batch 20/64 loss: -0.5226097106933594
Batch 21/64 loss: -0.8362674713134766
Batch 22/64 loss: -0.6571559906005859
Batch 23/64 loss: -0.9990730285644531
Batch 24/64 loss: -0.8710422515869141
Batch 25/64 loss: -0.7761001586914062
Batch 26/64 loss: -0.5956730842590332
Batch 27/64 loss: -0.319091796875
Batch 28/64 loss: -0.5952930450439453
Batch 29/64 loss: -0.36766815185546875
Batch 30/64 loss: -0.7833623886108398
Batch 31/64 loss: -0.5998125076293945
Batch 32/64 loss: -0.43465328216552734
Batch 33/64 loss: -0.6387243270874023
Batch 34/64 loss: -0.9516181945800781
Batch 35/64 loss: -0.5924224853515625
Batch 36/64 loss: -0.6800146102905273
Batch 37/64 loss: -0.24569320678710938
Batch 38/64 loss: -0.37571144104003906
Batch 39/64 loss: -1.042191505432129
Batch 40/64 loss: -0.7048606872558594
Batch 41/64 loss: -0.4723367691040039
Batch 42/64 loss: -0.6996021270751953
Batch 43/64 loss: -0.8290090560913086
Batch 44/64 loss: -0.7556543350219727
Batch 45/64 loss: -0.7203292846679688
Batch 46/64 loss: -0.5839776992797852
Batch 47/64 loss: -0.7029657363891602
Batch 48/64 loss: -0.8554658889770508
Batch 49/64 loss: -0.6120986938476562
Batch 50/64 loss: -0.6683158874511719
Batch 51/64 loss: -0.848179817199707
Batch 52/64 loss: -0.29647159576416016
Batch 53/64 loss: -1.0135612487792969
Batch 54/64 loss: -1.0902042388916016
Batch 55/64 loss: -0.4296398162841797
Batch 56/64 loss: -0.3553943634033203
Batch 57/64 loss: -0.8071155548095703
Batch 58/64 loss: -0.5385227203369141
Batch 59/64 loss: -0.8032560348510742
Batch 60/64 loss: -0.8548440933227539
Batch 61/64 loss: -0.5556888580322266
Batch 62/64 loss: -0.8992471694946289
Batch 63/64 loss: -0.542327880859375
Batch 64/64 loss: -4.59151029586792
Epoch 304  Train loss: -0.7048707793740665  Val loss: -0.7315685626157781
Epoch 305
-------------------------------
Batch 1/64 loss: -0.25485897064208984
Batch 2/64 loss: -0.42061519622802734
Batch 3/64 loss: -0.9311914443969727
Batch 4/64 loss: -0.7270717620849609
Batch 5/64 loss: -0.6966876983642578
Batch 6/64 loss: -0.5630292892456055
Batch 7/64 loss: -0.44812583923339844
Batch 8/64 loss: -0.7329692840576172
Batch 9/64 loss: -0.8957834243774414
Batch 10/64 loss: -0.07566165924072266
Batch 11/64 loss: -0.7528219223022461
Batch 12/64 loss: -0.3313331604003906
Batch 13/64 loss: -0.4869718551635742
Batch 14/64 loss: -0.4972381591796875
Batch 15/64 loss: -0.8772802352905273
Batch 16/64 loss: -0.7319097518920898
Batch 17/64 loss: -0.7154903411865234
Batch 18/64 loss: -0.22736263275146484
Batch 19/64 loss: -0.6541681289672852
Batch 20/64 loss: -0.720911979675293
Batch 21/64 loss: -0.3916816711425781
Batch 22/64 loss: -0.17272520065307617
Batch 23/64 loss: -0.7201213836669922
Batch 24/64 loss: -0.04975605010986328
Batch 25/64 loss: -0.5456314086914062
Batch 26/64 loss: -0.2173604965209961
Batch 27/64 loss: -0.7790889739990234
Batch 28/64 loss: -0.4629039764404297
Batch 29/64 loss: -0.14845752716064453
Batch 30/64 loss: -0.6015214920043945
Batch 31/64 loss: -0.4690084457397461
Batch 32/64 loss: -0.35390758514404297
Batch 33/64 loss: -0.6359424591064453
Batch 34/64 loss: -0.7474422454833984
Batch 35/64 loss: -0.9414691925048828
Batch 36/64 loss: -0.21399211883544922
Batch 37/64 loss: -0.5881004333496094
Batch 38/64 loss: -0.6824417114257812
Batch 39/64 loss: -0.705439567565918
Batch 40/64 loss: -0.7267179489135742
Batch 41/64 loss: -0.48786067962646484
Batch 42/64 loss: -0.974024772644043
Batch 43/64 loss: -0.329071044921875
Batch 44/64 loss: -0.9031953811645508
Batch 45/64 loss: -0.7395648956298828
Batch 46/64 loss: -0.3397092819213867
Batch 47/64 loss: -0.9259166717529297
Batch 48/64 loss: -0.8129920959472656
Batch 49/64 loss: -0.7882194519042969
Batch 50/64 loss: -0.3979072570800781
Batch 51/64 loss: -0.6407766342163086
Batch 52/64 loss: -0.5232725143432617
Batch 53/64 loss: -0.4568061828613281
Batch 54/64 loss: -0.47485828399658203
Batch 55/64 loss: -0.9260578155517578
Batch 56/64 loss: -0.7799844741821289
Batch 57/64 loss: -0.1902923583984375
Batch 58/64 loss: -0.7477855682373047
Batch 59/64 loss: -0.47005462646484375
Batch 60/64 loss: -0.9298229217529297
Batch 61/64 loss: -0.7652978897094727
Batch 62/64 loss: -0.7623014450073242
Batch 63/64 loss: -0.5246381759643555
Batch 64/64 loss: -4.47421407699585
Epoch 305  Train loss: -0.6296668800653196  Val loss: -0.6767908076650089
Epoch 306
-------------------------------
Batch 1/64 loss: -0.3208446502685547
Batch 2/64 loss: -0.715123176574707
Batch 3/64 loss: -0.4813861846923828
Batch 4/64 loss: -0.6387786865234375
Batch 5/64 loss: -0.16923236846923828
Batch 6/64 loss: -0.5465488433837891
Batch 7/64 loss: -0.28435182571411133
Batch 8/64 loss: -0.22763347625732422
Batch 9/64 loss: -0.6189022064208984
Batch 10/64 loss: -0.47220802307128906
Batch 11/64 loss: -0.4274625778198242
Batch 12/64 loss: -0.7743597030639648
Batch 13/64 loss: -0.5177574157714844
Batch 14/64 loss: -0.49523067474365234
Batch 15/64 loss: -0.4761486053466797
Batch 16/64 loss: -0.9043712615966797
Batch 17/64 loss: -0.2503347396850586
Batch 18/64 loss: -0.4972257614135742
Batch 19/64 loss: -0.7927627563476562
Batch 20/64 loss: -0.8231124877929688
Batch 21/64 loss: -0.8527636528015137
Batch 22/64 loss: -0.6478824615478516
Batch 23/64 loss: -0.34854650497436523
Batch 24/64 loss: -0.2774057388305664
Batch 25/64 loss: -0.9585952758789062
Batch 26/64 loss: -0.7738332748413086
Batch 27/64 loss: -0.6413793563842773
Batch 28/64 loss: -0.6303749084472656
Batch 29/64 loss: -0.7865476608276367
Batch 30/64 loss: -0.4809293746948242
Batch 31/64 loss: -0.6122303009033203
Batch 32/64 loss: -0.43059730529785156
Batch 33/64 loss: -0.43433380126953125
Batch 34/64 loss: -0.6243200302124023
Batch 35/64 loss: -0.8898067474365234
Batch 36/64 loss: -0.7520732879638672
Batch 37/64 loss: -0.8449516296386719
Batch 38/64 loss: -0.9389438629150391
Batch 39/64 loss: -0.9169902801513672
Batch 40/64 loss: -0.45096778869628906
Batch 41/64 loss: -0.6984348297119141
Batch 42/64 loss: -0.5770797729492188
Batch 43/64 loss: -0.6267185211181641
Batch 44/64 loss: -0.6093897819519043
Batch 45/64 loss: -0.5372858047485352
Batch 46/64 loss: -1.1506376266479492
Batch 47/64 loss: -0.7101240158081055
Batch 48/64 loss: -0.16185379028320312
Batch 49/64 loss: -0.8201103210449219
Batch 50/64 loss: -0.4073925018310547
Batch 51/64 loss: -0.8700523376464844
Batch 52/64 loss: -0.7821044921875
Batch 53/64 loss: -0.9535722732543945
Batch 54/64 loss: -0.9594945907592773
Batch 55/64 loss: -0.8592662811279297
Batch 56/64 loss: -0.5814876556396484
Batch 57/64 loss: -0.6673622131347656
Batch 58/64 loss: -0.7582502365112305
Batch 59/64 loss: -0.4559440612792969
Batch 60/64 loss: -1.1424942016601562
Batch 61/64 loss: -0.4613037109375
Batch 62/64 loss: -0.41402721405029297
Batch 63/64 loss: -0.5859737396240234
Batch 64/64 loss: -4.544862747192383
Epoch 306  Train loss: -0.6733530306348614  Val loss: -0.7706309446354502
Saving best model, epoch: 306
Epoch 307
-------------------------------
Batch 1/64 loss: -0.42102527618408203
Batch 2/64 loss: -0.7161006927490234
Batch 3/64 loss: -0.6782293319702148
Batch 4/64 loss: -1.0348005294799805
Batch 5/64 loss: -0.4551429748535156
Batch 6/64 loss: -0.6457500457763672
Batch 7/64 loss: -0.5675430297851562
Batch 8/64 loss: -0.6442880630493164
Batch 9/64 loss: -0.8564786911010742
Batch 10/64 loss: -0.6497573852539062
Batch 11/64 loss: -0.3985328674316406
Batch 12/64 loss: -0.579737663269043
Batch 13/64 loss: -1.0013227462768555
Batch 14/64 loss: -0.7194252014160156
Batch 15/64 loss: -0.5390214920043945
Batch 16/64 loss: -0.8292083740234375
Batch 17/64 loss: -0.3382682800292969
Batch 18/64 loss: -0.6711759567260742
Batch 19/64 loss: -0.5349645614624023
Batch 20/64 loss: -0.20742321014404297
Batch 21/64 loss: -0.9542093276977539
Batch 22/64 loss: -0.5015735626220703
Batch 23/64 loss: -0.7579622268676758
Batch 24/64 loss: -0.7517108917236328
Batch 25/64 loss: -0.33001136779785156
Batch 26/64 loss: -0.37406444549560547
Batch 27/64 loss: -0.7290868759155273
Batch 28/64 loss: -0.5678815841674805
Batch 29/64 loss: -0.7829694747924805
Batch 30/64 loss: -0.962092399597168
Batch 31/64 loss: -0.9635763168334961
Batch 32/64 loss: -0.4447059631347656
Batch 33/64 loss: -0.711674690246582
Batch 34/64 loss: -0.9899778366088867
Batch 35/64 loss: -0.9077367782592773
Batch 36/64 loss: -0.3051638603210449
Batch 37/64 loss: -0.9259548187255859
Batch 38/64 loss: -0.45096540451049805
Batch 39/64 loss: -0.4058570861816406
Batch 40/64 loss: -0.8087158203125
Batch 41/64 loss: -0.6553678512573242
Batch 42/64 loss: -0.6849517822265625
Batch 43/64 loss: -0.3664379119873047
Batch 44/64 loss: -0.6472945213317871
Batch 45/64 loss: -1.0984506607055664
Batch 46/64 loss: -0.6659574508666992
Batch 47/64 loss: -0.6221427917480469
Batch 48/64 loss: -0.7604541778564453
Batch 49/64 loss: -0.7540245056152344
Batch 50/64 loss: -0.7279672622680664
Batch 51/64 loss: -0.3825807571411133
Batch 52/64 loss: -0.6454401016235352
Batch 53/64 loss: -0.9701976776123047
Batch 54/64 loss: -0.49222564697265625
Batch 55/64 loss: -0.972538948059082
Batch 56/64 loss: -0.8806533813476562
Batch 57/64 loss: -0.48283910751342773
Batch 58/64 loss: -0.7068843841552734
Batch 59/64 loss: -0.5774431228637695
Batch 60/64 loss: -0.5751218795776367
Batch 61/64 loss: -0.7185516357421875
Batch 62/64 loss: -1.0184621810913086
Batch 63/64 loss: -0.6587376594543457
Batch 64/64 loss: -4.550037384033203
Epoch 307  Train loss: -0.7151582830092487  Val loss: -0.8787258318609389
Saving best model, epoch: 307
Epoch 308
-------------------------------
Batch 1/64 loss: -0.92431640625
Batch 2/64 loss: -0.6945343017578125
Batch 3/64 loss: -0.4586162567138672
Batch 4/64 loss: -0.7206873893737793
Batch 5/64 loss: -0.7658920288085938
Batch 6/64 loss: -0.4499168395996094
Batch 7/64 loss: -0.5602502822875977
Batch 8/64 loss: -0.9117326736450195
Batch 9/64 loss: -0.37186717987060547
Batch 10/64 loss: -0.942378044128418
Batch 11/64 loss: -0.7082257270812988
Batch 12/64 loss: -0.6109123229980469
Batch 13/64 loss: -0.4039306640625
Batch 14/64 loss: -0.7561159133911133
Batch 15/64 loss: -0.632960319519043
Batch 16/64 loss: -1.1385068893432617
Batch 17/64 loss: -0.5935277938842773
Batch 18/64 loss: -0.748903751373291
Batch 19/64 loss: -0.022014141082763672
Batch 20/64 loss: -0.6905975341796875
Batch 21/64 loss: -0.7761831283569336
Batch 22/64 loss: -0.8244962692260742
Batch 23/64 loss: -0.7091808319091797
Batch 24/64 loss: -0.7561922073364258
Batch 25/64 loss: -0.7240085601806641
Batch 26/64 loss: -0.8513326644897461
Batch 27/64 loss: -0.842808723449707
Batch 28/64 loss: -0.6297779083251953
Batch 29/64 loss: -0.7303791046142578
Batch 30/64 loss: -0.35117149353027344
Batch 31/64 loss: -0.4892268180847168
Batch 32/64 loss: -0.8033781051635742
Batch 33/64 loss: -0.11748647689819336
Batch 34/64 loss: -0.9295063018798828
Batch 35/64 loss: -0.549933910369873
Batch 36/64 loss: -0.9302530288696289
Batch 37/64 loss: -0.4291048049926758
Batch 38/64 loss: -0.8835124969482422
Batch 39/64 loss: -0.9162130355834961
Batch 40/64 loss: -0.4584341049194336
Batch 41/64 loss: -0.7355008125305176
Batch 42/64 loss: -0.8556995391845703
Batch 43/64 loss: -0.14414262771606445
Batch 44/64 loss: -0.33833837509155273
Batch 45/64 loss: -0.45355892181396484
Batch 46/64 loss: -0.6042213439941406
Batch 47/64 loss: -0.4643678665161133
Batch 48/64 loss: -0.6344738006591797
Batch 49/64 loss: -0.5590429306030273
Batch 50/64 loss: -0.43784332275390625
Batch 51/64 loss: -0.5191316604614258
Batch 52/64 loss: -0.563812255859375
Batch 53/64 loss: -0.6907892227172852
Batch 54/64 loss: -0.11805963516235352
Batch 55/64 loss: -0.4596395492553711
Batch 56/64 loss: -0.1558666229248047
Batch 57/64 loss: -0.8980817794799805
Batch 58/64 loss: -0.5161700248718262
Batch 59/64 loss: -0.8198862075805664
Batch 60/64 loss: -0.6575984954833984
Batch 61/64 loss: -0.8707542419433594
Batch 62/64 loss: -1.0306243896484375
Batch 63/64 loss: -0.5876369476318359
Batch 64/64 loss: -4.375012397766113
Epoch 308  Train loss: -0.6777249616735121  Val loss: -0.7317647114652129
Epoch 309
-------------------------------
Batch 1/64 loss: -0.6354694366455078
Batch 2/64 loss: -0.48169660568237305
Batch 3/64 loss: -0.7451319694519043
Batch 4/64 loss: -0.7545881271362305
Batch 5/64 loss: -0.8077659606933594
Batch 6/64 loss: -0.5243892669677734
Batch 7/64 loss: -0.6098861694335938
Batch 8/64 loss: -0.4926314353942871
Batch 9/64 loss: -0.7988319396972656
Batch 10/64 loss: -0.618983268737793
Batch 11/64 loss: -0.6375932693481445
Batch 12/64 loss: -0.3972597122192383
Batch 13/64 loss: -0.534550666809082
Batch 14/64 loss: -0.47336578369140625
Batch 15/64 loss: -0.7125940322875977
Batch 16/64 loss: -0.6812396049499512
Batch 17/64 loss: -0.5269842147827148
Batch 18/64 loss: -0.7895669937133789
Batch 19/64 loss: -0.5474600791931152
Batch 20/64 loss: -0.2962169647216797
Batch 21/64 loss: -0.7084236145019531
Batch 22/64 loss: -0.6351776123046875
Batch 23/64 loss: -0.4374074935913086
Batch 24/64 loss: -0.02992391586303711
Batch 25/64 loss: -0.753779411315918
Batch 26/64 loss: 0.30489158630371094
Batch 27/64 loss: -0.6284322738647461
Batch 28/64 loss: -0.5381622314453125
Batch 29/64 loss: -0.8299984931945801
Batch 30/64 loss: -0.6159896850585938
Batch 31/64 loss: -0.8347249031066895
Batch 32/64 loss: -0.7626991271972656
Batch 33/64 loss: -0.47684812545776367
Batch 34/64 loss: -0.8923110961914062
Batch 35/64 loss: -0.777226448059082
Batch 36/64 loss: -0.250457763671875
Batch 37/64 loss: -0.7630138397216797
Batch 38/64 loss: -1.0604352951049805
Batch 39/64 loss: -0.09842109680175781
Batch 40/64 loss: -0.8711404800415039
Batch 41/64 loss: -0.01855325698852539
Batch 42/64 loss: -0.6343545913696289
Batch 43/64 loss: -0.7810335159301758
Batch 44/64 loss: -0.8201589584350586
Batch 45/64 loss: -0.3665938377380371
Batch 46/64 loss: -0.8356742858886719
Batch 47/64 loss: -0.9177532196044922
Batch 48/64 loss: -0.8412923812866211
Batch 49/64 loss: -0.7239809036254883
Batch 50/64 loss: -0.7646493911743164
Batch 51/64 loss: 0.0540165901184082
Batch 52/64 loss: -0.787567138671875
Batch 53/64 loss: -0.9158458709716797
Batch 54/64 loss: -0.48283958435058594
Batch 55/64 loss: -0.6371698379516602
Batch 56/64 loss: -0.20287036895751953
Batch 57/64 loss: -0.23924732208251953
Batch 58/64 loss: -1.0334062576293945
Batch 59/64 loss: -0.16227483749389648
Batch 60/64 loss: -0.6888885498046875
Batch 61/64 loss: -0.7516250610351562
Batch 62/64 loss: -0.5757732391357422
Batch 63/64 loss: -0.6481838226318359
Batch 64/64 loss: -4.640802383422852
Epoch 309  Train loss: -0.6428581686580882  Val loss: -0.7446260353953568
Epoch 310
-------------------------------
Batch 1/64 loss: -0.5171632766723633
Batch 2/64 loss: -0.7644968032836914
Batch 3/64 loss: -0.11298704147338867
Batch 4/64 loss: -0.7308454513549805
Batch 5/64 loss: -0.556246280670166
Batch 6/64 loss: -0.7266073226928711
Batch 7/64 loss: -0.6723098754882812
Batch 8/64 loss: -0.7186384201049805
Batch 9/64 loss: -0.7178363800048828
Batch 10/64 loss: -0.16512727737426758
Batch 11/64 loss: -0.7699317932128906
Batch 12/64 loss: -0.759678840637207
Batch 13/64 loss: -0.7733383178710938
Batch 14/64 loss: -0.8135290145874023
Batch 15/64 loss: -0.7138833999633789
Batch 16/64 loss: -0.3716154098510742
Batch 17/64 loss: -0.7255859375
Batch 18/64 loss: -0.3092355728149414
Batch 19/64 loss: -0.32926368713378906
Batch 20/64 loss: -0.6057920455932617
Batch 21/64 loss: -0.6877365112304688
Batch 22/64 loss: -0.7415494918823242
Batch 23/64 loss: -0.8453178405761719
Batch 24/64 loss: -0.8171768188476562
Batch 25/64 loss: -0.39694929122924805
Batch 26/64 loss: -0.36898136138916016
Batch 27/64 loss: -0.6149663925170898
Batch 28/64 loss: -0.43640565872192383
Batch 29/64 loss: -0.7388916015625
Batch 30/64 loss: -0.6408481597900391
Batch 31/64 loss: -0.40645456314086914
Batch 32/64 loss: -0.7160978317260742
Batch 33/64 loss: -0.7716703414916992
Batch 34/64 loss: -0.5974807739257812
Batch 35/64 loss: -0.6132030487060547
Batch 36/64 loss: -0.9231777191162109
Batch 37/64 loss: -0.7269077301025391
Batch 38/64 loss: -0.5810966491699219
Batch 39/64 loss: -0.8693933486938477
Batch 40/64 loss: -0.4401397705078125
Batch 41/64 loss: -0.6940193176269531
Batch 42/64 loss: -0.8718948364257812
Batch 43/64 loss: -0.8016042709350586
Batch 44/64 loss: -0.5109338760375977
Batch 45/64 loss: -0.49340152740478516
Batch 46/64 loss: -0.4464731216430664
Batch 47/64 loss: 0.21036338806152344
Batch 48/64 loss: -0.6633529663085938
Batch 49/64 loss: -1.003152847290039
Batch 50/64 loss: -0.8742847442626953
Batch 51/64 loss: 0.05551767349243164
Batch 52/64 loss: -0.7117881774902344
Batch 53/64 loss: -0.2051715850830078
Batch 54/64 loss: -0.6467523574829102
Batch 55/64 loss: -0.6733407974243164
Batch 56/64 loss: -0.8142948150634766
Batch 57/64 loss: -0.7922525405883789
Batch 58/64 loss: -0.5972261428833008
Batch 59/64 loss: -0.5425834655761719
Batch 60/64 loss: -0.7992467880249023
Batch 61/64 loss: -0.44013547897338867
Batch 62/64 loss: -0.5213384628295898
Batch 63/64 loss: -0.9008502960205078
Batch 64/64 loss: -4.667846202850342
Epoch 310  Train loss: -0.65925739699719  Val loss: -0.7155060391245839
Epoch 311
-------------------------------
Batch 1/64 loss: -0.5803966522216797
Batch 2/64 loss: -0.39284706115722656
Batch 3/64 loss: -0.891453742980957
Batch 4/64 loss: -0.4167814254760742
Batch 5/64 loss: -0.09683895111083984
Batch 6/64 loss: -0.5804538726806641
Batch 7/64 loss: -0.5500283241271973
Batch 8/64 loss: -0.8349227905273438
Batch 9/64 loss: -0.5210752487182617
Batch 10/64 loss: -0.3661231994628906
Batch 11/64 loss: -0.685786247253418
Batch 12/64 loss: -0.9152183532714844
Batch 13/64 loss: -0.5739307403564453
Batch 14/64 loss: -0.6102018356323242
Batch 15/64 loss: -0.8470554351806641
Batch 16/64 loss: -0.6813545227050781
Batch 17/64 loss: -0.824615478515625
Batch 18/64 loss: -1.0520057678222656
Batch 19/64 loss: -0.8447027206420898
Batch 20/64 loss: -0.39749908447265625
Batch 21/64 loss: -0.4961709976196289
Batch 22/64 loss: -0.8446235656738281
Batch 23/64 loss: -0.6942205429077148
Batch 24/64 loss: -0.8020877838134766
Batch 25/64 loss: -0.8784942626953125
Batch 26/64 loss: -0.4836454391479492
Batch 27/64 loss: -0.9441156387329102
Batch 28/64 loss: -0.47008323669433594
Batch 29/64 loss: 0.058979034423828125
Batch 30/64 loss: -0.9238338470458984
Batch 31/64 loss: -0.8463039398193359
Batch 32/64 loss: -0.6909942626953125
Batch 33/64 loss: 1.0910429954528809
Batch 34/64 loss: -0.9700584411621094
Batch 35/64 loss: -0.6735725402832031
Batch 36/64 loss: -0.8296804428100586
Batch 37/64 loss: -0.7933645248413086
Batch 38/64 loss: -0.8749485015869141
Batch 39/64 loss: -0.2874155044555664
Batch 40/64 loss: -0.5407886505126953
Batch 41/64 loss: -0.26909446716308594
Batch 42/64 loss: -0.16335725784301758
Batch 43/64 loss: -0.6725397109985352
Batch 44/64 loss: -0.22884178161621094
Batch 45/64 loss: -0.750483512878418
Batch 46/64 loss: -0.6364936828613281
Batch 47/64 loss: -0.4095950126647949
Batch 48/64 loss: -0.8836174011230469
Batch 49/64 loss: -0.18459224700927734
Batch 50/64 loss: -0.5711078643798828
Batch 51/64 loss: -0.7463340759277344
Batch 52/64 loss: -0.7744827270507812
Batch 53/64 loss: -0.6982364654541016
Batch 54/64 loss: -0.6009559631347656
Batch 55/64 loss: -0.41656494140625
Batch 56/64 loss: -0.5153751373291016
Batch 57/64 loss: -0.7149686813354492
Batch 58/64 loss: -0.46875953674316406
Batch 59/64 loss: -0.8552007675170898
Batch 60/64 loss: -0.43317508697509766
Batch 61/64 loss: -0.784031867980957
Batch 62/64 loss: -0.4577665328979492
Batch 63/64 loss: -0.7408618927001953
Batch 64/64 loss: -4.505607604980469
Epoch 311  Train loss: -0.6417774724025352  Val loss: -0.6715334338420855
Epoch 312
-------------------------------
Batch 1/64 loss: -1.0356407165527344
Batch 2/64 loss: -0.3863506317138672
Batch 3/64 loss: -0.22938776016235352
Batch 4/64 loss: -0.9501209259033203
Batch 5/64 loss: -0.671046257019043
Batch 6/64 loss: -0.4528989791870117
Batch 7/64 loss: -0.5119390487670898
Batch 8/64 loss: -0.662203311920166
Batch 9/64 loss: -0.6609649658203125
Batch 10/64 loss: -0.7166748046875
Batch 11/64 loss: -0.5726995468139648
Batch 12/64 loss: -0.5364804267883301
Batch 13/64 loss: -0.3183321952819824
Batch 14/64 loss: -0.5113353729248047
Batch 15/64 loss: -0.6723728179931641
Batch 16/64 loss: -0.602386474609375
Batch 17/64 loss: -0.10431766510009766
Batch 18/64 loss: 0.25664520263671875
Batch 19/64 loss: -0.11784839630126953
Batch 20/64 loss: -0.9558677673339844
Batch 21/64 loss: -1.079538345336914
Batch 22/64 loss: -0.8226966857910156
Batch 23/64 loss: -0.8222455978393555
Batch 24/64 loss: -0.5476779937744141
Batch 25/64 loss: -0.7504739761352539
Batch 26/64 loss: -0.5073623657226562
Batch 27/64 loss: -0.9216699600219727
Batch 28/64 loss: -0.6833381652832031
Batch 29/64 loss: -0.4512186050415039
Batch 30/64 loss: -0.6915035247802734
Batch 31/64 loss: -0.44910764694213867
Batch 32/64 loss: -0.8703956604003906
Batch 33/64 loss: -0.6204824447631836
Batch 34/64 loss: -0.7025928497314453
Batch 35/64 loss: -0.9771127700805664
Batch 36/64 loss: -0.8245334625244141
Batch 37/64 loss: -0.6179656982421875
Batch 38/64 loss: -0.8263864517211914
Batch 39/64 loss: -0.7674713134765625
Batch 40/64 loss: -0.11243915557861328
Batch 41/64 loss: -0.9225587844848633
Batch 42/64 loss: -0.4152665138244629
Batch 43/64 loss: -0.6780147552490234
Batch 44/64 loss: -0.8440656661987305
Batch 45/64 loss: -0.8149757385253906
Batch 46/64 loss: -0.9205760955810547
Batch 47/64 loss: -0.7528543472290039
Batch 48/64 loss: -0.8175554275512695
Batch 49/64 loss: -0.6924905776977539
Batch 50/64 loss: -0.6468238830566406
Batch 51/64 loss: -0.566256046295166
Batch 52/64 loss: -0.48723602294921875
Batch 53/64 loss: -0.8377885818481445
Batch 54/64 loss: -0.7961492538452148
Batch 55/64 loss: -0.7173037528991699
Batch 56/64 loss: -0.8488864898681641
Batch 57/64 loss: -0.550806999206543
Batch 58/64 loss: -0.7505879402160645
Batch 59/64 loss: -0.42259740829467773
Batch 60/64 loss: -0.5254936218261719
Batch 61/64 loss: -0.9199619293212891
Batch 62/64 loss: -0.9598875045776367
Batch 63/64 loss: -0.8999547958374023
Batch 64/64 loss: -4.3156023025512695
Epoch 312  Train loss: -0.6977761474310183  Val loss: -0.7003600130376127
Epoch 313
-------------------------------
Batch 1/64 loss: -0.8965587615966797
Batch 2/64 loss: -0.05128002166748047
Batch 3/64 loss: -0.7019386291503906
Batch 4/64 loss: -0.5138192176818848
Batch 5/64 loss: -0.7573347091674805
Batch 6/64 loss: -0.4996194839477539
Batch 7/64 loss: -0.782923698425293
Batch 8/64 loss: -0.7236118316650391
Batch 9/64 loss: -0.5117535591125488
Batch 10/64 loss: -0.4894571304321289
Batch 11/64 loss: 0.03420734405517578
Batch 12/64 loss: -0.4919881820678711
Batch 13/64 loss: -0.6726922988891602
Batch 14/64 loss: -0.6258697509765625
Batch 15/64 loss: -0.12511157989501953
Batch 16/64 loss: -0.3419981002807617
Batch 17/64 loss: -0.5470676422119141
Batch 18/64 loss: -0.9372596740722656
Batch 19/64 loss: -0.9447660446166992
Batch 20/64 loss: -0.9615201950073242
Batch 21/64 loss: -0.4413566589355469
Batch 22/64 loss: -0.7980852127075195
Batch 23/64 loss: -0.7553043365478516
Batch 24/64 loss: -0.7478189468383789
Batch 25/64 loss: -0.7627849578857422
Batch 26/64 loss: -0.9617109298706055
Batch 27/64 loss: -0.42516279220581055
Batch 28/64 loss: -0.6908979415893555
Batch 29/64 loss: -0.5704545974731445
Batch 30/64 loss: -0.30143022537231445
Batch 31/64 loss: -0.5173287391662598
Batch 32/64 loss: -0.6911020278930664
Batch 33/64 loss: -0.8071928024291992
Batch 34/64 loss: -0.6087121963500977
Batch 35/64 loss: -0.37151527404785156
Batch 36/64 loss: -0.7336091995239258
Batch 37/64 loss: -0.5947551727294922
Batch 38/64 loss: -0.7491559982299805
Batch 39/64 loss: -0.5177679061889648
Batch 40/64 loss: -0.8199548721313477
Batch 41/64 loss: -0.4474973678588867
Batch 42/64 loss: -0.766322135925293
Batch 43/64 loss: -0.35133838653564453
Batch 44/64 loss: -0.8342723846435547
Batch 45/64 loss: -0.989314079284668
Batch 46/64 loss: -0.7278099060058594
Batch 47/64 loss: -0.8369436264038086
Batch 48/64 loss: -0.6243181228637695
Batch 49/64 loss: -0.5459938049316406
Batch 50/64 loss: -0.71539306640625
Batch 51/64 loss: -0.8137884140014648
Batch 52/64 loss: -0.4914093017578125
Batch 53/64 loss: -0.9295682907104492
Batch 54/64 loss: -0.3246164321899414
Batch 55/64 loss: -0.5575723648071289
Batch 56/64 loss: -0.5081024169921875
Batch 57/64 loss: -1.0277767181396484
Batch 58/64 loss: -0.6218290328979492
Batch 59/64 loss: -0.7035179138183594
Batch 60/64 loss: -0.2442035675048828
Batch 61/64 loss: -0.8355560302734375
Batch 62/64 loss: -0.638981819152832
Batch 63/64 loss: -0.32170867919921875
Batch 64/64 loss: -4.651111602783203
Epoch 313  Train loss: -0.6706608865775314  Val loss: -0.83255783552976
Epoch 314
-------------------------------
Batch 1/64 loss: -0.7440290451049805
Batch 2/64 loss: -0.9969720840454102
Batch 3/64 loss: -0.7192983627319336
Batch 4/64 loss: -0.7677278518676758
Batch 5/64 loss: -0.8387441635131836
Batch 6/64 loss: -1.0188961029052734
Batch 7/64 loss: -0.39322471618652344
Batch 8/64 loss: -0.9232044219970703
Batch 9/64 loss: 0.02597808837890625
Batch 10/64 loss: -0.6945428848266602
Batch 11/64 loss: -0.5216884613037109
Batch 12/64 loss: -0.49832820892333984
Batch 13/64 loss: -0.6873311996459961
Batch 14/64 loss: -1.1837282180786133
Batch 15/64 loss: -0.4254426956176758
Batch 16/64 loss: -0.8515596389770508
Batch 17/64 loss: -0.6419248580932617
Batch 18/64 loss: -0.6149110794067383
Batch 19/64 loss: -0.6197528839111328
Batch 20/64 loss: -1.0013847351074219
Batch 21/64 loss: -0.720026969909668
Batch 22/64 loss: -0.8679866790771484
Batch 23/64 loss: -0.42386341094970703
Batch 24/64 loss: -0.6990690231323242
Batch 25/64 loss: -0.3930025100708008
Batch 26/64 loss: -0.8689384460449219
Batch 27/64 loss: -0.16129589080810547
Batch 28/64 loss: -0.5120573043823242
Batch 29/64 loss: -0.6439075469970703
Batch 30/64 loss: -0.8045692443847656
Batch 31/64 loss: -0.6180906295776367
Batch 32/64 loss: -0.5520925521850586
Batch 33/64 loss: -0.8621597290039062
Batch 34/64 loss: -0.667694091796875
Batch 35/64 loss: -1.001479148864746
Batch 36/64 loss: -0.7976522445678711
Batch 37/64 loss: -0.4885883331298828
Batch 38/64 loss: -0.602229118347168
Batch 39/64 loss: -0.7305984497070312
Batch 40/64 loss: -0.8336849212646484
Batch 41/64 loss: -0.9633026123046875
Batch 42/64 loss: -0.7262430191040039
Batch 43/64 loss: -0.5503711700439453
Batch 44/64 loss: -0.649073600769043
Batch 45/64 loss: -0.16195106506347656
Batch 46/64 loss: -0.8673505783081055
Batch 47/64 loss: -0.49517154693603516
Batch 48/64 loss: -0.609893798828125
Batch 49/64 loss: -0.7125263214111328
Batch 50/64 loss: -0.5678253173828125
Batch 51/64 loss: 0.1386404037475586
Batch 52/64 loss: -0.5972452163696289
Batch 53/64 loss: -0.6035289764404297
Batch 54/64 loss: -0.783513069152832
Batch 55/64 loss: -0.7541685104370117
Batch 56/64 loss: -0.816767692565918
Batch 57/64 loss: -0.2701148986816406
Batch 58/64 loss: -0.24012279510498047
Batch 59/64 loss: -0.9052839279174805
Batch 60/64 loss: 0.6284055709838867
Batch 61/64 loss: -0.9862546920776367
Batch 62/64 loss: -0.5696792602539062
Batch 63/64 loss: -0.5015840530395508
Batch 64/64 loss: -4.504495620727539
Epoch 314  Train loss: -0.6793568255854588  Val loss: -0.5885497942003598
Epoch 315
-------------------------------
Batch 1/64 loss: -0.6601362228393555
Batch 2/64 loss: -0.051077842712402344
Batch 3/64 loss: -0.5473899841308594
Batch 4/64 loss: -0.46094512939453125
Batch 5/64 loss: -0.37993335723876953
Batch 6/64 loss: -0.28508853912353516
Batch 7/64 loss: -0.2171926498413086
Batch 8/64 loss: -0.5383186340332031
Batch 9/64 loss: -0.730708122253418
Batch 10/64 loss: -0.6081171035766602
Batch 11/64 loss: -0.5364999771118164
Batch 12/64 loss: -0.839695930480957
Batch 13/64 loss: -0.7734794616699219
Batch 14/64 loss: 0.004978179931640625
Batch 15/64 loss: -0.774322509765625
Batch 16/64 loss: -0.5033369064331055
Batch 17/64 loss: 0.09447765350341797
Batch 18/64 loss: -0.24145889282226562
Batch 19/64 loss: -0.8759775161743164
Batch 20/64 loss: -0.192962646484375
Batch 21/64 loss: -0.34406185150146484
Batch 22/64 loss: -0.8958683013916016
Batch 23/64 loss: -0.5506734848022461
Batch 24/64 loss: -0.5276317596435547
Batch 25/64 loss: -0.7062740325927734
Batch 26/64 loss: -0.24871110916137695
Batch 27/64 loss: -0.6702337265014648
Batch 28/64 loss: -0.7179489135742188
Batch 29/64 loss: -0.5972328186035156
Batch 30/64 loss: -0.2682003974914551
Batch 31/64 loss: -0.23695611953735352
Batch 32/64 loss: -0.7852563858032227
Batch 33/64 loss: -0.8877325057983398
Batch 34/64 loss: -0.49180173873901367
Batch 35/64 loss: -0.6760654449462891
Batch 36/64 loss: -0.6188883781433105
Batch 37/64 loss: -0.7547259330749512
Batch 38/64 loss: -0.6257209777832031
Batch 39/64 loss: -0.7532520294189453
Batch 40/64 loss: -0.8184337615966797
Batch 41/64 loss: -0.8888282775878906
Batch 42/64 loss: -0.2669858932495117
Batch 43/64 loss: -0.5095891952514648
Batch 44/64 loss: -0.6162853240966797
Batch 45/64 loss: -0.34821176528930664
Batch 46/64 loss: -0.45561885833740234
Batch 47/64 loss: -0.8227872848510742
Batch 48/64 loss: -0.7459244728088379
Batch 49/64 loss: -1.0101804733276367
Batch 50/64 loss: -0.842529296875
Batch 51/64 loss: -0.7862253189086914
Batch 52/64 loss: -0.48569583892822266
Batch 53/64 loss: -0.7778911590576172
Batch 54/64 loss: -0.9044208526611328
Batch 55/64 loss: -0.664459228515625
Batch 56/64 loss: -0.82415771484375
Batch 57/64 loss: -0.8562517166137695
Batch 58/64 loss: -0.7759685516357422
Batch 59/64 loss: -0.3563985824584961
Batch 60/64 loss: -0.8205089569091797
Batch 61/64 loss: 0.263242244720459
Batch 62/64 loss: -0.8753852844238281
Batch 63/64 loss: -1.041213035583496
Batch 64/64 loss: -4.636385917663574
Epoch 315  Train loss: -0.630312463348987  Val loss: -0.7578145581012738
Epoch 316
-------------------------------
Batch 1/64 loss: -0.4165830612182617
Batch 2/64 loss: -0.6263933181762695
Batch 3/64 loss: -0.8734130859375
Batch 4/64 loss: 0.14259910583496094
Batch 5/64 loss: -0.8033885955810547
Batch 6/64 loss: -0.9256925582885742
Batch 7/64 loss: -0.8570795059204102
Batch 8/64 loss: -0.33338069915771484
Batch 9/64 loss: -0.7044305801391602
Batch 10/64 loss: -0.7394380569458008
Batch 11/64 loss: -0.356353759765625
Batch 12/64 loss: -1.0312700271606445
Batch 13/64 loss: -0.5146780014038086
Batch 14/64 loss: -0.5638742446899414
Batch 15/64 loss: -0.7846860885620117
Batch 16/64 loss: -0.9893569946289062
Batch 17/64 loss: -0.7626276016235352
Batch 18/64 loss: -0.7636251449584961
Batch 19/64 loss: -0.5368204116821289
Batch 20/64 loss: -0.7443571090698242
Batch 21/64 loss: -0.6630158424377441
Batch 22/64 loss: -0.8094310760498047
Batch 23/64 loss: -0.6358537673950195
Batch 24/64 loss: -0.7230024337768555
Batch 25/64 loss: -0.7682609558105469
Batch 26/64 loss: -0.95428466796875
Batch 27/64 loss: -0.7944097518920898
Batch 28/64 loss: -0.24889135360717773
Batch 29/64 loss: -0.15500640869140625
Batch 30/64 loss: -1.0328712463378906
Batch 31/64 loss: -0.8067378997802734
Batch 32/64 loss: -0.9184246063232422
Batch 33/64 loss: -0.8605318069458008
Batch 34/64 loss: -0.6261186599731445
Batch 35/64 loss: -0.2224140167236328
Batch 36/64 loss: -1.027724266052246
Batch 37/64 loss: -0.6563549041748047
Batch 38/64 loss: -0.6535539627075195
Batch 39/64 loss: -0.6929693222045898
Batch 40/64 loss: -0.9591560363769531
Batch 41/64 loss: -0.8888053894042969
Batch 42/64 loss: -0.9591903686523438
Batch 43/64 loss: -0.6534490585327148
Batch 44/64 loss: -0.6401214599609375
Batch 45/64 loss: -0.7790775299072266
Batch 46/64 loss: -0.6211328506469727
Batch 47/64 loss: -0.5643730163574219
Batch 48/64 loss: -0.6580638885498047
Batch 49/64 loss: -0.8743925094604492
Batch 50/64 loss: -0.9509477615356445
Batch 51/64 loss: -0.771763801574707
Batch 52/64 loss: -0.7575454711914062
Batch 53/64 loss: -0.6863193511962891
Batch 54/64 loss: -0.8629493713378906
Batch 55/64 loss: -0.7315902709960938
Batch 56/64 loss: -0.30054664611816406
Batch 57/64 loss: -0.5887737274169922
Batch 58/64 loss: -0.44953060150146484
Batch 59/64 loss: -0.57403564453125
Batch 60/64 loss: -0.33513832092285156
Batch 61/64 loss: -0.867405891418457
Batch 62/64 loss: -0.5551376342773438
Batch 63/64 loss: -0.5993738174438477
Batch 64/64 loss: -3.340160369873047
Epoch 316  Train loss: -0.7148023119159773  Val loss: -0.7594356602409861
Epoch 317
-------------------------------
Batch 1/64 loss: -0.8365144729614258
Batch 2/64 loss: -0.6780204772949219
Batch 3/64 loss: -0.46603965759277344
Batch 4/64 loss: -0.906031608581543
Batch 5/64 loss: -0.7677488327026367
Batch 6/64 loss: -0.6768693923950195
Batch 7/64 loss: -0.3832235336303711
Batch 8/64 loss: -0.7730331420898438
Batch 9/64 loss: -0.7212820053100586
Batch 10/64 loss: -0.43310070037841797
Batch 11/64 loss: -0.8151702880859375
Batch 12/64 loss: -0.7607517242431641
Batch 13/64 loss: -0.9260759353637695
Batch 14/64 loss: -0.9789943695068359
Batch 15/64 loss: -0.2859182357788086
Batch 16/64 loss: -0.8103494644165039
Batch 17/64 loss: -1.0581903457641602
Batch 18/64 loss: -0.7376804351806641
Batch 19/64 loss: -0.5396585464477539
Batch 20/64 loss: -0.9834508895874023
Batch 21/64 loss: -0.6873636245727539
Batch 22/64 loss: -0.7626276016235352
Batch 23/64 loss: -0.8953561782836914
Batch 24/64 loss: -1.1185226440429688
Batch 25/64 loss: -1.0285511016845703
Batch 26/64 loss: -1.0366687774658203
Batch 27/64 loss: -0.8014364242553711
Batch 28/64 loss: -0.17756175994873047
Batch 29/64 loss: -0.6677885055541992
Batch 30/64 loss: -0.7185182571411133
Batch 31/64 loss: -0.8234462738037109
Batch 32/64 loss: -0.5918312072753906
Batch 33/64 loss: -0.7064895629882812
Batch 34/64 loss: -0.9813852310180664
Batch 35/64 loss: -0.6506328582763672
Batch 36/64 loss: -0.8158378601074219
Batch 37/64 loss: -0.2074270248413086
Batch 38/64 loss: -0.8068437576293945
Batch 39/64 loss: -0.7007274627685547
Batch 40/64 loss: -0.41673851013183594
Batch 41/64 loss: -0.6402063369750977
Batch 42/64 loss: -0.3989400863647461
Batch 43/64 loss: -0.9158992767333984
Batch 44/64 loss: -0.8714485168457031
Batch 45/64 loss: -0.1659073829650879
Batch 46/64 loss: -0.4694242477416992
Batch 47/64 loss: -0.34178733825683594
Batch 48/64 loss: -0.16772747039794922
Batch 49/64 loss: -0.7670249938964844
Batch 50/64 loss: -0.7600173950195312
Batch 51/64 loss: -0.48481273651123047
Batch 52/64 loss: -0.856928825378418
Batch 53/64 loss: -0.7066450119018555
Batch 54/64 loss: 0.10889911651611328
Batch 55/64 loss: -0.8475685119628906
Batch 56/64 loss: -0.705965518951416
Batch 57/64 loss: -0.6096301078796387
Batch 58/64 loss: -0.4589853286743164
Batch 59/64 loss: -0.3820667266845703
Batch 60/64 loss: -0.7781105041503906
Batch 61/64 loss: -0.7359914779663086
Batch 62/64 loss: -0.5397129058837891
Batch 63/64 loss: -0.3969087600708008
Batch 64/64 loss: -4.504024505615234
Epoch 317  Train loss: -0.712230390660903  Val loss: -0.6552720610628423
Epoch 318
-------------------------------
Batch 1/64 loss: -0.6059198379516602
Batch 2/64 loss: -1.1261844635009766
Batch 3/64 loss: -0.8871498107910156
Batch 4/64 loss: -0.6412925720214844
Batch 5/64 loss: -0.7102518081665039
Batch 6/64 loss: -0.43624210357666016
Batch 7/64 loss: -0.6745214462280273
Batch 8/64 loss: -0.30136775970458984
Batch 9/64 loss: -0.14387893676757812
Batch 10/64 loss: -0.6496467590332031
Batch 11/64 loss: -0.8026943206787109
Batch 12/64 loss: -0.4029884338378906
Batch 13/64 loss: -0.8514680862426758
Batch 14/64 loss: -0.8365240097045898
Batch 15/64 loss: -0.7211723327636719
Batch 16/64 loss: -0.7833318710327148
Batch 17/64 loss: -0.7270936965942383
Batch 18/64 loss: -0.8711528778076172
Batch 19/64 loss: -0.6412496566772461
Batch 20/64 loss: -0.5380878448486328
Batch 21/64 loss: -0.6133670806884766
Batch 22/64 loss: -0.7140417098999023
Batch 23/64 loss: -0.6258716583251953
Batch 24/64 loss: -0.5013837814331055
Batch 25/64 loss: -0.7880916595458984
Batch 26/64 loss: -0.5918493270874023
Batch 27/64 loss: -0.9744319915771484
Batch 28/64 loss: -0.4141974449157715
Batch 29/64 loss: -0.3950638771057129
Batch 30/64 loss: -0.7781734466552734
Batch 31/64 loss: -0.8397388458251953
Batch 32/64 loss: -0.23290586471557617
Batch 33/64 loss: -0.45525217056274414
Batch 34/64 loss: -0.9575080871582031
Batch 35/64 loss: -0.9276609420776367
Batch 36/64 loss: -0.7987051010131836
Batch 37/64 loss: -0.8371210098266602
Batch 38/64 loss: -0.6836929321289062
Batch 39/64 loss: -0.4321174621582031
Batch 40/64 loss: -0.6089611053466797
Batch 41/64 loss: -0.9108123779296875
Batch 42/64 loss: -0.6651821136474609
Batch 43/64 loss: -0.7287454605102539
Batch 44/64 loss: -0.5982522964477539
Batch 45/64 loss: -0.8720359802246094
Batch 46/64 loss: -0.9938983917236328
Batch 47/64 loss: -0.5605239868164062
Batch 48/64 loss: -0.7226676940917969
Batch 49/64 loss: -0.5721664428710938
Batch 50/64 loss: -0.8250942230224609
Batch 51/64 loss: -0.6937675476074219
Batch 52/64 loss: -0.7157831192016602
Batch 53/64 loss: -0.5874404907226562
Batch 54/64 loss: -0.9883613586425781
Batch 55/64 loss: -0.25443077087402344
Batch 56/64 loss: -0.8950481414794922
Batch 57/64 loss: -0.7512245178222656
Batch 58/64 loss: -0.7417621612548828
Batch 59/64 loss: -0.6229171752929688
Batch 60/64 loss: -0.8711013793945312
Batch 61/64 loss: -0.3355226516723633
Batch 62/64 loss: -0.2082347869873047
Batch 63/64 loss: -0.7698602676391602
Batch 64/64 loss: -4.823755264282227
Epoch 318  Train loss: -0.7219608456480737  Val loss: -0.8543684392450601
Epoch 319
-------------------------------
Batch 1/64 loss: -0.9148521423339844
Batch 2/64 loss: -0.8199663162231445
Batch 3/64 loss: -0.7344436645507812
Batch 4/64 loss: -0.8455209732055664
Batch 5/64 loss: -0.8259057998657227
Batch 6/64 loss: -0.6309223175048828
Batch 7/64 loss: -0.9242954254150391
Batch 8/64 loss: -0.04918670654296875
Batch 9/64 loss: -0.7797079086303711
Batch 10/64 loss: -0.5502405166625977
Batch 11/64 loss: -0.3006162643432617
Batch 12/64 loss: -0.8512201309204102
Batch 13/64 loss: -1.0738105773925781
Batch 14/64 loss: -0.945134162902832
Batch 15/64 loss: -0.5403451919555664
Batch 16/64 loss: -0.5746254920959473
Batch 17/64 loss: -0.4640045166015625
Batch 18/64 loss: -0.9300508499145508
Batch 19/64 loss: -0.9206142425537109
Batch 20/64 loss: -0.5127191543579102
Batch 21/64 loss: -0.5101804733276367
Batch 22/64 loss: -0.42007923126220703
Batch 23/64 loss: -0.5467596054077148
Batch 24/64 loss: -0.9869556427001953
Batch 25/64 loss: -0.838353157043457
Batch 26/64 loss: -0.6284980773925781
Batch 27/64 loss: -0.6740131378173828
Batch 28/64 loss: -0.5667085647583008
Batch 29/64 loss: -0.6013669967651367
Batch 30/64 loss: -0.4282217025756836
Batch 31/64 loss: -0.7932720184326172
Batch 32/64 loss: -0.4993267059326172
Batch 33/64 loss: -0.630579948425293
Batch 34/64 loss: -0.2843818664550781
Batch 35/64 loss: -0.3508009910583496
Batch 36/64 loss: -0.7390813827514648
Batch 37/64 loss: -0.48932456970214844
Batch 38/64 loss: -0.7190980911254883
Batch 39/64 loss: -0.9577484130859375
Batch 40/64 loss: -0.6208925247192383
Batch 41/64 loss: -0.4270334243774414
Batch 42/64 loss: -0.2493147850036621
Batch 43/64 loss: -0.6708974838256836
Batch 44/64 loss: -0.5023951530456543
Batch 45/64 loss: -0.6172161102294922
Batch 46/64 loss: -0.6155519485473633
Batch 47/64 loss: -0.5282993316650391
Batch 48/64 loss: -0.4587364196777344
Batch 49/64 loss: -0.7919216156005859
Batch 50/64 loss: -0.7769231796264648
Batch 51/64 loss: -0.7548360824584961
Batch 52/64 loss: -0.7086820602416992
Batch 53/64 loss: -0.6079902648925781
Batch 54/64 loss: -0.6582174301147461
Batch 55/64 loss: -0.6956748962402344
Batch 56/64 loss: -0.9921894073486328
Batch 57/64 loss: -0.5920143127441406
Batch 58/64 loss: -0.8149747848510742
Batch 59/64 loss: -0.7182960510253906
Batch 60/64 loss: -0.4033041000366211
Batch 61/64 loss: -0.7653951644897461
Batch 62/64 loss: -0.41086769104003906
Batch 63/64 loss: -0.5112628936767578
Batch 64/64 loss: -4.8846330642700195
Epoch 319  Train loss: -0.6966163897046855  Val loss: -0.7105812518457367
Epoch 320
-------------------------------
Batch 1/64 loss: -0.6439924240112305
Batch 2/64 loss: -0.12225151062011719
Batch 3/64 loss: -0.6707839965820312
Batch 4/64 loss: -0.4404621124267578
Batch 5/64 loss: -0.449798583984375
Batch 6/64 loss: -0.7267532348632812
Batch 7/64 loss: -0.6435728073120117
Batch 8/64 loss: -0.8030061721801758
Batch 9/64 loss: -0.41583728790283203
Batch 10/64 loss: -0.8190927505493164
Batch 11/64 loss: -0.1298689842224121
Batch 12/64 loss: -0.7121353149414062
Batch 13/64 loss: -0.9316835403442383
Batch 14/64 loss: 0.20163249969482422
Batch 15/64 loss: -0.7617397308349609
Batch 16/64 loss: -0.7342414855957031
Batch 17/64 loss: -0.7334079742431641
Batch 18/64 loss: -0.7976560592651367
Batch 19/64 loss: -1.0836191177368164
Batch 20/64 loss: -1.0661582946777344
Batch 21/64 loss: -0.8767728805541992
Batch 22/64 loss: -0.6563091278076172
Batch 23/64 loss: -0.7633934020996094
Batch 24/64 loss: -1.1361112594604492
Batch 25/64 loss: -0.7442970275878906
Batch 26/64 loss: -0.5614519119262695
Batch 27/64 loss: -0.3837137222290039
Batch 28/64 loss: -0.4116849899291992
Batch 29/64 loss: -1.0495777130126953
Batch 30/64 loss: -0.7327461242675781
Batch 31/64 loss: -0.9490756988525391
Batch 32/64 loss: -0.5722551345825195
Batch 33/64 loss: -0.8846769332885742
Batch 34/64 loss: -1.092991828918457
Batch 35/64 loss: -0.7628536224365234
Batch 36/64 loss: -0.7626628875732422
Batch 37/64 loss: -0.8514022827148438
Batch 38/64 loss: -0.6684494018554688
Batch 39/64 loss: -0.6838531494140625
Batch 40/64 loss: -0.5071172714233398
Batch 41/64 loss: -0.5383005142211914
Batch 42/64 loss: -0.7696437835693359
Batch 43/64 loss: -0.5374975204467773
Batch 44/64 loss: -1.1008081436157227
Batch 45/64 loss: -0.640655517578125
Batch 46/64 loss: -1.117445945739746
Batch 47/64 loss: -0.4895296096801758
Batch 48/64 loss: -0.6549959182739258
Batch 49/64 loss: -0.9189376831054688
Batch 50/64 loss: 0.018079757690429688
Batch 51/64 loss: -0.12594223022460938
Batch 52/64 loss: -0.8046550750732422
Batch 53/64 loss: -0.7899017333984375
Batch 54/64 loss: -0.5385303497314453
Batch 55/64 loss: -0.8660154342651367
Batch 56/64 loss: -0.7092504501342773
Batch 57/64 loss: -0.7652769088745117
Batch 58/64 loss: -0.5916557312011719
Batch 59/64 loss: -0.6467828750610352
Batch 60/64 loss: -0.4054527282714844
Batch 61/64 loss: -0.8105754852294922
Batch 62/64 loss: -0.902491569519043
Batch 63/64 loss: -0.22591304779052734
Batch 64/64 loss: -4.737751483917236
Epoch 320  Train loss: -0.7219030174554563  Val loss: -0.7901442091899229
Epoch 321
-------------------------------
Batch 1/64 loss: -0.7350406646728516
Batch 2/64 loss: -0.9922685623168945
Batch 3/64 loss: -0.6803865432739258
Batch 4/64 loss: -0.8649311065673828
Batch 5/64 loss: -0.3813972473144531
Batch 6/64 loss: -0.5433864593505859
Batch 7/64 loss: -0.6520929336547852
Batch 8/64 loss: -0.6238822937011719
Batch 9/64 loss: -0.6199359893798828
Batch 10/64 loss: -0.7209482192993164
Batch 11/64 loss: -0.6009855270385742
Batch 12/64 loss: -0.729522705078125
Batch 13/64 loss: -0.6554145812988281
Batch 14/64 loss: -0.8701639175415039
Batch 15/64 loss: 0.10589790344238281
Batch 16/64 loss: -0.44518089294433594
Batch 17/64 loss: -0.7439746856689453
Batch 18/64 loss: -0.5518369674682617
Batch 19/64 loss: -0.6896429061889648
Batch 20/64 loss: -0.6328678131103516
Batch 21/64 loss: -0.9203386306762695
Batch 22/64 loss: -0.5021352767944336
Batch 23/64 loss: -0.6901817321777344
Batch 24/64 loss: -0.4723324775695801
Batch 25/64 loss: -0.8264894485473633
Batch 26/64 loss: -0.7857370376586914
Batch 27/64 loss: -0.6686973571777344
Batch 28/64 loss: -0.6926984786987305
Batch 29/64 loss: -1.0416221618652344
Batch 30/64 loss: -0.9420070648193359
Batch 31/64 loss: -0.6706695556640625
Batch 32/64 loss: -0.8423318862915039
Batch 33/64 loss: -0.4499807357788086
Batch 34/64 loss: -0.3901796340942383
Batch 35/64 loss: -0.6110172271728516
Batch 36/64 loss: -0.7210445404052734
Batch 37/64 loss: -0.8634033203125
Batch 38/64 loss: -0.5399751663208008
Batch 39/64 loss: -0.23128175735473633
Batch 40/64 loss: -0.37456226348876953
Batch 41/64 loss: -0.6281232833862305
Batch 42/64 loss: -0.8372516632080078
Batch 43/64 loss: -0.8517007827758789
Batch 44/64 loss: -0.839935302734375
Batch 45/64 loss: -0.6146841049194336
Batch 46/64 loss: -0.7472963333129883
Batch 47/64 loss: -0.8665523529052734
Batch 48/64 loss: -0.6914958953857422
Batch 49/64 loss: -0.7045402526855469
Batch 50/64 loss: -0.9755859375
Batch 51/64 loss: -0.6814155578613281
Batch 52/64 loss: -0.9793624877929688
Batch 53/64 loss: -0.993922233581543
Batch 54/64 loss: -0.7301368713378906
Batch 55/64 loss: -0.9352169036865234
Batch 56/64 loss: -0.3850827217102051
Batch 57/64 loss: -0.6839694976806641
Batch 58/64 loss: -0.30493831634521484
Batch 59/64 loss: -0.5941781997680664
Batch 60/64 loss: -0.7112617492675781
Batch 61/64 loss: -0.5446672439575195
Batch 62/64 loss: -0.7862863540649414
Batch 63/64 loss: -0.8887844085693359
Batch 64/64 loss: -3.184983253479004
Epoch 321  Train loss: -0.7094866472132065  Val loss: -0.8469057378080702
Epoch 322
-------------------------------
Batch 1/64 loss: -1.11846923828125
Batch 2/64 loss: -0.8480281829833984
Batch 3/64 loss: -0.5645217895507812
Batch 4/64 loss: -0.6492948532104492
Batch 5/64 loss: -0.32923412322998047
Batch 6/64 loss: -0.6257181167602539
Batch 7/64 loss: -0.7087678909301758
Batch 8/64 loss: -1.0003728866577148
Batch 9/64 loss: -0.7676897048950195
Batch 10/64 loss: -0.7553691864013672
Batch 11/64 loss: -0.9102964401245117
Batch 12/64 loss: -0.985896110534668
Batch 13/64 loss: -0.5070533752441406
Batch 14/64 loss: -0.7254219055175781
Batch 15/64 loss: -0.7350149154663086
Batch 16/64 loss: -0.9323291778564453
Batch 17/64 loss: -0.8541288375854492
Batch 18/64 loss: -0.5906791687011719
Batch 19/64 loss: -0.4096488952636719
Batch 20/64 loss: -0.7720108032226562
Batch 21/64 loss: -0.5943479537963867
Batch 22/64 loss: -0.5236568450927734
Batch 23/64 loss: -0.6080837249755859
Batch 24/64 loss: -0.13583946228027344
Batch 25/64 loss: -0.13028812408447266
Batch 26/64 loss: -0.4043111801147461
Batch 27/64 loss: -0.6896495819091797
Batch 28/64 loss: 0.09335088729858398
Batch 29/64 loss: -0.22178077697753906
Batch 30/64 loss: -0.5673942565917969
Batch 31/64 loss: -0.33534717559814453
Batch 32/64 loss: -0.7156858444213867
Batch 33/64 loss: -0.8609552383422852
Batch 34/64 loss: -0.25965309143066406
Batch 35/64 loss: -0.3439922332763672
Batch 36/64 loss: -0.6440391540527344
Batch 37/64 loss: -0.960418701171875
Batch 38/64 loss: -0.6074123382568359
Batch 39/64 loss: -0.20551586151123047
Batch 40/64 loss: -0.47965335845947266
Batch 41/64 loss: -0.2060689926147461
Batch 42/64 loss: -0.9461479187011719
Batch 43/64 loss: -0.30493736267089844
Batch 44/64 loss: -0.7780027389526367
Batch 45/64 loss: -0.6166925430297852
Batch 46/64 loss: -0.6823101043701172
Batch 47/64 loss: -0.9423761367797852
Batch 48/64 loss: -0.7580986022949219
Batch 49/64 loss: -0.26224422454833984
Batch 50/64 loss: -0.1795358657836914
Batch 51/64 loss: -0.7954549789428711
Batch 52/64 loss: -0.5841445922851562
Batch 53/64 loss: -0.9129886627197266
Batch 54/64 loss: -0.2052755355834961
Batch 55/64 loss: -0.6391887664794922
Batch 56/64 loss: -0.7521085739135742
Batch 57/64 loss: -0.8575811386108398
Batch 58/64 loss: -0.6126823425292969
Batch 59/64 loss: -0.7431240081787109
Batch 60/64 loss: -0.7602062225341797
Batch 61/64 loss: -0.5509700775146484
Batch 62/64 loss: -0.678166389465332
Batch 63/64 loss: -0.1945796012878418
Batch 64/64 loss: -4.631621360778809
Epoch 322  Train loss: -0.6497446434170592  Val loss: -0.5344606773140504
Epoch 323
-------------------------------
Batch 1/64 loss: -0.5915336608886719
Batch 2/64 loss: -0.48296546936035156
Batch 3/64 loss: -0.9012727737426758
Batch 4/64 loss: -0.5221624374389648
Batch 5/64 loss: -0.5026493072509766
Batch 6/64 loss: -0.6408958435058594
Batch 7/64 loss: -0.4940004348754883
Batch 8/64 loss: -0.7316312789916992
Batch 9/64 loss: -0.6601228713989258
Batch 10/64 loss: -0.597686767578125
Batch 11/64 loss: -0.5874996185302734
Batch 12/64 loss: -0.32233285903930664
Batch 13/64 loss: -0.38403987884521484
Batch 14/64 loss: -0.4848823547363281
Batch 15/64 loss: -0.5379667282104492
Batch 16/64 loss: -0.8067073822021484
Batch 17/64 loss: -0.22365283966064453
Batch 18/64 loss: -0.015189170837402344
Batch 19/64 loss: -0.9358272552490234
Batch 20/64 loss: -0.6531600952148438
Batch 21/64 loss: -0.7179470062255859
Batch 22/64 loss: -0.8566455841064453
Batch 23/64 loss: -0.42348575592041016
Batch 24/64 loss: -0.6064109802246094
Batch 25/64 loss: -0.716984748840332
Batch 26/64 loss: -0.7571067810058594
Batch 27/64 loss: -0.8646278381347656
Batch 28/64 loss: -0.5862197875976562
Batch 29/64 loss: -0.8557548522949219
Batch 30/64 loss: -0.5685148239135742
Batch 31/64 loss: -0.5688343048095703
Batch 32/64 loss: -0.3469352722167969
Batch 33/64 loss: -0.7536945343017578
Batch 34/64 loss: -0.7590751647949219
Batch 35/64 loss: -0.6524333953857422
Batch 36/64 loss: -0.6627578735351562
Batch 37/64 loss: -0.8151578903198242
Batch 38/64 loss: -0.7790689468383789
Batch 39/64 loss: -0.2529258728027344
Batch 40/64 loss: -0.9275274276733398
Batch 41/64 loss: -0.5273818969726562
Batch 42/64 loss: -0.38816022872924805
Batch 43/64 loss: -0.8041896820068359
Batch 44/64 loss: -0.9574098587036133
Batch 45/64 loss: -0.45050716400146484
Batch 46/64 loss: -0.2050333023071289
Batch 47/64 loss: -0.9306135177612305
Batch 48/64 loss: -0.5560216903686523
Batch 49/64 loss: -0.26465463638305664
Batch 50/64 loss: -0.9991989135742188
Batch 51/64 loss: -0.7002391815185547
Batch 52/64 loss: -0.3612079620361328
Batch 53/64 loss: -0.5299844741821289
Batch 54/64 loss: -0.9356489181518555
Batch 55/64 loss: -0.7393722534179688
Batch 56/64 loss: -0.7680387496948242
Batch 57/64 loss: -0.7681903839111328
Batch 58/64 loss: -0.6757831573486328
Batch 59/64 loss: -0.4525737762451172
Batch 60/64 loss: -0.6376066207885742
Batch 61/64 loss: -0.8801813125610352
Batch 62/64 loss: -0.4406118392944336
Batch 63/64 loss: -0.8744649887084961
Batch 64/64 loss: -4.702882289886475
Epoch 323  Train loss: -0.6732944956012801  Val loss: -0.7911388751157781
Epoch 324
-------------------------------
Batch 1/64 loss: -1.1493635177612305
Batch 2/64 loss: -0.1748676300048828
Batch 3/64 loss: -0.8503170013427734
Batch 4/64 loss: -0.6091547012329102
Batch 5/64 loss: -0.983363151550293
Batch 6/64 loss: -0.48207950592041016
Batch 7/64 loss: -0.8820581436157227
Batch 8/64 loss: -0.8481588363647461
Batch 9/64 loss: -0.37702178955078125
Batch 10/64 loss: -0.3199729919433594
Batch 11/64 loss: -0.7762355804443359
Batch 12/64 loss: -0.15532732009887695
Batch 13/64 loss: -0.7777442932128906
Batch 14/64 loss: -0.09421253204345703
Batch 15/64 loss: -0.7454462051391602
Batch 16/64 loss: -0.9764719009399414
Batch 17/64 loss: -0.688847541809082
Batch 18/64 loss: -0.582427978515625
Batch 19/64 loss: -1.1243820190429688
Batch 20/64 loss: -0.32995033264160156
Batch 21/64 loss: -0.44911861419677734
Batch 22/64 loss: -0.8411111831665039
Batch 23/64 loss: -0.6243000030517578
Batch 24/64 loss: -0.4935140609741211
Batch 25/64 loss: -0.6473150253295898
Batch 26/64 loss: -0.81927490234375
Batch 27/64 loss: -0.27018260955810547
Batch 28/64 loss: 0.3106346130371094
Batch 29/64 loss: -1.1068124771118164
Batch 30/64 loss: -0.6233339309692383
Batch 31/64 loss: -0.42136192321777344
Batch 32/64 loss: -0.5114870071411133
Batch 33/64 loss: -0.9326143264770508
Batch 34/64 loss: -0.5431060791015625
Batch 35/64 loss: -0.5404043197631836
Batch 36/64 loss: -0.5954599380493164
Batch 37/64 loss: -0.7936878204345703
Batch 38/64 loss: -0.7193374633789062
Batch 39/64 loss: -1.0684614181518555
Batch 40/64 loss: -0.8914871215820312
Batch 41/64 loss: -0.37709617614746094
Batch 42/64 loss: -0.6894598007202148
Batch 43/64 loss: -0.5311803817749023
Batch 44/64 loss: -0.9222555160522461
Batch 45/64 loss: -0.9409418106079102
Batch 46/64 loss: -0.5110616683959961
Batch 47/64 loss: -0.8047599792480469
Batch 48/64 loss: 0.01742267608642578
Batch 49/64 loss: -1.106496810913086
Batch 50/64 loss: -0.5581321716308594
Batch 51/64 loss: -0.6718502044677734
Batch 52/64 loss: -0.5879755020141602
Batch 53/64 loss: -0.4919614791870117
Batch 54/64 loss: -0.45366668701171875
Batch 55/64 loss: -0.9861288070678711
Batch 56/64 loss: -0.7483024597167969
Batch 57/64 loss: -1.0041322708129883
Batch 58/64 loss: -0.6412172317504883
Batch 59/64 loss: -0.03687238693237305
Batch 60/64 loss: -0.4465160369873047
Batch 61/64 loss: -0.6381759643554688
Batch 62/64 loss: -0.6106252670288086
Batch 63/64 loss: -0.5841503143310547
Batch 64/64 loss: -4.474793434143066
Epoch 324  Train loss: -0.6775022731107824  Val loss: -0.7416560379500242
Epoch 325
-------------------------------
Batch 1/64 loss: -0.631843090057373
Batch 2/64 loss: -0.24596214294433594
Batch 3/64 loss: -0.9502506256103516
Batch 4/64 loss: -0.6131629943847656
Batch 5/64 loss: -0.7383556365966797
Batch 6/64 loss: -0.7717447280883789
Batch 7/64 loss: -0.6377754211425781
Batch 8/64 loss: -0.7794189453125
Batch 9/64 loss: 0.2479076385498047
Batch 10/64 loss: -0.49074840545654297
Batch 11/64 loss: -0.5138864517211914
Batch 12/64 loss: -0.7403602600097656
Batch 13/64 loss: -0.6341285705566406
Batch 14/64 loss: -0.6803121566772461
Batch 15/64 loss: -0.6727828979492188
Batch 16/64 loss: -0.5167922973632812
Batch 17/64 loss: -0.5062732696533203
Batch 18/64 loss: -0.39746856689453125
Batch 19/64 loss: -0.7772407531738281
Batch 20/64 loss: -0.29971790313720703
Batch 21/64 loss: -0.6737117767333984
Batch 22/64 loss: -0.3668231964111328
Batch 23/64 loss: -0.8864173889160156
Batch 24/64 loss: -0.8430509567260742
Batch 25/64 loss: -0.2107076644897461
Batch 26/64 loss: -0.7046136856079102
Batch 27/64 loss: -0.6487216949462891
Batch 28/64 loss: -0.07534122467041016
Batch 29/64 loss: -0.780609130859375
Batch 30/64 loss: -0.7114925384521484
Batch 31/64 loss: -0.7561311721801758
Batch 32/64 loss: -0.32933902740478516
Batch 33/64 loss: 0.7579512596130371
Batch 34/64 loss: -0.7526588439941406
Batch 35/64 loss: -0.3327341079711914
Batch 36/64 loss: -0.4324522018432617
Batch 37/64 loss: -0.6156911849975586
Batch 38/64 loss: -0.8988895416259766
Batch 39/64 loss: -0.8548688888549805
Batch 40/64 loss: -0.41335296630859375
Batch 41/64 loss: -1.0671882629394531
Batch 42/64 loss: -0.6623706817626953
Batch 43/64 loss: -0.2146434783935547
Batch 44/64 loss: -0.7540254592895508
Batch 45/64 loss: -0.8156471252441406
Batch 46/64 loss: -0.5253744125366211
Batch 47/64 loss: -0.8958683013916016
Batch 48/64 loss: -0.6006746292114258
Batch 49/64 loss: -0.9566526412963867
Batch 50/64 loss: -0.4052619934082031
Batch 51/64 loss: -0.2657604217529297
Batch 52/64 loss: -0.9530439376831055
Batch 53/64 loss: -0.9361000061035156
Batch 54/64 loss: -0.33359718322753906
Batch 55/64 loss: -0.7973537445068359
Batch 56/64 loss: -0.954559326171875
Batch 57/64 loss: -0.6525554656982422
Batch 58/64 loss: -1.1116304397583008
Batch 59/64 loss: -0.7597799301147461
Batch 60/64 loss: -0.5444736480712891
Batch 61/64 loss: -1.0014448165893555
Batch 62/64 loss: -0.8447713851928711
Batch 63/64 loss: -0.5620937347412109
Batch 64/64 loss: -4.698465347290039
Epoch 325  Train loss: -0.6591167599547143  Val loss: -0.8058351077574635
Epoch 326
-------------------------------
Batch 1/64 loss: -0.7312135696411133
Batch 2/64 loss: -0.8822650909423828
Batch 3/64 loss: -0.9589300155639648
Batch 4/64 loss: -0.1342315673828125
Batch 5/64 loss: -0.28876686096191406
Batch 6/64 loss: -1.0651779174804688
Batch 7/64 loss: -0.6190252304077148
Batch 8/64 loss: -0.6593217849731445
Batch 9/64 loss: -0.818882942199707
Batch 10/64 loss: -0.886296272277832
Batch 11/64 loss: -0.3806304931640625
Batch 12/64 loss: -0.27835607528686523
Batch 13/64 loss: -0.686253547668457
Batch 14/64 loss: -0.5440044403076172
Batch 15/64 loss: -0.7723121643066406
Batch 16/64 loss: -0.6550626754760742
Batch 17/64 loss: -0.547917366027832
Batch 18/64 loss: -0.45315980911254883
Batch 19/64 loss: -0.43177223205566406
Batch 20/64 loss: -0.4129638671875
Batch 21/64 loss: -0.48421669006347656
Batch 22/64 loss: -0.6196708679199219
Batch 23/64 loss: -0.6653170585632324
Batch 24/64 loss: -0.6321601867675781
Batch 25/64 loss: -0.8175945281982422
Batch 26/64 loss: -0.3851203918457031
Batch 27/64 loss: -0.621068000793457
Batch 28/64 loss: -0.4558677673339844
Batch 29/64 loss: -0.408935546875
Batch 30/64 loss: -0.972503662109375
Batch 31/64 loss: -0.6304059028625488
Batch 32/64 loss: -0.6262950897216797
Batch 33/64 loss: -0.9545745849609375
Batch 34/64 loss: -0.6648111343383789
Batch 35/64 loss: -0.3808460235595703
Batch 36/64 loss: -0.6070518493652344
Batch 37/64 loss: -0.4987220764160156
Batch 38/64 loss: -0.7071743011474609
Batch 39/64 loss: -0.6373367309570312
Batch 40/64 loss: -0.6345739364624023
Batch 41/64 loss: -0.7062473297119141
Batch 42/64 loss: -1.0398178100585938
Batch 43/64 loss: -0.43522119522094727
Batch 44/64 loss: -0.6440668106079102
Batch 45/64 loss: -0.8557653427124023
Batch 46/64 loss: -0.5646133422851562
Batch 47/64 loss: -0.38861656188964844
Batch 48/64 loss: -0.8015508651733398
Batch 49/64 loss: -0.6699838638305664
Batch 50/64 loss: -0.9215631484985352
Batch 51/64 loss: -0.4506950378417969
Batch 52/64 loss: -0.6297340393066406
Batch 53/64 loss: -0.6520633697509766
Batch 54/64 loss: -0.4249687194824219
Batch 55/64 loss: -1.0717277526855469
Batch 56/64 loss: -0.2780485153198242
Batch 57/64 loss: -0.34536266326904297
Batch 58/64 loss: -0.6582517623901367
Batch 59/64 loss: -0.6838369369506836
Batch 60/64 loss: -0.5609521865844727
Batch 61/64 loss: 0.026834487915039062
Batch 62/64 loss: -0.4707784652709961
Batch 63/64 loss: -0.6816549301147461
Batch 64/64 loss: -4.487462997436523
Epoch 326  Train loss: -0.6570207633224188  Val loss: -0.757324179423224
Epoch 327
-------------------------------
Batch 1/64 loss: -0.7924938201904297
Batch 2/64 loss: -0.728663444519043
Batch 3/64 loss: -0.4703693389892578
Batch 4/64 loss: -0.6149740219116211
Batch 5/64 loss: -0.5823974609375
Batch 6/64 loss: -0.5957565307617188
Batch 7/64 loss: -0.20583152770996094
Batch 8/64 loss: -0.6993789672851562
Batch 9/64 loss: -0.5890712738037109
Batch 10/64 loss: -0.5736494064331055
Batch 11/64 loss: -0.8895940780639648
Batch 12/64 loss: -0.8490028381347656
Batch 13/64 loss: -0.5056166648864746
Batch 14/64 loss: -0.7328128814697266
Batch 15/64 loss: -0.588740348815918
Batch 16/64 loss: -0.43372344970703125
Batch 17/64 loss: -0.6696691513061523
Batch 18/64 loss: -0.7491254806518555
Batch 19/64 loss: -0.3034839630126953
Batch 20/64 loss: -0.6090106964111328
Batch 21/64 loss: -0.7186546325683594
Batch 22/64 loss: -0.8486709594726562
Batch 23/64 loss: -0.5252656936645508
Batch 24/64 loss: -0.8194789886474609
Batch 25/64 loss: -0.9368839263916016
Batch 26/64 loss: 0.3787245750427246
Batch 27/64 loss: -0.46058225631713867
Batch 28/64 loss: -0.5040073394775391
Batch 29/64 loss: -0.4333024024963379
Batch 30/64 loss: -0.3762502670288086
Batch 31/64 loss: -0.48700571060180664
Batch 32/64 loss: -0.7567930221557617
Batch 33/64 loss: -0.786463737487793
Batch 34/64 loss: -0.4860672950744629
Batch 35/64 loss: -0.7563018798828125
Batch 36/64 loss: -0.3712482452392578
Batch 37/64 loss: -0.9821290969848633
Batch 38/64 loss: -0.9723043441772461
Batch 39/64 loss: 0.33954381942749023
Batch 40/64 loss: -0.8105173110961914
Batch 41/64 loss: -0.5565838813781738
Batch 42/64 loss: -0.9922561645507812
Batch 43/64 loss: -0.8056488037109375
Batch 44/64 loss: -0.8573169708251953
Batch 45/64 loss: -0.4053840637207031
Batch 46/64 loss: -0.7108469009399414
Batch 47/64 loss: -0.5009174346923828
Batch 48/64 loss: -0.8935842514038086
Batch 49/64 loss: -0.9137687683105469
Batch 50/64 loss: -0.8636150360107422
Batch 51/64 loss: -0.774683952331543
Batch 52/64 loss: -0.5997028350830078
Batch 53/64 loss: -0.5794620513916016
Batch 54/64 loss: -0.6491079330444336
Batch 55/64 loss: -0.6685404777526855
Batch 56/64 loss: -0.5500564575195312
Batch 57/64 loss: -0.6065006256103516
Batch 58/64 loss: -0.7601680755615234
Batch 59/64 loss: -0.5187482833862305
Batch 60/64 loss: -0.8403692245483398
Batch 61/64 loss: -0.6822018623352051
Batch 62/64 loss: -0.8447408676147461
Batch 63/64 loss: -0.1559739112854004
Batch 64/64 loss: -4.446786880493164
Epoch 327  Train loss: -0.6676438125909544  Val loss: -0.7556168860995892
Epoch 328
-------------------------------
Batch 1/64 loss: -0.506739616394043
Batch 2/64 loss: -0.8202190399169922
Batch 3/64 loss: -0.5145769119262695
Batch 4/64 loss: -0.7333712577819824
Batch 5/64 loss: -0.6046857833862305
Batch 6/64 loss: -0.7207245826721191
Batch 7/64 loss: -1.017634391784668
Batch 8/64 loss: -0.6546611785888672
Batch 9/64 loss: -0.5558629035949707
Batch 10/64 loss: -0.6935086250305176
Batch 11/64 loss: -0.7139291763305664
Batch 12/64 loss: -0.575103759765625
Batch 13/64 loss: -0.8903694152832031
Batch 14/64 loss: -0.5133466720581055
Batch 15/64 loss: -0.5491037368774414
Batch 16/64 loss: -0.024529457092285156
Batch 17/64 loss: -0.6161212921142578
Batch 18/64 loss: -0.7970876693725586
Batch 19/64 loss: -0.812525749206543
Batch 20/64 loss: -0.9050922393798828
Batch 21/64 loss: -0.8014106750488281
Batch 22/64 loss: -0.6796083450317383
Batch 23/64 loss: -0.8343486785888672
Batch 24/64 loss: -0.5731782913208008
Batch 25/64 loss: -0.9588594436645508
Batch 26/64 loss: -0.9213476181030273
Batch 27/64 loss: -0.5774011611938477
Batch 28/64 loss: -0.8150482177734375
Batch 29/64 loss: -0.9244136810302734
Batch 30/64 loss: -0.78765869140625
Batch 31/64 loss: -0.480074405670166
Batch 32/64 loss: -0.2734079360961914
Batch 33/64 loss: -0.4490318298339844
Batch 34/64 loss: -0.46199798583984375
Batch 35/64 loss: -0.7492341995239258
Batch 36/64 loss: -0.9088058471679688
Batch 37/64 loss: -0.8445863723754883
Batch 38/64 loss: -0.4967374801635742
Batch 39/64 loss: -0.5070829391479492
Batch 40/64 loss: -0.603053092956543
Batch 41/64 loss: -0.6971406936645508
Batch 42/64 loss: -0.5752363204956055
Batch 43/64 loss: -0.13438034057617188
Batch 44/64 loss: -0.6660327911376953
Batch 45/64 loss: 0.010342597961425781
Batch 46/64 loss: -0.7457208633422852
Batch 47/64 loss: -0.08471059799194336
Batch 48/64 loss: -1.03594970703125
Batch 49/64 loss: -0.8892688751220703
Batch 50/64 loss: -0.8307352066040039
Batch 51/64 loss: -0.7943382263183594
Batch 52/64 loss: -1.0338478088378906
Batch 53/64 loss: -0.8270692825317383
Batch 54/64 loss: -0.3817567825317383
Batch 55/64 loss: -0.34072113037109375
Batch 56/64 loss: -0.648371696472168
Batch 57/64 loss: -0.6415586471557617
Batch 58/64 loss: -0.5023889541625977
Batch 59/64 loss: -0.5465717315673828
Batch 60/64 loss: -0.8220424652099609
Batch 61/64 loss: -0.8239221572875977
Batch 62/64 loss: -0.6081142425537109
Batch 63/64 loss: -0.7322301864624023
Batch 64/64 loss: -4.780293941497803
Epoch 328  Train loss: -0.7027994810366163  Val loss: -0.7870700744419163
Epoch 329
-------------------------------
Batch 1/64 loss: -1.0134906768798828
Batch 2/64 loss: -0.8099594116210938
Batch 3/64 loss: -0.9098587036132812
Batch 4/64 loss: -0.7644510269165039
Batch 5/64 loss: -0.25885009765625
Batch 6/64 loss: -0.5954618453979492
Batch 7/64 loss: -0.7747802734375
Batch 8/64 loss: -0.7741060256958008
Batch 9/64 loss: -0.635594367980957
Batch 10/64 loss: -0.8761863708496094
Batch 11/64 loss: -0.6938905715942383
Batch 12/64 loss: -0.8323173522949219
Batch 13/64 loss: -0.4884195327758789
Batch 14/64 loss: -0.7132558822631836
Batch 15/64 loss: -0.8441610336303711
Batch 16/64 loss: -0.12087678909301758
Batch 17/64 loss: -0.5079679489135742
Batch 18/64 loss: -0.5053501129150391
Batch 19/64 loss: -0.7834796905517578
Batch 20/64 loss: -0.6000428199768066
Batch 21/64 loss: -0.9119329452514648
Batch 22/64 loss: -0.559814453125
Batch 23/64 loss: -0.9103479385375977
Batch 24/64 loss: -0.16598987579345703
Batch 25/64 loss: -0.3643026351928711
Batch 26/64 loss: -0.7275943756103516
Batch 27/64 loss: -0.7725563049316406
Batch 28/64 loss: -0.9474220275878906
Batch 29/64 loss: -0.7559051513671875
Batch 30/64 loss: -0.2292804718017578
Batch 31/64 loss: -0.7260408401489258
Batch 32/64 loss: -0.8009662628173828
Batch 33/64 loss: -0.3434162139892578
Batch 34/64 loss: -0.7486457824707031
Batch 35/64 loss: -0.6767654418945312
Batch 36/64 loss: -0.9865207672119141
Batch 37/64 loss: -0.7975997924804688
Batch 38/64 loss: -1.0305118560791016
Batch 39/64 loss: -0.6369209289550781
Batch 40/64 loss: -0.7845211029052734
Batch 41/64 loss: -0.661468505859375
Batch 42/64 loss: -0.4872112274169922
Batch 43/64 loss: -0.7764320373535156
Batch 44/64 loss: -0.8815526962280273
Batch 45/64 loss: -0.222869873046875
Batch 46/64 loss: -0.8918828964233398
Batch 47/64 loss: -0.6824908256530762
Batch 48/64 loss: -0.5614824295043945
Batch 49/64 loss: -0.5714359283447266
Batch 50/64 loss: -0.23398399353027344
Batch 51/64 loss: -0.800349235534668
Batch 52/64 loss: -0.5213098526000977
Batch 53/64 loss: -0.5548772811889648
Batch 54/64 loss: -0.6190481185913086
Batch 55/64 loss: -0.4022703170776367
Batch 56/64 loss: -0.7712202072143555
Batch 57/64 loss: -0.5702314376831055
Batch 58/64 loss: -0.43218517303466797
Batch 59/64 loss: -0.31659889221191406
Batch 60/64 loss: -0.8355846405029297
Batch 61/64 loss: -0.7667388916015625
Batch 62/64 loss: -0.5691928863525391
Batch 63/64 loss: -0.8717432022094727
Batch 64/64 loss: -4.531063556671143
Epoch 329  Train loss: -0.7024315908843396  Val loss: -0.680662214141531
Epoch 330
-------------------------------
Batch 1/64 loss: -0.9460325241088867
Batch 2/64 loss: -0.7237033843994141
Batch 3/64 loss: -0.9875831604003906
Batch 4/64 loss: -0.7249174118041992
Batch 5/64 loss: -0.2552042007446289
Batch 6/64 loss: -0.498260498046875
Batch 7/64 loss: -0.2715940475463867
Batch 8/64 loss: -0.7890796661376953
Batch 9/64 loss: -0.43927860260009766
Batch 10/64 loss: -1.0503425598144531
Batch 11/64 loss: -0.9256429672241211
Batch 12/64 loss: -0.5657882690429688
Batch 13/64 loss: -0.8922643661499023
Batch 14/64 loss: -0.6519432067871094
Batch 15/64 loss: -0.8100128173828125
Batch 16/64 loss: -0.5905208587646484
Batch 17/64 loss: -0.5071554183959961
Batch 18/64 loss: -0.3916339874267578
Batch 19/64 loss: -0.6036520004272461
Batch 20/64 loss: -0.6895723342895508
Batch 21/64 loss: -0.13731956481933594
Batch 22/64 loss: -1.0462007522583008
Batch 23/64 loss: -0.6856136322021484
Batch 24/64 loss: -0.43067455291748047
Batch 25/64 loss: -0.6335525512695312
Batch 26/64 loss: -0.74969482421875
Batch 27/64 loss: -0.7822341918945312
Batch 28/64 loss: -0.7256040573120117
Batch 29/64 loss: -0.8476057052612305
Batch 30/64 loss: -0.22669219970703125
Batch 31/64 loss: -0.5798244476318359
Batch 32/64 loss: -0.8670082092285156
Batch 33/64 loss: -0.5134735107421875
Batch 34/64 loss: -0.1745586395263672
Batch 35/64 loss: -0.6459646224975586
Batch 36/64 loss: -0.3977851867675781
Batch 37/64 loss: -0.462921142578125
Batch 38/64 loss: -0.29680442810058594
Batch 39/64 loss: -0.6133022308349609
Batch 40/64 loss: -0.2652292251586914
Batch 41/64 loss: -0.5135641098022461
Batch 42/64 loss: -0.7261152267456055
Batch 43/64 loss: -0.6886148452758789
Batch 44/64 loss: -0.29299354553222656
Batch 45/64 loss: -0.7658967971801758
Batch 46/64 loss: -0.6540966033935547
Batch 47/64 loss: -0.6860504150390625
Batch 48/64 loss: -0.9741477966308594
Batch 49/64 loss: -0.7054538726806641
Batch 50/64 loss: -0.6808853149414062
Batch 51/64 loss: -0.7876176834106445
Batch 52/64 loss: -0.3979654312133789
Batch 53/64 loss: -0.383758544921875
Batch 54/64 loss: -0.7481679916381836
Batch 55/64 loss: -0.7198276519775391
Batch 56/64 loss: -0.6052265167236328
Batch 57/64 loss: -0.5508546829223633
Batch 58/64 loss: -0.6147565841674805
Batch 59/64 loss: -0.7188892364501953
Batch 60/64 loss: -0.6966896057128906
Batch 61/64 loss: -0.870574951171875
Batch 62/64 loss: -0.6217765808105469
Batch 63/64 loss: -1.005162239074707
Batch 64/64 loss: -4.508232116699219
Epoch 330  Train loss: -0.6774353924919577  Val loss: -0.5839974314896101
Epoch 331
-------------------------------
Batch 1/64 loss: -0.9488039016723633
Batch 2/64 loss: -0.6143264770507812
Batch 3/64 loss: -0.45055580139160156
Batch 4/64 loss: -0.9586629867553711
Batch 5/64 loss: -0.812565803527832
Batch 6/64 loss: -0.5695056915283203
Batch 7/64 loss: -0.6006526947021484
Batch 8/64 loss: -1.0310649871826172
Batch 9/64 loss: -0.7480831146240234
Batch 10/64 loss: -0.5427656173706055
Batch 11/64 loss: -0.08504009246826172
Batch 12/64 loss: -0.20702552795410156
Batch 13/64 loss: -0.5677385330200195
Batch 14/64 loss: -0.5604615211486816
Batch 15/64 loss: -0.4755411148071289
Batch 16/64 loss: -0.5439167022705078
Batch 17/64 loss: -0.45657825469970703
Batch 18/64 loss: -0.1361093521118164
Batch 19/64 loss: -0.7013325691223145
Batch 20/64 loss: -0.6328630447387695
Batch 21/64 loss: -0.9364204406738281
Batch 22/64 loss: -0.6567001342773438
Batch 23/64 loss: -0.5605015754699707
Batch 24/64 loss: 0.2558155059814453
Batch 25/64 loss: -0.659210205078125
Batch 26/64 loss: -0.47022438049316406
Batch 27/64 loss: -0.46484947204589844
Batch 28/64 loss: -0.6353211402893066
Batch 29/64 loss: -0.6725888252258301
Batch 30/64 loss: -0.5655956268310547
Batch 31/64 loss: -0.6499614715576172
Batch 32/64 loss: -0.8756113052368164
Batch 33/64 loss: -0.031423091888427734
Batch 34/64 loss: -0.6753978729248047
Batch 35/64 loss: -0.9436120986938477
Batch 36/64 loss: -0.25974321365356445
Batch 37/64 loss: -0.5346136093139648
Batch 38/64 loss: -0.8563194274902344
Batch 39/64 loss: -0.44034767150878906
Batch 40/64 loss: -0.7495365142822266
Batch 41/64 loss: -0.21783685684204102
Batch 42/64 loss: -0.5188140869140625
Batch 43/64 loss: -0.3612995147705078
Batch 44/64 loss: -0.8003177642822266
Batch 45/64 loss: -0.6026086807250977
Batch 46/64 loss: -0.7012138366699219
Batch 47/64 loss: -0.9940204620361328
Batch 48/64 loss: -0.6172075271606445
Batch 49/64 loss: -0.8896312713623047
Batch 50/64 loss: -0.7524309158325195
Batch 51/64 loss: -0.8128232955932617
Batch 52/64 loss: -0.4922761917114258
Batch 53/64 loss: -0.8837919235229492
Batch 54/64 loss: -0.5661334991455078
Batch 55/64 loss: -0.4163699150085449
Batch 56/64 loss: -0.5095119476318359
Batch 57/64 loss: -0.7142419815063477
Batch 58/64 loss: -0.7292051315307617
Batch 59/64 loss: -0.8747215270996094
Batch 60/64 loss: -0.8804521560668945
Batch 61/64 loss: -0.5994977951049805
Batch 62/64 loss: -0.7789297103881836
Batch 63/64 loss: -0.5124263763427734
Batch 64/64 loss: -4.865316867828369
Epoch 331  Train loss: -0.6572628451328651  Val loss: -0.7754787169780928
Epoch 332
-------------------------------
Batch 1/64 loss: -0.9083595275878906
Batch 2/64 loss: -0.8765335083007812
Batch 3/64 loss: -0.70556640625
Batch 4/64 loss: -0.8892221450805664
Batch 5/64 loss: 0.22367382049560547
Batch 6/64 loss: -0.7389373779296875
Batch 7/64 loss: -0.43552494049072266
Batch 8/64 loss: -0.8390207290649414
Batch 9/64 loss: -0.5988597869873047
Batch 10/64 loss: -0.3110208511352539
Batch 11/64 loss: -0.8263025283813477
Batch 12/64 loss: -0.6333494186401367
Batch 13/64 loss: -0.9864912033081055
Batch 14/64 loss: -0.2638273239135742
Batch 15/64 loss: -0.7675819396972656
Batch 16/64 loss: -0.42375659942626953
Batch 17/64 loss: -0.4631490707397461
Batch 18/64 loss: -0.9101085662841797
Batch 19/64 loss: -0.8161106109619141
Batch 20/64 loss: -0.3403739929199219
Batch 21/64 loss: -0.9365463256835938
Batch 22/64 loss: -0.9585351943969727
Batch 23/64 loss: -0.8040323257446289
Batch 24/64 loss: -0.7867965698242188
Batch 25/64 loss: -0.7005987167358398
Batch 26/64 loss: -0.36345767974853516
Batch 27/64 loss: -0.23820877075195312
Batch 28/64 loss: -0.425046443939209
Batch 29/64 loss: -0.5414714813232422
Batch 30/64 loss: -0.6602859497070312
Batch 31/64 loss: -0.7011356353759766
Batch 32/64 loss: -0.7469882965087891
Batch 33/64 loss: -0.9074783325195312
Batch 34/64 loss: -0.7975559234619141
Batch 35/64 loss: -1.0612144470214844
Batch 36/64 loss: -0.7679424285888672
Batch 37/64 loss: -1.003727912902832
Batch 38/64 loss: -0.5832381248474121
Batch 39/64 loss: -0.18920516967773438
Batch 40/64 loss: -0.5294132232666016
Batch 41/64 loss: -0.8212051391601562
Batch 42/64 loss: -0.7719697952270508
Batch 43/64 loss: -0.7169122695922852
Batch 44/64 loss: -0.6005172729492188
Batch 45/64 loss: -0.5168952941894531
Batch 46/64 loss: -0.7247228622436523
Batch 47/64 loss: -0.48413658142089844
Batch 48/64 loss: -0.7501430511474609
Batch 49/64 loss: -0.6978645324707031
Batch 50/64 loss: -0.48973751068115234
Batch 51/64 loss: -0.38101863861083984
Batch 52/64 loss: -0.29927968978881836
Batch 53/64 loss: -0.07589912414550781
Batch 54/64 loss: -0.4252963066101074
Batch 55/64 loss: -0.6767129898071289
Batch 56/64 loss: -0.6300687789916992
Batch 57/64 loss: -0.7023115158081055
Batch 58/64 loss: -0.8520412445068359
Batch 59/64 loss: -0.6087312698364258
Batch 60/64 loss: -0.739405632019043
Batch 61/64 loss: -0.8473482131958008
Batch 62/64 loss: -1.1669130325317383
Batch 63/64 loss: -0.7887153625488281
Batch 64/64 loss: -4.80451774597168
Epoch 332  Train loss: -0.6993652717739928  Val loss: -0.6101320142188842
Epoch 333
-------------------------------
Batch 1/64 loss: -0.31378173828125
Batch 2/64 loss: -0.3378744125366211
Batch 3/64 loss: -0.6799287796020508
Batch 4/64 loss: -0.9877529144287109
Batch 5/64 loss: -0.9309549331665039
Batch 6/64 loss: -0.8352298736572266
Batch 7/64 loss: 0.5030393600463867
Batch 8/64 loss: -0.8973302841186523
Batch 9/64 loss: -0.9152002334594727
Batch 10/64 loss: -0.4787721633911133
Batch 11/64 loss: -0.8922529220581055
Batch 12/64 loss: -0.9096899032592773
Batch 13/64 loss: -0.5441980361938477
Batch 14/64 loss: -0.8867635726928711
Batch 15/64 loss: -0.9117984771728516
Batch 16/64 loss: -0.650054931640625
Batch 17/64 loss: -0.3951711654663086
Batch 18/64 loss: -0.6776676177978516
Batch 19/64 loss: -0.9047756195068359
Batch 20/64 loss: -0.03864002227783203
Batch 21/64 loss: -0.28019142150878906
Batch 22/64 loss: -0.8923883438110352
Batch 23/64 loss: -0.8523988723754883
Batch 24/64 loss: -0.7273931503295898
Batch 25/64 loss: -0.16526508331298828
Batch 26/64 loss: -0.5443801879882812
Batch 27/64 loss: -0.8755035400390625
Batch 28/64 loss: -0.7638978958129883
Batch 29/64 loss: -0.7922172546386719
Batch 30/64 loss: -0.7166719436645508
Batch 31/64 loss: -0.7080631256103516
Batch 32/64 loss: -0.6293888092041016
Batch 33/64 loss: -0.5912961959838867
Batch 34/64 loss: -1.1127471923828125
Batch 35/64 loss: -0.5994415283203125
Batch 36/64 loss: -0.7016735076904297
Batch 37/64 loss: -0.3059110641479492
Batch 38/64 loss: -0.06830024719238281
Batch 39/64 loss: -0.9221506118774414
Batch 40/64 loss: -0.9706659317016602
Batch 41/64 loss: -1.0619220733642578
Batch 42/64 loss: -0.9500207901000977
Batch 43/64 loss: -0.527313232421875
Batch 44/64 loss: -0.656895637512207
Batch 45/64 loss: -0.7791023254394531
Batch 46/64 loss: -0.5468425750732422
Batch 47/64 loss: -0.8235616683959961
Batch 48/64 loss: -0.6073780059814453
Batch 49/64 loss: -0.3831782341003418
Batch 50/64 loss: -0.7631902694702148
Batch 51/64 loss: -0.5380716323852539
Batch 52/64 loss: -0.9334878921508789
Batch 53/64 loss: -0.6848611831665039
Batch 54/64 loss: -0.7103500366210938
Batch 55/64 loss: -0.9216489791870117
Batch 56/64 loss: -1.0004940032958984
Batch 57/64 loss: -0.5431222915649414
Batch 58/64 loss: -0.6888523101806641
Batch 59/64 loss: -0.729954719543457
Batch 60/64 loss: -0.5378694534301758
Batch 61/64 loss: -0.8824949264526367
Batch 62/64 loss: -0.6667165756225586
Batch 63/64 loss: -0.999516487121582
Batch 64/64 loss: -4.550826072692871
Epoch 333  Train loss: -0.7255640628291111  Val loss: -0.7750418096063882
Epoch 334
-------------------------------
Batch 1/64 loss: -0.47164058685302734
Batch 2/64 loss: -1.0191459655761719
Batch 3/64 loss: -0.8812170028686523
Batch 4/64 loss: -0.8932571411132812
Batch 5/64 loss: -0.9510622024536133
Batch 6/64 loss: -0.6519031524658203
Batch 7/64 loss: -0.43140363693237305
Batch 8/64 loss: -0.40654563903808594
Batch 9/64 loss: -0.794952392578125
Batch 10/64 loss: -0.6137747764587402
Batch 11/64 loss: -0.3857259750366211
Batch 12/64 loss: -0.5872535705566406
Batch 13/64 loss: -0.9244356155395508
Batch 14/64 loss: -1.072789192199707
Batch 15/64 loss: -0.8131580352783203
Batch 16/64 loss: -0.78741455078125
Batch 17/64 loss: -0.6810970306396484
Batch 18/64 loss: -0.4413433074951172
Batch 19/64 loss: -0.917510986328125
Batch 20/64 loss: -0.6675043106079102
Batch 21/64 loss: -0.8404712677001953
Batch 22/64 loss: -0.7099390029907227
Batch 23/64 loss: -0.6915087699890137
Batch 24/64 loss: -0.18818283081054688
Batch 25/64 loss: -0.6762866973876953
Batch 26/64 loss: -0.23147010803222656
Batch 27/64 loss: -0.7386350631713867
Batch 28/64 loss: -0.859105110168457
Batch 29/64 loss: -1.1139612197875977
Batch 30/64 loss: -0.7236757278442383
Batch 31/64 loss: -0.5832700729370117
Batch 32/64 loss: -0.18026351928710938
Batch 33/64 loss: -0.832270622253418
Batch 34/64 loss: -0.8041172027587891
Batch 35/64 loss: -0.5908851623535156
Batch 36/64 loss: -0.7727203369140625
Batch 37/64 loss: -0.5570392608642578
Batch 38/64 loss: -0.889927864074707
Batch 39/64 loss: -0.5873584747314453
Batch 40/64 loss: -1.0177087783813477
Batch 41/64 loss: -0.46259403228759766
Batch 42/64 loss: -0.9284400939941406
Batch 43/64 loss: -0.3666386604309082
Batch 44/64 loss: -0.3623847961425781
Batch 45/64 loss: -0.6074151992797852
Batch 46/64 loss: -0.8295087814331055
Batch 47/64 loss: -0.8339691162109375
Batch 48/64 loss: -0.41887760162353516
Batch 49/64 loss: -0.12910079956054688
Batch 50/64 loss: -0.9235734939575195
Batch 51/64 loss: -0.9150543212890625
Batch 52/64 loss: -0.6120710372924805
Batch 53/64 loss: -0.5822629928588867
Batch 54/64 loss: -0.8727149963378906
Batch 55/64 loss: -0.9393634796142578
Batch 56/64 loss: -0.8458147048950195
Batch 57/64 loss: -0.15545368194580078
Batch 58/64 loss: -0.628849983215332
Batch 59/64 loss: -0.7198553085327148
Batch 60/64 loss: -0.8492155075073242
Batch 61/64 loss: -0.6045904159545898
Batch 62/64 loss: -0.7768487930297852
Batch 63/64 loss: -0.7631521224975586
Batch 64/64 loss: -4.693025588989258
Epoch 334  Train loss: -0.7314423729391659  Val loss: -0.6631279122788472
Epoch 335
-------------------------------
Batch 1/64 loss: -0.5679197311401367
Batch 2/64 loss: -0.717137336730957
Batch 3/64 loss: -0.5820693969726562
Batch 4/64 loss: -0.42665958404541016
Batch 5/64 loss: -0.3032035827636719
Batch 6/64 loss: -0.3402070999145508
Batch 7/64 loss: -0.3646526336669922
Batch 8/64 loss: -1.0152721405029297
Batch 9/64 loss: -0.9396486282348633
Batch 10/64 loss: -0.7261676788330078
Batch 11/64 loss: -0.7555427551269531
Batch 12/64 loss: -0.9236736297607422
Batch 13/64 loss: -0.9667186737060547
Batch 14/64 loss: -0.8405466079711914
Batch 15/64 loss: -0.8433904647827148
Batch 16/64 loss: -0.6770248413085938
Batch 17/64 loss: -0.5411701202392578
Batch 18/64 loss: -0.7924995422363281
Batch 19/64 loss: -0.4717426300048828
Batch 20/64 loss: -0.8676633834838867
Batch 21/64 loss: -0.3664407730102539
Batch 22/64 loss: -0.5661487579345703
Batch 23/64 loss: -0.49806976318359375
Batch 24/64 loss: -0.5112028121948242
Batch 25/64 loss: -0.9380683898925781
Batch 26/64 loss: -1.1287918090820312
Batch 27/64 loss: 0.10706901550292969
Batch 28/64 loss: -0.8629941940307617
Batch 29/64 loss: -0.7456779479980469
Batch 30/64 loss: -0.9400463104248047
Batch 31/64 loss: -0.6022396087646484
Batch 32/64 loss: -0.6946926116943359
Batch 33/64 loss: -0.7302579879760742
Batch 34/64 loss: -0.6033830642700195
Batch 35/64 loss: -0.8609142303466797
Batch 36/64 loss: -0.5843439102172852
Batch 37/64 loss: -0.39374732971191406
Batch 38/64 loss: -0.8369359970092773
Batch 39/64 loss: -0.8830423355102539
Batch 40/64 loss: -0.33586788177490234
Batch 41/64 loss: -0.6287736892700195
Batch 42/64 loss: -0.8435268402099609
Batch 43/64 loss: -0.8380861282348633
Batch 44/64 loss: -0.9151802062988281
Batch 45/64 loss: -0.9191551208496094
Batch 46/64 loss: -1.0181856155395508
Batch 47/64 loss: -0.9245424270629883
Batch 48/64 loss: -0.9292383193969727
Batch 49/64 loss: -0.5611162185668945
Batch 50/64 loss: -0.5272483825683594
Batch 51/64 loss: -0.588292121887207
Batch 52/64 loss: -0.5308351516723633
Batch 53/64 loss: -0.5225706100463867
Batch 54/64 loss: -1.1567020416259766
Batch 55/64 loss: -0.8162107467651367
Batch 56/64 loss: -0.907379150390625
Batch 57/64 loss: -0.10069656372070312
Batch 58/64 loss: -0.8774471282958984
Batch 59/64 loss: -0.5676956176757812
Batch 60/64 loss: -0.9383659362792969
Batch 61/64 loss: -0.9531669616699219
Batch 62/64 loss: -0.8157510757446289
Batch 63/64 loss: -0.31700992584228516
Batch 64/64 loss: -4.747247695922852
Epoch 335  Train loss: -0.743471669215782  Val loss: -0.8440574042985529
Epoch 336
-------------------------------
Batch 1/64 loss: -1.0744457244873047
Batch 2/64 loss: -0.6490421295166016
Batch 3/64 loss: -0.6268491744995117
Batch 4/64 loss: -0.9502115249633789
Batch 5/64 loss: -0.47841644287109375
Batch 6/64 loss: -0.7570858001708984
Batch 7/64 loss: -0.9690933227539062
Batch 8/64 loss: -1.040715217590332
Batch 9/64 loss: -0.72833251953125
Batch 10/64 loss: -0.9616775512695312
Batch 11/64 loss: -0.6077737808227539
Batch 12/64 loss: -0.5520429611206055
Batch 13/64 loss: -0.7894296646118164
Batch 14/64 loss: -0.10073471069335938
Batch 15/64 loss: -0.8266019821166992
Batch 16/64 loss: -0.6073751449584961
Batch 17/64 loss: -0.7586812973022461
Batch 18/64 loss: -0.6638679504394531
Batch 19/64 loss: -0.6467742919921875
Batch 20/64 loss: -0.8562450408935547
Batch 21/64 loss: -0.5386390686035156
Batch 22/64 loss: -0.811945915222168
Batch 23/64 loss: 0.1152653694152832
Batch 24/64 loss: -0.9034147262573242
Batch 25/64 loss: 0.058956146240234375
Batch 26/64 loss: -0.8543319702148438
Batch 27/64 loss: -0.8497447967529297
Batch 28/64 loss: -0.5743932723999023
Batch 29/64 loss: -0.6357269287109375
Batch 30/64 loss: -0.5978565216064453
Batch 31/64 loss: -0.9824438095092773
Batch 32/64 loss: -0.3661928176879883
Batch 33/64 loss: -0.8268623352050781
Batch 34/64 loss: -0.6754674911499023
Batch 35/64 loss: -0.35683155059814453
Batch 36/64 loss: -0.7040042877197266
Batch 37/64 loss: -0.8999042510986328
Batch 38/64 loss: -0.6849088668823242
Batch 39/64 loss: -0.6272220611572266
Batch 40/64 loss: -0.6016039848327637
Batch 41/64 loss: -0.6099357604980469
Batch 42/64 loss: -0.837336540222168
Batch 43/64 loss: -0.5485877990722656
Batch 44/64 loss: -0.744481086730957
Batch 45/64 loss: -0.4876232147216797
Batch 46/64 loss: -0.8539133071899414
Batch 47/64 loss: -0.8850936889648438
Batch 48/64 loss: -0.3626742362976074
Batch 49/64 loss: -0.6849756240844727
Batch 50/64 loss: -0.5824975967407227
Batch 51/64 loss: -0.6552534103393555
Batch 52/64 loss: -0.6589593887329102
Batch 53/64 loss: -0.40758609771728516
Batch 54/64 loss: -0.7857837677001953
Batch 55/64 loss: -0.7073707580566406
Batch 56/64 loss: -0.6439008712768555
Batch 57/64 loss: -0.7273235321044922
Batch 58/64 loss: -0.6595344543457031
Batch 59/64 loss: -0.4853534698486328
Batch 60/64 loss: -0.3502464294433594
Batch 61/64 loss: -0.6152410507202148
Batch 62/64 loss: -0.6018247604370117
Batch 63/64 loss: -0.9345340728759766
Batch 64/64 loss: -4.675683975219727
Epoch 336  Train loss: -0.7105798160328585  Val loss: -0.7289655495345387
Epoch 337
-------------------------------
Batch 1/64 loss: -0.8483514785766602
Batch 2/64 loss: -0.7738208770751953
Batch 3/64 loss: -0.8840351104736328
Batch 4/64 loss: -0.9414815902709961
Batch 5/64 loss: -0.9009332656860352
Batch 6/64 loss: -0.2780170440673828
Batch 7/64 loss: -0.9134426116943359
Batch 8/64 loss: -0.12978744506835938
Batch 9/64 loss: -0.6482162475585938
Batch 10/64 loss: -0.9118976593017578
Batch 11/64 loss: -0.9386558532714844
Batch 12/64 loss: -0.6506414413452148
Batch 13/64 loss: -0.6315488815307617
Batch 14/64 loss: -0.7109384536743164
Batch 15/64 loss: -0.9051704406738281
Batch 16/64 loss: -0.6810321807861328
Batch 17/64 loss: -0.7039146423339844
Batch 18/64 loss: -0.4582042694091797
Batch 19/64 loss: -0.388397216796875
Batch 20/64 loss: -0.7561874389648438
Batch 21/64 loss: -0.8770332336425781
Batch 22/64 loss: -0.4677276611328125
Batch 23/64 loss: -0.6758584976196289
Batch 24/64 loss: -0.6778707504272461
Batch 25/64 loss: -0.6172113418579102
Batch 26/64 loss: -0.6334075927734375
Batch 27/64 loss: -0.7623281478881836
Batch 28/64 loss: -0.35082435607910156
Batch 29/64 loss: -0.8736791610717773
Batch 30/64 loss: -0.8364486694335938
Batch 31/64 loss: -0.4958934783935547
Batch 32/64 loss: -0.7979660034179688
Batch 33/64 loss: -0.8540630340576172
Batch 34/64 loss: -0.6494913101196289
Batch 35/64 loss: -0.8839683532714844
Batch 36/64 loss: -0.7293338775634766
Batch 37/64 loss: -0.4941530227661133
Batch 38/64 loss: -1.1100921630859375
Batch 39/64 loss: -1.0172147750854492
Batch 40/64 loss: -0.23579025268554688
Batch 41/64 loss: -0.8124961853027344
Batch 42/64 loss: -0.6245441436767578
Batch 43/64 loss: -0.7634544372558594
Batch 44/64 loss: -0.749354362487793
Batch 45/64 loss: -0.815037727355957
Batch 46/64 loss: -0.3265037536621094
Batch 47/64 loss: -0.5667657852172852
Batch 48/64 loss: -0.7316017150878906
Batch 49/64 loss: -0.4223766326904297
Batch 50/64 loss: 0.16865921020507812
Batch 51/64 loss: -0.4675579071044922
Batch 52/64 loss: -0.9941320419311523
Batch 53/64 loss: -0.40813732147216797
Batch 54/64 loss: -0.5263681411743164
Batch 55/64 loss: -0.6913156509399414
Batch 56/64 loss: -0.8758115768432617
Batch 57/64 loss: -0.8779749870300293
Batch 58/64 loss: -0.345273494720459
Batch 59/64 loss: -0.6484317779541016
Batch 60/64 loss: -0.8692102432250977
Batch 61/64 loss: -0.8275823593139648
Batch 62/64 loss: -0.7461967468261719
Batch 63/64 loss: -0.8060274124145508
Batch 64/64 loss: -4.613276481628418
Epoch 337  Train loss: -0.7259997236962412  Val loss: -0.7925684165299144
Epoch 338
-------------------------------
Batch 1/64 loss: -0.2318568229675293
Batch 2/64 loss: -0.35111236572265625
Batch 3/64 loss: -0.8018798828125
Batch 4/64 loss: -1.1378889083862305
Batch 5/64 loss: -0.09479522705078125
Batch 6/64 loss: -0.9498376846313477
Batch 7/64 loss: -0.7484340667724609
Batch 8/64 loss: -0.5211739540100098
Batch 9/64 loss: -0.9617385864257812
Batch 10/64 loss: -0.6609954833984375
Batch 11/64 loss: -0.6954669952392578
Batch 12/64 loss: -0.5613455772399902
Batch 13/64 loss: -1.1536178588867188
Batch 14/64 loss: -0.16215229034423828
Batch 15/64 loss: -0.7390632629394531
Batch 16/64 loss: -0.9168701171875
Batch 17/64 loss: -0.8539266586303711
Batch 18/64 loss: -0.6765069961547852
Batch 19/64 loss: -0.43443965911865234
Batch 20/64 loss: -0.6225700378417969
Batch 21/64 loss: -0.8318920135498047
Batch 22/64 loss: -0.5395679473876953
Batch 23/64 loss: -0.4791526794433594
Batch 24/64 loss: 0.06031370162963867
Batch 25/64 loss: -0.9405117034912109
Batch 26/64 loss: -0.9220056533813477
Batch 27/64 loss: -0.5728178024291992
Batch 28/64 loss: -0.663970947265625
Batch 29/64 loss: -0.7625045776367188
Batch 30/64 loss: -0.2714385986328125
Batch 31/64 loss: -1.1362848281860352
Batch 32/64 loss: -0.5440244674682617
Batch 33/64 loss: -0.3823380470275879
Batch 34/64 loss: -0.5821571350097656
Batch 35/64 loss: -0.8359689712524414
Batch 36/64 loss: -0.5218505859375
Batch 37/64 loss: -0.8428955078125
Batch 38/64 loss: -0.6601037979125977
Batch 39/64 loss: -0.7785511016845703
Batch 40/64 loss: -0.5613279342651367
Batch 41/64 loss: -1.0561256408691406
Batch 42/64 loss: -0.8284530639648438
Batch 43/64 loss: -0.6100234985351562
Batch 44/64 loss: -0.6749439239501953
Batch 45/64 loss: -0.48934459686279297
Batch 46/64 loss: -0.7111692428588867
Batch 47/64 loss: -0.6694240570068359
Batch 48/64 loss: -1.0027389526367188
Batch 49/64 loss: -1.036808967590332
Batch 50/64 loss: -0.8632173538208008
Batch 51/64 loss: -0.9156742095947266
Batch 52/64 loss: -0.7369565963745117
Batch 53/64 loss: -0.6850919723510742
Batch 54/64 loss: -0.49970531463623047
Batch 55/64 loss: -0.6002712249755859
Batch 56/64 loss: -0.7668228149414062
Batch 57/64 loss: -0.6390953063964844
Batch 58/64 loss: -0.7277545928955078
Batch 59/64 loss: -0.9297666549682617
Batch 60/64 loss: -0.6946649551391602
Batch 61/64 loss: -0.7513799667358398
Batch 62/64 loss: -0.4931144714355469
Batch 63/64 loss: -0.5805110931396484
Batch 64/64 loss: -4.74249267578125
Epoch 338  Train loss: -0.7304258982340495  Val loss: -0.5526658808652478
Epoch 339
-------------------------------
Batch 1/64 loss: -0.5536050796508789
Batch 2/64 loss: -0.4232749938964844
Batch 3/64 loss: -0.29073619842529297
Batch 4/64 loss: -0.9469308853149414
Batch 5/64 loss: -0.49273681640625
Batch 6/64 loss: -0.8787784576416016
Batch 7/64 loss: -0.7376632690429688
Batch 8/64 loss: -0.6569356918334961
Batch 9/64 loss: -0.7606697082519531
Batch 10/64 loss: -0.4246559143066406
Batch 11/64 loss: -0.6053657531738281
Batch 12/64 loss: -0.7526769638061523
Batch 13/64 loss: -1.1284370422363281
Batch 14/64 loss: -0.578160285949707
Batch 15/64 loss: -0.7636899948120117
Batch 16/64 loss: -1.0114822387695312
Batch 17/64 loss: -0.7689781188964844
Batch 18/64 loss: -0.5658588409423828
Batch 19/64 loss: -0.8074131011962891
Batch 20/64 loss: -0.8652715682983398
Batch 21/64 loss: -0.44492626190185547
Batch 22/64 loss: -0.30979156494140625
Batch 23/64 loss: -0.36266517639160156
Batch 24/64 loss: -0.6780281066894531
Batch 25/64 loss: -0.5544252395629883
Batch 26/64 loss: -0.41933441162109375
Batch 27/64 loss: -0.44295310974121094
Batch 28/64 loss: -0.3695230484008789
Batch 29/64 loss: -0.9260568618774414
Batch 30/64 loss: -0.8271713256835938
Batch 31/64 loss: -0.7646484375
Batch 32/64 loss: -0.6616497039794922
Batch 33/64 loss: -0.083160400390625
Batch 34/64 loss: -0.9353160858154297
Batch 35/64 loss: -0.8610382080078125
Batch 36/64 loss: -0.8997182846069336
Batch 37/64 loss: -0.761439323425293
Batch 38/64 loss: -0.7518596649169922
Batch 39/64 loss: -0.6250228881835938
Batch 40/64 loss: -0.6422300338745117
Batch 41/64 loss: -0.896942138671875
Batch 42/64 loss: -0.6140127182006836
Batch 43/64 loss: -0.5911436080932617
Batch 44/64 loss: -0.8941402435302734
Batch 45/64 loss: -0.4743766784667969
Batch 46/64 loss: -0.8980951309204102
Batch 47/64 loss: -0.6794595718383789
Batch 48/64 loss: -0.7376489639282227
Batch 49/64 loss: -1.0278663635253906
Batch 50/64 loss: -0.8149929046630859
Batch 51/64 loss: -0.7798480987548828
Batch 52/64 loss: -0.9473342895507812
Batch 53/64 loss: -0.6023168563842773
Batch 54/64 loss: -0.7263917922973633
Batch 55/64 loss: -0.8452911376953125
Batch 56/64 loss: -0.5847673416137695
Batch 57/64 loss: -0.6741609573364258
Batch 58/64 loss: -0.5833864212036133
Batch 59/64 loss: -0.5287055969238281
Batch 60/64 loss: -0.49334716796875
Batch 61/64 loss: -0.2107081413269043
Batch 62/64 loss: -0.5422821044921875
Batch 63/64 loss: -0.8891324996948242
Batch 64/64 loss: -4.76417875289917
Epoch 339  Train loss: -0.7206864917979521  Val loss: -0.7182184592964723
Epoch 340
-------------------------------
Batch 1/64 loss: -0.7282562255859375
Batch 2/64 loss: -0.5317811965942383
Batch 3/64 loss: -0.5891180038452148
Batch 4/64 loss: -0.4376068115234375
Batch 5/64 loss: -0.8909101486206055
Batch 6/64 loss: -0.6926231384277344
Batch 7/64 loss: -0.8494510650634766
Batch 8/64 loss: -1.1412162780761719
Batch 9/64 loss: -0.14510202407836914
Batch 10/64 loss: -0.8727092742919922
Batch 11/64 loss: -0.9799251556396484
Batch 12/64 loss: -0.6288814544677734
Batch 13/64 loss: -0.8139686584472656
Batch 14/64 loss: -1.0176982879638672
Batch 15/64 loss: -0.8149471282958984
Batch 16/64 loss: -0.7306175231933594
Batch 17/64 loss: -0.5888233184814453
Batch 18/64 loss: -0.8571767807006836
Batch 19/64 loss: -0.5400190353393555
Batch 20/64 loss: -0.34076976776123047
Batch 21/64 loss: -0.5189266204833984
Batch 22/64 loss: -0.6853904724121094
Batch 23/64 loss: -0.8434762954711914
Batch 24/64 loss: -0.41786956787109375
Batch 25/64 loss: -0.5688133239746094
Batch 26/64 loss: -0.9342327117919922
Batch 27/64 loss: -0.8982324600219727
Batch 28/64 loss: -0.7410478591918945
Batch 29/64 loss: -0.7831792831420898
Batch 30/64 loss: -0.9307737350463867
Batch 31/64 loss: -0.5814170837402344
Batch 32/64 loss: -0.8805704116821289
Batch 33/64 loss: 0.4635019302368164
Batch 34/64 loss: -0.9044132232666016
Batch 35/64 loss: -0.6056623458862305
Batch 36/64 loss: -1.140829086303711
Batch 37/64 loss: -0.2911844253540039
Batch 38/64 loss: -0.5991744995117188
Batch 39/64 loss: -1.1398639678955078
Batch 40/64 loss: -0.6925830841064453
Batch 41/64 loss: -0.7202997207641602
Batch 42/64 loss: -0.5592565536499023
Batch 43/64 loss: -0.8012180328369141
Batch 44/64 loss: -0.6938934326171875
Batch 45/64 loss: -0.6823282241821289
Batch 46/64 loss: -0.5472097396850586
Batch 47/64 loss: -0.35724925994873047
Batch 48/64 loss: -0.7437782287597656
Batch 49/64 loss: -0.3533363342285156
Batch 50/64 loss: -0.5756855010986328
Batch 51/64 loss: -0.784358024597168
Batch 52/64 loss: -0.8093900680541992
Batch 53/64 loss: -0.45822906494140625
Batch 54/64 loss: -0.8834724426269531
Batch 55/64 loss: -0.3453102111816406
Batch 56/64 loss: -0.2797126770019531
Batch 57/64 loss: -0.43996143341064453
Batch 58/64 loss: -0.605900764465332
Batch 59/64 loss: -0.5001430511474609
Batch 60/64 loss: -0.7230062484741211
Batch 61/64 loss: -0.6669435501098633
Batch 62/64 loss: -0.2599606513977051
Batch 63/64 loss: -0.4996614456176758
Batch 64/64 loss: -4.490086078643799
Epoch 340  Train loss: -0.6987232787936342  Val loss: -0.7180484758619591
Epoch 341
-------------------------------
Batch 1/64 loss: -0.637263298034668
Batch 2/64 loss: -0.7716464996337891
Batch 3/64 loss: -0.9501714706420898
Batch 4/64 loss: -0.5262861251831055
Batch 5/64 loss: -0.49619293212890625
Batch 6/64 loss: -0.49738025665283203
Batch 7/64 loss: -0.3804903030395508
Batch 8/64 loss: -0.6116199493408203
Batch 9/64 loss: -0.09532451629638672
Batch 10/64 loss: -0.8494882583618164
Batch 11/64 loss: -0.5333957672119141
Batch 12/64 loss: -0.5937538146972656
Batch 13/64 loss: -0.5761137008666992
Batch 14/64 loss: -0.5848722457885742
Batch 15/64 loss: -0.6240148544311523
Batch 16/64 loss: -0.672205924987793
Batch 17/64 loss: -0.5044212341308594
Batch 18/64 loss: -0.9772920608520508
Batch 19/64 loss: -0.2871561050415039
Batch 20/64 loss: -0.9885072708129883
Batch 21/64 loss: -0.6868753433227539
Batch 22/64 loss: -0.3107728958129883
Batch 23/64 loss: -0.9190139770507812
Batch 24/64 loss: -0.6012706756591797
Batch 25/64 loss: -0.4737377166748047
Batch 26/64 loss: -0.7656993865966797
Batch 27/64 loss: -0.7162923812866211
Batch 28/64 loss: -0.7660531997680664
Batch 29/64 loss: -0.5158224105834961
Batch 30/64 loss: -0.9051513671875
Batch 31/64 loss: -0.5575942993164062
Batch 32/64 loss: -0.7350215911865234
Batch 33/64 loss: -0.5261526107788086
Batch 34/64 loss: -0.8563165664672852
Batch 35/64 loss: -0.7083311080932617
Batch 36/64 loss: -0.5531978607177734
Batch 37/64 loss: -0.2937335968017578
Batch 38/64 loss: -1.0408353805541992
Batch 39/64 loss: -1.0872230529785156
Batch 40/64 loss: -0.604640007019043
Batch 41/64 loss: -0.22142410278320312
Batch 42/64 loss: -0.42665576934814453
Batch 43/64 loss: 0.09728717803955078
Batch 44/64 loss: -0.34403038024902344
Batch 45/64 loss: -0.5941123962402344
Batch 46/64 loss: -0.7174320220947266
Batch 47/64 loss: -0.7401647567749023
Batch 48/64 loss: -0.6661767959594727
Batch 49/64 loss: -0.7306432723999023
Batch 50/64 loss: -0.9648599624633789
Batch 51/64 loss: -1.0121040344238281
Batch 52/64 loss: -0.7072010040283203
Batch 53/64 loss: -0.48760032653808594
Batch 54/64 loss: -0.6132993698120117
Batch 55/64 loss: -0.921605110168457
Batch 56/64 loss: -0.76678466796875
Batch 57/64 loss: -0.8391962051391602
Batch 58/64 loss: -0.6128139495849609
Batch 59/64 loss: -0.6790685653686523
Batch 60/64 loss: -0.7078952789306641
Batch 61/64 loss: -0.48494815826416016
Batch 62/64 loss: -0.7246332168579102
Batch 63/64 loss: -1.0153694152832031
Batch 64/64 loss: -4.760618209838867
Epoch 341  Train loss: -0.6938435648001876  Val loss: -0.7162141177252805
Epoch 342
-------------------------------
Batch 1/64 loss: -0.8302536010742188
Batch 2/64 loss: -0.7460241317749023
Batch 3/64 loss: -0.6132760047912598
Batch 4/64 loss: -0.7958288192749023
Batch 5/64 loss: -0.25591421127319336
Batch 6/64 loss: -1.0530452728271484
Batch 7/64 loss: -0.883392333984375
Batch 8/64 loss: -0.8909873962402344
Batch 9/64 loss: -0.7510137557983398
Batch 10/64 loss: -1.0057392120361328
Batch 11/64 loss: -0.6058025360107422
Batch 12/64 loss: -0.8514223098754883
Batch 13/64 loss: -0.9186849594116211
Batch 14/64 loss: 0.30838584899902344
Batch 15/64 loss: -0.06229400634765625
Batch 16/64 loss: -0.9089221954345703
Batch 17/64 loss: -0.4481525421142578
Batch 18/64 loss: -0.36641979217529297
Batch 19/64 loss: -1.0653800964355469
Batch 20/64 loss: -0.8278446197509766
Batch 21/64 loss: -0.829585075378418
Batch 22/64 loss: -0.9845104217529297
Batch 23/64 loss: -0.6588897705078125
Batch 24/64 loss: -0.5704593658447266
Batch 25/64 loss: -0.7237215042114258
Batch 26/64 loss: -0.8137531280517578
Batch 27/64 loss: -0.4782562255859375
Batch 28/64 loss: -0.6027498245239258
Batch 29/64 loss: -0.9920272827148438
Batch 30/64 loss: -0.980621337890625
Batch 31/64 loss: -0.7530679702758789
Batch 32/64 loss: -0.6501893997192383
Batch 33/64 loss: -0.27105045318603516
Batch 34/64 loss: -0.9893827438354492
Batch 35/64 loss: -0.5980968475341797
Batch 36/64 loss: -0.21228885650634766
Batch 37/64 loss: -0.44903087615966797
Batch 38/64 loss: -0.5777206420898438
Batch 39/64 loss: -0.9305000305175781
Batch 40/64 loss: -0.9120235443115234
Batch 41/64 loss: -0.2809934616088867
Batch 42/64 loss: 0.08752822875976562
Batch 43/64 loss: -0.5785140991210938
Batch 44/64 loss: -0.46279430389404297
Batch 45/64 loss: -0.6642427444458008
Batch 46/64 loss: -0.7663192749023438
Batch 47/64 loss: -0.5874080657958984
Batch 48/64 loss: -0.5341672897338867
Batch 49/64 loss: -0.6956071853637695
Batch 50/64 loss: -0.36693859100341797
Batch 51/64 loss: -0.3742990493774414
Batch 52/64 loss: -0.584503173828125
Batch 53/64 loss: -0.535792350769043
Batch 54/64 loss: -0.8283500671386719
Batch 55/64 loss: -0.791316032409668
Batch 56/64 loss: -0.8750038146972656
Batch 57/64 loss: -0.24504470825195312
Batch 58/64 loss: -0.6248302459716797
Batch 59/64 loss: -0.7712364196777344
Batch 60/64 loss: -0.6418237686157227
Batch 61/64 loss: -0.5492076873779297
Batch 62/64 loss: -0.5976858139038086
Batch 63/64 loss: -0.9540681838989258
Batch 64/64 loss: -4.5273566246032715
Epoch 342  Train loss: -0.6928325148189769  Val loss: -0.7944251253842488
Epoch 343
-------------------------------
Batch 1/64 loss: -0.7859659194946289
Batch 2/64 loss: -0.9102563858032227
Batch 3/64 loss: -0.8649024963378906
Batch 4/64 loss: -0.5752267837524414
Batch 5/64 loss: -0.6966466903686523
Batch 6/64 loss: -0.7211341857910156
Batch 7/64 loss: -0.4189453125
Batch 8/64 loss: -1.0182037353515625
Batch 9/64 loss: -0.564518928527832
Batch 10/64 loss: 0.13323307037353516
Batch 11/64 loss: -0.472259521484375
Batch 12/64 loss: -0.4696965217590332
Batch 13/64 loss: -0.9296188354492188
Batch 14/64 loss: -0.5438032150268555
Batch 15/64 loss: -0.5952787399291992
Batch 16/64 loss: -0.9818830490112305
Batch 17/64 loss: -0.7580051422119141
Batch 18/64 loss: -0.6777667999267578
Batch 19/64 loss: -0.887364387512207
Batch 20/64 loss: -0.8380889892578125
Batch 21/64 loss: -0.6737489700317383
Batch 22/64 loss: -0.36600208282470703
Batch 23/64 loss: -0.6232442855834961
Batch 24/64 loss: -0.9024524688720703
Batch 25/64 loss: -1.0304336547851562
Batch 26/64 loss: -0.28911304473876953
Batch 27/64 loss: -0.7784404754638672
Batch 28/64 loss: -0.9663934707641602
Batch 29/64 loss: -0.39681577682495117
Batch 30/64 loss: -1.072408676147461
Batch 31/64 loss: 0.033616065979003906
Batch 32/64 loss: -0.8997955322265625
Batch 33/64 loss: -0.6757917404174805
Batch 34/64 loss: -0.7478857040405273
Batch 35/64 loss: -0.9971685409545898
Batch 36/64 loss: 0.4216299057006836
Batch 37/64 loss: -0.7782106399536133
Batch 38/64 loss: -0.9677333831787109
Batch 39/64 loss: -0.8137807846069336
Batch 40/64 loss: -0.6572895050048828
Batch 41/64 loss: -0.8136453628540039
Batch 42/64 loss: -0.8899393081665039
Batch 43/64 loss: -0.9182548522949219
Batch 44/64 loss: -0.8039922714233398
Batch 45/64 loss: -0.902531623840332
Batch 46/64 loss: -0.5202665328979492
Batch 47/64 loss: -0.5501413345336914
Batch 48/64 loss: -0.8692045211791992
Batch 49/64 loss: -0.8047313690185547
Batch 50/64 loss: -0.9244289398193359
Batch 51/64 loss: -0.4664478302001953
Batch 52/64 loss: -1.0068788528442383
Batch 53/64 loss: -0.9139728546142578
Batch 54/64 loss: -0.7669191360473633
Batch 55/64 loss: -0.5509576797485352
Batch 56/64 loss: -0.7816505432128906
Batch 57/64 loss: -0.8834686279296875
Batch 58/64 loss: -1.0180778503417969
Batch 59/64 loss: -0.3292684555053711
Batch 60/64 loss: -0.8172760009765625
Batch 61/64 loss: 0.0517730712890625
Batch 62/64 loss: -0.7662448883056641
Batch 63/64 loss: -0.5177974700927734
Batch 64/64 loss: -4.678516387939453
Epoch 343  Train loss: -0.737741268382353  Val loss: -0.7880439627211528
Epoch 344
-------------------------------
Batch 1/64 loss: -0.6489095687866211
Batch 2/64 loss: -0.6075944900512695
Batch 3/64 loss: -0.8676729202270508
Batch 4/64 loss: -0.7724008560180664
Batch 5/64 loss: -0.9442300796508789
Batch 6/64 loss: -0.46109580993652344
Batch 7/64 loss: -0.5878400802612305
Batch 8/64 loss: -0.49945735931396484
Batch 9/64 loss: -0.3603630065917969
Batch 10/64 loss: -0.9907808303833008
Batch 11/64 loss: -0.9758930206298828
Batch 12/64 loss: -0.7128190994262695
Batch 13/64 loss: -0.2741374969482422
Batch 14/64 loss: 0.23322105407714844
Batch 15/64 loss: -0.6751432418823242
Batch 16/64 loss: -0.7906732559204102
Batch 17/64 loss: -0.5861682891845703
Batch 18/64 loss: 0.041796207427978516
Batch 19/64 loss: -0.48218250274658203
Batch 20/64 loss: -1.018874168395996
Batch 21/64 loss: -0.7249488830566406
Batch 22/64 loss: -0.10471391677856445
Batch 23/64 loss: -0.5749969482421875
Batch 24/64 loss: -0.261502742767334
Batch 25/64 loss: -0.8192119598388672
Batch 26/64 loss: -0.7757978439331055
Batch 27/64 loss: -0.9381570816040039
Batch 28/64 loss: -0.4208688735961914
Batch 29/64 loss: -0.6421403884887695
Batch 30/64 loss: -1.0260591506958008
Batch 31/64 loss: -0.4710988998413086
Batch 32/64 loss: -0.9094572067260742
Batch 33/64 loss: -0.49483489990234375
Batch 34/64 loss: -0.8182716369628906
Batch 35/64 loss: -0.8487997055053711
Batch 36/64 loss: -0.14609956741333008
Batch 37/64 loss: -0.4514808654785156
Batch 38/64 loss: -0.6551313400268555
Batch 39/64 loss: -1.021742820739746
Batch 40/64 loss: -0.6142587661743164
Batch 41/64 loss: -0.5672378540039062
Batch 42/64 loss: -1.0702838897705078
Batch 43/64 loss: -0.6503696441650391
Batch 44/64 loss: -0.8003778457641602
Batch 45/64 loss: -0.5860977172851562
Batch 46/64 loss: -0.5867033004760742
Batch 47/64 loss: -0.6593112945556641
Batch 48/64 loss: -0.5642299652099609
Batch 49/64 loss: -0.8152303695678711
Batch 50/64 loss: -0.9999017715454102
Batch 51/64 loss: -0.8997707366943359
Batch 52/64 loss: -0.72216796875
Batch 53/64 loss: -0.9359292984008789
Batch 54/64 loss: -0.8417587280273438
Batch 55/64 loss: -0.44924449920654297
Batch 56/64 loss: -0.7326903343200684
Batch 57/64 loss: -0.93072509765625
Batch 58/64 loss: -0.5388689041137695
Batch 59/64 loss: -0.6305780410766602
Batch 60/64 loss: -0.799738883972168
Batch 61/64 loss: -0.39702606201171875
Batch 62/64 loss: -0.8300619125366211
Batch 63/64 loss: -0.5636396408081055
Batch 64/64 loss: -5.108745574951172
Epoch 344  Train loss: -0.7075183569216261  Val loss: -0.8421710954908653
Epoch 345
-------------------------------
Batch 1/64 loss: -0.14045143127441406
Batch 2/64 loss: -0.7650623321533203
Batch 3/64 loss: -0.7520303726196289
Batch 4/64 loss: -0.7483062744140625
Batch 5/64 loss: -0.6837649345397949
Batch 6/64 loss: -0.20978450775146484
Batch 7/64 loss: -0.9812698364257812
Batch 8/64 loss: -0.8977413177490234
Batch 9/64 loss: -0.8067398071289062
Batch 10/64 loss: -0.4178171157836914
Batch 11/64 loss: -0.7827978134155273
Batch 12/64 loss: -0.8195409774780273
Batch 13/64 loss: -0.6124820709228516
Batch 14/64 loss: -0.8179616928100586
Batch 15/64 loss: -0.6545391082763672
Batch 16/64 loss: -0.4860820770263672
Batch 17/64 loss: -0.7206745147705078
Batch 18/64 loss: -0.8477392196655273
Batch 19/64 loss: -0.46839427947998047
Batch 20/64 loss: -0.653040885925293
Batch 21/64 loss: -0.904118537902832
Batch 22/64 loss: -0.29385948181152344
Batch 23/64 loss: -0.6720981597900391
Batch 24/64 loss: -0.3459491729736328
Batch 25/64 loss: -0.8956146240234375
Batch 26/64 loss: -0.6369495391845703
Batch 27/64 loss: -0.9108572006225586
Batch 28/64 loss: -0.7282638549804688
Batch 29/64 loss: -0.4485511779785156
Batch 30/64 loss: -0.33928394317626953
Batch 31/64 loss: -0.7903194427490234
Batch 32/64 loss: -0.42731475830078125
Batch 33/64 loss: -0.06323814392089844
Batch 34/64 loss: -0.7751750946044922
Batch 35/64 loss: -0.6270599365234375
Batch 36/64 loss: -1.095327377319336
Batch 37/64 loss: -0.5224285125732422
Batch 38/64 loss: -0.9601383209228516
Batch 39/64 loss: -0.9463624954223633
Batch 40/64 loss: -0.8829078674316406
Batch 41/64 loss: -0.28995704650878906
Batch 42/64 loss: -1.0315399169921875
Batch 43/64 loss: -0.9955072402954102
Batch 44/64 loss: -0.7284164428710938
Batch 45/64 loss: -0.6379270553588867
Batch 46/64 loss: -1.038111686706543
Batch 47/64 loss: -0.9510087966918945
Batch 48/64 loss: -0.6821117401123047
Batch 49/64 loss: -0.5314226150512695
Batch 50/64 loss: -0.6313772201538086
Batch 51/64 loss: -0.7915773391723633
Batch 52/64 loss: -0.5794095993041992
Batch 53/64 loss: -0.7144279479980469
Batch 54/64 loss: -0.855107307434082
Batch 55/64 loss: -0.9155292510986328
Batch 56/64 loss: -0.9773540496826172
Batch 57/64 loss: -0.6032342910766602
Batch 58/64 loss: -0.7981452941894531
Batch 59/64 loss: -0.6770153045654297
Batch 60/64 loss: -0.7514352798461914
Batch 61/64 loss: -0.6074075698852539
Batch 62/64 loss: -1.0252723693847656
Batch 63/64 loss: -0.6558494567871094
Batch 64/64 loss: -4.530241012573242
Epoch 345  Train loss: -0.743511588900697  Val loss: -0.8619522802608529
Epoch 346
-------------------------------
Batch 1/64 loss: -0.32813549041748047
Batch 2/64 loss: -0.8889446258544922
Batch 3/64 loss: -0.7085657119750977
Batch 4/64 loss: -0.8145627975463867
Batch 5/64 loss: -1.0417327880859375
Batch 6/64 loss: -0.6982946395874023
Batch 7/64 loss: -0.9771814346313477
Batch 8/64 loss: -0.7988557815551758
Batch 9/64 loss: -0.8068704605102539
Batch 10/64 loss: -0.803558349609375
Batch 11/64 loss: -0.7825288772583008
Batch 12/64 loss: -0.3493776321411133
Batch 13/64 loss: -0.8807029724121094
Batch 14/64 loss: -0.7528581619262695
Batch 15/64 loss: -0.6866912841796875
Batch 16/64 loss: -0.8801078796386719
Batch 17/64 loss: -0.8192138671875
Batch 18/64 loss: -0.47112083435058594
Batch 19/64 loss: -0.7789688110351562
Batch 20/64 loss: 0.24451828002929688
Batch 21/64 loss: -0.8306598663330078
Batch 22/64 loss: -0.5054092407226562
Batch 23/64 loss: -0.6077022552490234
Batch 24/64 loss: -0.8089208602905273
Batch 25/64 loss: -0.7262153625488281
Batch 26/64 loss: -1.1546239852905273
Batch 27/64 loss: -0.5937795639038086
Batch 28/64 loss: -0.9461994171142578
Batch 29/64 loss: -0.8040618896484375
Batch 30/64 loss: -0.856471061706543
Batch 31/64 loss: -0.9944419860839844
Batch 32/64 loss: -0.7512121200561523
Batch 33/64 loss: -0.3856468200683594
Batch 34/64 loss: -0.9679317474365234
Batch 35/64 loss: -0.7642240524291992
Batch 36/64 loss: -0.6282262802124023
Batch 37/64 loss: -0.8853273391723633
Batch 38/64 loss: -0.8281974792480469
Batch 39/64 loss: -0.6902551651000977
Batch 40/64 loss: -0.5229549407958984
Batch 41/64 loss: -0.8346529006958008
Batch 42/64 loss: -0.42995166778564453
Batch 43/64 loss: -0.7439470291137695
Batch 44/64 loss: -0.30405521392822266
Batch 45/64 loss: -0.7571306228637695
Batch 46/64 loss: -0.7341346740722656
Batch 47/64 loss: -0.8819894790649414
Batch 48/64 loss: -0.5568037033081055
Batch 49/64 loss: -0.3251533508300781
Batch 50/64 loss: -0.4197254180908203
Batch 51/64 loss: -0.8424921035766602
Batch 52/64 loss: -0.29386234283447266
Batch 53/64 loss: -1.040776252746582
Batch 54/64 loss: -0.9190540313720703
Batch 55/64 loss: -0.8436861038208008
Batch 56/64 loss: -0.6691474914550781
Batch 57/64 loss: -0.003921031951904297
Batch 58/64 loss: -0.5354499816894531
Batch 59/64 loss: -0.6296854019165039
Batch 60/64 loss: -0.7392911911010742
Batch 61/64 loss: -0.8950901031494141
Batch 62/64 loss: -0.6861276626586914
Batch 63/64 loss: -0.8061199188232422
Batch 64/64 loss: -4.371860504150391
Epoch 346  Train loss: -0.7442722918940525  Val loss: -0.8363230072755584
Epoch 347
-------------------------------
Batch 1/64 loss: -0.9335823059082031
Batch 2/64 loss: -0.7544803619384766
Batch 3/64 loss: -0.8178930282592773
Batch 4/64 loss: -0.7202730178833008
Batch 5/64 loss: -0.6767845153808594
Batch 6/64 loss: -0.9396190643310547
Batch 7/64 loss: -0.5928163528442383
Batch 8/64 loss: -0.14401483535766602
Batch 9/64 loss: -0.986699104309082
Batch 10/64 loss: -0.6385498046875
Batch 11/64 loss: -1.0324506759643555
Batch 12/64 loss: -0.5076580047607422
Batch 13/64 loss: -0.7647724151611328
Batch 14/64 loss: -1.000274658203125
Batch 15/64 loss: -0.7480754852294922
Batch 16/64 loss: -0.7768325805664062
Batch 17/64 loss: -1.0312871932983398
Batch 18/64 loss: -0.5005483627319336
Batch 19/64 loss: -0.49738597869873047
Batch 20/64 loss: -0.45085620880126953
Batch 21/64 loss: -0.20036697387695312
Batch 22/64 loss: 0.04537343978881836
Batch 23/64 loss: -0.7790288925170898
Batch 24/64 loss: -0.5348777770996094
Batch 25/64 loss: -0.9947223663330078
Batch 26/64 loss: -0.9412832260131836
Batch 27/64 loss: -0.8937501907348633
Batch 28/64 loss: -0.827174186706543
Batch 29/64 loss: -0.9105863571166992
Batch 30/64 loss: -0.6861915588378906
Batch 31/64 loss: -0.7255992889404297
Batch 32/64 loss: -0.8719711303710938
Batch 33/64 loss: -0.8739776611328125
Batch 34/64 loss: -0.6422204971313477
Batch 35/64 loss: -0.5131998062133789
Batch 36/64 loss: -0.5521945953369141
Batch 37/64 loss: -0.9352045059204102
Batch 38/64 loss: -0.5931124687194824
Batch 39/64 loss: -0.7351760864257812
Batch 40/64 loss: -0.5856618881225586
Batch 41/64 loss: -0.8787550926208496
Batch 42/64 loss: -0.8644742965698242
Batch 43/64 loss: -1.034322738647461
Batch 44/64 loss: -0.5697307586669922
Batch 45/64 loss: -0.7121486663818359
Batch 46/64 loss: -0.8753185272216797
Batch 47/64 loss: -0.6152019500732422
Batch 48/64 loss: -0.6313838958740234
Batch 49/64 loss: -0.903956413269043
Batch 50/64 loss: -0.7278652191162109
Batch 51/64 loss: -0.6833763122558594
Batch 52/64 loss: -0.47415733337402344
Batch 53/64 loss: -0.48041439056396484
Batch 54/64 loss: -0.5870389938354492
Batch 55/64 loss: -0.7649822235107422
Batch 56/64 loss: -0.8392496109008789
Batch 57/64 loss: -0.872981071472168
Batch 58/64 loss: -0.7106294631958008
Batch 59/64 loss: -0.7586450576782227
Batch 60/64 loss: -0.8405437469482422
Batch 61/64 loss: -1.0585956573486328
Batch 62/64 loss: -0.1390819549560547
Batch 63/64 loss: -0.8020706176757812
Batch 64/64 loss: -4.544043064117432
Epoch 347  Train loss: -0.7607644903893565  Val loss: -0.8396978869880598
Epoch 348
-------------------------------
Batch 1/64 loss: -0.11385631561279297
Batch 2/64 loss: -0.7413711547851562
Batch 3/64 loss: -0.6062698364257812
Batch 4/64 loss: -0.8795108795166016
Batch 5/64 loss: -0.8456716537475586
Batch 6/64 loss: -0.9128532409667969
Batch 7/64 loss: -0.4076881408691406
Batch 8/64 loss: -0.7086925506591797
Batch 9/64 loss: -0.1277179718017578
Batch 10/64 loss: -0.7392997741699219
Batch 11/64 loss: -0.8297386169433594
Batch 12/64 loss: -0.8420429229736328
Batch 13/64 loss: -0.42680931091308594
Batch 14/64 loss: -0.5598325729370117
Batch 15/64 loss: -0.6419963836669922
Batch 16/64 loss: -0.8051576614379883
Batch 17/64 loss: -0.8806962966918945
Batch 18/64 loss: -0.7382907867431641
Batch 19/64 loss: -0.850255012512207
Batch 20/64 loss: -0.6532516479492188
Batch 21/64 loss: -0.4266500473022461
Batch 22/64 loss: -0.6289138793945312
Batch 23/64 loss: -0.37479209899902344
Batch 24/64 loss: -0.8837451934814453
Batch 25/64 loss: -1.1480274200439453
Batch 26/64 loss: -0.9443235397338867
Batch 27/64 loss: 0.06274223327636719
Batch 28/64 loss: -0.9327411651611328
Batch 29/64 loss: -0.7482452392578125
Batch 30/64 loss: -0.9795103073120117
Batch 31/64 loss: -0.7699317932128906
Batch 32/64 loss: -0.5776858329772949
Batch 33/64 loss: 0.0766453742980957
Batch 34/64 loss: -0.8073110580444336
Batch 35/64 loss: -0.795654296875
Batch 36/64 loss: -0.47420692443847656
Batch 37/64 loss: -0.8776960372924805
Batch 38/64 loss: -0.731511116027832
Batch 39/64 loss: -0.9293289184570312
Batch 40/64 loss: -0.8558359146118164
Batch 41/64 loss: -0.6527872085571289
Batch 42/64 loss: -0.8928050994873047
Batch 43/64 loss: -1.0702600479125977
Batch 44/64 loss: -0.8779973983764648
Batch 45/64 loss: -0.9169187545776367
Batch 46/64 loss: -0.6110897064208984
Batch 47/64 loss: -0.7421607971191406
Batch 48/64 loss: -0.6359243392944336
Batch 49/64 loss: -1.0082988739013672
Batch 50/64 loss: -0.6988306045532227
Batch 51/64 loss: 0.16478824615478516
Batch 52/64 loss: -0.8933658599853516
Batch 53/64 loss: -0.6638984680175781
Batch 54/64 loss: -0.9468345642089844
Batch 55/64 loss: -1.0721187591552734
Batch 56/64 loss: -0.6524314880371094
Batch 57/64 loss: -0.9987316131591797
Batch 58/64 loss: -0.8596658706665039
Batch 59/64 loss: -0.6740407943725586
Batch 60/64 loss: -0.32967376708984375
Batch 61/64 loss: -1.1588354110717773
Batch 62/64 loss: -0.9182577133178711
Batch 63/64 loss: -0.4488372802734375
Batch 64/64 loss: -4.768734455108643
Epoch 348  Train loss: -0.7559725948408538  Val loss: -0.7891258685449555
Epoch 349
-------------------------------
Batch 1/64 loss: -0.889531135559082
Batch 2/64 loss: -0.33247947692871094
Batch 3/64 loss: -0.3815574645996094
Batch 4/64 loss: -0.5318098068237305
Batch 5/64 loss: -0.8644580841064453
Batch 6/64 loss: -0.4240713119506836
Batch 7/64 loss: -0.7506637573242188
Batch 8/64 loss: -0.8302536010742188
Batch 9/64 loss: -0.9313488006591797
Batch 10/64 loss: -0.9794349670410156
Batch 11/64 loss: -0.8332805633544922
Batch 12/64 loss: -0.8737545013427734
Batch 13/64 loss: -1.0057716369628906
Batch 14/64 loss: -0.06046772003173828
Batch 15/64 loss: -0.3836650848388672
Batch 16/64 loss: -0.8628654479980469
Batch 17/64 loss: -0.9138460159301758
Batch 18/64 loss: -1.0161609649658203
Batch 19/64 loss: -0.11945438385009766
Batch 20/64 loss: -0.4021730422973633
Batch 21/64 loss: -0.6727581024169922
Batch 22/64 loss: -0.6675453186035156
Batch 23/64 loss: -0.5669689178466797
Batch 24/64 loss: -0.9954519271850586
Batch 25/64 loss: -0.8632888793945312
Batch 26/64 loss: -0.4435548782348633
Batch 27/64 loss: -0.8206090927124023
Batch 28/64 loss: -0.8500032424926758
Batch 29/64 loss: -0.9638080596923828
Batch 30/64 loss: -0.9098176956176758
Batch 31/64 loss: -0.6854629516601562
Batch 32/64 loss: -0.7087516784667969
Batch 33/64 loss: -0.9481754302978516
Batch 34/64 loss: -0.9836091995239258
Batch 35/64 loss: -0.41367626190185547
Batch 36/64 loss: -0.789769172668457
Batch 37/64 loss: -0.7383661270141602
Batch 38/64 loss: -0.5577621459960938
Batch 39/64 loss: -0.46598243713378906
Batch 40/64 loss: -0.8529586791992188
Batch 41/64 loss: -0.32462596893310547
Batch 42/64 loss: -0.747288703918457
Batch 43/64 loss: -0.9119682312011719
Batch 44/64 loss: -0.6816577911376953
Batch 45/64 loss: -0.4761390686035156
Batch 46/64 loss: -0.974609375
Batch 47/64 loss: -0.943389892578125
Batch 48/64 loss: -0.7002725601196289
Batch 49/64 loss: -0.7701911926269531
Batch 50/64 loss: -0.08648204803466797
Batch 51/64 loss: -0.7369794845581055
Batch 52/64 loss: -0.786076545715332
Batch 53/64 loss: -0.8001480102539062
Batch 54/64 loss: -0.7055330276489258
Batch 55/64 loss: -0.826263427734375
Batch 56/64 loss: -0.3852109909057617
Batch 57/64 loss: -0.12447404861450195
Batch 58/64 loss: -0.672337532043457
Batch 59/64 loss: -0.8630123138427734
Batch 60/64 loss: -0.8361358642578125
Batch 61/64 loss: -0.5715827941894531
Batch 62/64 loss: -0.5460119247436523
Batch 63/64 loss: -0.8248453140258789
Batch 64/64 loss: -4.717130661010742
Epoch 349  Train loss: -0.7391129736806832  Val loss: -0.8531608057185956
Epoch 350
-------------------------------
Batch 1/64 loss: -1.0675106048583984
Batch 2/64 loss: -0.815455436706543
Batch 3/64 loss: -0.8953742980957031
Batch 4/64 loss: -0.8308801651000977
Batch 5/64 loss: -0.6974344253540039
Batch 6/64 loss: -0.8114585876464844
Batch 7/64 loss: -0.3430366516113281
Batch 8/64 loss: -0.6913375854492188
Batch 9/64 loss: -0.9915800094604492
Batch 10/64 loss: -0.9939479827880859
Batch 11/64 loss: -0.971562385559082
Batch 12/64 loss: -0.8485965728759766
Batch 13/64 loss: -0.697941780090332
Batch 14/64 loss: -0.6716442108154297
Batch 15/64 loss: -1.0337677001953125
Batch 16/64 loss: -0.7865085601806641
Batch 17/64 loss: -0.9554338455200195
Batch 18/64 loss: -0.6017513275146484
Batch 19/64 loss: -0.875065803527832
Batch 20/64 loss: -0.4044923782348633
Batch 21/64 loss: -0.3872690200805664
Batch 22/64 loss: -0.7011280059814453
Batch 23/64 loss: -0.9013071060180664
Batch 24/64 loss: -0.6117391586303711
Batch 25/64 loss: -0.79400634765625
Batch 26/64 loss: -0.5743989944458008
Batch 27/64 loss: -0.7431936264038086
Batch 28/64 loss: -0.47528839111328125
Batch 29/64 loss: -0.7127790451049805
Batch 30/64 loss: -0.879033088684082
Batch 31/64 loss: -0.48387718200683594
Batch 32/64 loss: -0.8759164810180664
Batch 33/64 loss: -0.8524713516235352
Batch 34/64 loss: -0.8663225173950195
Batch 35/64 loss: -0.40796566009521484
Batch 36/64 loss: -0.9405040740966797
Batch 37/64 loss: -0.6901540756225586
Batch 38/64 loss: -0.6741104125976562
Batch 39/64 loss: -0.7383298873901367
Batch 40/64 loss: -0.9564800262451172
Batch 41/64 loss: -0.9831199645996094
Batch 42/64 loss: -0.8013029098510742
Batch 43/64 loss: -0.8071880340576172
Batch 44/64 loss: -0.6923675537109375
Batch 45/64 loss: -1.1049861907958984
Batch 46/64 loss: -0.5784320831298828
Batch 47/64 loss: -0.3051166534423828
Batch 48/64 loss: -0.9714221954345703
Batch 49/64 loss: -0.3730659484863281
Batch 50/64 loss: -0.7089996337890625
Batch 51/64 loss: -0.309478759765625
Batch 52/64 loss: -0.6437540054321289
Batch 53/64 loss: -1.1656084060668945
Batch 54/64 loss: -0.5762081146240234
Batch 55/64 loss: -0.2840127944946289
Batch 56/64 loss: -0.22348880767822266
Batch 57/64 loss: -0.4971928596496582
Batch 58/64 loss: -0.3445262908935547
Batch 59/64 loss: -0.8974056243896484
Batch 60/64 loss: -0.8092546463012695
Batch 61/64 loss: -0.5277867317199707
Batch 62/64 loss: -1.034719467163086
Batch 63/64 loss: -0.9267311096191406
Batch 64/64 loss: -4.6884541511535645
Epoch 350  Train loss: -0.7738598299961464  Val loss: -0.7993870240306526
Epoch 351
-------------------------------
Batch 1/64 loss: -0.7846279144287109
Batch 2/64 loss: -0.7118301391601562
Batch 3/64 loss: -0.7953252792358398
Batch 4/64 loss: -0.7463798522949219
Batch 5/64 loss: -0.8470287322998047
Batch 6/64 loss: -0.8628225326538086
Batch 7/64 loss: -0.7575893402099609
Batch 8/64 loss: -0.6517906188964844
Batch 9/64 loss: -0.5985317230224609
Batch 10/64 loss: -0.8070821762084961
Batch 11/64 loss: -0.8429174423217773
Batch 12/64 loss: -0.7749042510986328
Batch 13/64 loss: -0.6835880279541016
Batch 14/64 loss: -1.0739402770996094
Batch 15/64 loss: -0.8559808731079102
Batch 16/64 loss: -0.693903923034668
Batch 17/64 loss: -0.8917875289916992
Batch 18/64 loss: -0.2118358612060547
Batch 19/64 loss: -0.7933616638183594
Batch 20/64 loss: -0.40871143341064453
Batch 21/64 loss: -0.8026037216186523
Batch 22/64 loss: -0.8234491348266602
Batch 23/64 loss: -0.8076715469360352
Batch 24/64 loss: -0.7567844390869141
Batch 25/64 loss: -0.634852409362793
Batch 26/64 loss: -0.7827787399291992
Batch 27/64 loss: -0.9664173126220703
Batch 28/64 loss: -0.5519962310791016
Batch 29/64 loss: -0.8758583068847656
Batch 30/64 loss: -0.7713727951049805
Batch 31/64 loss: -0.05670356750488281
Batch 32/64 loss: -0.633880615234375
Batch 33/64 loss: -0.7699146270751953
Batch 34/64 loss: -0.24473190307617188
Batch 35/64 loss: -0.6492414474487305
Batch 36/64 loss: -0.7682094573974609
Batch 37/64 loss: -0.2742443084716797
Batch 38/64 loss: -0.8913421630859375
Batch 39/64 loss: 0.09394025802612305
Batch 40/64 loss: -0.401120662689209
Batch 41/64 loss: -0.8310337066650391
Batch 42/64 loss: -0.4005098342895508
Batch 43/64 loss: -1.018052101135254
Batch 44/64 loss: -0.8440475463867188
Batch 45/64 loss: -1.0813169479370117
Batch 46/64 loss: -0.9358835220336914
Batch 47/64 loss: -0.5932979583740234
Batch 48/64 loss: 0.1042165756225586
Batch 49/64 loss: -0.5643701553344727
Batch 50/64 loss: -0.7577219009399414
Batch 51/64 loss: -0.5559282302856445
Batch 52/64 loss: -0.28753662109375
Batch 53/64 loss: -0.8250608444213867
Batch 54/64 loss: -0.918299674987793
Batch 55/64 loss: -0.07374095916748047
Batch 56/64 loss: -0.7573614120483398
Batch 57/64 loss: -0.9090118408203125
Batch 58/64 loss: -0.7797708511352539
Batch 59/64 loss: -0.9687738418579102
Batch 60/64 loss: -0.3625020980834961
Batch 61/64 loss: -1.0984668731689453
Batch 62/64 loss: -0.979680061340332
Batch 63/64 loss: -0.5043630599975586
Batch 64/64 loss: -4.583430290222168
Epoch 351  Train loss: -0.7300903357711492  Val loss: -0.8585558494751396
Epoch 352
-------------------------------
Batch 1/64 loss: -0.954380989074707
Batch 2/64 loss: -0.9726285934448242
Batch 3/64 loss: -0.3491039276123047
Batch 4/64 loss: -0.9190292358398438
Batch 5/64 loss: -0.9906101226806641
Batch 6/64 loss: -0.8628835678100586
Batch 7/64 loss: -0.29117679595947266
Batch 8/64 loss: -0.6532506942749023
Batch 9/64 loss: -0.8221969604492188
Batch 10/64 loss: 0.06899547576904297
Batch 11/64 loss: -0.7233428955078125
Batch 12/64 loss: -0.4070405960083008
Batch 13/64 loss: -0.6556472778320312
Batch 14/64 loss: -0.7029762268066406
Batch 15/64 loss: -0.6292533874511719
Batch 16/64 loss: -0.8481845855712891
Batch 17/64 loss: -0.4652261734008789
Batch 18/64 loss: -0.7452278137207031
Batch 19/64 loss: -0.4742298126220703
Batch 20/64 loss: -0.3936119079589844
Batch 21/64 loss: -0.7059116363525391
Batch 22/64 loss: -0.07700920104980469
Batch 23/64 loss: -0.7349157333374023
Batch 24/64 loss: -0.3191394805908203
Batch 25/64 loss: -0.8010234832763672
Batch 26/64 loss: -0.6099748611450195
Batch 27/64 loss: -0.6065654754638672
Batch 28/64 loss: -0.16141033172607422
Batch 29/64 loss: -0.697540283203125
Batch 30/64 loss: -0.8617773056030273
Batch 31/64 loss: -0.9572324752807617
Batch 32/64 loss: -0.6046857833862305
Batch 33/64 loss: 0.012790203094482422
Batch 34/64 loss: -0.7961931228637695
Batch 35/64 loss: -0.35846424102783203
Batch 36/64 loss: -0.18949127197265625
Batch 37/64 loss: -0.7744636535644531
Batch 38/64 loss: -0.8850126266479492
Batch 39/64 loss: -0.8958530426025391
Batch 40/64 loss: -0.7494306564331055
Batch 41/64 loss: -0.5543813705444336
Batch 42/64 loss: -0.5039796829223633
Batch 43/64 loss: -0.4734468460083008
Batch 44/64 loss: -0.26323986053466797
Batch 45/64 loss: -0.5329675674438477
Batch 46/64 loss: -0.6880254745483398
Batch 47/64 loss: -0.6833686828613281
Batch 48/64 loss: -0.7891473770141602
Batch 49/64 loss: -0.8616209030151367
Batch 50/64 loss: -0.85491943359375
Batch 51/64 loss: -0.8925743103027344
Batch 52/64 loss: -0.7071819305419922
Batch 53/64 loss: -0.9277448654174805
Batch 54/64 loss: -0.8057889938354492
Batch 55/64 loss: -0.7759084701538086
Batch 56/64 loss: 0.11317586898803711
Batch 57/64 loss: -0.49138545989990234
Batch 58/64 loss: -0.58050537109375
Batch 59/64 loss: -0.4594917297363281
Batch 60/64 loss: -1.0538148880004883
Batch 61/64 loss: -0.783233642578125
Batch 62/64 loss: -0.7738456726074219
Batch 63/64 loss: -0.7845211029052734
Batch 64/64 loss: -4.528903007507324
Epoch 352  Train loss: -0.6759044011433919  Val loss: -0.699674953709763
Epoch 353
-------------------------------
Batch 1/64 loss: -0.7440729141235352
Batch 2/64 loss: -0.31626415252685547
Batch 3/64 loss: -0.7896041870117188
Batch 4/64 loss: -0.8348979949951172
Batch 5/64 loss: -0.5642156600952148
Batch 6/64 loss: -0.31749629974365234
Batch 7/64 loss: -0.5509805679321289
Batch 8/64 loss: -0.8875083923339844
Batch 9/64 loss: -0.832000732421875
Batch 10/64 loss: -0.3752918243408203
Batch 11/64 loss: -0.4699554443359375
Batch 12/64 loss: -0.6543292999267578
Batch 13/64 loss: -0.7094001770019531
Batch 14/64 loss: -0.2911806106567383
Batch 15/64 loss: -0.8816204071044922
Batch 16/64 loss: 0.005208015441894531
Batch 17/64 loss: -0.5723800659179688
Batch 18/64 loss: -0.6689767837524414
Batch 19/64 loss: -0.5515604019165039
Batch 20/64 loss: 0.5169916152954102
Batch 21/64 loss: -0.7543001174926758
Batch 22/64 loss: -0.7639360427856445
Batch 23/64 loss: -0.7882184982299805
Batch 24/64 loss: -0.8664340972900391
Batch 25/64 loss: -0.9076080322265625
Batch 26/64 loss: -0.864171028137207
Batch 27/64 loss: -0.4351053237915039
Batch 28/64 loss: -0.39014339447021484
Batch 29/64 loss: -0.9550905227661133
Batch 30/64 loss: -0.6228809356689453
Batch 31/64 loss: -0.7100610733032227
Batch 32/64 loss: -0.7294206619262695
Batch 33/64 loss: -0.8202762603759766
Batch 34/64 loss: -0.8966522216796875
Batch 35/64 loss: -0.6747913360595703
Batch 36/64 loss: -0.5536394119262695
Batch 37/64 loss: -0.7218418121337891
Batch 38/64 loss: -0.9562950134277344
Batch 39/64 loss: -0.9953517913818359
Batch 40/64 loss: -0.3668251037597656
Batch 41/64 loss: -1.0361747741699219
Batch 42/64 loss: -0.8826313018798828
Batch 43/64 loss: -0.5173139572143555
Batch 44/64 loss: -0.7541780471801758
Batch 45/64 loss: -0.7947607040405273
Batch 46/64 loss: -0.9750204086303711
Batch 47/64 loss: -0.708155632019043
Batch 48/64 loss: -0.7339839935302734
Batch 49/64 loss: -0.9143762588500977
Batch 50/64 loss: -0.803131103515625
Batch 51/64 loss: -0.4076967239379883
Batch 52/64 loss: -0.6944694519042969
Batch 53/64 loss: -0.15370845794677734
Batch 54/64 loss: -0.5749702453613281
Batch 55/64 loss: -0.2845001220703125
Batch 56/64 loss: -0.9983854293823242
Batch 57/64 loss: -0.7970542907714844
Batch 58/64 loss: -0.9825229644775391
Batch 59/64 loss: -0.6456117630004883
Batch 60/64 loss: -0.6297607421875
Batch 61/64 loss: -0.5501689910888672
Batch 62/64 loss: -0.1420154571533203
Batch 63/64 loss: -0.7766962051391602
Batch 64/64 loss: -4.032309532165527
Epoch 353  Train loss: -0.6908878139421052  Val loss: -0.7489047230723798
Epoch 354
-------------------------------
Batch 1/64 loss: -1.0081043243408203
Batch 2/64 loss: -0.6365394592285156
Batch 3/64 loss: -0.8114242553710938
Batch 4/64 loss: -0.8162441253662109
Batch 5/64 loss: -0.5882501602172852
Batch 6/64 loss: -0.7122411727905273
Batch 7/64 loss: -0.4633150100708008
Batch 8/64 loss: -1.010869026184082
Batch 9/64 loss: -0.44766998291015625
Batch 10/64 loss: -0.8589382171630859
Batch 11/64 loss: -0.7402629852294922
Batch 12/64 loss: -0.9608879089355469
Batch 13/64 loss: -0.8064804077148438
Batch 14/64 loss: -0.4740457534790039
Batch 15/64 loss: -0.5753164291381836
Batch 16/64 loss: -0.45358943939208984
Batch 17/64 loss: -0.4457368850708008
Batch 18/64 loss: -0.5675029754638672
Batch 19/64 loss: -0.48310089111328125
Batch 20/64 loss: -0.14493274688720703
Batch 21/64 loss: -0.882725715637207
Batch 22/64 loss: -0.8338003158569336
Batch 23/64 loss: -0.8646116256713867
Batch 24/64 loss: -0.9129295349121094
Batch 25/64 loss: -0.43210697174072266
Batch 26/64 loss: -0.9140100479125977
Batch 27/64 loss: -0.4590129852294922
Batch 28/64 loss: -0.41601085662841797
Batch 29/64 loss: -0.5510482788085938
Batch 30/64 loss: -0.629826545715332
Batch 31/64 loss: -0.830927848815918
Batch 32/64 loss: -0.5345325469970703
Batch 33/64 loss: -0.5361833572387695
Batch 34/64 loss: -0.7016973495483398
Batch 35/64 loss: -0.5969390869140625
Batch 36/64 loss: -0.5994129180908203
Batch 37/64 loss: -0.5416631698608398
Batch 38/64 loss: -0.556401252746582
Batch 39/64 loss: -0.8064212799072266
Batch 40/64 loss: -0.8634328842163086
Batch 41/64 loss: -0.6431112289428711
Batch 42/64 loss: -0.5765600204467773
Batch 43/64 loss: -0.3227052688598633
Batch 44/64 loss: -0.2083606719970703
Batch 45/64 loss: -0.8244600296020508
Batch 46/64 loss: -1.0588998794555664
Batch 47/64 loss: -0.9408330917358398
Batch 48/64 loss: -1.038590431213379
Batch 49/64 loss: -0.668971061706543
Batch 50/64 loss: -0.3967914581298828
Batch 51/64 loss: -0.8810310363769531
Batch 52/64 loss: -0.8462705612182617
Batch 53/64 loss: -0.39986324310302734
Batch 54/64 loss: -0.9929189682006836
Batch 55/64 loss: -1.1124095916748047
Batch 56/64 loss: -0.9558639526367188
Batch 57/64 loss: -0.1921243667602539
Batch 58/64 loss: -0.7098550796508789
Batch 59/64 loss: -0.8356447219848633
Batch 60/64 loss: -0.7045621871948242
Batch 61/64 loss: -0.34710693359375
Batch 62/64 loss: -0.6924171447753906
Batch 63/64 loss: -1.053213119506836
Batch 64/64 loss: -4.750275135040283
Epoch 354  Train loss: -0.7283830137813793  Val loss: -0.8058207928110234
Epoch 355
-------------------------------
Batch 1/64 loss: -0.8138647079467773
Batch 2/64 loss: -1.0978193283081055
Batch 3/64 loss: -0.6951704025268555
Batch 4/64 loss: -0.5841999053955078
Batch 5/64 loss: -1.065810203552246
Batch 6/64 loss: -0.8836698532104492
Batch 7/64 loss: -0.46424198150634766
Batch 8/64 loss: -0.7188854217529297
Batch 9/64 loss: -0.5408716201782227
Batch 10/64 loss: -0.1667633056640625
Batch 11/64 loss: -0.8395967483520508
Batch 12/64 loss: -0.12378692626953125
Batch 13/64 loss: -0.29811573028564453
Batch 14/64 loss: -0.9312295913696289
Batch 15/64 loss: -0.6625576019287109
Batch 16/64 loss: -0.8907251358032227
Batch 17/64 loss: -0.43521976470947266
Batch 18/64 loss: -0.6270256042480469
Batch 19/64 loss: -0.5427560806274414
Batch 20/64 loss: -0.5665493011474609
Batch 21/64 loss: -0.8971385955810547
Batch 22/64 loss: -0.6067848205566406
Batch 23/64 loss: -1.0151786804199219
Batch 24/64 loss: -0.8193721771240234
Batch 25/64 loss: -0.6180849075317383
Batch 26/64 loss: -0.5507326126098633
Batch 27/64 loss: -0.777592658996582
Batch 28/64 loss: -0.4531583786010742
Batch 29/64 loss: -0.7776708602905273
Batch 30/64 loss: -0.6224269866943359
Batch 31/64 loss: 0.06870174407958984
Batch 32/64 loss: -0.7146291732788086
Batch 33/64 loss: -0.8823871612548828
Batch 34/64 loss: -0.6742525100708008
Batch 35/64 loss: -0.9092721939086914
Batch 36/64 loss: -0.8409719467163086
Batch 37/64 loss: -1.0460052490234375
Batch 38/64 loss: -0.4450044631958008
Batch 39/64 loss: -0.7321109771728516
Batch 40/64 loss: -0.6699371337890625
Batch 41/64 loss: -0.7929439544677734
Batch 42/64 loss: -0.7672033309936523
Batch 43/64 loss: -0.5698909759521484
Batch 44/64 loss: -0.5019006729125977
Batch 45/64 loss: -0.5830450057983398
Batch 46/64 loss: -0.5630569458007812
Batch 47/64 loss: -0.6996231079101562
Batch 48/64 loss: -0.6048526763916016
Batch 49/64 loss: -1.022080421447754
Batch 50/64 loss: -1.1188812255859375
Batch 51/64 loss: -0.36301088333129883
Batch 52/64 loss: -0.9809751510620117
Batch 53/64 loss: -0.6957559585571289
Batch 54/64 loss: -0.16619300842285156
Batch 55/64 loss: -0.9269351959228516
Batch 56/64 loss: -0.7317953109741211
Batch 57/64 loss: -0.6894779205322266
Batch 58/64 loss: -0.8010454177856445
Batch 59/64 loss: -0.9790277481079102
Batch 60/64 loss: -0.584477424621582
Batch 61/64 loss: -0.7416706085205078
Batch 62/64 loss: -0.6084003448486328
Batch 63/64 loss: -0.4887423515319824
Batch 64/64 loss: -4.699676990509033
Epoch 355  Train loss: -0.7284488210491106  Val loss: -0.8650614158394411
Epoch 356
-------------------------------
Batch 1/64 loss: -0.21131277084350586
Batch 2/64 loss: -0.8301334381103516
Batch 3/64 loss: -0.7474918365478516
Batch 4/64 loss: -1.0324783325195312
Batch 5/64 loss: -0.0988321304321289
Batch 6/64 loss: -0.6018457412719727
Batch 7/64 loss: -1.1073169708251953
Batch 8/64 loss: -0.8255157470703125
Batch 9/64 loss: -0.37558460235595703
Batch 10/64 loss: -0.9152908325195312
Batch 11/64 loss: -0.18762969970703125
Batch 12/64 loss: -0.9609756469726562
Batch 13/64 loss: -0.3928251266479492
Batch 14/64 loss: -0.9543123245239258
Batch 15/64 loss: -0.6502323150634766
Batch 16/64 loss: -0.43126583099365234
Batch 17/64 loss: -0.6464319229125977
Batch 18/64 loss: -0.39598655700683594
Batch 19/64 loss: -0.7635841369628906
Batch 20/64 loss: -0.9080734252929688
Batch 21/64 loss: -0.6435518264770508
Batch 22/64 loss: -0.6291341781616211
Batch 23/64 loss: -0.7251377105712891
Batch 24/64 loss: -0.3708624839782715
Batch 25/64 loss: -0.70635986328125
Batch 26/64 loss: -0.6563482284545898
Batch 27/64 loss: -0.4486865997314453
Batch 28/64 loss: -0.9948844909667969
Batch 29/64 loss: -0.5863828659057617
Batch 30/64 loss: -0.7049846649169922
Batch 31/64 loss: -0.8617477416992188
Batch 32/64 loss: -0.918309211730957
Batch 33/64 loss: -0.5708789825439453
Batch 34/64 loss: -0.8351593017578125
Batch 35/64 loss: -0.7355737686157227
Batch 36/64 loss: -0.35696983337402344
Batch 37/64 loss: -0.5695075988769531
Batch 38/64 loss: -0.856266975402832
Batch 39/64 loss: -0.8527412414550781
Batch 40/64 loss: -0.4416942596435547
Batch 41/64 loss: -0.5665979385375977
Batch 42/64 loss: -0.9686136245727539
Batch 43/64 loss: -0.9028768539428711
Batch 44/64 loss: -0.9145708084106445
Batch 45/64 loss: -0.8929166793823242
Batch 46/64 loss: -0.2833099365234375
Batch 47/64 loss: -0.8558425903320312
Batch 48/64 loss: -0.851776123046875
Batch 49/64 loss: -1.204275131225586
Batch 50/64 loss: -1.1042957305908203
Batch 51/64 loss: -0.5622963905334473
Batch 52/64 loss: -1.0130443572998047
Batch 53/64 loss: -0.2963733673095703
Batch 54/64 loss: -0.8206090927124023
Batch 55/64 loss: -0.7515792846679688
Batch 56/64 loss: -0.6805753707885742
Batch 57/64 loss: -0.7720108032226562
Batch 58/64 loss: -0.9262056350708008
Batch 59/64 loss: -0.8000326156616211
Batch 60/64 loss: -0.8497419357299805
Batch 61/64 loss: -0.9545459747314453
Batch 62/64 loss: -0.7148942947387695
Batch 63/64 loss: -0.6265554428100586
Batch 64/64 loss: -4.872152805328369
Epoch 356  Train loss: -0.7603130321876675  Val loss: -0.8624123773214334
Epoch 357
-------------------------------
Batch 1/64 loss: -0.9111881256103516
Batch 2/64 loss: -0.8840045928955078
Batch 3/64 loss: -1.0199098587036133
Batch 4/64 loss: -0.6819639205932617
Batch 5/64 loss: -0.9646072387695312
Batch 6/64 loss: -0.7610206604003906
Batch 7/64 loss: -0.7486162185668945
Batch 8/64 loss: -0.8500232696533203
Batch 9/64 loss: -0.20912694931030273
Batch 10/64 loss: -1.1036252975463867
Batch 11/64 loss: -1.0080766677856445
Batch 12/64 loss: -0.7186918258666992
Batch 13/64 loss: -1.082815170288086
Batch 14/64 loss: -0.3571815490722656
Batch 15/64 loss: -0.7787446975708008
Batch 16/64 loss: -0.5837068557739258
Batch 17/64 loss: -0.8853921890258789
Batch 18/64 loss: -0.8297843933105469
Batch 19/64 loss: -0.9473838806152344
Batch 20/64 loss: -0.6358776092529297
Batch 21/64 loss: -0.5331449508666992
Batch 22/64 loss: -0.8392438888549805
Batch 23/64 loss: -0.8062448501586914
Batch 24/64 loss: -0.8444757461547852
Batch 25/64 loss: -0.6181240081787109
Batch 26/64 loss: -0.7705678939819336
Batch 27/64 loss: -0.9605617523193359
Batch 28/64 loss: -0.6576557159423828
Batch 29/64 loss: -0.4705362319946289
Batch 30/64 loss: -0.4547557830810547
Batch 31/64 loss: -0.2845792770385742
Batch 32/64 loss: -0.6754903793334961
Batch 33/64 loss: -0.861454963684082
Batch 34/64 loss: -0.5562100410461426
Batch 35/64 loss: -0.8689813613891602
Batch 36/64 loss: -0.4641151428222656
Batch 37/64 loss: -0.7236824035644531
Batch 38/64 loss: -1.034515380859375
Batch 39/64 loss: -0.7454328536987305
Batch 40/64 loss: -0.8660478591918945
Batch 41/64 loss: -1.0475635528564453
Batch 42/64 loss: -0.8816938400268555
Batch 43/64 loss: -0.9587059020996094
Batch 44/64 loss: -0.3029475212097168
Batch 45/64 loss: -1.0328779220581055
Batch 46/64 loss: -0.8233451843261719
Batch 47/64 loss: -0.9243888854980469
Batch 48/64 loss: -1.1757984161376953
Batch 49/64 loss: -0.7865285873413086
Batch 50/64 loss: -0.6117262840270996
Batch 51/64 loss: -0.8362998962402344
Batch 52/64 loss: -0.3354959487915039
Batch 53/64 loss: -0.9855690002441406
Batch 54/64 loss: -0.5119686126708984
Batch 55/64 loss: -0.33225536346435547
Batch 56/64 loss: -0.9360523223876953
Batch 57/64 loss: -0.5382080078125
Batch 58/64 loss: -0.2867088317871094
Batch 59/64 loss: -0.8520088195800781
Batch 60/64 loss: -0.9200325012207031
Batch 61/64 loss: -0.908970832824707
Batch 62/64 loss: -0.622434139251709
Batch 63/64 loss: -0.8287649154663086
Batch 64/64 loss: -4.6820220947265625
Epoch 357  Train loss: -0.7992066402061313  Val loss: -0.7191998458809868
Epoch 358
-------------------------------
Batch 1/64 loss: -0.8382148742675781
Batch 2/64 loss: -0.48726844787597656
Batch 3/64 loss: -0.9603204727172852
Batch 4/64 loss: -0.4462575912475586
Batch 5/64 loss: -0.6781949996948242
Batch 6/64 loss: -0.5688419342041016
Batch 7/64 loss: -0.8749494552612305
Batch 8/64 loss: -0.8771114349365234
Batch 9/64 loss: -0.9491100311279297
Batch 10/64 loss: -0.6185464859008789
Batch 11/64 loss: -0.9448833465576172
Batch 12/64 loss: -0.6669778823852539
Batch 13/64 loss: -0.8205909729003906
Batch 14/64 loss: -0.879460334777832
Batch 15/64 loss: -0.904296875
Batch 16/64 loss: -0.7636089324951172
Batch 17/64 loss: -1.1372499465942383
Batch 18/64 loss: -0.9046220779418945
Batch 19/64 loss: -0.4403505325317383
Batch 20/64 loss: -1.2345914840698242
Batch 21/64 loss: -0.6755132675170898
Batch 22/64 loss: -0.6551074981689453
Batch 23/64 loss: -0.9024124145507812
Batch 24/64 loss: -0.8804378509521484
Batch 25/64 loss: -0.11753082275390625
Batch 26/64 loss: -0.6414575576782227
Batch 27/64 loss: -0.8879032135009766
Batch 28/64 loss: -0.6687994003295898
Batch 29/64 loss: -0.5751571655273438
Batch 30/64 loss: -0.2908506393432617
Batch 31/64 loss: -0.7517642974853516
Batch 32/64 loss: -0.5383996963500977
Batch 33/64 loss: -0.6365880966186523
Batch 34/64 loss: -0.9431037902832031
Batch 35/64 loss: -0.5359535217285156
Batch 36/64 loss: -0.596776008605957
Batch 37/64 loss: -1.009383201599121
Batch 38/64 loss: -0.9235563278198242
Batch 39/64 loss: -0.9129104614257812
Batch 40/64 loss: -0.7865171432495117
Batch 41/64 loss: -0.6422462463378906
Batch 42/64 loss: -0.6987104415893555
Batch 43/64 loss: -0.901402473449707
Batch 44/64 loss: -0.8720798492431641
Batch 45/64 loss: -0.8957395553588867
Batch 46/64 loss: -0.7603445053100586
Batch 47/64 loss: -1.036651611328125
Batch 48/64 loss: -0.7668981552124023
Batch 49/64 loss: -0.5545206069946289
Batch 50/64 loss: -1.0599851608276367
Batch 51/64 loss: -0.6575479507446289
Batch 52/64 loss: -1.0404472351074219
Batch 53/64 loss: -1.0046262741088867
Batch 54/64 loss: -0.03692150115966797
Batch 55/64 loss: -0.5778560638427734
Batch 56/64 loss: -0.40080833435058594
Batch 57/64 loss: -0.26343774795532227
Batch 58/64 loss: -0.5878009796142578
Batch 59/64 loss: -0.930933952331543
Batch 60/64 loss: -0.5106167793273926
Batch 61/64 loss: -0.5642132759094238
Batch 62/64 loss: -0.8369894027709961
Batch 63/64 loss: -0.9093847274780273
Batch 64/64 loss: -5.06588888168335
Epoch 358  Train loss: -0.7880023526210411  Val loss: -0.8700891212909082
Epoch 359
-------------------------------
Batch 1/64 loss: -0.9763956069946289
Batch 2/64 loss: -0.5999612808227539
Batch 3/64 loss: -0.7942028045654297
Batch 4/64 loss: -0.8138961791992188
Batch 5/64 loss: -0.9381446838378906
Batch 6/64 loss: -0.8920860290527344
Batch 7/64 loss: -0.8331127166748047
Batch 8/64 loss: -0.2943997383117676
Batch 9/64 loss: -0.8417186737060547
Batch 10/64 loss: -0.701869010925293
Batch 11/64 loss: -0.5939140319824219
Batch 12/64 loss: -0.7209072113037109
Batch 13/64 loss: -0.09020090103149414
Batch 14/64 loss: -0.41047000885009766
Batch 15/64 loss: -1.04949951171875
Batch 16/64 loss: -0.6297378540039062
Batch 17/64 loss: -0.4642667770385742
Batch 18/64 loss: -0.3717517852783203
Batch 19/64 loss: -0.36943912506103516
Batch 20/64 loss: -0.5341939926147461
Batch 21/64 loss: -0.5292072296142578
Batch 22/64 loss: -0.013616085052490234
Batch 23/64 loss: -0.5903539657592773
Batch 24/64 loss: -0.3134622573852539
Batch 25/64 loss: -0.6668996810913086
Batch 26/64 loss: -0.4141254425048828
Batch 27/64 loss: -0.3020591735839844
Batch 28/64 loss: -0.26166725158691406
Batch 29/64 loss: -0.7250022888183594
Batch 30/64 loss: -0.3238544464111328
Batch 31/64 loss: -0.5994806289672852
Batch 32/64 loss: -0.9789218902587891
Batch 33/64 loss: -0.3424062728881836
Batch 34/64 loss: -0.6270990371704102
Batch 35/64 loss: -0.7916126251220703
Batch 36/64 loss: -0.6935319900512695
Batch 37/64 loss: -0.5742349624633789
Batch 38/64 loss: -0.48110485076904297
Batch 39/64 loss: -0.41632747650146484
Batch 40/64 loss: -0.2886819839477539
Batch 41/64 loss: -0.8348703384399414
Batch 42/64 loss: -0.768214225769043
Batch 43/64 loss: -0.7161970138549805
Batch 44/64 loss: -0.6620779037475586
Batch 45/64 loss: -0.9790639877319336
Batch 46/64 loss: -0.3462505340576172
Batch 47/64 loss: -0.987034797668457
Batch 48/64 loss: -0.385009765625
Batch 49/64 loss: -0.7182531356811523
Batch 50/64 loss: -0.7527294158935547
Batch 51/64 loss: -0.19710636138916016
Batch 52/64 loss: -0.7938690185546875
Batch 53/64 loss: -0.8075590133666992
Batch 54/64 loss: -0.564875602722168
Batch 55/64 loss: -0.9629859924316406
Batch 56/64 loss: -0.9314756393432617
Batch 57/64 loss: -0.7914800643920898
Batch 58/64 loss: -0.9149627685546875
Batch 59/64 loss: -0.6105022430419922
Batch 60/64 loss: -0.9743013381958008
Batch 61/64 loss: -1.1343555450439453
Batch 62/64 loss: -0.7366056442260742
Batch 63/64 loss: -0.664149284362793
Batch 64/64 loss: -4.821867942810059
Epoch 359  Train loss: -0.6855552635940851  Val loss: -0.7780671529343858
Epoch 360
-------------------------------
Batch 1/64 loss: -0.756378173828125
Batch 2/64 loss: -0.6440801620483398
Batch 3/64 loss: -0.44295215606689453
Batch 4/64 loss: -1.0174503326416016
Batch 5/64 loss: -1.0516786575317383
Batch 6/64 loss: -0.7621803283691406
Batch 7/64 loss: -0.5813207626342773
Batch 8/64 loss: -0.6934881210327148
Batch 9/64 loss: -0.7195720672607422
Batch 10/64 loss: -0.6525259017944336
Batch 11/64 loss: -0.9144706726074219
Batch 12/64 loss: -0.44890785217285156
Batch 13/64 loss: -0.74755859375
Batch 14/64 loss: -0.8044834136962891
Batch 15/64 loss: -0.8734226226806641
Batch 16/64 loss: -0.6792154312133789
Batch 17/64 loss: -0.7228765487670898
Batch 18/64 loss: -0.8807735443115234
Batch 19/64 loss: -0.8260793685913086
Batch 20/64 loss: -0.3476142883300781
Batch 21/64 loss: -0.5987882614135742
Batch 22/64 loss: -1.053985595703125
Batch 23/64 loss: -0.9790554046630859
Batch 24/64 loss: -0.7121410369873047
Batch 25/64 loss: -0.9333381652832031
Batch 26/64 loss: -0.6799030303955078
Batch 27/64 loss: -0.48178958892822266
Batch 28/64 loss: -0.8575172424316406
Batch 29/64 loss: -0.7743711471557617
Batch 30/64 loss: -0.37722015380859375
Batch 31/64 loss: -0.4086189270019531
Batch 32/64 loss: -0.6040420532226562
Batch 33/64 loss: -0.6910219192504883
Batch 34/64 loss: -0.9458112716674805
Batch 35/64 loss: -0.5616130828857422
Batch 36/64 loss: -0.7306861877441406
Batch 37/64 loss: -0.42640209197998047
Batch 38/64 loss: -0.8311042785644531
Batch 39/64 loss: -1.2298345565795898
Batch 40/64 loss: -0.7746067047119141
Batch 41/64 loss: -0.8269491195678711
Batch 42/64 loss: -0.9269514083862305
Batch 43/64 loss: -0.39055347442626953
Batch 44/64 loss: -0.9039134979248047
Batch 45/64 loss: -0.9915370941162109
Batch 46/64 loss: -0.842951774597168
Batch 47/64 loss: -0.5505647659301758
Batch 48/64 loss: -0.9731531143188477
Batch 49/64 loss: -0.5898408889770508
Batch 50/64 loss: -0.6504907608032227
Batch 51/64 loss: -0.7333383560180664
Batch 52/64 loss: -0.7260990142822266
Batch 53/64 loss: 0.0577850341796875
Batch 54/64 loss: -0.4773387908935547
Batch 55/64 loss: -0.7856349945068359
Batch 56/64 loss: -0.7345571517944336
Batch 57/64 loss: -0.732203483581543
Batch 58/64 loss: -0.6227617263793945
Batch 59/64 loss: -0.9850940704345703
Batch 60/64 loss: -0.9944906234741211
Batch 61/64 loss: -0.8698558807373047
Batch 62/64 loss: -0.7880477905273438
Batch 63/64 loss: -0.4152803421020508
Batch 64/64 loss: -4.331193923950195
Epoch 360  Train loss: -0.7673897761924594  Val loss: -0.8504946993798325
Epoch 361
-------------------------------
Batch 1/64 loss: -0.8249359130859375
Batch 2/64 loss: -0.7241897583007812
Batch 3/64 loss: -0.8794651031494141
Batch 4/64 loss: -0.9322338104248047
Batch 5/64 loss: -0.5838966369628906
Batch 6/64 loss: -0.7683477401733398
Batch 7/64 loss: -0.5620412826538086
Batch 8/64 loss: -0.68951416015625
Batch 9/64 loss: -0.8123083114624023
Batch 10/64 loss: -0.4558563232421875
Batch 11/64 loss: -0.6982488632202148
Batch 12/64 loss: -0.38749122619628906
Batch 13/64 loss: -0.7198123931884766
Batch 14/64 loss: -0.7818822860717773
Batch 15/64 loss: -0.8600425720214844
Batch 16/64 loss: -0.8193540573120117
Batch 17/64 loss: -0.26139116287231445
Batch 18/64 loss: -1.001856803894043
Batch 19/64 loss: -1.0969905853271484
Batch 20/64 loss: -0.46445465087890625
Batch 21/64 loss: -0.009610176086425781
Batch 22/64 loss: -1.006516456604004
Batch 23/64 loss: -0.7187509536743164
Batch 24/64 loss: -0.3362607955932617
Batch 25/64 loss: -0.650568962097168
Batch 26/64 loss: -0.9171638488769531
Batch 27/64 loss: -0.7433452606201172
Batch 28/64 loss: -0.7689151763916016
Batch 29/64 loss: -0.8464946746826172
Batch 30/64 loss: -0.770014762878418
Batch 31/64 loss: -0.9317731857299805
Batch 32/64 loss: -0.990168571472168
Batch 33/64 loss: -0.989654541015625
Batch 34/64 loss: -0.7229366302490234
Batch 35/64 loss: 0.012081146240234375
Batch 36/64 loss: -0.8686513900756836
Batch 37/64 loss: -0.8874292373657227
Batch 38/64 loss: -0.5584354400634766
Batch 39/64 loss: -0.7749547958374023
Batch 40/64 loss: -0.8337669372558594
Batch 41/64 loss: -0.6898050308227539
Batch 42/64 loss: -1.0190467834472656
Batch 43/64 loss: -0.9892578125
Batch 44/64 loss: -0.6034479141235352
Batch 45/64 loss: -0.7399606704711914
Batch 46/64 loss: -0.21503925323486328
Batch 47/64 loss: -0.8714303970336914
Batch 48/64 loss: -0.7742118835449219
Batch 49/64 loss: -1.0922870635986328
Batch 50/64 loss: -0.7954940795898438
Batch 51/64 loss: -0.7761316299438477
Batch 52/64 loss: -0.7742195129394531
Batch 53/64 loss: -0.8208980560302734
Batch 54/64 loss: -0.734619140625
Batch 55/64 loss: -0.9838743209838867
Batch 56/64 loss: -0.5497970581054688
Batch 57/64 loss: -0.6263923645019531
Batch 58/64 loss: -0.7068252563476562
Batch 59/64 loss: -0.7509737014770508
Batch 60/64 loss: -0.4596595764160156
Batch 61/64 loss: -0.3212089538574219
Batch 62/64 loss: -0.5729680061340332
Batch 63/64 loss: -0.9063072204589844
Batch 64/64 loss: -5.045310974121094
Epoch 361  Train loss: -0.7716938542384727  Val loss: -0.8460084056526525
Epoch 362
-------------------------------
Batch 1/64 loss: -0.7531509399414062
Batch 2/64 loss: -0.9349431991577148
Batch 3/64 loss: -0.6825885772705078
Batch 4/64 loss: -1.0496091842651367
Batch 5/64 loss: -0.7237472534179688
Batch 6/64 loss: -0.791529655456543
Batch 7/64 loss: -0.7740983963012695
Batch 8/64 loss: -1.211054801940918
Batch 9/64 loss: -0.48598480224609375
Batch 10/64 loss: -0.746150016784668
Batch 11/64 loss: -0.4550323486328125
Batch 12/64 loss: -0.38272619247436523
Batch 13/64 loss: 0.05851554870605469
Batch 14/64 loss: -0.8621673583984375
Batch 15/64 loss: -0.8445415496826172
Batch 16/64 loss: -0.7775173187255859
Batch 17/64 loss: -0.8158130645751953
Batch 18/64 loss: -0.7388181686401367
Batch 19/64 loss: -0.6445722579956055
Batch 20/64 loss: -0.7388248443603516
Batch 21/64 loss: -0.44991588592529297
Batch 22/64 loss: -0.7972068786621094
Batch 23/64 loss: -0.8129634857177734
Batch 24/64 loss: -0.7965068817138672
Batch 25/64 loss: -0.5766077041625977
Batch 26/64 loss: -0.8633193969726562
Batch 27/64 loss: -0.8408775329589844
Batch 28/64 loss: -0.27286243438720703
Batch 29/64 loss: -1.320516586303711
Batch 30/64 loss: -0.9204540252685547
Batch 31/64 loss: -0.7223691940307617
Batch 32/64 loss: -0.48637866973876953
Batch 33/64 loss: -0.6478176116943359
Batch 34/64 loss: -0.7074451446533203
Batch 35/64 loss: -0.9358158111572266
Batch 36/64 loss: -0.6193561553955078
Batch 37/64 loss: -0.6538820266723633
Batch 38/64 loss: -0.6801776885986328
Batch 39/64 loss: -0.46532535552978516
Batch 40/64 loss: -0.7240972518920898
Batch 41/64 loss: -0.5336647033691406
Batch 42/64 loss: -0.35088253021240234
Batch 43/64 loss: -1.0127935409545898
Batch 44/64 loss: -0.7972221374511719
Batch 45/64 loss: -0.7939996719360352
Batch 46/64 loss: -0.6364536285400391
Batch 47/64 loss: -0.7978544235229492
Batch 48/64 loss: -0.8856077194213867
Batch 49/64 loss: -0.6550283432006836
Batch 50/64 loss: -0.8492507934570312
Batch 51/64 loss: -0.614008903503418
Batch 52/64 loss: -0.9286079406738281
Batch 53/64 loss: -0.7754840850830078
Batch 54/64 loss: -0.6219453811645508
Batch 55/64 loss: -0.20969009399414062
Batch 56/64 loss: -1.0799150466918945
Batch 57/64 loss: -0.24112319946289062
Batch 58/64 loss: -0.7367610931396484
Batch 59/64 loss: -0.5712451934814453
Batch 60/64 loss: -0.7333774566650391
Batch 61/64 loss: -0.9400625228881836
Batch 62/64 loss: -0.3400421142578125
Batch 63/64 loss: -0.9273405075073242
Batch 64/64 loss: -4.729794025421143
Epoch 362  Train loss: -0.7565169671002556  Val loss: -0.7560721066399538
Epoch 363
-------------------------------
Batch 1/64 loss: -0.5230274200439453
Batch 2/64 loss: -0.6160097122192383
Batch 3/64 loss: -0.51531982421875
Batch 4/64 loss: -0.908360481262207
Batch 5/64 loss: -0.8735618591308594
Batch 6/64 loss: -0.9786901473999023
Batch 7/64 loss: -0.4257335662841797
Batch 8/64 loss: -0.21175193786621094
Batch 9/64 loss: -0.7080230712890625
Batch 10/64 loss: -1.0365362167358398
Batch 11/64 loss: -0.3674287796020508
Batch 12/64 loss: -0.7434549331665039
Batch 13/64 loss: -0.8483476638793945
Batch 14/64 loss: -0.7067499160766602
Batch 15/64 loss: -0.9525222778320312
Batch 16/64 loss: -1.0292539596557617
Batch 17/64 loss: -0.9283037185668945
Batch 18/64 loss: -0.8265600204467773
Batch 19/64 loss: -0.7878665924072266
Batch 20/64 loss: -0.8084449768066406
Batch 21/64 loss: -0.5657310485839844
Batch 22/64 loss: -0.9960699081420898
Batch 23/64 loss: -0.907221794128418
Batch 24/64 loss: -0.8326444625854492
Batch 25/64 loss: -0.8864316940307617
Batch 26/64 loss: -1.0192222595214844
Batch 27/64 loss: -0.010110855102539062
Batch 28/64 loss: -0.7107391357421875
Batch 29/64 loss: -0.5670099258422852
Batch 30/64 loss: -0.9204540252685547
Batch 31/64 loss: 0.10268020629882812
Batch 32/64 loss: -0.581904411315918
Batch 33/64 loss: -0.9915733337402344
Batch 34/64 loss: -1.1800527572631836
Batch 35/64 loss: -0.8008890151977539
Batch 36/64 loss: -0.7031469345092773
Batch 37/64 loss: -0.8248825073242188
Batch 38/64 loss: -0.8127412796020508
Batch 39/64 loss: -0.6435050964355469
Batch 40/64 loss: -0.1180267333984375
Batch 41/64 loss: -0.7678756713867188
Batch 42/64 loss: -0.5462455749511719
Batch 43/64 loss: -0.3913898468017578
Batch 44/64 loss: -0.9826459884643555
Batch 45/64 loss: -0.6410980224609375
Batch 46/64 loss: -0.9334983825683594
Batch 47/64 loss: -0.8500299453735352
Batch 48/64 loss: -0.9204254150390625
Batch 49/64 loss: -0.6933860778808594
Batch 50/64 loss: -0.749171257019043
Batch 51/64 loss: -0.7544317245483398
Batch 52/64 loss: -0.9388141632080078
Batch 53/64 loss: -0.42424678802490234
Batch 54/64 loss: -0.501133918762207
Batch 55/64 loss: -0.722712516784668
Batch 56/64 loss: -0.9051122665405273
Batch 57/64 loss: -0.88909912109375
Batch 58/64 loss: -0.5162420272827148
Batch 59/64 loss: -0.8885049819946289
Batch 60/64 loss: -0.49179553985595703
Batch 61/64 loss: -0.9578151702880859
Batch 62/64 loss: -0.43007755279541016
Batch 63/64 loss: -0.6564254760742188
Batch 64/64 loss: -5.013330936431885
Epoch 363  Train loss: -0.769847838083903  Val loss: -0.7287395778800204
Epoch 364
-------------------------------
Batch 1/64 loss: -0.8492889404296875
Batch 2/64 loss: -0.7551965713500977
Batch 3/64 loss: -0.8783273696899414
Batch 4/64 loss: -0.49585819244384766
Batch 5/64 loss: -0.6146163940429688
Batch 6/64 loss: 0.010043144226074219
Batch 7/64 loss: -0.8080530166625977
Batch 8/64 loss: -0.8358268737792969
Batch 9/64 loss: -0.5513811111450195
Batch 10/64 loss: -0.7260885238647461
Batch 11/64 loss: -0.9549951553344727
Batch 12/64 loss: -0.8511533737182617
Batch 13/64 loss: -0.4586305618286133
Batch 14/64 loss: -0.47019004821777344
Batch 15/64 loss: -0.3261604309082031
Batch 16/64 loss: -0.9442291259765625
Batch 17/64 loss: -0.8400306701660156
Batch 18/64 loss: -0.773259162902832
Batch 19/64 loss: -0.38722896575927734
Batch 20/64 loss: -0.7954950332641602
Batch 21/64 loss: -0.6566247940063477
Batch 22/64 loss: -0.8695602416992188
Batch 23/64 loss: -0.9455347061157227
Batch 24/64 loss: -0.9053983688354492
Batch 25/64 loss: -0.9755630493164062
Batch 26/64 loss: -0.7884616851806641
Batch 27/64 loss: -0.6754388809204102
Batch 28/64 loss: -0.34931468963623047
Batch 29/64 loss: -0.35507869720458984
Batch 30/64 loss: -1.0288496017456055
Batch 31/64 loss: -0.16478443145751953
Batch 32/64 loss: -1.0583820343017578
Batch 33/64 loss: -0.2246685028076172
Batch 34/64 loss: -0.9693660736083984
Batch 35/64 loss: -0.7466840744018555
Batch 36/64 loss: -0.9910516738891602
Batch 37/64 loss: -0.5709047317504883
Batch 38/64 loss: -0.6158370971679688
Batch 39/64 loss: -0.6552829742431641
Batch 40/64 loss: -0.3258848190307617
Batch 41/64 loss: -0.7725429534912109
Batch 42/64 loss: -0.6509342193603516
Batch 43/64 loss: -0.5680170059204102
Batch 44/64 loss: -1.0508441925048828
Batch 45/64 loss: -0.9760885238647461
Batch 46/64 loss: -0.4711723327636719
Batch 47/64 loss: -0.8390598297119141
Batch 48/64 loss: -0.840092658996582
Batch 49/64 loss: -0.9040231704711914
Batch 50/64 loss: -0.35314178466796875
Batch 51/64 loss: -1.0069999694824219
Batch 52/64 loss: -0.6197757720947266
Batch 53/64 loss: -0.7524623870849609
Batch 54/64 loss: -0.9324445724487305
Batch 55/64 loss: -0.45005083084106445
Batch 56/64 loss: -0.6682195663452148
Batch 57/64 loss: -0.7687578201293945
Batch 58/64 loss: -0.9419097900390625
Batch 59/64 loss: -0.9530363082885742
Batch 60/64 loss: -0.4246377944946289
Batch 61/64 loss: -0.6894931793212891
Batch 62/64 loss: -0.5591769218444824
Batch 63/64 loss: -0.33684349060058594
Batch 64/64 loss: -4.696110725402832
Epoch 364  Train loss: -0.7408697352689855  Val loss: -0.9295717219716495
Saving best model, epoch: 364
Epoch 365
-------------------------------
Batch 1/64 loss: -0.9247598648071289
Batch 2/64 loss: -0.6825590133666992
Batch 3/64 loss: -1.2118959426879883
Batch 4/64 loss: -0.616978645324707
Batch 5/64 loss: -0.7972517013549805
Batch 6/64 loss: -0.8085203170776367
Batch 7/64 loss: -0.5983858108520508
Batch 8/64 loss: -1.105520248413086
Batch 9/64 loss: -0.6629419326782227
Batch 10/64 loss: -0.48686695098876953
Batch 11/64 loss: -0.8893909454345703
Batch 12/64 loss: -0.7608423233032227
Batch 13/64 loss: -0.2640037536621094
Batch 14/64 loss: -0.5257959365844727
Batch 15/64 loss: -1.1205101013183594
Batch 16/64 loss: -0.20621109008789062
Batch 17/64 loss: -0.8894691467285156
Batch 18/64 loss: -0.9225187301635742
Batch 19/64 loss: -0.29496002197265625
Batch 20/64 loss: -0.799285888671875
Batch 21/64 loss: -0.9102315902709961
Batch 22/64 loss: -0.6514911651611328
Batch 23/64 loss: -0.31218576431274414
Batch 24/64 loss: -0.9096565246582031
Batch 25/64 loss: -0.9701156616210938
Batch 26/64 loss: -0.8732948303222656
Batch 27/64 loss: -1.022597312927246
Batch 28/64 loss: -0.9522762298583984
Batch 29/64 loss: -0.7985610961914062
Batch 30/64 loss: -0.9173402786254883
Batch 31/64 loss: -0.951446533203125
Batch 32/64 loss: -0.8627433776855469
Batch 33/64 loss: -0.9680862426757812
Batch 34/64 loss: -0.6659526824951172
Batch 35/64 loss: -0.8392801284790039
Batch 36/64 loss: -1.054762840270996
Batch 37/64 loss: -1.152174949645996
Batch 38/64 loss: -0.41799068450927734
Batch 39/64 loss: -0.9735355377197266
Batch 40/64 loss: -0.668701171875
Batch 41/64 loss: -0.8757438659667969
Batch 42/64 loss: -0.7828521728515625
Batch 43/64 loss: -0.9528646469116211
Batch 44/64 loss: -0.7447700500488281
Batch 45/64 loss: -0.8789701461791992
Batch 46/64 loss: -0.5126018524169922
Batch 47/64 loss: -0.7050600051879883
Batch 48/64 loss: -1.0017423629760742
Batch 49/64 loss: -0.6489992141723633
Batch 50/64 loss: -0.5583267211914062
Batch 51/64 loss: -0.8090572357177734
Batch 52/64 loss: -0.6168184280395508
Batch 53/64 loss: -0.723109245300293
Batch 54/64 loss: -0.22789382934570312
Batch 55/64 loss: -0.8740558624267578
Batch 56/64 loss: -0.5226278305053711
Batch 57/64 loss: -0.8534135818481445
Batch 58/64 loss: -0.5707664489746094
Batch 59/64 loss: -0.9489774703979492
Batch 60/64 loss: -0.36469078063964844
Batch 61/64 loss: -0.7356204986572266
Batch 62/64 loss: -0.31992149353027344
Batch 63/64 loss: -0.8774871826171875
Batch 64/64 loss: -4.537590980529785
Epoch 365  Train loss: -0.7992259941849054  Val loss: -0.7555912450416801
Epoch 366
-------------------------------
Batch 1/64 loss: -0.7373895645141602
Batch 2/64 loss: -0.7408323287963867
Batch 3/64 loss: -0.8705177307128906
Batch 4/64 loss: -0.357330322265625
Batch 5/64 loss: -0.9433259963989258
Batch 6/64 loss: -1.0239429473876953
Batch 7/64 loss: -0.928624153137207
Batch 8/64 loss: -0.8015995025634766
Batch 9/64 loss: -0.5812149047851562
Batch 10/64 loss: -0.8826408386230469
Batch 11/64 loss: 0.06673860549926758
Batch 12/64 loss: -0.962336540222168
Batch 13/64 loss: -1.0615997314453125
Batch 14/64 loss: -0.6578702926635742
Batch 15/64 loss: -1.108994483947754
Batch 16/64 loss: -0.9191503524780273
Batch 17/64 loss: -0.8296527862548828
Batch 18/64 loss: -0.368560791015625
Batch 19/64 loss: -0.934300422668457
Batch 20/64 loss: -0.990056037902832
Batch 21/64 loss: -0.9651374816894531
Batch 22/64 loss: -0.8057918548583984
Batch 23/64 loss: -0.8860492706298828
Batch 24/64 loss: -1.0008611679077148
Batch 25/64 loss: -0.8464374542236328
Batch 26/64 loss: -0.8383846282958984
Batch 27/64 loss: -0.6007223129272461
Batch 28/64 loss: -0.6634283065795898
Batch 29/64 loss: -0.2050323486328125
Batch 30/64 loss: -0.7984142303466797
Batch 31/64 loss: -0.5903072357177734
Batch 32/64 loss: -0.7018852233886719
Batch 33/64 loss: -0.9923582077026367
Batch 34/64 loss: -0.8543453216552734
Batch 35/64 loss: -0.6678504943847656
Batch 36/64 loss: -0.8694829940795898
Batch 37/64 loss: -0.9767341613769531
Batch 38/64 loss: -0.9241943359375
Batch 39/64 loss: -0.9819297790527344
Batch 40/64 loss: -1.0036449432373047
Batch 41/64 loss: -0.6032466888427734
Batch 42/64 loss: -0.6922540664672852
Batch 43/64 loss: -0.9594888687133789
Batch 44/64 loss: -0.7015447616577148
Batch 45/64 loss: -0.3989286422729492
Batch 46/64 loss: -0.5524091720581055
Batch 47/64 loss: -0.5311613082885742
Batch 48/64 loss: -1.0150184631347656
Batch 49/64 loss: -0.5408115386962891
Batch 50/64 loss: -0.6863374710083008
Batch 51/64 loss: -0.8689689636230469
Batch 52/64 loss: -0.565333366394043
Batch 53/64 loss: -0.6739559173583984
Batch 54/64 loss: -0.3169240951538086
Batch 55/64 loss: -0.8515758514404297
Batch 56/64 loss: -0.7039203643798828
Batch 57/64 loss: -0.5869693756103516
Batch 58/64 loss: -0.8000640869140625
Batch 59/64 loss: -0.6580142974853516
Batch 60/64 loss: -0.5377054214477539
Batch 61/64 loss: -0.6815299987792969
Batch 62/64 loss: -0.5004463195800781
Batch 63/64 loss: -0.7741880416870117
Batch 64/64 loss: -4.829935073852539
Epoch 366  Train loss: -0.7941873139026119  Val loss: -0.892885149139719
Epoch 367
-------------------------------
Batch 1/64 loss: -0.5077571868896484
Batch 2/64 loss: -0.9087619781494141
Batch 3/64 loss: -0.5161561965942383
Batch 4/64 loss: -0.9975461959838867
Batch 5/64 loss: -0.6220436096191406
Batch 6/64 loss: -0.5688290596008301
Batch 7/64 loss: -0.7286338806152344
Batch 8/64 loss: -0.8027267456054688
Batch 9/64 loss: -1.0662994384765625
Batch 10/64 loss: -0.7880945205688477
Batch 11/64 loss: -0.7081594467163086
Batch 12/64 loss: -0.8363742828369141
Batch 13/64 loss: -0.5100040435791016
Batch 14/64 loss: -0.27044677734375
Batch 15/64 loss: -0.6648082733154297
Batch 16/64 loss: -0.7803993225097656
Batch 17/64 loss: -0.9650774002075195
Batch 18/64 loss: -1.0295181274414062
Batch 19/64 loss: -0.9703760147094727
Batch 20/64 loss: -0.7924280166625977
Batch 21/64 loss: -0.9318990707397461
Batch 22/64 loss: -0.8658151626586914
Batch 23/64 loss: -0.49896240234375
Batch 24/64 loss: -0.9979734420776367
Batch 25/64 loss: -0.7662153244018555
Batch 26/64 loss: -0.7347593307495117
Batch 27/64 loss: -0.8587884902954102
Batch 28/64 loss: -0.9385776519775391
Batch 29/64 loss: -0.803462028503418
Batch 30/64 loss: -0.79180908203125
Batch 31/64 loss: -0.7186517715454102
Batch 32/64 loss: -0.3484625816345215
Batch 33/64 loss: -0.9007139205932617
Batch 34/64 loss: -0.9139156341552734
Batch 35/64 loss: -0.6070528030395508
Batch 36/64 loss: -0.4835214614868164
Batch 37/64 loss: -0.9575901031494141
Batch 38/64 loss: -0.6575145721435547
Batch 39/64 loss: -0.6433610916137695
Batch 40/64 loss: -0.6379251480102539
Batch 41/64 loss: -0.23046588897705078
Batch 42/64 loss: -0.7057533264160156
Batch 43/64 loss: -1.0152320861816406
Batch 44/64 loss: -0.6848545074462891
Batch 45/64 loss: -0.5270109176635742
Batch 46/64 loss: -0.17690181732177734
Batch 47/64 loss: -0.8559370040893555
Batch 48/64 loss: -0.7695341110229492
Batch 49/64 loss: -0.5587263107299805
Batch 50/64 loss: -0.747889518737793
Batch 51/64 loss: -0.8039407730102539
Batch 52/64 loss: -0.6356573104858398
Batch 53/64 loss: -0.8182554244995117
Batch 54/64 loss: -0.6986942291259766
Batch 55/64 loss: -0.6929864883422852
Batch 56/64 loss: -0.7596778869628906
Batch 57/64 loss: -0.7009725570678711
Batch 58/64 loss: -0.47324657440185547
Batch 59/64 loss: -1.1082677841186523
Batch 60/64 loss: -0.14085769653320312
Batch 61/64 loss: -0.6197929382324219
Batch 62/64 loss: -0.639103889465332
Batch 63/64 loss: -0.42581748962402344
Batch 64/64 loss: -4.5182013511657715
Epoch 367  Train loss: -0.757170809951483  Val loss: -0.7548462910340824
Epoch 368
-------------------------------
Batch 1/64 loss: -0.8828344345092773
Batch 2/64 loss: -0.8851356506347656
Batch 3/64 loss: -0.864323616027832
Batch 4/64 loss: -0.5273990631103516
Batch 5/64 loss: -0.7997894287109375
Batch 6/64 loss: -0.4439239501953125
Batch 7/64 loss: -0.9638328552246094
Batch 8/64 loss: -0.9506397247314453
Batch 9/64 loss: -0.8498086929321289
Batch 10/64 loss: -0.7061452865600586
Batch 11/64 loss: -0.6535911560058594
Batch 12/64 loss: -0.9303255081176758
Batch 13/64 loss: -0.7773046493530273
Batch 14/64 loss: -0.6577768325805664
Batch 15/64 loss: -0.7520418167114258
Batch 16/64 loss: -0.851496696472168
Batch 17/64 loss: -0.9703941345214844
Batch 18/64 loss: -0.8827590942382812
Batch 19/64 loss: -0.6968193054199219
Batch 20/64 loss: -0.9537553787231445
Batch 21/64 loss: -0.6307477951049805
Batch 22/64 loss: -0.530003547668457
Batch 23/64 loss: -0.7203102111816406
Batch 24/64 loss: -0.1785907745361328
Batch 25/64 loss: -1.0141916275024414
Batch 26/64 loss: -0.9919853210449219
Batch 27/64 loss: -0.848114013671875
Batch 28/64 loss: -0.9831428527832031
Batch 29/64 loss: -0.734248161315918
Batch 30/64 loss: -0.486907958984375
Batch 31/64 loss: -0.9031200408935547
Batch 32/64 loss: -1.1024370193481445
Batch 33/64 loss: -0.37460899353027344
Batch 34/64 loss: -0.8436708450317383
Batch 35/64 loss: -0.2456502914428711
Batch 36/64 loss: -0.9369697570800781
Batch 37/64 loss: -0.5966386795043945
Batch 38/64 loss: -0.525670051574707
Batch 39/64 loss: -1.0114870071411133
Batch 40/64 loss: -0.72930908203125
Batch 41/64 loss: -0.23424530029296875
Batch 42/64 loss: -0.8004074096679688
Batch 43/64 loss: -0.9147195816040039
Batch 44/64 loss: -0.7003993988037109
Batch 45/64 loss: -1.1434106826782227
Batch 46/64 loss: -0.7494335174560547
Batch 47/64 loss: -0.9609165191650391
Batch 48/64 loss: -0.5706415176391602
Batch 49/64 loss: -0.4100484848022461
Batch 50/64 loss: -0.6712322235107422
Batch 51/64 loss: -0.673980712890625
Batch 52/64 loss: -0.6135139465332031
Batch 53/64 loss: -0.8485307693481445
Batch 54/64 loss: -0.9952888488769531
Batch 55/64 loss: -0.4487905502319336
Batch 56/64 loss: -0.8209657669067383
Batch 57/64 loss: -0.6306190490722656
Batch 58/64 loss: -0.8252191543579102
Batch 59/64 loss: -0.5956211090087891
Batch 60/64 loss: -0.527409553527832
Batch 61/64 loss: -0.9294309616088867
Batch 62/64 loss: 0.5276832580566406
Batch 63/64 loss: -0.8306493759155273
Batch 64/64 loss: -4.777122974395752
Epoch 368  Train loss: -0.7739377994163363  Val loss: -0.8275845714451111
Epoch 369
-------------------------------
Batch 1/64 loss: -0.44500064849853516
Batch 2/64 loss: -0.7120170593261719
Batch 3/64 loss: -0.6786699295043945
Batch 4/64 loss: -0.5325164794921875
Batch 5/64 loss: -0.6026592254638672
Batch 6/64 loss: -0.5915250778198242
Batch 7/64 loss: -0.5627460479736328
Batch 8/64 loss: -0.7832012176513672
Batch 9/64 loss: -0.7272939682006836
Batch 10/64 loss: -1.0612983703613281
Batch 11/64 loss: -0.8599452972412109
Batch 12/64 loss: -0.8843507766723633
Batch 13/64 loss: -0.4212932586669922
Batch 14/64 loss: -0.1606464385986328
Batch 15/64 loss: -0.7621612548828125
Batch 16/64 loss: -0.7921848297119141
Batch 17/64 loss: -0.9529943466186523
Batch 18/64 loss: -0.8669767379760742
Batch 19/64 loss: -0.6600217819213867
Batch 20/64 loss: -0.8116416931152344
Batch 21/64 loss: -0.9157962799072266
Batch 22/64 loss: -0.5733604431152344
Batch 23/64 loss: -0.8545961380004883
Batch 24/64 loss: -0.8304224014282227
Batch 25/64 loss: -0.8301525115966797
Batch 26/64 loss: -0.6065521240234375
Batch 27/64 loss: -0.48237037658691406
Batch 28/64 loss: -0.747462272644043
Batch 29/64 loss: -0.44287109375
Batch 30/64 loss: -0.9153671264648438
Batch 31/64 loss: -0.6727008819580078
Batch 32/64 loss: -0.968266487121582
Batch 33/64 loss: -0.8877944946289062
Batch 34/64 loss: -0.4734325408935547
Batch 35/64 loss: -0.9043292999267578
Batch 36/64 loss: -0.5188217163085938
Batch 37/64 loss: -0.7913389205932617
Batch 38/64 loss: -0.5576963424682617
Batch 39/64 loss: -0.762598991394043
Batch 40/64 loss: -0.15309619903564453
Batch 41/64 loss: -0.6362762451171875
Batch 42/64 loss: -0.4309568405151367
Batch 43/64 loss: -0.22433090209960938
Batch 44/64 loss: -0.6862802505493164
Batch 45/64 loss: -0.8370161056518555
Batch 46/64 loss: -0.37180328369140625
Batch 47/64 loss: -0.7109613418579102
Batch 48/64 loss: -0.09821605682373047
Batch 49/64 loss: -0.8475914001464844
Batch 50/64 loss: -1.1112632751464844
Batch 51/64 loss: -0.8758935928344727
Batch 52/64 loss: -0.45916128158569336
Batch 53/64 loss: -0.6639585494995117
Batch 54/64 loss: -0.7235097885131836
Batch 55/64 loss: -0.42187929153442383
Batch 56/64 loss: -0.6641845703125
Batch 57/64 loss: -0.8778514862060547
Batch 58/64 loss: -1.042719841003418
Batch 59/64 loss: -0.7235774993896484
Batch 60/64 loss: -0.8627948760986328
Batch 61/64 loss: -1.217142105102539
Batch 62/64 loss: -0.7738122940063477
Batch 63/64 loss: -0.6444187164306641
Batch 64/64 loss: -4.50732946395874
Epoch 369  Train loss: -0.7379492984098547  Val loss: -0.8161509405706346
Epoch 370
-------------------------------
Batch 1/64 loss: -0.653782844543457
Batch 2/64 loss: -0.7341842651367188
Batch 3/64 loss: -0.11067008972167969
Batch 4/64 loss: -0.9399013519287109
Batch 5/64 loss: -0.49579524993896484
Batch 6/64 loss: -0.4806537628173828
Batch 7/64 loss: -0.9093685150146484
Batch 8/64 loss: -0.8962039947509766
Batch 9/64 loss: -0.1999521255493164
Batch 10/64 loss: -0.9938802719116211
Batch 11/64 loss: -0.6600141525268555
Batch 12/64 loss: -0.5470390319824219
Batch 13/64 loss: -0.9817228317260742
Batch 14/64 loss: -0.7953109741210938
Batch 15/64 loss: -0.7787618637084961
Batch 16/64 loss: -0.5081453323364258
Batch 17/64 loss: -0.8101301193237305
Batch 18/64 loss: -0.6307811737060547
Batch 19/64 loss: -0.6592130661010742
Batch 20/64 loss: -0.8143777847290039
Batch 21/64 loss: -0.6912899017333984
Batch 22/64 loss: -1.0480642318725586
Batch 23/64 loss: -1.1784791946411133
Batch 24/64 loss: -0.7653980255126953
Batch 25/64 loss: -0.852198600769043
Batch 26/64 loss: -0.37197399139404297
Batch 27/64 loss: -0.8953094482421875
Batch 28/64 loss: -0.7084512710571289
Batch 29/64 loss: -0.9382858276367188
Batch 30/64 loss: -0.7947015762329102
Batch 31/64 loss: -0.5112409591674805
Batch 32/64 loss: -0.8428306579589844
Batch 33/64 loss: -0.5860671997070312
Batch 34/64 loss: -0.735748291015625
Batch 35/64 loss: -0.3617362976074219
Batch 36/64 loss: -0.8458242416381836
Batch 37/64 loss: -0.4821739196777344
Batch 38/64 loss: -0.9375877380371094
Batch 39/64 loss: -0.8074874877929688
Batch 40/64 loss: -0.5562152862548828
Batch 41/64 loss: -0.8658838272094727
Batch 42/64 loss: -0.8027420043945312
Batch 43/64 loss: -0.5927057266235352
Batch 44/64 loss: -0.37535572052001953
Batch 45/64 loss: -1.1441631317138672
Batch 46/64 loss: -0.795863151550293
Batch 47/64 loss: -0.691472053527832
Batch 48/64 loss: -0.6463260650634766
Batch 49/64 loss: -0.8837947845458984
Batch 50/64 loss: -0.7345991134643555
Batch 51/64 loss: -0.4942970275878906
Batch 52/64 loss: -0.7501850128173828
Batch 53/64 loss: -0.7770366668701172
Batch 54/64 loss: -0.21174049377441406
Batch 55/64 loss: -0.8415260314941406
Batch 56/64 loss: -0.13151073455810547
Batch 57/64 loss: -0.9000463485717773
Batch 58/64 loss: -0.7617225646972656
Batch 59/64 loss: -0.7582054138183594
Batch 60/64 loss: -0.5874366760253906
Batch 61/64 loss: -0.9615755081176758
Batch 62/64 loss: -0.9930801391601562
Batch 63/64 loss: -0.69378662109375
Batch 64/64 loss: -4.663994312286377
Epoch 370  Train loss: -0.75927848628923  Val loss: -0.8221077017767733
Epoch 371
-------------------------------
Batch 1/64 loss: -0.8736038208007812
Batch 2/64 loss: -0.739375114440918
Batch 3/64 loss: -0.637059211730957
Batch 4/64 loss: -0.6210966110229492
Batch 5/64 loss: -0.6799440383911133
Batch 6/64 loss: 0.3955574035644531
Batch 7/64 loss: -0.9251337051391602
Batch 8/64 loss: -0.9731101989746094
Batch 9/64 loss: -0.9198465347290039
Batch 10/64 loss: -0.9473466873168945
Batch 11/64 loss: -0.5163278579711914
Batch 12/64 loss: -0.7705888748168945
Batch 13/64 loss: -0.7170228958129883
Batch 14/64 loss: -1.0671520233154297
Batch 15/64 loss: -0.4299640655517578
Batch 16/64 loss: -0.8052043914794922
Batch 17/64 loss: -0.8287820816040039
Batch 18/64 loss: -0.5644083023071289
Batch 19/64 loss: -0.5710000991821289
Batch 20/64 loss: -0.8761663436889648
Batch 21/64 loss: -0.7952909469604492
Batch 22/64 loss: -0.4059162139892578
Batch 23/64 loss: -0.93829345703125
Batch 24/64 loss: 0.005878448486328125
Batch 25/64 loss: -0.7889280319213867
Batch 26/64 loss: -0.6356697082519531
Batch 27/64 loss: -0.8122644424438477
Batch 28/64 loss: -0.2622489929199219
Batch 29/64 loss: -0.7224998474121094
Batch 30/64 loss: -0.9407444000244141
Batch 31/64 loss: -0.6798582077026367
Batch 32/64 loss: -0.4675016403198242
Batch 33/64 loss: 0.18091154098510742
Batch 34/64 loss: -0.6332511901855469
Batch 35/64 loss: -0.7557210922241211
Batch 36/64 loss: -1.0366973876953125
Batch 37/64 loss: -0.6478872299194336
Batch 38/64 loss: -0.5603237152099609
Batch 39/64 loss: -0.8130998611450195
Batch 40/64 loss: -0.27077293395996094
Batch 41/64 loss: -0.7997961044311523
Batch 42/64 loss: -0.5547466278076172
Batch 43/64 loss: -0.8652286529541016
Batch 44/64 loss: -1.1463289260864258
Batch 45/64 loss: -0.9561910629272461
Batch 46/64 loss: -1.0414085388183594
Batch 47/64 loss: -0.7633275985717773
Batch 48/64 loss: -0.8189220428466797
Batch 49/64 loss: -0.7599401473999023
Batch 50/64 loss: -0.35164833068847656
Batch 51/64 loss: -0.8098669052124023
Batch 52/64 loss: -0.8066225051879883
Batch 53/64 loss: -0.5556812286376953
Batch 54/64 loss: -0.8644475936889648
Batch 55/64 loss: -0.5039777755737305
Batch 56/64 loss: -1.1160497665405273
Batch 57/64 loss: -0.2972145080566406
Batch 58/64 loss: -0.7006320953369141
Batch 59/64 loss: -1.1019792556762695
Batch 60/64 loss: -0.791316032409668
Batch 61/64 loss: -0.8208637237548828
Batch 62/64 loss: -0.8177881240844727
Batch 63/64 loss: -0.9797496795654297
Batch 64/64 loss: -4.92086124420166
Epoch 371  Train loss: -0.7523471046896542  Val loss: -0.9016505893563077
Epoch 372
-------------------------------
Batch 1/64 loss: -0.7734222412109375
Batch 2/64 loss: -0.9093170166015625
Batch 3/64 loss: -0.7703390121459961
Batch 4/64 loss: -0.8460626602172852
Batch 5/64 loss: -0.855712890625
Batch 6/64 loss: -1.109696388244629
Batch 7/64 loss: -0.7462778091430664
Batch 8/64 loss: -0.9577369689941406
Batch 9/64 loss: -1.177159309387207
Batch 10/64 loss: -0.694279670715332
Batch 11/64 loss: -0.7839221954345703
Batch 12/64 loss: -0.6394338607788086
Batch 13/64 loss: -0.8165750503540039
Batch 14/64 loss: -0.9518041610717773
Batch 15/64 loss: -0.9166288375854492
Batch 16/64 loss: -0.7225790023803711
Batch 17/64 loss: -0.8622474670410156
Batch 18/64 loss: -0.7150449752807617
Batch 19/64 loss: -0.8688325881958008
Batch 20/64 loss: -0.7091169357299805
Batch 21/64 loss: -0.8885011672973633
Batch 22/64 loss: -0.7557840347290039
Batch 23/64 loss: -0.778843879699707
Batch 24/64 loss: -0.6692104339599609
Batch 25/64 loss: -0.715601921081543
Batch 26/64 loss: -1.1056976318359375
Batch 27/64 loss: -0.962193489074707
Batch 28/64 loss: -0.8517923355102539
Batch 29/64 loss: -0.6454486846923828
Batch 30/64 loss: -0.529637336730957
Batch 31/64 loss: -0.6286582946777344
Batch 32/64 loss: -0.6021614074707031
Batch 33/64 loss: -0.9552297592163086
Batch 34/64 loss: -0.13662242889404297
Batch 35/64 loss: -0.6923189163208008
Batch 36/64 loss: -0.6902456283569336
Batch 37/64 loss: -0.5672178268432617
Batch 38/64 loss: -1.0912017822265625
Batch 39/64 loss: -0.8232955932617188
Batch 40/64 loss: -0.9896860122680664
Batch 41/64 loss: -0.7692680358886719
Batch 42/64 loss: -0.826873779296875
Batch 43/64 loss: 0.22237396240234375
Batch 44/64 loss: -1.0868053436279297
Batch 45/64 loss: -0.6019830703735352
Batch 46/64 loss: -0.8234148025512695
Batch 47/64 loss: -0.8164863586425781
Batch 48/64 loss: -0.6974687576293945
Batch 49/64 loss: -0.8966588973999023
Batch 50/64 loss: -0.583979606628418
Batch 51/64 loss: -0.4461479187011719
Batch 52/64 loss: -0.6934213638305664
Batch 53/64 loss: -0.9666004180908203
Batch 54/64 loss: -1.0380334854125977
Batch 55/64 loss: -0.45133495330810547
Batch 56/64 loss: -0.8626880645751953
Batch 57/64 loss: -0.7323837280273438
Batch 58/64 loss: -0.26768970489501953
Batch 59/64 loss: -0.2960519790649414
Batch 60/64 loss: -0.9876947402954102
Batch 61/64 loss: -0.9164161682128906
Batch 62/64 loss: -0.8346071243286133
Batch 63/64 loss: -0.9053144454956055
Batch 64/64 loss: -4.087630748748779
Epoch 372  Train loss: -0.8039248541289685  Val loss: -0.7710493290956897
Epoch 373
-------------------------------
Batch 1/64 loss: -0.41362476348876953
Batch 2/64 loss: -0.5559511184692383
Batch 3/64 loss: -0.45966434478759766
Batch 4/64 loss: -0.6404361724853516
Batch 5/64 loss: -0.8338356018066406
Batch 6/64 loss: -0.9287681579589844
Batch 7/64 loss: -0.6740474700927734
Batch 8/64 loss: -0.7636308670043945
Batch 9/64 loss: -1.110940933227539
Batch 10/64 loss: -1.0251541137695312
Batch 11/64 loss: -0.8023576736450195
Batch 12/64 loss: -0.8348979949951172
Batch 13/64 loss: -0.6320924758911133
Batch 14/64 loss: -0.9447832107543945
Batch 15/64 loss: -0.8617315292358398
Batch 16/64 loss: -1.0015630722045898
Batch 17/64 loss: -0.8212614059448242
Batch 18/64 loss: -0.3389911651611328
Batch 19/64 loss: -0.7979745864868164
Batch 20/64 loss: -1.0095624923706055
Batch 21/64 loss: -0.7106075286865234
Batch 22/64 loss: -0.8386096954345703
Batch 23/64 loss: -0.8364286422729492
Batch 24/64 loss: -0.7888298034667969
Batch 25/64 loss: -0.48157691955566406
Batch 26/64 loss: -0.8661880493164062
Batch 27/64 loss: -0.5373058319091797
Batch 28/64 loss: -0.8134288787841797
Batch 29/64 loss: -0.37569713592529297
Batch 30/64 loss: -0.4579305648803711
Batch 31/64 loss: -0.9186820983886719
Batch 32/64 loss: -0.9958868026733398
Batch 33/64 loss: -0.6060705184936523
Batch 34/64 loss: -0.5941448211669922
Batch 35/64 loss: -0.6543350219726562
Batch 36/64 loss: -0.6410055160522461
Batch 37/64 loss: -0.8403663635253906
Batch 38/64 loss: -0.6897439956665039
Batch 39/64 loss: -0.8038253784179688
Batch 40/64 loss: -0.3431243896484375
Batch 41/64 loss: -0.8078107833862305
Batch 42/64 loss: -0.4282255172729492
Batch 43/64 loss: -0.4427661895751953
Batch 44/64 loss: -0.7158727645874023
Batch 45/64 loss: -0.7588977813720703
Batch 46/64 loss: -0.8071012496948242
Batch 47/64 loss: -0.37947750091552734
Batch 48/64 loss: -0.6859626770019531
Batch 49/64 loss: -0.17451858520507812
Batch 50/64 loss: -0.6013984680175781
Batch 51/64 loss: -0.6044654846191406
Batch 52/64 loss: -0.9088554382324219
Batch 53/64 loss: -0.6175689697265625
Batch 54/64 loss: -0.5854158401489258
Batch 55/64 loss: -0.2653522491455078
Batch 56/64 loss: -0.755274772644043
Batch 57/64 loss: -0.7057418823242188
Batch 58/64 loss: -0.6057195663452148
Batch 59/64 loss: -0.705540657043457
Batch 60/64 loss: -0.8294849395751953
Batch 61/64 loss: -0.9289159774780273
Batch 62/64 loss: -0.34282588958740234
Batch 63/64 loss: -0.49747276306152344
Batch 64/64 loss: -4.755307674407959
Epoch 373  Train loss: -0.7367247656279919  Val loss: -0.6780721920052755
Epoch 374
-------------------------------
Batch 1/64 loss: -0.7081279754638672
Batch 2/64 loss: 0.11239910125732422
Batch 3/64 loss: -0.01858234405517578
Batch 4/64 loss: -0.9413633346557617
Batch 5/64 loss: -1.0353870391845703
Batch 6/64 loss: -0.9756555557250977
Batch 7/64 loss: -0.9576501846313477
Batch 8/64 loss: -0.01723766326904297
Batch 9/64 loss: -0.688079833984375
Batch 10/64 loss: -0.8484554290771484
Batch 11/64 loss: -0.9179372787475586
Batch 12/64 loss: -0.5630598068237305
Batch 13/64 loss: -0.7286481857299805
Batch 14/64 loss: -0.8575115203857422
Batch 15/64 loss: -0.8258743286132812
Batch 16/64 loss: -0.706852912902832
Batch 17/64 loss: -0.9215002059936523
Batch 18/64 loss: -0.752049446105957
Batch 19/64 loss: -0.848414421081543
Batch 20/64 loss: -0.7109174728393555
Batch 21/64 loss: -0.25516510009765625
Batch 22/64 loss: -0.9424219131469727
Batch 23/64 loss: -0.7163543701171875
Batch 24/64 loss: -1.1165132522583008
Batch 25/64 loss: -1.0679750442504883
Batch 26/64 loss: -0.5345268249511719
Batch 27/64 loss: -1.0814924240112305
Batch 28/64 loss: -1.0310916900634766
Batch 29/64 loss: -0.8537559509277344
Batch 30/64 loss: -1.0591363906860352
Batch 31/64 loss: -0.9630556106567383
Batch 32/64 loss: -1.0184173583984375
Batch 33/64 loss: -0.7559127807617188
Batch 34/64 loss: -0.6167154312133789
Batch 35/64 loss: -0.5856819152832031
Batch 36/64 loss: -0.59722900390625
Batch 37/64 loss: -0.8535556793212891
Batch 38/64 loss: -1.0937776565551758
Batch 39/64 loss: -0.5163354873657227
Batch 40/64 loss: -0.8389492034912109
Batch 41/64 loss: -0.18871116638183594
Batch 42/64 loss: -0.9286556243896484
Batch 43/64 loss: -0.4310417175292969
Batch 44/64 loss: -0.8318014144897461
Batch 45/64 loss: -0.7939395904541016
Batch 46/64 loss: -1.0935707092285156
Batch 47/64 loss: -0.5831947326660156
Batch 48/64 loss: -0.15212726593017578
Batch 49/64 loss: -1.0357837677001953
Batch 50/64 loss: -0.8127279281616211
Batch 51/64 loss: -0.8962736129760742
Batch 52/64 loss: -0.561279296875
Batch 53/64 loss: -1.138819694519043
Batch 54/64 loss: -0.8176498413085938
Batch 55/64 loss: -0.8921222686767578
Batch 56/64 loss: -0.11531543731689453
Batch 57/64 loss: -0.8257255554199219
Batch 58/64 loss: -0.5152626037597656
Batch 59/64 loss: -0.1923675537109375
Batch 60/64 loss: -0.43770599365234375
Batch 61/64 loss: -0.9923181533813477
Batch 62/64 loss: -0.9081516265869141
Batch 63/64 loss: -0.7492580413818359
Batch 64/64 loss: -4.717510223388672
Epoch 374  Train loss: -0.7818181206198299  Val loss: -0.859995878029525
Epoch 375
-------------------------------
Batch 1/64 loss: -0.9047412872314453
Batch 2/64 loss: -0.6887693405151367
Batch 3/64 loss: -0.8780641555786133
Batch 4/64 loss: -0.7353906631469727
Batch 5/64 loss: -0.6538095474243164
Batch 6/64 loss: -0.8460350036621094
Batch 7/64 loss: -0.699091911315918
Batch 8/64 loss: -0.8694801330566406
Batch 9/64 loss: -0.7573366165161133
Batch 10/64 loss: -0.6300983428955078
Batch 11/64 loss: -0.8095645904541016
Batch 12/64 loss: -0.5605812072753906
Batch 13/64 loss: 0.06319808959960938
Batch 14/64 loss: -0.5759944915771484
Batch 15/64 loss: -0.397674560546875
Batch 16/64 loss: -0.7713813781738281
Batch 17/64 loss: -0.6656417846679688
Batch 18/64 loss: -0.9117412567138672
Batch 19/64 loss: -0.4439373016357422
Batch 20/64 loss: -0.7395038604736328
Batch 21/64 loss: -1.0309581756591797
Batch 22/64 loss: -0.9921884536743164
Batch 23/64 loss: -0.902496337890625
Batch 24/64 loss: 0.3558797836303711
Batch 25/64 loss: -0.5372581481933594
Batch 26/64 loss: -0.9718236923217773
Batch 27/64 loss: -0.6511850357055664
Batch 28/64 loss: -0.35889244079589844
Batch 29/64 loss: -0.7705583572387695
Batch 30/64 loss: -0.8568611145019531
Batch 31/64 loss: -0.8050661087036133
Batch 32/64 loss: -0.7891473770141602
Batch 33/64 loss: -1.004185676574707
Batch 34/64 loss: -0.7722806930541992
Batch 35/64 loss: -0.9636220932006836
Batch 36/64 loss: -0.9548263549804688
Batch 37/64 loss: -1.0387067794799805
Batch 38/64 loss: -0.9607248306274414
Batch 39/64 loss: -0.6491632461547852
Batch 40/64 loss: -0.4391298294067383
Batch 41/64 loss: -0.8375215530395508
Batch 42/64 loss: -0.8854351043701172
Batch 43/64 loss: -0.8140411376953125
Batch 44/64 loss: -0.8368568420410156
Batch 45/64 loss: -0.6428670883178711
Batch 46/64 loss: -0.7506351470947266
Batch 47/64 loss: -0.8514547348022461
Batch 48/64 loss: -0.3878650665283203
Batch 49/64 loss: -0.8624105453491211
Batch 50/64 loss: -0.8079977035522461
Batch 51/64 loss: -0.8144931793212891
Batch 52/64 loss: -0.6870651245117188
Batch 53/64 loss: -0.5363502502441406
Batch 54/64 loss: -0.3307218551635742
Batch 55/64 loss: -0.7965230941772461
Batch 56/64 loss: -0.8961954116821289
Batch 57/64 loss: -1.1260795593261719
Batch 58/64 loss: -0.9535064697265625
Batch 59/64 loss: -0.7879695892333984
Batch 60/64 loss: -0.5953035354614258
Batch 61/64 loss: -1.008157730102539
Batch 62/64 loss: -0.7441091537475586
Batch 63/64 loss: -0.5465717315673828
Batch 64/64 loss: -4.797114372253418
Epoch 375  Train loss: -0.779087085349887  Val loss: -0.6749065569585951
Epoch 376
-------------------------------
Batch 1/64 loss: -0.7141551971435547
Batch 2/64 loss: -0.7005243301391602
Batch 3/64 loss: -0.8355255126953125
Batch 4/64 loss: -0.7993812561035156
Batch 5/64 loss: -0.7985668182373047
Batch 6/64 loss: -0.8405551910400391
Batch 7/64 loss: -0.7196817398071289
Batch 8/64 loss: -0.5730819702148438
Batch 9/64 loss: -0.541168212890625
Batch 10/64 loss: -0.8292512893676758
Batch 11/64 loss: -0.5186176300048828
Batch 12/64 loss: -0.612980842590332
Batch 13/64 loss: -0.8029603958129883
Batch 14/64 loss: -1.0016307830810547
Batch 15/64 loss: -0.6497163772583008
Batch 16/64 loss: -0.2954549789428711
Batch 17/64 loss: -0.7761096954345703
Batch 18/64 loss: -0.43647289276123047
Batch 19/64 loss: -0.6897392272949219
Batch 20/64 loss: -1.0781745910644531
Batch 21/64 loss: -0.8891096115112305
Batch 22/64 loss: -0.8073005676269531
Batch 23/64 loss: -1.0411100387573242
Batch 24/64 loss: -0.7683858871459961
Batch 25/64 loss: -0.840728759765625
Batch 26/64 loss: -0.935481071472168
Batch 27/64 loss: -1.018507957458496
Batch 28/64 loss: -0.5509433746337891
Batch 29/64 loss: -0.7553110122680664
Batch 30/64 loss: -0.8460626602172852
Batch 31/64 loss: -0.6864252090454102
Batch 32/64 loss: -0.8780155181884766
Batch 33/64 loss: -0.8345241546630859
Batch 34/64 loss: -0.8127660751342773
Batch 35/64 loss: -0.6985769271850586
Batch 36/64 loss: -1.0034065246582031
Batch 37/64 loss: -0.8346261978149414
Batch 38/64 loss: -0.8345365524291992
Batch 39/64 loss: -0.6026773452758789
Batch 40/64 loss: -0.8883438110351562
Batch 41/64 loss: -0.6165237426757812
Batch 42/64 loss: -0.5349016189575195
Batch 43/64 loss: -0.8733415603637695
Batch 44/64 loss: -0.6257009506225586
Batch 45/64 loss: -0.4619903564453125
Batch 46/64 loss: -0.9129886627197266
Batch 47/64 loss: -0.621800422668457
Batch 48/64 loss: -1.1089496612548828
Batch 49/64 loss: -1.065516471862793
Batch 50/64 loss: -0.6593475341796875
Batch 51/64 loss: -0.7169618606567383
Batch 52/64 loss: -0.45360660552978516
Batch 53/64 loss: -1.0606718063354492
Batch 54/64 loss: -0.5001773834228516
Batch 55/64 loss: -0.6905145645141602
Batch 56/64 loss: -0.7989292144775391
Batch 57/64 loss: -0.8751735687255859
Batch 58/64 loss: -0.5778436660766602
Batch 59/64 loss: -0.9287557601928711
Batch 60/64 loss: -0.6672544479370117
Batch 61/64 loss: -0.07760810852050781
Batch 62/64 loss: -0.8690099716186523
Batch 63/64 loss: -0.5365257263183594
Batch 64/64 loss: -4.480295181274414
Epoch 376  Train loss: -0.7895671096502566  Val loss: -0.9334667507315829
Saving best model, epoch: 376
Epoch 377
-------------------------------
Batch 1/64 loss: -0.7890481948852539
Batch 2/64 loss: -0.7085371017456055
Batch 3/64 loss: -0.8790216445922852
Batch 4/64 loss: -0.7647762298583984
Batch 5/64 loss: -1.0261726379394531
Batch 6/64 loss: -1.0137596130371094
Batch 7/64 loss: -1.018320083618164
Batch 8/64 loss: -0.757594108581543
Batch 9/64 loss: -0.834986686706543
Batch 10/64 loss: -0.6692581176757812
Batch 11/64 loss: -0.6259660720825195
Batch 12/64 loss: -0.6298761367797852
Batch 13/64 loss: -0.6943588256835938
Batch 14/64 loss: -0.6596174240112305
Batch 15/64 loss: -0.5672674179077148
Batch 16/64 loss: -0.5220174789428711
Batch 17/64 loss: -0.5240936279296875
Batch 18/64 loss: -0.4758615493774414
Batch 19/64 loss: -0.7113895416259766
Batch 20/64 loss: -0.8889551162719727
Batch 21/64 loss: -1.0523691177368164
Batch 22/64 loss: 0.3124351501464844
Batch 23/64 loss: -0.7832269668579102
Batch 24/64 loss: -0.7721624374389648
Batch 25/64 loss: -0.790949821472168
Batch 26/64 loss: -0.7289800643920898
Batch 27/64 loss: -0.9997234344482422
Batch 28/64 loss: -0.7816047668457031
Batch 29/64 loss: -0.5899934768676758
Batch 30/64 loss: -0.9730081558227539
Batch 31/64 loss: -0.6712856292724609
Batch 32/64 loss: -0.42676830291748047
Batch 33/64 loss: -1.0831470489501953
Batch 34/64 loss: -0.5514659881591797
Batch 35/64 loss: -0.8165578842163086
Batch 36/64 loss: -0.1208028793334961
Batch 37/64 loss: -0.39519786834716797
Batch 38/64 loss: -0.2486248016357422
Batch 39/64 loss: -0.9920234680175781
Batch 40/64 loss: -0.4235353469848633
Batch 41/64 loss: -0.8369293212890625
Batch 42/64 loss: -0.7569618225097656
Batch 43/64 loss: -0.6419582366943359
Batch 44/64 loss: -0.6605262756347656
Batch 45/64 loss: -0.910548210144043
Batch 46/64 loss: -0.9536714553833008
Batch 47/64 loss: -0.9102745056152344
Batch 48/64 loss: -0.7273197174072266
Batch 49/64 loss: -0.6667089462280273
Batch 50/64 loss: -0.6589603424072266
Batch 51/64 loss: -0.8645248413085938
Batch 52/64 loss: -0.7093782424926758
Batch 53/64 loss: -0.7694587707519531
Batch 54/64 loss: -1.028951644897461
Batch 55/64 loss: -0.5368022918701172
Batch 56/64 loss: -0.3730592727661133
Batch 57/64 loss: -0.7949600219726562
Batch 58/64 loss: -0.8620986938476562
Batch 59/64 loss: -0.7493486404418945
Batch 60/64 loss: -0.7784547805786133
Batch 61/64 loss: -1.0075311660766602
Batch 62/64 loss: -0.8197259902954102
Batch 63/64 loss: -0.565800666809082
Batch 64/64 loss: -4.886244773864746
Epoch 377  Train loss: -0.7670354095159793  Val loss: -0.8929657034857577
Epoch 378
-------------------------------
Batch 1/64 loss: -0.8217639923095703
Batch 2/64 loss: -1.1378545761108398
Batch 3/64 loss: -0.9386043548583984
Batch 4/64 loss: -0.6993141174316406
Batch 5/64 loss: -0.8864364624023438
Batch 6/64 loss: -0.7390565872192383
Batch 7/64 loss: -0.9142141342163086
Batch 8/64 loss: -1.0866985321044922
Batch 9/64 loss: -0.5049953460693359
Batch 10/64 loss: -0.7060403823852539
Batch 11/64 loss: -0.4929780960083008
Batch 12/64 loss: -0.33426856994628906
Batch 13/64 loss: -0.2081003189086914
Batch 14/64 loss: -0.57366943359375
Batch 15/64 loss: -0.3452157974243164
Batch 16/64 loss: -0.7311849594116211
Batch 17/64 loss: -0.7265777587890625
Batch 18/64 loss: -0.8226509094238281
Batch 19/64 loss: -0.9017086029052734
Batch 20/64 loss: -0.5204067230224609
Batch 21/64 loss: -1.0479373931884766
Batch 22/64 loss: -0.6341524124145508
Batch 23/64 loss: -0.9428625106811523
Batch 24/64 loss: -0.7896604537963867
Batch 25/64 loss: -0.8004264831542969
Batch 26/64 loss: -0.9554519653320312
Batch 27/64 loss: -0.7330961227416992
Batch 28/64 loss: -0.9871940612792969
Batch 29/64 loss: -0.867945671081543
Batch 30/64 loss: -0.9990520477294922
Batch 31/64 loss: -0.7348604202270508
Batch 32/64 loss: -0.6463689804077148
Batch 33/64 loss: -0.9907512664794922
Batch 34/64 loss: -0.6735906600952148
Batch 35/64 loss: -0.9061851501464844
Batch 36/64 loss: -0.9369087219238281
Batch 37/64 loss: -0.7185430526733398
Batch 38/64 loss: -0.8308181762695312
Batch 39/64 loss: -1.0367069244384766
Batch 40/64 loss: -0.9966144561767578
Batch 41/64 loss: -0.9791164398193359
Batch 42/64 loss: -0.9097633361816406
Batch 43/64 loss: -0.6261701583862305
Batch 44/64 loss: -0.7203187942504883
Batch 45/64 loss: -0.6273994445800781
Batch 46/64 loss: -0.913609504699707
Batch 47/64 loss: -0.6050338745117188
Batch 48/64 loss: -0.6404552459716797
Batch 49/64 loss: -0.6763191223144531
Batch 50/64 loss: -0.621577262878418
Batch 51/64 loss: 0.4383821487426758
Batch 52/64 loss: -0.779994010925293
Batch 53/64 loss: -0.8449792861938477
Batch 54/64 loss: -0.4970273971557617
Batch 55/64 loss: -0.9102554321289062
Batch 56/64 loss: -0.6835718154907227
Batch 57/64 loss: -0.8185281753540039
Batch 58/64 loss: -0.7086219787597656
Batch 59/64 loss: -0.4289112091064453
Batch 60/64 loss: -0.7976980209350586
Batch 61/64 loss: -0.14719200134277344
Batch 62/64 loss: -1.0561609268188477
Batch 63/64 loss: -0.7297487258911133
Batch 64/64 loss: -4.706188201904297
Epoch 378  Train loss: -0.7864247490377987  Val loss: -0.5673321596125966
Epoch 379
-------------------------------
Batch 1/64 loss: -0.7259044647216797
Batch 2/64 loss: -0.5908002853393555
Batch 3/64 loss: -0.3700246810913086
Batch 4/64 loss: -0.8532314300537109
Batch 5/64 loss: -0.8398590087890625
Batch 6/64 loss: -0.7191753387451172
Batch 7/64 loss: 0.009675025939941406
Batch 8/64 loss: -0.6624059677124023
Batch 9/64 loss: -0.5246706008911133
Batch 10/64 loss: -0.8811836242675781
Batch 11/64 loss: -0.9347114562988281
Batch 12/64 loss: -0.9393882751464844
Batch 13/64 loss: -0.548893928527832
Batch 14/64 loss: -1.0920753479003906
Batch 15/64 loss: -0.9042587280273438
Batch 16/64 loss: -0.9400186538696289
Batch 17/64 loss: -0.5404844284057617
Batch 18/64 loss: -0.8821239471435547
Batch 19/64 loss: -0.8538017272949219
Batch 20/64 loss: -1.0093021392822266
Batch 21/64 loss: -0.9325857162475586
Batch 22/64 loss: -1.043910026550293
Batch 23/64 loss: 0.12891483306884766
Batch 24/64 loss: -0.5044898986816406
Batch 25/64 loss: -0.7827463150024414
Batch 26/64 loss: -0.5088810920715332
Batch 27/64 loss: -0.6810922622680664
Batch 28/64 loss: -1.0641756057739258
Batch 29/64 loss: -0.7264471054077148
Batch 30/64 loss: -0.9013042449951172
Batch 31/64 loss: -0.5903139114379883
Batch 32/64 loss: -0.8321533203125
Batch 33/64 loss: -0.5164709091186523
Batch 34/64 loss: -0.9702329635620117
Batch 35/64 loss: -0.6375951766967773
Batch 36/64 loss: -0.7943820953369141
Batch 37/64 loss: -0.43617916107177734
Batch 38/64 loss: -0.3450183868408203
Batch 39/64 loss: -0.525416374206543
Batch 40/64 loss: -0.6173944473266602
Batch 41/64 loss: -0.8713016510009766
Batch 42/64 loss: -1.1718673706054688
Batch 43/64 loss: -1.017190933227539
Batch 44/64 loss: -0.7728385925292969
Batch 45/64 loss: -0.8794851303100586
Batch 46/64 loss: -0.7802162170410156
Batch 47/64 loss: -0.819697380065918
Batch 48/64 loss: -0.7801923751831055
Batch 49/64 loss: -0.4758281707763672
Batch 50/64 loss: -0.635716438293457
Batch 51/64 loss: -0.5749111175537109
Batch 52/64 loss: -0.7940835952758789
Batch 53/64 loss: -1.1489553451538086
Batch 54/64 loss: -0.9286098480224609
Batch 55/64 loss: -0.7424449920654297
Batch 56/64 loss: -0.8938941955566406
Batch 57/64 loss: -0.7340402603149414
Batch 58/64 loss: -0.6723108291625977
Batch 59/64 loss: -0.5954437255859375
Batch 60/64 loss: -0.7995061874389648
Batch 61/64 loss: -0.8814687728881836
Batch 62/64 loss: -0.9706144332885742
Batch 63/64 loss: -0.8600969314575195
Batch 64/64 loss: -4.741000652313232
Epoch 379  Train loss: -0.791231030108882  Val loss: -0.790346597887806
Epoch 380
-------------------------------
Batch 1/64 loss: -0.8357639312744141
Batch 2/64 loss: -0.912750244140625
Batch 3/64 loss: -0.8025693893432617
Batch 4/64 loss: -0.31249332427978516
Batch 5/64 loss: -0.713282585144043
Batch 6/64 loss: -0.7086734771728516
Batch 7/64 loss: -0.9738597869873047
Batch 8/64 loss: -0.3831644058227539
Batch 9/64 loss: -0.8318653106689453
Batch 10/64 loss: -0.4984407424926758
Batch 11/64 loss: -0.26200389862060547
Batch 12/64 loss: -0.8534870147705078
Batch 13/64 loss: -0.5887794494628906
Batch 14/64 loss: -1.0254192352294922
Batch 15/64 loss: -0.7628898620605469
Batch 16/64 loss: -1.140702247619629
Batch 17/64 loss: -0.694758415222168
Batch 18/64 loss: -1.0206241607666016
Batch 19/64 loss: -0.4986438751220703
Batch 20/64 loss: -0.84814453125
Batch 21/64 loss: -0.2613687515258789
Batch 22/64 loss: -0.7839746475219727
Batch 23/64 loss: -0.9026899337768555
Batch 24/64 loss: -0.7311716079711914
Batch 25/64 loss: -0.8000478744506836
Batch 26/64 loss: -0.8914003372192383
Batch 27/64 loss: -0.6663656234741211
Batch 28/64 loss: -0.7732954025268555
Batch 29/64 loss: -0.7689809799194336
Batch 30/64 loss: -0.7331647872924805
Batch 31/64 loss: -1.0801124572753906
Batch 32/64 loss: -0.5514001846313477
Batch 33/64 loss: -0.9867687225341797
Batch 34/64 loss: -0.8176765441894531
Batch 35/64 loss: -0.6648149490356445
Batch 36/64 loss: -0.5060644149780273
Batch 37/64 loss: -0.24443912506103516
Batch 38/64 loss: -0.8311281204223633
Batch 39/64 loss: -0.7110719680786133
Batch 40/64 loss: -0.9146757125854492
Batch 41/64 loss: -0.6261091232299805
Batch 42/64 loss: -0.832305908203125
Batch 43/64 loss: -0.3173942565917969
Batch 44/64 loss: -0.3910808563232422
Batch 45/64 loss: -0.5675802230834961
Batch 46/64 loss: -1.0580530166625977
Batch 47/64 loss: -0.8263168334960938
Batch 48/64 loss: -0.8504877090454102
Batch 49/64 loss: -0.9100942611694336
Batch 50/64 loss: -0.3295326232910156
Batch 51/64 loss: -0.9819746017456055
Batch 52/64 loss: -0.7364072799682617
Batch 53/64 loss: -0.9667873382568359
Batch 54/64 loss: -0.5382270812988281
Batch 55/64 loss: -0.9221763610839844
Batch 56/64 loss: -0.7702322006225586
Batch 57/64 loss: -0.9668989181518555
Batch 58/64 loss: -0.8541584014892578
Batch 59/64 loss: -0.6877403259277344
Batch 60/64 loss: -0.7135801315307617
Batch 61/64 loss: -0.5968399047851562
Batch 62/64 loss: -0.9965696334838867
Batch 63/64 loss: -0.7829084396362305
Batch 64/64 loss: -4.812800407409668
Epoch 380  Train loss: -0.78622719633813  Val loss: -0.8840294408634356
Epoch 381
-------------------------------
Batch 1/64 loss: -0.8106861114501953
Batch 2/64 loss: -0.8522768020629883
Batch 3/64 loss: -0.7042388916015625
Batch 4/64 loss: -0.7558479309082031
Batch 5/64 loss: -0.5218496322631836
Batch 6/64 loss: -0.9905538558959961
Batch 7/64 loss: -0.9299650192260742
Batch 8/64 loss: -0.876373291015625
Batch 9/64 loss: -0.80328369140625
Batch 10/64 loss: -0.5353851318359375
Batch 11/64 loss: -1.0730924606323242
Batch 12/64 loss: -0.6481800079345703
Batch 13/64 loss: -0.8655872344970703
Batch 14/64 loss: -0.23247146606445312
Batch 15/64 loss: -0.7550506591796875
Batch 16/64 loss: -0.7047996520996094
Batch 17/64 loss: -0.6476545333862305
Batch 18/64 loss: -0.5169897079467773
Batch 19/64 loss: -1.0077190399169922
Batch 20/64 loss: -0.9317283630371094
Batch 21/64 loss: -1.0774421691894531
Batch 22/64 loss: -0.12541866302490234
Batch 23/64 loss: -0.8282651901245117
Batch 24/64 loss: -0.5465936660766602
Batch 25/64 loss: -0.23114299774169922
Batch 26/64 loss: -0.4456911087036133
Batch 27/64 loss: -1.2162513732910156
Batch 28/64 loss: -0.6736555099487305
Batch 29/64 loss: -0.97802734375
Batch 30/64 loss: -0.6361961364746094
Batch 31/64 loss: -0.8948698043823242
Batch 32/64 loss: -0.6896257400512695
Batch 33/64 loss: -1.0320682525634766
Batch 34/64 loss: -0.8499259948730469
Batch 35/64 loss: -1.1203737258911133
Batch 36/64 loss: -0.5964984893798828
Batch 37/64 loss: -0.65911865234375
Batch 38/64 loss: -0.8696660995483398
Batch 39/64 loss: -0.7321186065673828
Batch 40/64 loss: -0.90594482421875
Batch 41/64 loss: -0.6561174392700195
Batch 42/64 loss: -0.5716514587402344
Batch 43/64 loss: -0.36430931091308594
Batch 44/64 loss: -0.6290273666381836
Batch 45/64 loss: -0.7711191177368164
Batch 46/64 loss: -0.8093461990356445
Batch 47/64 loss: -0.6500186920166016
Batch 48/64 loss: -0.7540168762207031
Batch 49/64 loss: 0.16791963577270508
Batch 50/64 loss: -0.7719812393188477
Batch 51/64 loss: -0.8134069442749023
Batch 52/64 loss: -1.216104507446289
Batch 53/64 loss: -1.056375503540039
Batch 54/64 loss: -0.6000251770019531
Batch 55/64 loss: -0.9314632415771484
Batch 56/64 loss: -0.6372013092041016
Batch 57/64 loss: -0.8728885650634766
Batch 58/64 loss: -0.14407920837402344
Batch 59/64 loss: -0.6989116668701172
Batch 60/64 loss: -0.8527584075927734
Batch 61/64 loss: -0.9292678833007812
Batch 62/64 loss: -0.6573324203491211
Batch 63/64 loss: -0.9546375274658203
Batch 64/64 loss: -4.250026702880859
Epoch 381  Train loss: -0.7785767573936313  Val loss: -0.8188227492919082
Epoch 382
-------------------------------
Batch 1/64 loss: -0.7581090927124023
Batch 2/64 loss: -0.5006122589111328
Batch 3/64 loss: -0.4595222473144531
Batch 4/64 loss: -0.7141036987304688
Batch 5/64 loss: -0.7546682357788086
Batch 6/64 loss: -0.9513216018676758
Batch 7/64 loss: -0.682316780090332
Batch 8/64 loss: -0.532771110534668
Batch 9/64 loss: -0.583587646484375
Batch 10/64 loss: -1.0117273330688477
Batch 11/64 loss: -0.7339811325073242
Batch 12/64 loss: -0.831878662109375
Batch 13/64 loss: -0.7408475875854492
Batch 14/64 loss: -0.6295585632324219
Batch 15/64 loss: -0.7363138198852539
Batch 16/64 loss: -0.7536096572875977
Batch 17/64 loss: -0.7940874099731445
Batch 18/64 loss: -0.7980422973632812
Batch 19/64 loss: -0.9955663681030273
Batch 20/64 loss: -0.5162153244018555
Batch 21/64 loss: -0.5416216850280762
Batch 22/64 loss: -1.004450798034668
Batch 23/64 loss: -0.7493562698364258
Batch 24/64 loss: -0.8584785461425781
Batch 25/64 loss: -0.9095611572265625
Batch 26/64 loss: -0.9411172866821289
Batch 27/64 loss: -0.7046308517456055
Batch 28/64 loss: -0.8888998031616211
Batch 29/64 loss: -0.8572254180908203
Batch 30/64 loss: -0.7521390914916992
Batch 31/64 loss: -0.664276123046875
Batch 32/64 loss: -1.1719112396240234
Batch 33/64 loss: -0.717595100402832
Batch 34/64 loss: -0.6386280059814453
Batch 35/64 loss: -0.7451276779174805
Batch 36/64 loss: -0.6834077835083008
Batch 37/64 loss: -0.6369943618774414
Batch 38/64 loss: -0.2956352233886719
Batch 39/64 loss: -0.8602838516235352
Batch 40/64 loss: -0.3807640075683594
Batch 41/64 loss: -0.4202747344970703
Batch 42/64 loss: -0.4091329574584961
Batch 43/64 loss: -0.3536720275878906
Batch 44/64 loss: -1.0179691314697266
Batch 45/64 loss: -0.8741588592529297
Batch 46/64 loss: -0.5563430786132812
Batch 47/64 loss: -0.7732734680175781
Batch 48/64 loss: -1.0026283264160156
Batch 49/64 loss: -0.8609790802001953
Batch 50/64 loss: -0.5898065567016602
Batch 51/64 loss: -0.9659557342529297
Batch 52/64 loss: -0.6500120162963867
Batch 53/64 loss: -0.91278076171875
Batch 54/64 loss: -0.9775276184082031
Batch 55/64 loss: -0.8975667953491211
Batch 56/64 loss: -0.6011724472045898
Batch 57/64 loss: -0.7714786529541016
Batch 58/64 loss: -0.9896383285522461
Batch 59/64 loss: -0.937957763671875
Batch 60/64 loss: -0.4398307800292969
Batch 61/64 loss: -0.2989168167114258
Batch 62/64 loss: -0.5362691879272461
Batch 63/64 loss: -0.9814577102661133
Batch 64/64 loss: -5.0830841064453125
Epoch 382  Train loss: -0.7860715454699947  Val loss: -0.7546620975245315
Epoch 383
-------------------------------
Batch 1/64 loss: -0.680079460144043
Batch 2/64 loss: -0.7053823471069336
Batch 3/64 loss: -0.9001665115356445
Batch 4/64 loss: -1.005324363708496
Batch 5/64 loss: -0.48255348205566406
Batch 6/64 loss: -0.8274660110473633
Batch 7/64 loss: -0.8698501586914062
Batch 8/64 loss: -0.7662067413330078
Batch 9/64 loss: -1.1451311111450195
Batch 10/64 loss: -0.38234519958496094
Batch 11/64 loss: -0.9143362045288086
Batch 12/64 loss: -0.45687198638916016
Batch 13/64 loss: -0.5268802642822266
Batch 14/64 loss: -0.18453025817871094
Batch 15/64 loss: -0.7220430374145508
Batch 16/64 loss: -0.69366455078125
Batch 17/64 loss: -0.04165363311767578
Batch 18/64 loss: -0.7928142547607422
Batch 19/64 loss: -0.4659738540649414
Batch 20/64 loss: -0.7034845352172852
Batch 21/64 loss: -0.9713191986083984
Batch 22/64 loss: -0.7751522064208984
Batch 23/64 loss: -0.799473762512207
Batch 24/64 loss: -0.9002981185913086
Batch 25/64 loss: -0.6204910278320312
Batch 26/64 loss: -0.5428323745727539
Batch 27/64 loss: -0.9010677337646484
Batch 28/64 loss: -0.9699211120605469
Batch 29/64 loss: -0.9574670791625977
Batch 30/64 loss: -0.64154052734375
Batch 31/64 loss: -0.9712848663330078
Batch 32/64 loss: -0.37153053283691406
Batch 33/64 loss: -0.9783668518066406
Batch 34/64 loss: -0.8725833892822266
Batch 35/64 loss: -0.8151845932006836
Batch 36/64 loss: -0.9527006149291992
Batch 37/64 loss: -0.556950569152832
Batch 38/64 loss: -0.9431495666503906
Batch 39/64 loss: -0.8397006988525391
Batch 40/64 loss: -0.8558063507080078
Batch 41/64 loss: -0.7553567886352539
Batch 42/64 loss: -0.6751708984375
Batch 43/64 loss: -0.6661863327026367
Batch 44/64 loss: -0.924525260925293
Batch 45/64 loss: -0.6238412857055664
Batch 46/64 loss: -0.7752237319946289
Batch 47/64 loss: -0.3892841339111328
Batch 48/64 loss: -1.0049934387207031
Batch 49/64 loss: -0.9881649017333984
Batch 50/64 loss: -1.1761236190795898
Batch 51/64 loss: -0.5790805816650391
Batch 52/64 loss: -0.9711856842041016
Batch 53/64 loss: -0.8485250473022461
Batch 54/64 loss: -1.0687675476074219
Batch 55/64 loss: -0.8648653030395508
Batch 56/64 loss: -0.8078174591064453
Batch 57/64 loss: -0.4782543182373047
Batch 58/64 loss: -0.849299430847168
Batch 59/64 loss: -0.7868871688842773
Batch 60/64 loss: -0.8336601257324219
Batch 61/64 loss: -0.9267501831054688
Batch 62/64 loss: -1.0589237213134766
Batch 63/64 loss: -0.17034149169921875
Batch 64/64 loss: -4.748146057128906
Epoch 383  Train loss: -0.804516347249349  Val loss: -0.9572800311845603
Saving best model, epoch: 383
Epoch 384
-------------------------------
Batch 1/64 loss: -0.7353067398071289
Batch 2/64 loss: -1.0388946533203125
Batch 3/64 loss: -0.9447431564331055
Batch 4/64 loss: -0.6012172698974609
Batch 5/64 loss: -0.3448190689086914
Batch 6/64 loss: -0.6908912658691406
Batch 7/64 loss: -0.7115650177001953
Batch 8/64 loss: -0.999110221862793
Batch 9/64 loss: -0.4324808120727539
Batch 10/64 loss: -0.7044363021850586
Batch 11/64 loss: -0.8543357849121094
Batch 12/64 loss: -0.7606496810913086
Batch 13/64 loss: -0.36548614501953125
Batch 14/64 loss: -1.0416669845581055
Batch 15/64 loss: -0.6938390731811523
Batch 16/64 loss: -0.5932245254516602
Batch 17/64 loss: -1.1054487228393555
Batch 18/64 loss: -0.8489246368408203
Batch 19/64 loss: -0.2062082290649414
Batch 20/64 loss: -1.1194963455200195
Batch 21/64 loss: -0.5987176895141602
Batch 22/64 loss: -1.0229835510253906
Batch 23/64 loss: -0.7528829574584961
Batch 24/64 loss: -1.0028448104858398
Batch 25/64 loss: -0.9865436553955078
Batch 26/64 loss: -1.1410894393920898
Batch 27/64 loss: -0.9227590560913086
Batch 28/64 loss: -0.485687255859375
Batch 29/64 loss: -0.7360172271728516
Batch 30/64 loss: -0.968897819519043
Batch 31/64 loss: -0.6633701324462891
Batch 32/64 loss: -0.7809896469116211
Batch 33/64 loss: -0.8494644165039062
Batch 34/64 loss: -0.6046342849731445
Batch 35/64 loss: -0.9905023574829102
Batch 36/64 loss: -0.5166702270507812
Batch 37/64 loss: -0.657841682434082
Batch 38/64 loss: -0.9918432235717773
Batch 39/64 loss: -0.8886957168579102
Batch 40/64 loss: -0.8837203979492188
Batch 41/64 loss: -1.0154190063476562
Batch 42/64 loss: -0.6518735885620117
Batch 43/64 loss: -0.5703153610229492
Batch 44/64 loss: -0.9421005249023438
Batch 45/64 loss: -0.9350347518920898
Batch 46/64 loss: 0.30855894088745117
Batch 47/64 loss: 0.0975179672241211
Batch 48/64 loss: -0.602076530456543
Batch 49/64 loss: -0.988006591796875
Batch 50/64 loss: -0.7466640472412109
Batch 51/64 loss: -1.0658025741577148
Batch 52/64 loss: -0.9598541259765625
Batch 53/64 loss: -0.9264545440673828
Batch 54/64 loss: -0.9994306564331055
Batch 55/64 loss: -0.8300724029541016
Batch 56/64 loss: -0.970799446105957
Batch 57/64 loss: -0.6489763259887695
Batch 58/64 loss: -0.4835548400878906
Batch 59/64 loss: -0.9548110961914062
Batch 60/64 loss: -0.6600675582885742
Batch 61/64 loss: -0.6496877670288086
Batch 62/64 loss: -0.49208974838256836
Batch 63/64 loss: -0.7158489227294922
Batch 64/64 loss: -4.845510959625244
Epoch 384  Train loss: -0.8043277946172976  Val loss: -0.5925796220392706
Epoch 385
-------------------------------
Batch 1/64 loss: -1.0035858154296875
Batch 2/64 loss: -0.8855628967285156
Batch 3/64 loss: -0.7781143188476562
Batch 4/64 loss: -0.2788705825805664
Batch 5/64 loss: -0.7498998641967773
Batch 6/64 loss: -0.6051158905029297
Batch 7/64 loss: -1.0452861785888672
Batch 8/64 loss: -1.0012311935424805
Batch 9/64 loss: -0.17900848388671875
Batch 10/64 loss: -0.730189323425293
Batch 11/64 loss: -0.0012445449829101562
Batch 12/64 loss: -0.7775583267211914
Batch 13/64 loss: -0.24831867218017578
Batch 14/64 loss: -1.0314226150512695
Batch 15/64 loss: -0.9124565124511719
Batch 16/64 loss: -0.8221244812011719
Batch 17/64 loss: -0.8150720596313477
Batch 18/64 loss: -0.7992439270019531
Batch 19/64 loss: -0.8205108642578125
Batch 20/64 loss: -0.5623621940612793
Batch 21/64 loss: -0.3603677749633789
Batch 22/64 loss: -0.6165990829467773
Batch 23/64 loss: -0.8682289123535156
Batch 24/64 loss: -0.6295642852783203
Batch 25/64 loss: -0.5243091583251953
Batch 26/64 loss: -0.8640270233154297
Batch 27/64 loss: -0.7612733840942383
Batch 28/64 loss: -0.6418962478637695
Batch 29/64 loss: -0.4434165954589844
Batch 30/64 loss: -0.9976139068603516
Batch 31/64 loss: -0.8482542037963867
Batch 32/64 loss: -0.8533639907836914
Batch 33/64 loss: -0.26551342010498047
Batch 34/64 loss: -0.9367456436157227
Batch 35/64 loss: -0.9142932891845703
Batch 36/64 loss: -0.6067638397216797
Batch 37/64 loss: -0.7584972381591797
Batch 38/64 loss: -0.7967433929443359
Batch 39/64 loss: -0.5851764678955078
Batch 40/64 loss: -0.4708681106567383
Batch 41/64 loss: -0.7675104141235352
Batch 42/64 loss: -0.7045078277587891
Batch 43/64 loss: -0.9811277389526367
Batch 44/64 loss: -0.6066522598266602
Batch 45/64 loss: -0.8598546981811523
Batch 46/64 loss: -0.39733314514160156
Batch 47/64 loss: -0.6753864288330078
Batch 48/64 loss: -0.7808923721313477
Batch 49/64 loss: -0.7408590316772461
Batch 50/64 loss: -0.9995641708374023
Batch 51/64 loss: -1.0831165313720703
Batch 52/64 loss: -0.3709402084350586
Batch 53/64 loss: -0.5533123016357422
Batch 54/64 loss: -0.9653835296630859
Batch 55/64 loss: -0.6756782531738281
Batch 56/64 loss: 0.08422613143920898
Batch 57/64 loss: -0.8385992050170898
Batch 58/64 loss: -0.8664226531982422
Batch 59/64 loss: -0.5851764678955078
Batch 60/64 loss: -0.39054012298583984
Batch 61/64 loss: -1.0643243789672852
Batch 62/64 loss: -0.5959348678588867
Batch 63/64 loss: -0.7326908111572266
Batch 64/64 loss: -4.181646823883057
Epoch 385  Train loss: -0.7384864489237467  Val loss: -0.8741060499473126
Epoch 386
-------------------------------
Batch 1/64 loss: -1.003270149230957
Batch 2/64 loss: -0.8301515579223633
Batch 3/64 loss: -0.6663999557495117
Batch 4/64 loss: -0.7197046279907227
Batch 5/64 loss: -1.0142707824707031
Batch 6/64 loss: -0.7398509979248047
Batch 7/64 loss: -0.6880989074707031
Batch 8/64 loss: -0.7714214324951172
Batch 9/64 loss: -0.7592191696166992
Batch 10/64 loss: -0.6002197265625
Batch 11/64 loss: -0.843022346496582
Batch 12/64 loss: -1.006540298461914
Batch 13/64 loss: -0.7565422058105469
Batch 14/64 loss: -0.7984104156494141
Batch 15/64 loss: -0.35797977447509766
Batch 16/64 loss: -0.7813339233398438
Batch 17/64 loss: -0.9525117874145508
Batch 18/64 loss: -0.6025600433349609
Batch 19/64 loss: -0.8025846481323242
Batch 20/64 loss: -0.29984569549560547
Batch 21/64 loss: -1.023207664489746
Batch 22/64 loss: -0.6902046203613281
Batch 23/64 loss: -0.639491081237793
Batch 24/64 loss: -0.938044548034668
Batch 25/64 loss: -0.8771762847900391
Batch 26/64 loss: -0.7931289672851562
Batch 27/64 loss: -1.214803695678711
Batch 28/64 loss: -0.95697021484375
Batch 29/64 loss: -1.027292251586914
Batch 30/64 loss: -1.1761703491210938
Batch 31/64 loss: -0.5942764282226562
Batch 32/64 loss: -0.6510248184204102
Batch 33/64 loss: -0.20710039138793945
Batch 34/64 loss: -0.8146810531616211
Batch 35/64 loss: -0.5943994522094727
Batch 36/64 loss: -0.5737485885620117
Batch 37/64 loss: -1.077117919921875
Batch 38/64 loss: -0.06623649597167969
Batch 39/64 loss: -1.0661611557006836
Batch 40/64 loss: -0.5769281387329102
Batch 41/64 loss: -0.9473056793212891
Batch 42/64 loss: -0.911402702331543
Batch 43/64 loss: -0.6795282363891602
Batch 44/64 loss: -0.7020435333251953
Batch 45/64 loss: -0.6421003341674805
Batch 46/64 loss: -0.6764259338378906
Batch 47/64 loss: -0.5992059707641602
Batch 48/64 loss: -0.8214101791381836
Batch 49/64 loss: -0.4515056610107422
Batch 50/64 loss: -0.7175769805908203
Batch 51/64 loss: -0.7145805358886719
Batch 52/64 loss: -0.27254772186279297
Batch 53/64 loss: -0.8225641250610352
Batch 54/64 loss: -0.8651037216186523
Batch 55/64 loss: -0.5283145904541016
Batch 56/64 loss: -0.6828689575195312
Batch 57/64 loss: -1.0015869140625
Batch 58/64 loss: -0.556157112121582
Batch 59/64 loss: -0.9911212921142578
Batch 60/64 loss: -0.9012041091918945
Batch 61/64 loss: -0.9601116180419922
Batch 62/64 loss: -0.5801210403442383
Batch 63/64 loss: -0.7706003189086914
Batch 64/64 loss: -4.974316120147705
Epoch 386  Train loss: -0.8012270890030206  Val loss: -0.7833540480571104
Epoch 387
-------------------------------
Batch 1/64 loss: -0.735600471496582
Batch 2/64 loss: -0.4922151565551758
Batch 3/64 loss: -1.0094680786132812
Batch 4/64 loss: -0.7714824676513672
Batch 5/64 loss: -0.7352743148803711
Batch 6/64 loss: -0.6358985900878906
Batch 7/64 loss: -0.46813106536865234
Batch 8/64 loss: -0.7662429809570312
Batch 9/64 loss: -0.5836038589477539
Batch 10/64 loss: -0.9132490158081055
Batch 11/64 loss: -0.6983985900878906
Batch 12/64 loss: -0.45615196228027344
Batch 13/64 loss: -0.8922929763793945
Batch 14/64 loss: 0.042140960693359375
Batch 15/64 loss: -0.6780643463134766
Batch 16/64 loss: -0.6546173095703125
Batch 17/64 loss: -0.5672016143798828
Batch 18/64 loss: -0.5727291107177734
Batch 19/64 loss: -0.8944730758666992
Batch 20/64 loss: -0.9481821060180664
Batch 21/64 loss: -0.6404266357421875
Batch 22/64 loss: -0.3634634017944336
Batch 23/64 loss: -0.7373886108398438
Batch 24/64 loss: -1.146209716796875
Batch 25/64 loss: -0.8935127258300781
Batch 26/64 loss: -0.8641319274902344
Batch 27/64 loss: -0.36962318420410156
Batch 28/64 loss: -1.1334905624389648
Batch 29/64 loss: -0.6968193054199219
Batch 30/64 loss: -1.1642990112304688
Batch 31/64 loss: -0.9288606643676758
Batch 32/64 loss: -0.855280876159668
Batch 33/64 loss: -0.5376739501953125
Batch 34/64 loss: -0.7098827362060547
Batch 35/64 loss: -0.7048873901367188
Batch 36/64 loss: -0.7653884887695312
Batch 37/64 loss: -0.6405963897705078
Batch 38/64 loss: -0.5664558410644531
Batch 39/64 loss: -0.4032459259033203
Batch 40/64 loss: -0.8075656890869141
Batch 41/64 loss: -0.8211021423339844
Batch 42/64 loss: 0.05286121368408203
Batch 43/64 loss: -0.7618608474731445
Batch 44/64 loss: -1.0632095336914062
Batch 45/64 loss: -0.31372547149658203
Batch 46/64 loss: -0.8207130432128906
Batch 47/64 loss: -0.7052268981933594
Batch 48/64 loss: -0.4334859848022461
Batch 49/64 loss: -0.5128107070922852
Batch 50/64 loss: -0.5308523178100586
Batch 51/64 loss: -0.744084358215332
Batch 52/64 loss: -0.7156772613525391
Batch 53/64 loss: -0.6445074081420898
Batch 54/64 loss: -0.8537836074829102
Batch 55/64 loss: -0.44637203216552734
Batch 56/64 loss: -1.1941919326782227
Batch 57/64 loss: -0.9774532318115234
Batch 58/64 loss: -0.057608604431152344
Batch 59/64 loss: -0.9594049453735352
Batch 60/64 loss: -0.5542812347412109
Batch 61/64 loss: -0.8931493759155273
Batch 62/64 loss: -0.30931520462036133
Batch 63/64 loss: -0.6134805679321289
Batch 64/64 loss: -4.698367595672607
Epoch 387  Train loss: -0.7334517703336828  Val loss: -0.7887084410362637
Epoch 388
-------------------------------
Batch 1/64 loss: -0.38883113861083984
Batch 2/64 loss: -0.4227628707885742
Batch 3/64 loss: -1.1286563873291016
Batch 4/64 loss: -0.28365516662597656
Batch 5/64 loss: -0.5460729598999023
Batch 6/64 loss: -0.6413326263427734
Batch 7/64 loss: -0.7916994094848633
Batch 8/64 loss: -0.7057523727416992
Batch 9/64 loss: -0.7384481430053711
Batch 10/64 loss: -0.48237133026123047
Batch 11/64 loss: -0.5052013397216797
Batch 12/64 loss: -0.8810443878173828
Batch 13/64 loss: -0.6398496627807617
Batch 14/64 loss: -0.8511734008789062
Batch 15/64 loss: -0.6685991287231445
Batch 16/64 loss: -0.7171182632446289
Batch 17/64 loss: -0.7720928192138672
Batch 18/64 loss: -0.8047990798950195
Batch 19/64 loss: -0.9513559341430664
Batch 20/64 loss: -0.7995729446411133
Batch 21/64 loss: -0.9372806549072266
Batch 22/64 loss: -1.0838727951049805
Batch 23/64 loss: -0.8144378662109375
Batch 24/64 loss: -0.8571929931640625
Batch 25/64 loss: -0.7921104431152344
Batch 26/64 loss: -0.7910890579223633
Batch 27/64 loss: -0.9363803863525391
Batch 28/64 loss: -0.644526481628418
Batch 29/64 loss: -0.34159040451049805
Batch 30/64 loss: -0.549891471862793
Batch 31/64 loss: -0.6882162094116211
Batch 32/64 loss: -0.8668909072875977
Batch 33/64 loss: -0.8767280578613281
Batch 34/64 loss: -0.9848117828369141
Batch 35/64 loss: -0.8892803192138672
Batch 36/64 loss: -0.08238792419433594
Batch 37/64 loss: -0.6867818832397461
Batch 38/64 loss: -0.941554069519043
Batch 39/64 loss: -0.8976860046386719
Batch 40/64 loss: -0.8308448791503906
Batch 41/64 loss: -0.597203254699707
Batch 42/64 loss: -0.5676078796386719
Batch 43/64 loss: -0.7898321151733398
Batch 44/64 loss: -0.8621435165405273
Batch 45/64 loss: -0.8484964370727539
Batch 46/64 loss: -0.6563053131103516
Batch 47/64 loss: -0.7271900177001953
Batch 48/64 loss: -0.9393558502197266
Batch 49/64 loss: -1.023036003112793
Batch 50/64 loss: -0.34175682067871094
Batch 51/64 loss: -0.9188060760498047
Batch 52/64 loss: -0.004115581512451172
Batch 53/64 loss: -0.8291444778442383
Batch 54/64 loss: -0.5893306732177734
Batch 55/64 loss: -0.5684604644775391
Batch 56/64 loss: -0.5810813903808594
Batch 57/64 loss: -0.94793701171875
Batch 58/64 loss: -0.7957544326782227
Batch 59/64 loss: -0.9987373352050781
Batch 60/64 loss: -0.5257701873779297
Batch 61/64 loss: -0.9794721603393555
Batch 62/64 loss: -0.9946708679199219
Batch 63/64 loss: -0.8499279022216797
Batch 64/64 loss: -4.722733497619629
Epoch 388  Train loss: -0.7794843898100011  Val loss: -0.8476598025187594
Epoch 389
-------------------------------
Batch 1/64 loss: -0.8157634735107422
Batch 2/64 loss: -0.5904426574707031
Batch 3/64 loss: -0.9303789138793945
Batch 4/64 loss: -0.8676748275756836
Batch 5/64 loss: -0.653712272644043
Batch 6/64 loss: -1.0775012969970703
Batch 7/64 loss: -0.9499645233154297
Batch 8/64 loss: -0.7986164093017578
Batch 9/64 loss: -0.7142457962036133
Batch 10/64 loss: -0.05969715118408203
Batch 11/64 loss: -0.7170381546020508
Batch 12/64 loss: -0.5293292999267578
Batch 13/64 loss: -0.5953278541564941
Batch 14/64 loss: -0.39410400390625
Batch 15/64 loss: -0.7913761138916016
Batch 16/64 loss: -0.7264013290405273
Batch 17/64 loss: -0.4009227752685547
Batch 18/64 loss: -0.9663305282592773
Batch 19/64 loss: -0.8013172149658203
Batch 20/64 loss: -0.8063249588012695
Batch 21/64 loss: -0.8248567581176758
Batch 22/64 loss: -1.0082359313964844
Batch 23/64 loss: -0.8128938674926758
Batch 24/64 loss: -0.48980712890625
Batch 25/64 loss: -0.7435016632080078
Batch 26/64 loss: -0.7522420883178711
Batch 27/64 loss: -0.6705656051635742
Batch 28/64 loss: -0.7772579193115234
Batch 29/64 loss: -0.9248876571655273
Batch 30/64 loss: -0.46224498748779297
Batch 31/64 loss: -0.8244972229003906
Batch 32/64 loss: -0.9614505767822266
Batch 33/64 loss: -0.9359073638916016
Batch 34/64 loss: -0.5032577514648438
Batch 35/64 loss: -1.033792495727539
Batch 36/64 loss: -0.5871448516845703
Batch 37/64 loss: -0.9043283462524414
Batch 38/64 loss: -0.9981908798217773
Batch 39/64 loss: -0.7304468154907227
Batch 40/64 loss: -0.6941795349121094
Batch 41/64 loss: -1.0825023651123047
Batch 42/64 loss: -0.8528242111206055
Batch 43/64 loss: -0.6800613403320312
Batch 44/64 loss: -0.9005365371704102
Batch 45/64 loss: -0.705169677734375
Batch 46/64 loss: -0.7456684112548828
Batch 47/64 loss: -0.6527509689331055
Batch 48/64 loss: -1.025918960571289
Batch 49/64 loss: -0.7131271362304688
Batch 50/64 loss: -0.4891519546508789
Batch 51/64 loss: -0.8178186416625977
Batch 52/64 loss: -0.9033222198486328
Batch 53/64 loss: -0.8358478546142578
Batch 54/64 loss: -0.8637571334838867
Batch 55/64 loss: -0.6935663223266602
Batch 56/64 loss: -1.045802116394043
Batch 57/64 loss: -0.7713871002197266
Batch 58/64 loss: -0.7351369857788086
Batch 59/64 loss: -0.7178354263305664
Batch 60/64 loss: -0.8230047225952148
Batch 61/64 loss: -0.5269269943237305
Batch 62/64 loss: -1.0975446701049805
Batch 63/64 loss: -0.4578847885131836
Batch 64/64 loss: -4.815932273864746
Epoch 389  Train loss: -0.8090298783545401  Val loss: -0.8758607713627242
Epoch 390
-------------------------------
Batch 1/64 loss: -0.6137580871582031
Batch 2/64 loss: -0.6324539184570312
Batch 3/64 loss: -0.642791748046875
Batch 4/64 loss: -0.9315052032470703
Batch 5/64 loss: -0.823796272277832
Batch 6/64 loss: -0.5119342803955078
Batch 7/64 loss: -0.2923717498779297
Batch 8/64 loss: -0.8702735900878906
Batch 9/64 loss: -0.6789493560791016
Batch 10/64 loss: -0.7381772994995117
Batch 11/64 loss: -1.0393590927124023
Batch 12/64 loss: -1.0100727081298828
Batch 13/64 loss: -1.0319480895996094
Batch 14/64 loss: -0.6170520782470703
Batch 15/64 loss: -0.640233039855957
Batch 16/64 loss: -0.18380355834960938
Batch 17/64 loss: -0.7757806777954102
Batch 18/64 loss: -0.8674211502075195
Batch 19/64 loss: -0.9916057586669922
Batch 20/64 loss: -0.6496419906616211
Batch 21/64 loss: -0.5711574554443359
Batch 22/64 loss: -0.7655658721923828
Batch 23/64 loss: -0.9987411499023438
Batch 24/64 loss: -0.7773933410644531
Batch 25/64 loss: -0.48711204528808594
Batch 26/64 loss: -0.7528247833251953
Batch 27/64 loss: -0.7764825820922852
Batch 28/64 loss: -0.7160072326660156
Batch 29/64 loss: -1.0320987701416016
Batch 30/64 loss: -0.7111053466796875
Batch 31/64 loss: -0.9482030868530273
Batch 32/64 loss: -0.9134922027587891
Batch 33/64 loss: -0.9722890853881836
Batch 34/64 loss: -0.7922048568725586
Batch 35/64 loss: -0.9835166931152344
Batch 36/64 loss: -0.5785799026489258
Batch 37/64 loss: -0.804631233215332
Batch 38/64 loss: -0.8357830047607422
Batch 39/64 loss: -0.542780876159668
Batch 40/64 loss: -0.81878662109375
Batch 41/64 loss: -0.7226657867431641
Batch 42/64 loss: -0.7026844024658203
Batch 43/64 loss: -0.8297977447509766
Batch 44/64 loss: -0.6903905868530273
Batch 45/64 loss: -0.8210334777832031
Batch 46/64 loss: -0.6012544631958008
Batch 47/64 loss: -0.9478139877319336
Batch 48/64 loss: -0.9083318710327148
Batch 49/64 loss: -0.8653402328491211
Batch 50/64 loss: -0.8378715515136719
Batch 51/64 loss: -0.6175613403320312
Batch 52/64 loss: -0.4105234146118164
Batch 53/64 loss: -0.8259611129760742
Batch 54/64 loss: -0.5723161697387695
Batch 55/64 loss: -0.7960186004638672
Batch 56/64 loss: -0.8456935882568359
Batch 57/64 loss: -0.8465690612792969
Batch 58/64 loss: -0.7085151672363281
Batch 59/64 loss: -0.5274686813354492
Batch 60/64 loss: -0.8746986389160156
Batch 61/64 loss: -0.714238166809082
Batch 62/64 loss: -0.42572689056396484
Batch 63/64 loss: -0.7529134750366211
Batch 64/64 loss: -4.538167953491211
Epoch 390  Train loss: -0.7932658849977979  Val loss: -0.7633593582205757
Epoch 391
-------------------------------
Batch 1/64 loss: -1.1763458251953125
Batch 2/64 loss: -1.0150461196899414
Batch 3/64 loss: -0.7682790756225586
Batch 4/64 loss: -0.9058980941772461
Batch 5/64 loss: -0.29414844512939453
Batch 6/64 loss: -0.9374475479125977
Batch 7/64 loss: -0.6545076370239258
Batch 8/64 loss: -0.9422607421875
Batch 9/64 loss: -1.0799789428710938
Batch 10/64 loss: -0.4033784866333008
Batch 11/64 loss: -0.564997673034668
Batch 12/64 loss: -0.6369743347167969
Batch 13/64 loss: -0.8808927536010742
Batch 14/64 loss: -0.815190315246582
Batch 15/64 loss: -0.5778694152832031
Batch 16/64 loss: -0.8668031692504883
Batch 17/64 loss: -0.9941558837890625
Batch 18/64 loss: -0.7933139801025391
Batch 19/64 loss: -0.6549091339111328
Batch 20/64 loss: -0.5897302627563477
Batch 21/64 loss: -0.6312007904052734
Batch 22/64 loss: -0.653630256652832
Batch 23/64 loss: -0.2913684844970703
Batch 24/64 loss: -0.014316082000732422
Batch 25/64 loss: -0.8209762573242188
Batch 26/64 loss: -0.719182014465332
Batch 27/64 loss: -0.875910758972168
Batch 28/64 loss: -0.7210960388183594
Batch 29/64 loss: -0.7305688858032227
Batch 30/64 loss: -0.7844724655151367
Batch 31/64 loss: -0.43382835388183594
Batch 32/64 loss: -0.6110401153564453
Batch 33/64 loss: -0.7769069671630859
Batch 34/64 loss: -0.4958457946777344
Batch 35/64 loss: -0.9253206253051758
Batch 36/64 loss: -0.8826103210449219
Batch 37/64 loss: -0.9388751983642578
Batch 38/64 loss: -0.7887716293334961
Batch 39/64 loss: -0.5692138671875
Batch 40/64 loss: -1.141230583190918
Batch 41/64 loss: -0.885498046875
Batch 42/64 loss: -0.7918252944946289
Batch 43/64 loss: -0.9961395263671875
Batch 44/64 loss: -0.8029890060424805
Batch 45/64 loss: -0.7086191177368164
Batch 46/64 loss: -1.039285659790039
Batch 47/64 loss: -0.9469718933105469
Batch 48/64 loss: -0.6484775543212891
Batch 49/64 loss: -0.4916391372680664
Batch 50/64 loss: -0.6962614059448242
Batch 51/64 loss: -0.530055046081543
Batch 52/64 loss: -0.6920766830444336
Batch 53/64 loss: -0.5697083473205566
Batch 54/64 loss: -1.07208251953125
Batch 55/64 loss: -0.8848352432250977
Batch 56/64 loss: -0.7655506134033203
Batch 57/64 loss: -1.045475959777832
Batch 58/64 loss: -0.5838193893432617
Batch 59/64 loss: -0.4725055694580078
Batch 60/64 loss: -0.9088735580444336
Batch 61/64 loss: -0.6442708969116211
Batch 62/64 loss: -0.7790079116821289
Batch 63/64 loss: -0.6828737258911133
Batch 64/64 loss: -4.734007835388184
Epoch 391  Train loss: -0.792907314674527  Val loss: -0.9097454031718146
Epoch 392
-------------------------------
Batch 1/64 loss: -0.9818611145019531
Batch 2/64 loss: -0.8432369232177734
Batch 3/64 loss: -0.8550024032592773
Batch 4/64 loss: -0.3654670715332031
Batch 5/64 loss: -0.7113590240478516
Batch 6/64 loss: -1.120584487915039
Batch 7/64 loss: -0.9761981964111328
Batch 8/64 loss: -0.7292146682739258
Batch 9/64 loss: -0.8231954574584961
Batch 10/64 loss: -1.007584571838379
Batch 11/64 loss: -0.8983678817749023
Batch 12/64 loss: -0.13193941116333008
Batch 13/64 loss: -0.8114175796508789
Batch 14/64 loss: -1.0224103927612305
Batch 15/64 loss: -0.8948450088500977
Batch 16/64 loss: -0.5186710357666016
Batch 17/64 loss: -0.72369384765625
Batch 18/64 loss: -1.0441579818725586
Batch 19/64 loss: -0.19595718383789062
Batch 20/64 loss: -0.8478422164916992
Batch 21/64 loss: -0.6640720367431641
Batch 22/64 loss: -0.707188606262207
Batch 23/64 loss: -1.1946468353271484
Batch 24/64 loss: -0.7157773971557617
Batch 25/64 loss: -0.5891666412353516
Batch 26/64 loss: -0.3670825958251953
Batch 27/64 loss: -0.8307466506958008
Batch 28/64 loss: -0.26708316802978516
Batch 29/64 loss: -0.4752168655395508
Batch 30/64 loss: -0.5587186813354492
Batch 31/64 loss: -0.8650884628295898
Batch 32/64 loss: -0.8781032562255859
Batch 33/64 loss: -0.670501708984375
Batch 34/64 loss: -0.6387405395507812
Batch 35/64 loss: -1.053502082824707
Batch 36/64 loss: -1.0737619400024414
Batch 37/64 loss: -0.8566646575927734
Batch 38/64 loss: -1.036818504333496
Batch 39/64 loss: -0.9395437240600586
Batch 40/64 loss: -0.9444980621337891
Batch 41/64 loss: -0.0001811981201171875
Batch 42/64 loss: -0.6564178466796875
Batch 43/64 loss: -0.8934469223022461
Batch 44/64 loss: -0.8579893112182617
Batch 45/64 loss: -1.0253276824951172
Batch 46/64 loss: -0.8981781005859375
Batch 47/64 loss: -1.0243186950683594
Batch 48/64 loss: -1.0281553268432617
Batch 49/64 loss: -0.9552650451660156
Batch 50/64 loss: -0.7246417999267578
Batch 51/64 loss: -0.485809326171875
Batch 52/64 loss: -0.9896268844604492
Batch 53/64 loss: -0.9371376037597656
Batch 54/64 loss: -0.7431325912475586
Batch 55/64 loss: -0.847874641418457
Batch 56/64 loss: -0.8528566360473633
Batch 57/64 loss: -0.6200122833251953
Batch 58/64 loss: -0.43167591094970703
Batch 59/64 loss: -0.8697118759155273
Batch 60/64 loss: -0.8979406356811523
Batch 61/64 loss: -0.7640666961669922
Batch 62/64 loss: -0.5345630645751953
Batch 63/64 loss: -0.6487522125244141
Batch 64/64 loss: -5.0022501945495605
Epoch 392  Train loss: -0.8199011578279383  Val loss: -0.9040605866212615
Epoch 393
-------------------------------
Batch 1/64 loss: -0.59912109375
Batch 2/64 loss: -1.076517105102539
Batch 3/64 loss: -0.8379764556884766
Batch 4/64 loss: -0.9794807434082031
Batch 5/64 loss: -0.7201576232910156
Batch 6/64 loss: -1.1628789901733398
Batch 7/64 loss: -0.8508443832397461
Batch 8/64 loss: -0.937047004699707
Batch 9/64 loss: -1.0024595260620117
Batch 10/64 loss: -0.921483039855957
Batch 11/64 loss: -0.3719758987426758
Batch 12/64 loss: -0.7217874526977539
Batch 13/64 loss: -0.9745817184448242
Batch 14/64 loss: -0.856114387512207
Batch 15/64 loss: -0.9505653381347656
Batch 16/64 loss: -0.8069009780883789
Batch 17/64 loss: -0.41875743865966797
Batch 18/64 loss: -0.6592903137207031
Batch 19/64 loss: -0.7823314666748047
Batch 20/64 loss: -0.7824058532714844
Batch 21/64 loss: -0.7895965576171875
Batch 22/64 loss: -0.8631038665771484
Batch 23/64 loss: -0.5742521286010742
Batch 24/64 loss: -0.7338104248046875
Batch 25/64 loss: -0.7917461395263672
Batch 26/64 loss: -0.8672285079956055
Batch 27/64 loss: -1.0571603775024414
Batch 28/64 loss: -0.8588981628417969
Batch 29/64 loss: -0.30947113037109375
Batch 30/64 loss: -0.5907964706420898
Batch 31/64 loss: -0.8004245758056641
Batch 32/64 loss: -0.9343814849853516
Batch 33/64 loss: -0.8758668899536133
Batch 34/64 loss: -0.96502685546875
Batch 35/64 loss: -0.5607900619506836
Batch 36/64 loss: -0.49712657928466797
Batch 37/64 loss: -1.1155614852905273
Batch 38/64 loss: -0.8344707489013672
Batch 39/64 loss: -0.3888998031616211
Batch 40/64 loss: -0.9753322601318359
Batch 41/64 loss: -0.595855712890625
Batch 42/64 loss: -0.9058637619018555
Batch 43/64 loss: -0.22279071807861328
Batch 44/64 loss: -0.8540925979614258
Batch 45/64 loss: -0.5838708877563477
Batch 46/64 loss: -0.42945384979248047
Batch 47/64 loss: -0.7752571105957031
Batch 48/64 loss: -0.5366830825805664
Batch 49/64 loss: -0.5800085067749023
Batch 50/64 loss: -0.6961536407470703
Batch 51/64 loss: -0.7937793731689453
Batch 52/64 loss: -0.5124578475952148
Batch 53/64 loss: -1.0879344940185547
Batch 54/64 loss: -0.8439674377441406
Batch 55/64 loss: -0.6575508117675781
Batch 56/64 loss: -0.9347782135009766
Batch 57/64 loss: -0.4906644821166992
Batch 58/64 loss: -0.4045724868774414
Batch 59/64 loss: -0.7723979949951172
Batch 60/64 loss: -0.4347190856933594
Batch 61/64 loss: -0.47476673126220703
Batch 62/64 loss: -0.927393913269043
Batch 63/64 loss: -0.8539304733276367
Batch 64/64 loss: -3.759824752807617
Epoch 393  Train loss: -0.7840852251239852  Val loss: -0.8024394635072688
Epoch 394
-------------------------------
Batch 1/64 loss: -0.6542825698852539
Batch 2/64 loss: -0.954777717590332
Batch 3/64 loss: -0.7830696105957031
Batch 4/64 loss: -0.8219203948974609
Batch 5/64 loss: -0.560612678527832
Batch 6/64 loss: -0.627598762512207
Batch 7/64 loss: -0.959040641784668
Batch 8/64 loss: -0.8884983062744141
Batch 9/64 loss: -0.8718385696411133
Batch 10/64 loss: -0.7401113510131836
Batch 11/64 loss: -0.6626768112182617
Batch 12/64 loss: -0.7050819396972656
Batch 13/64 loss: -0.9728994369506836
Batch 14/64 loss: -0.7079219818115234
Batch 15/64 loss: -0.8253297805786133
Batch 16/64 loss: -0.7706432342529297
Batch 17/64 loss: -0.5031309127807617
Batch 18/64 loss: -0.8218832015991211
Batch 19/64 loss: -0.8426322937011719
Batch 20/64 loss: -0.6822433471679688
Batch 21/64 loss: -0.8272418975830078
Batch 22/64 loss: -1.0164966583251953
Batch 23/64 loss: -0.7675819396972656
Batch 24/64 loss: -0.6931009292602539
Batch 25/64 loss: -0.42450523376464844
Batch 26/64 loss: -0.9666042327880859
Batch 27/64 loss: -0.594059944152832
Batch 28/64 loss: -0.6814231872558594
Batch 29/64 loss: -0.6874675750732422
Batch 30/64 loss: -0.6522083282470703
Batch 31/64 loss: -0.8661165237426758
Batch 32/64 loss: -0.7827005386352539
Batch 33/64 loss: -0.9145574569702148
Batch 34/64 loss: -0.7181968688964844
Batch 35/64 loss: -0.6431684494018555
Batch 36/64 loss: -0.7026920318603516
Batch 37/64 loss: -0.6500072479248047
Batch 38/64 loss: -0.8816604614257812
Batch 39/64 loss: -0.8198003768920898
Batch 40/64 loss: -0.4713325500488281
Batch 41/64 loss: -0.6246728897094727
Batch 42/64 loss: -0.44004344940185547
Batch 43/64 loss: -0.8584651947021484
Batch 44/64 loss: -0.7384757995605469
Batch 45/64 loss: -0.6810302734375
Batch 46/64 loss: -0.8371715545654297
Batch 47/64 loss: -0.45488834381103516
Batch 48/64 loss: -0.522465705871582
Batch 49/64 loss: -0.2684450149536133
Batch 50/64 loss: -0.5570993423461914
Batch 51/64 loss: -0.06182289123535156
Batch 52/64 loss: -0.7763519287109375
Batch 53/64 loss: -1.0306568145751953
Batch 54/64 loss: -0.9184541702270508
Batch 55/64 loss: -0.1876049041748047
Batch 56/64 loss: -0.47514820098876953
Batch 57/64 loss: 0.3421144485473633
Batch 58/64 loss: -0.40437793731689453
Batch 59/64 loss: -0.4369468688964844
Batch 60/64 loss: -0.7663583755493164
Batch 61/64 loss: -0.38867664337158203
Batch 62/64 loss: -0.7458791732788086
Batch 63/64 loss: -0.8573846817016602
Batch 64/64 loss: -4.793406009674072
Epoch 394  Train loss: -0.7278819794748344  Val loss: -0.7174477331417123
Epoch 395
-------------------------------
Batch 1/64 loss: -0.6555900573730469
Batch 2/64 loss: -0.9361515045166016
Batch 3/64 loss: -0.42967891693115234
Batch 4/64 loss: -0.8971185684204102
Batch 5/64 loss: -0.5102434158325195
Batch 6/64 loss: -0.8331212997436523
Batch 7/64 loss: -0.5966014862060547
Batch 8/64 loss: -0.1629176139831543
Batch 9/64 loss: -0.8534116744995117
Batch 10/64 loss: -0.5466165542602539
Batch 11/64 loss: -0.7252206802368164
Batch 12/64 loss: -0.8421182632446289
Batch 13/64 loss: -0.7869930267333984
Batch 14/64 loss: -0.1672210693359375
Batch 15/64 loss: -0.8554153442382812
Batch 16/64 loss: -1.048701286315918
Batch 17/64 loss: -1.1064023971557617
Batch 18/64 loss: -1.2369518280029297
Batch 19/64 loss: -0.49776268005371094
Batch 20/64 loss: -0.9298028945922852
Batch 21/64 loss: -0.7567310333251953
Batch 22/64 loss: -0.8214559555053711
Batch 23/64 loss: -1.2086849212646484
Batch 24/64 loss: -0.7016658782958984
Batch 25/64 loss: -0.7379693984985352
Batch 26/64 loss: -0.19070720672607422
Batch 27/64 loss: -0.6186141967773438
Batch 28/64 loss: -0.9812765121459961
Batch 29/64 loss: -0.3803415298461914
Batch 30/64 loss: -0.5813188552856445
Batch 31/64 loss: -0.7717199325561523
Batch 32/64 loss: -0.6934289932250977
Batch 33/64 loss: -0.8704690933227539
Batch 34/64 loss: -0.6877117156982422
Batch 35/64 loss: -0.6432638168334961
Batch 36/64 loss: -0.9400749206542969
Batch 37/64 loss: -1.0392951965332031
Batch 38/64 loss: -0.6377696990966797
Batch 39/64 loss: -0.9689416885375977
Batch 40/64 loss: -1.056777000427246
Batch 41/64 loss: -0.7494840621948242
Batch 42/64 loss: -0.6021766662597656
Batch 43/64 loss: -0.9228610992431641
Batch 44/64 loss: -0.7369098663330078
Batch 45/64 loss: -0.23245811462402344
Batch 46/64 loss: -0.8945446014404297
Batch 47/64 loss: -0.9000539779663086
Batch 48/64 loss: -0.9725551605224609
Batch 49/64 loss: -0.6729516983032227
Batch 50/64 loss: -0.7638778686523438
Batch 51/64 loss: -0.8437519073486328
Batch 52/64 loss: -0.3075904846191406
Batch 53/64 loss: -0.7953701019287109
Batch 54/64 loss: -0.8217439651489258
Batch 55/64 loss: -0.7860260009765625
Batch 56/64 loss: -0.6746406555175781
Batch 57/64 loss: -0.7234916687011719
Batch 58/64 loss: -0.8475112915039062
Batch 59/64 loss: -0.6565170288085938
Batch 60/64 loss: -0.49665164947509766
Batch 61/64 loss: -0.37299060821533203
Batch 62/64 loss: -0.8248205184936523
Batch 63/64 loss: -0.9463281631469727
Batch 64/64 loss: -4.93934440612793
Epoch 395  Train loss: -0.7867619383568857  Val loss: -0.8607488153726375
Epoch 396
-------------------------------
Batch 1/64 loss: -0.8259162902832031
Batch 2/64 loss: -0.761836051940918
Batch 3/64 loss: -0.5588579177856445
Batch 4/64 loss: -0.9620914459228516
Batch 5/64 loss: -0.5025339126586914
Batch 6/64 loss: -0.6817550659179688
Batch 7/64 loss: -0.9499177932739258
Batch 8/64 loss: -0.977299690246582
Batch 9/64 loss: -0.6421489715576172
Batch 10/64 loss: -0.7710533142089844
Batch 11/64 loss: -0.7739677429199219
Batch 12/64 loss: -0.73834228515625
Batch 13/64 loss: -0.9206781387329102
Batch 14/64 loss: -0.1932992935180664
Batch 15/64 loss: -0.6110744476318359
Batch 16/64 loss: -0.7906484603881836
Batch 17/64 loss: -0.6293439865112305
Batch 18/64 loss: -0.9529714584350586
Batch 19/64 loss: -0.6968250274658203
Batch 20/64 loss: -0.775700569152832
Batch 21/64 loss: -0.6760835647583008
Batch 22/64 loss: -0.8290386199951172
Batch 23/64 loss: -0.25122547149658203
Batch 24/64 loss: -1.115229606628418
Batch 25/64 loss: -0.7690744400024414
Batch 26/64 loss: -0.6561479568481445
Batch 27/64 loss: -0.3677787780761719
Batch 28/64 loss: -0.5903415679931641
Batch 29/64 loss: -0.5207157135009766
Batch 30/64 loss: -0.3503293991088867
Batch 31/64 loss: -0.5012578964233398
Batch 32/64 loss: -0.8717365264892578
Batch 33/64 loss: -1.1140308380126953
Batch 34/64 loss: -0.9814367294311523
Batch 35/64 loss: -0.5569324493408203
Batch 36/64 loss: -0.913578987121582
Batch 37/64 loss: -0.9277439117431641
Batch 38/64 loss: -0.4567375183105469
Batch 39/64 loss: -0.4497818946838379
Batch 40/64 loss: -0.6445522308349609
Batch 41/64 loss: -0.6272687911987305
Batch 42/64 loss: -1.0018348693847656
Batch 43/64 loss: -0.8757877349853516
Batch 44/64 loss: -1.1128778457641602
Batch 45/64 loss: -0.8572978973388672
Batch 46/64 loss: -0.7927846908569336
Batch 47/64 loss: -0.319612979888916
Batch 48/64 loss: -0.8383493423461914
Batch 49/64 loss: -0.7401008605957031
Batch 50/64 loss: -0.8368711471557617
Batch 51/64 loss: -1.1227455139160156
Batch 52/64 loss: -0.889185905456543
Batch 53/64 loss: -0.8139209747314453
Batch 54/64 loss: -0.7503843307495117
Batch 55/64 loss: -0.483245849609375
Batch 56/64 loss: -0.884246826171875
Batch 57/64 loss: -1.040360450744629
Batch 58/64 loss: -0.6328067779541016
Batch 59/64 loss: -0.6430282592773438
Batch 60/64 loss: -0.5640478134155273
Batch 61/64 loss: -0.8438844680786133
Batch 62/64 loss: -0.4603862762451172
Batch 63/64 loss: -1.0191726684570312
Batch 64/64 loss: -4.5499396324157715
Epoch 396  Train loss: -0.7815321249120376  Val loss: -0.8237745147390464
Epoch 397
-------------------------------
Batch 1/64 loss: -0.8545818328857422
Batch 2/64 loss: -0.9465560913085938
Batch 3/64 loss: -0.39336681365966797
Batch 4/64 loss: -0.29081249237060547
Batch 5/64 loss: -0.8908510208129883
Batch 6/64 loss: -0.8731117248535156
Batch 7/64 loss: -0.5600738525390625
Batch 8/64 loss: -0.5328893661499023
Batch 9/64 loss: -0.9373302459716797
Batch 10/64 loss: -0.3926048278808594
Batch 11/64 loss: -0.36863136291503906
Batch 12/64 loss: -0.7806100845336914
Batch 13/64 loss: -0.6644001007080078
Batch 14/64 loss: -0.7937812805175781
Batch 15/64 loss: -0.7145729064941406
Batch 16/64 loss: -0.7220821380615234
Batch 17/64 loss: -0.8776483535766602
Batch 18/64 loss: -0.7542715072631836
Batch 19/64 loss: -0.9739875793457031
Batch 20/64 loss: -0.8178987503051758
Batch 21/64 loss: -0.7270908355712891
Batch 22/64 loss: -0.8054409027099609
Batch 23/64 loss: -0.87640380859375
Batch 24/64 loss: -0.7669315338134766
Batch 25/64 loss: -0.7480010986328125
Batch 26/64 loss: -0.13272380828857422
Batch 27/64 loss: -0.9895992279052734
Batch 28/64 loss: -0.8799524307250977
Batch 29/64 loss: -0.2996091842651367
Batch 30/64 loss: -0.6648244857788086
Batch 31/64 loss: -0.27083778381347656
Batch 32/64 loss: -1.0365571975708008
Batch 33/64 loss: -0.7964639663696289
Batch 34/64 loss: -1.0468759536743164
Batch 35/64 loss: -0.7559986114501953
Batch 36/64 loss: -0.673884391784668
Batch 37/64 loss: -0.7704982757568359
Batch 38/64 loss: -0.6042928695678711
Batch 39/64 loss: -0.7027082443237305
Batch 40/64 loss: -0.8607950210571289
Batch 41/64 loss: -1.0265731811523438
Batch 42/64 loss: -1.0644989013671875
Batch 43/64 loss: -0.689605712890625
Batch 44/64 loss: -0.40885257720947266
Batch 45/64 loss: -0.44174766540527344
Batch 46/64 loss: -0.7277164459228516
Batch 47/64 loss: -0.5396509170532227
Batch 48/64 loss: -0.6984720230102539
Batch 49/64 loss: -0.8376922607421875
Batch 50/64 loss: -0.792353630065918
Batch 51/64 loss: -0.9186677932739258
Batch 52/64 loss: -0.7178335189819336
Batch 53/64 loss: -1.000518798828125
Batch 54/64 loss: -0.9821529388427734
Batch 55/64 loss: -0.854949951171875
Batch 56/64 loss: -0.8428287506103516
Batch 57/64 loss: -0.966242790222168
Batch 58/64 loss: -0.7005186080932617
Batch 59/64 loss: -0.8495998382568359
Batch 60/64 loss: -0.9469461441040039
Batch 61/64 loss: -1.0256338119506836
Batch 62/64 loss: -0.5526657104492188
Batch 63/64 loss: -0.7697229385375977
Batch 64/64 loss: -4.525030612945557
Epoch 397  Train loss: -0.7890003110848222  Val loss: -0.8421643837211058
Epoch 398
-------------------------------
Batch 1/64 loss: -0.7720108032226562
Batch 2/64 loss: -0.8834629058837891
Batch 3/64 loss: -0.7362146377563477
Batch 4/64 loss: -0.9900979995727539
Batch 5/64 loss: -0.7482757568359375
Batch 6/64 loss: -0.3854103088378906
Batch 7/64 loss: -0.8935832977294922
Batch 8/64 loss: -0.2498464584350586
Batch 9/64 loss: -0.8378973007202148
Batch 10/64 loss: -0.8927755355834961
Batch 11/64 loss: -0.7450923919677734
Batch 12/64 loss: -0.610722541809082
Batch 13/64 loss: -0.7203283309936523
Batch 14/64 loss: -0.773777961730957
Batch 15/64 loss: -0.8652276992797852
Batch 16/64 loss: -0.6644926071166992
Batch 17/64 loss: -0.6864986419677734
Batch 18/64 loss: -0.7560610771179199
Batch 19/64 loss: -0.8035364151000977
Batch 20/64 loss: -0.7011470794677734
Batch 21/64 loss: -1.0349435806274414
Batch 22/64 loss: -0.7201013565063477
Batch 23/64 loss: -0.2281661033630371
Batch 24/64 loss: -1.0602989196777344
Batch 25/64 loss: -0.6266665458679199
Batch 26/64 loss: -0.6830434799194336
Batch 27/64 loss: -0.7439289093017578
Batch 28/64 loss: -0.678187370300293
Batch 29/64 loss: -0.6389369964599609
Batch 30/64 loss: -0.9472684860229492
Batch 31/64 loss: -0.7815275192260742
Batch 32/64 loss: -0.6938819885253906
Batch 33/64 loss: -0.8141756057739258
Batch 34/64 loss: -0.9610080718994141
Batch 35/64 loss: -0.5366401672363281
Batch 36/64 loss: -0.5244169235229492
Batch 37/64 loss: -0.9701642990112305
Batch 38/64 loss: -1.2821359634399414
Batch 39/64 loss: -0.13253498077392578
Batch 40/64 loss: -0.3499574661254883
Batch 41/64 loss: -0.8516416549682617
Batch 42/64 loss: -0.9004249572753906
Batch 43/64 loss: -0.7697649002075195
Batch 44/64 loss: -0.8850631713867188
Batch 45/64 loss: -0.7622547149658203
Batch 46/64 loss: -1.0189266204833984
Batch 47/64 loss: -0.773219108581543
Batch 48/64 loss: -0.27566003799438477
Batch 49/64 loss: -0.6197128295898438
Batch 50/64 loss: -0.557520866394043
Batch 51/64 loss: -1.055302619934082
Batch 52/64 loss: -0.7840175628662109
Batch 53/64 loss: -0.9043073654174805
Batch 54/64 loss: -0.34967803955078125
Batch 55/64 loss: -1.1923274993896484
Batch 56/64 loss: -1.0986566543579102
Batch 57/64 loss: -0.6254215240478516
Batch 58/64 loss: -0.814356803894043
Batch 59/64 loss: -0.9212808609008789
Batch 60/64 loss: -0.6377401351928711
Batch 61/64 loss: -0.8833169937133789
Batch 62/64 loss: -1.1137828826904297
Batch 63/64 loss: -0.8037881851196289
Batch 64/64 loss: -4.856257915496826
Epoch 398  Train loss: -0.8057224142785165  Val loss: -0.9166608987395296
Epoch 399
-------------------------------
Batch 1/64 loss: -0.6880922317504883
Batch 2/64 loss: -0.9312725067138672
Batch 3/64 loss: -0.9183597564697266
Batch 4/64 loss: -0.7228031158447266
Batch 5/64 loss: -0.5357704162597656
Batch 6/64 loss: -0.9204673767089844
Batch 7/64 loss: -1.0036334991455078
Batch 8/64 loss: -1.0588788986206055
Batch 9/64 loss: -0.43395423889160156
Batch 10/64 loss: -0.5387916564941406
Batch 11/64 loss: -0.9949493408203125
Batch 12/64 loss: -0.9093141555786133
Batch 13/64 loss: -0.902104377746582
Batch 14/64 loss: -0.5731983184814453
Batch 15/64 loss: -0.6780176162719727
Batch 16/64 loss: -0.8292236328125
Batch 17/64 loss: -0.8628330230712891
Batch 18/64 loss: -0.553950309753418
Batch 19/64 loss: -0.7262001037597656
Batch 20/64 loss: -0.757411003112793
Batch 21/64 loss: -0.9836254119873047
Batch 22/64 loss: -0.9468097686767578
Batch 23/64 loss: -1.0639028549194336
Batch 24/64 loss: -1.0879230499267578
Batch 25/64 loss: -0.7603693008422852
Batch 26/64 loss: -0.8186721801757812
Batch 27/64 loss: -0.4589204788208008
Batch 28/64 loss: -0.6891937255859375
Batch 29/64 loss: -0.7464466094970703
Batch 30/64 loss: -0.9669866561889648
Batch 31/64 loss: -0.3014507293701172
Batch 32/64 loss: -0.8290472030639648
Batch 33/64 loss: -0.5699243545532227
Batch 34/64 loss: -0.7439022064208984
Batch 35/64 loss: -0.8233222961425781
Batch 36/64 loss: -0.36939239501953125
Batch 37/64 loss: -0.19333600997924805
Batch 38/64 loss: -0.8022794723510742
Batch 39/64 loss: -0.47326231002807617
Batch 40/64 loss: -0.8086681365966797
Batch 41/64 loss: -0.46187305450439453
Batch 42/64 loss: -0.5426139831542969
Batch 43/64 loss: -0.7554330825805664
Batch 44/64 loss: -0.8921747207641602
Batch 45/64 loss: -0.4935922622680664
Batch 46/64 loss: -0.8338212966918945
Batch 47/64 loss: -0.6394176483154297
Batch 48/64 loss: -0.7280683517456055
Batch 49/64 loss: -0.4717540740966797
Batch 50/64 loss: 0.20554447174072266
Batch 51/64 loss: -0.5900964736938477
Batch 52/64 loss: -0.8344125747680664
Batch 53/64 loss: -0.977635383605957
Batch 54/64 loss: -1.2446441650390625
Batch 55/64 loss: -0.8942337036132812
Batch 56/64 loss: -0.9489049911499023
Batch 57/64 loss: -0.501739501953125
Batch 58/64 loss: -0.8250627517700195
Batch 59/64 loss: -0.6269769668579102
Batch 60/64 loss: -0.49336814880371094
Batch 61/64 loss: -0.815739631652832
Batch 62/64 loss: -0.9571151733398438
Batch 63/64 loss: -0.7526302337646484
Batch 64/64 loss: -4.731599807739258
Epoch 399  Train loss: -0.7780568515553193  Val loss: -0.78904776556795
Epoch 400
-------------------------------
Batch 1/64 loss: -0.8241939544677734
Batch 2/64 loss: -0.9710674285888672
Batch 3/64 loss: -0.9437141418457031
Batch 4/64 loss: -0.7879457473754883
Batch 5/64 loss: -0.425537109375
Batch 6/64 loss: -1.1198959350585938
Batch 7/64 loss: -0.15476131439208984
Batch 8/64 loss: -0.7658624649047852
Batch 9/64 loss: -0.38201045989990234
Batch 10/64 loss: -0.7975616455078125
Batch 11/64 loss: -0.2572307586669922
Batch 12/64 loss: -0.6150789260864258
Batch 13/64 loss: -0.5955886840820312
Batch 14/64 loss: -0.7135438919067383
Batch 15/64 loss: -0.4755668640136719
Batch 16/64 loss: -0.525477409362793
Batch 17/64 loss: -0.6878147125244141
Batch 18/64 loss: -0.5412898063659668
Batch 19/64 loss: -0.6930274963378906
Batch 20/64 loss: -0.9431333541870117
Batch 21/64 loss: -0.7245054244995117
Batch 22/64 loss: -0.7304592132568359
Batch 23/64 loss: -0.7871456146240234
Batch 24/64 loss: -0.8822202682495117
Batch 25/64 loss: -1.0041847229003906
Batch 26/64 loss: -0.8390293121337891
Batch 27/64 loss: -0.6248035430908203
Batch 28/64 loss: -0.8383607864379883
Batch 29/64 loss: -0.9607992172241211
Batch 30/64 loss: -0.9870929718017578
Batch 31/64 loss: -0.7871103286743164
Batch 32/64 loss: -0.8195390701293945
Batch 33/64 loss: -0.8580875396728516
Batch 34/64 loss: -0.5288915634155273
Batch 35/64 loss: -0.9790210723876953
Batch 36/64 loss: -0.5804634094238281
Batch 37/64 loss: -0.4415597915649414
Batch 38/64 loss: -0.8643341064453125
Batch 39/64 loss: -1.0855093002319336
Batch 40/64 loss: -0.9273509979248047
Batch 41/64 loss: -1.022465705871582
Batch 42/64 loss: -0.7920932769775391
Batch 43/64 loss: -0.7854881286621094
Batch 44/64 loss: -0.8743228912353516
Batch 45/64 loss: -0.8475818634033203
Batch 46/64 loss: -0.7107267379760742
Batch 47/64 loss: -0.9916467666625977
Batch 48/64 loss: -0.7683658599853516
Batch 49/64 loss: -1.0310325622558594
Batch 50/64 loss: -0.20661544799804688
Batch 51/64 loss: -1.1062068939208984
Batch 52/64 loss: -1.0462398529052734
Batch 53/64 loss: -1.0172176361083984
Batch 54/64 loss: -0.7883567810058594
Batch 55/64 loss: -0.7180519104003906
Batch 56/64 loss: -0.47830772399902344
Batch 57/64 loss: -0.8363142013549805
Batch 58/64 loss: -0.7687325477600098
Batch 59/64 loss: -0.870427131652832
Batch 60/64 loss: -0.8423871994018555
Batch 61/64 loss: -0.8999576568603516
Batch 62/64 loss: -0.9336309432983398
Batch 63/64 loss: -0.926875114440918
Batch 64/64 loss: -4.72685432434082
Epoch 400  Train loss: -0.8200620539048139  Val loss: -0.8649451003451527
Epoch 401
-------------------------------
Batch 1/64 loss: -0.0037088394165039062
Batch 2/64 loss: -0.8835563659667969
Batch 3/64 loss: -0.8438920974731445
Batch 4/64 loss: -0.5225954055786133
Batch 5/64 loss: -0.8629226684570312
Batch 6/64 loss: -0.8190441131591797
Batch 7/64 loss: -0.6110601425170898
Batch 8/64 loss: -0.7572011947631836
Batch 9/64 loss: -1.0186738967895508
Batch 10/64 loss: -0.9505271911621094
Batch 11/64 loss: -1.1623468399047852
Batch 12/64 loss: -0.8199520111083984
Batch 13/64 loss: -0.95037841796875
Batch 14/64 loss: -0.9862689971923828
Batch 15/64 loss: -0.3446063995361328
Batch 16/64 loss: -1.1629209518432617
Batch 17/64 loss: -0.805877685546875
Batch 18/64 loss: -0.87176513671875
Batch 19/64 loss: -0.6410808563232422
Batch 20/64 loss: -0.5032062530517578
Batch 21/64 loss: -0.5887527465820312
Batch 22/64 loss: -1.0664997100830078
Batch 23/64 loss: -0.901036262512207
Batch 24/64 loss: -0.7280197143554688
Batch 25/64 loss: -0.9381113052368164
Batch 26/64 loss: -0.8040456771850586
Batch 27/64 loss: -0.9110002517700195
Batch 28/64 loss: -0.7905740737915039
Batch 29/64 loss: -0.7218484878540039
Batch 30/64 loss: -1.031315803527832
Batch 31/64 loss: -0.6664943695068359
Batch 32/64 loss: -0.7614278793334961
Batch 33/64 loss: -0.9610481262207031
Batch 34/64 loss: -0.8625650405883789
Batch 35/64 loss: -0.9364500045776367
Batch 36/64 loss: -0.8571834564208984
Batch 37/64 loss: -1.0102567672729492
Batch 38/64 loss: -0.8442411422729492
Batch 39/64 loss: -0.6552791595458984
Batch 40/64 loss: -0.9389333724975586
Batch 41/64 loss: -0.9587335586547852
Batch 42/64 loss: -0.6495256423950195
Batch 43/64 loss: -0.8509292602539062
Batch 44/64 loss: -1.0758419036865234
Batch 45/64 loss: -1.1172876358032227
Batch 46/64 loss: -1.1190986633300781
Batch 47/64 loss: -0.842167854309082
Batch 48/64 loss: -1.003098487854004
Batch 49/64 loss: -0.7229776382446289
Batch 50/64 loss: -0.8585472106933594
Batch 51/64 loss: -0.8507747650146484
Batch 52/64 loss: -1.0275564193725586
Batch 53/64 loss: -0.8778209686279297
Batch 54/64 loss: -0.8261947631835938
Batch 55/64 loss: -0.676483154296875
Batch 56/64 loss: -0.5560951232910156
Batch 57/64 loss: -0.6503162384033203
Batch 58/64 loss: -0.6954708099365234
Batch 59/64 loss: -0.6748552322387695
Batch 60/64 loss: -0.721226692199707
Batch 61/64 loss: -0.799189567565918
Batch 62/64 loss: -0.9040355682373047
Batch 63/64 loss: -0.5801553726196289
Batch 64/64 loss: -4.430493354797363
Epoch 401  Train loss: -0.8605164135203642  Val loss: -0.6589450115190748
Epoch 402
-------------------------------
Batch 1/64 loss: -0.9863462448120117
Batch 2/64 loss: -0.9841451644897461
Batch 3/64 loss: -0.8028659820556641
Batch 4/64 loss: -0.5345964431762695
Batch 5/64 loss: -0.6150226593017578
Batch 6/64 loss: -0.5603694915771484
Batch 7/64 loss: -0.4806203842163086
Batch 8/64 loss: -0.861790657043457
Batch 9/64 loss: -0.8613557815551758
Batch 10/64 loss: -0.6600160598754883
Batch 11/64 loss: -0.9959840774536133
Batch 12/64 loss: -0.9673023223876953
Batch 13/64 loss: -0.8976459503173828
Batch 14/64 loss: -0.7477235794067383
Batch 15/64 loss: -0.17285919189453125
Batch 16/64 loss: -0.915705680847168
Batch 17/64 loss: -0.574702262878418
Batch 18/64 loss: -0.8717241287231445
Batch 19/64 loss: -0.8668613433837891
Batch 20/64 loss: -0.6666736602783203
Batch 21/64 loss: -0.50982666015625
Batch 22/64 loss: -0.7677516937255859
Batch 23/64 loss: -0.7333879470825195
Batch 24/64 loss: -1.0962390899658203
Batch 25/64 loss: -1.0251359939575195
Batch 26/64 loss: -0.4400520324707031
Batch 27/64 loss: -0.49154090881347656
Batch 28/64 loss: -0.4036903381347656
Batch 29/64 loss: -0.9624700546264648
Batch 30/64 loss: -0.826807975769043
Batch 31/64 loss: -0.804621696472168
Batch 32/64 loss: -0.9997968673706055
Batch 33/64 loss: -0.08048248291015625
Batch 34/64 loss: -0.9752054214477539
Batch 35/64 loss: -0.5556774139404297
Batch 36/64 loss: -0.5293188095092773
Batch 37/64 loss: -1.0416688919067383
Batch 38/64 loss: -1.132338523864746
Batch 39/64 loss: -0.86981201171875
Batch 40/64 loss: -0.8546934127807617
Batch 41/64 loss: -0.6774826049804688
Batch 42/64 loss: -0.945002555847168
Batch 43/64 loss: -0.45041942596435547
Batch 44/64 loss: -0.6042814254760742
Batch 45/64 loss: -0.6509561538696289
Batch 46/64 loss: -0.7317180633544922
Batch 47/64 loss: -0.7691526412963867
Batch 48/64 loss: -0.9638786315917969
Batch 49/64 loss: -0.5457897186279297
Batch 50/64 loss: -0.9200038909912109
Batch 51/64 loss: -0.23113727569580078
Batch 52/64 loss: -0.5943021774291992
Batch 53/64 loss: -0.5707130432128906
Batch 54/64 loss: -0.7275362014770508
Batch 55/64 loss: -0.8929281234741211
Batch 56/64 loss: -0.819270133972168
Batch 57/64 loss: -0.5178213119506836
Batch 58/64 loss: -0.7595500946044922
Batch 59/64 loss: -0.41167449951171875
Batch 60/64 loss: -0.968536376953125
Batch 61/64 loss: -0.9475231170654297
Batch 62/64 loss: -0.5755634307861328
Batch 63/64 loss: -0.2168560028076172
Batch 64/64 loss: -4.881925582885742
Epoch 402  Train loss: -0.7729940451827704  Val loss: -0.8845113905025102
Epoch 403
-------------------------------
Batch 1/64 loss: -0.9367561340332031
Batch 2/64 loss: -0.8749980926513672
Batch 3/64 loss: -0.6816730499267578
Batch 4/64 loss: -0.8203792572021484
Batch 5/64 loss: -0.578033447265625
Batch 6/64 loss: -1.0540142059326172
Batch 7/64 loss: -0.7953853607177734
Batch 8/64 loss: -0.7174873352050781
Batch 9/64 loss: -1.024693489074707
Batch 10/64 loss: -0.8055143356323242
Batch 11/64 loss: -1.0948915481567383
Batch 12/64 loss: -0.9535207748413086
Batch 13/64 loss: -0.9179019927978516
Batch 14/64 loss: -0.26891136169433594
Batch 15/64 loss: -0.5718898773193359
Batch 16/64 loss: -0.8053350448608398
Batch 17/64 loss: -0.782862663269043
Batch 18/64 loss: -0.8230152130126953
Batch 19/64 loss: -0.5971488952636719
Batch 20/64 loss: -0.7798871994018555
Batch 21/64 loss: -0.6539573669433594
Batch 22/64 loss: -0.7065658569335938
Batch 23/64 loss: -1.0224199295043945
Batch 24/64 loss: -0.905487060546875
Batch 25/64 loss: -0.49611854553222656
Batch 26/64 loss: -0.6946334838867188
Batch 27/64 loss: -0.7401609420776367
Batch 28/64 loss: -0.4693593978881836
Batch 29/64 loss: -0.7243070602416992
Batch 30/64 loss: -1.1187810897827148
Batch 31/64 loss: -0.5365762710571289
Batch 32/64 loss: -1.060805320739746
Batch 33/64 loss: -0.7286243438720703
Batch 34/64 loss: -0.6002225875854492
Batch 35/64 loss: -0.9891786575317383
Batch 36/64 loss: -0.4833049774169922
Batch 37/64 loss: -0.9905118942260742
Batch 38/64 loss: -0.32454967498779297
Batch 39/64 loss: -0.8926734924316406
Batch 40/64 loss: -0.5241384506225586
Batch 41/64 loss: -0.6780881881713867
Batch 42/64 loss: -0.8231773376464844
Batch 43/64 loss: -0.1808300018310547
Batch 44/64 loss: -0.09970760345458984
Batch 45/64 loss: -0.7191152572631836
Batch 46/64 loss: -0.6291608810424805
Batch 47/64 loss: -0.8444910049438477
Batch 48/64 loss: -0.5209188461303711
Batch 49/64 loss: -0.3151588439941406
Batch 50/64 loss: -0.8114919662475586
Batch 51/64 loss: -0.6750068664550781
Batch 52/64 loss: -0.6204843521118164
Batch 53/64 loss: -0.8347492218017578
Batch 54/64 loss: -0.7071752548217773
Batch 55/64 loss: -0.17372417449951172
Batch 56/64 loss: -0.7335853576660156
Batch 57/64 loss: -0.8893413543701172
Batch 58/64 loss: -0.4861431121826172
Batch 59/64 loss: -0.571868896484375
Batch 60/64 loss: -0.6414251327514648
Batch 61/64 loss: -0.7613964080810547
Batch 62/64 loss: -0.8814792633056641
Batch 63/64 loss: -0.5452289581298828
Batch 64/64 loss: -4.317216873168945
Epoch 403  Train loss: -0.7518170450247971  Val loss: -0.7660621564412854
Epoch 404
-------------------------------
Batch 1/64 loss: -0.5141334533691406
Batch 2/64 loss: -0.36201047897338867
Batch 3/64 loss: -0.7491474151611328
Batch 4/64 loss: -1.0068511962890625
Batch 5/64 loss: -0.9884271621704102
Batch 6/64 loss: -0.5529747009277344
Batch 7/64 loss: -0.586665153503418
Batch 8/64 loss: -0.6884069442749023
Batch 9/64 loss: -0.4750041961669922
Batch 10/64 loss: -0.2609109878540039
Batch 11/64 loss: -0.898681640625
Batch 12/64 loss: -0.4885215759277344
Batch 13/64 loss: -0.5556106567382812
Batch 14/64 loss: -0.7204971313476562
Batch 15/64 loss: -0.4136390686035156
Batch 16/64 loss: -0.36639881134033203
Batch 17/64 loss: -0.5045204162597656
Batch 18/64 loss: -0.7397623062133789
Batch 19/64 loss: -0.6192684173583984
Batch 20/64 loss: -0.7288017272949219
Batch 21/64 loss: -0.9188528060913086
Batch 22/64 loss: -0.7748079299926758
Batch 23/64 loss: -0.7420139312744141
Batch 24/64 loss: -0.5810470581054688
Batch 25/64 loss: -0.8297357559204102
Batch 26/64 loss: -0.8971633911132812
Batch 27/64 loss: -0.5760831832885742
Batch 28/64 loss: -1.0776748657226562
Batch 29/64 loss: -0.7806234359741211
Batch 30/64 loss: -0.5649013519287109
Batch 31/64 loss: -1.0441579818725586
Batch 32/64 loss: -0.8041477203369141
Batch 33/64 loss: -0.5797929763793945
Batch 34/64 loss: -0.9944581985473633
Batch 35/64 loss: -0.8751125335693359
Batch 36/64 loss: -0.9439058303833008
Batch 37/64 loss: -0.711029052734375
Batch 38/64 loss: -1.020512580871582
Batch 39/64 loss: -0.9152078628540039
Batch 40/64 loss: -0.8376913070678711
Batch 41/64 loss: -0.7206745147705078
Batch 42/64 loss: -0.7092227935791016
Batch 43/64 loss: -0.7468128204345703
Batch 44/64 loss: -0.11738109588623047
Batch 45/64 loss: -0.31519412994384766
Batch 46/64 loss: -0.8479042053222656
Batch 47/64 loss: -1.012099266052246
Batch 48/64 loss: -0.4400348663330078
Batch 49/64 loss: -0.496091365814209
Batch 50/64 loss: -0.8916511535644531
Batch 51/64 loss: -0.12942790985107422
Batch 52/64 loss: -0.47511959075927734
Batch 53/64 loss: -1.1641416549682617
Batch 54/64 loss: -0.723480224609375
Batch 55/64 loss: -1.0321359634399414
Batch 56/64 loss: -0.9734725952148438
Batch 57/64 loss: -0.8262968063354492
Batch 58/64 loss: -0.7463369369506836
Batch 59/64 loss: -1.0550556182861328
Batch 60/64 loss: -0.9634275436401367
Batch 61/64 loss: -0.7281303405761719
Batch 62/64 loss: -0.4797048568725586
Batch 63/64 loss: -0.4631214141845703
Batch 64/64 loss: -4.524431228637695
Epoch 404  Train loss: -0.755127753463446  Val loss: -0.8388355818810741
Epoch 405
-------------------------------
Batch 1/64 loss: -1.045135498046875
Batch 2/64 loss: -0.8873443603515625
Batch 3/64 loss: -0.8899564743041992
Batch 4/64 loss: -0.7427244186401367
Batch 5/64 loss: -0.717921257019043
Batch 6/64 loss: -0.28899097442626953
Batch 7/64 loss: -0.6960935592651367
Batch 8/64 loss: -0.6608905792236328
Batch 9/64 loss: -1.0715446472167969
Batch 10/64 loss: -0.7742061614990234
Batch 11/64 loss: -0.772435188293457
Batch 12/64 loss: -0.7990198135375977
Batch 13/64 loss: -0.6627225875854492
Batch 14/64 loss: -0.656437873840332
Batch 15/64 loss: -0.6668939590454102
Batch 16/64 loss: -0.7112541198730469
Batch 17/64 loss: -1.128000259399414
Batch 18/64 loss: -0.9020509719848633
Batch 19/64 loss: -1.0887250900268555
Batch 20/64 loss: -0.9721946716308594
Batch 21/64 loss: -0.9212007522583008
Batch 22/64 loss: -0.8550605773925781
Batch 23/64 loss: -0.6398239135742188
Batch 24/64 loss: -0.7456483840942383
Batch 25/64 loss: -0.7016181945800781
Batch 26/64 loss: -0.6803207397460938
Batch 27/64 loss: -1.034043312072754
Batch 28/64 loss: -0.8662424087524414
Batch 29/64 loss: -0.6994152069091797
Batch 30/64 loss: -0.6545257568359375
Batch 31/64 loss: -0.7263927459716797
Batch 32/64 loss: -0.4317646026611328
Batch 33/64 loss: -0.43244266510009766
Batch 34/64 loss: -0.9377593994140625
Batch 35/64 loss: -1.062361717224121
Batch 36/64 loss: -0.5948905944824219
Batch 37/64 loss: -0.6933126449584961
Batch 38/64 loss: -0.500819206237793
Batch 39/64 loss: -0.23811054229736328
Batch 40/64 loss: -1.038985252380371
Batch 41/64 loss: -0.8786020278930664
Batch 42/64 loss: -0.5557670593261719
Batch 43/64 loss: -0.9867057800292969
Batch 44/64 loss: -0.8409080505371094
Batch 45/64 loss: -0.7452220916748047
Batch 46/64 loss: -0.5600738525390625
Batch 47/64 loss: -0.5397377014160156
Batch 48/64 loss: -0.4294147491455078
Batch 49/64 loss: -0.7481536865234375
Batch 50/64 loss: -0.5108308792114258
Batch 51/64 loss: -0.8163213729858398
Batch 52/64 loss: -0.883061408996582
Batch 53/64 loss: -0.592005729675293
Batch 54/64 loss: -0.5918092727661133
Batch 55/64 loss: -0.7835140228271484
Batch 56/64 loss: -1.1335811614990234
Batch 57/64 loss: -0.31237316131591797
Batch 58/64 loss: -0.3218402862548828
Batch 59/64 loss: -0.5728139877319336
Batch 60/64 loss: -0.9146394729614258
Batch 61/64 loss: -0.9488716125488281
Batch 62/64 loss: -0.4220571517944336
Batch 63/64 loss: -0.7046365737915039
Batch 64/64 loss: -4.8656816482543945
Epoch 405  Train loss: -0.7848075829300226  Val loss: -0.7460299619694346
Epoch 406
-------------------------------
Batch 1/64 loss: -0.5125951766967773
Batch 2/64 loss: -0.7284822463989258
Batch 3/64 loss: -0.4037437438964844
Batch 4/64 loss: -0.721257209777832
Batch 5/64 loss: -0.6543989181518555
Batch 6/64 loss: -0.7143430709838867
Batch 7/64 loss: -0.7594404220581055
Batch 8/64 loss: -0.9094467163085938
Batch 9/64 loss: -0.9952096939086914
Batch 10/64 loss: -0.5213813781738281
Batch 11/64 loss: -0.9935131072998047
Batch 12/64 loss: -1.0023527145385742
Batch 13/64 loss: -0.7994403839111328
Batch 14/64 loss: -0.807011604309082
Batch 15/64 loss: -0.9570722579956055
Batch 16/64 loss: -0.5404214859008789
Batch 17/64 loss: -0.7220640182495117
Batch 18/64 loss: -0.24410057067871094
Batch 19/64 loss: -0.8809413909912109
Batch 20/64 loss: -0.6866817474365234
Batch 21/64 loss: -1.143331527709961
Batch 22/64 loss: -0.9110021591186523
Batch 23/64 loss: -0.6492362022399902
Batch 24/64 loss: -1.1489324569702148
Batch 25/64 loss: -0.6438302993774414
Batch 26/64 loss: -1.0207109451293945
Batch 27/64 loss: -0.5025148391723633
Batch 28/64 loss: -0.7763128280639648
Batch 29/64 loss: -0.8388605117797852
Batch 30/64 loss: -0.44412994384765625
Batch 31/64 loss: -1.0213184356689453
Batch 32/64 loss: -0.9771862030029297
Batch 33/64 loss: -0.858612060546875
Batch 34/64 loss: -0.8473958969116211
Batch 35/64 loss: -0.879115104675293
Batch 36/64 loss: -0.9222040176391602
Batch 37/64 loss: -0.7676820755004883
Batch 38/64 loss: -0.27310943603515625
Batch 39/64 loss: -0.6127090454101562
Batch 40/64 loss: -0.6048107147216797
Batch 41/64 loss: -0.9301967620849609
Batch 42/64 loss: -1.057474136352539
Batch 43/64 loss: -0.4389963150024414
Batch 44/64 loss: -1.0334768295288086
Batch 45/64 loss: -0.9576473236083984
Batch 46/64 loss: -1.0301752090454102
Batch 47/64 loss: -0.5359992980957031
Batch 48/64 loss: -0.9417238235473633
Batch 49/64 loss: -1.1710357666015625
Batch 50/64 loss: -0.4722456932067871
Batch 51/64 loss: -0.7667379379272461
Batch 52/64 loss: -1.0127220153808594
Batch 53/64 loss: -0.867833137512207
Batch 54/64 loss: -0.9056787490844727
Batch 55/64 loss: -0.7975950241088867
Batch 56/64 loss: -0.8610239028930664
Batch 57/64 loss: -0.8960514068603516
Batch 58/64 loss: -0.5392684936523438
Batch 59/64 loss: -0.6388025283813477
Batch 60/64 loss: -0.36501312255859375
Batch 61/64 loss: -0.839869499206543
Batch 62/64 loss: -0.7666358947753906
Batch 63/64 loss: -0.9798564910888672
Batch 64/64 loss: -5.149514198303223
Epoch 406  Train loss: -0.8323936873791264  Val loss: -0.9187593361766068
Epoch 407
-------------------------------
Batch 1/64 loss: -1.0130558013916016
Batch 2/64 loss: -0.8302421569824219
Batch 3/64 loss: -0.8543481826782227
Batch 4/64 loss: -1.0465669631958008
Batch 5/64 loss: -0.8116769790649414
Batch 6/64 loss: -1.1152715682983398
Batch 7/64 loss: -0.9464302062988281
Batch 8/64 loss: -1.0287294387817383
Batch 9/64 loss: -0.9002094268798828
Batch 10/64 loss: -0.9654350280761719
Batch 11/64 loss: -0.5713567733764648
Batch 12/64 loss: -0.9896116256713867
Batch 13/64 loss: -0.6244106292724609
Batch 14/64 loss: -0.5392637252807617
Batch 15/64 loss: -0.5931549072265625
Batch 16/64 loss: -0.9087114334106445
Batch 17/64 loss: -0.9755783081054688
Batch 18/64 loss: -1.004434585571289
Batch 19/64 loss: -0.6826915740966797
Batch 20/64 loss: -0.4394369125366211
Batch 21/64 loss: -0.7613677978515625
Batch 22/64 loss: -0.8224201202392578
Batch 23/64 loss: -0.7831354141235352
Batch 24/64 loss: -1.1140851974487305
Batch 25/64 loss: -0.7904167175292969
Batch 26/64 loss: -0.9275712966918945
Batch 27/64 loss: -0.8898983001708984
Batch 28/64 loss: -0.02580118179321289
Batch 29/64 loss: -0.9514570236206055
Batch 30/64 loss: -0.5997610092163086
Batch 31/64 loss: -0.49811363220214844
Batch 32/64 loss: -0.9120826721191406
Batch 33/64 loss: -0.9011945724487305
Batch 34/64 loss: -0.6655378341674805
Batch 35/64 loss: -0.3922395706176758
Batch 36/64 loss: -0.6366763114929199
Batch 37/64 loss: -0.7320585250854492
Batch 38/64 loss: -0.8341255187988281
Batch 39/64 loss: -0.6030550003051758
Batch 40/64 loss: -0.6907205581665039
Batch 41/64 loss: -0.8502235412597656
Batch 42/64 loss: -0.7986841201782227
Batch 43/64 loss: -0.6881809234619141
Batch 44/64 loss: -0.9388227462768555
Batch 45/64 loss: -0.8782176971435547
Batch 46/64 loss: -0.9105548858642578
Batch 47/64 loss: -0.5060920715332031
Batch 48/64 loss: -0.9929943084716797
Batch 49/64 loss: -0.07082128524780273
Batch 50/64 loss: -0.7521886825561523
Batch 51/64 loss: -0.17029285430908203
Batch 52/64 loss: -0.7000551223754883
Batch 53/64 loss: -0.6711034774780273
Batch 54/64 loss: -0.3836851119995117
Batch 55/64 loss: -0.4725818634033203
Batch 56/64 loss: -0.5178098678588867
Batch 57/64 loss: -1.0642509460449219
Batch 58/64 loss: 0.04374980926513672
Batch 59/64 loss: -1.1218137741088867
Batch 60/64 loss: -1.0432977676391602
Batch 61/64 loss: -0.9117975234985352
Batch 62/64 loss: -0.818638801574707
Batch 63/64 loss: -1.087198257446289
Batch 64/64 loss: -4.985019683837891
Epoch 407  Train loss: -0.8065357657039867  Val loss: -0.7186653556692641
Epoch 408
-------------------------------
Batch 1/64 loss: -1.0013694763183594
Batch 2/64 loss: -0.6942272186279297
Batch 3/64 loss: -0.7930135726928711
Batch 4/64 loss: -0.8230972290039062
Batch 5/64 loss: -0.945347785949707
Batch 6/64 loss: -0.587493896484375
Batch 7/64 loss: -0.7365140914916992
Batch 8/64 loss: -0.7542181015014648
Batch 9/64 loss: -0.7062005996704102
Batch 10/64 loss: -0.5328073501586914
Batch 11/64 loss: -0.7194805145263672
Batch 12/64 loss: -0.4425506591796875
Batch 13/64 loss: -0.9175291061401367
Batch 14/64 loss: -0.7749366760253906
Batch 15/64 loss: -0.7574806213378906
Batch 16/64 loss: -0.8897771835327148
Batch 17/64 loss: -0.7717723846435547
Batch 18/64 loss: -0.6212453842163086
Batch 19/64 loss: -0.4970073699951172
Batch 20/64 loss: -0.7762184143066406
Batch 21/64 loss: -0.8108310699462891
Batch 22/64 loss: -0.6899375915527344
Batch 23/64 loss: -0.9265499114990234
Batch 24/64 loss: -0.47705554962158203
Batch 25/64 loss: -0.6564788818359375
Batch 26/64 loss: -0.8472805023193359
Batch 27/64 loss: -0.6756458282470703
Batch 28/64 loss: -0.2166910171508789
Batch 29/64 loss: -0.7442922592163086
Batch 30/64 loss: 0.3523845672607422
Batch 31/64 loss: -0.5967864990234375
Batch 32/64 loss: -0.7825841903686523
Batch 33/64 loss: -0.9773674011230469
Batch 34/64 loss: -0.6203708648681641
Batch 35/64 loss: -0.555330753326416
Batch 36/64 loss: -0.8768291473388672
Batch 37/64 loss: -0.8927898406982422
Batch 38/64 loss: -0.7641725540161133
Batch 39/64 loss: -1.0556936264038086
Batch 40/64 loss: -1.0822162628173828
Batch 41/64 loss: -0.714747428894043
Batch 42/64 loss: -0.7648897171020508
Batch 43/64 loss: -0.6411075592041016
Batch 44/64 loss: -0.544764518737793
Batch 45/64 loss: -0.5376901626586914
Batch 46/64 loss: -1.027419090270996
Batch 47/64 loss: -0.4759702682495117
Batch 48/64 loss: -0.848210334777832
Batch 49/64 loss: -0.4528684616088867
Batch 50/64 loss: -0.8475656509399414
Batch 51/64 loss: -1.0060453414916992
Batch 52/64 loss: -0.7545719146728516
Batch 53/64 loss: -0.5261077880859375
Batch 54/64 loss: -0.7234983444213867
Batch 55/64 loss: -0.7929849624633789
Batch 56/64 loss: -0.6915922164916992
Batch 57/64 loss: -0.42235851287841797
Batch 58/64 loss: -0.5776042938232422
Batch 59/64 loss: -0.9044294357299805
Batch 60/64 loss: -0.7797431945800781
Batch 61/64 loss: -0.9354143142700195
Batch 62/64 loss: -0.5588912963867188
Batch 63/64 loss: -1.049576759338379
Batch 64/64 loss: -3.692795753479004
Epoch 408  Train loss: -0.7527287090525908  Val loss: -0.8799391022252873
Epoch 409
-------------------------------
Batch 1/64 loss: -0.9966011047363281
Batch 2/64 loss: -0.8686895370483398
Batch 3/64 loss: -0.3290271759033203
Batch 4/64 loss: -0.7353105545043945
Batch 5/64 loss: -0.7592267990112305
Batch 6/64 loss: -1.0214357376098633
Batch 7/64 loss: -0.49346160888671875
Batch 8/64 loss: -0.9220685958862305
Batch 9/64 loss: -0.5911989212036133
Batch 10/64 loss: -0.5312795639038086
Batch 11/64 loss: -0.4965658187866211
Batch 12/64 loss: -0.5503301620483398
Batch 13/64 loss: -0.9451484680175781
Batch 14/64 loss: -0.9126005172729492
Batch 15/64 loss: -0.9377403259277344
Batch 16/64 loss: -0.6074943542480469
Batch 17/64 loss: -0.891230583190918
Batch 18/64 loss: -0.942347526550293
Batch 19/64 loss: -0.7832870483398438
Batch 20/64 loss: -0.8639144897460938
Batch 21/64 loss: -0.8533439636230469
Batch 22/64 loss: -0.8050537109375
Batch 23/64 loss: -0.6538162231445312
Batch 24/64 loss: -0.9867944717407227
Batch 25/64 loss: -0.7837800979614258
Batch 26/64 loss: -0.8107528686523438
Batch 27/64 loss: -0.1122121810913086
Batch 28/64 loss: -0.83319091796875
Batch 29/64 loss: -0.8120355606079102
Batch 30/64 loss: -0.8647060394287109
Batch 31/64 loss: -0.8660697937011719
Batch 32/64 loss: -0.6572036743164062
Batch 33/64 loss: -0.6165132522583008
Batch 34/64 loss: -0.7538509368896484
Batch 35/64 loss: -0.8279447555541992
Batch 36/64 loss: -0.5543851852416992
Batch 37/64 loss: -0.40801429748535156
Batch 38/64 loss: -0.39040088653564453
Batch 39/64 loss: -0.9831676483154297
Batch 40/64 loss: -0.805293083190918
Batch 41/64 loss: -0.9330739974975586
Batch 42/64 loss: -0.9157752990722656
Batch 43/64 loss: -0.9253063201904297
Batch 44/64 loss: -0.07820510864257812
Batch 45/64 loss: -0.7509031295776367
Batch 46/64 loss: -1.0699682235717773
Batch 47/64 loss: -0.4540700912475586
Batch 48/64 loss: -0.8653097152709961
Batch 49/64 loss: -0.4621601104736328
Batch 50/64 loss: -1.1424379348754883
Batch 51/64 loss: -0.7010202407836914
Batch 52/64 loss: -0.29698801040649414
Batch 53/64 loss: -1.074045181274414
Batch 54/64 loss: -0.5715427398681641
Batch 55/64 loss: -0.5577354431152344
Batch 56/64 loss: -0.9758148193359375
Batch 57/64 loss: -0.9335212707519531
Batch 58/64 loss: -0.4560403823852539
Batch 59/64 loss: -0.5058803558349609
Batch 60/64 loss: -0.3764967918395996
Batch 61/64 loss: -0.958155632019043
Batch 62/64 loss: -0.10024881362915039
Batch 63/64 loss: -0.8407926559448242
Batch 64/64 loss: -4.42624568939209
Epoch 409  Train loss: -0.7658457251156078  Val loss: -0.8046926255897968
Epoch 410
-------------------------------
Batch 1/64 loss: -0.3472280502319336
Batch 2/64 loss: -0.896306037902832
Batch 3/64 loss: -0.5865373611450195
Batch 4/64 loss: -0.43157196044921875
Batch 5/64 loss: -0.9264192581176758
Batch 6/64 loss: -0.6680612564086914
Batch 7/64 loss: -1.0610761642456055
Batch 8/64 loss: -1.0060853958129883
Batch 9/64 loss: -0.9043464660644531
Batch 10/64 loss: -0.6705121994018555
Batch 11/64 loss: -0.6132068634033203
Batch 12/64 loss: -1.0633745193481445
Batch 13/64 loss: -0.8409557342529297
Batch 14/64 loss: -0.6796379089355469
Batch 15/64 loss: -0.7616710662841797
Batch 16/64 loss: -1.0921821594238281
Batch 17/64 loss: -0.6658449172973633
Batch 18/64 loss: -0.7912178039550781
Batch 19/64 loss: -0.9561424255371094
Batch 20/64 loss: -0.3676323890686035
Batch 21/64 loss: -0.9065456390380859
Batch 22/64 loss: -1.0417089462280273
Batch 23/64 loss: -0.46773624420166016
Batch 24/64 loss: -0.4754180908203125
Batch 25/64 loss: -0.7455339431762695
Batch 26/64 loss: -0.9105062484741211
Batch 27/64 loss: -0.7012042999267578
Batch 28/64 loss: -0.4440317153930664
Batch 29/64 loss: -0.9028501510620117
Batch 30/64 loss: -0.42469215393066406
Batch 31/64 loss: -0.9627094268798828
Batch 32/64 loss: -0.7228274345397949
Batch 33/64 loss: -1.0447359085083008
Batch 34/64 loss: -0.5768957138061523
Batch 35/64 loss: -0.5798540115356445
Batch 36/64 loss: -0.6154041290283203
Batch 37/64 loss: -0.9071788787841797
Batch 38/64 loss: -0.7071008682250977
Batch 39/64 loss: -0.7784309387207031
Batch 40/64 loss: -0.7683925628662109
Batch 41/64 loss: -1.1717119216918945
Batch 42/64 loss: -0.53167724609375
Batch 43/64 loss: -0.43892860412597656
Batch 44/64 loss: -0.5758485794067383
Batch 45/64 loss: -1.0511817932128906
Batch 46/64 loss: -0.8318548202514648
Batch 47/64 loss: -0.33249950408935547
Batch 48/64 loss: -0.9959192276000977
Batch 49/64 loss: -1.0236740112304688
Batch 50/64 loss: -0.9654455184936523
Batch 51/64 loss: -0.6188497543334961
Batch 52/64 loss: -0.8238506317138672
Batch 53/64 loss: -0.9514303207397461
Batch 54/64 loss: -0.5611171722412109
Batch 55/64 loss: -0.8059635162353516
Batch 56/64 loss: -0.5882139205932617
Batch 57/64 loss: -0.5731954574584961
Batch 58/64 loss: -0.7302322387695312
Batch 59/64 loss: -0.7513151168823242
Batch 60/64 loss: -0.5100069046020508
Batch 61/64 loss: -0.9284868240356445
Batch 62/64 loss: -0.8598318099975586
Batch 63/64 loss: -0.5236282348632812
Batch 64/64 loss: -4.570977687835693
Epoch 410  Train loss: -0.7935194295995376  Val loss: -0.8389321553338435
Epoch 411
-------------------------------
Batch 1/64 loss: -0.6296482086181641
Batch 2/64 loss: -1.1611270904541016
Batch 3/64 loss: -0.6371135711669922
Batch 4/64 loss: -0.6106691360473633
Batch 5/64 loss: -0.6352872848510742
Batch 6/64 loss: -0.4985027313232422
Batch 7/64 loss: -0.1723313331604004
Batch 8/64 loss: -0.7405300140380859
Batch 9/64 loss: -0.6998443603515625
Batch 10/64 loss: -0.9353713989257812
Batch 11/64 loss: -0.5926027297973633
Batch 12/64 loss: -0.5127019882202148
Batch 13/64 loss: -0.5238437652587891
Batch 14/64 loss: -0.5989627838134766
Batch 15/64 loss: -0.8148975372314453
Batch 16/64 loss: -0.6543197631835938
Batch 17/64 loss: -0.8039617538452148
Batch 18/64 loss: -0.44023990631103516
Batch 19/64 loss: -0.7265377044677734
Batch 20/64 loss: -0.9415769577026367
Batch 21/64 loss: -0.9188270568847656
Batch 22/64 loss: 0.30084705352783203
Batch 23/64 loss: -0.6895713806152344
Batch 24/64 loss: -0.9744205474853516
Batch 25/64 loss: -1.0724554061889648
Batch 26/64 loss: -0.7413835525512695
Batch 27/64 loss: -0.6272544860839844
Batch 28/64 loss: -0.9222087860107422
Batch 29/64 loss: -0.8953065872192383
Batch 30/64 loss: -0.5908536911010742
Batch 31/64 loss: -0.8417520523071289
Batch 32/64 loss: -0.7412929534912109
Batch 33/64 loss: -1.112992286682129
Batch 34/64 loss: -0.854456901550293
Batch 35/64 loss: -0.8008651733398438
Batch 36/64 loss: -0.7811450958251953
Batch 37/64 loss: -0.5769586563110352
Batch 38/64 loss: -0.9359254837036133
Batch 39/64 loss: -0.8544254302978516
Batch 40/64 loss: -0.7085418701171875
Batch 41/64 loss: -0.5770359039306641
Batch 42/64 loss: -1.0163993835449219
Batch 43/64 loss: -0.9188508987426758
Batch 44/64 loss: -0.7133970260620117
Batch 45/64 loss: -0.46697998046875
Batch 46/64 loss: -0.7840604782104492
Batch 47/64 loss: -1.0757865905761719
Batch 48/64 loss: -0.6816673278808594
Batch 49/64 loss: -0.35706329345703125
Batch 50/64 loss: -0.8089094161987305
Batch 51/64 loss: -1.0657548904418945
Batch 52/64 loss: -0.6629714965820312
Batch 53/64 loss: -0.3832721710205078
Batch 54/64 loss: -0.9389352798461914
Batch 55/64 loss: -1.0470952987670898
Batch 56/64 loss: -0.9578104019165039
Batch 57/64 loss: -0.5829744338989258
Batch 58/64 loss: -0.7652931213378906
Batch 59/64 loss: -0.9530057907104492
Batch 60/64 loss: -0.9997711181640625
Batch 61/64 loss: -1.096841812133789
Batch 62/64 loss: -0.9269142150878906
Batch 63/64 loss: -0.5384225845336914
Batch 64/64 loss: -4.798622131347656
Epoch 411  Train loss: -0.7935378803926356  Val loss: -0.8689250093964777
Epoch 412
-------------------------------
Batch 1/64 loss: -0.9944295883178711
Batch 2/64 loss: -1.0817756652832031
Batch 3/64 loss: -0.6384134292602539
Batch 4/64 loss: -0.6129484176635742
Batch 5/64 loss: -0.5234012603759766
Batch 6/64 loss: -0.9737749099731445
Batch 7/64 loss: -0.7391853332519531
Batch 8/64 loss: -0.8035364151000977
Batch 9/64 loss: -1.0392284393310547
Batch 10/64 loss: -0.9911355972290039
Batch 11/64 loss: -0.6285123825073242
Batch 12/64 loss: -1.0421314239501953
Batch 13/64 loss: -0.43515968322753906
Batch 14/64 loss: -0.44202232360839844
Batch 15/64 loss: -0.7249612808227539
Batch 16/64 loss: -0.7436676025390625
Batch 17/64 loss: -0.6556205749511719
Batch 18/64 loss: -1.0211601257324219
Batch 19/64 loss: -1.053297996520996
Batch 20/64 loss: -0.20556068420410156
Batch 21/64 loss: -0.8964033126831055
Batch 22/64 loss: -1.1299171447753906
Batch 23/64 loss: -0.5757427215576172
Batch 24/64 loss: -0.5575494766235352
Batch 25/64 loss: -0.9905624389648438
Batch 26/64 loss: -0.7785272598266602
Batch 27/64 loss: -0.725764274597168
Batch 28/64 loss: -0.80621337890625
Batch 29/64 loss: -0.7715635299682617
Batch 30/64 loss: -1.0073137283325195
Batch 31/64 loss: -0.7041635513305664
Batch 32/64 loss: -0.9633989334106445
Batch 33/64 loss: -0.9082212448120117
Batch 34/64 loss: -1.2306060791015625
Batch 35/64 loss: -0.5553731918334961
Batch 36/64 loss: -0.591883659362793
Batch 37/64 loss: -0.49722862243652344
Batch 38/64 loss: -0.8946552276611328
Batch 39/64 loss: -0.7243671417236328
Batch 40/64 loss: -0.8457889556884766
Batch 41/64 loss: -1.1040477752685547
Batch 42/64 loss: -0.8651752471923828
Batch 43/64 loss: -1.0704679489135742
Batch 44/64 loss: -0.2883148193359375
Batch 45/64 loss: -0.9121437072753906
Batch 46/64 loss: -0.34160614013671875
Batch 47/64 loss: -0.758824348449707
Batch 48/64 loss: -0.6834859848022461
Batch 49/64 loss: -0.5145359039306641
Batch 50/64 loss: -0.633234977722168
Batch 51/64 loss: -0.8718633651733398
Batch 52/64 loss: -1.165283203125
Batch 53/64 loss: -0.8729248046875
Batch 54/64 loss: -0.8143730163574219
Batch 55/64 loss: -1.125905990600586
Batch 56/64 loss: -0.8696050643920898
Batch 57/64 loss: -0.7787303924560547
Batch 58/64 loss: -0.9552621841430664
Batch 59/64 loss: -0.9734888076782227
Batch 60/64 loss: -0.8317461013793945
Batch 61/64 loss: -0.6429595947265625
Batch 62/64 loss: -0.6387853622436523
Batch 63/64 loss: -1.2692451477050781
Batch 64/64 loss: -4.885620594024658
Epoch 412  Train loss: -0.8494336053436877  Val loss: -0.9038992812953044
Epoch 413
-------------------------------
Batch 1/64 loss: -1.0654525756835938
Batch 2/64 loss: -0.40161991119384766
Batch 3/64 loss: -0.5287761688232422
Batch 4/64 loss: -0.8520240783691406
Batch 5/64 loss: -0.9952201843261719
Batch 6/64 loss: -0.9523000717163086
Batch 7/64 loss: -0.744842529296875
Batch 8/64 loss: -0.6381559371948242
Batch 9/64 loss: -0.7722558975219727
Batch 10/64 loss: -1.1618318557739258
Batch 11/64 loss: -0.6148653030395508
Batch 12/64 loss: -0.8057842254638672
Batch 13/64 loss: -1.0307445526123047
Batch 14/64 loss: -0.9174823760986328
Batch 15/64 loss: -0.6173038482666016
Batch 16/64 loss: -1.1934661865234375
Batch 17/64 loss: -0.7645902633666992
Batch 18/64 loss: -0.994999885559082
Batch 19/64 loss: -1.0503292083740234
Batch 20/64 loss: -0.8318996429443359
Batch 21/64 loss: -1.0523080825805664
Batch 22/64 loss: -0.4500389099121094
Batch 23/64 loss: -1.0693187713623047
Batch 24/64 loss: -0.3926410675048828
Batch 25/64 loss: -0.793212890625
Batch 26/64 loss: -0.7840232849121094
Batch 27/64 loss: -1.1777095794677734
Batch 28/64 loss: -1.0161609649658203
Batch 29/64 loss: -0.5558176040649414
Batch 30/64 loss: -0.7725534439086914
Batch 31/64 loss: -0.7363710403442383
Batch 32/64 loss: -0.7926797866821289
Batch 33/64 loss: -0.35575294494628906
Batch 34/64 loss: -0.6887998580932617
Batch 35/64 loss: -0.15566062927246094
Batch 36/64 loss: -0.7496509552001953
Batch 37/64 loss: -0.8935203552246094
Batch 38/64 loss: -0.801234245300293
Batch 39/64 loss: -1.0930280685424805
Batch 40/64 loss: -0.3662748336791992
Batch 41/64 loss: -0.5606508255004883
Batch 42/64 loss: -1.0955791473388672
Batch 43/64 loss: -0.8523597717285156
Batch 44/64 loss: -0.41396045684814453
Batch 45/64 loss: -0.4954719543457031
Batch 46/64 loss: -0.7027673721313477
Batch 47/64 loss: -1.1706457138061523
Batch 48/64 loss: -0.7222375869750977
Batch 49/64 loss: -0.7638940811157227
Batch 50/64 loss: -0.5825386047363281
Batch 51/64 loss: -1.1033658981323242
Batch 52/64 loss: -0.6088895797729492
Batch 53/64 loss: -0.8230600357055664
Batch 54/64 loss: -0.6790256500244141
Batch 55/64 loss: -0.8398418426513672
Batch 56/64 loss: -0.5222082138061523
Batch 57/64 loss: -0.6633434295654297
Batch 58/64 loss: -0.8388605117797852
Batch 59/64 loss: -0.6292285919189453
Batch 60/64 loss: -1.036036491394043
Batch 61/64 loss: -0.7997636795043945
Batch 62/64 loss: -0.9620342254638672
Batch 63/64 loss: 0.029764175415039062
Batch 64/64 loss: -4.332632541656494
Epoch 413  Train loss: -0.8112341319813448  Val loss: -0.5278217931793318
Epoch 414
-------------------------------
Batch 1/64 loss: -0.609583854675293
Batch 2/64 loss: -0.43792152404785156
Batch 3/64 loss: -0.3927888870239258
Batch 4/64 loss: -0.3797788619995117
Batch 5/64 loss: -0.45552730560302734
Batch 6/64 loss: -0.7035303115844727
Batch 7/64 loss: -0.5319967269897461
Batch 8/64 loss: -0.8874225616455078
Batch 9/64 loss: -0.5789871215820312
Batch 10/64 loss: -0.5029535293579102
Batch 11/64 loss: -0.9357070922851562
Batch 12/64 loss: -0.8637218475341797
Batch 13/64 loss: -0.936863899230957
Batch 14/64 loss: -0.3993072509765625
Batch 15/64 loss: -0.6026115417480469
Batch 16/64 loss: 0.15685224533081055
Batch 17/64 loss: -0.8358325958251953
Batch 18/64 loss: -0.9492330551147461
Batch 19/64 loss: -0.688227653503418
Batch 20/64 loss: -0.3234691619873047
Batch 21/64 loss: 0.1697988510131836
Batch 22/64 loss: -0.890599250793457
Batch 23/64 loss: -0.9960451126098633
Batch 24/64 loss: -0.43116283416748047
Batch 25/64 loss: -0.8167314529418945
Batch 26/64 loss: -0.7551364898681641
Batch 27/64 loss: -0.5914363861083984
Batch 28/64 loss: -0.8034400939941406
Batch 29/64 loss: -0.6930551528930664
Batch 30/64 loss: -0.6090526580810547
Batch 31/64 loss: -1.1306276321411133
Batch 32/64 loss: -0.637141227722168
Batch 33/64 loss: -1.0130949020385742
Batch 34/64 loss: -0.25846099853515625
Batch 35/64 loss: -0.6145153045654297
Batch 36/64 loss: -0.9999799728393555
Batch 37/64 loss: -0.9314727783203125
Batch 38/64 loss: -0.8416776657104492
Batch 39/64 loss: -1.0701370239257812
Batch 40/64 loss: -0.7193994522094727
Batch 41/64 loss: -0.9638280868530273
Batch 42/64 loss: -0.7077999114990234
Batch 43/64 loss: -1.094304084777832
Batch 44/64 loss: -0.7438478469848633
Batch 45/64 loss: -0.4308052062988281
Batch 46/64 loss: -1.2511663436889648
Batch 47/64 loss: -1.0492467880249023
Batch 48/64 loss: -0.8982486724853516
Batch 49/64 loss: -0.6402616500854492
Batch 50/64 loss: -0.5315151214599609
Batch 51/64 loss: -0.8549327850341797
Batch 52/64 loss: -0.7101259231567383
Batch 53/64 loss: -0.9235658645629883
Batch 54/64 loss: -0.6209774017333984
Batch 55/64 loss: -0.7128534317016602
Batch 56/64 loss: -0.3430595397949219
Batch 57/64 loss: -0.5889396667480469
Batch 58/64 loss: -0.7533121109008789
Batch 59/64 loss: -0.8484001159667969
Batch 60/64 loss: -0.8282470703125
Batch 61/64 loss: -0.9054145812988281
Batch 62/64 loss: -0.21826744079589844
Batch 63/64 loss: -0.8407812118530273
Batch 64/64 loss: -4.827242374420166
Epoch 414  Train loss: -0.7462323562771667  Val loss: -0.8231269731554379
Epoch 415
-------------------------------
Batch 1/64 loss: -0.8498935699462891
Batch 2/64 loss: -0.8397121429443359
Batch 3/64 loss: -1.0055904388427734
Batch 4/64 loss: -0.4856090545654297
Batch 5/64 loss: -0.8859434127807617
Batch 6/64 loss: -0.9376306533813477
Batch 7/64 loss: -0.5223369598388672
Batch 8/64 loss: -0.679163932800293
Batch 9/64 loss: -1.060983657836914
Batch 10/64 loss: -0.530756950378418
Batch 11/64 loss: -0.9713411331176758
Batch 12/64 loss: -0.3567676544189453
Batch 13/64 loss: -0.8740825653076172
Batch 14/64 loss: -0.4724407196044922
Batch 15/64 loss: -0.5042142868041992
Batch 16/64 loss: -0.7862386703491211
Batch 17/64 loss: -0.9087543487548828
Batch 18/64 loss: -0.3292837142944336
Batch 19/64 loss: -0.6929140090942383
Batch 20/64 loss: -0.7300901412963867
Batch 21/64 loss: -1.0157384872436523
Batch 22/64 loss: -0.9322595596313477
Batch 23/64 loss: -0.7146110534667969
Batch 24/64 loss: -0.7401552200317383
Batch 25/64 loss: -0.5058526992797852
Batch 26/64 loss: -0.6512594223022461
Batch 27/64 loss: -0.8522624969482422
Batch 28/64 loss: -0.5298175811767578
Batch 29/64 loss: -0.34101200103759766
Batch 30/64 loss: -0.8791580200195312
Batch 31/64 loss: -0.9484596252441406
Batch 32/64 loss: -0.9440078735351562
Batch 33/64 loss: -0.9190855026245117
Batch 34/64 loss: -0.9740581512451172
Batch 35/64 loss: -0.8134031295776367
Batch 36/64 loss: -0.33832359313964844
Batch 37/64 loss: -0.9197530746459961
Batch 38/64 loss: -1.1513347625732422
Batch 39/64 loss: -1.1212282180786133
Batch 40/64 loss: -1.195347785949707
Batch 41/64 loss: -0.9500560760498047
Batch 42/64 loss: -1.0820388793945312
Batch 43/64 loss: -0.548431396484375
Batch 44/64 loss: -0.9294300079345703
Batch 45/64 loss: -1.0273923873901367
Batch 46/64 loss: -0.5788583755493164
Batch 47/64 loss: -0.8141717910766602
Batch 48/64 loss: -1.2640266418457031
Batch 49/64 loss: -0.8842086791992188
Batch 50/64 loss: -0.6445388793945312
Batch 51/64 loss: -0.6646690368652344
Batch 52/64 loss: -0.9714326858520508
Batch 53/64 loss: -1.2228517532348633
Batch 54/64 loss: -0.840484619140625
Batch 55/64 loss: -0.7007045745849609
Batch 56/64 loss: -0.6950798034667969
Batch 57/64 loss: -1.1284332275390625
Batch 58/64 loss: -0.7261362075805664
Batch 59/64 loss: -0.1917591094970703
Batch 60/64 loss: -0.6974306106567383
Batch 61/64 loss: -0.7727470397949219
Batch 62/64 loss: -0.7482852935791016
Batch 63/64 loss: -0.7825384140014648
Batch 64/64 loss: -4.693059921264648
Epoch 415  Train loss: -0.8360215953752106  Val loss: -0.8800468838091978
Epoch 416
-------------------------------
Batch 1/64 loss: -0.41544246673583984
Batch 2/64 loss: -0.33852672576904297
Batch 3/64 loss: -0.920689582824707
Batch 4/64 loss: -1.0060100555419922
Batch 5/64 loss: -0.8674154281616211
Batch 6/64 loss: -0.5053205490112305
Batch 7/64 loss: -0.6467819213867188
Batch 8/64 loss: -1.1384878158569336
Batch 9/64 loss: -0.8644199371337891
Batch 10/64 loss: -1.1106834411621094
Batch 11/64 loss: -0.978184700012207
Batch 12/64 loss: -0.7661752700805664
Batch 13/64 loss: -0.962092399597168
Batch 14/64 loss: -0.5331754684448242
Batch 15/64 loss: -0.8641986846923828
Batch 16/64 loss: -0.6730508804321289
Batch 17/64 loss: -0.7824344635009766
Batch 18/64 loss: -0.8389663696289062
Batch 19/64 loss: -0.701441764831543
Batch 20/64 loss: -1.1346588134765625
Batch 21/64 loss: -0.8387651443481445
Batch 22/64 loss: -0.914372444152832
Batch 23/64 loss: -0.7954797744750977
Batch 24/64 loss: -0.8057422637939453
Batch 25/64 loss: -0.5003461837768555
Batch 26/64 loss: -1.08526611328125
Batch 27/64 loss: -0.9019279479980469
Batch 28/64 loss: -0.9752111434936523
Batch 29/64 loss: -1.1697940826416016
Batch 30/64 loss: -0.9561309814453125
Batch 31/64 loss: -0.8934135437011719
Batch 32/64 loss: -0.6333484649658203
Batch 33/64 loss: -0.8863105773925781
Batch 34/64 loss: -0.8754949569702148
Batch 35/64 loss: -0.6577157974243164
Batch 36/64 loss: -0.7060861587524414
Batch 37/64 loss: -0.6453390121459961
Batch 38/64 loss: -0.8685579299926758
Batch 39/64 loss: -0.7605438232421875
Batch 40/64 loss: -0.3600168228149414
Batch 41/64 loss: -0.7317581176757812
Batch 42/64 loss: -0.6938371658325195
Batch 43/64 loss: -0.24496173858642578
Batch 44/64 loss: -0.9810104370117188
Batch 45/64 loss: -0.6065673828125
Batch 46/64 loss: -1.187148094177246
Batch 47/64 loss: -0.9871311187744141
Batch 48/64 loss: -0.9779605865478516
Batch 49/64 loss: -0.7005853652954102
Batch 50/64 loss: -0.8785085678100586
Batch 51/64 loss: -0.7182512283325195
Batch 52/64 loss: -1.0811071395874023
Batch 53/64 loss: -0.5557222366333008
Batch 54/64 loss: -0.9048852920532227
Batch 55/64 loss: -0.8221597671508789
Batch 56/64 loss: -0.7599172592163086
Batch 57/64 loss: -0.3310384750366211
Batch 58/64 loss: -0.6732540130615234
Batch 59/64 loss: -0.9923648834228516
Batch 60/64 loss: -0.8425331115722656
Batch 61/64 loss: -0.8620681762695312
Batch 62/64 loss: -1.0648279190063477
Batch 63/64 loss: -0.5868463516235352
Batch 64/64 loss: -5.005087375640869
Epoch 416  Train loss: -0.8504514488519407  Val loss: -0.8815311418775841
Epoch 417
-------------------------------
Batch 1/64 loss: -0.9231576919555664
Batch 2/64 loss: -0.9272060394287109
Batch 3/64 loss: -0.8184633255004883
Batch 4/64 loss: -0.8093910217285156
Batch 5/64 loss: -0.7793045043945312
Batch 6/64 loss: -0.8899593353271484
Batch 7/64 loss: -1.247060775756836
Batch 8/64 loss: -0.9737434387207031
Batch 9/64 loss: -0.8510522842407227
Batch 10/64 loss: -0.6464776992797852
Batch 11/64 loss: -0.40735340118408203
Batch 12/64 loss: -0.5257329940795898
Batch 13/64 loss: -0.49272727966308594
Batch 14/64 loss: -0.8858776092529297
Batch 15/64 loss: -0.9394521713256836
Batch 16/64 loss: -0.8123559951782227
Batch 17/64 loss: -0.035366058349609375
Batch 18/64 loss: -0.22302913665771484
Batch 19/64 loss: -1.2627859115600586
Batch 20/64 loss: -0.45311641693115234
Batch 21/64 loss: -1.0274009704589844
Batch 22/64 loss: -0.7514219284057617
Batch 23/64 loss: -0.7996463775634766
Batch 24/64 loss: -0.7963323593139648
Batch 25/64 loss: -0.8865127563476562
Batch 26/64 loss: 0.20159053802490234
Batch 27/64 loss: -0.9803791046142578
Batch 28/64 loss: -0.7145233154296875
Batch 29/64 loss: -0.5573625564575195
Batch 30/64 loss: -0.8605947494506836
Batch 31/64 loss: -0.7346553802490234
Batch 32/64 loss: -0.38918113708496094
Batch 33/64 loss: -0.37013816833496094
Batch 34/64 loss: -0.33104801177978516
Batch 35/64 loss: -0.9239826202392578
Batch 36/64 loss: -0.8520774841308594
Batch 37/64 loss: -0.9162988662719727
Batch 38/64 loss: -0.4309706687927246
Batch 39/64 loss: -0.9327058792114258
Batch 40/64 loss: -0.6243257522583008
Batch 41/64 loss: -0.6784572601318359
Batch 42/64 loss: -1.026627540588379
Batch 43/64 loss: -0.9572782516479492
Batch 44/64 loss: -0.9475631713867188
Batch 45/64 loss: -0.6815195083618164
Batch 46/64 loss: -0.8927412033081055
Batch 47/64 loss: -0.6868200302124023
Batch 48/64 loss: -0.9828166961669922
Batch 49/64 loss: -0.820063591003418
Batch 50/64 loss: -0.9077844619750977
Batch 51/64 loss: -0.850067138671875
Batch 52/64 loss: -0.7790164947509766
Batch 53/64 loss: -0.8836164474487305
Batch 54/64 loss: -0.7532672882080078
Batch 55/64 loss: -0.7698221206665039
Batch 56/64 loss: -0.8727359771728516
Batch 57/64 loss: -0.5736055374145508
Batch 58/64 loss: -1.1603870391845703
Batch 59/64 loss: -0.5793170928955078
Batch 60/64 loss: -0.52252197265625
Batch 61/64 loss: -0.7537517547607422
Batch 62/64 loss: -0.8649930953979492
Batch 63/64 loss: -0.537653923034668
Batch 64/64 loss: -4.94464111328125
Epoch 417  Train loss: -0.7964312011120366  Val loss: -0.8669602436708012
Epoch 418
-------------------------------
Batch 1/64 loss: -0.9146051406860352
Batch 2/64 loss: -0.8159770965576172
Batch 3/64 loss: -0.8732328414916992
Batch 4/64 loss: -0.8778724670410156
Batch 5/64 loss: -0.6436119079589844
Batch 6/64 loss: -0.6826629638671875
Batch 7/64 loss: -0.39016246795654297
Batch 8/64 loss: -0.5892410278320312
Batch 9/64 loss: -0.5585851669311523
Batch 10/64 loss: -0.8370161056518555
Batch 11/64 loss: -0.7565298080444336
Batch 12/64 loss: -1.007974624633789
Batch 13/64 loss: -0.8362827301025391
Batch 14/64 loss: -1.0565261840820312
Batch 15/64 loss: -0.6079263687133789
Batch 16/64 loss: -0.9439496994018555
Batch 17/64 loss: -0.5840082168579102
Batch 18/64 loss: -0.1488513946533203
Batch 19/64 loss: -0.7947626113891602
Batch 20/64 loss: -0.942011833190918
Batch 21/64 loss: -0.9759244918823242
Batch 22/64 loss: -0.8921871185302734
Batch 23/64 loss: -1.1619787216186523
Batch 24/64 loss: -0.5568151473999023
Batch 25/64 loss: -1.0659008026123047
Batch 26/64 loss: -0.6973772048950195
Batch 27/64 loss: -0.6315460205078125
Batch 28/64 loss: -0.7130146026611328
Batch 29/64 loss: -0.7632312774658203
Batch 30/64 loss: -1.0560407638549805
Batch 31/64 loss: -0.13519859313964844
Batch 32/64 loss: -0.5572776794433594
Batch 33/64 loss: -0.8058052062988281
Batch 34/64 loss: -0.7885494232177734
Batch 35/64 loss: -0.44664764404296875
Batch 36/64 loss: -1.0997638702392578
Batch 37/64 loss: -0.8074131011962891
Batch 38/64 loss: -0.3916492462158203
Batch 39/64 loss: -0.6814289093017578
Batch 40/64 loss: -0.3714027404785156
Batch 41/64 loss: -1.0666351318359375
Batch 42/64 loss: -1.0242853164672852
Batch 43/64 loss: -0.579411506652832
Batch 44/64 loss: -0.7160482406616211
Batch 45/64 loss: -1.0696535110473633
Batch 46/64 loss: -0.5401105880737305
Batch 47/64 loss: -0.44744300842285156
Batch 48/64 loss: -0.7206554412841797
Batch 49/64 loss: -0.9319620132446289
Batch 50/64 loss: -0.8127536773681641
Batch 51/64 loss: -0.6672000885009766
Batch 52/64 loss: -0.2532777786254883
Batch 53/64 loss: -0.8184881210327148
Batch 54/64 loss: -0.5964574813842773
Batch 55/64 loss: -0.5406866073608398
Batch 56/64 loss: -0.8319950103759766
Batch 57/64 loss: -0.6456079483032227
Batch 58/64 loss: -0.8115100860595703
Batch 59/64 loss: -0.4707307815551758
Batch 60/64 loss: -0.7970046997070312
Batch 61/64 loss: -0.9265832901000977
Batch 62/64 loss: -0.5970859527587891
Batch 63/64 loss: -1.0379142761230469
Batch 64/64 loss: -4.866932392120361
Epoch 418  Train loss: -0.784543420754227  Val loss: -0.8624700172660277
Epoch 419
-------------------------------
Batch 1/64 loss: -1.0647974014282227
Batch 2/64 loss: -0.41141319274902344
Batch 3/64 loss: -1.007659912109375
Batch 4/64 loss: -0.43518924713134766
Batch 5/64 loss: -0.9650030136108398
Batch 6/64 loss: -1.0701217651367188
Batch 7/64 loss: -0.2335500717163086
Batch 8/64 loss: -0.5345172882080078
Batch 9/64 loss: -0.7552900314331055
Batch 10/64 loss: -0.9390573501586914
Batch 11/64 loss: -0.9329166412353516
Batch 12/64 loss: -0.8524599075317383
Batch 13/64 loss: -0.9402999877929688
Batch 14/64 loss: -0.44142818450927734
Batch 15/64 loss: -0.3171072006225586
Batch 16/64 loss: -0.6276016235351562
Batch 17/64 loss: -0.8747959136962891
Batch 18/64 loss: -0.7150373458862305
Batch 19/64 loss: -0.9580135345458984
Batch 20/64 loss: -0.9755229949951172
Batch 21/64 loss: -0.6318740844726562
Batch 22/64 loss: -0.7964611053466797
Batch 23/64 loss: -0.8333091735839844
Batch 24/64 loss: -0.7997665405273438
Batch 25/64 loss: -0.8755464553833008
Batch 26/64 loss: -0.5898141860961914
Batch 27/64 loss: -0.9504919052124023
Batch 28/64 loss: -0.3441333770751953
Batch 29/64 loss: -0.3829841613769531
Batch 30/64 loss: -0.5870857238769531
Batch 31/64 loss: -0.3630504608154297
Batch 32/64 loss: -1.169931411743164
Batch 33/64 loss: -0.7631063461303711
Batch 34/64 loss: -1.0000066757202148
Batch 35/64 loss: -0.9881725311279297
Batch 36/64 loss: -1.0335350036621094
Batch 37/64 loss: -0.6255559921264648
Batch 38/64 loss: -0.624730110168457
Batch 39/64 loss: -0.8194856643676758
Batch 40/64 loss: -0.7645912170410156
Batch 41/64 loss: -1.0941543579101562
Batch 42/64 loss: -0.7789144515991211
Batch 43/64 loss: -0.8804483413696289
Batch 44/64 loss: -1.176534652709961
Batch 45/64 loss: -1.0710725784301758
Batch 46/64 loss: -0.8625898361206055
Batch 47/64 loss: -0.18741989135742188
Batch 48/64 loss: -0.8172492980957031
Batch 49/64 loss: -0.5214061737060547
Batch 50/64 loss: -0.8968315124511719
Batch 51/64 loss: -0.5723991394042969
Batch 52/64 loss: -0.703608512878418
Batch 53/64 loss: -1.2356319427490234
Batch 54/64 loss: -0.8695287704467773
Batch 55/64 loss: -0.19136428833007812
Batch 56/64 loss: -0.9516572952270508
Batch 57/64 loss: -0.7009468078613281
Batch 58/64 loss: -1.0485000610351562
Batch 59/64 loss: -1.1003131866455078
Batch 60/64 loss: -0.8248119354248047
Batch 61/64 loss: -0.6980791091918945
Batch 62/64 loss: -0.4198446273803711
Batch 63/64 loss: -0.6980056762695312
Batch 64/64 loss: -4.313436985015869
Epoch 419  Train loss: -0.8083420379489076  Val loss: -0.783898422398518
Epoch 420
-------------------------------
Batch 1/64 loss: -0.9838666915893555
Batch 2/64 loss: -1.1667594909667969
Batch 3/64 loss: -0.7657690048217773
Batch 4/64 loss: -0.5008974075317383
Batch 5/64 loss: -0.24801921844482422
Batch 6/64 loss: -0.7476596832275391
Batch 7/64 loss: -0.8655834197998047
Batch 8/64 loss: -0.9926223754882812
Batch 9/64 loss: -0.8365335464477539
Batch 10/64 loss: -0.6691141128540039
Batch 11/64 loss: -0.4702568054199219
Batch 12/64 loss: -0.7916240692138672
Batch 13/64 loss: -0.7854108810424805
Batch 14/64 loss: -0.6781406402587891
Batch 15/64 loss: -0.7389631271362305
Batch 16/64 loss: -0.7380056381225586
Batch 17/64 loss: -0.7888803482055664
Batch 18/64 loss: -0.6785554885864258
Batch 19/64 loss: -0.6623029708862305
Batch 20/64 loss: -0.6362009048461914
Batch 21/64 loss: -1.0777969360351562
Batch 22/64 loss: -0.7598743438720703
Batch 23/64 loss: -0.643890380859375
Batch 24/64 loss: -0.964116096496582
Batch 25/64 loss: -0.8637313842773438
Batch 26/64 loss: -0.4717874526977539
Batch 27/64 loss: -0.8611516952514648
Batch 28/64 loss: -0.9783077239990234
Batch 29/64 loss: -0.9415426254272461
Batch 30/64 loss: -0.504730224609375
Batch 31/64 loss: -0.8627071380615234
Batch 32/64 loss: -1.1059417724609375
Batch 33/64 loss: -0.8023490905761719
Batch 34/64 loss: -1.0774688720703125
Batch 35/64 loss: -0.8053855895996094
Batch 36/64 loss: -1.067403793334961
Batch 37/64 loss: -1.0697851181030273
Batch 38/64 loss: -0.9513111114501953
Batch 39/64 loss: -0.909541130065918
Batch 40/64 loss: -1.2180299758911133
Batch 41/64 loss: -0.6205148696899414
Batch 42/64 loss: -0.8952522277832031
Batch 43/64 loss: -0.14398193359375
Batch 44/64 loss: -0.9093341827392578
Batch 45/64 loss: -0.5232839584350586
Batch 46/64 loss: -0.7251548767089844
Batch 47/64 loss: -1.1333389282226562
Batch 48/64 loss: -0.5807027816772461
Batch 49/64 loss: -0.9224977493286133
Batch 50/64 loss: -0.8627748489379883
Batch 51/64 loss: -0.9073810577392578
Batch 52/64 loss: -0.8764810562133789
Batch 53/64 loss: -0.8880586624145508
Batch 54/64 loss: -0.9603910446166992
Batch 55/64 loss: -0.9563369750976562
Batch 56/64 loss: -1.1246814727783203
Batch 57/64 loss: -0.6029291152954102
Batch 58/64 loss: -0.500950813293457
Batch 59/64 loss: -0.3668069839477539
Batch 60/64 loss: -1.0805988311767578
Batch 61/64 loss: -0.7617454528808594
Batch 62/64 loss: -0.2481098175048828
Batch 63/64 loss: -0.3017854690551758
Batch 64/64 loss: -4.800553321838379
Epoch 420  Train loss: -0.834125904008454  Val loss: -0.778054030900149
Epoch 421
-------------------------------
Batch 1/64 loss: -0.28525638580322266
Batch 2/64 loss: -0.7053327560424805
Batch 3/64 loss: -0.7920703887939453
Batch 4/64 loss: -0.47756481170654297
Batch 5/64 loss: -0.563079833984375
Batch 6/64 loss: -1.036421775817871
Batch 7/64 loss: -0.6603794097900391
Batch 8/64 loss: -0.3379793167114258
Batch 9/64 loss: -0.5194482803344727
Batch 10/64 loss: -0.8827333450317383
Batch 11/64 loss: -0.356414794921875
Batch 12/64 loss: -0.9766435623168945
Batch 13/64 loss: -0.7973031997680664
Batch 14/64 loss: -0.5413131713867188
Batch 15/64 loss: -0.9985504150390625
Batch 16/64 loss: -0.6677942276000977
Batch 17/64 loss: -0.5480775833129883
Batch 18/64 loss: -0.8794765472412109
Batch 19/64 loss: -0.4819221496582031
Batch 20/64 loss: -0.8553991317749023
Batch 21/64 loss: -0.1218557357788086
Batch 22/64 loss: -0.8893241882324219
Batch 23/64 loss: -0.8520917892456055
Batch 24/64 loss: -0.9831180572509766
Batch 25/64 loss: -0.4691810607910156
Batch 26/64 loss: -0.963953971862793
Batch 27/64 loss: -0.4936485290527344
Batch 28/64 loss: -0.7094411849975586
Batch 29/64 loss: -0.8883657455444336
Batch 30/64 loss: -0.5036125183105469
Batch 31/64 loss: -0.9794034957885742
Batch 32/64 loss: -0.9017324447631836
Batch 33/64 loss: -0.616459846496582
Batch 34/64 loss: -0.8365335464477539
Batch 35/64 loss: -0.2338094711303711
Batch 36/64 loss: -0.7466583251953125
Batch 37/64 loss: -0.8464250564575195
Batch 38/64 loss: -0.8738012313842773
Batch 39/64 loss: -0.9792613983154297
Batch 40/64 loss: -0.735504150390625
Batch 41/64 loss: -0.9459590911865234
Batch 42/64 loss: -0.6157588958740234
Batch 43/64 loss: -0.9377937316894531
Batch 44/64 loss: -1.0194892883300781
Batch 45/64 loss: -0.6920232772827148
Batch 46/64 loss: -0.7513523101806641
Batch 47/64 loss: -0.8203306198120117
Batch 48/64 loss: -0.8736085891723633
Batch 49/64 loss: -0.7200746536254883
Batch 50/64 loss: -0.9471588134765625
Batch 51/64 loss: -0.8697853088378906
Batch 52/64 loss: -1.0144329071044922
Batch 53/64 loss: -1.0441856384277344
Batch 54/64 loss: -0.9682846069335938
Batch 55/64 loss: -0.7304782867431641
Batch 56/64 loss: -0.9314641952514648
Batch 57/64 loss: -0.7935380935668945
Batch 58/64 loss: -0.8180379867553711
Batch 59/64 loss: -0.941009521484375
Batch 60/64 loss: -0.7346830368041992
Batch 61/64 loss: -0.4803481101989746
Batch 62/64 loss: -0.5604047775268555
Batch 63/64 loss: -0.8459711074829102
Batch 64/64 loss: -4.07222843170166
Epoch 421  Train loss: -0.7858460706823013  Val loss: -0.78619068840525
Epoch 422
-------------------------------
Batch 1/64 loss: -0.9111995697021484
Batch 2/64 loss: -0.9955072402954102
Batch 3/64 loss: -0.6530065536499023
Batch 4/64 loss: -0.8051767349243164
Batch 5/64 loss: -0.9445953369140625
Batch 6/64 loss: -0.7779731750488281
Batch 7/64 loss: -0.9291305541992188
Batch 8/64 loss: -0.9419069290161133
Batch 9/64 loss: -0.6370677947998047
Batch 10/64 loss: -0.7733669281005859
Batch 11/64 loss: -0.6606254577636719
Batch 12/64 loss: -0.27352285385131836
Batch 13/64 loss: -0.30339813232421875
Batch 14/64 loss: -0.1643209457397461
Batch 15/64 loss: -0.6715927124023438
Batch 16/64 loss: -0.9946670532226562
Batch 17/64 loss: -0.9539966583251953
Batch 18/64 loss: -0.4340400695800781
Batch 19/64 loss: -1.0961236953735352
Batch 20/64 loss: -0.7061252593994141
Batch 21/64 loss: -0.7886486053466797
Batch 22/64 loss: -0.23044490814208984
Batch 23/64 loss: -1.1730899810791016
Batch 24/64 loss: -0.9055023193359375
Batch 25/64 loss: -1.1454811096191406
Batch 26/64 loss: -0.9605255126953125
Batch 27/64 loss: -1.0816717147827148
Batch 28/64 loss: -0.5776472091674805
Batch 29/64 loss: -0.9292268753051758
Batch 30/64 loss: -0.5703315734863281
Batch 31/64 loss: -0.6250343322753906
Batch 32/64 loss: -0.5579929351806641
Batch 33/64 loss: -0.9719276428222656
Batch 34/64 loss: -0.6546010971069336
Batch 35/64 loss: -0.7770795822143555
Batch 36/64 loss: -0.8765983581542969
Batch 37/64 loss: -0.6202468872070312
Batch 38/64 loss: -0.988133430480957
Batch 39/64 loss: -0.899357795715332
Batch 40/64 loss: -0.9958152770996094
Batch 41/64 loss: -0.8732109069824219
Batch 42/64 loss: -1.0504770278930664
Batch 43/64 loss: -0.9407100677490234
Batch 44/64 loss: -0.8348188400268555
Batch 45/64 loss: -0.4813041687011719
Batch 46/64 loss: -0.7326869964599609
Batch 47/64 loss: -0.9398088455200195
Batch 48/64 loss: -0.38428783416748047
Batch 49/64 loss: -0.6801414489746094
Batch 50/64 loss: -0.895228385925293
Batch 51/64 loss: -0.8662290573120117
Batch 52/64 loss: -0.8270664215087891
Batch 53/64 loss: -0.7284679412841797
Batch 54/64 loss: -0.6776704788208008
Batch 55/64 loss: -0.6941938400268555
Batch 56/64 loss: -0.5403680801391602
Batch 57/64 loss: -0.5779047012329102
Batch 58/64 loss: -0.6836919784545898
Batch 59/64 loss: -0.7921838760375977
Batch 60/64 loss: -0.4160480499267578
Batch 61/64 loss: -0.4903135299682617
Batch 62/64 loss: -0.3673229217529297
Batch 63/64 loss: -0.8870439529418945
Batch 64/64 loss: -4.346495628356934
Epoch 422  Train loss: -0.7933764999988032  Val loss: -0.6304966641455582
Epoch 423
-------------------------------
Batch 1/64 loss: -0.6578550338745117
Batch 2/64 loss: -0.6058731079101562
Batch 3/64 loss: -0.3977527618408203
Batch 4/64 loss: -0.6166858673095703
Batch 5/64 loss: -0.3741769790649414
Batch 6/64 loss: -0.8010873794555664
Batch 7/64 loss: -0.7038040161132812
Batch 8/64 loss: -0.9472789764404297
Batch 9/64 loss: -0.3890876770019531
Batch 10/64 loss: -0.5749034881591797
Batch 11/64 loss: -0.694122314453125
Batch 12/64 loss: -0.6392631530761719
Batch 13/64 loss: -0.40642547607421875
Batch 14/64 loss: -0.7536659240722656
Batch 15/64 loss: -0.8399372100830078
Batch 16/64 loss: -0.8366174697875977
Batch 17/64 loss: -0.9575777053833008
Batch 18/64 loss: -0.6364116668701172
Batch 19/64 loss: -0.45774269104003906
Batch 20/64 loss: -0.9399652481079102
Batch 21/64 loss: -0.8559665679931641
Batch 22/64 loss: -0.558985710144043
Batch 23/64 loss: -0.7189464569091797
Batch 24/64 loss: -0.5648422241210938
Batch 25/64 loss: -0.8226604461669922
Batch 26/64 loss: -1.1112003326416016
Batch 27/64 loss: -0.8577566146850586
Batch 28/64 loss: -0.7983283996582031
Batch 29/64 loss: -0.8443727493286133
Batch 30/64 loss: -0.5555973052978516
Batch 31/64 loss: -0.5683584213256836
Batch 32/64 loss: -0.9320955276489258
Batch 33/64 loss: -0.9668235778808594
Batch 34/64 loss: -1.0839338302612305
Batch 35/64 loss: -0.6124744415283203
Batch 36/64 loss: -0.5311260223388672
Batch 37/64 loss: -0.9297208786010742
Batch 38/64 loss: -0.8744421005249023
Batch 39/64 loss: -0.40219688415527344
Batch 40/64 loss: -0.712763786315918
Batch 41/64 loss: -0.5564308166503906
Batch 42/64 loss: -0.8550453186035156
Batch 43/64 loss: -0.9492511749267578
Batch 44/64 loss: -1.0808582305908203
Batch 45/64 loss: -0.9710836410522461
Batch 46/64 loss: -0.8429145812988281
Batch 47/64 loss: -0.4938077926635742
Batch 48/64 loss: -0.7081489562988281
Batch 49/64 loss: -0.8352622985839844
Batch 50/64 loss: -0.9161930084228516
Batch 51/64 loss: -0.7596521377563477
Batch 52/64 loss: -0.6706399917602539
Batch 53/64 loss: -0.8152132034301758
Batch 54/64 loss: -0.868433952331543
Batch 55/64 loss: -1.1062202453613281
Batch 56/64 loss: -1.0552797317504883
Batch 57/64 loss: -0.9983940124511719
Batch 58/64 loss: -0.6478691101074219
Batch 59/64 loss: -0.5760869979858398
Batch 60/64 loss: -0.6784591674804688
Batch 61/64 loss: -0.5725288391113281
Batch 62/64 loss: -0.7979001998901367
Batch 63/64 loss: -0.8754215240478516
Batch 64/64 loss: -4.594003677368164
Epoch 423  Train loss: -0.7938733194388595  Val loss: -0.911819562879215
Epoch 424
-------------------------------
Batch 1/64 loss: -0.4092826843261719
Batch 2/64 loss: -1.0582027435302734
Batch 3/64 loss: -0.9563236236572266
Batch 4/64 loss: -1.0087823867797852
Batch 5/64 loss: -0.9229469299316406
Batch 6/64 loss: -0.8153171539306641
Batch 7/64 loss: -0.5014467239379883
Batch 8/64 loss: -0.9095516204833984
Batch 9/64 loss: -0.653834342956543
Batch 10/64 loss: -1.1337785720825195
Batch 11/64 loss: -0.39353084564208984
Batch 12/64 loss: -1.0338754653930664
Batch 13/64 loss: -1.1831607818603516
Batch 14/64 loss: -0.4352750778198242
Batch 15/64 loss: -0.3276023864746094
Batch 16/64 loss: -1.0328435897827148
Batch 17/64 loss: -0.6679105758666992
Batch 18/64 loss: -0.8127651214599609
Batch 19/64 loss: -0.9771404266357422
Batch 20/64 loss: -0.20166873931884766
Batch 21/64 loss: -1.0054683685302734
Batch 22/64 loss: -0.7016191482543945
Batch 23/64 loss: -0.9255714416503906
Batch 24/64 loss: -0.4646739959716797
Batch 25/64 loss: -0.4747028350830078
Batch 26/64 loss: -0.9906215667724609
Batch 27/64 loss: -0.7354593276977539
Batch 28/64 loss: -0.7969760894775391
Batch 29/64 loss: -0.7439765930175781
Batch 30/64 loss: -0.06892585754394531
Batch 31/64 loss: -0.3800182342529297
Batch 32/64 loss: -0.9191684722900391
Batch 33/64 loss: -0.9774799346923828
Batch 34/64 loss: -1.0150012969970703
Batch 35/64 loss: -0.9988689422607422
Batch 36/64 loss: -0.8247470855712891
Batch 37/64 loss: -0.8773784637451172
Batch 38/64 loss: -0.887211799621582
Batch 39/64 loss: -0.9594345092773438
Batch 40/64 loss: -0.6281461715698242
Batch 41/64 loss: -0.3193826675415039
Batch 42/64 loss: -0.551971435546875
Batch 43/64 loss: -0.917119026184082
Batch 44/64 loss: -1.0093193054199219
Batch 45/64 loss: -0.8283786773681641
Batch 46/64 loss: -0.7026844024658203
Batch 47/64 loss: -0.9210529327392578
Batch 48/64 loss: -0.9929771423339844
Batch 49/64 loss: -0.25494384765625
Batch 50/64 loss: -1.063673973083496
Batch 51/64 loss: -0.9085502624511719
Batch 52/64 loss: -0.8210458755493164
Batch 53/64 loss: -0.9593706130981445
Batch 54/64 loss: -0.8459224700927734
Batch 55/64 loss: -0.7088384628295898
Batch 56/64 loss: -0.9114131927490234
Batch 57/64 loss: -1.0124015808105469
Batch 58/64 loss: -0.7203683853149414
Batch 59/64 loss: -0.620121955871582
Batch 60/64 loss: -0.8299407958984375
Batch 61/64 loss: -0.8301305770874023
Batch 62/64 loss: -0.7646389007568359
Batch 63/64 loss: -0.8620290756225586
Batch 64/64 loss: -4.138204574584961
Epoch 424  Train loss: -0.81993127710679  Val loss: -0.8817470065506873
Epoch 425
-------------------------------
Batch 1/64 loss: -0.6681003570556641
Batch 2/64 loss: -0.6037969589233398
Batch 3/64 loss: -1.0962276458740234
Batch 4/64 loss: -1.152364730834961
Batch 5/64 loss: -0.9773721694946289
Batch 6/64 loss: -1.0030317306518555
Batch 7/64 loss: -0.45386409759521484
Batch 8/64 loss: -0.6268196105957031
Batch 9/64 loss: -0.9497585296630859
Batch 10/64 loss: -0.7753744125366211
Batch 11/64 loss: -0.9751148223876953
Batch 12/64 loss: -0.7684135437011719
Batch 13/64 loss: -0.7574663162231445
Batch 14/64 loss: -1.0164880752563477
Batch 15/64 loss: -0.7738761901855469
Batch 16/64 loss: -0.7095727920532227
Batch 17/64 loss: -0.5478181838989258
Batch 18/64 loss: -0.8097944259643555
Batch 19/64 loss: -0.36521053314208984
Batch 20/64 loss: -0.9421300888061523
Batch 21/64 loss: -0.6662359237670898
Batch 22/64 loss: -0.5003747940063477
Batch 23/64 loss: -0.8471965789794922
Batch 24/64 loss: -0.3519554138183594
Batch 25/64 loss: -0.9080657958984375
Batch 26/64 loss: -0.7506484985351562
Batch 27/64 loss: -0.629547119140625
Batch 28/64 loss: -0.9564905166625977
Batch 29/64 loss: -0.7820606231689453
Batch 30/64 loss: -1.0647239685058594
Batch 31/64 loss: -0.9408130645751953
Batch 32/64 loss: -0.809056282043457
Batch 33/64 loss: -1.0222625732421875
Batch 34/64 loss: -0.5194149017333984
Batch 35/64 loss: -0.8271093368530273
Batch 36/64 loss: -0.7235403060913086
Batch 37/64 loss: -0.639805793762207
Batch 38/64 loss: -1.261016845703125
Batch 39/64 loss: -0.7322835922241211
Batch 40/64 loss: -1.070509910583496
Batch 41/64 loss: -0.83343505859375
Batch 42/64 loss: -0.5281047821044922
Batch 43/64 loss: -0.7722492218017578
Batch 44/64 loss: -0.5122404098510742
Batch 45/64 loss: -0.6789236068725586
Batch 46/64 loss: -1.0457630157470703
Batch 47/64 loss: -1.059652328491211
Batch 48/64 loss: -1.023153305053711
Batch 49/64 loss: -1.0905866622924805
Batch 50/64 loss: -0.11405563354492188
Batch 51/64 loss: -0.8613224029541016
Batch 52/64 loss: -0.38306617736816406
Batch 53/64 loss: -0.5771369934082031
Batch 54/64 loss: -1.1523475646972656
Batch 55/64 loss: -0.34204769134521484
Batch 56/64 loss: -0.6067752838134766
Batch 57/64 loss: -0.8449497222900391
Batch 58/64 loss: -0.7996387481689453
Batch 59/64 loss: -0.7392721176147461
Batch 60/64 loss: -1.0044078826904297
Batch 61/64 loss: -1.0418729782104492
Batch 62/64 loss: -1.05511474609375
Batch 63/64 loss: -0.46894359588623047
Batch 64/64 loss: -4.9809346199035645
Epoch 425  Train loss: -0.835238712909175  Val loss: -0.8073558610739168
Epoch 426
-------------------------------
Batch 1/64 loss: -0.3066892623901367
Batch 2/64 loss: -0.7649030685424805
Batch 3/64 loss: -0.7346553802490234
Batch 4/64 loss: -0.49469757080078125
Batch 5/64 loss: -1.0175037384033203
Batch 6/64 loss: -0.774174690246582
Batch 7/64 loss: -1.0538082122802734
Batch 8/64 loss: -0.9342069625854492
Batch 9/64 loss: -0.9864463806152344
Batch 10/64 loss: -1.0680208206176758
Batch 11/64 loss: -0.5180587768554688
Batch 12/64 loss: -1.010939598083496
Batch 13/64 loss: -0.8246955871582031
Batch 14/64 loss: -0.5283613204956055
Batch 15/64 loss: -0.7502908706665039
Batch 16/64 loss: -1.0272560119628906
Batch 17/64 loss: -0.8824787139892578
Batch 18/64 loss: -0.8662166595458984
Batch 19/64 loss: -0.8290596008300781
Batch 20/64 loss: -0.3978710174560547
Batch 21/64 loss: -0.8680295944213867
Batch 22/64 loss: -0.5281791687011719
Batch 23/64 loss: -0.7392644882202148
Batch 24/64 loss: -0.6311922073364258
Batch 25/64 loss: -0.9808816909790039
Batch 26/64 loss: -0.7781190872192383
Batch 27/64 loss: -1.096440315246582
Batch 28/64 loss: -0.6325607299804688
Batch 29/64 loss: -0.2702188491821289
Batch 30/64 loss: -0.7556781768798828
Batch 31/64 loss: -1.0423717498779297
Batch 32/64 loss: -0.3969144821166992
Batch 33/64 loss: -0.5290498733520508
Batch 34/64 loss: -0.9350776672363281
Batch 35/64 loss: -0.780146598815918
Batch 36/64 loss: -0.9915199279785156
Batch 37/64 loss: -1.034891128540039
Batch 38/64 loss: -0.9726266860961914
Batch 39/64 loss: -0.5109128952026367
Batch 40/64 loss: -0.4569873809814453
Batch 41/64 loss: -0.7741079330444336
Batch 42/64 loss: -0.7037277221679688
Batch 43/64 loss: -0.797114372253418
Batch 44/64 loss: -0.5251045227050781
Batch 45/64 loss: -0.9641132354736328
Batch 46/64 loss: -0.7270679473876953
Batch 47/64 loss: -0.942530632019043
Batch 48/64 loss: -0.3510456085205078
Batch 49/64 loss: -0.8795013427734375
Batch 50/64 loss: -0.8676280975341797
Batch 51/64 loss: -0.2729616165161133
Batch 52/64 loss: -0.8752164840698242
Batch 53/64 loss: -0.8680572509765625
Batch 54/64 loss: -0.3895540237426758
Batch 55/64 loss: -0.9119501113891602
Batch 56/64 loss: -0.4353036880493164
Batch 57/64 loss: -0.6363010406494141
Batch 58/64 loss: -0.8490114212036133
Batch 59/64 loss: -0.6491403579711914
Batch 60/64 loss: -1.0379467010498047
Batch 61/64 loss: -1.1555213928222656
Batch 62/64 loss: -0.5966997146606445
Batch 63/64 loss: -0.6868400573730469
Batch 64/64 loss: -4.838959217071533
Epoch 426  Train loss: -0.8035617510477702  Val loss: -0.8883111567022055
Epoch 427
-------------------------------
Batch 1/64 loss: -0.9966068267822266
Batch 2/64 loss: -1.02545166015625
Batch 3/64 loss: -1.0267105102539062
Batch 4/64 loss: -0.8546037673950195
Batch 5/64 loss: -0.9099760055541992
Batch 6/64 loss: -0.6843652725219727
Batch 7/64 loss: -0.7208461761474609
Batch 8/64 loss: -0.566899299621582
Batch 9/64 loss: -0.7079277038574219
Batch 10/64 loss: -0.9993648529052734
Batch 11/64 loss: -0.8713140487670898
Batch 12/64 loss: -0.4107246398925781
Batch 13/64 loss: -0.29790687561035156
Batch 14/64 loss: -1.213273048400879
Batch 15/64 loss: -0.9075136184692383
Batch 16/64 loss: -0.924046516418457
Batch 17/64 loss: -0.6683416366577148
Batch 18/64 loss: -0.7311286926269531
Batch 19/64 loss: -0.5460987091064453
Batch 20/64 loss: -0.44526004791259766
Batch 21/64 loss: -0.788823127746582
Batch 22/64 loss: -0.15555763244628906
Batch 23/64 loss: -0.8416843414306641
Batch 24/64 loss: -1.063058853149414
Batch 25/64 loss: -0.7624263763427734
Batch 26/64 loss: -0.47424888610839844
Batch 27/64 loss: -0.7984075546264648
Batch 28/64 loss: -0.5417680740356445
Batch 29/64 loss: -0.6448736190795898
Batch 30/64 loss: -0.5655612945556641
Batch 31/64 loss: -1.076873779296875
Batch 32/64 loss: -0.9104137420654297
Batch 33/64 loss: -0.47010231018066406
Batch 34/64 loss: -0.7495937347412109
Batch 35/64 loss: -0.4650917053222656
Batch 36/64 loss: -0.30258655548095703
Batch 37/64 loss: -0.982269287109375
Batch 38/64 loss: -0.5417156219482422
Batch 39/64 loss: -0.8779668807983398
Batch 40/64 loss: -0.8064689636230469
Batch 41/64 loss: -0.8222484588623047
Batch 42/64 loss: -0.8550834655761719
Batch 43/64 loss: -0.9607753753662109
Batch 44/64 loss: -1.249603271484375
Batch 45/64 loss: -0.9642667770385742
Batch 46/64 loss: -0.9836034774780273
Batch 47/64 loss: -0.8881263732910156
Batch 48/64 loss: -0.9946908950805664
Batch 49/64 loss: -0.6444520950317383
Batch 50/64 loss: -0.9123601913452148
Batch 51/64 loss: -0.8213205337524414
Batch 52/64 loss: -0.5546360015869141
Batch 53/64 loss: -0.7187232971191406
Batch 54/64 loss: -0.39269065856933594
Batch 55/64 loss: -1.1253480911254883
Batch 56/64 loss: -0.9806928634643555
Batch 57/64 loss: -0.922358512878418
Batch 58/64 loss: -1.1938495635986328
Batch 59/64 loss: -1.0259695053100586
Batch 60/64 loss: -0.5901756286621094
Batch 61/64 loss: -0.9133100509643555
Batch 62/64 loss: -0.827580451965332
Batch 63/64 loss: -1.2420063018798828
Batch 64/64 loss: -4.63789701461792
Epoch 427  Train loss: -0.8374924996319939  Val loss: -0.8820323354190158
Epoch 428
-------------------------------
Batch 1/64 loss: -0.8396396636962891
Batch 2/64 loss: -0.7385330200195312
Batch 3/64 loss: -0.7223911285400391
Batch 4/64 loss: -0.9894418716430664
Batch 5/64 loss: -0.9135904312133789
Batch 6/64 loss: -0.7953014373779297
Batch 7/64 loss: -1.032820701599121
Batch 8/64 loss: -0.6581754684448242
Batch 9/64 loss: -0.6805877685546875
Batch 10/64 loss: -0.7967033386230469
Batch 11/64 loss: -0.7538652420043945
Batch 12/64 loss: -0.8298368453979492
Batch 13/64 loss: -0.4963216781616211
Batch 14/64 loss: -0.6776266098022461
Batch 15/64 loss: -0.6015300750732422
Batch 16/64 loss: -0.9680862426757812
Batch 17/64 loss: -0.9492998123168945
Batch 18/64 loss: -0.5571470260620117
Batch 19/64 loss: -1.0642004013061523
Batch 20/64 loss: -0.879399299621582
Batch 21/64 loss: -0.9469127655029297
Batch 22/64 loss: -1.100067138671875
Batch 23/64 loss: -0.589482307434082
Batch 24/64 loss: -1.0774059295654297
Batch 25/64 loss: -0.40158748626708984
Batch 26/64 loss: -0.8836498260498047
Batch 27/64 loss: -0.5276498794555664
Batch 28/64 loss: -0.8560342788696289
Batch 29/64 loss: -0.4038105010986328
Batch 30/64 loss: -0.826716423034668
Batch 31/64 loss: -0.6807518005371094
Batch 32/64 loss: -0.7650089263916016
Batch 33/64 loss: -0.645045280456543
Batch 34/64 loss: -0.5842266082763672
Batch 35/64 loss: 0.07250785827636719
Batch 36/64 loss: -0.8249578475952148
Batch 37/64 loss: -0.33646106719970703
Batch 38/64 loss: -1.0748233795166016
Batch 39/64 loss: -0.5786151885986328
Batch 40/64 loss: -0.577580451965332
Batch 41/64 loss: -0.8887472152709961
Batch 42/64 loss: -1.0253410339355469
Batch 43/64 loss: -0.6651887893676758
Batch 44/64 loss: -0.8023862838745117
Batch 45/64 loss: -0.8989295959472656
Batch 46/64 loss: -0.6436672210693359
Batch 47/64 loss: -0.4870901107788086
Batch 48/64 loss: -0.6280889511108398
Batch 49/64 loss: -0.7327642440795898
Batch 50/64 loss: -0.4819498062133789
Batch 51/64 loss: -0.7858219146728516
Batch 52/64 loss: -0.9918546676635742
Batch 53/64 loss: -0.8114461898803711
Batch 54/64 loss: -0.529536247253418
Batch 55/64 loss: -0.9070663452148438
Batch 56/64 loss: -0.8496236801147461
Batch 57/64 loss: -0.6357822418212891
Batch 58/64 loss: -0.9858989715576172
Batch 59/64 loss: -0.8678035736083984
Batch 60/64 loss: -0.6367721557617188
Batch 61/64 loss: -0.5231704711914062
Batch 62/64 loss: -0.6562089920043945
Batch 63/64 loss: -1.0075769424438477
Batch 64/64 loss: -4.963504314422607
Epoch 428  Train loss: -0.7955783713097666  Val loss: -0.7589748421895135
Epoch 429
-------------------------------
Batch 1/64 loss: -0.5845184326171875
Batch 2/64 loss: -0.9083261489868164
Batch 3/64 loss: -0.608729362487793
Batch 4/64 loss: -0.7203607559204102
Batch 5/64 loss: -0.5533914566040039
Batch 6/64 loss: -0.6483297348022461
Batch 7/64 loss: -1.0613555908203125
Batch 8/64 loss: -0.9062652587890625
Batch 9/64 loss: -0.9523515701293945
Batch 10/64 loss: -0.8403444290161133
Batch 11/64 loss: -0.3507242202758789
Batch 12/64 loss: -0.6685247421264648
Batch 13/64 loss: -0.6061973571777344
Batch 14/64 loss: -1.0238494873046875
Batch 15/64 loss: -0.9770593643188477
Batch 16/64 loss: -0.6896657943725586
Batch 17/64 loss: -0.8546123504638672
Batch 18/64 loss: -0.4600372314453125
Batch 19/64 loss: -0.6148996353149414
Batch 20/64 loss: -0.5643672943115234
Batch 21/64 loss: -0.6549186706542969
Batch 22/64 loss: -0.8306465148925781
Batch 23/64 loss: -0.25426673889160156
Batch 24/64 loss: -0.9217300415039062
Batch 25/64 loss: -1.0554962158203125
Batch 26/64 loss: -0.6615734100341797
Batch 27/64 loss: -0.8121671676635742
Batch 28/64 loss: -0.8576574325561523
Batch 29/64 loss: -0.6338319778442383
Batch 30/64 loss: -0.836726188659668
Batch 31/64 loss: -0.7530536651611328
Batch 32/64 loss: -0.8433628082275391
Batch 33/64 loss: -0.3103961944580078
Batch 34/64 loss: -0.7632102966308594
Batch 35/64 loss: -0.5521831512451172
Batch 36/64 loss: -0.88775634765625
Batch 37/64 loss: -0.7194890975952148
Batch 38/64 loss: -1.1817874908447266
Batch 39/64 loss: -1.0528020858764648
Batch 40/64 loss: -0.8647346496582031
Batch 41/64 loss: -0.9370489120483398
Batch 42/64 loss: -0.937774658203125
Batch 43/64 loss: -0.7868442535400391
Batch 44/64 loss: -0.9910030364990234
Batch 45/64 loss: -0.7150964736938477
Batch 46/64 loss: -0.7336654663085938
Batch 47/64 loss: -0.2953462600708008
Batch 48/64 loss: -0.9189662933349609
Batch 49/64 loss: -0.8512783050537109
Batch 50/64 loss: -0.8195629119873047
Batch 51/64 loss: -0.7580089569091797
Batch 52/64 loss: -0.7369356155395508
Batch 53/64 loss: -0.831390380859375
Batch 54/64 loss: -0.8805637359619141
Batch 55/64 loss: -0.5644435882568359
Batch 56/64 loss: -0.7946605682373047
Batch 57/64 loss: -0.9198217391967773
Batch 58/64 loss: 0.0927591323852539
Batch 59/64 loss: -0.9389553070068359
Batch 60/64 loss: -0.7503852844238281
Batch 61/64 loss: -0.6952667236328125
Batch 62/64 loss: -0.5836420059204102
Batch 63/64 loss: -0.13795089721679688
Batch 64/64 loss: -3.9077558517456055
Epoch 429  Train loss: -0.7758175606821097  Val loss: -0.7860616310355589
Epoch 430
-------------------------------
Batch 1/64 loss: -0.90423583984375
Batch 2/64 loss: -1.144181251525879
Batch 3/64 loss: -0.4869670867919922
Batch 4/64 loss: -0.8789300918579102
Batch 5/64 loss: -0.9319620132446289
Batch 6/64 loss: -0.5842752456665039
Batch 7/64 loss: -0.6585445404052734
Batch 8/64 loss: -0.8243789672851562
Batch 9/64 loss: -0.7607088088989258
Batch 10/64 loss: -0.9081363677978516
Batch 11/64 loss: -1.0209169387817383
Batch 12/64 loss: -1.0362367630004883
Batch 13/64 loss: -1.0234031677246094
Batch 14/64 loss: -0.4465599060058594
Batch 15/64 loss: -0.9143657684326172
Batch 16/64 loss: -0.5424184799194336
Batch 17/64 loss: -0.9745445251464844
Batch 18/64 loss: -0.638519287109375
Batch 19/64 loss: -1.1022224426269531
Batch 20/64 loss: -0.8760824203491211
Batch 21/64 loss: -0.8269739151000977
Batch 22/64 loss: -0.9119710922241211
Batch 23/64 loss: -0.39690399169921875
Batch 24/64 loss: -0.5087041854858398
Batch 25/64 loss: -0.8466892242431641
Batch 26/64 loss: -0.9276494979858398
Batch 27/64 loss: -0.6575088500976562
Batch 28/64 loss: -0.8835506439208984
Batch 29/64 loss: -0.5718297958374023
Batch 30/64 loss: -0.5776271820068359
Batch 31/64 loss: -0.33561134338378906
Batch 32/64 loss: -0.5923175811767578
Batch 33/64 loss: -0.8058071136474609
Batch 34/64 loss: -0.5336217880249023
Batch 35/64 loss: -0.35074329376220703
Batch 36/64 loss: -0.8625764846801758
Batch 37/64 loss: -0.9285516738891602
Batch 38/64 loss: -1.0350818634033203
Batch 39/64 loss: -1.0872650146484375
Batch 40/64 loss: -0.6031150817871094
Batch 41/64 loss: -0.6601572036743164
Batch 42/64 loss: -0.5493297576904297
Batch 43/64 loss: -0.917841911315918
Batch 44/64 loss: -0.3217287063598633
Batch 45/64 loss: -0.8012876510620117
Batch 46/64 loss: -1.0810890197753906
Batch 47/64 loss: -0.8379411697387695
Batch 48/64 loss: -0.6145410537719727
Batch 49/64 loss: -0.7290105819702148
Batch 50/64 loss: -0.8704586029052734
Batch 51/64 loss: -1.001338005065918
Batch 52/64 loss: 0.23631906509399414
Batch 53/64 loss: -0.5012798309326172
Batch 54/64 loss: -0.6285467147827148
Batch 55/64 loss: -1.0566072463989258
Batch 56/64 loss: -0.9502058029174805
Batch 57/64 loss: -0.882868766784668
Batch 58/64 loss: -0.93121337890625
Batch 59/64 loss: -1.0532283782958984
Batch 60/64 loss: -0.6683216094970703
Batch 61/64 loss: -0.8367471694946289
Batch 62/64 loss: -0.9943714141845703
Batch 63/64 loss: -0.5831890106201172
Batch 64/64 loss: -5.229670524597168
Epoch 430  Train loss: -0.8166106093163584  Val loss: -0.9365353272952575
Epoch 431
-------------------------------
Batch 1/64 loss: -1.0693979263305664
Batch 2/64 loss: -0.9520606994628906
Batch 3/64 loss: -0.689061164855957
Batch 4/64 loss: -0.5653715133666992
Batch 5/64 loss: -0.9271030426025391
Batch 6/64 loss: -1.086583137512207
Batch 7/64 loss: -0.9083576202392578
Batch 8/64 loss: -0.7637100219726562
Batch 9/64 loss: -0.6947975158691406
Batch 10/64 loss: -0.8558740615844727
Batch 11/64 loss: -0.6197471618652344
Batch 12/64 loss: -0.8094673156738281
Batch 13/64 loss: -0.7192897796630859
Batch 14/64 loss: -1.0643320083618164
Batch 15/64 loss: -0.9357757568359375
Batch 16/64 loss: -0.6854534149169922
Batch 17/64 loss: -1.0193920135498047
Batch 18/64 loss: -0.8359298706054688
Batch 19/64 loss: -0.7656784057617188
Batch 20/64 loss: -0.9762401580810547
Batch 21/64 loss: -0.9100246429443359
Batch 22/64 loss: -0.6867742538452148
Batch 23/64 loss: -0.8496732711791992
Batch 24/64 loss: -1.136103630065918
Batch 25/64 loss: -0.04702329635620117
Batch 26/64 loss: -0.8690767288208008
Batch 27/64 loss: -0.4460124969482422
Batch 28/64 loss: -0.8542871475219727
Batch 29/64 loss: -0.6531715393066406
Batch 30/64 loss: -0.7410011291503906
Batch 31/64 loss: -0.8307552337646484
Batch 32/64 loss: -0.4755210876464844
Batch 33/64 loss: -0.746546745300293
Batch 34/64 loss: -0.9595251083374023
Batch 35/64 loss: 0.21464157104492188
Batch 36/64 loss: -0.5775880813598633
Batch 37/64 loss: -0.5097999572753906
Batch 38/64 loss: -0.7247638702392578
Batch 39/64 loss: -0.6376047134399414
Batch 40/64 loss: -0.1460127830505371
Batch 41/64 loss: -0.8205347061157227
Batch 42/64 loss: -0.8810234069824219
Batch 43/64 loss: -0.7086763381958008
Batch 44/64 loss: -0.805516242980957
Batch 45/64 loss: -0.7760095596313477
Batch 46/64 loss: -0.6436357498168945
Batch 47/64 loss: -0.930180549621582
Batch 48/64 loss: -0.7801027297973633
Batch 49/64 loss: -0.6719474792480469
Batch 50/64 loss: -0.6510486602783203
Batch 51/64 loss: -0.5949220657348633
Batch 52/64 loss: -0.9121065139770508
Batch 53/64 loss: -0.7864532470703125
Batch 54/64 loss: -0.6615505218505859
Batch 55/64 loss: -0.8174753189086914
Batch 56/64 loss: -0.959507942199707
Batch 57/64 loss: -1.0201234817504883
Batch 58/64 loss: -0.905517578125
Batch 59/64 loss: 0.2076869010925293
Batch 60/64 loss: -0.38121700286865234
Batch 61/64 loss: -0.5835065841674805
Batch 62/64 loss: -0.5132541656494141
Batch 63/64 loss: -1.0082225799560547
Batch 64/64 loss: -4.847054481506348
Epoch 431  Train loss: -0.7807118920718923  Val loss: -0.9101055184590447
Epoch 432
-------------------------------
Batch 1/64 loss: -0.9757318496704102
Batch 2/64 loss: -0.8357381820678711
Batch 3/64 loss: -0.9345188140869141
Batch 4/64 loss: -0.6785154342651367
Batch 5/64 loss: -0.9207239151000977
Batch 6/64 loss: -0.6032810211181641
Batch 7/64 loss: -0.8918561935424805
Batch 8/64 loss: -0.92095947265625
Batch 9/64 loss: -0.8275175094604492
Batch 10/64 loss: -0.6508960723876953
Batch 11/64 loss: -0.3403463363647461
Batch 12/64 loss: -0.7905340194702148
Batch 13/64 loss: -0.5404720306396484
Batch 14/64 loss: -1.045090675354004
Batch 15/64 loss: -0.8986377716064453
Batch 16/64 loss: -0.6336050033569336
Batch 17/64 loss: -0.8918542861938477
Batch 18/64 loss: -0.5744609832763672
Batch 19/64 loss: -0.9746637344360352
Batch 20/64 loss: -0.9312705993652344
Batch 21/64 loss: -0.7634983062744141
Batch 22/64 loss: -0.6994228363037109
Batch 23/64 loss: -0.7658786773681641
Batch 24/64 loss: -0.8751354217529297
Batch 25/64 loss: -0.7519044876098633
Batch 26/64 loss: -0.9372682571411133
Batch 27/64 loss: -0.5004005432128906
Batch 28/64 loss: -0.698577880859375
Batch 29/64 loss: -1.0096244812011719
Batch 30/64 loss: -0.3574185371398926
Batch 31/64 loss: -1.0002918243408203
Batch 32/64 loss: -0.5965385437011719
Batch 33/64 loss: -0.4673728942871094
Batch 34/64 loss: -0.6741046905517578
Batch 35/64 loss: -0.8583230972290039
Batch 36/64 loss: -0.6816024780273438
Batch 37/64 loss: -0.796238899230957
Batch 38/64 loss: -0.8997831344604492
Batch 39/64 loss: -0.9887189865112305
Batch 40/64 loss: -0.5871143341064453
Batch 41/64 loss: -0.7152194976806641
Batch 42/64 loss: -0.41628074645996094
Batch 43/64 loss: 0.3197021484375
Batch 44/64 loss: -1.0208930969238281
Batch 45/64 loss: -0.9489116668701172
Batch 46/64 loss: -0.6885309219360352
Batch 47/64 loss: -1.0626516342163086
Batch 48/64 loss: -0.9220876693725586
Batch 49/64 loss: -0.9402542114257812
Batch 50/64 loss: -0.9262418746948242
Batch 51/64 loss: -0.8919897079467773
Batch 52/64 loss: -0.9456577301025391
Batch 53/64 loss: -0.8165531158447266
Batch 54/64 loss: -0.9882707595825195
Batch 55/64 loss: -0.890904426574707
Batch 56/64 loss: -0.4798393249511719
Batch 57/64 loss: -0.8505687713623047
Batch 58/64 loss: -0.8602666854858398
Batch 59/64 loss: -0.6287870407104492
Batch 60/64 loss: -0.46135759353637695
Batch 61/64 loss: -0.9099750518798828
Batch 62/64 loss: -1.0916776657104492
Batch 63/64 loss: -0.8973102569580078
Batch 64/64 loss: -3.9386038780212402
Epoch 432  Train loss: -0.8118960380554199  Val loss: -0.8852090475075843
Epoch 433
-------------------------------
Batch 1/64 loss: -0.9595184326171875
Batch 2/64 loss: -0.7515745162963867
Batch 3/64 loss: -0.8617219924926758
Batch 4/64 loss: -0.8106517791748047
Batch 5/64 loss: -0.9641189575195312
Batch 6/64 loss: -0.2518777847290039
Batch 7/64 loss: -0.8034505844116211
Batch 8/64 loss: -0.8472700119018555
Batch 9/64 loss: -0.921727180480957
Batch 10/64 loss: -0.7482261657714844
Batch 11/64 loss: -0.6108560562133789
Batch 12/64 loss: -0.8082084655761719
Batch 13/64 loss: -0.8607149124145508
Batch 14/64 loss: -0.7143869400024414
Batch 15/64 loss: -0.5238142013549805
Batch 16/64 loss: -0.7985734939575195
Batch 17/64 loss: -1.0970897674560547
Batch 18/64 loss: -0.9525604248046875
Batch 19/64 loss: -0.713078498840332
Batch 20/64 loss: -0.7133045196533203
Batch 21/64 loss: -0.8291444778442383
Batch 22/64 loss: -0.6671562194824219
Batch 23/64 loss: -1.0136070251464844
Batch 24/64 loss: -0.6200151443481445
Batch 25/64 loss: -0.6279230117797852
Batch 26/64 loss: -0.8668460845947266
Batch 27/64 loss: -0.6437673568725586
Batch 28/64 loss: -1.126692771911621
Batch 29/64 loss: -0.7578067779541016
Batch 30/64 loss: -0.3314175605773926
Batch 31/64 loss: -0.5050516128540039
Batch 32/64 loss: -0.7068462371826172
Batch 33/64 loss: -0.32709312438964844
Batch 34/64 loss: -0.339937686920166
Batch 35/64 loss: -0.9429225921630859
Batch 36/64 loss: -0.6406497955322266
Batch 37/64 loss: -0.6688137054443359
Batch 38/64 loss: -0.8680343627929688
Batch 39/64 loss: -0.7020940780639648
Batch 40/64 loss: -0.8923959732055664
Batch 41/64 loss: -1.0268926620483398
Batch 42/64 loss: -0.9183158874511719
Batch 43/64 loss: -1.019775390625
Batch 44/64 loss: -0.6660385131835938
Batch 45/64 loss: -0.8481588363647461
Batch 46/64 loss: -1.0484962463378906
Batch 47/64 loss: -0.7298679351806641
Batch 48/64 loss: -0.5818042755126953
Batch 49/64 loss: -0.6388397216796875
Batch 50/64 loss: -0.7570695877075195
Batch 51/64 loss: -0.7292823791503906
Batch 52/64 loss: -0.8268241882324219
Batch 53/64 loss: -0.8614864349365234
Batch 54/64 loss: -0.8836002349853516
Batch 55/64 loss: -0.38592004776000977
Batch 56/64 loss: -0.6745281219482422
Batch 57/64 loss: -0.5094451904296875
Batch 58/64 loss: -0.5593252182006836
Batch 59/64 loss: -1.043558120727539
Batch 60/64 loss: -0.5748386383056641
Batch 61/64 loss: -0.33097314834594727
Batch 62/64 loss: -0.9518909454345703
Batch 63/64 loss: -0.6718978881835938
Batch 64/64 loss: -3.8531699180603027
Epoch 433  Train loss: -0.7830532915451948  Val loss: -0.9495813953098153
Epoch 434
-------------------------------
Batch 1/64 loss: -0.5577545166015625
Batch 2/64 loss: -0.6791114807128906
Batch 3/64 loss: -0.8470821380615234
Batch 4/64 loss: -1.2521934509277344
Batch 5/64 loss: -0.7906942367553711
Batch 6/64 loss: -1.0080585479736328
Batch 7/64 loss: -0.9469289779663086
Batch 8/64 loss: -0.8460378646850586
Batch 9/64 loss: -1.0180778503417969
Batch 10/64 loss: -0.6479768753051758
Batch 11/64 loss: -1.1476964950561523
Batch 12/64 loss: -0.7284107208251953
Batch 13/64 loss: -0.8127899169921875
Batch 14/64 loss: -1.0777626037597656
Batch 15/64 loss: -0.9379091262817383
Batch 16/64 loss: -0.7744359970092773
Batch 17/64 loss: -1.0947332382202148
Batch 18/64 loss: -1.1838159561157227
Batch 19/64 loss: -0.8087081909179688
Batch 20/64 loss: -0.8518714904785156
Batch 21/64 loss: -0.6294889450073242
Batch 22/64 loss: -0.7925968170166016
Batch 23/64 loss: -0.5796699523925781
Batch 24/64 loss: -0.8097457885742188
Batch 25/64 loss: -0.8885107040405273
Batch 26/64 loss: -0.8119354248046875
Batch 27/64 loss: -0.8806400299072266
Batch 28/64 loss: -0.6733894348144531
Batch 29/64 loss: -0.9850435256958008
Batch 30/64 loss: -1.0764837265014648
Batch 31/64 loss: -0.7624959945678711
Batch 32/64 loss: -0.7017488479614258
Batch 33/64 loss: -0.9513626098632812
Batch 34/64 loss: -0.8525857925415039
Batch 35/64 loss: -0.866826057434082
Batch 36/64 loss: -0.689788818359375
Batch 37/64 loss: -0.5130805969238281
Batch 38/64 loss: -0.8889923095703125
Batch 39/64 loss: -1.1439151763916016
Batch 40/64 loss: -0.21629047393798828
Batch 41/64 loss: -0.9104948043823242
Batch 42/64 loss: -0.8688297271728516
Batch 43/64 loss: -0.32618236541748047
Batch 44/64 loss: -0.7851476669311523
Batch 45/64 loss: -0.8886489868164062
Batch 46/64 loss: -0.254274845123291
Batch 47/64 loss: -0.6486911773681641
Batch 48/64 loss: -0.49903297424316406
Batch 49/64 loss: -0.8952169418334961
Batch 50/64 loss: -0.9867134094238281
Batch 51/64 loss: -1.2418365478515625
Batch 52/64 loss: -0.6083822250366211
Batch 53/64 loss: -0.8061237335205078
Batch 54/64 loss: -0.6462254524230957
Batch 55/64 loss: -0.9785289764404297
Batch 56/64 loss: -0.7446279525756836
Batch 57/64 loss: -0.9050092697143555
Batch 58/64 loss: -0.754826545715332
Batch 59/64 loss: -0.9556369781494141
Batch 60/64 loss: -0.7139921188354492
Batch 61/64 loss: -0.856358528137207
Batch 62/64 loss: -0.7383804321289062
Batch 63/64 loss: -0.8357124328613281
Batch 64/64 loss: -4.276584625244141
Epoch 434  Train loss: -0.8593404433306526  Val loss: -0.9158046761738885
Epoch 435
-------------------------------
Batch 1/64 loss: -0.8764944076538086
Batch 2/64 loss: -0.4950065612792969
Batch 3/64 loss: -0.6755704879760742
Batch 4/64 loss: -0.6919517517089844
Batch 5/64 loss: -0.14350605010986328
Batch 6/64 loss: -1.2757139205932617
Batch 7/64 loss: -0.6254229545593262
Batch 8/64 loss: -1.1643648147583008
Batch 9/64 loss: -0.8858880996704102
Batch 10/64 loss: -1.051065444946289
Batch 11/64 loss: -0.5178861618041992
Batch 12/64 loss: -0.9373464584350586
Batch 13/64 loss: -0.6599969863891602
Batch 14/64 loss: -0.7376613616943359
Batch 15/64 loss: -0.9987735748291016
Batch 16/64 loss: -1.0521440505981445
Batch 17/64 loss: -0.9075593948364258
Batch 18/64 loss: -0.7919902801513672
Batch 19/64 loss: -0.7910232543945312
Batch 20/64 loss: -1.0897626876831055
Batch 21/64 loss: -0.5480337142944336
Batch 22/64 loss: -0.499420166015625
Batch 23/64 loss: -0.8937740325927734
Batch 24/64 loss: -0.7825450897216797
Batch 25/64 loss: -0.9673519134521484
Batch 26/64 loss: -1.002089500427246
Batch 27/64 loss: -0.9201831817626953
Batch 28/64 loss: -0.7135915756225586
Batch 29/64 loss: -0.7958917617797852
Batch 30/64 loss: -0.4110870361328125
Batch 31/64 loss: -0.90582275390625
Batch 32/64 loss: -0.4549722671508789
Batch 33/64 loss: -0.7975444793701172
Batch 34/64 loss: -0.9231061935424805
Batch 35/64 loss: -0.9949769973754883
Batch 36/64 loss: -0.9271717071533203
Batch 37/64 loss: -1.0084304809570312
Batch 38/64 loss: -0.7422113418579102
Batch 39/64 loss: -0.5210838317871094
Batch 40/64 loss: -1.115565299987793
Batch 41/64 loss: -0.6169233322143555
Batch 42/64 loss: -0.8670215606689453
Batch 43/64 loss: -0.7508811950683594
Batch 44/64 loss: -0.9182825088500977
Batch 45/64 loss: -1.0080013275146484
Batch 46/64 loss: -0.5876388549804688
Batch 47/64 loss: -0.1324901580810547
Batch 48/64 loss: -0.9686698913574219
Batch 49/64 loss: -0.8903045654296875
Batch 50/64 loss: -0.8230428695678711
Batch 51/64 loss: -0.5229992866516113
Batch 52/64 loss: -0.9260969161987305
Batch 53/64 loss: -0.7605352401733398
Batch 54/64 loss: -0.8815326690673828
Batch 55/64 loss: -0.9252719879150391
Batch 56/64 loss: -0.9040536880493164
Batch 57/64 loss: -0.5419654846191406
Batch 58/64 loss: -0.32579612731933594
Batch 59/64 loss: -0.9112625122070312
Batch 60/64 loss: -0.7529125213623047
Batch 61/64 loss: -1.098001480102539
Batch 62/64 loss: -0.9560546875
Batch 63/64 loss: -0.6329269409179688
Batch 64/64 loss: -5.0218586921691895
Epoch 435  Train loss: -0.843341833002427  Val loss: -0.8906384628662949
Epoch 436
-------------------------------
Batch 1/64 loss: -0.7577381134033203
Batch 2/64 loss: -0.6625404357910156
Batch 3/64 loss: -1.052830696105957
Batch 4/64 loss: -0.9128713607788086
Batch 5/64 loss: -1.1114473342895508
Batch 6/64 loss: -1.059173583984375
Batch 7/64 loss: -0.6701316833496094
Batch 8/64 loss: -0.6056632995605469
Batch 9/64 loss: -0.7828226089477539
Batch 10/64 loss: -1.0051774978637695
Batch 11/64 loss: -1.1145009994506836
Batch 12/64 loss: -0.7161102294921875
Batch 13/64 loss: -0.7032947540283203
Batch 14/64 loss: -0.7426900863647461
Batch 15/64 loss: -0.6073408126831055
Batch 16/64 loss: -0.7847251892089844
Batch 17/64 loss: -0.9497413635253906
Batch 18/64 loss: -0.34918975830078125
Batch 19/64 loss: -1.0067596435546875
Batch 20/64 loss: -0.8273115158081055
Batch 21/64 loss: -0.6376895904541016
Batch 22/64 loss: -0.8224630355834961
Batch 23/64 loss: -0.676060676574707
Batch 24/64 loss: -0.8817768096923828
Batch 25/64 loss: -0.7684516906738281
Batch 26/64 loss: -0.5574550628662109
Batch 27/64 loss: -0.7259893417358398
Batch 28/64 loss: -0.9714212417602539
Batch 29/64 loss: -0.8749752044677734
Batch 30/64 loss: -0.583561897277832
Batch 31/64 loss: -0.6629562377929688
Batch 32/64 loss: -0.8622808456420898
Batch 33/64 loss: -0.9587249755859375
Batch 34/64 loss: -0.6384344100952148
Batch 35/64 loss: -0.8816242218017578
Batch 36/64 loss: -0.9064369201660156
Batch 37/64 loss: -0.7499351501464844
Batch 38/64 loss: -0.8568658828735352
Batch 39/64 loss: -1.144770622253418
Batch 40/64 loss: -0.7208642959594727
Batch 41/64 loss: -0.7746877670288086
Batch 42/64 loss: -0.6653995513916016
Batch 43/64 loss: -0.6325168609619141
Batch 44/64 loss: -0.9340581893920898
Batch 45/64 loss: -0.8865966796875
Batch 46/64 loss: -0.4713582992553711
Batch 47/64 loss: -1.050593376159668
Batch 48/64 loss: -0.7604475021362305
Batch 49/64 loss: -1.1055355072021484
Batch 50/64 loss: -1.3025827407836914
Batch 51/64 loss: -1.1302833557128906
Batch 52/64 loss: -0.9784450531005859
Batch 53/64 loss: -0.1933279037475586
Batch 54/64 loss: -1.0137567520141602
Batch 55/64 loss: -0.7197465896606445
Batch 56/64 loss: -0.8474454879760742
Batch 57/64 loss: -0.6671276092529297
Batch 58/64 loss: -1.0660629272460938
Batch 59/64 loss: -0.8376178741455078
Batch 60/64 loss: -0.8895339965820312
Batch 61/64 loss: -0.6832952499389648
Batch 62/64 loss: -0.9180030822753906
Batch 63/64 loss: -0.9901027679443359
Batch 64/64 loss: -4.861856460571289
Epoch 436  Train loss: -0.8701132007673675  Val loss: -0.8323728161579145
Epoch 437
-------------------------------
Batch 1/64 loss: -0.872899055480957
Batch 2/64 loss: -0.8124971389770508
Batch 3/64 loss: -0.8560323715209961
Batch 4/64 loss: -0.8472442626953125
Batch 5/64 loss: -0.8960647583007812
Batch 6/64 loss: -1.0742673873901367
Batch 7/64 loss: -0.8973426818847656
Batch 8/64 loss: -0.2722969055175781
Batch 9/64 loss: -0.9658308029174805
Batch 10/64 loss: -0.7981786727905273
Batch 11/64 loss: -0.7645139694213867
Batch 12/64 loss: -0.9085550308227539
Batch 13/64 loss: -0.7585248947143555
Batch 14/64 loss: -0.7851839065551758
Batch 15/64 loss: -1.1803874969482422
Batch 16/64 loss: -0.609919548034668
Batch 17/64 loss: -1.0733051300048828
Batch 18/64 loss: -0.8229665756225586
Batch 19/64 loss: -0.9442071914672852
Batch 20/64 loss: -0.7139272689819336
Batch 21/64 loss: -0.5842475891113281
Batch 22/64 loss: -1.0244550704956055
Batch 23/64 loss: -0.4806966781616211
Batch 24/64 loss: -0.89459228515625
Batch 25/64 loss: -1.1289796829223633
Batch 26/64 loss: -0.8534336090087891
Batch 27/64 loss: -0.8905849456787109
Batch 28/64 loss: -1.1975574493408203
Batch 29/64 loss: -0.8720607757568359
Batch 30/64 loss: -0.9347505569458008
Batch 31/64 loss: -0.8027591705322266
Batch 32/64 loss: -1.0508794784545898
Batch 33/64 loss: -0.8582830429077148
Batch 34/64 loss: -0.7297134399414062
Batch 35/64 loss: -1.0125980377197266
Batch 36/64 loss: -0.42318058013916016
Batch 37/64 loss: -0.9693365097045898
Batch 38/64 loss: -0.9922990798950195
Batch 39/64 loss: -0.5207185745239258
Batch 40/64 loss: -0.8356962203979492
Batch 41/64 loss: -0.8120813369750977
Batch 42/64 loss: -0.9494209289550781
Batch 43/64 loss: -0.2945528030395508
Batch 44/64 loss: -1.0628013610839844
Batch 45/64 loss: -0.8632450103759766
Batch 46/64 loss: -0.8394269943237305
Batch 47/64 loss: -1.0128803253173828
Batch 48/64 loss: -0.8561677932739258
Batch 49/64 loss: -1.1398096084594727
Batch 50/64 loss: -1.127115249633789
Batch 51/64 loss: -1.0597219467163086
Batch 52/64 loss: -0.9472637176513672
Batch 53/64 loss: -0.7565774917602539
Batch 54/64 loss: -1.0943880081176758
Batch 55/64 loss: -0.8967189788818359
Batch 56/64 loss: -0.7863121032714844
Batch 57/64 loss: -0.8821926116943359
Batch 58/64 loss: -0.37404918670654297
Batch 59/64 loss: -0.5299091339111328
Batch 60/64 loss: -0.9688119888305664
Batch 61/64 loss: -0.9918909072875977
Batch 62/64 loss: -0.9431676864624023
Batch 63/64 loss: -0.8935708999633789
Batch 64/64 loss: -3.8416571617126465
Epoch 437  Train loss: -0.8921456748364018  Val loss: -0.9390316337244617
Epoch 438
-------------------------------
Batch 1/64 loss: -1.1068172454833984
Batch 2/64 loss: -0.28464698791503906
Batch 3/64 loss: -0.802760124206543
Batch 4/64 loss: -1.1020574569702148
Batch 5/64 loss: -0.804631233215332
Batch 6/64 loss: -0.533139705657959
Batch 7/64 loss: -0.49463748931884766
Batch 8/64 loss: -0.4935283660888672
Batch 9/64 loss: -0.6349124908447266
Batch 10/64 loss: -1.3682007789611816
Batch 11/64 loss: -0.6189250946044922
Batch 12/64 loss: -1.066629409790039
Batch 13/64 loss: -1.0013322830200195
Batch 14/64 loss: -0.9959115982055664
Batch 15/64 loss: -0.6740102767944336
Batch 16/64 loss: -0.6561260223388672
Batch 17/64 loss: -1.1780166625976562
Batch 18/64 loss: -0.8326902389526367
Batch 19/64 loss: -0.873805046081543
Batch 20/64 loss: -0.4711952209472656
Batch 21/64 loss: -1.062443733215332
Batch 22/64 loss: -1.078714370727539
Batch 23/64 loss: -1.1200284957885742
Batch 24/64 loss: -0.6255559921264648
Batch 25/64 loss: -1.0442914962768555
Batch 26/64 loss: -0.8143329620361328
Batch 27/64 loss: -0.7905263900756836
Batch 28/64 loss: -1.0755300521850586
Batch 29/64 loss: -0.4449348449707031
Batch 30/64 loss: -0.6701154708862305
Batch 31/64 loss: 0.3340015411376953
Batch 32/64 loss: -0.8386163711547852
Batch 33/64 loss: -1.1811246871948242
Batch 34/64 loss: -1.064962387084961
Batch 35/64 loss: -0.8101768493652344
Batch 36/64 loss: -0.9290847778320312
Batch 37/64 loss: -0.9612245559692383
Batch 38/64 loss: -0.8628292083740234
Batch 39/64 loss: -0.9130105972290039
Batch 40/64 loss: -1.1123533248901367
Batch 41/64 loss: -0.5542221069335938
Batch 42/64 loss: 0.2980232238769531
Batch 43/64 loss: -0.8048038482666016
Batch 44/64 loss: -1.232588768005371
Batch 45/64 loss: -1.143601417541504
Batch 46/64 loss: -0.9095382690429688
Batch 47/64 loss: -1.057938575744629
Batch 48/64 loss: -0.7329540252685547
Batch 49/64 loss: -0.7998046875
Batch 50/64 loss: -0.3858804702758789
Batch 51/64 loss: -0.589167594909668
Batch 52/64 loss: -0.7509098052978516
Batch 53/64 loss: -0.5757923126220703
Batch 54/64 loss: -0.7844705581665039
Batch 55/64 loss: -0.9050750732421875
Batch 56/64 loss: -0.9158182144165039
Batch 57/64 loss: -0.7878026962280273
Batch 58/64 loss: -0.67083740234375
Batch 59/64 loss: -0.6805152893066406
Batch 60/64 loss: -0.5082864761352539
Batch 61/64 loss: -0.9723453521728516
Batch 62/64 loss: -0.9400119781494141
Batch 63/64 loss: -0.6743755340576172
Batch 64/64 loss: -4.860278129577637
Epoch 438  Train loss: -0.8436667386223288  Val loss: -0.9123476821532364
Epoch 439
-------------------------------
Batch 1/64 loss: -0.6830663681030273
Batch 2/64 loss: -0.8546791076660156
Batch 3/64 loss: -0.3708305358886719
Batch 4/64 loss: -0.9406051635742188
Batch 5/64 loss: -1.0321178436279297
Batch 6/64 loss: -0.41717958450317383
Batch 7/64 loss: -0.89495849609375
Batch 8/64 loss: -0.8079051971435547
Batch 9/64 loss: -0.7465295791625977
Batch 10/64 loss: -0.9810504913330078
Batch 11/64 loss: -0.8979053497314453
Batch 12/64 loss: -0.7935352325439453
Batch 13/64 loss: -1.1581287384033203
Batch 14/64 loss: -1.0884761810302734
Batch 15/64 loss: -0.9120121002197266
Batch 16/64 loss: -1.1482715606689453
Batch 17/64 loss: -0.6367654800415039
Batch 18/64 loss: -0.8526525497436523
Batch 19/64 loss: -0.8664283752441406
Batch 20/64 loss: -0.38791656494140625
Batch 21/64 loss: -1.060140609741211
Batch 22/64 loss: -0.6001830101013184
Batch 23/64 loss: -0.5868244171142578
Batch 24/64 loss: -1.0329217910766602
Batch 25/64 loss: -0.9093465805053711
Batch 26/64 loss: -0.6217679977416992
Batch 27/64 loss: -0.6260910034179688
Batch 28/64 loss: -0.6342945098876953
Batch 29/64 loss: -0.7093582153320312
Batch 30/64 loss: -0.9405784606933594
Batch 31/64 loss: -0.9057703018188477
Batch 32/64 loss: -0.1521005630493164
Batch 33/64 loss: -1.2026948928833008
Batch 34/64 loss: -0.838231086730957
Batch 35/64 loss: -1.188009262084961
Batch 36/64 loss: -1.128509521484375
Batch 37/64 loss: -0.7934846878051758
Batch 38/64 loss: -0.4871244430541992
Batch 39/64 loss: -0.847987174987793
Batch 40/64 loss: -0.506077766418457
Batch 41/64 loss: -0.5829267501831055
Batch 42/64 loss: -1.0675277709960938
Batch 43/64 loss: -0.843806266784668
Batch 44/64 loss: -0.994227409362793
Batch 45/64 loss: -0.9689903259277344
Batch 46/64 loss: -0.8921890258789062
Batch 47/64 loss: -0.17389583587646484
Batch 48/64 loss: -0.7627687454223633
Batch 49/64 loss: -1.0733213424682617
Batch 50/64 loss: -0.2111063003540039
Batch 51/64 loss: -0.700261116027832
Batch 52/64 loss: -1.202157974243164
Batch 53/64 loss: -0.8549184799194336
Batch 54/64 loss: -1.0440673828125
Batch 55/64 loss: -1.030318260192871
Batch 56/64 loss: -0.751103401184082
Batch 57/64 loss: -0.8274641036987305
Batch 58/64 loss: -0.7004823684692383
Batch 59/64 loss: -0.6163730621337891
Batch 60/64 loss: -0.9727277755737305
Batch 61/64 loss: -0.7371530532836914
Batch 62/64 loss: -0.8550119400024414
Batch 63/64 loss: -0.748967170715332
Batch 64/64 loss: -4.758560657501221
Epoch 439  Train loss: -0.8536972101996927  Val loss: -0.985790973676439
Saving best model, epoch: 439
Epoch 440
-------------------------------
Batch 1/64 loss: -0.8099899291992188
Batch 2/64 loss: -1.002293586730957
Batch 3/64 loss: -0.6243257522583008
Batch 4/64 loss: -0.8423986434936523
Batch 5/64 loss: -0.5459756851196289
Batch 6/64 loss: -0.7918853759765625
Batch 7/64 loss: -0.2302265167236328
Batch 8/64 loss: -0.12897396087646484
Batch 9/64 loss: -0.7225837707519531
Batch 10/64 loss: -0.9391069412231445
Batch 11/64 loss: -0.7184610366821289
Batch 12/64 loss: -0.6203508377075195
Batch 13/64 loss: -0.46705055236816406
Batch 14/64 loss: -1.2254114151000977
Batch 15/64 loss: -0.9331598281860352
Batch 16/64 loss: -0.9393749237060547
Batch 17/64 loss: -0.9011945724487305
Batch 18/64 loss: -0.888270378112793
Batch 19/64 loss: -1.123225212097168
Batch 20/64 loss: -0.7480201721191406
Batch 21/64 loss: -0.8118696212768555
Batch 22/64 loss: -0.7442808151245117
Batch 23/64 loss: -0.8520402908325195
Batch 24/64 loss: -1.183614730834961
Batch 25/64 loss: -0.5576095581054688
Batch 26/64 loss: -1.049168586730957
Batch 27/64 loss: -0.3577394485473633
Batch 28/64 loss: -0.9876279830932617
Batch 29/64 loss: -1.0761499404907227
Batch 30/64 loss: -1.1321601867675781
Batch 31/64 loss: -0.6100606918334961
Batch 32/64 loss: -0.940007209777832
Batch 33/64 loss: -0.9599428176879883
Batch 34/64 loss: -0.8850412368774414
Batch 35/64 loss: -0.9177446365356445
Batch 36/64 loss: -0.8869724273681641
Batch 37/64 loss: -0.641880989074707
Batch 38/64 loss: -0.9974489212036133
Batch 39/64 loss: -0.9096755981445312
Batch 40/64 loss: -0.881622314453125
Batch 41/64 loss: -0.5966281890869141
Batch 42/64 loss: -0.6285867691040039
Batch 43/64 loss: -0.7579708099365234
Batch 44/64 loss: -0.8905849456787109
Batch 45/64 loss: -0.4531402587890625
Batch 46/64 loss: -1.036458969116211
Batch 47/64 loss: -0.9987726211547852
Batch 48/64 loss: -0.8899860382080078
Batch 49/64 loss: -0.7666254043579102
Batch 50/64 loss: -0.6621150970458984
Batch 51/64 loss: -0.5261764526367188
Batch 52/64 loss: -0.8587532043457031
Batch 53/64 loss: -0.4820442199707031
Batch 54/64 loss: -0.14962387084960938
Batch 55/64 loss: -0.7523345947265625
Batch 56/64 loss: -0.9737272262573242
Batch 57/64 loss: -0.7934045791625977
Batch 58/64 loss: -1.0734643936157227
Batch 59/64 loss: -0.2103252410888672
Batch 60/64 loss: -0.7125892639160156
Batch 61/64 loss: -1.0586328506469727
Batch 62/64 loss: -0.844487190246582
Batch 63/64 loss: -1.1891508102416992
Batch 64/64 loss: -4.240132808685303
Epoch 440  Train loss: -0.8324803090563008  Val loss: -0.8873468379384464
Epoch 441
-------------------------------
Batch 1/64 loss: -0.9028215408325195
Batch 2/64 loss: -0.6296854019165039
Batch 3/64 loss: -0.7220392227172852
Batch 4/64 loss: -0.7632894515991211
Batch 5/64 loss: -0.9204978942871094
Batch 6/64 loss: -0.7435379028320312
Batch 7/64 loss: -0.8506441116333008
Batch 8/64 loss: -0.598576545715332
Batch 9/64 loss: -0.8793497085571289
Batch 10/64 loss: -0.7993574142456055
Batch 11/64 loss: -0.9956521987915039
Batch 12/64 loss: -0.9685153961181641
Batch 13/64 loss: -0.28255748748779297
Batch 14/64 loss: -0.7457265853881836
Batch 15/64 loss: -0.9532918930053711
Batch 16/64 loss: -0.8957805633544922
Batch 17/64 loss: -0.6826601028442383
Batch 18/64 loss: -1.0615367889404297
Batch 19/64 loss: -0.9811182022094727
Batch 20/64 loss: -0.9394741058349609
Batch 21/64 loss: -0.8683042526245117
Batch 22/64 loss: -0.7560443878173828
Batch 23/64 loss: -0.6669692993164062
Batch 24/64 loss: -1.043107032775879
Batch 25/64 loss: -0.8043584823608398
Batch 26/64 loss: -0.868804931640625
Batch 27/64 loss: -0.8981437683105469
Batch 28/64 loss: -0.753997802734375
Batch 29/64 loss: -0.2791132926940918
Batch 30/64 loss: -0.854736328125
Batch 31/64 loss: -1.0234880447387695
Batch 32/64 loss: -0.8316335678100586
Batch 33/64 loss: -0.6215019226074219
Batch 34/64 loss: -1.0075654983520508
Batch 35/64 loss: -0.7479677200317383
Batch 36/64 loss: -0.9508209228515625
Batch 37/64 loss: -0.42875146865844727
Batch 38/64 loss: -0.6300134658813477
Batch 39/64 loss: -0.9161262512207031
Batch 40/64 loss: -0.9322071075439453
Batch 41/64 loss: -0.9066190719604492
Batch 42/64 loss: -0.916356086730957
Batch 43/64 loss: -0.6648883819580078
Batch 44/64 loss: -0.9134912490844727
Batch 45/64 loss: -1.1559991836547852
Batch 46/64 loss: -0.18102169036865234
Batch 47/64 loss: -0.7994241714477539
Batch 48/64 loss: -1.0506868362426758
Batch 49/64 loss: -0.6933393478393555
Batch 50/64 loss: -0.6459112167358398
Batch 51/64 loss: -0.6545028686523438
Batch 52/64 loss: -1.062413215637207
Batch 53/64 loss: -1.0105390548706055
Batch 54/64 loss: -1.0133428573608398
Batch 55/64 loss: -0.7555866241455078
Batch 56/64 loss: -1.038588523864746
Batch 57/64 loss: -0.5189332962036133
Batch 58/64 loss: -0.47611045837402344
Batch 59/64 loss: -0.809234619140625
Batch 60/64 loss: -1.031571388244629
Batch 61/64 loss: -0.4270801544189453
Batch 62/64 loss: -0.6916904449462891
Batch 63/64 loss: -0.8500404357910156
Batch 64/64 loss: -4.608292102813721
Epoch 441  Train loss: -0.845856601116704  Val loss: -0.8261521591763317
Epoch 442
-------------------------------
Batch 1/64 loss: -0.7687263488769531
Batch 2/64 loss: -0.7004270553588867
Batch 3/64 loss: -0.6528768539428711
Batch 4/64 loss: -0.617396354675293
Batch 5/64 loss: -0.6100845336914062
Batch 6/64 loss: -0.24214649200439453
Batch 7/64 loss: -0.4115610122680664
Batch 8/64 loss: -0.8860549926757812
Batch 9/64 loss: -1.055851936340332
Batch 10/64 loss: -0.6308927536010742
Batch 11/64 loss: -0.8887004852294922
Batch 12/64 loss: -1.0456047058105469
Batch 13/64 loss: -0.29807567596435547
Batch 14/64 loss: -0.8822221755981445
Batch 15/64 loss: -0.5114774703979492
Batch 16/64 loss: -0.7596826553344727
Batch 17/64 loss: -0.8924493789672852
Batch 18/64 loss: -1.005497932434082
Batch 19/64 loss: -0.753143310546875
Batch 20/64 loss: -0.9306468963623047
Batch 21/64 loss: -0.7852954864501953
Batch 22/64 loss: -0.6102113723754883
Batch 23/64 loss: -0.6919851303100586
Batch 24/64 loss: -0.7840442657470703
Batch 25/64 loss: -0.9734039306640625
Batch 26/64 loss: -0.8100013732910156
Batch 27/64 loss: -0.7277441024780273
Batch 28/64 loss: -0.8074226379394531
Batch 29/64 loss: -1.020761489868164
Batch 30/64 loss: -1.0597000122070312
Batch 31/64 loss: -1.0673017501831055
Batch 32/64 loss: -0.9206562042236328
Batch 33/64 loss: -0.7020683288574219
Batch 34/64 loss: -0.5725536346435547
Batch 35/64 loss: -0.3667945861816406
Batch 36/64 loss: -0.5156517028808594
Batch 37/64 loss: -0.8859367370605469
Batch 38/64 loss: -0.841944694519043
Batch 39/64 loss: -0.9093589782714844
Batch 40/64 loss: -1.141937255859375
Batch 41/64 loss: -0.8275051116943359
Batch 42/64 loss: -1.0297765731811523
Batch 43/64 loss: -0.9305152893066406
Batch 44/64 loss: -1.238016128540039
Batch 45/64 loss: -1.0937719345092773
Batch 46/64 loss: -0.892613410949707
Batch 47/64 loss: -1.1336545944213867
Batch 48/64 loss: -0.9135007858276367
Batch 49/64 loss: -1.0263853073120117
Batch 50/64 loss: -1.1631097793579102
Batch 51/64 loss: -0.72430419921875
Batch 52/64 loss: -1.183736801147461
Batch 53/64 loss: -0.5119152069091797
Batch 54/64 loss: -0.7588129043579102
Batch 55/64 loss: -1.024505615234375
Batch 56/64 loss: -1.0627460479736328
Batch 57/64 loss: -0.6011343002319336
Batch 58/64 loss: -0.3275880813598633
Batch 59/64 loss: -0.9950542449951172
Batch 60/64 loss: -0.23453903198242188
Batch 61/64 loss: -0.9535026550292969
Batch 62/64 loss: -0.7311687469482422
Batch 63/64 loss: -0.5052309036254883
Batch 64/64 loss: -5.101752758026123
Epoch 442  Train loss: -0.8537677633996104  Val loss: -0.9254465267010981
Epoch 443
-------------------------------
Batch 1/64 loss: -0.7735795974731445
Batch 2/64 loss: -1.0028352737426758
Batch 3/64 loss: -0.917205810546875
Batch 4/64 loss: -0.2628498077392578
Batch 5/64 loss: -0.8624382019042969
Batch 6/64 loss: -0.7972164154052734
Batch 7/64 loss: -1.047384262084961
Batch 8/64 loss: -1.0507240295410156
Batch 9/64 loss: -0.708430290222168
Batch 10/64 loss: -1.2023468017578125
Batch 11/64 loss: -0.9881658554077148
Batch 12/64 loss: -0.8408441543579102
Batch 13/64 loss: -0.7158689498901367
Batch 14/64 loss: -0.5995512008666992
Batch 15/64 loss: -0.8674030303955078
Batch 16/64 loss: -0.8703861236572266
Batch 17/64 loss: -0.6546516418457031
Batch 18/64 loss: -0.9156007766723633
Batch 19/64 loss: -1.1481761932373047
Batch 20/64 loss: -1.0432510375976562
Batch 21/64 loss: -0.6150598526000977
Batch 22/64 loss: -0.5395412445068359
Batch 23/64 loss: -0.9866752624511719
Batch 24/64 loss: -0.7981986999511719
Batch 25/64 loss: -1.109731674194336
Batch 26/64 loss: -0.860713005065918
Batch 27/64 loss: -0.49620532989501953
Batch 28/64 loss: -0.8205099105834961
Batch 29/64 loss: -0.8292312622070312
Batch 30/64 loss: -1.0452384948730469
Batch 31/64 loss: -0.7551870346069336
Batch 32/64 loss: -0.8196315765380859
Batch 33/64 loss: -0.47411441802978516
Batch 34/64 loss: 0.27073097229003906
Batch 35/64 loss: -0.9893760681152344
Batch 36/64 loss: -1.0411348342895508
Batch 37/64 loss: -0.8986854553222656
Batch 38/64 loss: -0.5347557067871094
Batch 39/64 loss: -1.035888671875
Batch 40/64 loss: -0.6242561340332031
Batch 41/64 loss: -0.5917520523071289
Batch 42/64 loss: -0.9895915985107422
Batch 43/64 loss: -0.5788764953613281
Batch 44/64 loss: -0.4606904983520508
Batch 45/64 loss: -0.9651050567626953
Batch 46/64 loss: -0.9018678665161133
Batch 47/64 loss: -0.6012105941772461
Batch 48/64 loss: -0.5873737335205078
Batch 49/64 loss: -0.9965438842773438
Batch 50/64 loss: -0.9020957946777344
Batch 51/64 loss: -0.40396976470947266
Batch 52/64 loss: -1.047994613647461
Batch 53/64 loss: -0.5952425003051758
Batch 54/64 loss: -1.032949447631836
Batch 55/64 loss: -1.1857213973999023
Batch 56/64 loss: -0.7728338241577148
Batch 57/64 loss: -0.7421417236328125
Batch 58/64 loss: -1.017491340637207
Batch 59/64 loss: -0.8403596878051758
Batch 60/64 loss: -1.0853242874145508
Batch 61/64 loss: -0.7961206436157227
Batch 62/64 loss: -0.8937788009643555
Batch 63/64 loss: -0.68157958984375
Batch 64/64 loss: -4.6894659996032715
Epoch 443  Train loss: -0.8542749461005715  Val loss: -0.8411544590061882
Epoch 444
-------------------------------
Batch 1/64 loss: -0.6544017791748047
Batch 2/64 loss: -0.7710256576538086
Batch 3/64 loss: -0.5568208694458008
Batch 4/64 loss: -0.20296001434326172
Batch 5/64 loss: -1.042983055114746
Batch 6/64 loss: -0.9371881484985352
Batch 7/64 loss: -0.6049060821533203
Batch 8/64 loss: -0.45554161071777344
Batch 9/64 loss: -0.6928367614746094
Batch 10/64 loss: -0.5930461883544922
Batch 11/64 loss: -0.8550481796264648
Batch 12/64 loss: -0.9798965454101562
Batch 13/64 loss: -0.6681737899780273
Batch 14/64 loss: -0.7401103973388672
Batch 15/64 loss: -0.8909788131713867
Batch 16/64 loss: -0.9090795516967773
Batch 17/64 loss: -0.8320941925048828
Batch 18/64 loss: -0.8073663711547852
Batch 19/64 loss: -1.1381473541259766
Batch 20/64 loss: -1.1790122985839844
Batch 21/64 loss: -0.8344221115112305
Batch 22/64 loss: -0.7664222717285156
Batch 23/64 loss: -0.985804557800293
Batch 24/64 loss: -1.0498228073120117
Batch 25/64 loss: -1.2105188369750977
Batch 26/64 loss: -0.9552812576293945
Batch 27/64 loss: -0.5848159790039062
Batch 28/64 loss: -0.9148883819580078
Batch 29/64 loss: -0.9842500686645508
Batch 30/64 loss: -0.9181880950927734
Batch 31/64 loss: -0.9819831848144531
Batch 32/64 loss: -0.8256034851074219
Batch 33/64 loss: -1.0946531295776367
Batch 34/64 loss: -0.5268316268920898
Batch 35/64 loss: -0.5325403213500977
Batch 36/64 loss: -0.826939582824707
Batch 37/64 loss: -0.9841222763061523
Batch 38/64 loss: -0.9433689117431641
Batch 39/64 loss: -0.7533903121948242
Batch 40/64 loss: -0.7038564682006836
Batch 41/64 loss: -0.7566747665405273
Batch 42/64 loss: -0.8155174255371094
Batch 43/64 loss: -0.9661941528320312
Batch 44/64 loss: -1.2159786224365234
Batch 45/64 loss: -0.17675209045410156
Batch 46/64 loss: -0.7550764083862305
Batch 47/64 loss: -0.5333747863769531
Batch 48/64 loss: -0.6545886993408203
Batch 49/64 loss: -0.5731744766235352
Batch 50/64 loss: -1.0309333801269531
Batch 51/64 loss: -0.9459095001220703
Batch 52/64 loss: -0.8742904663085938
Batch 53/64 loss: -1.00115966796875
Batch 54/64 loss: -0.746943473815918
Batch 55/64 loss: -0.6891136169433594
Batch 56/64 loss: -0.7217254638671875
Batch 57/64 loss: -1.0007209777832031
Batch 58/64 loss: -1.0675640106201172
Batch 59/64 loss: -0.6340980529785156
Batch 60/64 loss: -0.671137809753418
Batch 61/64 loss: -0.920720100402832
Batch 62/64 loss: -0.8279571533203125
Batch 63/64 loss: -0.9298563003540039
Batch 64/64 loss: -4.698764801025391
Epoch 444  Train loss: -0.8615350012685739  Val loss: -0.8639729817708334
Epoch 445
-------------------------------
Batch 1/64 loss: -1.1763620376586914
Batch 2/64 loss: -0.6629581451416016
Batch 3/64 loss: -1.1208686828613281
Batch 4/64 loss: -1.1771125793457031
Batch 5/64 loss: -0.9194765090942383
Batch 6/64 loss: -1.2038345336914062
Batch 7/64 loss: -0.2425670623779297
Batch 8/64 loss: -0.8047113418579102
Batch 9/64 loss: -0.05525684356689453
Batch 10/64 loss: -1.009695053100586
Batch 11/64 loss: -0.7173976898193359
Batch 12/64 loss: -0.9547529220581055
Batch 13/64 loss: -1.0468645095825195
Batch 14/64 loss: -0.5103607177734375
Batch 15/64 loss: -0.7600250244140625
Batch 16/64 loss: -0.6023235321044922
Batch 17/64 loss: -0.7490568161010742
Batch 18/64 loss: -0.8752880096435547
Batch 19/64 loss: -0.8505592346191406
Batch 20/64 loss: -0.9123630523681641
Batch 21/64 loss: -0.7461948394775391
Batch 22/64 loss: -1.0631322860717773
Batch 23/64 loss: -0.8406839370727539
Batch 24/64 loss: -0.8643274307250977
Batch 25/64 loss: -0.9173212051391602
Batch 26/64 loss: -0.6389665603637695
Batch 27/64 loss: -0.5699176788330078
Batch 28/64 loss: -0.9204092025756836
Batch 29/64 loss: -0.46109771728515625
Batch 30/64 loss: -0.9017429351806641
Batch 31/64 loss: -0.6804981231689453
Batch 32/64 loss: -1.0335502624511719
Batch 33/64 loss: -1.0060787200927734
Batch 34/64 loss: -1.0485305786132812
Batch 35/64 loss: -1.0250425338745117
Batch 36/64 loss: -0.8396854400634766
Batch 37/64 loss: -0.9590158462524414
Batch 38/64 loss: -0.9154062271118164
Batch 39/64 loss: -0.9705896377563477
Batch 40/64 loss: -0.8168020248413086
Batch 41/64 loss: -0.41300392150878906
Batch 42/64 loss: -1.0787982940673828
Batch 43/64 loss: -1.0214014053344727
Batch 44/64 loss: -0.6900291442871094
Batch 45/64 loss: -0.6991100311279297
Batch 46/64 loss: 0.41150474548339844
Batch 47/64 loss: -0.6261568069458008
Batch 48/64 loss: -1.1213188171386719
Batch 49/64 loss: -0.6194295883178711
Batch 50/64 loss: -0.794276237487793
Batch 51/64 loss: -0.740382194519043
Batch 52/64 loss: -1.091017723083496
Batch 53/64 loss: -0.7146320343017578
Batch 54/64 loss: -0.8235483169555664
Batch 55/64 loss: -0.5245580673217773
Batch 56/64 loss: -0.4770345687866211
Batch 57/64 loss: -0.04744434356689453
Batch 58/64 loss: -0.6585102081298828
Batch 59/64 loss: -0.7748231887817383
Batch 60/64 loss: -0.6400461196899414
Batch 61/64 loss: -0.9377899169921875
Batch 62/64 loss: -0.6415195465087891
Batch 63/64 loss: -0.7319841384887695
Batch 64/64 loss: -4.778900623321533
Epoch 445  Train loss: -0.8252598089330336  Val loss: -0.851617138000698
Epoch 446
-------------------------------
Batch 1/64 loss: -0.43070411682128906
Batch 2/64 loss: -0.4506540298461914
Batch 3/64 loss: -0.9351778030395508
Batch 4/64 loss: -1.0501232147216797
Batch 5/64 loss: -0.8773174285888672
Batch 6/64 loss: -0.5823345184326172
Batch 7/64 loss: -0.7591714859008789
Batch 8/64 loss: -0.9058732986450195
Batch 9/64 loss: -0.3720684051513672
Batch 10/64 loss: -0.5232486724853516
Batch 11/64 loss: -1.0956182479858398
Batch 12/64 loss: -0.7759628295898438
Batch 13/64 loss: -0.48670005798339844
Batch 14/64 loss: -0.8033924102783203
Batch 15/64 loss: -0.7833032608032227
Batch 16/64 loss: -0.7120018005371094
Batch 17/64 loss: -1.073984146118164
Batch 18/64 loss: -0.7130699157714844
Batch 19/64 loss: -0.5819587707519531
Batch 20/64 loss: -0.511988639831543
Batch 21/64 loss: -1.083724021911621
Batch 22/64 loss: -0.7623977661132812
Batch 23/64 loss: -1.0132865905761719
Batch 24/64 loss: -0.5804033279418945
Batch 25/64 loss: -0.7642631530761719
Batch 26/64 loss: -0.8858871459960938
Batch 27/64 loss: -1.053863525390625
Batch 28/64 loss: -0.7683725357055664
Batch 29/64 loss: -0.6535682678222656
Batch 30/64 loss: -1.0952835083007812
Batch 31/64 loss: 0.09538650512695312
Batch 32/64 loss: -1.1873092651367188
Batch 33/64 loss: -0.697850227355957
Batch 34/64 loss: -0.7000970840454102
Batch 35/64 loss: -0.635615348815918
Batch 36/64 loss: -0.6836643218994141
Batch 37/64 loss: -1.139012336730957
Batch 38/64 loss: -0.7043619155883789
Batch 39/64 loss: -0.9067621231079102
Batch 40/64 loss: -0.9648227691650391
Batch 41/64 loss: -0.8229284286499023
Batch 42/64 loss: -0.7651653289794922
Batch 43/64 loss: -0.6919422149658203
Batch 44/64 loss: -0.9211854934692383
Batch 45/64 loss: -0.5769424438476562
Batch 46/64 loss: -0.649357795715332
Batch 47/64 loss: -0.8677778244018555
Batch 48/64 loss: -0.8230867385864258
Batch 49/64 loss: -0.5509605407714844
Batch 50/64 loss: -0.3484945297241211
Batch 51/64 loss: -0.6981868743896484
Batch 52/64 loss: -0.6957521438598633
Batch 53/64 loss: -0.9128589630126953
Batch 54/64 loss: -0.8909034729003906
Batch 55/64 loss: -0.6225824356079102
Batch 56/64 loss: -1.007211685180664
Batch 57/64 loss: -1.0088214874267578
Batch 58/64 loss: -0.8452997207641602
Batch 59/64 loss: -0.575068473815918
Batch 60/64 loss: -1.038313865661621
Batch 61/64 loss: -1.0023622512817383
Batch 62/64 loss: -0.5630331039428711
Batch 63/64 loss: -0.7794303894042969
Batch 64/64 loss: -4.759308815002441
Epoch 446  Train loss: -0.8131287780462527  Val loss: -0.8259458902365563
Epoch 447
-------------------------------
Batch 1/64 loss: -0.5929183959960938
Batch 2/64 loss: -0.8372163772583008
Batch 3/64 loss: -0.1857738494873047
Batch 4/64 loss: -1.0922050476074219
Batch 5/64 loss: -0.9405574798583984
Batch 6/64 loss: -0.9602327346801758
Batch 7/64 loss: -0.848388671875
Batch 8/64 loss: -0.7873067855834961
Batch 9/64 loss: -0.6222095489501953
Batch 10/64 loss: -0.7785806655883789
Batch 11/64 loss: -1.093796730041504
Batch 12/64 loss: -1.103367805480957
Batch 13/64 loss: -1.094970703125
Batch 14/64 loss: -1.0786323547363281
Batch 15/64 loss: -0.715825080871582
Batch 16/64 loss: -0.9154338836669922
Batch 17/64 loss: -0.7772226333618164
Batch 18/64 loss: -0.7522621154785156
Batch 19/64 loss: -0.5636758804321289
Batch 20/64 loss: -0.8861923217773438
Batch 21/64 loss: -0.7899112701416016
Batch 22/64 loss: -1.2311286926269531
Batch 23/64 loss: -0.6845474243164062
Batch 24/64 loss: -0.9529838562011719
Batch 25/64 loss: -0.44086265563964844
Batch 26/64 loss: -0.6753072738647461
Batch 27/64 loss: -0.7359819412231445
Batch 28/64 loss: -1.1240777969360352
Batch 29/64 loss: -1.0400590896606445
Batch 30/64 loss: -0.835392951965332
Batch 31/64 loss: -0.060817718505859375
Batch 32/64 loss: -0.9743967056274414
Batch 33/64 loss: -0.5671310424804688
Batch 34/64 loss: -1.0327816009521484
Batch 35/64 loss: -0.6895027160644531
Batch 36/64 loss: -1.0470390319824219
Batch 37/64 loss: -0.6554441452026367
Batch 38/64 loss: -0.823211669921875
Batch 39/64 loss: -0.614349365234375
Batch 40/64 loss: -0.958526611328125
Batch 41/64 loss: -1.018667221069336
Batch 42/64 loss: -0.9152202606201172
Batch 43/64 loss: -0.3339262008666992
Batch 44/64 loss: -1.095052719116211
Batch 45/64 loss: -0.5247945785522461
Batch 46/64 loss: -0.9342174530029297
Batch 47/64 loss: -0.8512983322143555
Batch 48/64 loss: -0.5534286499023438
Batch 49/64 loss: -0.31473636627197266
Batch 50/64 loss: -0.6456718444824219
Batch 51/64 loss: -0.8616628646850586
Batch 52/64 loss: -0.871851921081543
Batch 53/64 loss: -0.9592409133911133
Batch 54/64 loss: -1.0864810943603516
Batch 55/64 loss: -0.5461721420288086
Batch 56/64 loss: -0.13983583450317383
Batch 57/64 loss: -0.8157463073730469
Batch 58/64 loss: -0.7639961242675781
Batch 59/64 loss: -0.9869823455810547
Batch 60/64 loss: -0.9206457138061523
Batch 61/64 loss: -0.8857917785644531
Batch 62/64 loss: -1.0387744903564453
Batch 63/64 loss: -0.603947639465332
Batch 64/64 loss: -5.027263164520264
Epoch 447  Train loss: -0.8470402007009469  Val loss: -0.8167218080500966
Epoch 448
-------------------------------
Batch 1/64 loss: -0.5267848968505859
Batch 2/64 loss: -0.4168586730957031
Batch 3/64 loss: -0.319976806640625
Batch 4/64 loss: -1.1456270217895508
Batch 5/64 loss: -0.5056562423706055
Batch 6/64 loss: -1.0408601760864258
Batch 7/64 loss: -1.0811071395874023
Batch 8/64 loss: -0.21991443634033203
Batch 9/64 loss: -0.9757204055786133
Batch 10/64 loss: -0.6188888549804688
Batch 11/64 loss: -0.43107032775878906
Batch 12/64 loss: -0.9855108261108398
Batch 13/64 loss: -1.0517101287841797
Batch 14/64 loss: -0.7222681045532227
Batch 15/64 loss: -1.2076425552368164
Batch 16/64 loss: -0.8074712753295898
Batch 17/64 loss: -1.1028146743774414
Batch 18/64 loss: -0.8517646789550781
Batch 19/64 loss: -0.7929468154907227
Batch 20/64 loss: -0.9561243057250977
Batch 21/64 loss: -0.8591737747192383
Batch 22/64 loss: -1.0177497863769531
Batch 23/64 loss: -0.8374824523925781
Batch 24/64 loss: -0.6800327301025391
Batch 25/64 loss: -0.16674423217773438
Batch 26/64 loss: -0.649907112121582
Batch 27/64 loss: -0.8417949676513672
Batch 28/64 loss: -0.8968715667724609
Batch 29/64 loss: -1.0072908401489258
Batch 30/64 loss: -0.6754693984985352
Batch 31/64 loss: -0.9030599594116211
Batch 32/64 loss: -0.7329854965209961
Batch 33/64 loss: -0.5242681503295898
Batch 34/64 loss: -0.8765726089477539
Batch 35/64 loss: -0.8550052642822266
Batch 36/64 loss: -0.7833948135375977
Batch 37/64 loss: -0.5445747375488281
Batch 38/64 loss: -0.1913127899169922
Batch 39/64 loss: -1.1119394302368164
Batch 40/64 loss: -0.9834375381469727
Batch 41/64 loss: -0.7175865173339844
Batch 42/64 loss: -0.9331769943237305
Batch 43/64 loss: -1.0449895858764648
Batch 44/64 loss: -0.7487268447875977
Batch 45/64 loss: -0.9339561462402344
Batch 46/64 loss: -1.1993341445922852
Batch 47/64 loss: -1.0895748138427734
Batch 48/64 loss: -1.0620641708374023
Batch 49/64 loss: -0.5070257186889648
Batch 50/64 loss: -1.2364387512207031
Batch 51/64 loss: -0.8772811889648438
Batch 52/64 loss: -1.0490083694458008
Batch 53/64 loss: -1.062173843383789
Batch 54/64 loss: -0.3504295349121094
Batch 55/64 loss: -1.0608234405517578
Batch 56/64 loss: -0.7208042144775391
Batch 57/64 loss: -1.0440006256103516
Batch 58/64 loss: -0.8431997299194336
Batch 59/64 loss: -0.8402853012084961
Batch 60/64 loss: -0.886530876159668
Batch 61/64 loss: -0.7121810913085938
Batch 62/64 loss: -0.7852716445922852
Batch 63/64 loss: -0.47632408142089844
Batch 64/64 loss: -4.607215404510498
Epoch 448  Train loss: -0.8554727086833879  Val loss: -0.9759553863420519
Epoch 449
-------------------------------
Batch 1/64 loss: -0.8503818511962891
Batch 2/64 loss: -0.7108879089355469
Batch 3/64 loss: -0.5243310928344727
Batch 4/64 loss: -0.35016727447509766
Batch 5/64 loss: -1.130753517150879
Batch 6/64 loss: -0.4712028503417969
Batch 7/64 loss: -0.747126579284668
Batch 8/64 loss: -0.7717313766479492
Batch 9/64 loss: -0.8341178894042969
Batch 10/64 loss: -0.4757051467895508
Batch 11/64 loss: -0.6197977066040039
Batch 12/64 loss: -0.6524620056152344
Batch 13/64 loss: -0.8971223831176758
Batch 14/64 loss: -0.9869966506958008
Batch 15/64 loss: -0.6220827102661133
Batch 16/64 loss: -1.0457067489624023
Batch 17/64 loss: -1.013406753540039
Batch 18/64 loss: -0.5466299057006836
Batch 19/64 loss: -0.6293582916259766
Batch 20/64 loss: -0.6122188568115234
Batch 21/64 loss: -0.8790645599365234
Batch 22/64 loss: -0.9959659576416016
Batch 23/64 loss: -0.8427391052246094
Batch 24/64 loss: -0.8711757659912109
Batch 25/64 loss: -0.6953134536743164
Batch 26/64 loss: -0.6579999923706055
Batch 27/64 loss: -0.27912235260009766
Batch 28/64 loss: -0.8425636291503906
Batch 29/64 loss: -0.7963676452636719
Batch 30/64 loss: -0.888641357421875
Batch 31/64 loss: -0.9360322952270508
Batch 32/64 loss: -0.42937755584716797
Batch 33/64 loss: -0.927699089050293
Batch 34/64 loss: -0.9840364456176758
Batch 35/64 loss: -1.0367631912231445
Batch 36/64 loss: -0.9321184158325195
Batch 37/64 loss: -1.0158662796020508
Batch 38/64 loss: -0.7203073501586914
Batch 39/64 loss: -0.9974727630615234
Batch 40/64 loss: -0.7279272079467773
Batch 41/64 loss: -1.1432523727416992
Batch 42/64 loss: -0.8264036178588867
Batch 43/64 loss: -0.9270830154418945
Batch 44/64 loss: -0.9800167083740234
Batch 45/64 loss: -0.9596233367919922
Batch 46/64 loss: -0.8429775238037109
Batch 47/64 loss: -0.42151451110839844
Batch 48/64 loss: -0.8269834518432617
Batch 49/64 loss: -0.9771289825439453
Batch 50/64 loss: -0.6068496704101562
Batch 51/64 loss: -0.6219539642333984
Batch 52/64 loss: -1.0000982284545898
Batch 53/64 loss: -0.9628782272338867
Batch 54/64 loss: -1.0203542709350586
Batch 55/64 loss: -1.2679862976074219
Batch 56/64 loss: -0.7648582458496094
Batch 57/64 loss: -0.7697153091430664
Batch 58/64 loss: -0.7392168045043945
Batch 59/64 loss: -0.5134620666503906
Batch 60/64 loss: -0.7879219055175781
Batch 61/64 loss: -1.1210708618164062
Batch 62/64 loss: -0.9720983505249023
Batch 63/64 loss: -0.7800350189208984
Batch 64/64 loss: -4.243139266967773
Epoch 449  Train loss: -0.8465032016529757  Val loss: -0.8903928671505853
Epoch 450
-------------------------------
Batch 1/64 loss: -0.7689123153686523
Batch 2/64 loss: -0.9895038604736328
Batch 3/64 loss: -0.8699541091918945
Batch 4/64 loss: -1.0171031951904297
Batch 5/64 loss: -0.9769487380981445
Batch 6/64 loss: -1.069676399230957
Batch 7/64 loss: -0.913456916809082
Batch 8/64 loss: -0.9052219390869141
Batch 9/64 loss: -0.7017240524291992
Batch 10/64 loss: -0.8252105712890625
Batch 11/64 loss: -0.6145467758178711
Batch 12/64 loss: -0.79541015625
Batch 13/64 loss: -0.7302255630493164
Batch 14/64 loss: -0.8168096542358398
Batch 15/64 loss: -0.8919334411621094
Batch 16/64 loss: -0.5343084335327148
Batch 17/64 loss: -0.6972036361694336
Batch 18/64 loss: -0.5909099578857422
Batch 19/64 loss: -0.9814538955688477
Batch 20/64 loss: -0.9368267059326172
Batch 21/64 loss: -0.8430290222167969
Batch 22/64 loss: -1.0986547470092773
Batch 23/64 loss: -1.1357011795043945
Batch 24/64 loss: -0.9569025039672852
Batch 25/64 loss: -0.8655681610107422
Batch 26/64 loss: -0.2956562042236328
Batch 27/64 loss: -0.989898681640625
Batch 28/64 loss: -0.6143112182617188
Batch 29/64 loss: -0.5303301811218262
Batch 30/64 loss: -1.1600227355957031
Batch 31/64 loss: -0.9656572341918945
Batch 32/64 loss: -0.8871440887451172
Batch 33/64 loss: -1.0947036743164062
Batch 34/64 loss: -0.8836164474487305
Batch 35/64 loss: -0.8741140365600586
Batch 36/64 loss: -0.8973722457885742
Batch 37/64 loss: -0.6447668075561523
Batch 38/64 loss: -0.8062248229980469
Batch 39/64 loss: -0.8176498413085938
Batch 40/64 loss: -0.9151239395141602
Batch 41/64 loss: -1.0578603744506836
Batch 42/64 loss: -1.0495109558105469
Batch 43/64 loss: -1.1542177200317383
Batch 44/64 loss: -0.8970375061035156
Batch 45/64 loss: -0.45427608489990234
Batch 46/64 loss: -1.0608510971069336
Batch 47/64 loss: -0.8784952163696289
Batch 48/64 loss: -0.46026134490966797
Batch 49/64 loss: -1.0186452865600586
Batch 50/64 loss: -0.8582420349121094
Batch 51/64 loss: -1.1053810119628906
Batch 52/64 loss: -1.1775999069213867
Batch 53/64 loss: -0.8370828628540039
Batch 54/64 loss: -0.6089658737182617
Batch 55/64 loss: -0.9207334518432617
Batch 56/64 loss: -0.6152687072753906
Batch 57/64 loss: -0.4369182586669922
Batch 58/64 loss: -0.7728614807128906
Batch 59/64 loss: -0.8339319229125977
Batch 60/64 loss: -0.7875499725341797
Batch 61/64 loss: -0.8686304092407227
Batch 62/64 loss: -1.0268993377685547
Batch 63/64 loss: -1.000199317932129
Batch 64/64 loss: -4.508613586425781
Epoch 450  Train loss: -0.8967320535697189  Val loss: -0.9078994829630115
Epoch 451
-------------------------------
Batch 1/64 loss: -0.7324752807617188
Batch 2/64 loss: -1.355031967163086
Batch 3/64 loss: -0.8037538528442383
Batch 4/64 loss: -1.072688102722168
Batch 5/64 loss: -0.781367301940918
Batch 6/64 loss: -1.1299591064453125
Batch 7/64 loss: -0.9683942794799805
Batch 8/64 loss: -1.124185562133789
Batch 9/64 loss: -1.0672111511230469
Batch 10/64 loss: -0.8544578552246094
Batch 11/64 loss: -1.3001174926757812
Batch 12/64 loss: -0.6331281661987305
Batch 13/64 loss: -0.8329133987426758
Batch 14/64 loss: -0.5628061294555664
Batch 15/64 loss: -0.5354270935058594
Batch 16/64 loss: -1.002650260925293
Batch 17/64 loss: -0.54193115234375
Batch 18/64 loss: -1.0774898529052734
Batch 19/64 loss: -1.01776123046875
Batch 20/64 loss: -0.8733463287353516
Batch 21/64 loss: -0.7712430953979492
Batch 22/64 loss: -1.1983404159545898
Batch 23/64 loss: -0.8622674942016602
Batch 24/64 loss: -1.1753511428833008
Batch 25/64 loss: -0.8247861862182617
Batch 26/64 loss: -0.7685213088989258
Batch 27/64 loss: -0.726984977722168
Batch 28/64 loss: -0.5253229141235352
Batch 29/64 loss: -1.007277488708496
Batch 30/64 loss: -0.9941120147705078
Batch 31/64 loss: -0.22955989837646484
Batch 32/64 loss: -0.7913017272949219
Batch 33/64 loss: -0.7878150939941406
Batch 34/64 loss: -1.1978816986083984
Batch 35/64 loss: -1.1950864791870117
Batch 36/64 loss: -0.859227180480957
Batch 37/64 loss: -0.7132472991943359
Batch 38/64 loss: -0.7878818511962891
Batch 39/64 loss: -1.0563955307006836
Batch 40/64 loss: -0.7576017379760742
Batch 41/64 loss: -1.1121940612792969
Batch 42/64 loss: -0.9020214080810547
Batch 43/64 loss: -1.0756950378417969
Batch 44/64 loss: -0.8235464096069336
Batch 45/64 loss: -0.9982099533081055
Batch 46/64 loss: -0.9664192199707031
Batch 47/64 loss: -0.3427591323852539
Batch 48/64 loss: -0.2291707992553711
Batch 49/64 loss: -0.9397745132446289
Batch 50/64 loss: -1.1389789581298828
Batch 51/64 loss: -0.5782575607299805
Batch 52/64 loss: -0.9538421630859375
Batch 53/64 loss: -0.6692285537719727
Batch 54/64 loss: -0.6353645324707031
Batch 55/64 loss: -0.7441310882568359
Batch 56/64 loss: -0.5613088607788086
Batch 57/64 loss: -0.7561779022216797
Batch 58/64 loss: -0.9894323348999023
Batch 59/64 loss: -0.8750457763671875
Batch 60/64 loss: -0.9015207290649414
Batch 61/64 loss: -0.23703670501708984
Batch 62/64 loss: -0.8616867065429688
Batch 63/64 loss: -0.7669448852539062
Batch 64/64 loss: -4.348243713378906
Epoch 451  Train loss: -0.8912506851495481  Val loss: -0.9177471173997597
Epoch 452
-------------------------------
Batch 1/64 loss: -1.130910873413086
Batch 2/64 loss: -0.9398479461669922
Batch 3/64 loss: -0.9319963455200195
Batch 4/64 loss: -0.8238410949707031
Batch 5/64 loss: -0.3407773971557617
Batch 6/64 loss: -0.6883182525634766
Batch 7/64 loss: -0.6874170303344727
Batch 8/64 loss: -0.9542112350463867
Batch 9/64 loss: -1.0705070495605469
Batch 10/64 loss: -0.9806299209594727
Batch 11/64 loss: -0.8859386444091797
Batch 12/64 loss: -0.8141775131225586
Batch 13/64 loss: -0.8053312301635742
Batch 14/64 loss: -0.67242431640625
Batch 15/64 loss: -0.5065851211547852
Batch 16/64 loss: -0.6364231109619141
Batch 17/64 loss: -0.9381599426269531
Batch 18/64 loss: -0.7070159912109375
Batch 19/64 loss: -1.0874271392822266
Batch 20/64 loss: -1.1219854354858398
Batch 21/64 loss: -1.088165283203125
Batch 22/64 loss: -1.1659412384033203
Batch 23/64 loss: -0.6487846374511719
Batch 24/64 loss: -0.7348260879516602
Batch 25/64 loss: -0.5019044876098633
Batch 26/64 loss: -0.8317947387695312
Batch 27/64 loss: -1.004960060119629
Batch 28/64 loss: -0.7497987747192383
Batch 29/64 loss: -0.6883974075317383
Batch 30/64 loss: -0.5455055236816406
Batch 31/64 loss: -0.8012304306030273
Batch 32/64 loss: -0.4068565368652344
Batch 33/64 loss: -1.0153675079345703
Batch 34/64 loss: -0.7318220138549805
Batch 35/64 loss: -0.875218391418457
Batch 36/64 loss: -0.8287487030029297
Batch 37/64 loss: -0.9806938171386719
Batch 38/64 loss: -0.7145166397094727
Batch 39/64 loss: -0.6437931060791016
Batch 40/64 loss: -1.0147743225097656
Batch 41/64 loss: -0.890467643737793
Batch 42/64 loss: -0.8751039505004883
Batch 43/64 loss: -0.8255023956298828
Batch 44/64 loss: -0.8247365951538086
Batch 45/64 loss: -0.890934944152832
Batch 46/64 loss: -0.7283468246459961
Batch 47/64 loss: -1.2202472686767578
Batch 48/64 loss: -0.6109647750854492
Batch 49/64 loss: -1.1921148300170898
Batch 50/64 loss: -1.0206785202026367
Batch 51/64 loss: -0.7644252777099609
Batch 52/64 loss: -1.1763534545898438
Batch 53/64 loss: -1.3136491775512695
Batch 54/64 loss: -0.1412954330444336
Batch 55/64 loss: -0.799229621887207
Batch 56/64 loss: -0.5147409439086914
Batch 57/64 loss: -1.002065658569336
Batch 58/64 loss: -0.9329948425292969
Batch 59/64 loss: -1.0265541076660156
Batch 60/64 loss: -0.6634035110473633
Batch 61/64 loss: -0.9044208526611328
Batch 62/64 loss: 0.07547378540039062
Batch 63/64 loss: -0.9730987548828125
Batch 64/64 loss: -4.9165568351745605
Epoch 452  Train loss: -0.8721615454729865  Val loss: -0.8235604853154868
Epoch 453
-------------------------------
Batch 1/64 loss: -0.6612071990966797
Batch 2/64 loss: -0.9295225143432617
Batch 3/64 loss: -0.5826101303100586
Batch 4/64 loss: -0.4880800247192383
Batch 5/64 loss: -0.5725069046020508
Batch 6/64 loss: -0.9734783172607422
Batch 7/64 loss: -0.9165754318237305
Batch 8/64 loss: -0.9744091033935547
Batch 9/64 loss: -1.1191167831420898
Batch 10/64 loss: -0.8187284469604492
Batch 11/64 loss: -0.8556079864501953
Batch 12/64 loss: -0.6672792434692383
Batch 13/64 loss: -0.7789878845214844
Batch 14/64 loss: -1.2356538772583008
Batch 15/64 loss: -0.885340690612793
Batch 16/64 loss: -0.8299522399902344
Batch 17/64 loss: -0.424224853515625
Batch 18/64 loss: -0.8287115097045898
Batch 19/64 loss: -1.0336132049560547
Batch 20/64 loss: -0.7332401275634766
Batch 21/64 loss: -0.6444797515869141
Batch 22/64 loss: -0.7650041580200195
Batch 23/64 loss: -0.6073408126831055
Batch 24/64 loss: -0.6298637390136719
Batch 25/64 loss: -0.9445638656616211
Batch 26/64 loss: -0.884791374206543
Batch 27/64 loss: -0.8820466995239258
Batch 28/64 loss: -0.9723176956176758
Batch 29/64 loss: -0.9045381546020508
Batch 30/64 loss: -0.564361572265625
Batch 31/64 loss: -1.0564985275268555
Batch 32/64 loss: -0.6608505249023438
Batch 33/64 loss: -1.2276315689086914
Batch 34/64 loss: -0.38636112213134766
Batch 35/64 loss: -0.9136219024658203
Batch 36/64 loss: -0.7905826568603516
Batch 37/64 loss: -0.6946239471435547
Batch 38/64 loss: -0.9366874694824219
Batch 39/64 loss: -1.1581945419311523
Batch 40/64 loss: -0.7885885238647461
Batch 41/64 loss: -1.035140037536621
Batch 42/64 loss: -0.3960447311401367
Batch 43/64 loss: -0.985931396484375
Batch 44/64 loss: -0.9871807098388672
Batch 45/64 loss: -0.9759521484375
Batch 46/64 loss: -0.8027095794677734
Batch 47/64 loss: -0.8211116790771484
Batch 48/64 loss: -0.8416852951049805
Batch 49/64 loss: -0.6306276321411133
Batch 50/64 loss: -1.1530799865722656
Batch 51/64 loss: -0.25563526153564453
Batch 52/64 loss: -1.0468225479125977
Batch 53/64 loss: -1.0386314392089844
Batch 54/64 loss: -0.9874935150146484
Batch 55/64 loss: -1.0461492538452148
Batch 56/64 loss: -0.974064826965332
Batch 57/64 loss: -1.101618766784668
Batch 58/64 loss: -0.7624320983886719
Batch 59/64 loss: -0.6687297821044922
Batch 60/64 loss: -0.6371574401855469
Batch 61/64 loss: -0.7962923049926758
Batch 62/64 loss: -0.8912534713745117
Batch 63/64 loss: -0.7874011993408203
Batch 64/64 loss: -4.600747108459473
Epoch 453  Train loss: -0.8752235375198664  Val loss: -0.9084567958137014
Epoch 454
-------------------------------
Batch 1/64 loss: -0.8071165084838867
Batch 2/64 loss: -0.6423425674438477
Batch 3/64 loss: -1.1291265487670898
Batch 4/64 loss: -0.5600156784057617
Batch 5/64 loss: -0.8687953948974609
Batch 6/64 loss: -0.6242246627807617
Batch 7/64 loss: -1.036606788635254
Batch 8/64 loss: -0.9549322128295898
Batch 9/64 loss: -0.7171344757080078
Batch 10/64 loss: -0.9323663711547852
Batch 11/64 loss: -0.6431417465209961
Batch 12/64 loss: -0.9182529449462891
Batch 13/64 loss: -1.0126895904541016
Batch 14/64 loss: -0.7954120635986328
Batch 15/64 loss: -0.5179452896118164
Batch 16/64 loss: -0.7791032791137695
Batch 17/64 loss: -1.2253618240356445
Batch 18/64 loss: -1.073190689086914
Batch 19/64 loss: -1.1612863540649414
Batch 20/64 loss: -1.0529813766479492
Batch 21/64 loss: -1.0187244415283203
Batch 22/64 loss: -0.5385780334472656
Batch 23/64 loss: -0.6887826919555664
Batch 24/64 loss: -0.8665904998779297
Batch 25/64 loss: -1.0666189193725586
Batch 26/64 loss: -0.9814414978027344
Batch 27/64 loss: -0.9447317123413086
Batch 28/64 loss: -0.6224269866943359
Batch 29/64 loss: -0.8917884826660156
Batch 30/64 loss: -0.7605981826782227
Batch 31/64 loss: -0.9137134552001953
Batch 32/64 loss: -0.8819713592529297
Batch 33/64 loss: -0.514124870300293
Batch 34/64 loss: -0.9083652496337891
Batch 35/64 loss: -0.8577127456665039
Batch 36/64 loss: -0.9201736450195312
Batch 37/64 loss: -1.0451087951660156
Batch 38/64 loss: -0.8811311721801758
Batch 39/64 loss: -0.6146764755249023
Batch 40/64 loss: -0.8931236267089844
Batch 41/64 loss: -1.0832891464233398
Batch 42/64 loss: -1.121983528137207
Batch 43/64 loss: -0.8580865859985352
Batch 44/64 loss: -0.8576650619506836
Batch 45/64 loss: -0.6552104949951172
Batch 46/64 loss: -1.0407600402832031
Batch 47/64 loss: -1.0295820236206055
Batch 48/64 loss: -1.0739641189575195
Batch 49/64 loss: -0.9705705642700195
Batch 50/64 loss: -0.41036128997802734
Batch 51/64 loss: -0.7755002975463867
Batch 52/64 loss: -0.7210426330566406
Batch 53/64 loss: -0.9111042022705078
Batch 54/64 loss: -0.8678474426269531
Batch 55/64 loss: -0.8475894927978516
Batch 56/64 loss: -0.9206991195678711
Batch 57/64 loss: -1.0425128936767578
Batch 58/64 loss: -0.061186790466308594
Batch 59/64 loss: -0.7336015701293945
Batch 60/64 loss: -0.9808588027954102
Batch 61/64 loss: -0.22887706756591797
Batch 62/64 loss: -1.0331315994262695
Batch 63/64 loss: -1.0378036499023438
Batch 64/64 loss: -4.842926979064941
Epoch 454  Train loss: -0.8965934566423005  Val loss: -1.009301844331407
Saving best model, epoch: 454
Epoch 455
-------------------------------
Batch 1/64 loss: -1.1397895812988281
Batch 2/64 loss: -0.32713937759399414
Batch 3/64 loss: -0.9103031158447266
Batch 4/64 loss: -1.1625194549560547
Batch 5/64 loss: -0.8600921630859375
Batch 6/64 loss: -0.8697843551635742
Batch 7/64 loss: -0.7405729293823242
Batch 8/64 loss: 0.12606048583984375
Batch 9/64 loss: -0.6495208740234375
Batch 10/64 loss: -0.47866058349609375
Batch 11/64 loss: -1.2132673263549805
Batch 12/64 loss: -0.8912944793701172
Batch 13/64 loss: -0.8228044509887695
Batch 14/64 loss: -0.7901506423950195
Batch 15/64 loss: -0.6635236740112305
Batch 16/64 loss: -0.37138938903808594
Batch 17/64 loss: -0.821406364440918
Batch 18/64 loss: -0.9327354431152344
Batch 19/64 loss: -0.746678352355957
Batch 20/64 loss: -1.033492088317871
Batch 21/64 loss: -0.5428781509399414
Batch 22/64 loss: -0.7543163299560547
Batch 23/64 loss: -0.9965219497680664
Batch 24/64 loss: -1.2425642013549805
Batch 25/64 loss: -0.8551740646362305
Batch 26/64 loss: -0.9540376663208008
Batch 27/64 loss: -0.815770149230957
Batch 28/64 loss: -0.7651405334472656
Batch 29/64 loss: -0.8803596496582031
Batch 30/64 loss: -0.8746252059936523
Batch 31/64 loss: -0.8485107421875
Batch 32/64 loss: -0.9426116943359375
Batch 33/64 loss: -0.5964498519897461
Batch 34/64 loss: -0.8624496459960938
Batch 35/64 loss: -0.39803028106689453
Batch 36/64 loss: -0.5296764373779297
Batch 37/64 loss: -1.0233325958251953
Batch 38/64 loss: -0.9699621200561523
Batch 39/64 loss: -0.7797269821166992
Batch 40/64 loss: -0.8252410888671875
Batch 41/64 loss: -1.0885992050170898
Batch 42/64 loss: -0.5980195999145508
Batch 43/64 loss: -0.8803119659423828
Batch 44/64 loss: -0.8401956558227539
Batch 45/64 loss: -0.5303955078125
Batch 46/64 loss: -0.9256172180175781
Batch 47/64 loss: -0.34235095977783203
Batch 48/64 loss: -0.22753143310546875
Batch 49/64 loss: -0.8068389892578125
Batch 50/64 loss: -0.660675048828125
Batch 51/64 loss: -0.40123939514160156
Batch 52/64 loss: -0.6713380813598633
Batch 53/64 loss: -0.7379589080810547
Batch 54/64 loss: -0.8713188171386719
Batch 55/64 loss: -0.4579954147338867
Batch 56/64 loss: -0.3421506881713867
Batch 57/64 loss: -0.8479070663452148
Batch 58/64 loss: -0.9502229690551758
Batch 59/64 loss: -0.8823709487915039
Batch 60/64 loss: -0.9938745498657227
Batch 61/64 loss: -0.8878536224365234
Batch 62/64 loss: -0.9979572296142578
Batch 63/64 loss: -0.8364028930664062
Batch 64/64 loss: -4.769731044769287
Epoch 455  Train loss: -0.8174567521787157  Val loss: -0.7849718467476442
Epoch 456
-------------------------------
Batch 1/64 loss: -0.7859039306640625
Batch 2/64 loss: -0.9009227752685547
Batch 3/64 loss: -1.0877199172973633
Batch 4/64 loss: -0.4560203552246094
Batch 5/64 loss: -0.9626436233520508
Batch 6/64 loss: -0.7861413955688477
Batch 7/64 loss: -0.8416690826416016
Batch 8/64 loss: -0.8164682388305664
Batch 9/64 loss: -0.37819862365722656
Batch 10/64 loss: -0.36222171783447266
Batch 11/64 loss: -0.761723518371582
Batch 12/64 loss: -0.9049882888793945
Batch 13/64 loss: -1.1183805465698242
Batch 14/64 loss: -0.8215141296386719
Batch 15/64 loss: -0.5818262100219727
Batch 16/64 loss: -1.0208816528320312
Batch 17/64 loss: -0.8364200592041016
Batch 18/64 loss: -0.8835620880126953
Batch 19/64 loss: -1.0723152160644531
Batch 20/64 loss: -0.8376789093017578
Batch 21/64 loss: -1.0517692565917969
Batch 22/64 loss: -0.965540885925293
Batch 23/64 loss: -0.7006101608276367
Batch 24/64 loss: -0.8096036911010742
Batch 25/64 loss: -0.5069284439086914
Batch 26/64 loss: -0.9815082550048828
Batch 27/64 loss: -0.6900482177734375
Batch 28/64 loss: -0.9650812149047852
Batch 29/64 loss: -0.7228584289550781
Batch 30/64 loss: -1.0767450332641602
Batch 31/64 loss: -0.34334468841552734
Batch 32/64 loss: -0.9795713424682617
Batch 33/64 loss: -0.7267580032348633
Batch 34/64 loss: -0.5794429779052734
Batch 35/64 loss: -0.9318609237670898
Batch 36/64 loss: -0.7526617050170898
Batch 37/64 loss: -0.7366476058959961
Batch 38/64 loss: -0.9417095184326172
Batch 39/64 loss: -0.6442165374755859
Batch 40/64 loss: -0.9704999923706055
Batch 41/64 loss: -0.7991342544555664
Batch 42/64 loss: -0.8830051422119141
Batch 43/64 loss: -0.7770481109619141
Batch 44/64 loss: -0.8415365219116211
Batch 45/64 loss: -0.6827230453491211
Batch 46/64 loss: -0.7796192169189453
Batch 47/64 loss: -0.37285947799682617
Batch 48/64 loss: -0.8400430679321289
Batch 49/64 loss: -0.9117927551269531
Batch 50/64 loss: -1.0031194686889648
Batch 51/64 loss: -0.9569740295410156
Batch 52/64 loss: -1.0159893035888672
Batch 53/64 loss: -0.6262950897216797
Batch 54/64 loss: -1.055403709411621
Batch 55/64 loss: -0.31620121002197266
Batch 56/64 loss: -0.8132820129394531
Batch 57/64 loss: -0.8873233795166016
Batch 58/64 loss: -0.8369522094726562
Batch 59/64 loss: -0.5032930374145508
Batch 60/64 loss: -0.9381990432739258
Batch 61/64 loss: -0.9582252502441406
Batch 62/64 loss: -0.7613010406494141
Batch 63/64 loss: -0.5916271209716797
Batch 64/64 loss: -4.817360877990723
Epoch 456  Train loss: -0.8479933383418065  Val loss: -0.8970425137129846
Epoch 457
-------------------------------
Batch 1/64 loss: -0.6295757293701172
Batch 2/64 loss: -0.5490789413452148
Batch 3/64 loss: -1.0034656524658203
Batch 4/64 loss: -0.35784053802490234
Batch 5/64 loss: -0.8465700149536133
Batch 6/64 loss: -0.9989748001098633
Batch 7/64 loss: -0.899836540222168
Batch 8/64 loss: -1.1685972213745117
Batch 9/64 loss: -0.7464761734008789
Batch 10/64 loss: -0.5315761566162109
Batch 11/64 loss: -1.1079750061035156
Batch 12/64 loss: -0.8094663619995117
Batch 13/64 loss: -1.133204460144043
Batch 14/64 loss: -0.9456472396850586
Batch 15/64 loss: -0.7558040618896484
Batch 16/64 loss: -0.9530429840087891
Batch 17/64 loss: -1.1343259811401367
Batch 18/64 loss: -0.80633544921875
Batch 19/64 loss: -0.7076358795166016
Batch 20/64 loss: -0.9130592346191406
Batch 21/64 loss: -0.8536176681518555
Batch 22/64 loss: -1.0037012100219727
Batch 23/64 loss: -0.5591249465942383
Batch 24/64 loss: -1.2977466583251953
Batch 25/64 loss: -0.6483755111694336
Batch 26/64 loss: -0.4692811965942383
Batch 27/64 loss: -0.44828224182128906
Batch 28/64 loss: -0.6434497833251953
Batch 29/64 loss: -0.7249422073364258
Batch 30/64 loss: -1.053114891052246
Batch 31/64 loss: -0.2282109260559082
Batch 32/64 loss: -0.4774608612060547
Batch 33/64 loss: -0.7395553588867188
Batch 34/64 loss: -0.9513273239135742
Batch 35/64 loss: -0.41644287109375
Batch 36/64 loss: -0.5203142166137695
Batch 37/64 loss: -0.7613325119018555
Batch 38/64 loss: -0.8109607696533203
Batch 39/64 loss: -1.111541748046875
Batch 40/64 loss: -0.8928470611572266
Batch 41/64 loss: -0.35656118392944336
Batch 42/64 loss: -0.683222770690918
Batch 43/64 loss: -0.9873571395874023
Batch 44/64 loss: -0.6471004486083984
Batch 45/64 loss: -0.980494499206543
Batch 46/64 loss: -0.7376518249511719
Batch 47/64 loss: -0.6532955169677734
Batch 48/64 loss: -0.7979278564453125
Batch 49/64 loss: -0.8754148483276367
Batch 50/64 loss: -0.6329269409179688
Batch 51/64 loss: -0.7329339981079102
Batch 52/64 loss: -0.8979034423828125
Batch 53/64 loss: -0.8387918472290039
Batch 54/64 loss: -1.0090360641479492
Batch 55/64 loss: -0.9535818099975586
Batch 56/64 loss: -1.198394775390625
Batch 57/64 loss: -0.8932027816772461
Batch 58/64 loss: -0.76434326171875
Batch 59/64 loss: -0.5641584396362305
Batch 60/64 loss: -0.7230634689331055
Batch 61/64 loss: -0.5687990188598633
Batch 62/64 loss: -0.9260025024414062
Batch 63/64 loss: -0.8968620300292969
Batch 64/64 loss: -4.787866592407227
Epoch 457  Train loss: -0.8395301145665786  Val loss: -0.8999309212071789
Epoch 458
-------------------------------
Batch 1/64 loss: -1.128218650817871
Batch 2/64 loss: -0.9267520904541016
Batch 3/64 loss: -1.0708036422729492
Batch 4/64 loss: -0.7313127517700195
Batch 5/64 loss: -0.9257354736328125
Batch 6/64 loss: -0.985844612121582
Batch 7/64 loss: -0.9576749801635742
Batch 8/64 loss: -0.747584342956543
Batch 9/64 loss: -0.503392219543457
Batch 10/64 loss: -0.8798866271972656
Batch 11/64 loss: -0.9532566070556641
Batch 12/64 loss: -0.6175603866577148
Batch 13/64 loss: -0.6176786422729492
Batch 14/64 loss: -0.614501953125
Batch 15/64 loss: -0.9407148361206055
Batch 16/64 loss: -0.6954746246337891
Batch 17/64 loss: -1.186269760131836
Batch 18/64 loss: -0.9737358093261719
Batch 19/64 loss: -1.0926084518432617
Batch 20/64 loss: -0.9298896789550781
Batch 21/64 loss: -0.9617109298706055
Batch 22/64 loss: -0.39434242248535156
Batch 23/64 loss: -0.9602165222167969
Batch 24/64 loss: -1.0533676147460938
Batch 25/64 loss: -1.211634635925293
Batch 26/64 loss: -0.5899667739868164
Batch 27/64 loss: -1.149160385131836
Batch 28/64 loss: -0.513545036315918
Batch 29/64 loss: -0.7059192657470703
Batch 30/64 loss: -1.1644277572631836
Batch 31/64 loss: -0.01061248779296875
Batch 32/64 loss: -0.4355497360229492
Batch 33/64 loss: -0.8540840148925781
Batch 34/64 loss: -0.9971780776977539
Batch 35/64 loss: -0.49948787689208984
Batch 36/64 loss: -1.212874412536621
Batch 37/64 loss: -0.6126174926757812
Batch 38/64 loss: -0.9293794631958008
Batch 39/64 loss: -0.8333921432495117
Batch 40/64 loss: -1.0470733642578125
Batch 41/64 loss: -0.6551904678344727
Batch 42/64 loss: -0.8825464248657227
Batch 43/64 loss: -0.624176025390625
Batch 44/64 loss: -0.8178005218505859
Batch 45/64 loss: -0.75823974609375
Batch 46/64 loss: -0.8231430053710938
Batch 47/64 loss: -0.6332197189331055
Batch 48/64 loss: -0.8753232955932617
Batch 49/64 loss: -1.0084476470947266
Batch 50/64 loss: -0.9985780715942383
Batch 51/64 loss: -0.6446437835693359
Batch 52/64 loss: -0.9564104080200195
Batch 53/64 loss: -0.29700136184692383
Batch 54/64 loss: -1.0963611602783203
Batch 55/64 loss: -0.9224853515625
Batch 56/64 loss: -1.1020927429199219
Batch 57/64 loss: -0.9164094924926758
Batch 58/64 loss: -0.8785228729248047
Batch 59/64 loss: -1.0356855392456055
Batch 60/64 loss: -0.9017257690429688
Batch 61/64 loss: -0.8398971557617188
Batch 62/64 loss: -1.0444021224975586
Batch 63/64 loss: -0.6654930114746094
Batch 64/64 loss: -5.123657703399658
Epoch 458  Train loss: -0.891544714160994  Val loss: -0.8435668814223247
Epoch 459
-------------------------------
Batch 1/64 loss: -0.8631305694580078
Batch 2/64 loss: -0.8567886352539062
Batch 3/64 loss: -1.0212469100952148
Batch 4/64 loss: -1.0921096801757812
Batch 5/64 loss: -0.9875097274780273
Batch 6/64 loss: -0.7006711959838867
Batch 7/64 loss: -0.8756275177001953
Batch 8/64 loss: -0.7896871566772461
Batch 9/64 loss: -0.9703216552734375
Batch 10/64 loss: -1.1871423721313477
Batch 11/64 loss: -0.8637571334838867
Batch 12/64 loss: -1.0676145553588867
Batch 13/64 loss: -1.1362476348876953
Batch 14/64 loss: -1.1247892379760742
Batch 15/64 loss: -0.723790168762207
Batch 16/64 loss: -1.1856536865234375
Batch 17/64 loss: -0.5053834915161133
Batch 18/64 loss: -0.6647825241088867
Batch 19/64 loss: -0.6271648406982422
Batch 20/64 loss: -0.8555212020874023
Batch 21/64 loss: -0.7761983871459961
Batch 22/64 loss: -1.0632877349853516
Batch 23/64 loss: -0.8347387313842773
Batch 24/64 loss: -0.9032354354858398
Batch 25/64 loss: -1.2295150756835938
Batch 26/64 loss: -0.9243736267089844
Batch 27/64 loss: -1.216792106628418
Batch 28/64 loss: -1.1050071716308594
Batch 29/64 loss: -1.2541189193725586
Batch 30/64 loss: -1.1779403686523438
Batch 31/64 loss: -1.2733535766601562
Batch 32/64 loss: -0.7262506484985352
Batch 33/64 loss: -0.7788429260253906
Batch 34/64 loss: -1.019953727722168
Batch 35/64 loss: -0.9779424667358398
Batch 36/64 loss: -0.9995651245117188
Batch 37/64 loss: -0.9410648345947266
Batch 38/64 loss: -0.7072086334228516
Batch 39/64 loss: -0.7646856307983398
Batch 40/64 loss: -1.1127920150756836
Batch 41/64 loss: -0.46042823791503906
Batch 42/64 loss: -0.6328153610229492
Batch 43/64 loss: -0.6643552780151367
Batch 44/64 loss: -0.7415742874145508
Batch 45/64 loss: -1.0200672149658203
Batch 46/64 loss: -0.8381948471069336
Batch 47/64 loss: -0.628692626953125
Batch 48/64 loss: -0.7524776458740234
Batch 49/64 loss: -0.09053611755371094
Batch 50/64 loss: -0.7463159561157227
Batch 51/64 loss: -1.0893754959106445
Batch 52/64 loss: -0.8368959426879883
Batch 53/64 loss: -0.8658838272094727
Batch 54/64 loss: -0.9008398056030273
Batch 55/64 loss: -0.30971479415893555
Batch 56/64 loss: -1.0937471389770508
Batch 57/64 loss: -0.5200643539428711
Batch 58/64 loss: -0.8717803955078125
Batch 59/64 loss: -0.5258512496948242
Batch 60/64 loss: -0.8075895309448242
Batch 61/64 loss: -0.6082925796508789
Batch 62/64 loss: -0.6998205184936523
Batch 63/64 loss: -0.5449705123901367
Batch 64/64 loss: -4.37932825088501
Epoch 459  Train loss: -0.9007150893117867  Val loss: -0.838238863600898
Epoch 460
-------------------------------
Batch 1/64 loss: -0.8530750274658203
Batch 2/64 loss: -0.9885225296020508
Batch 3/64 loss: -0.9156417846679688
Batch 4/64 loss: -0.6358966827392578
Batch 5/64 loss: -0.7344627380371094
Batch 6/64 loss: -0.8768453598022461
Batch 7/64 loss: -1.0392398834228516
Batch 8/64 loss: -0.19692134857177734
Batch 9/64 loss: -1.0451393127441406
Batch 10/64 loss: -1.1365032196044922
Batch 11/64 loss: -1.1538801193237305
Batch 12/64 loss: -0.9592761993408203
Batch 13/64 loss: -0.7240114212036133
Batch 14/64 loss: -0.6677465438842773
Batch 15/64 loss: -1.0497150421142578
Batch 16/64 loss: -1.0050315856933594
Batch 17/64 loss: -1.1151533126831055
Batch 18/64 loss: -0.8041315078735352
Batch 19/64 loss: -1.244248390197754
Batch 20/64 loss: -1.1448345184326172
Batch 21/64 loss: -0.7617549896240234
Batch 22/64 loss: -0.7951135635375977
Batch 23/64 loss: -1.0320472717285156
Batch 24/64 loss: -0.9251155853271484
Batch 25/64 loss: -0.8083038330078125
Batch 26/64 loss: -0.7610015869140625
Batch 27/64 loss: -1.063746452331543
Batch 28/64 loss: -0.9822187423706055
Batch 29/64 loss: -0.7631549835205078
Batch 30/64 loss: -0.7674751281738281
Batch 31/64 loss: -0.6302061080932617
Batch 32/64 loss: -0.971705436706543
Batch 33/64 loss: -0.8115167617797852
Batch 34/64 loss: -0.8616838455200195
Batch 35/64 loss: -0.7198200225830078
Batch 36/64 loss: -0.9238824844360352
Batch 37/64 loss: -1.082204818725586
Batch 38/64 loss: -0.7178916931152344
Batch 39/64 loss: -0.839390754699707
Batch 40/64 loss: -1.1553411483764648
Batch 41/64 loss: -0.8783178329467773
Batch 42/64 loss: -0.5696048736572266
Batch 43/64 loss: -0.7947854995727539
Batch 44/64 loss: -0.9828433990478516
Batch 45/64 loss: -0.39050865173339844
Batch 46/64 loss: -0.9779949188232422
Batch 47/64 loss: -0.046840667724609375
Batch 48/64 loss: -0.8877840042114258
Batch 49/64 loss: -1.020878791809082
Batch 50/64 loss: 0.051715850830078125
Batch 51/64 loss: -0.6399812698364258
Batch 52/64 loss: -1.0934572219848633
Batch 53/64 loss: -0.6873044967651367
Batch 54/64 loss: -0.4219627380371094
Batch 55/64 loss: -0.886225700378418
Batch 56/64 loss: -0.48462867736816406
Batch 57/64 loss: -0.8412876129150391
Batch 58/64 loss: -0.5578947067260742
Batch 59/64 loss: -0.864506721496582
Batch 60/64 loss: -0.9946260452270508
Batch 61/64 loss: -0.8237638473510742
Batch 62/64 loss: -1.128087043762207
Batch 63/64 loss: -0.6414499282836914
Batch 64/64 loss: -4.613914966583252
Epoch 460  Train loss: -0.8735263805763395  Val loss: -0.8063825626963192
Epoch 461
-------------------------------
Batch 1/64 loss: -0.7727422714233398
Batch 2/64 loss: -1.0619449615478516
Batch 3/64 loss: -0.6670627593994141
Batch 4/64 loss: -0.5336503982543945
Batch 5/64 loss: -0.798396110534668
Batch 6/64 loss: -0.9891805648803711
Batch 7/64 loss: -0.7825117111206055
Batch 8/64 loss: -0.8437566757202148
Batch 9/64 loss: -1.125472068786621
Batch 10/64 loss: -0.35515594482421875
Batch 11/64 loss: -0.9162282943725586
Batch 12/64 loss: -0.7431879043579102
Batch 13/64 loss: -0.829371452331543
Batch 14/64 loss: -0.8352146148681641
Batch 15/64 loss: -0.8702630996704102
Batch 16/64 loss: -1.065455436706543
Batch 17/64 loss: -0.6538057327270508
Batch 18/64 loss: -0.9570608139038086
Batch 19/64 loss: -0.7152671813964844
Batch 20/64 loss: -0.8789100646972656
Batch 21/64 loss: -0.7732048034667969
Batch 22/64 loss: -0.792780876159668
Batch 23/64 loss: -0.4425983428955078
Batch 24/64 loss: -1.054539680480957
Batch 25/64 loss: -1.0536603927612305
Batch 26/64 loss: -0.7621135711669922
Batch 27/64 loss: -0.9102792739868164
Batch 28/64 loss: -1.074934959411621
Batch 29/64 loss: -0.7576284408569336
Batch 30/64 loss: -0.34093761444091797
Batch 31/64 loss: -0.6056852340698242
Batch 32/64 loss: -1.0393800735473633
Batch 33/64 loss: -0.7821741104125977
Batch 34/64 loss: -0.6022872924804688
Batch 35/64 loss: -1.0288400650024414
Batch 36/64 loss: -1.1454181671142578
Batch 37/64 loss: -0.7743053436279297
Batch 38/64 loss: -0.8967428207397461
Batch 39/64 loss: -1.0487051010131836
Batch 40/64 loss: -1.0038385391235352
Batch 41/64 loss: -0.9547529220581055
Batch 42/64 loss: -0.9961004257202148
Batch 43/64 loss: -0.9275960922241211
Batch 44/64 loss: -0.7253484725952148
Batch 45/64 loss: -1.0767383575439453
Batch 46/64 loss: -0.8682775497436523
Batch 47/64 loss: -0.7889318466186523
Batch 48/64 loss: -1.1004142761230469
Batch 49/64 loss: -0.7858591079711914
Batch 50/64 loss: -0.8979597091674805
Batch 51/64 loss: -0.8600263595581055
Batch 52/64 loss: -0.9709005355834961
Batch 53/64 loss: 0.18009662628173828
Batch 54/64 loss: -0.8278694152832031
Batch 55/64 loss: -0.8414478302001953
Batch 56/64 loss: -0.9183731079101562
Batch 57/64 loss: -1.0496625900268555
Batch 58/64 loss: -0.9662570953369141
Batch 59/64 loss: -0.6746759414672852
Batch 60/64 loss: -0.5404739379882812
Batch 61/64 loss: -0.7775487899780273
Batch 62/64 loss: -0.7401390075683594
Batch 63/64 loss: -0.8749799728393555
Batch 64/64 loss: -4.71970796585083
Epoch 461  Train loss: -0.875430752249325  Val loss: -0.8609187988071507
Epoch 462
-------------------------------
Batch 1/64 loss: -0.9496278762817383
Batch 2/64 loss: -0.5306568145751953
Batch 3/64 loss: -0.3819303512573242
Batch 4/64 loss: -0.4989461898803711
Batch 5/64 loss: -0.9582986831665039
Batch 6/64 loss: -0.9392547607421875
Batch 7/64 loss: -1.1284685134887695
Batch 8/64 loss: -1.045522689819336
Batch 9/64 loss: -1.1258668899536133
Batch 10/64 loss: -0.5083141326904297
Batch 11/64 loss: -0.9934768676757812
Batch 12/64 loss: -0.7952728271484375
Batch 13/64 loss: -0.7342252731323242
Batch 14/64 loss: -1.1472349166870117
Batch 15/64 loss: -0.5284862518310547
Batch 16/64 loss: -0.7779483795166016
Batch 17/64 loss: -0.41736793518066406
Batch 18/64 loss: -1.2340774536132812
Batch 19/64 loss: -1.0076684951782227
Batch 20/64 loss: -0.22624778747558594
Batch 21/64 loss: -0.8336753845214844
Batch 22/64 loss: -0.6944551467895508
Batch 23/64 loss: -0.7563629150390625
Batch 24/64 loss: -0.8171157836914062
Batch 25/64 loss: -1.0319833755493164
Batch 26/64 loss: -0.8610801696777344
Batch 27/64 loss: -1.0573453903198242
Batch 28/64 loss: -0.9681615829467773
Batch 29/64 loss: -0.7330455780029297
Batch 30/64 loss: -0.8262672424316406
Batch 31/64 loss: -1.0780515670776367
Batch 32/64 loss: -0.8052301406860352
Batch 33/64 loss: -0.9814968109130859
Batch 34/64 loss: -0.8664474487304688
Batch 35/64 loss: -0.9104337692260742
Batch 36/64 loss: -1.0028762817382812
Batch 37/64 loss: -0.710479736328125
Batch 38/64 loss: -0.5708227157592773
Batch 39/64 loss: -0.6559247970581055
Batch 40/64 loss: -0.9886875152587891
Batch 41/64 loss: -1.1149635314941406
Batch 42/64 loss: -1.0376262664794922
Batch 43/64 loss: -0.7053823471069336
Batch 44/64 loss: -1.0006790161132812
Batch 45/64 loss: -0.904876708984375
Batch 46/64 loss: -1.0483169555664062
Batch 47/64 loss: -1.0776948928833008
Batch 48/64 loss: -0.9426984786987305
Batch 49/64 loss: -1.0759544372558594
Batch 50/64 loss: -0.6564245223999023
Batch 51/64 loss: -1.1544609069824219
Batch 52/64 loss: -1.124746322631836
Batch 53/64 loss: -0.9822721481323242
Batch 54/64 loss: -0.9653596878051758
Batch 55/64 loss: -0.9291963577270508
Batch 56/64 loss: -0.6581020355224609
Batch 57/64 loss: -0.7875137329101562
Batch 58/64 loss: -0.6366024017333984
Batch 59/64 loss: -0.7906208038330078
Batch 60/64 loss: -1.1235942840576172
Batch 61/64 loss: -0.9162454605102539
Batch 62/64 loss: -0.6052522659301758
Batch 63/64 loss: -0.5109024047851562
Batch 64/64 loss: -4.662029266357422
Epoch 462  Train loss: -0.8992132448682598  Val loss: -0.9505554933318567
Epoch 463
-------------------------------
Batch 1/64 loss: -0.8532133102416992
Batch 2/64 loss: -0.46338748931884766
Batch 3/64 loss: -0.9607324600219727
Batch 4/64 loss: -0.8213930130004883
Batch 5/64 loss: -0.9771127700805664
Batch 6/64 loss: -0.6891679763793945
Batch 7/64 loss: -0.5544891357421875
Batch 8/64 loss: -0.6380748748779297
Batch 9/64 loss: -0.8907651901245117
Batch 10/64 loss: -1.018183708190918
Batch 11/64 loss: -0.8024349212646484
Batch 12/64 loss: -1.1247978210449219
Batch 13/64 loss: -0.9568004608154297
Batch 14/64 loss: -0.7468490600585938
Batch 15/64 loss: -1.0608272552490234
Batch 16/64 loss: -0.9146242141723633
Batch 17/64 loss: -0.958338737487793
Batch 18/64 loss: -0.8784618377685547
Batch 19/64 loss: -0.5382766723632812
Batch 20/64 loss: -0.6606569290161133
Batch 21/64 loss: -0.7014741897583008
Batch 22/64 loss: -1.0907440185546875
Batch 23/64 loss: -0.9358968734741211
Batch 24/64 loss: -1.055558204650879
Batch 25/64 loss: -1.1219940185546875
Batch 26/64 loss: -0.8740739822387695
Batch 27/64 loss: -1.0494909286499023
Batch 28/64 loss: -0.9889068603515625
Batch 29/64 loss: -1.1071500778198242
Batch 30/64 loss: -0.8472108840942383
Batch 31/64 loss: -1.0055513381958008
Batch 32/64 loss: -0.6590900421142578
Batch 33/64 loss: -1.0016860961914062
Batch 34/64 loss: -1.0949535369873047
Batch 35/64 loss: -1.0127792358398438
Batch 36/64 loss: -0.8989953994750977
Batch 37/64 loss: -0.8891115188598633
Batch 38/64 loss: -0.707341194152832
Batch 39/64 loss: -0.8610925674438477
Batch 40/64 loss: -0.9805498123168945
Batch 41/64 loss: -0.8720169067382812
Batch 42/64 loss: -0.953338623046875
Batch 43/64 loss: -0.5175380706787109
Batch 44/64 loss: -0.8305187225341797
Batch 45/64 loss: -0.6590061187744141
Batch 46/64 loss: -1.265878677368164
Batch 47/64 loss: -0.5004253387451172
Batch 48/64 loss: -0.6202430725097656
Batch 49/64 loss: -0.5902185440063477
Batch 50/64 loss: -0.8500461578369141
Batch 51/64 loss: -0.9944334030151367
Batch 52/64 loss: -0.8215389251708984
Batch 53/64 loss: -0.7835760116577148
Batch 54/64 loss: -0.6184101104736328
Batch 55/64 loss: -0.8182592391967773
Batch 56/64 loss: -0.7389802932739258
Batch 57/64 loss: -0.8200130462646484
Batch 58/64 loss: -0.8945722579956055
Batch 59/64 loss: -0.9397754669189453
Batch 60/64 loss: -1.0640783309936523
Batch 61/64 loss: -0.7593412399291992
Batch 62/64 loss: -1.1825122833251953
Batch 63/64 loss: -1.1431398391723633
Batch 64/64 loss: -4.077589988708496
Epoch 463  Train loss: -0.9049143809898227  Val loss: -0.8598867724441581
Epoch 464
-------------------------------
Batch 1/64 loss: -0.3801078796386719
Batch 2/64 loss: -0.8734025955200195
Batch 3/64 loss: -0.912785530090332
Batch 4/64 loss: -0.6849880218505859
Batch 5/64 loss: -0.935638427734375
Batch 6/64 loss: -1.0209836959838867
Batch 7/64 loss: -0.8019065856933594
Batch 8/64 loss: -0.7585659027099609
Batch 9/64 loss: -0.7891244888305664
Batch 10/64 loss: -0.7417154312133789
Batch 11/64 loss: -0.7190876007080078
Batch 12/64 loss: -0.8209934234619141
Batch 13/64 loss: -0.8664207458496094
Batch 14/64 loss: -0.9071807861328125
Batch 15/64 loss: -1.0298337936401367
Batch 16/64 loss: -0.6818723678588867
Batch 17/64 loss: -1.0346136093139648
Batch 18/64 loss: -0.8771495819091797
Batch 19/64 loss: -0.7731876373291016
Batch 20/64 loss: -0.9838638305664062
Batch 21/64 loss: -0.9188680648803711
Batch 22/64 loss: -1.0082635879516602
Batch 23/64 loss: -0.9626665115356445
Batch 24/64 loss: -1.2119779586791992
Batch 25/64 loss: -0.6951713562011719
Batch 26/64 loss: -1.1633033752441406
Batch 27/64 loss: -0.8866786956787109
Batch 28/64 loss: -1.1323270797729492
Batch 29/64 loss: -0.7921314239501953
Batch 30/64 loss: -0.7213706970214844
Batch 31/64 loss: -0.8806362152099609
Batch 32/64 loss: -0.8277645111083984
Batch 33/64 loss: -0.9190483093261719
Batch 34/64 loss: -0.7359085083007812
Batch 35/64 loss: -0.7254505157470703
Batch 36/64 loss: -1.0555620193481445
Batch 37/64 loss: -0.9810104370117188
Batch 38/64 loss: -1.0789003372192383
Batch 39/64 loss: -0.4501533508300781
Batch 40/64 loss: -1.3084383010864258
Batch 41/64 loss: -0.9298276901245117
Batch 42/64 loss: -0.775787353515625
Batch 43/64 loss: -0.5531454086303711
Batch 44/64 loss: -0.8102998733520508
Batch 45/64 loss: -0.8202848434448242
Batch 46/64 loss: -1.2195587158203125
Batch 47/64 loss: -0.9505329132080078
Batch 48/64 loss: -0.8533258438110352
Batch 49/64 loss: -0.8361682891845703
Batch 50/64 loss: -0.9207162857055664
Batch 51/64 loss: -1.0588207244873047
Batch 52/64 loss: -0.9763374328613281
Batch 53/64 loss: -0.4420757293701172
Batch 54/64 loss: -1.1722211837768555
Batch 55/64 loss: -0.7617044448852539
Batch 56/64 loss: -1.2125539779663086
Batch 57/64 loss: -0.8970365524291992
Batch 58/64 loss: -1.1423721313476562
Batch 59/64 loss: -0.4322357177734375
Batch 60/64 loss: -1.3112897872924805
Batch 61/64 loss: -0.9510002136230469
Batch 62/64 loss: -0.8929910659790039
Batch 63/64 loss: -0.4746561050415039
Batch 64/64 loss: -4.921863079071045
Epoch 464  Train loss: -0.9276140044717228  Val loss: -0.9275411494409096
Epoch 465
-------------------------------
Batch 1/64 loss: -0.5910663604736328
Batch 2/64 loss: -0.7535648345947266
Batch 3/64 loss: -1.053788185119629
Batch 4/64 loss: -1.0619144439697266
Batch 5/64 loss: -0.7888364791870117
Batch 6/64 loss: -1.1083259582519531
Batch 7/64 loss: -0.8296518325805664
Batch 8/64 loss: -0.6805524826049805
Batch 9/64 loss: -0.8407888412475586
Batch 10/64 loss: -0.9440803527832031
Batch 11/64 loss: -0.5295495986938477
Batch 12/64 loss: -0.5497035980224609
Batch 13/64 loss: -0.9287643432617188
Batch 14/64 loss: -0.7463083267211914
Batch 15/64 loss: -0.8233966827392578
Batch 16/64 loss: -0.36260414123535156
Batch 17/64 loss: -0.9329204559326172
Batch 18/64 loss: -0.8572902679443359
Batch 19/64 loss: -0.6640090942382812
Batch 20/64 loss: -1.048029899597168
Batch 21/64 loss: -1.1319503784179688
Batch 22/64 loss: -0.8273506164550781
Batch 23/64 loss: -0.40273475646972656
Batch 24/64 loss: -0.7145614624023438
Batch 25/64 loss: -0.937739372253418
Batch 26/64 loss: -0.9134807586669922
Batch 27/64 loss: -0.9954833984375
Batch 28/64 loss: -1.073044776916504
Batch 29/64 loss: -0.8744821548461914
Batch 30/64 loss: -0.6427431106567383
Batch 31/64 loss: -0.945220947265625
Batch 32/64 loss: -0.8317937850952148
Batch 33/64 loss: -1.0902719497680664
Batch 34/64 loss: -0.969752311706543
Batch 35/64 loss: -0.9266557693481445
Batch 36/64 loss: -0.9856300354003906
Batch 37/64 loss: -0.3488636016845703
Batch 38/64 loss: -0.9884614944458008
Batch 39/64 loss: -0.7348814010620117
Batch 40/64 loss: -0.6861238479614258
Batch 41/64 loss: -0.9331750869750977
Batch 42/64 loss: -1.013132095336914
Batch 43/64 loss: -0.4764232635498047
Batch 44/64 loss: -1.208806037902832
Batch 45/64 loss: -0.8031120300292969
Batch 46/64 loss: -1.2693662643432617
Batch 47/64 loss: -0.7299737930297852
Batch 48/64 loss: -1.0598735809326172
Batch 49/64 loss: -0.6016197204589844
Batch 50/64 loss: -0.6152811050415039
Batch 51/64 loss: -1.140066146850586
Batch 52/64 loss: -0.8691043853759766
Batch 53/64 loss: -1.2347230911254883
Batch 54/64 loss: -0.8324356079101562
Batch 55/64 loss: -1.0923395156860352
Batch 56/64 loss: -0.9978218078613281
Batch 57/64 loss: -1.1587152481079102
Batch 58/64 loss: -0.8912057876586914
Batch 59/64 loss: -0.8926219940185547
Batch 60/64 loss: -1.0174989700317383
Batch 61/64 loss: -0.6552362442016602
Batch 62/64 loss: -1.1749544143676758
Batch 63/64 loss: -0.8498439788818359
Batch 64/64 loss: -5.092792510986328
Epoch 465  Train loss: -0.916914457433364  Val loss: -1.0098028281300337
Saving best model, epoch: 465
Epoch 466
-------------------------------
Batch 1/64 loss: -0.535344123840332
Batch 2/64 loss: -0.8671159744262695
Batch 3/64 loss: -0.7416772842407227
Batch 4/64 loss: -1.083475112915039
Batch 5/64 loss: -1.0166559219360352
Batch 6/64 loss: -0.8297691345214844
Batch 7/64 loss: -0.8418636322021484
Batch 8/64 loss: -1.0749931335449219
Batch 9/64 loss: -0.9902753829956055
Batch 10/64 loss: -0.9351444244384766
Batch 11/64 loss: -0.8892612457275391
Batch 12/64 loss: -1.0063915252685547
Batch 13/64 loss: -0.6938762664794922
Batch 14/64 loss: -0.9417352676391602
Batch 15/64 loss: -1.1153497695922852
Batch 16/64 loss: -0.5013065338134766
Batch 17/64 loss: -0.7101373672485352
Batch 18/64 loss: -0.9811258316040039
Batch 19/64 loss: -0.3030118942260742
Batch 20/64 loss: -1.134836196899414
Batch 21/64 loss: -0.6994705200195312
Batch 22/64 loss: -0.8890438079833984
Batch 23/64 loss: -1.0952768325805664
Batch 24/64 loss: -0.8763704299926758
Batch 25/64 loss: -1.0650033950805664
Batch 26/64 loss: -0.6948394775390625
Batch 27/64 loss: -0.7382793426513672
Batch 28/64 loss: -0.8386564254760742
Batch 29/64 loss: -1.109450340270996
Batch 30/64 loss: -1.1107492446899414
Batch 31/64 loss: -0.8832235336303711
Batch 32/64 loss: -0.8075265884399414
Batch 33/64 loss: -0.9122867584228516
Batch 34/64 loss: -0.9241752624511719
Batch 35/64 loss: -0.599390983581543
Batch 36/64 loss: -0.8274078369140625
Batch 37/64 loss: -0.96551513671875
Batch 38/64 loss: -0.9200506210327148
Batch 39/64 loss: -1.2149019241333008
Batch 40/64 loss: -0.9582710266113281
Batch 41/64 loss: -0.6358060836791992
Batch 42/64 loss: -1.0219545364379883
Batch 43/64 loss: -0.537236213684082
Batch 44/64 loss: -0.8532695770263672
Batch 45/64 loss: -0.9225625991821289
Batch 46/64 loss: -0.9945707321166992
Batch 47/64 loss: -0.8775157928466797
Batch 48/64 loss: 0.009546279907226562
Batch 49/64 loss: -0.9055347442626953
Batch 50/64 loss: -0.9491024017333984
Batch 51/64 loss: -0.4600677490234375
Batch 52/64 loss: -0.8947429656982422
Batch 53/64 loss: -1.0042381286621094
Batch 54/64 loss: -0.8757801055908203
Batch 55/64 loss: -1.0331144332885742
Batch 56/64 loss: -0.3222188949584961
Batch 57/64 loss: -0.6262531280517578
Batch 58/64 loss: -0.778569221496582
Batch 59/64 loss: -0.8826980590820312
Batch 60/64 loss: -1.213552474975586
Batch 61/64 loss: -1.0183048248291016
Batch 62/64 loss: -1.1312665939331055
Batch 63/64 loss: -0.846684455871582
Batch 64/64 loss: -4.978760719299316
Epoch 466  Train loss: -0.9071812311808268  Val loss: -0.8587394530830514
Epoch 467
-------------------------------
Batch 1/64 loss: -1.0590896606445312
Batch 2/64 loss: -0.3094606399536133
Batch 3/64 loss: -0.5160360336303711
Batch 4/64 loss: -0.7918491363525391
Batch 5/64 loss: -0.6088438034057617
Batch 6/64 loss: -0.7156238555908203
Batch 7/64 loss: -1.1482963562011719
Batch 8/64 loss: -1.0564937591552734
Batch 9/64 loss: -0.8006095886230469
Batch 10/64 loss: -0.8932037353515625
Batch 11/64 loss: -0.5635929107666016
Batch 12/64 loss: -0.2784748077392578
Batch 13/64 loss: -1.0107784271240234
Batch 14/64 loss: -0.31854915618896484
Batch 15/64 loss: -0.4654569625854492
Batch 16/64 loss: -0.8988866806030273
Batch 17/64 loss: -1.2099485397338867
Batch 18/64 loss: -1.127671241760254
Batch 19/64 loss: -1.2669620513916016
Batch 20/64 loss: -0.26276588439941406
Batch 21/64 loss: -1.1820869445800781
Batch 22/64 loss: -1.2472457885742188
Batch 23/64 loss: -0.7269229888916016
Batch 24/64 loss: -0.664362907409668
Batch 25/64 loss: -0.9174861907958984
Batch 26/64 loss: -1.1659460067749023
Batch 27/64 loss: -1.0570344924926758
Batch 28/64 loss: -0.9919214248657227
Batch 29/64 loss: -1.0691499710083008
Batch 30/64 loss: -1.1392955780029297
Batch 31/64 loss: -1.0816268920898438
Batch 32/64 loss: -0.9194393157958984
Batch 33/64 loss: -0.7652931213378906
Batch 34/64 loss: -0.9996175765991211
Batch 35/64 loss: -1.1004505157470703
Batch 36/64 loss: -0.7825527191162109
Batch 37/64 loss: -1.1109132766723633
Batch 38/64 loss: -0.5125465393066406
Batch 39/64 loss: -1.01251220703125
Batch 40/64 loss: -0.9555692672729492
Batch 41/64 loss: -0.9774198532104492
Batch 42/64 loss: -0.6340799331665039
Batch 43/64 loss: -0.8599929809570312
Batch 44/64 loss: -0.9760942459106445
Batch 45/64 loss: -1.109297752380371
Batch 46/64 loss: -0.9334144592285156
Batch 47/64 loss: -0.9144630432128906
Batch 48/64 loss: -1.0990676879882812
Batch 49/64 loss: -0.8871297836303711
Batch 50/64 loss: -0.7994060516357422
Batch 51/64 loss: -0.7607746124267578
Batch 52/64 loss: -1.0045757293701172
Batch 53/64 loss: -0.9222049713134766
Batch 54/64 loss: -0.9764509201049805
Batch 55/64 loss: -1.1639842987060547
Batch 56/64 loss: -0.8248634338378906
Batch 57/64 loss: -1.03118896484375
Batch 58/64 loss: -1.1759490966796875
Batch 59/64 loss: -0.8875246047973633
Batch 60/64 loss: -0.959075927734375
Batch 61/64 loss: -0.955164909362793
Batch 62/64 loss: -0.7798910140991211
Batch 63/64 loss: -1.0663118362426758
Batch 64/64 loss: -5.2360687255859375
Epoch 467  Train loss: -0.9463520723230698  Val loss: -0.8832752581724187
Epoch 468
-------------------------------
Batch 1/64 loss: -0.9732160568237305
Batch 2/64 loss: -1.0077886581420898
Batch 3/64 loss: -1.0704689025878906
Batch 4/64 loss: -0.6593732833862305
Batch 5/64 loss: -0.8315496444702148
Batch 6/64 loss: -0.7816324234008789
Batch 7/64 loss: -1.0578031539916992
Batch 8/64 loss: -1.1083555221557617
Batch 9/64 loss: -0.7476291656494141
Batch 10/64 loss: -0.580348014831543
Batch 11/64 loss: -0.8697261810302734
Batch 12/64 loss: -0.47212886810302734
Batch 13/64 loss: -0.6660861968994141
Batch 14/64 loss: -1.1867094039916992
Batch 15/64 loss: -0.8497476577758789
Batch 16/64 loss: -1.1609935760498047
Batch 17/64 loss: -1.0397281646728516
Batch 18/64 loss: -0.9953699111938477
Batch 19/64 loss: -0.8869848251342773
Batch 20/64 loss: -0.6198034286499023
Batch 21/64 loss: -1.0369787216186523
Batch 22/64 loss: -1.1937522888183594
Batch 23/64 loss: -0.9446172714233398
Batch 24/64 loss: -1.0549383163452148
Batch 25/64 loss: -0.831425666809082
Batch 26/64 loss: -1.0553092956542969
Batch 27/64 loss: -0.6547298431396484
Batch 28/64 loss: -1.1460552215576172
Batch 29/64 loss: -1.0270557403564453
Batch 30/64 loss: -0.8405094146728516
Batch 31/64 loss: -1.1417455673217773
Batch 32/64 loss: -0.6721630096435547
Batch 33/64 loss: -0.6395120620727539
Batch 34/64 loss: -0.9623517990112305
Batch 35/64 loss: -0.7086935043334961
Batch 36/64 loss: -0.7124338150024414
Batch 37/64 loss: -0.9235916137695312
Batch 38/64 loss: -0.8659162521362305
Batch 39/64 loss: -0.7170324325561523
Batch 40/64 loss: -1.1722040176391602
Batch 41/64 loss: -0.9077787399291992
Batch 42/64 loss: -0.8178930282592773
Batch 43/64 loss: -0.9361257553100586
Batch 44/64 loss: -1.1657915115356445
Batch 45/64 loss: -1.1282787322998047
Batch 46/64 loss: -0.7808561325073242
Batch 47/64 loss: -0.9486207962036133
Batch 48/64 loss: -0.8012170791625977
Batch 49/64 loss: -0.8891897201538086
Batch 50/64 loss: -0.1094970703125
Batch 51/64 loss: -0.84521484375
Batch 52/64 loss: -0.9128532409667969
Batch 53/64 loss: -0.9884719848632812
Batch 54/64 loss: -1.0951509475708008
Batch 55/64 loss: -0.36305809020996094
Batch 56/64 loss: -0.1750946044921875
Batch 57/64 loss: -0.5661354064941406
Batch 58/64 loss: -0.7948989868164062
Batch 59/64 loss: -1.0131282806396484
Batch 60/64 loss: -0.5497970581054688
Batch 61/64 loss: -0.9461803436279297
Batch 62/64 loss: -0.9408130645751953
Batch 63/64 loss: -0.8567228317260742
Batch 64/64 loss: -4.231584548950195
Epoch 468  Train loss: -0.9031045577105354  Val loss: -0.9166387053289774
Epoch 469
-------------------------------
Batch 1/64 loss: -1.0509624481201172
Batch 2/64 loss: -0.7413902282714844
Batch 3/64 loss: -0.8915309906005859
Batch 4/64 loss: -0.920649528503418
Batch 5/64 loss: -0.8937005996704102
Batch 6/64 loss: -1.0324583053588867
Batch 7/64 loss: -0.8515691757202148
Batch 8/64 loss: -0.9142284393310547
Batch 9/64 loss: -0.7962856292724609
Batch 10/64 loss: -0.8197212219238281
Batch 11/64 loss: -0.7283258438110352
Batch 12/64 loss: -0.5973129272460938
Batch 13/64 loss: -0.9417972564697266
Batch 14/64 loss: -0.7631998062133789
Batch 15/64 loss: -0.9640626907348633
Batch 16/64 loss: -0.9636268615722656
Batch 17/64 loss: -1.054952621459961
Batch 18/64 loss: -0.8228664398193359
Batch 19/64 loss: -1.1613578796386719
Batch 20/64 loss: -0.240997314453125
Batch 21/64 loss: -0.7205514907836914
Batch 22/64 loss: -0.5486993789672852
Batch 23/64 loss: -1.1160268783569336
Batch 24/64 loss: -0.6886100769042969
Batch 25/64 loss: -0.9172267913818359
Batch 26/64 loss: -0.6411294937133789
Batch 27/64 loss: -1.2186574935913086
Batch 28/64 loss: -0.5825920104980469
Batch 29/64 loss: -0.6403417587280273
Batch 30/64 loss: -0.953669548034668
Batch 31/64 loss: -0.7773847579956055
Batch 32/64 loss: -0.9859085083007812
Batch 33/64 loss: -1.286980152130127
Batch 34/64 loss: -0.37771034240722656
Batch 35/64 loss: -0.6021251678466797
Batch 36/64 loss: -0.5828952789306641
Batch 37/64 loss: -0.77508544921875
Batch 38/64 loss: -0.5061216354370117
Batch 39/64 loss: -0.7461414337158203
Batch 40/64 loss: -0.8367404937744141
Batch 41/64 loss: -0.7040958404541016
Batch 42/64 loss: -0.8285665512084961
Batch 43/64 loss: -0.5186710357666016
Batch 44/64 loss: -0.9599580764770508
Batch 45/64 loss: -0.9303760528564453
Batch 46/64 loss: -0.523625373840332
Batch 47/64 loss: -0.9648370742797852
Batch 48/64 loss: -0.6149673461914062
Batch 49/64 loss: -0.836604118347168
Batch 50/64 loss: -0.8574771881103516
Batch 51/64 loss: -0.6061811447143555
Batch 52/64 loss: -0.9576377868652344
Batch 53/64 loss: -0.8100461959838867
Batch 54/64 loss: -0.9709692001342773
Batch 55/64 loss: -0.5411577224731445
Batch 56/64 loss: -0.7632341384887695
Batch 57/64 loss: -1.101527214050293
Batch 58/64 loss: -0.9837045669555664
Batch 59/64 loss: -1.1566314697265625
Batch 60/64 loss: -1.2069692611694336
Batch 61/64 loss: -0.6744327545166016
Batch 62/64 loss: -0.9848718643188477
Batch 63/64 loss: -0.8696966171264648
Batch 64/64 loss: -4.802186965942383
Epoch 469  Train loss: -0.8725250692928539  Val loss: -0.9887647989279625
Epoch 470
-------------------------------
Batch 1/64 loss: -0.9089956283569336
Batch 2/64 loss: -0.8370323181152344
Batch 3/64 loss: -0.7855014801025391
Batch 4/64 loss: -1.0351953506469727
Batch 5/64 loss: -0.9141225814819336
Batch 6/64 loss: -1.075775146484375
Batch 7/64 loss: -0.8744029998779297
Batch 8/64 loss: -0.8107051849365234
Batch 9/64 loss: -0.9362831115722656
Batch 10/64 loss: -0.9124078750610352
Batch 11/64 loss: -0.5824174880981445
Batch 12/64 loss: -1.1039104461669922
Batch 13/64 loss: -0.545802116394043
Batch 14/64 loss: -0.7541341781616211
Batch 15/64 loss: -0.978236198425293
Batch 16/64 loss: -1.0121994018554688
Batch 17/64 loss: -0.7827043533325195
Batch 18/64 loss: -0.9843254089355469
Batch 19/64 loss: -0.8654670715332031
Batch 20/64 loss: -0.9066562652587891
Batch 21/64 loss: -1.3097782135009766
Batch 22/64 loss: -1.0981636047363281
Batch 23/64 loss: -0.8452892303466797
Batch 24/64 loss: -0.7499532699584961
Batch 25/64 loss: -0.8496789932250977
Batch 26/64 loss: -1.119314193725586
Batch 27/64 loss: -0.749842643737793
Batch 28/64 loss: -1.0750360488891602
Batch 29/64 loss: -0.3088960647583008
Batch 30/64 loss: -1.0505495071411133
Batch 31/64 loss: -0.1794109344482422
Batch 32/64 loss: -0.4885826110839844
Batch 33/64 loss: -0.7193574905395508
Batch 34/64 loss: -1.1552772521972656
Batch 35/64 loss: -0.9547863006591797
Batch 36/64 loss: -0.8629026412963867
Batch 37/64 loss: -0.7922258377075195
Batch 38/64 loss: -0.5490732192993164
Batch 39/64 loss: -0.9057760238647461
Batch 40/64 loss: -0.6190757751464844
Batch 41/64 loss: -0.9679861068725586
Batch 42/64 loss: -0.6543421745300293
Batch 43/64 loss: -0.8574028015136719
Batch 44/64 loss: -0.8150773048400879
Batch 45/64 loss: -1.0510892868041992
Batch 46/64 loss: -0.7139186859130859
Batch 47/64 loss: -0.6425142288208008
Batch 48/64 loss: -0.8245773315429688
Batch 49/64 loss: -0.6150875091552734
Batch 50/64 loss: -0.5969333648681641
Batch 51/64 loss: -0.7371788024902344
Batch 52/64 loss: -0.11394929885864258
Batch 53/64 loss: -0.5049619674682617
Batch 54/64 loss: -0.8319177627563477
Batch 55/64 loss: -0.6016674041748047
Batch 56/64 loss: -0.6761541366577148
Batch 57/64 loss: -1.0408573150634766
Batch 58/64 loss: -0.9675273895263672
Batch 59/64 loss: -0.7365865707397461
Batch 60/64 loss: -0.7051401138305664
Batch 61/64 loss: -0.5391445159912109
Batch 62/64 loss: -0.87237548828125
Batch 63/64 loss: -1.123464584350586
Batch 64/64 loss: -4.6054840087890625
Epoch 470  Train loss: -0.8569915547090419  Val loss: -0.9183253914220226
Epoch 471
-------------------------------
Batch 1/64 loss: -0.6958379745483398
Batch 2/64 loss: -0.9793415069580078
Batch 3/64 loss: -0.6265096664428711
Batch 4/64 loss: -0.5900154113769531
Batch 5/64 loss: -0.3546323776245117
Batch 6/64 loss: -1.1207351684570312
Batch 7/64 loss: -0.7160720825195312
Batch 8/64 loss: -0.9287557601928711
Batch 9/64 loss: -0.8865690231323242
Batch 10/64 loss: -0.907954216003418
Batch 11/64 loss: -0.9005918502807617
Batch 12/64 loss: -0.7387981414794922
Batch 13/64 loss: -0.4747161865234375
Batch 14/64 loss: -1.0213251113891602
Batch 15/64 loss: -0.9041996002197266
Batch 16/64 loss: -0.7787675857543945
Batch 17/64 loss: -0.9078702926635742
Batch 18/64 loss: -0.8263034820556641
Batch 19/64 loss: -0.31255531311035156
Batch 20/64 loss: -1.056304931640625
Batch 21/64 loss: -1.1079092025756836
Batch 22/64 loss: -0.5886983871459961
Batch 23/64 loss: -0.8434581756591797
Batch 24/64 loss: -0.9454975128173828
Batch 25/64 loss: -0.7750730514526367
Batch 26/64 loss: -0.839228630065918
Batch 27/64 loss: -0.5526423454284668
Batch 28/64 loss: -1.1212310791015625
Batch 29/64 loss: -1.2968063354492188
Batch 30/64 loss: -0.8666114807128906
Batch 31/64 loss: -1.0283870697021484
Batch 32/64 loss: -1.020127296447754
Batch 33/64 loss: -1.1606416702270508
Batch 34/64 loss: -1.0361909866333008
Batch 35/64 loss: -1.0415382385253906
Batch 36/64 loss: -0.7972536087036133
Batch 37/64 loss: -1.0681352615356445
Batch 38/64 loss: -0.8643884658813477
Batch 39/64 loss: -0.24569988250732422
Batch 40/64 loss: -0.9378604888916016
Batch 41/64 loss: -1.0059089660644531
Batch 42/64 loss: -1.127187728881836
Batch 43/64 loss: -0.976658821105957
Batch 44/64 loss: -0.6488742828369141
Batch 45/64 loss: -0.9271421432495117
Batch 46/64 loss: -0.9284963607788086
Batch 47/64 loss: -0.7043666839599609
Batch 48/64 loss: -0.7437953948974609
Batch 49/64 loss: -0.7913942337036133
Batch 50/64 loss: -0.9109535217285156
Batch 51/64 loss: -0.34190845489501953
Batch 52/64 loss: -0.36748600006103516
Batch 53/64 loss: -0.6388072967529297
Batch 54/64 loss: -0.7342605590820312
Batch 55/64 loss: -0.6599674224853516
Batch 56/64 loss: -0.6454000473022461
Batch 57/64 loss: -0.6490116119384766
Batch 58/64 loss: -0.5882902145385742
Batch 59/64 loss: -0.7157831192016602
Batch 60/64 loss: -0.5212974548339844
Batch 61/64 loss: -0.5527763366699219
Batch 62/64 loss: -0.5575037002563477
Batch 63/64 loss: -1.042088508605957
Batch 64/64 loss: -4.586475372314453
Epoch 471  Train loss: -0.8483835332533892  Val loss: -0.8061678384997181
Epoch 472
-------------------------------
Batch 1/64 loss: -0.7278728485107422
Batch 2/64 loss: -0.9378423690795898
Batch 3/64 loss: -0.7889585494995117
Batch 4/64 loss: -0.9744424819946289
Batch 5/64 loss: -1.0634937286376953
Batch 6/64 loss: -0.8529033660888672
Batch 7/64 loss: -0.6566305160522461
Batch 8/64 loss: -0.8973197937011719
Batch 9/64 loss: -1.1327085494995117
Batch 10/64 loss: -0.9078168869018555
Batch 11/64 loss: -0.5204534530639648
Batch 12/64 loss: -0.7630853652954102
Batch 13/64 loss: -0.7994346618652344
Batch 14/64 loss: -1.0441083908081055
Batch 15/64 loss: -1.0101375579833984
Batch 16/64 loss: -0.7673254013061523
Batch 17/64 loss: -0.8062839508056641
Batch 18/64 loss: -0.7849349975585938
Batch 19/64 loss: -0.4209752082824707
Batch 20/64 loss: -0.9028310775756836
Batch 21/64 loss: -0.7587652206420898
Batch 22/64 loss: -0.5183506011962891
Batch 23/64 loss: -0.6263999938964844
Batch 24/64 loss: -0.9140110015869141
Batch 25/64 loss: -0.923609733581543
Batch 26/64 loss: -1.1976699829101562
Batch 27/64 loss: -0.8158969879150391
Batch 28/64 loss: -0.5880374908447266
Batch 29/64 loss: -0.7380075454711914
Batch 30/64 loss: -0.6122627258300781
Batch 31/64 loss: -0.8959770202636719
Batch 32/64 loss: -1.0408811569213867
Batch 33/64 loss: -0.4205198287963867
Batch 34/64 loss: -0.7034549713134766
Batch 35/64 loss: -1.2228431701660156
Batch 36/64 loss: -1.1953706741333008
Batch 37/64 loss: -1.0371456146240234
Batch 38/64 loss: -0.9945278167724609
Batch 39/64 loss: -0.1331615447998047
Batch 40/64 loss: -0.8665370941162109
Batch 41/64 loss: -0.34610652923583984
Batch 42/64 loss: -1.0387420654296875
Batch 43/64 loss: -0.33361148834228516
Batch 44/64 loss: -1.1868343353271484
Batch 45/64 loss: -0.8204221725463867
Batch 46/64 loss: -1.1269340515136719
Batch 47/64 loss: -0.8254594802856445
Batch 48/64 loss: -1.0197267532348633
Batch 49/64 loss: -1.1675596237182617
Batch 50/64 loss: -0.9973001480102539
Batch 51/64 loss: -1.204483985900879
Batch 52/64 loss: -0.7880382537841797
Batch 53/64 loss: -0.6384563446044922
Batch 54/64 loss: -1.1389093399047852
Batch 55/64 loss: -0.9426155090332031
Batch 56/64 loss: -0.7696619033813477
Batch 57/64 loss: -0.4045438766479492
Batch 58/64 loss: -0.6403179168701172
Batch 59/64 loss: -0.9068355560302734
Batch 60/64 loss: -0.5265941619873047
Batch 61/64 loss: -1.1669549942016602
Batch 62/64 loss: -0.7219114303588867
Batch 63/64 loss: -1.0581655502319336
Batch 64/64 loss: -5.064522743225098
Epoch 472  Train loss: -0.8867696762084961  Val loss: -0.898108374212206
Epoch 473
-------------------------------
Batch 1/64 loss: -0.8299541473388672
Batch 2/64 loss: -1.0591163635253906
Batch 3/64 loss: -0.670140266418457
Batch 4/64 loss: -0.9685783386230469
Batch 5/64 loss: -0.8126029968261719
Batch 6/64 loss: -1.087571144104004
Batch 7/64 loss: -1.0609941482543945
Batch 8/64 loss: -1.03765869140625
Batch 9/64 loss: -1.2134599685668945
Batch 10/64 loss: -1.1789045333862305
Batch 11/64 loss: -0.4877185821533203
Batch 12/64 loss: -0.5768136978149414
Batch 13/64 loss: -0.827728271484375
Batch 14/64 loss: -1.1034822463989258
Batch 15/64 loss: -0.9251012802124023
Batch 16/64 loss: -0.7357959747314453
Batch 17/64 loss: -0.6203498840332031
Batch 18/64 loss: -0.7392978668212891
Batch 19/64 loss: -0.9020595550537109
Batch 20/64 loss: 0.15585708618164062
Batch 21/64 loss: -1.1175909042358398
Batch 22/64 loss: -1.0356969833374023
Batch 23/64 loss: -0.21044063568115234
Batch 24/64 loss: -0.7662830352783203
Batch 25/64 loss: -0.8389892578125
Batch 26/64 loss: -0.9963502883911133
Batch 27/64 loss: 0.20488834381103516
Batch 28/64 loss: -0.3439970016479492
Batch 29/64 loss: -1.0667543411254883
Batch 30/64 loss: -1.0690221786499023
Batch 31/64 loss: -0.9888811111450195
Batch 32/64 loss: -1.0704765319824219
Batch 33/64 loss: -0.8961629867553711
Batch 34/64 loss: -0.6665506362915039
Batch 35/64 loss: -0.5158300399780273
Batch 36/64 loss: -1.1456279754638672
Batch 37/64 loss: -0.8747072219848633
Batch 38/64 loss: -0.8166923522949219
Batch 39/64 loss: -1.1372804641723633
Batch 40/64 loss: -0.7573442459106445
Batch 41/64 loss: -1.1745100021362305
Batch 42/64 loss: -0.7270746231079102
Batch 43/64 loss: -0.7585525512695312
Batch 44/64 loss: -0.6680259704589844
Batch 45/64 loss: -1.261765480041504
Batch 46/64 loss: -1.1300344467163086
Batch 47/64 loss: -0.600555419921875
Batch 48/64 loss: -0.7689228057861328
Batch 49/64 loss: -0.8260574340820312
Batch 50/64 loss: -0.8669939041137695
Batch 51/64 loss: -0.8202896118164062
Batch 52/64 loss: -1.1047945022583008
Batch 53/64 loss: -0.7814416885375977
Batch 54/64 loss: -0.9468107223510742
Batch 55/64 loss: -1.1369953155517578
Batch 56/64 loss: -0.7666807174682617
Batch 57/64 loss: -0.9112005233764648
Batch 58/64 loss: -0.9571046829223633
Batch 59/64 loss: -0.6182832717895508
Batch 60/64 loss: -1.084700584411621
Batch 61/64 loss: -0.8816232681274414
Batch 62/64 loss: -1.2269620895385742
Batch 63/64 loss: -1.0178661346435547
Batch 64/64 loss: -4.849357604980469
Epoch 473  Train loss: -0.901419994877834  Val loss: -0.958549342204615
Epoch 474
-------------------------------
Batch 1/64 loss: -1.073751449584961
Batch 2/64 loss: -0.9531097412109375
Batch 3/64 loss: -1.0340499877929688
Batch 4/64 loss: -1.0202531814575195
Batch 5/64 loss: -0.9461879730224609
Batch 6/64 loss: -0.9313039779663086
Batch 7/64 loss: -0.8318614959716797
Batch 8/64 loss: -0.7464151382446289
Batch 9/64 loss: -0.7884664535522461
Batch 10/64 loss: -0.8743410110473633
Batch 11/64 loss: -1.0708961486816406
Batch 12/64 loss: 0.7219228744506836
Batch 13/64 loss: -1.0340766906738281
Batch 14/64 loss: -0.6907367706298828
Batch 15/64 loss: -0.9147739410400391
Batch 16/64 loss: -0.7756814956665039
Batch 17/64 loss: -0.9308004379272461
Batch 18/64 loss: -0.9641695022583008
Batch 19/64 loss: -1.0378007888793945
Batch 20/64 loss: -0.68701171875
Batch 21/64 loss: -0.6480274200439453
Batch 22/64 loss: -1.1895732879638672
Batch 23/64 loss: -0.9049797058105469
Batch 24/64 loss: -0.8062057495117188
Batch 25/64 loss: -0.5190258026123047
Batch 26/64 loss: -0.5893535614013672
Batch 27/64 loss: -0.7545747756958008
Batch 28/64 loss: -0.8106021881103516
Batch 29/64 loss: -1.0241212844848633
Batch 30/64 loss: -0.9907493591308594
Batch 31/64 loss: -0.9089593887329102
Batch 32/64 loss: -0.9931526184082031
Batch 33/64 loss: -0.789769172668457
Batch 34/64 loss: -1.0219917297363281
Batch 35/64 loss: -0.9260482788085938
Batch 36/64 loss: -0.9334135055541992
Batch 37/64 loss: -0.6023330688476562
Batch 38/64 loss: -0.9393758773803711
Batch 39/64 loss: 0.03845024108886719
Batch 40/64 loss: -0.15311241149902344
Batch 41/64 loss: -0.47242259979248047
Batch 42/64 loss: -0.9983158111572266
Batch 43/64 loss: -0.5329875946044922
Batch 44/64 loss: -0.810333251953125
Batch 45/64 loss: -0.6341133117675781
Batch 46/64 loss: -0.882720947265625
Batch 47/64 loss: -0.8319053649902344
Batch 48/64 loss: -0.812779426574707
Batch 49/64 loss: -0.5711698532104492
Batch 50/64 loss: -1.0570449829101562
Batch 51/64 loss: -0.5941591262817383
Batch 52/64 loss: -0.9099845886230469
Batch 53/64 loss: -0.4839296340942383
Batch 54/64 loss: -0.6863269805908203
Batch 55/64 loss: -0.9831676483154297
Batch 56/64 loss: -0.6883583068847656
Batch 57/64 loss: -0.6472625732421875
Batch 58/64 loss: -0.4447193145751953
Batch 59/64 loss: -0.8519277572631836
Batch 60/64 loss: -0.8572921752929688
Batch 61/64 loss: -1.0497779846191406
Batch 62/64 loss: -0.5725460052490234
Batch 63/64 loss: -0.7347497940063477
Batch 64/64 loss: -4.83152961730957
Epoch 474  Train loss: -0.8279580583759383  Val loss: -0.8765407968632544
Epoch 475
-------------------------------
Batch 1/64 loss: -1.127394676208496
Batch 2/64 loss: -1.0546398162841797
Batch 3/64 loss: -0.6165494918823242
Batch 4/64 loss: -0.8551702499389648
Batch 5/64 loss: -1.017059326171875
Batch 6/64 loss: -1.0918455123901367
Batch 7/64 loss: -0.7397432327270508
Batch 8/64 loss: -0.9959573745727539
Batch 9/64 loss: -0.6570749282836914
Batch 10/64 loss: -0.7440662384033203
Batch 11/64 loss: -0.7783927917480469
Batch 12/64 loss: -0.3402881622314453
Batch 13/64 loss: -1.0704498291015625
Batch 14/64 loss: -1.0284042358398438
Batch 15/64 loss: -0.7290496826171875
Batch 16/64 loss: -0.8104619979858398
Batch 17/64 loss: -0.8207178115844727
Batch 18/64 loss: -0.8386497497558594
Batch 19/64 loss: -1.2500791549682617
Batch 20/64 loss: -0.979304313659668
Batch 21/64 loss: -0.6436042785644531
Batch 22/64 loss: -0.9498682022094727
Batch 23/64 loss: -0.8227548599243164
Batch 24/64 loss: -0.5876951217651367
Batch 25/64 loss: -1.0705461502075195
Batch 26/64 loss: -0.7941551208496094
Batch 27/64 loss: -0.9405984878540039
Batch 28/64 loss: -0.927241325378418
Batch 29/64 loss: -0.9625091552734375
Batch 30/64 loss: -0.8504104614257812
Batch 31/64 loss: -0.44021129608154297
Batch 32/64 loss: -0.9045085906982422
Batch 33/64 loss: -0.3525724411010742
Batch 34/64 loss: -0.6783370971679688
Batch 35/64 loss: -0.6552276611328125
Batch 36/64 loss: -1.0236701965332031
Batch 37/64 loss: -0.9507246017456055
Batch 38/64 loss: -1.0066642761230469
Batch 39/64 loss: -0.9035177230834961
Batch 40/64 loss: -1.063140869140625
Batch 41/64 loss: -0.5721340179443359
Batch 42/64 loss: -0.6406793594360352
Batch 43/64 loss: -0.7556991577148438
Batch 44/64 loss: -0.8901844024658203
Batch 45/64 loss: -0.5476064682006836
Batch 46/64 loss: -0.9610233306884766
Batch 47/64 loss: -0.8975505828857422
Batch 48/64 loss: -1.1415271759033203
Batch 49/64 loss: -0.858759880065918
Batch 50/64 loss: -0.6437711715698242
Batch 51/64 loss: -1.0875492095947266
Batch 52/64 loss: -0.36273193359375
Batch 53/64 loss: -0.9812288284301758
Batch 54/64 loss: -0.8818349838256836
Batch 55/64 loss: -0.860173225402832
Batch 56/64 loss: -0.8620529174804688
Batch 57/64 loss: -0.8978404998779297
Batch 58/64 loss: -0.8202142715454102
Batch 59/64 loss: -0.6879873275756836
Batch 60/64 loss: -0.732417106628418
Batch 61/64 loss: -0.8817682266235352
Batch 62/64 loss: -0.3928871154785156
Batch 63/64 loss: -0.6641817092895508
Batch 64/64 loss: -4.858187675476074
Epoch 475  Train loss: -0.8743325513951918  Val loss: -0.8444709646742778
Epoch 476
-------------------------------
Batch 1/64 loss: -0.8813076019287109
Batch 2/64 loss: -0.8666210174560547
Batch 3/64 loss: -1.254312515258789
Batch 4/64 loss: -0.8691568374633789
Batch 5/64 loss: -1.004591941833496
Batch 6/64 loss: -0.5681972503662109
Batch 7/64 loss: -0.513514518737793
Batch 8/64 loss: -0.8661422729492188
Batch 9/64 loss: -0.9631671905517578
Batch 10/64 loss: -0.8037281036376953
Batch 11/64 loss: -1.1377382278442383
Batch 12/64 loss: -0.6233921051025391
Batch 13/64 loss: -0.7537727355957031
Batch 14/64 loss: -0.9160928726196289
Batch 15/64 loss: -0.4566316604614258
Batch 16/64 loss: -0.8956069946289062
Batch 17/64 loss: -0.9204006195068359
Batch 18/64 loss: -0.9318790435791016
Batch 19/64 loss: -1.0952587127685547
Batch 20/64 loss: -0.7294473648071289
Batch 21/64 loss: -1.0353965759277344
Batch 22/64 loss: -0.9355411529541016
Batch 23/64 loss: -1.0303916931152344
Batch 24/64 loss: -1.0807828903198242
Batch 25/64 loss: -1.0034332275390625
Batch 26/64 loss: -0.563572883605957
Batch 27/64 loss: -0.3881692886352539
Batch 28/64 loss: -0.8355522155761719
Batch 29/64 loss: -0.8465595245361328
Batch 30/64 loss: -1.0181884765625
Batch 31/64 loss: -0.7065219879150391
Batch 32/64 loss: -0.9777116775512695
Batch 33/64 loss: -0.8898248672485352
Batch 34/64 loss: -0.2368946075439453
Batch 35/64 loss: -0.9196939468383789
Batch 36/64 loss: -0.8348770141601562
Batch 37/64 loss: -0.5273046493530273
Batch 38/64 loss: -0.8617696762084961
Batch 39/64 loss: -0.7683639526367188
Batch 40/64 loss: -0.639378547668457
Batch 41/64 loss: -0.7048187255859375
Batch 42/64 loss: -1.100113868713379
Batch 43/64 loss: -0.9331254959106445
Batch 44/64 loss: -0.899439811706543
Batch 45/64 loss: -1.0357065200805664
Batch 46/64 loss: -0.572962760925293
Batch 47/64 loss: -0.6792078018188477
Batch 48/64 loss: -0.6546144485473633
Batch 49/64 loss: -0.8794088363647461
Batch 50/64 loss: -0.6838579177856445
Batch 51/64 loss: -0.2798147201538086
Batch 52/64 loss: -0.7572116851806641
Batch 53/64 loss: -1.2248420715332031
Batch 54/64 loss: -0.29139232635498047
Batch 55/64 loss: -0.15891361236572266
Batch 56/64 loss: -0.572479248046875
Batch 57/64 loss: -0.5600614547729492
Batch 58/64 loss: -0.718317985534668
Batch 59/64 loss: -0.993077278137207
Batch 60/64 loss: -0.5229711532592773
Batch 61/64 loss: -1.0292348861694336
Batch 62/64 loss: -0.4605245590209961
Batch 63/64 loss: -1.0640573501586914
Batch 64/64 loss: -4.918631553649902
Epoch 476  Train loss: -0.8410355549232632  Val loss: -0.8911773576769223
Epoch 477
-------------------------------
Batch 1/64 loss: -1.1333131790161133
Batch 2/64 loss: -0.6864013671875
Batch 3/64 loss: -0.9966888427734375
Batch 4/64 loss: -0.9304437637329102
Batch 5/64 loss: -0.8205041885375977
Batch 6/64 loss: -0.7297210693359375
Batch 7/64 loss: -1.1136970520019531
Batch 8/64 loss: -0.8764276504516602
Batch 9/64 loss: -0.7767801284790039
Batch 10/64 loss: -1.2169513702392578
Batch 11/64 loss: -1.1400299072265625
Batch 12/64 loss: -0.9880790710449219
Batch 13/64 loss: -1.0235471725463867
Batch 14/64 loss: -0.819188117980957
Batch 15/64 loss: -1.0509510040283203
Batch 16/64 loss: -1.0743951797485352
Batch 17/64 loss: -0.8496770858764648
Batch 18/64 loss: -1.012481689453125
Batch 19/64 loss: -0.9207744598388672
Batch 20/64 loss: 0.0636587142944336
Batch 21/64 loss: -0.7869024276733398
Batch 22/64 loss: -0.3923063278198242
Batch 23/64 loss: -0.9864959716796875
Batch 24/64 loss: -0.8103351593017578
Batch 25/64 loss: -0.9388341903686523
Batch 26/64 loss: -0.6878795623779297
Batch 27/64 loss: -0.43877696990966797
Batch 28/64 loss: -1.1412105560302734
Batch 29/64 loss: -0.9405040740966797
Batch 30/64 loss: -0.9536008834838867
Batch 31/64 loss: -0.751194953918457
Batch 32/64 loss: -0.07689094543457031
Batch 33/64 loss: -0.9874811172485352
Batch 34/64 loss: -0.4448871612548828
Batch 35/64 loss: -0.950531005859375
Batch 36/64 loss: -1.0115718841552734
Batch 37/64 loss: -1.013331413269043
Batch 38/64 loss: -0.8498992919921875
Batch 39/64 loss: -0.8493728637695312
Batch 40/64 loss: -0.5873937606811523
Batch 41/64 loss: -0.7924642562866211
Batch 42/64 loss: -1.032196044921875
Batch 43/64 loss: -0.41588401794433594
Batch 44/64 loss: -1.2457609176635742
Batch 45/64 loss: -0.7221689224243164
Batch 46/64 loss: -0.10875988006591797
Batch 47/64 loss: -1.0708942413330078
Batch 48/64 loss: -1.058222770690918
Batch 49/64 loss: -0.31043148040771484
Batch 50/64 loss: -1.0731468200683594
Batch 51/64 loss: -0.6269235610961914
Batch 52/64 loss: -0.7371625900268555
Batch 53/64 loss: -0.742680549621582
Batch 54/64 loss: -1.1088581085205078
Batch 55/64 loss: -0.7997283935546875
Batch 56/64 loss: -0.8096036911010742
Batch 57/64 loss: -0.8417806625366211
Batch 58/64 loss: -0.7921552658081055
Batch 59/64 loss: -0.9835681915283203
Batch 60/64 loss: -1.0925483703613281
Batch 61/64 loss: -1.1093873977661133
Batch 62/64 loss: -0.5032424926757812
Batch 63/64 loss: -1.1916255950927734
Batch 64/64 loss: -5.13718318939209
Epoch 477  Train loss: -0.8896921681422814  Val loss: -0.832636384210226
Epoch 478
-------------------------------
Batch 1/64 loss: -0.7258329391479492
Batch 2/64 loss: -0.7298488616943359
Batch 3/64 loss: -0.726165771484375
Batch 4/64 loss: -0.7676639556884766
Batch 5/64 loss: -0.6560249328613281
Batch 6/64 loss: -0.15707015991210938
Batch 7/64 loss: -1.041377067565918
Batch 8/64 loss: -1.0535087585449219
Batch 9/64 loss: -0.9361286163330078
Batch 10/64 loss: -0.7322483062744141
Batch 11/64 loss: -0.7585697174072266
Batch 12/64 loss: -0.6081790924072266
Batch 13/64 loss: -0.4657325744628906
Batch 14/64 loss: -0.9877700805664062
Batch 15/64 loss: -0.7060842514038086
Batch 16/64 loss: -0.7673959732055664
Batch 17/64 loss: -0.8682479858398438
Batch 18/64 loss: -0.7729759216308594
Batch 19/64 loss: -0.9498147964477539
Batch 20/64 loss: -0.7716283798217773
Batch 21/64 loss: -0.9297170639038086
Batch 22/64 loss: -0.5868072509765625
Batch 23/64 loss: -1.2437686920166016
Batch 24/64 loss: -0.9004116058349609
Batch 25/64 loss: -0.8065338134765625
Batch 26/64 loss: -1.0299491882324219
Batch 27/64 loss: -0.5469655990600586
Batch 28/64 loss: -1.0204353332519531
Batch 29/64 loss: -0.7041292190551758
Batch 30/64 loss: -1.0366325378417969
Batch 31/64 loss: -1.073476791381836
Batch 32/64 loss: -0.9635152816772461
Batch 33/64 loss: -0.34245967864990234
Batch 34/64 loss: -0.49120235443115234
Batch 35/64 loss: -0.4621448516845703
Batch 36/64 loss: -1.1172065734863281
Batch 37/64 loss: -0.7121057510375977
Batch 38/64 loss: -0.9884471893310547
Batch 39/64 loss: -0.7886686325073242
Batch 40/64 loss: -1.0447797775268555
Batch 41/64 loss: -1.0801057815551758
Batch 42/64 loss: -0.6570272445678711
Batch 43/64 loss: -1.058152198791504
Batch 44/64 loss: -1.3497486114501953
Batch 45/64 loss: -0.7498130798339844
Batch 46/64 loss: -0.4586029052734375
Batch 47/64 loss: -0.934600830078125
Batch 48/64 loss: -0.24735069274902344
Batch 49/64 loss: -1.006819725036621
Batch 50/64 loss: -0.245635986328125
Batch 51/64 loss: -0.8875417709350586
Batch 52/64 loss: -0.9939002990722656
Batch 53/64 loss: -0.5358905792236328
Batch 54/64 loss: -1.0102481842041016
Batch 55/64 loss: -0.6884784698486328
Batch 56/64 loss: -0.9747581481933594
Batch 57/64 loss: -0.8632383346557617
Batch 58/64 loss: -1.1360149383544922
Batch 59/64 loss: -0.9657249450683594
Batch 60/64 loss: -0.9017887115478516
Batch 61/64 loss: -0.5892257690429688
Batch 62/64 loss: -1.172536849975586
Batch 63/64 loss: -1.107381820678711
Batch 64/64 loss: -4.846271514892578
Epoch 478  Train loss: -0.8662104288736979  Val loss: -0.8974555628406223
Epoch 479
-------------------------------
Batch 1/64 loss: -0.3557567596435547
Batch 2/64 loss: -0.7132101058959961
Batch 3/64 loss: -1.0660104751586914
Batch 4/64 loss: -1.0238056182861328
Batch 5/64 loss: -1.2291107177734375
Batch 6/64 loss: -1.0447816848754883
Batch 7/64 loss: -0.4646425247192383
Batch 8/64 loss: -0.945652961730957
Batch 9/64 loss: -0.8458185195922852
Batch 10/64 loss: -0.8007774353027344
Batch 11/64 loss: -1.062727928161621
Batch 12/64 loss: -0.7977590560913086
Batch 13/64 loss: -1.0804738998413086
Batch 14/64 loss: -0.7971229553222656
Batch 15/64 loss: -0.865696907043457
Batch 16/64 loss: -0.9931802749633789
Batch 17/64 loss: -0.8856296539306641
Batch 18/64 loss: -0.7416009902954102
Batch 19/64 loss: -1.0534820556640625
Batch 20/64 loss: -0.8677310943603516
Batch 21/64 loss: -1.087113380432129
Batch 22/64 loss: -1.0979032516479492
Batch 23/64 loss: -0.8196048736572266
Batch 24/64 loss: -0.9787578582763672
Batch 25/64 loss: -0.9449043273925781
Batch 26/64 loss: -0.9953737258911133
Batch 27/64 loss: -0.6290187835693359
Batch 28/64 loss: -0.953944206237793
Batch 29/64 loss: -1.1837272644042969
Batch 30/64 loss: -0.6612577438354492
Batch 31/64 loss: -0.7465362548828125
Batch 32/64 loss: -0.664637565612793
Batch 33/64 loss: -1.051483154296875
Batch 34/64 loss: -1.0721635818481445
Batch 35/64 loss: -1.0144166946411133
Batch 36/64 loss: -0.5947723388671875
Batch 37/64 loss: -0.9359512329101562
Batch 38/64 loss: -0.9781656265258789
Batch 39/64 loss: -1.0406923294067383
Batch 40/64 loss: -0.9593296051025391
Batch 41/64 loss: -0.7900094985961914
Batch 42/64 loss: -0.9845304489135742
Batch 43/64 loss: -0.9520530700683594
Batch 44/64 loss: -1.2191181182861328
Batch 45/64 loss: -0.8702354431152344
Batch 46/64 loss: -0.5206384658813477
Batch 47/64 loss: -1.178593635559082
Batch 48/64 loss: -0.8746538162231445
Batch 49/64 loss: -0.8287744522094727
Batch 50/64 loss: -0.03963661193847656
Batch 51/64 loss: -1.231083869934082
Batch 52/64 loss: -0.7180242538452148
Batch 53/64 loss: -1.0020036697387695
Batch 54/64 loss: -0.7418012619018555
Batch 55/64 loss: -1.0721521377563477
Batch 56/64 loss: -0.7632217407226562
Batch 57/64 loss: -0.7804203033447266
Batch 58/64 loss: -0.885045051574707
Batch 59/64 loss: -0.7545671463012695
Batch 60/64 loss: -1.2102022171020508
Batch 61/64 loss: -0.691929817199707
Batch 62/64 loss: -1.1067390441894531
Batch 63/64 loss: -1.319915771484375
Batch 64/64 loss: -4.517935276031494
Epoch 479  Train loss: -0.9406827720941282  Val loss: -1.0226700248587173
Saving best model, epoch: 479
Epoch 480
-------------------------------
Batch 1/64 loss: -1.0714969635009766
Batch 2/64 loss: -0.8223342895507812
Batch 3/64 loss: -0.8131389617919922
Batch 4/64 loss: -0.7918586730957031
Batch 5/64 loss: -1.1473636627197266
Batch 6/64 loss: -0.7417392730712891
Batch 7/64 loss: -1.032292366027832
Batch 8/64 loss: -0.6251220703125
Batch 9/64 loss: -0.9907016754150391
Batch 10/64 loss: -0.9034919738769531
Batch 11/64 loss: -0.9288063049316406
Batch 12/64 loss: -0.8368911743164062
Batch 13/64 loss: -0.7506580352783203
Batch 14/64 loss: -0.8541278839111328
Batch 15/64 loss: -0.8527069091796875
Batch 16/64 loss: -0.7531032562255859
Batch 17/64 loss: -0.9374246597290039
Batch 18/64 loss: -0.8305387496948242
Batch 19/64 loss: -1.0266199111938477
Batch 20/64 loss: -0.9682378768920898
Batch 21/64 loss: -0.8105859756469727
Batch 22/64 loss: -1.0723333358764648
Batch 23/64 loss: -1.0002679824829102
Batch 24/64 loss: -0.9837102890014648
Batch 25/64 loss: -1.1057634353637695
Batch 26/64 loss: -0.9612903594970703
Batch 27/64 loss: -0.8288469314575195
Batch 28/64 loss: -1.0586929321289062
Batch 29/64 loss: -0.6376190185546875
Batch 30/64 loss: -1.1918268203735352
Batch 31/64 loss: -0.6513204574584961
Batch 32/64 loss: -1.080052375793457
Batch 33/64 loss: -1.1106843948364258
Batch 34/64 loss: -1.0029401779174805
Batch 35/64 loss: -0.9432287216186523
Batch 36/64 loss: -1.08929443359375
Batch 37/64 loss: -0.713958740234375
Batch 38/64 loss: -0.6815118789672852
Batch 39/64 loss: -0.5386075973510742
Batch 40/64 loss: -0.45013999938964844
Batch 41/64 loss: -0.5808830261230469
Batch 42/64 loss: -0.8352823257446289
Batch 43/64 loss: -0.8909492492675781
Batch 44/64 loss: -0.8446874618530273
Batch 45/64 loss: -1.0406904220581055
Batch 46/64 loss: -0.7701587677001953
Batch 47/64 loss: -1.0807437896728516
Batch 48/64 loss: -1.033609390258789
Batch 49/64 loss: -0.8493576049804688
Batch 50/64 loss: -0.8553752899169922
Batch 51/64 loss: -0.7947425842285156
Batch 52/64 loss: -0.9815540313720703
Batch 53/64 loss: -0.9759531021118164
Batch 54/64 loss: -0.9218578338623047
Batch 55/64 loss: -1.1949424743652344
Batch 56/64 loss: -0.7045440673828125
Batch 57/64 loss: -1.1657638549804688
Batch 58/64 loss: -0.78558349609375
Batch 59/64 loss: -0.981781005859375
Batch 60/64 loss: -1.2398738861083984
Batch 61/64 loss: -0.6280574798583984
Batch 62/64 loss: -1.0463838577270508
Batch 63/64 loss: -0.9979000091552734
Batch 64/64 loss: -4.275219440460205
Epoch 480  Train loss: -0.9411516881456562  Val loss: -0.9606892693903029
Epoch 481
-------------------------------
Batch 1/64 loss: -0.8350248336791992
Batch 2/64 loss: -0.8850288391113281
Batch 3/64 loss: -0.4180002212524414
Batch 4/64 loss: -0.9751100540161133
Batch 5/64 loss: -0.4366130828857422
Batch 6/64 loss: -1.2153549194335938
Batch 7/64 loss: -1.12677001953125
Batch 8/64 loss: -0.5991764068603516
Batch 9/64 loss: -1.2003974914550781
Batch 10/64 loss: -0.8578681945800781
Batch 11/64 loss: -1.115365982055664
Batch 12/64 loss: -0.7627639770507812
Batch 13/64 loss: -1.0317630767822266
Batch 14/64 loss: -0.5749549865722656
Batch 15/64 loss: -0.23157310485839844
Batch 16/64 loss: -1.1422920227050781
Batch 17/64 loss: -0.6369762420654297
Batch 18/64 loss: -1.1441574096679688
Batch 19/64 loss: -1.2076950073242188
Batch 20/64 loss: -0.8651676177978516
Batch 21/64 loss: -1.1304254531860352
Batch 22/64 loss: -0.9319229125976562
Batch 23/64 loss: -0.49764060974121094
Batch 24/64 loss: -1.0277099609375
Batch 25/64 loss: -0.7383041381835938
Batch 26/64 loss: -0.9077939987182617
Batch 27/64 loss: -0.8467693328857422
Batch 28/64 loss: -0.9926624298095703
Batch 29/64 loss: -1.1161069869995117
Batch 30/64 loss: -0.7028112411499023
Batch 31/64 loss: -0.5698938369750977
Batch 32/64 loss: -0.6912908554077148
Batch 33/64 loss: -0.9686336517333984
Batch 34/64 loss: -1.2638139724731445
Batch 35/64 loss: -1.1229448318481445
Batch 36/64 loss: -0.5900869369506836
Batch 37/64 loss: -0.42534446716308594
Batch 38/64 loss: -0.6558475494384766
Batch 39/64 loss: -1.1729154586791992
Batch 40/64 loss: -0.7242212295532227
Batch 41/64 loss: -0.9526472091674805
Batch 42/64 loss: -1.0711183547973633
Batch 43/64 loss: -0.7186212539672852
Batch 44/64 loss: -0.8173589706420898
Batch 45/64 loss: -0.9777812957763672
Batch 46/64 loss: -0.7069339752197266
Batch 47/64 loss: -1.13623046875
Batch 48/64 loss: -0.4665231704711914
Batch 49/64 loss: -1.1604080200195312
Batch 50/64 loss: -0.9490213394165039
Batch 51/64 loss: -0.6356315612792969
Batch 52/64 loss: -0.49675750732421875
Batch 53/64 loss: -1.0092334747314453
Batch 54/64 loss: -1.1378612518310547
Batch 55/64 loss: -0.7669992446899414
Batch 56/64 loss: -0.8679037094116211
Batch 57/64 loss: -1.1154050827026367
Batch 58/64 loss: -1.008814811706543
Batch 59/64 loss: -0.9727783203125
Batch 60/64 loss: -0.7907867431640625
Batch 61/64 loss: -0.8511848449707031
Batch 62/64 loss: -1.1511974334716797
Batch 63/64 loss: -1.0090980529785156
Batch 64/64 loss: -5.0024566650390625
Epoch 481  Train loss: -0.9233150108187806  Val loss: -0.8662414681870503
Epoch 482
-------------------------------
Batch 1/64 loss: -1.065852165222168
Batch 2/64 loss: -0.9388265609741211
Batch 3/64 loss: -0.9305648803710938
Batch 4/64 loss: -0.9171419143676758
Batch 5/64 loss: -1.1440181732177734
Batch 6/64 loss: -0.8448276519775391
Batch 7/64 loss: -0.87371826171875
Batch 8/64 loss: -0.841395378112793
Batch 9/64 loss: -0.9036750793457031
Batch 10/64 loss: -1.0476722717285156
Batch 11/64 loss: -0.958308219909668
Batch 12/64 loss: -0.5771398544311523
Batch 13/64 loss: -0.8455276489257812
Batch 14/64 loss: -0.6111154556274414
Batch 15/64 loss: -0.7512245178222656
Batch 16/64 loss: -0.8105649948120117
Batch 17/64 loss: -0.283935546875
Batch 18/64 loss: -0.7023229598999023
Batch 19/64 loss: -0.792363166809082
Batch 20/64 loss: -0.9586124420166016
Batch 21/64 loss: -0.866790771484375
Batch 22/64 loss: -0.9036016464233398
Batch 23/64 loss: -0.9131507873535156
Batch 24/64 loss: -0.4289236068725586
Batch 25/64 loss: -0.9907608032226562
Batch 26/64 loss: -0.9661827087402344
Batch 27/64 loss: -1.0087318420410156
Batch 28/64 loss: -1.0697784423828125
Batch 29/64 loss: -0.8248872756958008
Batch 30/64 loss: -0.7919034957885742
Batch 31/64 loss: -0.6052188873291016
Batch 32/64 loss: -0.8034353256225586
Batch 33/64 loss: -0.80364990234375
Batch 34/64 loss: -0.5239095687866211
Batch 35/64 loss: -0.9027090072631836
Batch 36/64 loss: -1.0377349853515625
Batch 37/64 loss: -1.0144634246826172
Batch 38/64 loss: -0.9021701812744141
Batch 39/64 loss: -0.7006959915161133
Batch 40/64 loss: -0.4251117706298828
Batch 41/64 loss: -0.9754199981689453
Batch 42/64 loss: -0.9763469696044922
Batch 43/64 loss: -1.0171308517456055
Batch 44/64 loss: -0.8539276123046875
Batch 45/64 loss: -0.9864139556884766
Batch 46/64 loss: -0.8327064514160156
Batch 47/64 loss: -1.0402336120605469
Batch 48/64 loss: -0.8406171798706055
Batch 49/64 loss: -1.1900749206542969
Batch 50/64 loss: -0.9748525619506836
Batch 51/64 loss: -1.2255887985229492
Batch 52/64 loss: -0.7193527221679688
Batch 53/64 loss: -1.029799461364746
Batch 54/64 loss: -0.9240989685058594
Batch 55/64 loss: -1.0947208404541016
Batch 56/64 loss: -0.970728874206543
Batch 57/64 loss: -1.1907415390014648
Batch 58/64 loss: -0.5040855407714844
Batch 59/64 loss: -0.9701719284057617
Batch 60/64 loss: -0.9140224456787109
Batch 61/64 loss: -0.8128290176391602
Batch 62/64 loss: -0.8128337860107422
Batch 63/64 loss: -0.871455192565918
Batch 64/64 loss: -4.971363544464111
Epoch 482  Train loss: -0.9214006816639619  Val loss: -0.9442520141601562
Epoch 483
-------------------------------
Batch 1/64 loss: -1.0965375900268555
Batch 2/64 loss: -1.1601753234863281
Batch 3/64 loss: -0.9100971221923828
Batch 4/64 loss: -0.8090181350708008
Batch 5/64 loss: -0.7828130722045898
Batch 6/64 loss: -1.1152105331420898
Batch 7/64 loss: -1.0113248825073242
Batch 8/64 loss: -0.8960304260253906
Batch 9/64 loss: -1.026885986328125
Batch 10/64 loss: -0.9660367965698242
Batch 11/64 loss: -0.8570470809936523
Batch 12/64 loss: -0.9972047805786133
Batch 13/64 loss: -0.5606250762939453
Batch 14/64 loss: -1.0871047973632812
Batch 15/64 loss: -1.25860595703125
Batch 16/64 loss: -0.8663873672485352
Batch 17/64 loss: -1.319809913635254
Batch 18/64 loss: -0.9380455017089844
Batch 19/64 loss: -0.19370269775390625
Batch 20/64 loss: -0.32437992095947266
Batch 21/64 loss: -1.0548810958862305
Batch 22/64 loss: -0.7404613494873047
Batch 23/64 loss: -1.0185565948486328
Batch 24/64 loss: -1.0165081024169922
Batch 25/64 loss: -1.0795660018920898
Batch 26/64 loss: -0.4519214630126953
Batch 27/64 loss: -1.1500253677368164
Batch 28/64 loss: -0.8888425827026367
Batch 29/64 loss: -1.0558710098266602
Batch 30/64 loss: -0.6652069091796875
Batch 31/64 loss: -1.190464973449707
Batch 32/64 loss: -0.7105979919433594
Batch 33/64 loss: -1.1559123992919922
Batch 34/64 loss: -0.6592559814453125
Batch 35/64 loss: -1.0294418334960938
Batch 36/64 loss: -0.8207111358642578
Batch 37/64 loss: -0.9478387832641602
Batch 38/64 loss: -0.9401302337646484
Batch 39/64 loss: -0.8949642181396484
Batch 40/64 loss: -0.8166179656982422
Batch 41/64 loss: -0.984095573425293
Batch 42/64 loss: -1.1142778396606445
Batch 43/64 loss: -0.945343017578125
Batch 44/64 loss: -0.39810657501220703
Batch 45/64 loss: -0.8724546432495117
Batch 46/64 loss: -0.08002471923828125
Batch 47/64 loss: -0.6243200302124023
Batch 48/64 loss: -0.7767238616943359
Batch 49/64 loss: -0.9773931503295898
Batch 50/64 loss: -1.0502347946166992
Batch 51/64 loss: -1.0797786712646484
Batch 52/64 loss: -0.7745199203491211
Batch 53/64 loss: -0.8327016830444336
Batch 54/64 loss: -0.45801830291748047
Batch 55/64 loss: -1.0711593627929688
Batch 56/64 loss: -0.7532634735107422
Batch 57/64 loss: -0.7649927139282227
Batch 58/64 loss: -0.7218647003173828
Batch 59/64 loss: -1.1560287475585938
Batch 60/64 loss: -1.206756591796875
Batch 61/64 loss: -0.5115318298339844
Batch 62/64 loss: -0.9227151870727539
Batch 63/64 loss: -0.5057973861694336
Batch 64/64 loss: -4.968653202056885
Epoch 483  Train loss: -0.9219358687307321  Val loss: -0.9156978384326004
Epoch 484
-------------------------------
Batch 1/64 loss: -1.2467517852783203
Batch 2/64 loss: -0.06843090057373047
Batch 3/64 loss: -0.5497274398803711
Batch 4/64 loss: -1.042001724243164
Batch 5/64 loss: -0.24849605560302734
Batch 6/64 loss: -0.7539749145507812
Batch 7/64 loss: -0.9977130889892578
Batch 8/64 loss: -0.9171295166015625
Batch 9/64 loss: -0.6793174743652344
Batch 10/64 loss: -0.9134445190429688
Batch 11/64 loss: -0.7210092544555664
Batch 12/64 loss: -0.8598089218139648
Batch 13/64 loss: -0.7515401840209961
Batch 14/64 loss: -0.2626914978027344
Batch 15/64 loss: -1.026071548461914
Batch 16/64 loss: -0.8767595291137695
Batch 17/64 loss: -0.7210807800292969
Batch 18/64 loss: -0.15893840789794922
Batch 19/64 loss: -0.800206184387207
Batch 20/64 loss: -0.9086208343505859
Batch 21/64 loss: -1.0577449798583984
Batch 22/64 loss: -0.5359659194946289
Batch 23/64 loss: -0.9069843292236328
Batch 24/64 loss: -1.071542739868164
Batch 25/64 loss: -1.0058612823486328
Batch 26/64 loss: -0.8357124328613281
Batch 27/64 loss: -0.7315654754638672
Batch 28/64 loss: -0.9215221405029297
Batch 29/64 loss: -1.1619195938110352
Batch 30/64 loss: -0.7204513549804688
Batch 31/64 loss: -1.002120018005371
Batch 32/64 loss: -0.8223304748535156
Batch 33/64 loss: -1.106795310974121
Batch 34/64 loss: -0.9239492416381836
Batch 35/64 loss: -0.9896574020385742
Batch 36/64 loss: -0.7702512741088867
Batch 37/64 loss: -1.0000677108764648
Batch 38/64 loss: -1.0179309844970703
Batch 39/64 loss: -0.7436361312866211
Batch 40/64 loss: -0.9071874618530273
Batch 41/64 loss: -0.46684837341308594
Batch 42/64 loss: -1.1328983306884766
Batch 43/64 loss: -0.7725687026977539
Batch 44/64 loss: -0.9825963973999023
Batch 45/64 loss: -0.6885881423950195
Batch 46/64 loss: -0.6641883850097656
Batch 47/64 loss: -0.6556634902954102
Batch 48/64 loss: -1.1972780227661133
Batch 49/64 loss: -0.8206691741943359
Batch 50/64 loss: -1.0523929595947266
Batch 51/64 loss: -0.48839282989501953
Batch 52/64 loss: -0.9355955123901367
Batch 53/64 loss: -0.8027286529541016
Batch 54/64 loss: -0.8696632385253906
Batch 55/64 loss: -0.6865568161010742
Batch 56/64 loss: -0.8182992935180664
Batch 57/64 loss: -0.665766716003418
Batch 58/64 loss: -0.9994144439697266
Batch 59/64 loss: -1.0710887908935547
Batch 60/64 loss: -1.0225343704223633
Batch 61/64 loss: -1.1969327926635742
Batch 62/64 loss: -0.7015066146850586
Batch 63/64 loss: -0.9436092376708984
Batch 64/64 loss: -4.598438262939453
Epoch 484  Train loss: -0.8756316989075904  Val loss: -1.0116841424371779
Epoch 485
-------------------------------
Batch 1/64 loss: -0.9823455810546875
Batch 2/64 loss: -0.7542257308959961
Batch 3/64 loss: -1.266000747680664
Batch 4/64 loss: -0.4018516540527344
Batch 5/64 loss: -0.6503973007202148
Batch 6/64 loss: -1.216191291809082
Batch 7/64 loss: -1.1439332962036133
Batch 8/64 loss: -0.9242067337036133
Batch 9/64 loss: -0.8232488632202148
Batch 10/64 loss: -1.021738052368164
Batch 11/64 loss: -1.2804241180419922
Batch 12/64 loss: -1.0416431427001953
Batch 13/64 loss: -1.149271011352539
Batch 14/64 loss: -0.940892219543457
Batch 15/64 loss: -0.9606513977050781
Batch 16/64 loss: -0.7963542938232422
Batch 17/64 loss: -0.8285999298095703
Batch 18/64 loss: -0.6824789047241211
Batch 19/64 loss: -1.0941505432128906
Batch 20/64 loss: -0.7094602584838867
Batch 21/64 loss: -1.0470199584960938
Batch 22/64 loss: -0.7822408676147461
Batch 23/64 loss: -0.5571317672729492
Batch 24/64 loss: -0.989384651184082
Batch 25/64 loss: -1.013646125793457
Batch 26/64 loss: -1.0301437377929688
Batch 27/64 loss: -0.9948606491088867
Batch 28/64 loss: -0.9921741485595703
Batch 29/64 loss: -0.7921571731567383
Batch 30/64 loss: -0.8959674835205078
Batch 31/64 loss: -0.7777090072631836
Batch 32/64 loss: -0.9100027084350586
Batch 33/64 loss: -0.7900390625
Batch 34/64 loss: -1.2464838027954102
Batch 35/64 loss: -0.5094089508056641
Batch 36/64 loss: -0.5072441101074219
Batch 37/64 loss: -0.6673822402954102
Batch 38/64 loss: -1.2863483428955078
Batch 39/64 loss: -1.0760784149169922
Batch 40/64 loss: -0.7613525390625
Batch 41/64 loss: -0.8438005447387695
Batch 42/64 loss: -0.9520301818847656
Batch 43/64 loss: -1.0999784469604492
Batch 44/64 loss: -0.8632488250732422
Batch 45/64 loss: -0.5501184463500977
Batch 46/64 loss: -0.6874074935913086
Batch 47/64 loss: -0.5114707946777344
Batch 48/64 loss: -0.8906345367431641
Batch 49/64 loss: -1.2321138381958008
Batch 50/64 loss: -0.9205036163330078
Batch 51/64 loss: -0.5786428451538086
Batch 52/64 loss: -0.9888792037963867
Batch 53/64 loss: -1.1529302597045898
Batch 54/64 loss: -0.6009016036987305
Batch 55/64 loss: -0.40526676177978516
Batch 56/64 loss: -1.200932502746582
Batch 57/64 loss: -0.6522607803344727
Batch 58/64 loss: -0.603179931640625
Batch 59/64 loss: -0.46839046478271484
Batch 60/64 loss: -1.2823657989501953
Batch 61/64 loss: -1.0994844436645508
Batch 62/64 loss: -1.193049430847168
Batch 63/64 loss: -0.950230598449707
Batch 64/64 loss: -4.601469993591309
Epoch 485  Train loss: -0.9329217985564587  Val loss: -0.9788761597728401
Epoch 486
-------------------------------
Batch 1/64 loss: -1.0433216094970703
Batch 2/64 loss: -1.0351448059082031
Batch 3/64 loss: -1.2092952728271484
Batch 4/64 loss: -1.0504837036132812
Batch 5/64 loss: -0.9413318634033203
Batch 6/64 loss: -0.9226951599121094
Batch 7/64 loss: -0.9353981018066406
Batch 8/64 loss: -0.959233283996582
Batch 9/64 loss: -0.8811511993408203
Batch 10/64 loss: -1.009042739868164
Batch 11/64 loss: -0.8287715911865234
Batch 12/64 loss: -0.9013643264770508
Batch 13/64 loss: -0.6299209594726562
Batch 14/64 loss: -0.7521038055419922
Batch 15/64 loss: -0.653935432434082
Batch 16/64 loss: -0.694610595703125
Batch 17/64 loss: -1.0096969604492188
Batch 18/64 loss: -0.9445428848266602
Batch 19/64 loss: -1.180272102355957
Batch 20/64 loss: -1.0820903778076172
Batch 21/64 loss: -0.7785015106201172
Batch 22/64 loss: -0.8716049194335938
Batch 23/64 loss: -1.1823720932006836
Batch 24/64 loss: -0.8283929824829102
Batch 25/64 loss: -1.0315752029418945
Batch 26/64 loss: -0.585881233215332
Batch 27/64 loss: -0.6833562850952148
Batch 28/64 loss: -0.9193544387817383
Batch 29/64 loss: -0.7309064865112305
Batch 30/64 loss: -0.9780473709106445
Batch 31/64 loss: -1.2195606231689453
Batch 32/64 loss: -0.7085227966308594
Batch 33/64 loss: -0.7167530059814453
Batch 34/64 loss: -0.8308010101318359
Batch 35/64 loss: -1.0585050582885742
Batch 36/64 loss: -0.980626106262207
Batch 37/64 loss: -1.1502723693847656
Batch 38/64 loss: -1.1169071197509766
Batch 39/64 loss: -1.1011772155761719
Batch 40/64 loss: -0.936553955078125
Batch 41/64 loss: -1.0195178985595703
Batch 42/64 loss: -0.6794109344482422
Batch 43/64 loss: -0.8316526412963867
Batch 44/64 loss: -0.6417942047119141
Batch 45/64 loss: -0.8814144134521484
Batch 46/64 loss: -0.7106924057006836
Batch 47/64 loss: -1.054987907409668
Batch 48/64 loss: -1.0476608276367188
Batch 49/64 loss: -0.898900032043457
Batch 50/64 loss: -1.3093814849853516
Batch 51/64 loss: -1.2461433410644531
Batch 52/64 loss: -1.118250846862793
Batch 53/64 loss: -1.2310829162597656
Batch 54/64 loss: -0.6402349472045898
Batch 55/64 loss: -1.2422447204589844
Batch 56/64 loss: -0.34128284454345703
Batch 57/64 loss: -0.8403263092041016
Batch 58/64 loss: -0.8462839126586914
Batch 59/64 loss: -0.46909046173095703
Batch 60/64 loss: 0.02513885498046875
Batch 61/64 loss: -0.3989906311035156
Batch 62/64 loss: -0.6578807830810547
Batch 63/64 loss: -1.1698493957519531
Batch 64/64 loss: -4.84034538269043
Epoch 486  Train loss: -0.9404905506208832  Val loss: -0.9865535854064312
Epoch 487
-------------------------------
Batch 1/64 loss: -0.8414430618286133
Batch 2/64 loss: -0.936732292175293
Batch 3/64 loss: -0.6763696670532227
Batch 4/64 loss: -0.49823570251464844
Batch 5/64 loss: -0.7633380889892578
Batch 6/64 loss: -0.9982080459594727
Batch 7/64 loss: -1.1230573654174805
Batch 8/64 loss: -1.107290267944336
Batch 9/64 loss: -1.052210807800293
Batch 10/64 loss: -1.2486906051635742
Batch 11/64 loss: -1.1154451370239258
Batch 12/64 loss: -0.8949394226074219
Batch 13/64 loss: -0.9088850021362305
Batch 14/64 loss: -0.8771915435791016
Batch 15/64 loss: -0.5772409439086914
Batch 16/64 loss: -0.6594934463500977
Batch 17/64 loss: -0.9564123153686523
Batch 18/64 loss: -1.0608863830566406
Batch 19/64 loss: -0.7117738723754883
Batch 20/64 loss: -1.1426401138305664
Batch 21/64 loss: -0.5862588882446289
Batch 22/64 loss: -0.998225212097168
Batch 23/64 loss: -0.9715166091918945
Batch 24/64 loss: -1.0804052352905273
Batch 25/64 loss: -1.0497369766235352
Batch 26/64 loss: -0.8833074569702148
Batch 27/64 loss: -0.2843046188354492
Batch 28/64 loss: -0.9594287872314453
Batch 29/64 loss: -0.9994421005249023
Batch 30/64 loss: -0.6653175354003906
Batch 31/64 loss: -0.8611717224121094
Batch 32/64 loss: -0.8825397491455078
Batch 33/64 loss: -0.5222129821777344
Batch 34/64 loss: -1.1364450454711914
Batch 35/64 loss: -0.5946760177612305
Batch 36/64 loss: -0.7515783309936523
Batch 37/64 loss: -0.7177276611328125
Batch 38/64 loss: -0.8389902114868164
Batch 39/64 loss: -1.123941421508789
Batch 40/64 loss: -0.4941291809082031
Batch 41/64 loss: -0.4921598434448242
Batch 42/64 loss: -0.6171398162841797
Batch 43/64 loss: -0.8906269073486328
Batch 44/64 loss: -1.003046989440918
Batch 45/64 loss: -1.0694122314453125
Batch 46/64 loss: -1.1010608673095703
Batch 47/64 loss: -1.0837717056274414
Batch 48/64 loss: -0.7830867767333984
Batch 49/64 loss: -0.9251070022583008
Batch 50/64 loss: -1.2154321670532227
Batch 51/64 loss: -1.1313180923461914
Batch 52/64 loss: -1.0901708602905273
Batch 53/64 loss: -0.710235595703125
Batch 54/64 loss: -0.27152538299560547
Batch 55/64 loss: -0.7421607971191406
Batch 56/64 loss: -1.1605873107910156
Batch 57/64 loss: -0.8821277618408203
Batch 58/64 loss: -1.2913246154785156
Batch 59/64 loss: -1.0108146667480469
Batch 60/64 loss: -0.6856479644775391
Batch 61/64 loss: -1.1799545288085938
Batch 62/64 loss: -1.0818719863891602
Batch 63/64 loss: -1.0333528518676758
Batch 64/64 loss: -4.744121074676514
Epoch 487  Train loss: -0.9343038016674565  Val loss: -0.9820848969659445
Epoch 488
-------------------------------
Batch 1/64 loss: -1.1333894729614258
Batch 2/64 loss: -0.8123340606689453
Batch 3/64 loss: -0.7404499053955078
Batch 4/64 loss: -0.5460081100463867
Batch 5/64 loss: -1.0805578231811523
Batch 6/64 loss: -1.0537843704223633
Batch 7/64 loss: -1.0160245895385742
Batch 8/64 loss: -1.0645923614501953
Batch 9/64 loss: -1.1266860961914062
Batch 10/64 loss: -0.8566179275512695
Batch 11/64 loss: -1.0390348434448242
Batch 12/64 loss: -1.0226287841796875
Batch 13/64 loss: -1.1115903854370117
Batch 14/64 loss: -0.8753395080566406
Batch 15/64 loss: -0.4503192901611328
Batch 16/64 loss: -0.9884777069091797
Batch 17/64 loss: -0.6990671157836914
Batch 18/64 loss: -1.1051177978515625
Batch 19/64 loss: -0.9290504455566406
Batch 20/64 loss: -0.8803262710571289
Batch 21/64 loss: -1.0596113204956055
Batch 22/64 loss: -0.7697763442993164
Batch 23/64 loss: -0.5904865264892578
Batch 24/64 loss: -0.8391838073730469
Batch 25/64 loss: -1.1151313781738281
Batch 26/64 loss: -0.9851312637329102
Batch 27/64 loss: -0.9643955230712891
Batch 28/64 loss: -0.7533178329467773
Batch 29/64 loss: -0.11187362670898438
Batch 30/64 loss: -0.8495616912841797
Batch 31/64 loss: -0.7411069869995117
Batch 32/64 loss: -1.1318778991699219
Batch 33/64 loss: -0.9250516891479492
Batch 34/64 loss: -0.9973821640014648
Batch 35/64 loss: -1.0843372344970703
Batch 36/64 loss: -0.7688846588134766
Batch 37/64 loss: -0.9930229187011719
Batch 38/64 loss: -0.9970550537109375
Batch 39/64 loss: -0.9427299499511719
Batch 40/64 loss: -1.117990493774414
Batch 41/64 loss: -0.9341230392456055
Batch 42/64 loss: -1.212015151977539
Batch 43/64 loss: -0.7627058029174805
Batch 44/64 loss: -0.7450447082519531
Batch 45/64 loss: -1.107290267944336
Batch 46/64 loss: -1.1857099533081055
Batch 47/64 loss: -0.6933450698852539
Batch 48/64 loss: -1.0608930587768555
Batch 49/64 loss: -0.7638397216796875
Batch 50/64 loss: -0.3335533142089844
Batch 51/64 loss: -1.0483150482177734
Batch 52/64 loss: -1.2749786376953125
Batch 53/64 loss: -0.6418008804321289
Batch 54/64 loss: -0.9390621185302734
Batch 55/64 loss: -0.5822725296020508
Batch 56/64 loss: -1.004709243774414
Batch 57/64 loss: -0.907017707824707
Batch 58/64 loss: -0.4633979797363281
Batch 59/64 loss: -0.8477811813354492
Batch 60/64 loss: -0.9815330505371094
Batch 61/64 loss: -0.7527046203613281
Batch 62/64 loss: -1.1629724502563477
Batch 63/64 loss: -1.0707988739013672
Batch 64/64 loss: -4.875914573669434
Epoch 488  Train loss: -0.9474840089386585  Val loss: -0.8995591520853469
Epoch 489
-------------------------------
Batch 1/64 loss: -1.0079574584960938
Batch 2/64 loss: -0.8727226257324219
Batch 3/64 loss: -1.1300344467163086
Batch 4/64 loss: -0.4179220199584961
Batch 5/64 loss: -0.8371667861938477
Batch 6/64 loss: -1.183237075805664
Batch 7/64 loss: -0.7832355499267578
Batch 8/64 loss: -0.8163938522338867
Batch 9/64 loss: -1.113067626953125
Batch 10/64 loss: -1.1844348907470703
Batch 11/64 loss: -0.8082599639892578
Batch 12/64 loss: -0.7856502532958984
Batch 13/64 loss: -0.6460456848144531
Batch 14/64 loss: -1.0359764099121094
Batch 15/64 loss: -1.0411195755004883
Batch 16/64 loss: -0.9643802642822266
Batch 17/64 loss: -0.902897834777832
Batch 18/64 loss: -1.1182069778442383
Batch 19/64 loss: -1.0121479034423828
Batch 20/64 loss: -0.5944910049438477
Batch 21/64 loss: -0.7624692916870117
Batch 22/64 loss: -0.9258518218994141
Batch 23/64 loss: -1.2254199981689453
Batch 24/64 loss: -0.665165901184082
Batch 25/64 loss: -1.233123779296875
Batch 26/64 loss: -0.76397705078125
Batch 27/64 loss: -1.151834487915039
Batch 28/64 loss: -1.2768783569335938
Batch 29/64 loss: -0.7000455856323242
Batch 30/64 loss: -0.6673612594604492
Batch 31/64 loss: -1.0110349655151367
Batch 32/64 loss: -1.0415277481079102
Batch 33/64 loss: -1.2418889999389648
Batch 34/64 loss: -1.2251853942871094
Batch 35/64 loss: -0.9130220413208008
Batch 36/64 loss: -0.8612918853759766
Batch 37/64 loss: -0.9299087524414062
Batch 38/64 loss: -1.047133445739746
Batch 39/64 loss: -1.25384521484375
Batch 40/64 loss: -1.1555471420288086
Batch 41/64 loss: -0.9182529449462891
Batch 42/64 loss: -0.2420196533203125
Batch 43/64 loss: -0.8792591094970703
Batch 44/64 loss: -1.1764698028564453
Batch 45/64 loss: -0.661036491394043
Batch 46/64 loss: -0.7795639038085938
Batch 47/64 loss: -0.14232540130615234
Batch 48/64 loss: -1.1255483627319336
Batch 49/64 loss: -0.7204990386962891
Batch 50/64 loss: -0.8527565002441406
Batch 51/64 loss: -0.8744773864746094
Batch 52/64 loss: -1.0232229232788086
Batch 53/64 loss: -1.138117790222168
Batch 54/64 loss: -0.9941606521606445
Batch 55/64 loss: -0.9717807769775391
Batch 56/64 loss: -1.2310428619384766
Batch 57/64 loss: -0.7637472152709961
Batch 58/64 loss: -0.8769550323486328
Batch 59/64 loss: -1.093672752380371
Batch 60/64 loss: -1.0248565673828125
Batch 61/64 loss: -0.7167749404907227
Batch 62/64 loss: -0.6296577453613281
Batch 63/64 loss: -0.7857637405395508
Batch 64/64 loss: -5.006656646728516
Epoch 489  Train loss: -0.9676049475576364  Val loss: -0.9645278773356959
Epoch 490
-------------------------------
Batch 1/64 loss: -1.0221672058105469
Batch 2/64 loss: -0.7326326370239258
Batch 3/64 loss: -0.7039175033569336
Batch 4/64 loss: -0.9925012588500977
Batch 5/64 loss: -0.9830999374389648
Batch 6/64 loss: -0.6926069259643555
Batch 7/64 loss: -0.992070198059082
Batch 8/64 loss: -0.5001211166381836
Batch 9/64 loss: -0.9021005630493164
Batch 10/64 loss: -0.5406150817871094
Batch 11/64 loss: -0.979619026184082
Batch 12/64 loss: -1.096445083618164
Batch 13/64 loss: -1.1106443405151367
Batch 14/64 loss: -1.1818628311157227
Batch 15/64 loss: -1.0921878814697266
Batch 16/64 loss: -0.6350030899047852
Batch 17/64 loss: -0.9521560668945312
Batch 18/64 loss: -0.8906984329223633
Batch 19/64 loss: -0.9697818756103516
Batch 20/64 loss: -0.9443445205688477
Batch 21/64 loss: -1.1133956909179688
Batch 22/64 loss: -0.8176517486572266
Batch 23/64 loss: -0.8785839080810547
Batch 24/64 loss: -0.7565298080444336
Batch 25/64 loss: -0.32871150970458984
Batch 26/64 loss: -1.0745773315429688
Batch 27/64 loss: -0.7235307693481445
Batch 28/64 loss: -0.8167104721069336
Batch 29/64 loss: -0.5378332138061523
Batch 30/64 loss: -0.961970329284668
Batch 31/64 loss: -0.4623260498046875
Batch 32/64 loss: -0.3752765655517578
Batch 33/64 loss: -0.9138965606689453
Batch 34/64 loss: -1.100092887878418
Batch 35/64 loss: -0.930445671081543
Batch 36/64 loss: -0.4292011260986328
Batch 37/64 loss: -0.4638633728027344
Batch 38/64 loss: -0.7977781295776367
Batch 39/64 loss: -1.019709587097168
Batch 40/64 loss: -0.9382610321044922
Batch 41/64 loss: -1.2219362258911133
Batch 42/64 loss: -1.273726463317871
Batch 43/64 loss: -1.0681438446044922
Batch 44/64 loss: -1.2349185943603516
Batch 45/64 loss: -0.5079936981201172
Batch 46/64 loss: -1.1424179077148438
Batch 47/64 loss: -0.9947032928466797
Batch 48/64 loss: -0.9882268905639648
Batch 49/64 loss: -0.7870302200317383
Batch 50/64 loss: -0.8700714111328125
Batch 51/64 loss: -0.8174362182617188
Batch 52/64 loss: -1.019597053527832
Batch 53/64 loss: -0.8614959716796875
Batch 54/64 loss: -0.3284320831298828
Batch 55/64 loss: -0.8631362915039062
Batch 56/64 loss: -0.6939229965209961
Batch 57/64 loss: -0.8672876358032227
Batch 58/64 loss: -0.8575019836425781
Batch 59/64 loss: -0.6824407577514648
Batch 60/64 loss: -0.7902812957763672
Batch 61/64 loss: -0.5927362442016602
Batch 62/64 loss: -1.085240364074707
Batch 63/64 loss: -0.8915243148803711
Batch 64/64 loss: -4.905896186828613
Epoch 490  Train loss: -0.9015614939670936  Val loss: -0.9687394079883483
Epoch 491
-------------------------------
Batch 1/64 loss: -1.0112648010253906
Batch 2/64 loss: -0.9360895156860352
Batch 3/64 loss: -1.0323104858398438
Batch 4/64 loss: -0.827916145324707
Batch 5/64 loss: -0.7641620635986328
Batch 6/64 loss: -0.7213716506958008
Batch 7/64 loss: -1.004542350769043
Batch 8/64 loss: -1.0065679550170898
Batch 9/64 loss: -0.5958995819091797
Batch 10/64 loss: -1.1611366271972656
Batch 11/64 loss: -1.0463085174560547
Batch 12/64 loss: -0.7022638320922852
Batch 13/64 loss: -0.05852508544921875
Batch 14/64 loss: -1.2128210067749023
Batch 15/64 loss: -0.4269571304321289
Batch 16/64 loss: -0.6015644073486328
Batch 17/64 loss: -1.0080976486206055
Batch 18/64 loss: -0.8998756408691406
Batch 19/64 loss: -0.677363395690918
Batch 20/64 loss: -1.116501808166504
Batch 21/64 loss: -1.121002197265625
Batch 22/64 loss: -0.9332504272460938
Batch 23/64 loss: -0.6975879669189453
Batch 24/64 loss: -0.8709774017333984
Batch 25/64 loss: -0.6296310424804688
Batch 26/64 loss: -1.007491111755371
Batch 27/64 loss: -1.011153221130371
Batch 28/64 loss: -1.1049413681030273
Batch 29/64 loss: -1.1973600387573242
Batch 30/64 loss: -0.6627912521362305
Batch 31/64 loss: -0.22240638732910156
Batch 32/64 loss: -1.0567646026611328
Batch 33/64 loss: -1.2112064361572266
Batch 34/64 loss: -1.0749626159667969
Batch 35/64 loss: -0.9620656967163086
Batch 36/64 loss: -0.9787235260009766
Batch 37/64 loss: -1.0096158981323242
Batch 38/64 loss: -0.7838172912597656
Batch 39/64 loss: -0.7471504211425781
Batch 40/64 loss: -0.546900749206543
Batch 41/64 loss: -0.9124746322631836
Batch 42/64 loss: -0.8142919540405273
Batch 43/64 loss: -0.7240657806396484
Batch 44/64 loss: -0.6646614074707031
Batch 45/64 loss: -0.7699680328369141
Batch 46/64 loss: -0.9864912033081055
Batch 47/64 loss: -0.8899145126342773
Batch 48/64 loss: -1.006382942199707
Batch 49/64 loss: -1.2264814376831055
Batch 50/64 loss: -0.7538509368896484
Batch 51/64 loss: -0.933384895324707
Batch 52/64 loss: -0.7078762054443359
Batch 53/64 loss: -0.8791999816894531
Batch 54/64 loss: -0.8732109069824219
Batch 55/64 loss: -0.8726024627685547
Batch 56/64 loss: -0.9244251251220703
Batch 57/64 loss: -0.7275590896606445
Batch 58/64 loss: -0.15326309204101562
Batch 59/64 loss: -1.0111207962036133
Batch 60/64 loss: -0.9781045913696289
Batch 61/64 loss: -1.0384368896484375
Batch 62/64 loss: -0.32623767852783203
Batch 63/64 loss: -0.7420253753662109
Batch 64/64 loss: -4.408389568328857
Epoch 491  Train loss: -0.8919790024850883  Val loss: -0.8101513262876531
Epoch 492
-------------------------------
Batch 1/64 loss: -1.233445167541504
Batch 2/64 loss: -0.07586956024169922
Batch 3/64 loss: -0.9105930328369141
Batch 4/64 loss: -0.9510278701782227
Batch 5/64 loss: -0.7332706451416016
Batch 6/64 loss: -1.0903444290161133
Batch 7/64 loss: -0.3539438247680664
Batch 8/64 loss: -0.8043956756591797
Batch 9/64 loss: -0.93267822265625
Batch 10/64 loss: -0.9352874755859375
Batch 11/64 loss: -0.9185209274291992
Batch 12/64 loss: -1.120539665222168
Batch 13/64 loss: -0.8812894821166992
Batch 14/64 loss: -0.8782548904418945
Batch 15/64 loss: -1.0233573913574219
Batch 16/64 loss: -1.1800355911254883
Batch 17/64 loss: -1.1678647994995117
Batch 18/64 loss: -0.9565153121948242
Batch 19/64 loss: -0.3309602737426758
Batch 20/64 loss: -0.9379987716674805
Batch 21/64 loss: -0.9335956573486328
Batch 22/64 loss: -1.0042495727539062
Batch 23/64 loss: -0.6270895004272461
Batch 24/64 loss: -0.5068817138671875
Batch 25/64 loss: -0.9045934677124023
Batch 26/64 loss: -0.3894491195678711
Batch 27/64 loss: -0.7238435745239258
Batch 28/64 loss: -0.9846963882446289
Batch 29/64 loss: -0.9520206451416016
Batch 30/64 loss: -0.6742410659790039
Batch 31/64 loss: -0.8537797927856445
Batch 32/64 loss: -1.0707416534423828
Batch 33/64 loss: -1.1003494262695312
Batch 34/64 loss: -0.7784214019775391
Batch 35/64 loss: -1.1101446151733398
Batch 36/64 loss: -0.7634143829345703
Batch 37/64 loss: -0.8480777740478516
Batch 38/64 loss: -1.1638221740722656
Batch 39/64 loss: -0.9429664611816406
Batch 40/64 loss: -0.9187812805175781
Batch 41/64 loss: -0.8920297622680664
Batch 42/64 loss: -0.584895133972168
Batch 43/64 loss: -0.7363204956054688
Batch 44/64 loss: -0.7842988967895508
Batch 45/64 loss: -0.6810388565063477
Batch 46/64 loss: -0.790196418762207
Batch 47/64 loss: -0.8873834609985352
Batch 48/64 loss: -0.5344972610473633
Batch 49/64 loss: -0.5777091979980469
Batch 50/64 loss: -0.8702945709228516
Batch 51/64 loss: -0.36982250213623047
Batch 52/64 loss: -0.547119140625
Batch 53/64 loss: -1.1837778091430664
Batch 54/64 loss: -0.8357343673706055
Batch 55/64 loss: -1.1049842834472656
Batch 56/64 loss: -1.0741462707519531
Batch 57/64 loss: -1.0130891799926758
Batch 58/64 loss: -1.0773372650146484
Batch 59/64 loss: -0.5157699584960938
Batch 60/64 loss: -0.8835859298706055
Batch 61/64 loss: -0.699493408203125
Batch 62/64 loss: -0.8754949569702148
Batch 63/64 loss: -0.989990234375
Batch 64/64 loss: -4.191400527954102
Epoch 492  Train loss: -0.8834496068019493  Val loss: -0.8633757653514954
Epoch 493
-------------------------------
Batch 1/64 loss: -0.3331947326660156
Batch 2/64 loss: -1.196237564086914
Batch 3/64 loss: -0.7437953948974609
Batch 4/64 loss: -0.8417396545410156
Batch 5/64 loss: -0.7763404846191406
Batch 6/64 loss: -0.8915071487426758
Batch 7/64 loss: -0.44683170318603516
Batch 8/64 loss: -1.038252830505371
Batch 9/64 loss: -0.8315563201904297
Batch 10/64 loss: -1.103480339050293
Batch 11/64 loss: -0.8573274612426758
Batch 12/64 loss: -1.1185102462768555
Batch 13/64 loss: -0.9597911834716797
Batch 14/64 loss: -1.028986930847168
Batch 15/64 loss: -0.8329248428344727
Batch 16/64 loss: -1.1414213180541992
Batch 17/64 loss: -0.6817531585693359
Batch 18/64 loss: -0.5276412963867188
Batch 19/64 loss: -0.7609834671020508
Batch 20/64 loss: -0.42911815643310547
Batch 21/64 loss: -0.25371837615966797
Batch 22/64 loss: -1.045480728149414
Batch 23/64 loss: -1.0172195434570312
Batch 24/64 loss: -1.0544281005859375
Batch 25/64 loss: -0.7712125778198242
Batch 26/64 loss: -0.8742694854736328
Batch 27/64 loss: -0.7847814559936523
Batch 28/64 loss: -0.7474184036254883
Batch 29/64 loss: -0.7392206192016602
Batch 30/64 loss: -0.5060405731201172
Batch 31/64 loss: -0.43085575103759766
Batch 32/64 loss: -0.9136667251586914
Batch 33/64 loss: -0.8345346450805664
Batch 34/64 loss: -0.8366622924804688
Batch 35/64 loss: -0.970545768737793
Batch 36/64 loss: -0.9386787414550781
Batch 37/64 loss: -0.8405017852783203
Batch 38/64 loss: -0.5538902282714844
Batch 39/64 loss: -0.7088718414306641
Batch 40/64 loss: -0.733738899230957
Batch 41/64 loss: -0.854710578918457
Batch 42/64 loss: -0.9615879058837891
Batch 43/64 loss: -0.3363494873046875
Batch 44/64 loss: -0.9980220794677734
Batch 45/64 loss: -0.48749256134033203
Batch 46/64 loss: -0.5003948211669922
Batch 47/64 loss: -0.7852802276611328
Batch 48/64 loss: -1.1370296478271484
Batch 49/64 loss: -1.0678653717041016
Batch 50/64 loss: -1.1047677993774414
Batch 51/64 loss: -0.9350080490112305
Batch 52/64 loss: -0.8997287750244141
Batch 53/64 loss: -0.8811178207397461
Batch 54/64 loss: -0.6971445083618164
Batch 55/64 loss: -0.9301023483276367
Batch 56/64 loss: -0.7521982192993164
Batch 57/64 loss: -0.709813117980957
Batch 58/64 loss: -0.6959705352783203
Batch 59/64 loss: -0.7117824554443359
Batch 60/64 loss: -0.7326364517211914
Batch 61/64 loss: -0.6030731201171875
Batch 62/64 loss: -1.1166925430297852
Batch 63/64 loss: -0.4413127899169922
Batch 64/64 loss: -4.775243282318115
Epoch 493  Train loss: -0.8473512855230593  Val loss: -1.0431956130614395
Saving best model, epoch: 493
Epoch 494
-------------------------------
Batch 1/64 loss: -1.141800880432129
Batch 2/64 loss: -1.2336559295654297
Batch 3/64 loss: -0.9059944152832031
Batch 4/64 loss: -0.9969501495361328
Batch 5/64 loss: -1.132481575012207
Batch 6/64 loss: -0.7038908004760742
Batch 7/64 loss: -0.9687843322753906
Batch 8/64 loss: -1.1162548065185547
Batch 9/64 loss: -0.7431116104125977
Batch 10/64 loss: -0.9277706146240234
Batch 11/64 loss: -0.8073930740356445
Batch 12/64 loss: -0.9008941650390625
Batch 13/64 loss: -1.1791582107543945
Batch 14/64 loss: -0.63763427734375
Batch 15/64 loss: -0.9014406204223633
Batch 16/64 loss: -0.9817304611206055
Batch 17/64 loss: -0.9725933074951172
Batch 18/64 loss: -0.5534687042236328
Batch 19/64 loss: -0.6013460159301758
Batch 20/64 loss: -0.6657295227050781
Batch 21/64 loss: -0.9438648223876953
Batch 22/64 loss: -0.4504280090332031
Batch 23/64 loss: -1.02581787109375
Batch 24/64 loss: -0.7859687805175781
Batch 25/64 loss: -0.8708877563476562
Batch 26/64 loss: -1.236027717590332
Batch 27/64 loss: -1.174546241760254
Batch 28/64 loss: -0.7428951263427734
Batch 29/64 loss: -0.6100988388061523
Batch 30/64 loss: -0.8904151916503906
Batch 31/64 loss: -0.9644126892089844
Batch 32/64 loss: -1.0208234786987305
Batch 33/64 loss: -0.9005727767944336
Batch 34/64 loss: -1.053171157836914
Batch 35/64 loss: -1.0971384048461914
Batch 36/64 loss: -1.13037109375
Batch 37/64 loss: -0.9434013366699219
Batch 38/64 loss: -1.0821914672851562
Batch 39/64 loss: -0.6347732543945312
Batch 40/64 loss: -0.022925376892089844
Batch 41/64 loss: -0.6490211486816406
Batch 42/64 loss: -0.7654457092285156
Batch 43/64 loss: -1.1380157470703125
Batch 44/64 loss: -1.2902336120605469
Batch 45/64 loss: -0.8095903396606445
Batch 46/64 loss: -1.119795799255371
Batch 47/64 loss: -0.9815454483032227
Batch 48/64 loss: -0.4968738555908203
Batch 49/64 loss: -0.9962549209594727
Batch 50/64 loss: -0.984832763671875
Batch 51/64 loss: -0.5346364974975586
Batch 52/64 loss: -0.8490076065063477
Batch 53/64 loss: -0.9158449172973633
Batch 54/64 loss: -0.48345088958740234
Batch 55/64 loss: -0.33501338958740234
Batch 56/64 loss: -0.8186368942260742
Batch 57/64 loss: -0.8601589202880859
Batch 58/64 loss: -0.9897403717041016
Batch 59/64 loss: -0.8615102767944336
Batch 60/64 loss: -0.5716514587402344
Batch 61/64 loss: -0.9648199081420898
Batch 62/64 loss: -0.805171012878418
Batch 63/64 loss: -0.973139762878418
Batch 64/64 loss: -4.889878273010254
Epoch 494  Train loss: -0.9178763109094956  Val loss: -0.9555506755396262
Epoch 495
-------------------------------
Batch 1/64 loss: -0.8805761337280273
Batch 2/64 loss: -0.8191375732421875
Batch 3/64 loss: -0.4690122604370117
Batch 4/64 loss: -1.0640993118286133
Batch 5/64 loss: -1.0579147338867188
Batch 6/64 loss: -0.502100944519043
Batch 7/64 loss: -0.6498785018920898
Batch 8/64 loss: -0.6295442581176758
Batch 9/64 loss: -1.022623062133789
Batch 10/64 loss: -0.7973651885986328
Batch 11/64 loss: -0.8223800659179688
Batch 12/64 loss: -0.8860406875610352
Batch 13/64 loss: -0.75372314453125
Batch 14/64 loss: -0.4267263412475586
Batch 15/64 loss: -0.9043216705322266
Batch 16/64 loss: -0.9081020355224609
Batch 17/64 loss: -0.9520368576049805
Batch 18/64 loss: -0.865509033203125
Batch 19/64 loss: -0.6575489044189453
Batch 20/64 loss: -1.072230339050293
Batch 21/64 loss: -1.1164312362670898
Batch 22/64 loss: -1.0768051147460938
Batch 23/64 loss: -0.6657609939575195
Batch 24/64 loss: -1.010519027709961
Batch 25/64 loss: -0.9086837768554688
Batch 26/64 loss: -0.7855491638183594
Batch 27/64 loss: -1.1533966064453125
Batch 28/64 loss: -0.6895227432250977
Batch 29/64 loss: -0.8528203964233398
Batch 30/64 loss: -0.8857259750366211
Batch 31/64 loss: -0.9349737167358398
Batch 32/64 loss: -0.9700851440429688
Batch 33/64 loss: -0.8659658432006836
Batch 34/64 loss: -0.831904411315918
Batch 35/64 loss: -0.7722873687744141
Batch 36/64 loss: -0.8164634704589844
Batch 37/64 loss: -0.9474782943725586
Batch 38/64 loss: -0.6239070892333984
Batch 39/64 loss: -1.244420051574707
Batch 40/64 loss: -0.6906957626342773
Batch 41/64 loss: -0.9934244155883789
Batch 42/64 loss: -0.7295360565185547
Batch 43/64 loss: -0.7389078140258789
Batch 44/64 loss: -0.818328857421875
Batch 45/64 loss: -0.9288215637207031
Batch 46/64 loss: 0.18778133392333984
Batch 47/64 loss: -1.1253185272216797
Batch 48/64 loss: -0.9832248687744141
Batch 49/64 loss: -0.5961713790893555
Batch 50/64 loss: -0.696751594543457
Batch 51/64 loss: -0.9829692840576172
Batch 52/64 loss: -0.8200168609619141
Batch 53/64 loss: -0.7359504699707031
Batch 54/64 loss: -0.9503278732299805
Batch 55/64 loss: -0.5181360244750977
Batch 56/64 loss: -0.7137470245361328
Batch 57/64 loss: -0.814539909362793
Batch 58/64 loss: -1.104945182800293
Batch 59/64 loss: -0.9693841934204102
Batch 60/64 loss: -0.7589569091796875
Batch 61/64 loss: -0.8701772689819336
Batch 62/64 loss: -0.7958135604858398
Batch 63/64 loss: -0.9971141815185547
Batch 64/64 loss: -4.393520355224609
Epoch 495  Train loss: -0.8742618037205117  Val loss: -0.9569481394135255
Epoch 496
-------------------------------
Batch 1/64 loss: -0.7086849212646484
Batch 2/64 loss: -1.2773971557617188
Batch 3/64 loss: -1.1057014465332031
Batch 4/64 loss: -0.6339988708496094
Batch 5/64 loss: -0.9883747100830078
Batch 6/64 loss: -0.03147602081298828
Batch 7/64 loss: -1.2189626693725586
Batch 8/64 loss: -0.5823879241943359
Batch 9/64 loss: -0.8913469314575195
Batch 10/64 loss: -0.562739372253418
Batch 11/64 loss: -0.03341388702392578
Batch 12/64 loss: -0.9878683090209961
Batch 13/64 loss: -0.9184389114379883
Batch 14/64 loss: -0.9747638702392578
Batch 15/64 loss: -1.0859241485595703
Batch 16/64 loss: -0.865788459777832
Batch 17/64 loss: -0.6121606826782227
Batch 18/64 loss: -0.8651409149169922
Batch 19/64 loss: -0.8194379806518555
Batch 20/64 loss: -0.7272586822509766
Batch 21/64 loss: -1.002777099609375
Batch 22/64 loss: -0.9006118774414062
Batch 23/64 loss: -0.515228271484375
Batch 24/64 loss: -0.9288597106933594
Batch 25/64 loss: -0.4422578811645508
Batch 26/64 loss: -0.9981374740600586
Batch 27/64 loss: -0.6174821853637695
Batch 28/64 loss: -0.6566400527954102
Batch 29/64 loss: -1.1201114654541016
Batch 30/64 loss: -0.31334781646728516
Batch 31/64 loss: -1.2376556396484375
Batch 32/64 loss: -0.94329833984375
Batch 33/64 loss: -1.1102447509765625
Batch 34/64 loss: -0.8427162170410156
Batch 35/64 loss: -0.6055440902709961
Batch 36/64 loss: -1.0622949600219727
Batch 37/64 loss: -0.9124460220336914
Batch 38/64 loss: -0.8355512619018555
Batch 39/64 loss: -1.0742626190185547
Batch 40/64 loss: -1.0454216003417969
Batch 41/64 loss: -0.44239139556884766
Batch 42/64 loss: -0.34063720703125
Batch 43/64 loss: -0.9051427841186523
Batch 44/64 loss: -0.8630990982055664
Batch 45/64 loss: -0.7286357879638672
Batch 46/64 loss: -0.8268508911132812
Batch 47/64 loss: -1.0161962509155273
Batch 48/64 loss: -0.9155855178833008
Batch 49/64 loss: -1.0837860107421875
Batch 50/64 loss: -1.1607446670532227
Batch 51/64 loss: -0.7576761245727539
Batch 52/64 loss: -0.8271903991699219
Batch 53/64 loss: -0.5114507675170898
Batch 54/64 loss: -0.7553548812866211
Batch 55/64 loss: -0.8547306060791016
Batch 56/64 loss: -0.8581104278564453
Batch 57/64 loss: -0.951995849609375
Batch 58/64 loss: -1.005411148071289
Batch 59/64 loss: -0.8934564590454102
Batch 60/64 loss: -1.0538969039916992
Batch 61/64 loss: -1.1135425567626953
Batch 62/64 loss: -1.071432113647461
Batch 63/64 loss: -0.7189655303955078
Batch 64/64 loss: -4.930420875549316
Epoch 496  Train loss: -0.8848667331770355  Val loss: -0.9415227096924668
Epoch 497
-------------------------------
Batch 1/64 loss: -0.5023107528686523
Batch 2/64 loss: -0.6484231948852539
Batch 3/64 loss: 0.12707233428955078
Batch 4/64 loss: -0.7720184326171875
Batch 5/64 loss: -0.37000179290771484
Batch 6/64 loss: -0.7073135375976562
Batch 7/64 loss: -0.7860383987426758
Batch 8/64 loss: -0.8408136367797852
Batch 9/64 loss: -1.0762319564819336
Batch 10/64 loss: -1.2336969375610352
Batch 11/64 loss: -0.9530868530273438
Batch 12/64 loss: -0.9543142318725586
Batch 13/64 loss: -1.2764110565185547
Batch 14/64 loss: -0.7840099334716797
Batch 15/64 loss: -0.8294897079467773
Batch 16/64 loss: -0.45838451385498047
Batch 17/64 loss: -1.0468111038208008
Batch 18/64 loss: -1.201277732849121
Batch 19/64 loss: -1.162959098815918
Batch 20/64 loss: -0.837040901184082
Batch 21/64 loss: -0.4665079116821289
Batch 22/64 loss: -1.1666269302368164
Batch 23/64 loss: -1.0437707901000977
Batch 24/64 loss: -0.888270378112793
Batch 25/64 loss: -0.9815578460693359
Batch 26/64 loss: -1.0303993225097656
Batch 27/64 loss: -0.9876384735107422
Batch 28/64 loss: -0.8622531890869141
Batch 29/64 loss: -0.7069797515869141
Batch 30/64 loss: -0.9072484970092773
Batch 31/64 loss: -0.44155406951904297
Batch 32/64 loss: -0.5577592849731445
Batch 33/64 loss: -0.5984811782836914
Batch 34/64 loss: -1.1157760620117188
Batch 35/64 loss: -0.9734477996826172
Batch 36/64 loss: -0.9043035507202148
Batch 37/64 loss: -0.6696968078613281
Batch 38/64 loss: -1.1010732650756836
Batch 39/64 loss: -0.48668575286865234
Batch 40/64 loss: -0.9866552352905273
Batch 41/64 loss: -0.7119054794311523
Batch 42/64 loss: -0.7165307998657227
Batch 43/64 loss: -0.9211492538452148
Batch 44/64 loss: -0.9173030853271484
Batch 45/64 loss: -1.069716453552246
Batch 46/64 loss: -0.8696966171264648
Batch 47/64 loss: -0.9565458297729492
Batch 48/64 loss: -0.5086526870727539
Batch 49/64 loss: -0.6183280944824219
Batch 50/64 loss: -1.1090621948242188
Batch 51/64 loss: -0.8467235565185547
Batch 52/64 loss: -0.5693626403808594
Batch 53/64 loss: -1.072352409362793
Batch 54/64 loss: -1.2071619033813477
Batch 55/64 loss: -0.7502431869506836
Batch 56/64 loss: -0.8759059906005859
Batch 57/64 loss: -0.7319412231445312
Batch 58/64 loss: -0.8847818374633789
Batch 59/64 loss: -1.0204029083251953
Batch 60/64 loss: -1.1523780822753906
Batch 61/64 loss: -1.1513700485229492
Batch 62/64 loss: -0.8770170211791992
Batch 63/64 loss: -1.1115961074829102
Batch 64/64 loss: -4.493911266326904
Epoch 497  Train loss: -0.8974244454327751  Val loss: -0.8507837380740241
Epoch 498
-------------------------------
Batch 1/64 loss: -0.9947404861450195
Batch 2/64 loss: -1.0690574645996094
Batch 3/64 loss: -0.8742990493774414
Batch 4/64 loss: -1.1236600875854492
Batch 5/64 loss: -0.9820117950439453
Batch 6/64 loss: -0.9482784271240234
Batch 7/64 loss: -1.023148536682129
Batch 8/64 loss: -0.8181276321411133
Batch 9/64 loss: -0.6600522994995117
Batch 10/64 loss: -0.6686840057373047
Batch 11/64 loss: -1.049203872680664
Batch 12/64 loss: -0.4244117736816406
Batch 13/64 loss: -0.4766054153442383
Batch 14/64 loss: -0.8350257873535156
Batch 15/64 loss: -0.7161846160888672
Batch 16/64 loss: -0.9769468307495117
Batch 17/64 loss: -0.7988452911376953
Batch 18/64 loss: -0.6417074203491211
Batch 19/64 loss: -0.8076686859130859
Batch 20/64 loss: -0.6112546920776367
Batch 21/64 loss: -0.6927719116210938
Batch 22/64 loss: -0.6101703643798828
Batch 23/64 loss: -0.8104486465454102
Batch 24/64 loss: -0.5777359008789062
Batch 25/64 loss: -0.7768392562866211
Batch 26/64 loss: -1.0734148025512695
Batch 27/64 loss: -1.387784481048584
Batch 28/64 loss: -0.7976455688476562
Batch 29/64 loss: -0.6127138137817383
Batch 30/64 loss: -0.9316740036010742
Batch 31/64 loss: -1.0570144653320312
Batch 32/64 loss: -0.9050989151000977
Batch 33/64 loss: -1.0631990432739258
Batch 34/64 loss: -0.6145963668823242
Batch 35/64 loss: -0.9389333724975586
Batch 36/64 loss: -1.1684579849243164
Batch 37/64 loss: -0.9598312377929688
Batch 38/64 loss: -0.6196432113647461
Batch 39/64 loss: -1.0422353744506836
Batch 40/64 loss: -0.5631694793701172
Batch 41/64 loss: -0.6731233596801758
Batch 42/64 loss: -0.8347902297973633
Batch 43/64 loss: -0.978236198425293
Batch 44/64 loss: -0.830561637878418
Batch 45/64 loss: -1.1223230361938477
Batch 46/64 loss: -1.0814285278320312
Batch 47/64 loss: -1.2489023208618164
Batch 48/64 loss: -1.1131343841552734
Batch 49/64 loss: -1.0868110656738281
Batch 50/64 loss: -0.7277936935424805
Batch 51/64 loss: -0.8900661468505859
Batch 52/64 loss: -1.0765037536621094
Batch 53/64 loss: -1.0737934112548828
Batch 54/64 loss: -1.1518306732177734
Batch 55/64 loss: -0.9011964797973633
Batch 56/64 loss: -0.913482666015625
Batch 57/64 loss: -1.0111513137817383
Batch 58/64 loss: -0.6275224685668945
Batch 59/64 loss: -0.7355194091796875
Batch 60/64 loss: -1.1059722900390625
Batch 61/64 loss: -0.49535274505615234
Batch 62/64 loss: -1.0355033874511719
Batch 63/64 loss: -0.9707279205322266
Batch 64/64 loss: -4.33820104598999
Epoch 498  Train loss: -0.9198850239024443  Val loss: -1.0729076083992766
Saving best model, epoch: 498
Epoch 499
-------------------------------
Batch 1/64 loss: -1.2702093124389648
Batch 2/64 loss: -1.171675682067871
Batch 3/64 loss: -1.0943222045898438
Batch 4/64 loss: -1.0211820602416992
Batch 5/64 loss: -0.6915426254272461
Batch 6/64 loss: -0.5856246948242188
Batch 7/64 loss: -1.0720643997192383
Batch 8/64 loss: -1.050149917602539
Batch 9/64 loss: -0.9771995544433594
Batch 10/64 loss: -0.39867401123046875
Batch 11/64 loss: -1.0070724487304688
Batch 12/64 loss: -0.431671142578125
Batch 13/64 loss: -1.0574722290039062
Batch 14/64 loss: -0.5220403671264648
Batch 15/64 loss: -0.9165592193603516
Batch 16/64 loss: -0.28949832916259766
Batch 17/64 loss: -0.9024381637573242
Batch 18/64 loss: -1.0618867874145508
Batch 19/64 loss: -0.9367361068725586
Batch 20/64 loss: -1.154733657836914
Batch 21/64 loss: -1.154428482055664
Batch 22/64 loss: -1.0334749221801758
Batch 23/64 loss: -1.087346076965332
Batch 24/64 loss: -0.9206781387329102
Batch 25/64 loss: -0.7565956115722656
Batch 26/64 loss: -0.8773479461669922
Batch 27/64 loss: -0.8702049255371094
Batch 28/64 loss: -0.5770645141601562
Batch 29/64 loss: -0.39675426483154297
Batch 30/64 loss: -0.628413200378418
Batch 31/64 loss: -0.4420919418334961
Batch 32/64 loss: -0.8305683135986328
Batch 33/64 loss: -0.9536905288696289
Batch 34/64 loss: -1.1173219680786133
Batch 35/64 loss: -1.1418371200561523
Batch 36/64 loss: -1.223419189453125
Batch 37/64 loss: -0.9930400848388672
Batch 38/64 loss: -0.584808349609375
Batch 39/64 loss: -0.9627923965454102
Batch 40/64 loss: -1.1955718994140625
Batch 41/64 loss: -0.4280557632446289
Batch 42/64 loss: -1.0285005569458008
Batch 43/64 loss: -1.0821657180786133
Batch 44/64 loss: -0.38749217987060547
Batch 45/64 loss: -0.8097515106201172
Batch 46/64 loss: -1.1735897064208984
Batch 47/64 loss: -1.1010990142822266
Batch 48/64 loss: -1.1233463287353516
Batch 49/64 loss: -0.9443187713623047
Batch 50/64 loss: -0.8183126449584961
Batch 51/64 loss: -1.0706005096435547
Batch 52/64 loss: -0.7817058563232422
Batch 53/64 loss: -0.9792690277099609
Batch 54/64 loss: -1.1380739212036133
Batch 55/64 loss: -0.46468353271484375
Batch 56/64 loss: -0.8902168273925781
Batch 57/64 loss: -1.3302135467529297
Batch 58/64 loss: -1.0224180221557617
Batch 59/64 loss: -0.07305049896240234
Batch 60/64 loss: -1.0023384094238281
Batch 61/64 loss: -0.6736354827880859
Batch 62/64 loss: -0.8538389205932617
Batch 63/64 loss: -0.7432518005371094
Batch 64/64 loss: -4.720510959625244
Epoch 499  Train loss: -0.9226747381920908  Val loss: -0.7515828240778029
Epoch 500
-------------------------------
Batch 1/64 loss: -1.0243415832519531
Batch 2/64 loss: -1.0283279418945312
Batch 3/64 loss: -1.0296449661254883
Batch 4/64 loss: -0.8731775283813477
Batch 5/64 loss: -0.8465662002563477
Batch 6/64 loss: -0.39452648162841797
Batch 7/64 loss: -0.9616117477416992
Batch 8/64 loss: -0.8593673706054688
Batch 9/64 loss: -0.8899316787719727
Batch 10/64 loss: -1.001241683959961
Batch 11/64 loss: -0.6271095275878906
Batch 12/64 loss: -1.1172733306884766
Batch 13/64 loss: -0.7348880767822266
Batch 14/64 loss: -1.1758041381835938
Batch 15/64 loss: -0.7659940719604492
Batch 16/64 loss: -1.1193046569824219
Batch 17/64 loss: -0.938258171081543
Batch 18/64 loss: -1.2324848175048828
Batch 19/64 loss: -1.050135612487793
Batch 20/64 loss: -1.181300163269043
Batch 21/64 loss: -0.7042236328125
Batch 22/64 loss: -1.1488103866577148
Batch 23/64 loss: -0.6790390014648438
Batch 24/64 loss: -0.9699230194091797
Batch 25/64 loss: -0.609135627746582
Batch 26/64 loss: -1.0950393676757812
Batch 27/64 loss: -0.9995098114013672
Batch 28/64 loss: -1.1241941452026367
Batch 29/64 loss: -0.6928520202636719
Batch 30/64 loss: -0.9985742568969727
Batch 31/64 loss: -1.135274887084961
Batch 32/64 loss: -1.0961904525756836
Batch 33/64 loss: -1.1024370193481445
Batch 34/64 loss: -0.8412952423095703
Batch 35/64 loss: -1.2973785400390625
Batch 36/64 loss: -0.8740053176879883
Batch 37/64 loss: -1.006911277770996
Batch 38/64 loss: -1.0521821975708008
Batch 39/64 loss: -0.8291988372802734
Batch 40/64 loss: -0.7142734527587891
Batch 41/64 loss: -0.9900636672973633
Batch 42/64 loss: -1.026677131652832
Batch 43/64 loss: -0.87347412109375
Batch 44/64 loss: -0.8815526962280273
Batch 45/64 loss: -1.0721330642700195
Batch 46/64 loss: -0.8818416595458984
Batch 47/64 loss: -1.3382463455200195
Batch 48/64 loss: -0.9957742691040039
Batch 49/64 loss: -0.8101091384887695
Batch 50/64 loss: -0.6961402893066406
Batch 51/64 loss: -0.8362913131713867
Batch 52/64 loss: -1.1416816711425781
Batch 53/64 loss: -0.7043666839599609
Batch 54/64 loss: -0.5877866744995117
Batch 55/64 loss: -0.7502870559692383
Batch 56/64 loss: -0.3944425582885742
Batch 57/64 loss: -0.028989791870117188
Batch 58/64 loss: -1.100006103515625
Batch 59/64 loss: -1.030116081237793
Batch 60/64 loss: -0.7953386306762695
Batch 61/64 loss: -0.7858209609985352
Batch 62/64 loss: -1.0205049514770508
Batch 63/64 loss: -1.0511045455932617
Batch 64/64 loss: -4.600979328155518
Epoch 500  Train loss: -0.9578858375549316  Val loss: -0.9826073925110073
SLIC undersegmentation error: 0.12412920962199316
SLIC inter-cluster variation: 0.13904419774313004
SLIC number of superpixels: 21483
SLIC superpixels per image: 73.82474226804123
Model loaded
Test metrics:
-1.7508359561671096 0.3122048109965636 29.297978600070245 tensor(0.2668, dtype=torch.float64) 0.8083604528578632 4.056973919420635 21817
Inference time: 0.0025711592120403275 seconds
Relabeled undersegmentation error: 0.09883298969072164
Relabeled inter-cluster variation: 0.055074584127475
Relabeled mean superpixels count: 304.1786941580756
Original mean superpixels count: 74.97250859106529
Done!
Job id: 488552
Job id: 492259
