Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 0.42042481899261475
Batch 2/64 loss: 0.36777186393737793
Batch 3/64 loss: 0.3488340377807617
Batch 4/64 loss: 0.3367016315460205
Batch 5/64 loss: 0.338312566280365
Batch 6/64 loss: 0.33383941650390625
Batch 7/64 loss: 0.33460623025894165
Batch 8/64 loss: 0.3285638093948364
Batch 9/64 loss: 0.3313456177711487
Batch 10/64 loss: 0.32804030179977417
Batch 11/64 loss: 0.3244776725769043
Batch 12/64 loss: 0.3242313861846924
Batch 13/64 loss: 0.3228851556777954
Batch 14/64 loss: 0.3263601064682007
Batch 15/64 loss: 0.3222222328186035
Batch 16/64 loss: 0.3217740058898926
Batch 17/64 loss: 0.32077574729919434
Batch 18/64 loss: 0.3244510889053345
Batch 19/64 loss: 0.3184424638748169
Batch 20/64 loss: 0.3206530809402466
Batch 21/64 loss: 0.32215559482574463
Batch 22/64 loss: 0.3145524263381958
Batch 23/64 loss: 0.31650859117507935
Batch 24/64 loss: 0.31625378131866455
Batch 25/64 loss: 0.31170833110809326
Batch 26/64 loss: 0.31261730194091797
Batch 27/64 loss: 0.3127763271331787
Batch 28/64 loss: 0.31353843212127686
Batch 29/64 loss: 0.3136793375015259
Batch 30/64 loss: 0.31018292903900146
Batch 31/64 loss: 0.30784016847610474
Batch 32/64 loss: 0.3083379864692688
Batch 33/64 loss: 0.30375438928604126
Batch 34/64 loss: 0.30502212047576904
Batch 35/64 loss: 0.3077198266983032
Batch 36/64 loss: 0.3025209307670593
Batch 37/64 loss: 0.30078327655792236
Batch 38/64 loss: 0.2961815595626831
Batch 39/64 loss: 0.30571818351745605
Batch 40/64 loss: 0.3000417947769165
Batch 41/64 loss: 0.2996988892555237
Batch 42/64 loss: 0.30326831340789795
Batch 43/64 loss: 0.29292356967926025
Batch 44/64 loss: 0.2970314025878906
Batch 45/64 loss: 0.30382877588272095
Batch 46/64 loss: 0.29522645473480225
Batch 47/64 loss: 0.2986598610877991
Batch 48/64 loss: 0.294366717338562
Batch 49/64 loss: 0.2960996627807617
Batch 50/64 loss: 0.2983875274658203
Batch 51/64 loss: 0.2952742576599121
Batch 52/64 loss: 0.29540133476257324
Batch 53/64 loss: 0.28961431980133057
Batch 54/64 loss: 0.29483914375305176
Batch 55/64 loss: 0.2954796552658081
Batch 56/64 loss: 0.2945512533187866
Batch 57/64 loss: 0.2961444854736328
Batch 58/64 loss: 0.29024720191955566
Batch 59/64 loss: 0.29155951738357544
Batch 60/64 loss: 0.2914057970046997
Batch 61/64 loss: 0.28717970848083496
Batch 62/64 loss: 0.29059940576553345
Batch 63/64 loss: 0.2854587435722351
Batch 64/64 loss: 0.29308366775512695
Epoch 1  Train loss: 0.31174376899120854  Val loss: 0.2987513374217187
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 0.2848505973815918
Batch 2/64 loss: 0.28544145822525024
Batch 3/64 loss: 0.2908247709274292
Batch 4/64 loss: 0.28633391857147217
Batch 5/64 loss: 0.2880157232284546
Batch 6/64 loss: 0.28594839572906494
Batch 7/64 loss: 0.2840656638145447
Batch 8/64 loss: 0.28344404697418213
Batch 9/64 loss: 0.2915570139884949
Batch 10/64 loss: 0.2824743986129761
Batch 11/64 loss: 0.2862277626991272
Batch 12/64 loss: 0.2818402051925659
Batch 13/64 loss: 0.2840536832809448
Batch 14/64 loss: 0.28544604778289795
Batch 15/64 loss: 0.27869534492492676
Batch 16/64 loss: 0.2800372838973999
Batch 17/64 loss: 0.2739325165748596
Batch 18/64 loss: 0.2864276170730591
Batch 19/64 loss: 0.27871572971343994
Batch 20/64 loss: 0.2812923192977905
Batch 21/64 loss: 0.28600573539733887
Batch 22/64 loss: 0.28517401218414307
Batch 23/64 loss: 0.2868797779083252
Batch 24/64 loss: 0.27891457080841064
Batch 25/64 loss: 0.2738799452781677
Batch 26/64 loss: 0.28216850757598877
Batch 27/64 loss: 0.27991747856140137
Batch 28/64 loss: 0.2718067765235901
Batch 29/64 loss: 0.2694042921066284
Batch 30/64 loss: 0.2811093330383301
Batch 31/64 loss: 0.2821829915046692
Batch 32/64 loss: 0.27992522716522217
Batch 33/64 loss: 0.28065741062164307
Batch 34/64 loss: 0.2827640771865845
Batch 35/64 loss: 0.2825183868408203
Batch 36/64 loss: 0.2760509252548218
Batch 37/64 loss: 0.2670711278915405
Batch 38/64 loss: 0.2745751142501831
Batch 39/64 loss: 0.2829020023345947
Batch 40/64 loss: 0.2763192653656006
Batch 41/64 loss: 0.2792908549308777
Batch 42/64 loss: 0.2748720645904541
Batch 43/64 loss: 0.27553480863571167
Batch 44/64 loss: 0.27677977085113525
Batch 45/64 loss: 0.2801257371902466
Batch 46/64 loss: 0.28144437074661255
Batch 47/64 loss: 0.28056323528289795
Batch 48/64 loss: 0.2779063582420349
Batch 49/64 loss: 0.284329891204834
Batch 50/64 loss: 0.2725048065185547
Batch 51/64 loss: 0.2765001654624939
Batch 52/64 loss: 0.27304625511169434
Batch 53/64 loss: 0.2861602306365967
Batch 54/64 loss: 0.266143262386322
Batch 55/64 loss: 0.2766820192337036
Batch 56/64 loss: 0.27287817001342773
Batch 57/64 loss: 0.27863115072250366
Batch 58/64 loss: 0.27993619441986084
Batch 59/64 loss: 0.2741544246673584
Batch 60/64 loss: 0.28370481729507446
Batch 61/64 loss: 0.28354132175445557
Batch 62/64 loss: 0.28324413299560547
Batch 63/64 loss: 0.27164530754089355
Batch 64/64 loss: 0.2738165855407715
Epoch 2  Train loss: 0.2800758156121946  Val loss: 0.27482113027081045
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 0.27136290073394775
Batch 2/64 loss: 0.27011096477508545
Batch 3/64 loss: 0.26853811740875244
Batch 4/64 loss: 0.2704460620880127
Batch 5/64 loss: 0.2714166045188904
Batch 6/64 loss: 0.2672008275985718
Batch 7/64 loss: 0.25953173637390137
Batch 8/64 loss: 0.27264881134033203
Batch 9/64 loss: 0.25852954387664795
Batch 10/64 loss: 0.26718807220458984
Batch 11/64 loss: 0.26433122158050537
Batch 12/64 loss: 0.26640546321868896
Batch 13/64 loss: 0.2673056125640869
Batch 14/64 loss: 0.2748482823371887
Batch 15/64 loss: 0.259519100189209
Batch 16/64 loss: 0.2720673084259033
Batch 17/64 loss: 0.27044355869293213
Batch 18/64 loss: 0.2601712942123413
Batch 19/64 loss: 0.26392847299575806
Batch 20/64 loss: 0.2728379964828491
Batch 21/64 loss: 0.26624077558517456
Batch 22/64 loss: 0.26759517192840576
Batch 23/64 loss: 0.2673479914665222
Batch 24/64 loss: 0.2583022713661194
Batch 25/64 loss: 0.2741484045982361
Batch 26/64 loss: 0.2645505666732788
Batch 27/64 loss: 0.2714899778366089
Batch 28/64 loss: 0.2626793384552002
Batch 29/64 loss: 0.26577574014663696
Batch 30/64 loss: 0.2694441080093384
Batch 31/64 loss: 0.2678080201148987
Batch 32/64 loss: 0.275803804397583
Batch 33/64 loss: 0.26128721237182617
Batch 34/64 loss: 0.2686270475387573
Batch 35/64 loss: 0.2674318552017212
Batch 36/64 loss: 0.2569929361343384
Batch 37/64 loss: 0.2625309228897095
Batch 38/64 loss: 0.268801212310791
Batch 39/64 loss: 0.26278072595596313
Batch 40/64 loss: 0.2677868604660034
Batch 41/64 loss: 0.2634192109107971
Batch 42/64 loss: 0.2693135738372803
Batch 43/64 loss: 0.2619757652282715
Batch 44/64 loss: 0.26156169176101685
Batch 45/64 loss: 0.2660733461380005
Batch 46/64 loss: 0.267192006111145
Batch 47/64 loss: 0.25062990188598633
Batch 48/64 loss: 0.2665613889694214
Batch 49/64 loss: 0.2613229751586914
Batch 50/64 loss: 0.26886820793151855
Batch 51/64 loss: 0.26773571968078613
Batch 52/64 loss: 0.257995069026947
Batch 53/64 loss: 0.2609872817993164
Batch 54/64 loss: 0.25616300106048584
Batch 55/64 loss: 0.25445008277893066
Batch 56/64 loss: 0.25678861141204834
Batch 57/64 loss: 0.26903367042541504
Batch 58/64 loss: 0.2723551392555237
Batch 59/64 loss: 0.26154279708862305
Batch 60/64 loss: 0.26676928997039795
Batch 61/64 loss: 0.2666042447090149
Batch 62/64 loss: 0.26144516468048096
Batch 63/64 loss: 0.25942254066467285
Batch 64/64 loss: 0.2599743604660034
Epoch 3  Train loss: 0.26540311130822875  Val loss: 0.2695954501424049
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 0.2607584595680237
Batch 2/64 loss: 0.25424230098724365
Batch 3/64 loss: 0.2653392553329468
Batch 4/64 loss: 0.26017826795578003
Batch 5/64 loss: 0.24403584003448486
Batch 6/64 loss: 0.25627779960632324
Batch 7/64 loss: 0.2653895616531372
Batch 8/64 loss: 0.26230883598327637
Batch 9/64 loss: 0.27479028701782227
Batch 10/64 loss: 0.25757473707199097
Batch 11/64 loss: 0.2594214081764221
Batch 12/64 loss: 0.2510857582092285
Batch 13/64 loss: 0.2474936842918396
Batch 14/64 loss: 0.2606830596923828
Batch 15/64 loss: 0.26344406604766846
Batch 16/64 loss: 0.2504609227180481
Batch 17/64 loss: 0.2577999234199524
Batch 18/64 loss: 0.2645372748374939
Batch 19/64 loss: 0.25553184747695923
Batch 20/64 loss: 0.26102012395858765
Batch 21/64 loss: 0.2621889114379883
Batch 22/64 loss: 0.2574823498725891
Batch 23/64 loss: 0.2695350646972656
Batch 24/64 loss: 0.25474536418914795
Batch 25/64 loss: 0.26078593730926514
Batch 26/64 loss: 0.25262248516082764
Batch 27/64 loss: 0.2580827474594116
Batch 28/64 loss: 0.2539060711860657
Batch 29/64 loss: 0.25869619846343994
Batch 30/64 loss: 0.2544647455215454
Batch 31/64 loss: 0.2621919512748718
Batch 32/64 loss: 0.2421727180480957
Batch 33/64 loss: 0.25312840938568115
Batch 34/64 loss: 0.24908435344696045
Batch 35/64 loss: 0.24614572525024414
Batch 36/64 loss: 0.26312506198883057
Batch 37/64 loss: 0.25224053859710693
Batch 38/64 loss: 0.24936425685882568
Batch 39/64 loss: 0.2434099316596985
Batch 40/64 loss: 0.2551385760307312
Batch 41/64 loss: 0.25871890783309937
Batch 42/64 loss: 0.2537423372268677
Batch 43/64 loss: 0.2708818316459656
Batch 44/64 loss: 0.2592482566833496
Batch 45/64 loss: 0.2509441375732422
Batch 46/64 loss: 0.25122690200805664
Batch 47/64 loss: 0.2552589178085327
Batch 48/64 loss: 0.26433634757995605
Batch 49/64 loss: 0.25400733947753906
Batch 50/64 loss: 0.2601223587989807
Batch 51/64 loss: 0.26301419734954834
Batch 52/64 loss: 0.2541022300720215
Batch 53/64 loss: 0.24506217241287231
Batch 54/64 loss: 0.253678560256958
Batch 55/64 loss: 0.24727892875671387
Batch 56/64 loss: 0.25340890884399414
Batch 57/64 loss: 0.26260948181152344
Batch 58/64 loss: 0.2599794864654541
Batch 59/64 loss: 0.2587970495223999
Batch 60/64 loss: 0.25234484672546387
Batch 61/64 loss: 0.24893569946289062
Batch 62/64 loss: 0.246362566947937
Batch 63/64 loss: 0.2580656409263611
Batch 64/64 loss: 0.2487497329711914
Epoch 4  Train loss: 0.25630704655366787  Val loss: 0.2565869377650756
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 0.2567790746688843
Batch 2/64 loss: 0.2473142147064209
Batch 3/64 loss: 0.25776660442352295
Batch 4/64 loss: 0.24768757820129395
Batch 5/64 loss: 0.2509603500366211
Batch 6/64 loss: 0.24565833806991577
Batch 7/64 loss: 0.25356531143188477
Batch 8/64 loss: 0.25563156604766846
Batch 9/64 loss: 0.26122498512268066
Batch 10/64 loss: 0.2579203248023987
Batch 11/64 loss: 0.2496775984764099
Batch 12/64 loss: 0.24988555908203125
Batch 13/64 loss: 0.2566235065460205
Batch 14/64 loss: 0.25609052181243896
Batch 15/64 loss: 0.2582331895828247
Batch 16/64 loss: 0.2529337406158447
Batch 17/64 loss: 0.24961650371551514
Batch 18/64 loss: 0.25351035594940186
Batch 19/64 loss: 0.250821053981781
Batch 20/64 loss: 0.2513892650604248
Batch 21/64 loss: 0.239763081073761
Batch 22/64 loss: 0.2524186372756958
Batch 23/64 loss: 0.24972593784332275
Batch 24/64 loss: 0.2460198998451233
Batch 25/64 loss: 0.25349700450897217
Batch 26/64 loss: 0.24912303686141968
Batch 27/64 loss: 0.2506459951400757
Batch 28/64 loss: 0.24811631441116333
Batch 29/64 loss: 0.2597392797470093
Batch 30/64 loss: 0.24082553386688232
Batch 31/64 loss: 0.249664306640625
Batch 32/64 loss: 0.25512367486953735
Batch 33/64 loss: 0.24399757385253906
Batch 34/64 loss: 0.24039852619171143
Batch 35/64 loss: 0.24675101041793823
Batch 36/64 loss: 0.25375086069107056
Batch 37/64 loss: 0.24252909421920776
Batch 38/64 loss: 0.24283230304718018
Batch 39/64 loss: 0.25359195470809937
Batch 40/64 loss: 0.2530266046524048
Batch 41/64 loss: 0.25288665294647217
Batch 42/64 loss: 0.2419392466545105
Batch 43/64 loss: 0.24713242053985596
Batch 44/64 loss: 0.25247395038604736
Batch 45/64 loss: 0.2524031400680542
Batch 46/64 loss: 0.24409329891204834
Batch 47/64 loss: 0.24978172779083252
Batch 48/64 loss: 0.23925083875656128
Batch 49/64 loss: 0.2466064691543579
Batch 50/64 loss: 0.2403346300125122
Batch 51/64 loss: 0.24845826625823975
Batch 52/64 loss: 0.2625728249549866
Batch 53/64 loss: 0.2589881420135498
Batch 54/64 loss: 0.2463337779045105
Batch 55/64 loss: 0.24408936500549316
Batch 56/64 loss: 0.25317811965942383
Batch 57/64 loss: 0.24230897426605225
Batch 58/64 loss: 0.23592227697372437
Batch 59/64 loss: 0.24541258811950684
Batch 60/64 loss: 0.25895142555236816
Batch 61/64 loss: 0.2418973445892334
Batch 62/64 loss: 0.2452852725982666
Batch 63/64 loss: 0.25968486070632935
Batch 64/64 loss: 0.24312680959701538
Epoch 5  Train loss: 0.24983781950146544  Val loss: 0.2526617412714614
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 0.250487744808197
Batch 2/64 loss: 0.24852168560028076
Batch 3/64 loss: 0.24151325225830078
Batch 4/64 loss: 0.24797511100769043
Batch 5/64 loss: 0.23814821243286133
Batch 6/64 loss: 0.23975598812103271
Batch 7/64 loss: 0.25054478645324707
Batch 8/64 loss: 0.23505771160125732
Batch 9/64 loss: 0.2557916045188904
Batch 10/64 loss: 0.22884070873260498
Batch 11/64 loss: 0.24016571044921875
Batch 12/64 loss: 0.24583494663238525
Batch 13/64 loss: 0.24726402759552002
Batch 14/64 loss: 0.24219590425491333
Batch 15/64 loss: 0.24248343706130981
Batch 16/64 loss: 0.24987775087356567
Batch 17/64 loss: 0.2599114179611206
Batch 18/64 loss: 0.24760949611663818
Batch 19/64 loss: 0.23784387111663818
Batch 20/64 loss: 0.24224865436553955
Batch 21/64 loss: 0.2373049259185791
Batch 22/64 loss: 0.23728430271148682
Batch 23/64 loss: 0.2425329089164734
Batch 24/64 loss: 0.25177985429763794
Batch 25/64 loss: 0.24536621570587158
Batch 26/64 loss: 0.23499202728271484
Batch 27/64 loss: 0.24671298265457153
Batch 28/64 loss: 0.24172163009643555
Batch 29/64 loss: 0.23645836114883423
Batch 30/64 loss: 0.24728775024414062
Batch 31/64 loss: 0.25182467699050903
Batch 32/64 loss: 0.2438904047012329
Batch 33/64 loss: 0.24200057983398438
Batch 34/64 loss: 0.23577791452407837
Batch 35/64 loss: 0.24949240684509277
Batch 36/64 loss: 0.24697411060333252
Batch 37/64 loss: 0.2526353597640991
Batch 38/64 loss: 0.244981586933136
Batch 39/64 loss: 0.24490904808044434
Batch 40/64 loss: 0.24718713760375977
Batch 41/64 loss: 0.25689488649368286
Batch 42/64 loss: 0.24323570728302002
Batch 43/64 loss: 0.2417629361152649
Batch 44/64 loss: 0.23533451557159424
Batch 45/64 loss: 0.24105262756347656
Batch 46/64 loss: 0.24164390563964844
Batch 47/64 loss: 0.253055214881897
Batch 48/64 loss: 0.24575740098953247
Batch 49/64 loss: 0.24532872438430786
Batch 50/64 loss: 0.24266088008880615
Batch 51/64 loss: 0.24400031566619873
Batch 52/64 loss: 0.2365809679031372
Batch 53/64 loss: 0.24815785884857178
Batch 54/64 loss: 0.2402881383895874
Batch 55/64 loss: 0.23997282981872559
Batch 56/64 loss: 0.24968451261520386
Batch 57/64 loss: 0.24313855171203613
Batch 58/64 loss: 0.24609428644180298
Batch 59/64 loss: 0.23846489191055298
Batch 60/64 loss: 0.24118047952651978
Batch 61/64 loss: 0.24314022064208984
Batch 62/64 loss: 0.25317615270614624
Batch 63/64 loss: 0.23716038465499878
Batch 64/64 loss: 0.2280513048171997
Epoch 6  Train loss: 0.24395280118082083  Val loss: 0.24432267356164677
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 0.24344682693481445
Batch 2/64 loss: 0.23200619220733643
Batch 3/64 loss: 0.22153985500335693
Batch 4/64 loss: 0.24315392971038818
Batch 5/64 loss: 0.2525407671928406
Batch 6/64 loss: 0.2360926866531372
Batch 7/64 loss: 0.2372211217880249
Batch 8/64 loss: 0.2421398162841797
Batch 9/64 loss: 0.245125412940979
Batch 10/64 loss: 0.23530685901641846
Batch 11/64 loss: 0.2459101676940918
Batch 12/64 loss: 0.2240889072418213
Batch 13/64 loss: 0.23956525325775146
Batch 14/64 loss: 0.23797184228897095
Batch 15/64 loss: 0.24195313453674316
Batch 16/64 loss: 0.2320643663406372
Batch 17/64 loss: 0.2431963086128235
Batch 18/64 loss: 0.24268114566802979
Batch 19/64 loss: 0.24107420444488525
Batch 20/64 loss: 0.24527525901794434
Batch 21/64 loss: 0.24208557605743408
Batch 22/64 loss: 0.24655771255493164
Batch 23/64 loss: 0.2340611219406128
Batch 24/64 loss: 0.23320674896240234
Batch 25/64 loss: 0.22732234001159668
Batch 26/64 loss: 0.252532958984375
Batch 27/64 loss: 0.2320500612258911
Batch 28/64 loss: 0.24250924587249756
Batch 29/64 loss: 0.2505110502243042
Batch 30/64 loss: 0.24295496940612793
Batch 31/64 loss: 0.23435360193252563
Batch 32/64 loss: 0.24822211265563965
Batch 33/64 loss: 0.24437636137008667
Batch 34/64 loss: 0.23245060443878174
Batch 35/64 loss: 0.23368501663208008
Batch 36/64 loss: 0.24696004390716553
Batch 37/64 loss: 0.24298995733261108
Batch 38/64 loss: 0.23147189617156982
Batch 39/64 loss: 0.2371082305908203
Batch 40/64 loss: 0.23673248291015625
Batch 41/64 loss: 0.2278767228126526
Batch 42/64 loss: 0.24288254976272583
Batch 43/64 loss: 0.2386488914489746
Batch 44/64 loss: 0.22909784317016602
Batch 45/64 loss: 0.2480626106262207
Batch 46/64 loss: 0.23871886730194092
Batch 47/64 loss: 0.23617666959762573
Batch 48/64 loss: 0.2297431230545044
Batch 49/64 loss: 0.23669201135635376
Batch 50/64 loss: 0.22897732257843018
Batch 51/64 loss: 0.23550164699554443
Batch 52/64 loss: 0.23414510488510132
Batch 53/64 loss: 0.2554645538330078
Batch 54/64 loss: 0.23790478706359863
Batch 55/64 loss: 0.2390785813331604
Batch 56/64 loss: 0.23942452669143677
Batch 57/64 loss: 0.22652840614318848
Batch 58/64 loss: 0.23975467681884766
Batch 59/64 loss: 0.22861826419830322
Batch 60/64 loss: 0.2403714656829834
Batch 61/64 loss: 0.24895024299621582
Batch 62/64 loss: 0.22880464792251587
Batch 63/64 loss: 0.23367774486541748
Batch 64/64 loss: 0.2369837760925293
Epoch 7  Train loss: 0.23838957244274664  Val loss: 0.23874926382733375
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 0.24177980422973633
Batch 2/64 loss: 0.22592997550964355
Batch 3/64 loss: 0.23167210817337036
Batch 4/64 loss: 0.2325846552848816
Batch 5/64 loss: 0.23417150974273682
Batch 6/64 loss: 0.24002140760421753
Batch 7/64 loss: 0.24123704433441162
Batch 8/64 loss: 0.23890089988708496
Batch 9/64 loss: 0.23036599159240723
Batch 10/64 loss: 0.24360084533691406
Batch 11/64 loss: 0.23896169662475586
Batch 12/64 loss: 0.23905527591705322
Batch 13/64 loss: 0.24222123622894287
Batch 14/64 loss: 0.2271607518196106
Batch 15/64 loss: 0.22758734226226807
Batch 16/64 loss: 0.22988957166671753
Batch 17/64 loss: 0.2361810803413391
Batch 18/64 loss: 0.23588252067565918
Batch 19/64 loss: 0.23967516422271729
Batch 20/64 loss: 0.23487436771392822
Batch 21/64 loss: 0.2445847988128662
Batch 22/64 loss: 0.2329803705215454
Batch 23/64 loss: 0.23201370239257812
Batch 24/64 loss: 0.2264050841331482
Batch 25/64 loss: 0.22161293029785156
Batch 26/64 loss: 0.23366200923919678
Batch 27/64 loss: 0.23812896013259888
Batch 28/64 loss: 0.2233126163482666
Batch 29/64 loss: 0.22807353734970093
Batch 30/64 loss: 0.24164408445358276
Batch 31/64 loss: 0.23027753829956055
Batch 32/64 loss: 0.23356640338897705
Batch 33/64 loss: 0.23008382320404053
Batch 34/64 loss: 0.23988163471221924
Batch 35/64 loss: 0.22905588150024414
Batch 36/64 loss: 0.22492659091949463
Batch 37/64 loss: 0.23175257444381714
Batch 38/64 loss: 0.24636763334274292
Batch 39/64 loss: 0.23114728927612305
Batch 40/64 loss: 0.2302870750427246
Batch 41/64 loss: 0.2271481156349182
Batch 42/64 loss: 0.2300548553466797
Batch 43/64 loss: 0.21750271320343018
Batch 44/64 loss: 0.23689782619476318
Batch 45/64 loss: 0.2272869348526001
Batch 46/64 loss: 0.2312936782836914
Batch 47/64 loss: 0.23311805725097656
Batch 48/64 loss: 0.2270960807800293
Batch 49/64 loss: 0.23432868719100952
Batch 50/64 loss: 0.23304802179336548
Batch 51/64 loss: 0.23139023780822754
Batch 52/64 loss: 0.22745585441589355
Batch 53/64 loss: 0.23648309707641602
Batch 54/64 loss: 0.2341557741165161
Batch 55/64 loss: 0.2347627878189087
Batch 56/64 loss: 0.22266948223114014
Batch 57/64 loss: 0.22405683994293213
Batch 58/64 loss: 0.22661805152893066
Batch 59/64 loss: 0.22950410842895508
Batch 60/64 loss: 0.23432409763336182
Batch 61/64 loss: 0.2354062795639038
Batch 62/64 loss: 0.22807276248931885
Batch 63/64 loss: 0.2281673550605774
Batch 64/64 loss: 0.2379399538040161
Epoch 8  Train loss: 0.2326402266820272  Val loss: 0.23683600900918758
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 0.2410845160484314
Batch 2/64 loss: 0.227866530418396
Batch 3/64 loss: 0.2321566343307495
Batch 4/64 loss: 0.23276233673095703
Batch 5/64 loss: 0.2367231845855713
Batch 6/64 loss: 0.23753297328948975
Batch 7/64 loss: 0.24543660879135132
Batch 8/64 loss: 0.21890270709991455
Batch 9/64 loss: 0.23195934295654297
Batch 10/64 loss: 0.23583060503005981
Batch 11/64 loss: 0.23248165845870972
Batch 12/64 loss: 0.22933411598205566
Batch 13/64 loss: 0.22394728660583496
Batch 14/64 loss: 0.22313565015792847
Batch 15/64 loss: 0.2197948694229126
Batch 16/64 loss: 0.22619199752807617
Batch 17/64 loss: 0.22421729564666748
Batch 18/64 loss: 0.23397576808929443
Batch 19/64 loss: 0.23652708530426025
Batch 20/64 loss: 0.23189020156860352
Batch 21/64 loss: 0.22871768474578857
Batch 22/64 loss: 0.23515385389328003
Batch 23/64 loss: 0.22627687454223633
Batch 24/64 loss: 0.22191429138183594
Batch 25/64 loss: 0.23104310035705566
Batch 26/64 loss: 0.25108802318573
Batch 27/64 loss: 0.22892582416534424
Batch 28/64 loss: 0.2388087511062622
Batch 29/64 loss: 0.2328110933303833
Batch 30/64 loss: 0.2302098274230957
Batch 31/64 loss: 0.22065812349319458
Batch 32/64 loss: 0.23602604866027832
Batch 33/64 loss: 0.2321116328239441
Batch 34/64 loss: 0.21832400560379028
Batch 35/64 loss: 0.23377519845962524
Batch 36/64 loss: 0.22124159336090088
Batch 37/64 loss: 0.22668075561523438
Batch 38/64 loss: 0.21869462728500366
Batch 39/64 loss: 0.23567843437194824
Batch 40/64 loss: 0.2217698097229004
Batch 41/64 loss: 0.22685110569000244
Batch 42/64 loss: 0.2292664647102356
Batch 43/64 loss: 0.2190537452697754
Batch 44/64 loss: 0.2145368456840515
Batch 45/64 loss: 0.22456347942352295
Batch 46/64 loss: 0.21308958530426025
Batch 47/64 loss: 0.23007285594940186
Batch 48/64 loss: 0.2322712540626526
Batch 49/64 loss: 0.21165114641189575
Batch 50/64 loss: 0.22587299346923828
Batch 51/64 loss: 0.20861399173736572
Batch 52/64 loss: 0.2298235297203064
Batch 53/64 loss: 0.24093157052993774
Batch 54/64 loss: 0.2274489402770996
Batch 55/64 loss: 0.21566003561019897
Batch 56/64 loss: 0.22113311290740967
Batch 57/64 loss: 0.2354602813720703
Batch 58/64 loss: 0.23002564907073975
Batch 59/64 loss: 0.23056262731552124
Batch 60/64 loss: 0.22602880001068115
Batch 61/64 loss: 0.2306957244873047
Batch 62/64 loss: 0.2212369441986084
Batch 63/64 loss: 0.2372208833694458
Batch 64/64 loss: 0.22605246305465698
Epoch 9  Train loss: 0.22844347977170756  Val loss: 0.23530436852543624
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 0.22594892978668213
Batch 2/64 loss: 0.23154306411743164
Batch 3/64 loss: 0.22215545177459717
Batch 4/64 loss: 0.20996367931365967
Batch 5/64 loss: 0.224936842918396
Batch 6/64 loss: 0.22320878505706787
Batch 7/64 loss: 0.22394800186157227
Batch 8/64 loss: 0.2237606644630432
Batch 9/64 loss: 0.2344568967819214
Batch 10/64 loss: 0.22974354028701782
Batch 11/64 loss: 0.2256109118461609
Batch 12/64 loss: 0.21915143728256226
Batch 13/64 loss: 0.21920305490493774
Batch 14/64 loss: 0.224249005317688
Batch 15/64 loss: 0.23419880867004395
Batch 16/64 loss: 0.23316681385040283
Batch 17/64 loss: 0.2098894715309143
Batch 18/64 loss: 0.21635746955871582
Batch 19/64 loss: 0.22061824798583984
Batch 20/64 loss: 0.23416739702224731
Batch 21/64 loss: 0.21410715579986572
Batch 22/64 loss: 0.22434961795806885
Batch 23/64 loss: 0.23169636726379395
Batch 24/64 loss: 0.23294275999069214
Batch 25/64 loss: 0.23139798641204834
Batch 26/64 loss: 0.23935675621032715
Batch 27/64 loss: 0.22290295362472534
Batch 28/64 loss: 0.22330379486083984
Batch 29/64 loss: 0.22832107543945312
Batch 30/64 loss: 0.24310576915740967
Batch 31/64 loss: 0.21907103061676025
Batch 32/64 loss: 0.22406995296478271
Batch 33/64 loss: 0.2203357219696045
Batch 34/64 loss: 0.2128758430480957
Batch 35/64 loss: 0.2166513204574585
Batch 36/64 loss: 0.2226986289024353
Batch 37/64 loss: 0.22091931104660034
Batch 38/64 loss: 0.22288060188293457
Batch 39/64 loss: 0.2214055061340332
Batch 40/64 loss: 0.22150051593780518
Batch 41/64 loss: 0.2199898362159729
Batch 42/64 loss: 0.22258591651916504
Batch 43/64 loss: 0.22420012950897217
Batch 44/64 loss: 0.232355535030365
Batch 45/64 loss: 0.20949244499206543
Batch 46/64 loss: 0.22484374046325684
Batch 47/64 loss: 0.21427452564239502
Batch 48/64 loss: 0.22218096256256104
Batch 49/64 loss: 0.23024868965148926
Batch 50/64 loss: 0.2120402455329895
Batch 51/64 loss: 0.2176513671875
Batch 52/64 loss: 0.2100280523300171
Batch 53/64 loss: 0.22774863243103027
Batch 54/64 loss: 0.2131868600845337
Batch 55/64 loss: 0.21640074253082275
Batch 56/64 loss: 0.21326637268066406
Batch 57/64 loss: 0.22465264797210693
Batch 58/64 loss: 0.23463910818099976
Batch 59/64 loss: 0.22588175535202026
Batch 60/64 loss: 0.21971571445465088
Batch 61/64 loss: 0.21532386541366577
Batch 62/64 loss: 0.23167645931243896
Batch 63/64 loss: 0.22216010093688965
Batch 64/64 loss: 0.21711063385009766
Epoch 10  Train loss: 0.22317722077463187  Val loss: 0.22696246682983084
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: 0.2173938751220703
Batch 2/64 loss: 0.22051048278808594
Batch 3/64 loss: 0.21463549137115479
Batch 4/64 loss: 0.2202010154724121
Batch 5/64 loss: 0.21239906549453735
Batch 6/64 loss: 0.2073577642440796
Batch 7/64 loss: 0.20514458417892456
Batch 8/64 loss: 0.21006619930267334
Batch 9/64 loss: 0.22369319200515747
Batch 10/64 loss: 0.2163359522819519
Batch 11/64 loss: 0.19709885120391846
Batch 12/64 loss: 0.23874640464782715
Batch 13/64 loss: 0.2157890796661377
Batch 14/64 loss: 0.22055649757385254
Batch 15/64 loss: 0.21943670511245728
Batch 16/64 loss: 0.21447741985321045
Batch 17/64 loss: 0.20816433429718018
Batch 18/64 loss: 0.21889817714691162
Batch 19/64 loss: 0.21994996070861816
Batch 20/64 loss: 0.2169169783592224
Batch 21/64 loss: 0.20951271057128906
Batch 22/64 loss: 0.22006213665008545
Batch 23/64 loss: 0.22008955478668213
Batch 24/64 loss: 0.21799921989440918
Batch 25/64 loss: 0.20810675621032715
Batch 26/64 loss: 0.22219246625900269
Batch 27/64 loss: 0.1981542706489563
Batch 28/64 loss: 0.21210795640945435
Batch 29/64 loss: 0.23150485754013062
Batch 30/64 loss: 0.20949018001556396
Batch 31/64 loss: 0.21379369497299194
Batch 32/64 loss: 0.20968371629714966
Batch 33/64 loss: 0.22378718852996826
Batch 34/64 loss: 0.21153688430786133
Batch 35/64 loss: 0.21290254592895508
Batch 36/64 loss: 0.22942888736724854
Batch 37/64 loss: 0.21710503101348877
Batch 38/64 loss: 0.22051680088043213
Batch 39/64 loss: 0.2269502878189087
Batch 40/64 loss: 0.23037117719650269
Batch 41/64 loss: 0.22721195220947266
Batch 42/64 loss: 0.22316670417785645
Batch 43/64 loss: 0.22906404733657837
Batch 44/64 loss: 0.22522306442260742
Batch 45/64 loss: 0.22612547874450684
Batch 46/64 loss: 0.22365570068359375
Batch 47/64 loss: 0.2292710542678833
Batch 48/64 loss: 0.2199016809463501
Batch 49/64 loss: 0.21425139904022217
Batch 50/64 loss: 0.22543275356292725
Batch 51/64 loss: 0.213981032371521
Batch 52/64 loss: 0.22985458374023438
Batch 53/64 loss: 0.21935003995895386
Batch 54/64 loss: 0.22294890880584717
Batch 55/64 loss: 0.2195112705230713
Batch 56/64 loss: 0.2154340147972107
Batch 57/64 loss: 0.22896331548690796
Batch 58/64 loss: 0.21936464309692383
Batch 59/64 loss: 0.2208927869796753
Batch 60/64 loss: 0.22196459770202637
Batch 61/64 loss: 0.2196306586265564
Batch 62/64 loss: 0.23715567588806152
Batch 63/64 loss: 0.22568917274475098
Batch 64/64 loss: 0.2082865834236145
Epoch 11  Train loss: 0.21893847573037242  Val loss: 0.2233510078843107
Saving best model, epoch: 11
Epoch 12
-------------------------------
Batch 1/64 loss: 0.2032564878463745
Batch 2/64 loss: 0.21790623664855957
Batch 3/64 loss: 0.1978456974029541
Batch 4/64 loss: 0.227699875831604
Batch 5/64 loss: 0.20960557460784912
Batch 6/64 loss: 0.2263495922088623
Batch 7/64 loss: 0.20932531356811523
Batch 8/64 loss: 0.21468651294708252
Batch 9/64 loss: 0.2280580997467041
Batch 10/64 loss: 0.22184628248214722
Batch 11/64 loss: 0.21339327096939087
Batch 12/64 loss: 0.22639620304107666
Batch 13/64 loss: 0.21856874227523804
Batch 14/64 loss: 0.21548283100128174
Batch 15/64 loss: 0.22323399782180786
Batch 16/64 loss: 0.21939599514007568
Batch 17/64 loss: 0.22116512060165405
Batch 18/64 loss: 0.2231506109237671
Batch 19/64 loss: 0.20901060104370117
Batch 20/64 loss: 0.2304699420928955
Batch 21/64 loss: 0.2069801688194275
Batch 22/64 loss: 0.21988153457641602
Batch 23/64 loss: 0.19962382316589355
Batch 24/64 loss: 0.22136342525482178
Batch 25/64 loss: 0.21287477016448975
Batch 26/64 loss: 0.20762991905212402
Batch 27/64 loss: 0.22629833221435547
Batch 28/64 loss: 0.21199655532836914
Batch 29/64 loss: 0.2069525122642517
Batch 30/64 loss: 0.20751166343688965
Batch 31/64 loss: 0.21955335140228271
Batch 32/64 loss: 0.21455347537994385
Batch 33/64 loss: 0.21043187379837036
Batch 34/64 loss: 0.20337629318237305
Batch 35/64 loss: 0.2112032175064087
Batch 36/64 loss: 0.2115817666053772
Batch 37/64 loss: 0.22076570987701416
Batch 38/64 loss: 0.20485156774520874
Batch 39/64 loss: 0.20760929584503174
Batch 40/64 loss: 0.2167080044746399
Batch 41/64 loss: 0.2140178680419922
Batch 42/64 loss: 0.21134328842163086
Batch 43/64 loss: 0.21501481533050537
Batch 44/64 loss: 0.22280240058898926
Batch 45/64 loss: 0.22235655784606934
Batch 46/64 loss: 0.1990639567375183
Batch 47/64 loss: 0.20943182706832886
Batch 48/64 loss: 0.21726000308990479
Batch 49/64 loss: 0.20657014846801758
Batch 50/64 loss: 0.2138378620147705
Batch 51/64 loss: 0.1970497965812683
Batch 52/64 loss: 0.20247185230255127
Batch 53/64 loss: 0.20861279964447021
Batch 54/64 loss: 0.20895755290985107
Batch 55/64 loss: 0.20641517639160156
Batch 56/64 loss: 0.19946187734603882
Batch 57/64 loss: 0.20759141445159912
Batch 58/64 loss: 0.20932793617248535
Batch 59/64 loss: 0.2152637243270874
Batch 60/64 loss: 0.22317218780517578
Batch 61/64 loss: 0.2193920612335205
Batch 62/64 loss: 0.20702677965164185
Batch 63/64 loss: 0.1919519305229187
Batch 64/64 loss: 0.206323504447937
Epoch 12  Train loss: 0.21304675597770542  Val loss: 0.2191640492976736
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 0.22374212741851807
Batch 2/64 loss: 0.19635486602783203
Batch 3/64 loss: 0.20930349826812744
Batch 4/64 loss: 0.21817374229431152
Batch 5/64 loss: 0.20261740684509277
Batch 6/64 loss: 0.1986890435218811
Batch 7/64 loss: 0.2159099578857422
Batch 8/64 loss: 0.21257543563842773
Batch 9/64 loss: 0.21217650175094604
Batch 10/64 loss: 0.19541382789611816
Batch 11/64 loss: 0.20675605535507202
Batch 12/64 loss: 0.22328245639801025
Batch 13/64 loss: 0.21004849672317505
Batch 14/64 loss: 0.19920754432678223
Batch 15/64 loss: 0.2118922472000122
Batch 16/64 loss: 0.21848630905151367
Batch 17/64 loss: 0.19910883903503418
Batch 18/64 loss: 0.20428264141082764
Batch 19/64 loss: 0.19816482067108154
Batch 20/64 loss: 0.20859527587890625
Batch 21/64 loss: 0.22724437713623047
Batch 22/64 loss: 0.2138269543647766
Batch 23/64 loss: 0.20941376686096191
Batch 24/64 loss: 0.21349114179611206
Batch 25/64 loss: 0.22471094131469727
Batch 26/64 loss: 0.21710503101348877
Batch 27/64 loss: 0.20097285509109497
Batch 28/64 loss: 0.20088982582092285
Batch 29/64 loss: 0.20367681980133057
Batch 30/64 loss: 0.20244646072387695
Batch 31/64 loss: 0.20060503482818604
Batch 32/64 loss: 0.20926451683044434
Batch 33/64 loss: 0.2194940447807312
Batch 34/64 loss: 0.20530563592910767
Batch 35/64 loss: 0.20877420902252197
Batch 36/64 loss: 0.20463287830352783
Batch 37/64 loss: 0.2069993019104004
Batch 38/64 loss: 0.20969080924987793
Batch 39/64 loss: 0.21022677421569824
Batch 40/64 loss: 0.20937341451644897
Batch 41/64 loss: 0.19056224822998047
Batch 42/64 loss: 0.21082329750061035
Batch 43/64 loss: 0.20747679471969604
Batch 44/64 loss: 0.2065056562423706
Batch 45/64 loss: 0.20967769622802734
Batch 46/64 loss: 0.19343137741088867
Batch 47/64 loss: 0.2101283073425293
Batch 48/64 loss: 0.1907622218132019
Batch 49/64 loss: 0.19558608531951904
Batch 50/64 loss: 0.2061939835548401
Batch 51/64 loss: 0.20862948894500732
Batch 52/64 loss: 0.20706474781036377
Batch 53/64 loss: 0.2068232297897339
Batch 54/64 loss: 0.2068690061569214
Batch 55/64 loss: 0.20882678031921387
Batch 56/64 loss: 0.18899106979370117
Batch 57/64 loss: 0.1997644305229187
Batch 58/64 loss: 0.18548732995986938
Batch 59/64 loss: 0.1960587501525879
Batch 60/64 loss: 0.19528460502624512
Batch 61/64 loss: 0.20724046230316162
Batch 62/64 loss: 0.20946025848388672
Batch 63/64 loss: 0.18903350830078125
Batch 64/64 loss: 0.2045750617980957
Epoch 13  Train loss: 0.20622805520599963  Val loss: 0.22354958454767862
Epoch 14
-------------------------------
Batch 1/64 loss: 0.20341932773590088
Batch 2/64 loss: 0.20132511854171753
Batch 3/64 loss: 0.21550512313842773
Batch 4/64 loss: 0.20939135551452637
Batch 5/64 loss: 0.1900044083595276
Batch 6/64 loss: 0.19787025451660156
Batch 7/64 loss: 0.18783223628997803
Batch 8/64 loss: 0.19977891445159912
Batch 9/64 loss: 0.19318777322769165
Batch 10/64 loss: 0.21594905853271484
Batch 11/64 loss: 0.20942938327789307
Batch 12/64 loss: 0.20997226238250732
Batch 13/64 loss: 0.20399343967437744
Batch 14/64 loss: 0.22073936462402344
Batch 15/64 loss: 0.20219814777374268
Batch 16/64 loss: 0.18784570693969727
Batch 17/64 loss: 0.1954730749130249
Batch 18/64 loss: 0.1970902681350708
Batch 19/64 loss: 0.21056586503982544
Batch 20/64 loss: 0.20100712776184082
Batch 21/64 loss: 0.19355535507202148
Batch 22/64 loss: 0.197282075881958
Batch 23/64 loss: 0.2025585174560547
Batch 24/64 loss: 0.19639325141906738
Batch 25/64 loss: 0.19695746898651123
Batch 26/64 loss: 0.19995450973510742
Batch 27/64 loss: 0.21840524673461914
Batch 28/64 loss: 0.20283222198486328
Batch 29/64 loss: 0.21135956048965454
Batch 30/64 loss: 0.18516480922698975
Batch 31/64 loss: 0.21227753162384033
Batch 32/64 loss: 0.20339298248291016
Batch 33/64 loss: 0.1931127905845642
Batch 34/64 loss: 0.1897268295288086
Batch 35/64 loss: 0.2075800895690918
Batch 36/64 loss: 0.20473933219909668
Batch 37/64 loss: 0.19253653287887573
Batch 38/64 loss: 0.2136412262916565
Batch 39/64 loss: 0.1913442611694336
Batch 40/64 loss: 0.21180319786071777
Batch 41/64 loss: 0.1934981346130371
Batch 42/64 loss: 0.2027994990348816
Batch 43/64 loss: 0.19481003284454346
Batch 44/64 loss: 0.1973961591720581
Batch 45/64 loss: 0.20073604583740234
Batch 46/64 loss: 0.1917075514793396
Batch 47/64 loss: 0.19457244873046875
Batch 48/64 loss: 0.19927579164505005
Batch 49/64 loss: 0.21537750959396362
Batch 50/64 loss: 0.200309157371521
Batch 51/64 loss: 0.180594801902771
Batch 52/64 loss: 0.21561521291732788
Batch 53/64 loss: 0.210976243019104
Batch 54/64 loss: 0.20235675573349
Batch 55/64 loss: 0.1929687261581421
Batch 56/64 loss: 0.20475876331329346
Batch 57/64 loss: 0.19667136669158936
Batch 58/64 loss: 0.20294082164764404
Batch 59/64 loss: 0.20123732089996338
Batch 60/64 loss: 0.1814899444580078
Batch 61/64 loss: 0.20462656021118164
Batch 62/64 loss: 0.1927098035812378
Batch 63/64 loss: 0.2079857587814331
Batch 64/64 loss: 0.1921064853668213
Epoch 14  Train loss: 0.20088925922618192  Val loss: 0.2183011551083568
Saving best model, epoch: 14
Epoch 15
-------------------------------
Batch 1/64 loss: 0.21598273515701294
Batch 2/64 loss: 0.2003898024559021
Batch 3/64 loss: 0.20057499408721924
Batch 4/64 loss: 0.1987929344177246
Batch 5/64 loss: 0.20148974657058716
Batch 6/64 loss: 0.19738513231277466
Batch 7/64 loss: 0.18707692623138428
Batch 8/64 loss: 0.19519859552383423
Batch 9/64 loss: 0.19346565008163452
Batch 10/64 loss: 0.1978602409362793
Batch 11/64 loss: 0.1862196922302246
Batch 12/64 loss: 0.17937111854553223
Batch 13/64 loss: 0.20429301261901855
Batch 14/64 loss: 0.20247459411621094
Batch 15/64 loss: 0.17588448524475098
Batch 16/64 loss: 0.18504774570465088
Batch 17/64 loss: 0.19261276721954346
Batch 18/64 loss: 0.204107403755188
Batch 19/64 loss: 0.20017337799072266
Batch 20/64 loss: 0.20612597465515137
Batch 21/64 loss: 0.19873690605163574
Batch 22/64 loss: 0.21744287014007568
Batch 23/64 loss: 0.18242764472961426
Batch 24/64 loss: 0.20254814624786377
Batch 25/64 loss: 0.20114648342132568
Batch 26/64 loss: 0.18484532833099365
Batch 27/64 loss: 0.20642399787902832
Batch 28/64 loss: 0.19826865196228027
Batch 29/64 loss: 0.20045208930969238
Batch 30/64 loss: 0.20164859294891357
Batch 31/64 loss: 0.19957220554351807
Batch 32/64 loss: 0.20237743854522705
Batch 33/64 loss: 0.1944822072982788
Batch 34/64 loss: 0.20186829566955566
Batch 35/64 loss: 0.21315431594848633
Batch 36/64 loss: 0.20137643814086914
Batch 37/64 loss: 0.19377410411834717
Batch 38/64 loss: 0.19456219673156738
Batch 39/64 loss: 0.18725574016571045
Batch 40/64 loss: 0.20404654741287231
Batch 41/64 loss: 0.18417131900787354
Batch 42/64 loss: 0.182015061378479
Batch 43/64 loss: 0.20001888275146484
Batch 44/64 loss: 0.19661718606948853
Batch 45/64 loss: 0.18525028228759766
Batch 46/64 loss: 0.18209141492843628
Batch 47/64 loss: 0.1769425868988037
Batch 48/64 loss: 0.18114781379699707
Batch 49/64 loss: 0.19224125146865845
Batch 50/64 loss: 0.2022383213043213
Batch 51/64 loss: 0.19674915075302124
Batch 52/64 loss: 0.18516415357589722
Batch 53/64 loss: 0.17320740222930908
Batch 54/64 loss: 0.21438419818878174
Batch 55/64 loss: 0.21029669046401978
Batch 56/64 loss: 0.18745559453964233
Batch 57/64 loss: 0.18347930908203125
Batch 58/64 loss: 0.20683550834655762
Batch 59/64 loss: 0.1882007122039795
Batch 60/64 loss: 0.21080344915390015
Batch 61/64 loss: 0.22021102905273438
Batch 62/64 loss: 0.20154470205307007
Batch 63/64 loss: 0.2067280411720276
Batch 64/64 loss: 0.21201831102371216
Epoch 15  Train loss: 0.1966705086184483  Val loss: 0.20438064721851415
Saving best model, epoch: 15
Epoch 16
-------------------------------
Batch 1/64 loss: 0.20231831073760986
Batch 2/64 loss: 0.2023918628692627
Batch 3/64 loss: 0.18881499767303467
Batch 4/64 loss: 0.19909298419952393
Batch 5/64 loss: 0.20433270931243896
Batch 6/64 loss: 0.20318275690078735
Batch 7/64 loss: 0.19426417350769043
Batch 8/64 loss: 0.183937668800354
Batch 9/64 loss: 0.1859416961669922
Batch 10/64 loss: 0.19632422924041748
Batch 11/64 loss: 0.19473564624786377
Batch 12/64 loss: 0.20547157526016235
Batch 13/64 loss: 0.18586862087249756
Batch 14/64 loss: 0.19918406009674072
Batch 15/64 loss: 0.20340299606323242
Batch 16/64 loss: 0.1886412501335144
Batch 17/64 loss: 0.20489048957824707
Batch 18/64 loss: 0.18723809719085693
Batch 19/64 loss: 0.19011592864990234
Batch 20/64 loss: 0.19703269004821777
Batch 21/64 loss: 0.20435774326324463
Batch 22/64 loss: 0.19753026962280273
Batch 23/64 loss: 0.18362271785736084
Batch 24/64 loss: 0.1962815523147583
Batch 25/64 loss: 0.18672454357147217
Batch 26/64 loss: 0.18776506185531616
Batch 27/64 loss: 0.20863258838653564
Batch 28/64 loss: 0.1897650957107544
Batch 29/64 loss: 0.19812870025634766
Batch 30/64 loss: 0.1892017126083374
Batch 31/64 loss: 0.2077431082725525
Batch 32/64 loss: 0.19474422931671143
Batch 33/64 loss: 0.1820787787437439
Batch 34/64 loss: 0.20036637783050537
Batch 35/64 loss: 0.21004903316497803
Batch 36/64 loss: 0.1993967890739441
Batch 37/64 loss: 0.18754947185516357
Batch 38/64 loss: 0.1860339641571045
Batch 39/64 loss: 0.21134501695632935
Batch 40/64 loss: 0.18578511476516724
Batch 41/64 loss: 0.19986200332641602
Batch 42/64 loss: 0.17603576183319092
Batch 43/64 loss: 0.17998260259628296
Batch 44/64 loss: 0.20347940921783447
Batch 45/64 loss: 0.19662821292877197
Batch 46/64 loss: 0.1882932186126709
Batch 47/64 loss: 0.18131089210510254
Batch 48/64 loss: 0.17987018823623657
Batch 49/64 loss: 0.18411672115325928
Batch 50/64 loss: 0.189414381980896
Batch 51/64 loss: 0.1866346001625061
Batch 52/64 loss: 0.19368863105773926
Batch 53/64 loss: 0.19340676069259644
Batch 54/64 loss: 0.2004002332687378
Batch 55/64 loss: 0.1888413429260254
Batch 56/64 loss: 0.18305587768554688
Batch 57/64 loss: 0.17241418361663818
Batch 58/64 loss: 0.18174773454666138
Batch 59/64 loss: 0.1810629963874817
Batch 60/64 loss: 0.18486839532852173
Batch 61/64 loss: 0.1803644299507141
Batch 62/64 loss: 0.19481819868087769
Batch 63/64 loss: 0.17829948663711548
Batch 64/64 loss: 0.16304010152816772
Epoch 16  Train loss: 0.19208095611310472  Val loss: 0.19594170567915611
Saving best model, epoch: 16
Epoch 17
-------------------------------
Batch 1/64 loss: 0.18314391374588013
Batch 2/64 loss: 0.19561725854873657
Batch 3/64 loss: 0.18431508541107178
Batch 4/64 loss: 0.17940044403076172
Batch 5/64 loss: 0.17995166778564453
Batch 6/64 loss: 0.17879372835159302
Batch 7/64 loss: 0.19340503215789795
Batch 8/64 loss: 0.19550663232803345
Batch 9/64 loss: 0.18415844440460205
Batch 10/64 loss: 0.17973411083221436
Batch 11/64 loss: 0.17854106426239014
Batch 12/64 loss: 0.18567270040512085
Batch 13/64 loss: 0.19884783029556274
Batch 14/64 loss: 0.1794862151145935
Batch 15/64 loss: 0.19809222221374512
Batch 16/64 loss: 0.17830193042755127
Batch 17/64 loss: 0.2058645486831665
Batch 18/64 loss: 0.19195640087127686
Batch 19/64 loss: 0.19634735584259033
Batch 20/64 loss: 0.1744176745414734
Batch 21/64 loss: 0.19209015369415283
Batch 22/64 loss: 0.17581075429916382
Batch 23/64 loss: 0.16743707656860352
Batch 24/64 loss: 0.19268155097961426
Batch 25/64 loss: 0.1819615364074707
Batch 26/64 loss: 0.19211113452911377
Batch 27/64 loss: 0.1891423463821411
Batch 28/64 loss: 0.2031245231628418
Batch 29/64 loss: 0.19987523555755615
Batch 30/64 loss: 0.19713377952575684
Batch 31/64 loss: 0.16388410329818726
Batch 32/64 loss: 0.17133235931396484
Batch 33/64 loss: 0.18755829334259033
Batch 34/64 loss: 0.19120049476623535
Batch 35/64 loss: 0.19611549377441406
Batch 36/64 loss: 0.18131756782531738
Batch 37/64 loss: 0.17946362495422363
Batch 38/64 loss: 0.180680513381958
Batch 39/64 loss: 0.18702232837677002
Batch 40/64 loss: 0.17216122150421143
Batch 41/64 loss: 0.18453097343444824
Batch 42/64 loss: 0.1823575496673584
Batch 43/64 loss: 0.19011163711547852
Batch 44/64 loss: 0.1966117024421692
Batch 45/64 loss: 0.1802741289138794
Batch 46/64 loss: 0.1864529252052307
Batch 47/64 loss: 0.173445463180542
Batch 48/64 loss: 0.18689018487930298
Batch 49/64 loss: 0.1718461513519287
Batch 50/64 loss: 0.18880128860473633
Batch 51/64 loss: 0.18498563766479492
Batch 52/64 loss: 0.1974543333053589
Batch 53/64 loss: 0.17921710014343262
Batch 54/64 loss: 0.17218095064163208
Batch 55/64 loss: 0.17980968952178955
Batch 56/64 loss: 0.17759239673614502
Batch 57/64 loss: 0.17589473724365234
Batch 58/64 loss: 0.20049017667770386
Batch 59/64 loss: 0.17116522789001465
Batch 60/64 loss: 0.17570549249649048
Batch 61/64 loss: 0.1976940631866455
Batch 62/64 loss: 0.1741858720779419
Batch 63/64 loss: 0.1900758147239685
Batch 64/64 loss: 0.18573695421218872
Epoch 17  Train loss: 0.1851095617986193  Val loss: 0.19437915516882828
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 0.1696382761001587
Batch 2/64 loss: 0.17218482494354248
Batch 3/64 loss: 0.18119168281555176
Batch 4/64 loss: 0.1794496774673462
Batch 5/64 loss: 0.1767057180404663
Batch 6/64 loss: 0.17112958431243896
Batch 7/64 loss: 0.19931071996688843
Batch 8/64 loss: 0.18304443359375
Batch 9/64 loss: 0.19538193941116333
Batch 10/64 loss: 0.17169725894927979
Batch 11/64 loss: 0.17318034172058105
Batch 12/64 loss: 0.17984414100646973
Batch 13/64 loss: 0.1906103491783142
Batch 14/64 loss: 0.19800174236297607
Batch 15/64 loss: 0.17936253547668457
Batch 16/64 loss: 0.16805648803710938
Batch 17/64 loss: 0.18042206764221191
Batch 18/64 loss: 0.18257832527160645
Batch 19/64 loss: 0.17549574375152588
Batch 20/64 loss: 0.18826067447662354
Batch 21/64 loss: 0.18443864583969116
Batch 22/64 loss: 0.17985820770263672
Batch 23/64 loss: 0.18994176387786865
Batch 24/64 loss: 0.18488550186157227
Batch 25/64 loss: 0.16857939958572388
Batch 26/64 loss: 0.16962450742721558
Batch 27/64 loss: 0.18027567863464355
Batch 28/64 loss: 0.18206804990768433
Batch 29/64 loss: 0.18342620134353638
Batch 30/64 loss: 0.19497841596603394
Batch 31/64 loss: 0.17917823791503906
Batch 32/64 loss: 0.19278395175933838
Batch 33/64 loss: 0.179085373878479
Batch 34/64 loss: 0.18630635738372803
Batch 35/64 loss: 0.17808353900909424
Batch 36/64 loss: 0.17208796739578247
Batch 37/64 loss: 0.1742267608642578
Batch 38/64 loss: 0.17650896310806274
Batch 39/64 loss: 0.1865471601486206
Batch 40/64 loss: 0.16907405853271484
Batch 41/64 loss: 0.1767098307609558
Batch 42/64 loss: 0.19713610410690308
Batch 43/64 loss: 0.18190228939056396
Batch 44/64 loss: 0.20322012901306152
Batch 45/64 loss: 0.18540239334106445
Batch 46/64 loss: 0.18178510665893555
Batch 47/64 loss: 0.17866110801696777
Batch 48/64 loss: 0.1864197850227356
Batch 49/64 loss: 0.19328486919403076
Batch 50/64 loss: 0.17841041088104248
Batch 51/64 loss: 0.1910618543624878
Batch 52/64 loss: 0.17097687721252441
Batch 53/64 loss: 0.17689049243927002
Batch 54/64 loss: 0.1835780143737793
Batch 55/64 loss: 0.15950316190719604
Batch 56/64 loss: 0.16978245973587036
Batch 57/64 loss: 0.18144595623016357
Batch 58/64 loss: 0.1811842918395996
Batch 59/64 loss: 0.1838778257369995
Batch 60/64 loss: 0.17420005798339844
Batch 61/64 loss: 0.1756802797317505
Batch 62/64 loss: 0.16313666105270386
Batch 63/64 loss: 0.1624642014503479
Batch 64/64 loss: 0.17771852016448975
Epoch 18  Train loss: 0.18035307164285697  Val loss: 0.1929957721241561
Saving best model, epoch: 18
Epoch 19
-------------------------------
Batch 1/64 loss: 0.19656729698181152
Batch 2/64 loss: 0.19267427921295166
Batch 3/64 loss: 0.1782333254814148
Batch 4/64 loss: 0.18114739656448364
Batch 5/64 loss: 0.16461634635925293
Batch 6/64 loss: 0.19363874197006226
Batch 7/64 loss: 0.16982996463775635
Batch 8/64 loss: 0.17087090015411377
Batch 9/64 loss: 0.18198788166046143
Batch 10/64 loss: 0.17050135135650635
Batch 11/64 loss: 0.17975246906280518
Batch 12/64 loss: 0.17598748207092285
Batch 13/64 loss: 0.17904841899871826
Batch 14/64 loss: 0.18616443872451782
Batch 15/64 loss: 0.16890788078308105
Batch 16/64 loss: 0.2076336145401001
Batch 17/64 loss: 0.1773231029510498
Batch 18/64 loss: 0.17521262168884277
Batch 19/64 loss: 0.17455518245697021
Batch 20/64 loss: 0.1952061653137207
Batch 21/64 loss: 0.17091816663742065
Batch 22/64 loss: 0.18453115224838257
Batch 23/64 loss: 0.17543238401412964
Batch 24/64 loss: 0.1681792140007019
Batch 25/64 loss: 0.1841219663619995
Batch 26/64 loss: 0.1681826114654541
Batch 27/64 loss: 0.17083758115768433
Batch 28/64 loss: 0.19021737575531006
Batch 29/64 loss: 0.19037538766860962
Batch 30/64 loss: 0.18652290105819702
Batch 31/64 loss: 0.16429245471954346
Batch 32/64 loss: 0.17932653427124023
Batch 33/64 loss: 0.1808110475540161
Batch 34/64 loss: 0.17099761962890625
Batch 35/64 loss: 0.17397284507751465
Batch 36/64 loss: 0.16614019870758057
Batch 37/64 loss: 0.17938125133514404
Batch 38/64 loss: 0.17226684093475342
Batch 39/64 loss: 0.1680469512939453
Batch 40/64 loss: 0.18627691268920898
Batch 41/64 loss: 0.17394089698791504
Batch 42/64 loss: 0.17548727989196777
Batch 43/64 loss: 0.15632861852645874
Batch 44/64 loss: 0.16388314962387085
Batch 45/64 loss: 0.17441177368164062
Batch 46/64 loss: 0.17024028301239014
Batch 47/64 loss: 0.17320513725280762
Batch 48/64 loss: 0.18191134929656982
Batch 49/64 loss: 0.169685959815979
Batch 50/64 loss: 0.16922199726104736
Batch 51/64 loss: 0.1649090051651001
Batch 52/64 loss: 0.17026972770690918
Batch 53/64 loss: 0.16401302814483643
Batch 54/64 loss: 0.166750967502594
Batch 55/64 loss: 0.15453356504440308
Batch 56/64 loss: 0.17078900337219238
Batch 57/64 loss: 0.1706146001815796
Batch 58/64 loss: 0.15503758192062378
Batch 59/64 loss: 0.16698741912841797
Batch 60/64 loss: 0.16741275787353516
Batch 61/64 loss: 0.16862285137176514
Batch 62/64 loss: 0.17118442058563232
Batch 63/64 loss: 0.17491275072097778
Batch 64/64 loss: 0.19493508338928223
Epoch 19  Train loss: 0.17523522657506607  Val loss: 0.18543489610206632
Saving best model, epoch: 19
Epoch 20
-------------------------------
Batch 1/64 loss: 0.18615472316741943
Batch 2/64 loss: 0.18282175064086914
Batch 3/64 loss: 0.15310513973236084
Batch 4/64 loss: 0.17290306091308594
Batch 5/64 loss: 0.1651948094367981
Batch 6/64 loss: 0.16869306564331055
Batch 7/64 loss: 0.17062890529632568
Batch 8/64 loss: 0.18215996026992798
Batch 9/64 loss: 0.15668410062789917
Batch 10/64 loss: 0.1681879162788391
Batch 11/64 loss: 0.1586223840713501
Batch 12/64 loss: 0.17754369974136353
Batch 13/64 loss: 0.1788696050643921
Batch 14/64 loss: 0.16994380950927734
Batch 15/64 loss: 0.15764540433883667
Batch 16/64 loss: 0.15753614902496338
Batch 17/64 loss: 0.166143536567688
Batch 18/64 loss: 0.15557479858398438
Batch 19/64 loss: 0.15677118301391602
Batch 20/64 loss: 0.17062443494796753
Batch 21/64 loss: 0.17983973026275635
Batch 22/64 loss: 0.1601167917251587
Batch 23/64 loss: 0.16050750017166138
Batch 24/64 loss: 0.1493445634841919
Batch 25/64 loss: 0.1562555432319641
Batch 26/64 loss: 0.1732972264289856
Batch 27/64 loss: 0.15886634588241577
Batch 28/64 loss: 0.16190236806869507
Batch 29/64 loss: 0.17443078756332397
Batch 30/64 loss: 0.18157893419265747
Batch 31/64 loss: 0.16227155923843384
Batch 32/64 loss: 0.1664595603942871
Batch 33/64 loss: 0.14905446767807007
Batch 34/64 loss: 0.17883199453353882
Batch 35/64 loss: 0.1901942491531372
Batch 36/64 loss: 0.14547091722488403
Batch 37/64 loss: 0.16944992542266846
Batch 38/64 loss: 0.16915905475616455
Batch 39/64 loss: 0.16823071241378784
Batch 40/64 loss: 0.15355950593948364
Batch 41/64 loss: 0.16887009143829346
Batch 42/64 loss: 0.19344723224639893
Batch 43/64 loss: 0.18184399604797363
Batch 44/64 loss: 0.17064189910888672
Batch 45/64 loss: 0.18425005674362183
Batch 46/64 loss: 0.17994332313537598
Batch 47/64 loss: 0.17605799436569214
Batch 48/64 loss: 0.1746464967727661
Batch 49/64 loss: 0.1607871651649475
Batch 50/64 loss: 0.1612454652786255
Batch 51/64 loss: 0.169103741645813
Batch 52/64 loss: 0.16866570711135864
Batch 53/64 loss: 0.16577613353729248
Batch 54/64 loss: 0.16431057453155518
Batch 55/64 loss: 0.17528223991394043
Batch 56/64 loss: 0.17561936378479004
Batch 57/64 loss: 0.19324719905853271
Batch 58/64 loss: 0.15937674045562744
Batch 59/64 loss: 0.1626606583595276
Batch 60/64 loss: 0.18185698986053467
Batch 61/64 loss: 0.16214710474014282
Batch 62/64 loss: 0.16567617654800415
Batch 63/64 loss: 0.15870791673660278
Batch 64/64 loss: 0.15765219926834106
Epoch 20  Train loss: 0.16842405632430432  Val loss: 0.17816258910595348
Saving best model, epoch: 20
Epoch 21
-------------------------------
Batch 1/64 loss: 0.15765732526779175
Batch 2/64 loss: 0.17599111795425415
Batch 3/64 loss: 0.16642165184020996
Batch 4/64 loss: 0.1851096749305725
Batch 5/64 loss: 0.16794955730438232
Batch 6/64 loss: 0.16044878959655762
Batch 7/64 loss: 0.17388570308685303
Batch 8/64 loss: 0.16588497161865234
Batch 9/64 loss: 0.18083906173706055
Batch 10/64 loss: 0.16780227422714233
Batch 11/64 loss: 0.16360926628112793
Batch 12/64 loss: 0.1608954668045044
Batch 13/64 loss: 0.15067237615585327
Batch 14/64 loss: 0.1661829948425293
Batch 15/64 loss: 0.16839253902435303
Batch 16/64 loss: 0.16327548027038574
Batch 17/64 loss: 0.14560353755950928
Batch 18/64 loss: 0.1497710943222046
Batch 19/64 loss: 0.15607786178588867
Batch 20/64 loss: 0.17160260677337646
Batch 21/64 loss: 0.1703624129295349
Batch 22/64 loss: 0.16746211051940918
Batch 23/64 loss: 0.170660138130188
Batch 24/64 loss: 0.15997886657714844
Batch 25/64 loss: 0.1792978048324585
Batch 26/64 loss: 0.1573745608329773
Batch 27/64 loss: 0.1590820550918579
Batch 28/64 loss: 0.15468603372573853
Batch 29/64 loss: 0.15192484855651855
Batch 30/64 loss: 0.1630265712738037
Batch 31/64 loss: 0.19282233715057373
Batch 32/64 loss: 0.17511844635009766
Batch 33/64 loss: 0.1652439832687378
Batch 34/64 loss: 0.18927514553070068
Batch 35/64 loss: 0.13763844966888428
Batch 36/64 loss: 0.18915826082229614
Batch 37/64 loss: 0.1458505392074585
Batch 38/64 loss: 0.1669442057609558
Batch 39/64 loss: 0.16305887699127197
Batch 40/64 loss: 0.15326368808746338
Batch 41/64 loss: 0.19309675693511963
Batch 42/64 loss: 0.14061200618743896
Batch 43/64 loss: 0.1635282039642334
Batch 44/64 loss: 0.17423152923583984
Batch 45/64 loss: 0.15086907148361206
Batch 46/64 loss: 0.14892667531967163
Batch 47/64 loss: 0.1851198673248291
Batch 48/64 loss: 0.16687214374542236
Batch 49/64 loss: 0.17138612270355225
Batch 50/64 loss: 0.15303027629852295
Batch 51/64 loss: 0.16898095607757568
Batch 52/64 loss: 0.15328878164291382
Batch 53/64 loss: 0.16100454330444336
Batch 54/64 loss: 0.1570889949798584
Batch 55/64 loss: 0.17002683877944946
Batch 56/64 loss: 0.1619827151298523
Batch 57/64 loss: 0.1667553186416626
Batch 58/64 loss: 0.13928192853927612
Batch 59/64 loss: 0.1670563817024231
Batch 60/64 loss: 0.1671832799911499
Batch 61/64 loss: 0.16649532318115234
Batch 62/64 loss: 0.1567489504814148
Batch 63/64 loss: 0.15813583135604858
Batch 64/64 loss: 0.14920556545257568
Epoch 21  Train loss: 0.16413975463194005  Val loss: 0.17813659001052176
Saving best model, epoch: 21
Epoch 22
-------------------------------
Batch 1/64 loss: 0.15671712160110474
Batch 2/64 loss: 0.162178635597229
Batch 3/64 loss: 0.14689522981643677
Batch 4/64 loss: 0.18029558658599854
Batch 5/64 loss: 0.17274034023284912
Batch 6/64 loss: 0.16873717308044434
Batch 7/64 loss: 0.13383382558822632
Batch 8/64 loss: 0.17054009437561035
Batch 9/64 loss: 0.1708032488822937
Batch 10/64 loss: 0.17858678102493286
Batch 11/64 loss: 0.1485205888748169
Batch 12/64 loss: 0.13823574781417847
Batch 13/64 loss: 0.17488199472427368
Batch 14/64 loss: 0.14867520332336426
Batch 15/64 loss: 0.15900975465774536
Batch 16/64 loss: 0.16389119625091553
Batch 17/64 loss: 0.17231982946395874
Batch 18/64 loss: 0.14360970258712769
Batch 19/64 loss: 0.15278983116149902
Batch 20/64 loss: 0.15720725059509277
Batch 21/64 loss: 0.1420050859451294
Batch 22/64 loss: 0.14929604530334473
Batch 23/64 loss: 0.1568361520767212
Batch 24/64 loss: 0.16711294651031494
Batch 25/64 loss: 0.1506602168083191
Batch 26/64 loss: 0.15159934759140015
Batch 27/64 loss: 0.17304551601409912
Batch 28/64 loss: 0.17460286617279053
Batch 29/64 loss: 0.16112935543060303
Batch 30/64 loss: 0.16115784645080566
Batch 31/64 loss: 0.16323494911193848
Batch 32/64 loss: 0.16490060091018677
Batch 33/64 loss: 0.16080355644226074
Batch 34/64 loss: 0.15047919750213623
Batch 35/64 loss: 0.1659177541732788
Batch 36/64 loss: 0.1445852518081665
Batch 37/64 loss: 0.1663709282875061
Batch 38/64 loss: 0.15354949235916138
Batch 39/64 loss: 0.1450170874595642
Batch 40/64 loss: 0.16390907764434814
Batch 41/64 loss: 0.14944052696228027
Batch 42/64 loss: 0.13909202814102173
Batch 43/64 loss: 0.1358640193939209
Batch 44/64 loss: 0.15553295612335205
Batch 45/64 loss: 0.14610612392425537
Batch 46/64 loss: 0.16254514455795288
Batch 47/64 loss: 0.1619335412979126
Batch 48/64 loss: 0.14277487993240356
Batch 49/64 loss: 0.15497994422912598
Batch 50/64 loss: 0.1716935634613037
Batch 51/64 loss: 0.14196956157684326
Batch 52/64 loss: 0.17260527610778809
Batch 53/64 loss: 0.15180253982543945
Batch 54/64 loss: 0.13971638679504395
Batch 55/64 loss: 0.15151822566986084
Batch 56/64 loss: 0.1580744981765747
Batch 57/64 loss: 0.16341537237167358
Batch 58/64 loss: 0.13778680562973022
Batch 59/64 loss: 0.1719198226928711
Batch 60/64 loss: 0.12601816654205322
Batch 61/64 loss: 0.1482452154159546
Batch 62/64 loss: 0.1691666841506958
Batch 63/64 loss: 0.15856695175170898
Batch 64/64 loss: 0.15750503540039062
Epoch 22  Train loss: 0.15679340269051345  Val loss: 0.17193252954286398
Saving best model, epoch: 22
Epoch 23
-------------------------------
Batch 1/64 loss: 0.13605594635009766
Batch 2/64 loss: 0.15974652767181396
Batch 3/64 loss: 0.16386008262634277
Batch 4/64 loss: 0.15257024765014648
Batch 5/64 loss: 0.18004381656646729
Batch 6/64 loss: 0.1649659276008606
Batch 7/64 loss: 0.1457664966583252
Batch 8/64 loss: 0.15903884172439575
Batch 9/64 loss: 0.1422513723373413
Batch 10/64 loss: 0.14586001634597778
Batch 11/64 loss: 0.14694970846176147
Batch 12/64 loss: 0.1378437876701355
Batch 13/64 loss: 0.13290244340896606
Batch 14/64 loss: 0.13570374250411987
Batch 15/64 loss: 0.1669449806213379
Batch 16/64 loss: 0.14967280626296997
Batch 17/64 loss: 0.15536147356033325
Batch 18/64 loss: 0.15271210670471191
Batch 19/64 loss: 0.14797848463058472
Batch 20/64 loss: 0.14686518907546997
Batch 21/64 loss: 0.14998233318328857
Batch 22/64 loss: 0.15449368953704834
Batch 23/64 loss: 0.139978289604187
Batch 24/64 loss: 0.17063772678375244
Batch 25/64 loss: 0.17392408847808838
Batch 26/64 loss: 0.16264653205871582
Batch 27/64 loss: 0.16369163990020752
Batch 28/64 loss: 0.12708991765975952
Batch 29/64 loss: 0.14611279964447021
Batch 30/64 loss: 0.16406553983688354
Batch 31/64 loss: 0.12406051158905029
Batch 32/64 loss: 0.1475542187690735
Batch 33/64 loss: 0.14890581369400024
Batch 34/64 loss: 0.14935052394866943
Batch 35/64 loss: 0.12530267238616943
Batch 36/64 loss: 0.15254902839660645
Batch 37/64 loss: 0.17820250988006592
Batch 38/64 loss: 0.16244685649871826
Batch 39/64 loss: 0.14558148384094238
Batch 40/64 loss: 0.1775919795036316
Batch 41/64 loss: 0.14021849632263184
Batch 42/64 loss: 0.15740621089935303
Batch 43/64 loss: 0.14707589149475098
Batch 44/64 loss: 0.16755861043930054
Batch 45/64 loss: 0.1699284315109253
Batch 46/64 loss: 0.13634008169174194
Batch 47/64 loss: 0.15563839673995972
Batch 48/64 loss: 0.15231895446777344
Batch 49/64 loss: 0.1683565378189087
Batch 50/64 loss: 0.16467320919036865
Batch 51/64 loss: 0.15552133321762085
Batch 52/64 loss: 0.1591256856918335
Batch 53/64 loss: 0.15379905700683594
Batch 54/64 loss: 0.14138758182525635
Batch 55/64 loss: 0.17209267616271973
Batch 56/64 loss: 0.13610965013504028
Batch 57/64 loss: 0.1557062864303589
Batch 58/64 loss: 0.16690003871917725
Batch 59/64 loss: 0.1317346692085266
Batch 60/64 loss: 0.12721681594848633
Batch 61/64 loss: 0.1410616636276245
Batch 62/64 loss: 0.14065659046173096
Batch 63/64 loss: 0.1597001552581787
Batch 64/64 loss: 0.15684884786605835
Epoch 23  Train loss: 0.15224197354971195  Val loss: 0.1745836687661528
Epoch 24
-------------------------------
Batch 1/64 loss: 0.17930763959884644
Batch 2/64 loss: 0.1561017632484436
Batch 3/64 loss: 0.1598014235496521
Batch 4/64 loss: 0.1519966721534729
Batch 5/64 loss: 0.16542881727218628
Batch 6/64 loss: 0.1322813630104065
Batch 7/64 loss: 0.14281582832336426
Batch 8/64 loss: 0.13517969846725464
Batch 9/64 loss: 0.1383923888206482
Batch 10/64 loss: 0.16288578510284424
Batch 11/64 loss: 0.15129488706588745
Batch 12/64 loss: 0.17377471923828125
Batch 13/64 loss: 0.16472965478897095
Batch 14/64 loss: 0.15561199188232422
Batch 15/64 loss: 0.1332763433456421
Batch 16/64 loss: 0.1394284963607788
Batch 17/64 loss: 0.13972944021224976
Batch 18/64 loss: 0.133337140083313
Batch 19/64 loss: 0.13786965608596802
Batch 20/64 loss: 0.14733481407165527
Batch 21/64 loss: 0.14976251125335693
Batch 22/64 loss: 0.1456204056739807
Batch 23/64 loss: 0.15828073024749756
Batch 24/64 loss: 0.1531747579574585
Batch 25/64 loss: 0.1437990665435791
Batch 26/64 loss: 0.13087230920791626
Batch 27/64 loss: 0.13658607006072998
Batch 28/64 loss: 0.14302098751068115
Batch 29/64 loss: 0.13631272315979004
Batch 30/64 loss: 0.16232120990753174
Batch 31/64 loss: 0.15204715728759766
Batch 32/64 loss: 0.1447739601135254
Batch 33/64 loss: 0.18094795942306519
Batch 34/64 loss: 0.1582617163658142
Batch 35/64 loss: 0.1295924186706543
Batch 36/64 loss: 0.14664298295974731
Batch 37/64 loss: 0.12949788570404053
Batch 38/64 loss: 0.1582154631614685
Batch 39/64 loss: 0.15332281589508057
Batch 40/64 loss: 0.1548076868057251
Batch 41/64 loss: 0.15202176570892334
Batch 42/64 loss: 0.15025854110717773
Batch 43/64 loss: 0.14112168550491333
Batch 44/64 loss: 0.15116482973098755
Batch 45/64 loss: 0.14675766229629517
Batch 46/64 loss: 0.14932376146316528
Batch 47/64 loss: 0.1393011212348938
Batch 48/64 loss: 0.14285463094711304
Batch 49/64 loss: 0.15795838832855225
Batch 50/64 loss: 0.1624516248703003
Batch 51/64 loss: 0.14384692907333374
Batch 52/64 loss: 0.1289827823638916
Batch 53/64 loss: 0.13622939586639404
Batch 54/64 loss: 0.14212065935134888
Batch 55/64 loss: 0.1360459327697754
Batch 56/64 loss: 0.14631986618041992
Batch 57/64 loss: 0.1638340950012207
Batch 58/64 loss: 0.13451766967773438
Batch 59/64 loss: 0.17455756664276123
Batch 60/64 loss: 0.15680396556854248
Batch 61/64 loss: 0.15909737348556519
Batch 62/64 loss: 0.1418757438659668
Batch 63/64 loss: 0.13177454471588135
Batch 64/64 loss: 0.13337087631225586
Epoch 24  Train loss: 0.14835589072283575  Val loss: 0.16506521152876497
Saving best model, epoch: 24
Epoch 25
-------------------------------
Batch 1/64 loss: 0.164345383644104
Batch 2/64 loss: 0.15501219034194946
Batch 3/64 loss: 0.13470739126205444
Batch 4/64 loss: 0.1396346092224121
Batch 5/64 loss: 0.1629033088684082
Batch 6/64 loss: 0.15567219257354736
Batch 7/64 loss: 0.16494375467300415
Batch 8/64 loss: 0.15895378589630127
Batch 9/64 loss: 0.1438509225845337
Batch 10/64 loss: 0.1369953155517578
Batch 11/64 loss: 0.13237005472183228
Batch 12/64 loss: 0.1573314666748047
Batch 13/64 loss: 0.153603196144104
Batch 14/64 loss: 0.13682711124420166
Batch 15/64 loss: 0.1439892053604126
Batch 16/64 loss: 0.14894086122512817
Batch 17/64 loss: 0.12914466857910156
Batch 18/64 loss: 0.13758838176727295
Batch 19/64 loss: 0.11300891637802124
Batch 20/64 loss: 0.15744411945343018
Batch 21/64 loss: 0.13294875621795654
Batch 22/64 loss: 0.12546467781066895
Batch 23/64 loss: 0.1263195276260376
Batch 24/64 loss: 0.12400585412979126
Batch 25/64 loss: 0.14730358123779297
Batch 26/64 loss: 0.14924514293670654
Batch 27/64 loss: 0.14269787073135376
Batch 28/64 loss: 0.14173752069473267
Batch 29/64 loss: 0.12467128038406372
Batch 30/64 loss: 0.13971161842346191
Batch 31/64 loss: 0.14403170347213745
Batch 32/64 loss: 0.14044243097305298
Batch 33/64 loss: 0.1615346074104309
Batch 34/64 loss: 0.1410285234451294
Batch 35/64 loss: 0.1449284553527832
Batch 36/64 loss: 0.12738871574401855
Batch 37/64 loss: 0.14745092391967773
Batch 38/64 loss: 0.1478431224822998
Batch 39/64 loss: 0.15126651525497437
Batch 40/64 loss: 0.1465967893600464
Batch 41/64 loss: 0.13869315385818481
Batch 42/64 loss: 0.12689083814620972
Batch 43/64 loss: 0.13180488348007202
Batch 44/64 loss: 0.12948215007781982
Batch 45/64 loss: 0.14491266012191772
Batch 46/64 loss: 0.15735554695129395
Batch 47/64 loss: 0.14754384756088257
Batch 48/64 loss: 0.13674497604370117
Batch 49/64 loss: 0.16593235731124878
Batch 50/64 loss: 0.14186477661132812
Batch 51/64 loss: 0.1541513204574585
Batch 52/64 loss: 0.14205777645111084
Batch 53/64 loss: 0.14972782135009766
Batch 54/64 loss: 0.14542531967163086
Batch 55/64 loss: 0.15701395273208618
Batch 56/64 loss: 0.1685236096382141
Batch 57/64 loss: 0.13478678464889526
Batch 58/64 loss: 0.14812684059143066
Batch 59/64 loss: 0.1520472764968872
Batch 60/64 loss: 0.14538902044296265
Batch 61/64 loss: 0.1298837661743164
Batch 62/64 loss: 0.1335376501083374
Batch 63/64 loss: 0.13705825805664062
Batch 64/64 loss: 0.13197064399719238
Epoch 25  Train loss: 0.14355791409810384  Val loss: 0.15923931119368248
Saving best model, epoch: 25
Epoch 26
-------------------------------
Batch 1/64 loss: 0.13507193326950073
Batch 2/64 loss: 0.15425783395767212
Batch 3/64 loss: 0.14437735080718994
Batch 4/64 loss: 0.1324251890182495
Batch 5/64 loss: 0.12750452756881714
Batch 6/64 loss: 0.10712569952011108
Batch 7/64 loss: 0.14335644245147705
Batch 8/64 loss: 0.13237839937210083
Batch 9/64 loss: 0.13943809270858765
Batch 10/64 loss: 0.13388371467590332
Batch 11/64 loss: 0.13753622770309448
Batch 12/64 loss: 0.13879024982452393
Batch 13/64 loss: 0.13229155540466309
Batch 14/64 loss: 0.15286165475845337
Batch 15/64 loss: 0.14392530918121338
Batch 16/64 loss: 0.13832616806030273
Batch 17/64 loss: 0.15277349948883057
Batch 18/64 loss: 0.1350245475769043
Batch 19/64 loss: 0.1093980073928833
Batch 20/64 loss: 0.1513945460319519
Batch 21/64 loss: 0.15269029140472412
Batch 22/64 loss: 0.1285528540611267
Batch 23/64 loss: 0.1336514949798584
Batch 24/64 loss: 0.13505661487579346
Batch 25/64 loss: 0.12957358360290527
Batch 26/64 loss: 0.13507461547851562
Batch 27/64 loss: 0.1450830101966858
Batch 28/64 loss: 0.15600210428237915
Batch 29/64 loss: 0.16157633066177368
Batch 30/64 loss: 0.13198906183242798
Batch 31/64 loss: 0.13516008853912354
Batch 32/64 loss: 0.16228240728378296
Batch 33/64 loss: 0.149944007396698
Batch 34/64 loss: 0.13878631591796875
Batch 35/64 loss: 0.1397838592529297
Batch 36/64 loss: 0.14992070198059082
Batch 37/64 loss: 0.13477957248687744
Batch 38/64 loss: 0.13546335697174072
Batch 39/64 loss: 0.15025532245635986
Batch 40/64 loss: 0.14034831523895264
Batch 41/64 loss: 0.1297764778137207
Batch 42/64 loss: 0.15254873037338257
Batch 43/64 loss: 0.1381986141204834
Batch 44/64 loss: 0.13941198587417603
Batch 45/64 loss: 0.12322109937667847
Batch 46/64 loss: 0.1452884078025818
Batch 47/64 loss: 0.12381160259246826
Batch 48/64 loss: 0.11162811517715454
Batch 49/64 loss: 0.11495542526245117
Batch 50/64 loss: 0.14321529865264893
Batch 51/64 loss: 0.1529710292816162
Batch 52/64 loss: 0.1405695676803589
Batch 53/64 loss: 0.11406600475311279
Batch 54/64 loss: 0.13071930408477783
Batch 55/64 loss: 0.137722909450531
Batch 56/64 loss: 0.1263035535812378
Batch 57/64 loss: 0.15635251998901367
Batch 58/64 loss: 0.13434123992919922
Batch 59/64 loss: 0.15223556756973267
Batch 60/64 loss: 0.13009697198867798
Batch 61/64 loss: 0.14322417974472046
Batch 62/64 loss: 0.14807069301605225
Batch 63/64 loss: 0.15039008855819702
Batch 64/64 loss: 0.14638715982437134
Epoch 26  Train loss: 0.13861999394846897  Val loss: 0.15605267462451844
Saving best model, epoch: 26
Epoch 27
-------------------------------
Batch 1/64 loss: 0.14503157138824463
Batch 2/64 loss: 0.14691156148910522
Batch 3/64 loss: 0.12080645561218262
Batch 4/64 loss: 0.13663095235824585
Batch 5/64 loss: 0.13755279779434204
Batch 6/64 loss: 0.10995882749557495
Batch 7/64 loss: 0.1327446699142456
Batch 8/64 loss: 0.14071500301361084
Batch 9/64 loss: 0.152959406375885
Batch 10/64 loss: 0.13140887022018433
Batch 11/64 loss: 0.1413785219192505
Batch 12/64 loss: 0.15554678440093994
Batch 13/64 loss: 0.13174694776535034
Batch 14/64 loss: 0.1303466558456421
Batch 15/64 loss: 0.14261066913604736
Batch 16/64 loss: 0.140488862991333
Batch 17/64 loss: 0.1407182216644287
Batch 18/64 loss: 0.1434621810913086
Batch 19/64 loss: 0.1365402340888977
Batch 20/64 loss: 0.12427318096160889
Batch 21/64 loss: 0.15034770965576172
Batch 22/64 loss: 0.1281604766845703
Batch 23/64 loss: 0.12771838903427124
Batch 24/64 loss: 0.15856856107711792
Batch 25/64 loss: 0.10967051982879639
Batch 26/64 loss: 0.12350809574127197
Batch 27/64 loss: 0.14822745323181152
Batch 28/64 loss: 0.12514150142669678
Batch 29/64 loss: 0.1388213038444519
Batch 30/64 loss: 0.12746495008468628
Batch 31/64 loss: 0.12292659282684326
Batch 32/64 loss: 0.14593440294265747
Batch 33/64 loss: 0.1407691240310669
Batch 34/64 loss: 0.11960214376449585
Batch 35/64 loss: 0.12941652536392212
Batch 36/64 loss: 0.1196104884147644
Batch 37/64 loss: 0.1371248960494995
Batch 38/64 loss: 0.13761800527572632
Batch 39/64 loss: 0.1341039538383484
Batch 40/64 loss: 0.13558626174926758
Batch 41/64 loss: 0.11772298812866211
Batch 42/64 loss: 0.16595453023910522
Batch 43/64 loss: 0.15246999263763428
Batch 44/64 loss: 0.1658775806427002
Batch 45/64 loss: 0.13055497407913208
Batch 46/64 loss: 0.14525723457336426
Batch 47/64 loss: 0.12219387292861938
Batch 48/64 loss: 0.12891924381256104
Batch 49/64 loss: 0.14587116241455078
Batch 50/64 loss: 0.11690700054168701
Batch 51/64 loss: 0.1489275097846985
Batch 52/64 loss: 0.13956373929977417
Batch 53/64 loss: 0.13351988792419434
Batch 54/64 loss: 0.13361316919326782
Batch 55/64 loss: 0.16323977708816528
Batch 56/64 loss: 0.1302066445350647
Batch 57/64 loss: 0.15241003036499023
Batch 58/64 loss: 0.13625049591064453
Batch 59/64 loss: 0.13863539695739746
Batch 60/64 loss: 0.12388598918914795
Batch 61/64 loss: 0.13511896133422852
Batch 62/64 loss: 0.11417186260223389
Batch 63/64 loss: 0.12944334745407104
Batch 64/64 loss: 0.11591148376464844
Epoch 27  Train loss: 0.13584004289963666  Val loss: 0.14936965355758405
Saving best model, epoch: 27
Epoch 28
-------------------------------
Batch 1/64 loss: 0.12920153141021729
Batch 2/64 loss: 0.12796711921691895
Batch 3/64 loss: 0.13243496417999268
Batch 4/64 loss: 0.1393851637840271
Batch 5/64 loss: 0.14076167345046997
Batch 6/64 loss: 0.12834441661834717
Batch 7/64 loss: 0.11413592100143433
Batch 8/64 loss: 0.1352350115776062
Batch 9/64 loss: 0.1430344581604004
Batch 10/64 loss: 0.11678004264831543
Batch 11/64 loss: 0.14451348781585693
Batch 12/64 loss: 0.1149446964263916
Batch 13/64 loss: 0.1420019268989563
Batch 14/64 loss: 0.12909060716629028
Batch 15/64 loss: 0.1513569951057434
Batch 16/64 loss: 0.14867085218429565
Batch 17/64 loss: 0.13361024856567383
Batch 18/64 loss: 0.12676650285720825
Batch 19/64 loss: 0.14477640390396118
Batch 20/64 loss: 0.13983440399169922
Batch 21/64 loss: 0.12036508321762085
Batch 22/64 loss: 0.1307048797607422
Batch 23/64 loss: 0.1399933099746704
Batch 24/64 loss: 0.10522949695587158
Batch 25/64 loss: 0.1158667802810669
Batch 26/64 loss: 0.12263649702072144
Batch 27/64 loss: 0.13785946369171143
Batch 28/64 loss: 0.1081882119178772
Batch 29/64 loss: 0.13389480113983154
Batch 30/64 loss: 0.14427995681762695
Batch 31/64 loss: 0.13483679294586182
Batch 32/64 loss: 0.1365795135498047
Batch 33/64 loss: 0.12870335578918457
Batch 34/64 loss: 0.1391456127166748
Batch 35/64 loss: 0.13975632190704346
Batch 36/64 loss: 0.1194373369216919
Batch 37/64 loss: 0.14951908588409424
Batch 38/64 loss: 0.12344372272491455
Batch 39/64 loss: 0.16668349504470825
Batch 40/64 loss: 0.13016360998153687
Batch 41/64 loss: 0.14386141300201416
Batch 42/64 loss: 0.12224096059799194
Batch 43/64 loss: 0.14431118965148926
Batch 44/64 loss: 0.1192924976348877
Batch 45/64 loss: 0.13112163543701172
Batch 46/64 loss: 0.11602574586868286
Batch 47/64 loss: 0.11847102642059326
Batch 48/64 loss: 0.12245452404022217
Batch 49/64 loss: 0.1557818055152893
Batch 50/64 loss: 0.14273864030838013
Batch 51/64 loss: 0.12448269128799438
Batch 52/64 loss: 0.13005328178405762
Batch 53/64 loss: 0.11762052774429321
Batch 54/64 loss: 0.11978590488433838
Batch 55/64 loss: 0.125369131565094
Batch 56/64 loss: 0.14034950733184814
Batch 57/64 loss: 0.14256739616394043
Batch 58/64 loss: 0.11923116445541382
Batch 59/64 loss: 0.13422590494155884
Batch 60/64 loss: 0.1497597098350525
Batch 61/64 loss: 0.12370103597640991
Batch 62/64 loss: 0.1598137617111206
Batch 63/64 loss: 0.14455771446228027
Batch 64/64 loss: 0.147322416305542
Epoch 28  Train loss: 0.13283831745970484  Val loss: 0.1488507224112442
Saving best model, epoch: 28
Epoch 29
-------------------------------
Batch 1/64 loss: 0.14558768272399902
Batch 2/64 loss: 0.12672317028045654
Batch 3/64 loss: 0.13349181413650513
Batch 4/64 loss: 0.14864403009414673
Batch 5/64 loss: 0.12214362621307373
Batch 6/64 loss: 0.1483478546142578
Batch 7/64 loss: 0.14221996068954468
Batch 8/64 loss: 0.12495231628417969
Batch 9/64 loss: 0.11624467372894287
Batch 10/64 loss: 0.1384207010269165
Batch 11/64 loss: 0.11924022436141968
Batch 12/64 loss: 0.13452845811843872
Batch 13/64 loss: 0.1181139349937439
Batch 14/64 loss: 0.11865437030792236
Batch 15/64 loss: 0.1412045955657959
Batch 16/64 loss: 0.0990149974822998
Batch 17/64 loss: 0.13715237379074097
Batch 18/64 loss: 0.11442720890045166
Batch 19/64 loss: 0.14313745498657227
Batch 20/64 loss: 0.11517071723937988
Batch 21/64 loss: 0.16357827186584473
Batch 22/64 loss: 0.11391294002532959
Batch 23/64 loss: 0.11478471755981445
Batch 24/64 loss: 0.14539003372192383
Batch 25/64 loss: 0.12922513484954834
Batch 26/64 loss: 0.10714066028594971
Batch 27/64 loss: 0.13007497787475586
Batch 28/64 loss: 0.1282581090927124
Batch 29/64 loss: 0.13131386041641235
Batch 30/64 loss: 0.12829828262329102
Batch 31/64 loss: 0.11957186460494995
Batch 32/64 loss: 0.12872064113616943
Batch 33/64 loss: 0.14338994026184082
Batch 34/64 loss: 0.1389094591140747
Batch 35/64 loss: 0.14987486600875854
Batch 36/64 loss: 0.12011510133743286
Batch 37/64 loss: 0.14785796403884888
Batch 38/64 loss: 0.1367642879486084
Batch 39/64 loss: 0.12134057283401489
Batch 40/64 loss: 0.14351701736450195
Batch 41/64 loss: 0.11910450458526611
Batch 42/64 loss: 0.12598425149917603
Batch 43/64 loss: 0.14874839782714844
Batch 44/64 loss: 0.10788148641586304
Batch 45/64 loss: 0.12589752674102783
Batch 46/64 loss: 0.12367421388626099
Batch 47/64 loss: 0.1261008381843567
Batch 48/64 loss: 0.12139201164245605
Batch 49/64 loss: 0.12952303886413574
Batch 50/64 loss: 0.14809352159500122
Batch 51/64 loss: 0.12353134155273438
Batch 52/64 loss: 0.12894928455352783
Batch 53/64 loss: 0.11000263690948486
Batch 54/64 loss: 0.1416662335395813
Batch 55/64 loss: 0.11192160844802856
Batch 56/64 loss: 0.12320005893707275
Batch 57/64 loss: 0.12839257717132568
Batch 58/64 loss: 0.09470927715301514
Batch 59/64 loss: 0.11201256513595581
Batch 60/64 loss: 0.0993729829788208
Batch 61/64 loss: 0.15114259719848633
Batch 62/64 loss: 0.15108561515808105
Batch 63/64 loss: 0.12933558225631714
Batch 64/64 loss: 0.12127178907394409
Epoch 29  Train loss: 0.12866093902026907  Val loss: 0.14985589227315896
Epoch 30
-------------------------------
Batch 1/64 loss: 0.15036994218826294
Batch 2/64 loss: 0.13500308990478516
Batch 3/64 loss: 0.12779384851455688
Batch 4/64 loss: 0.1282569169998169
Batch 5/64 loss: 0.1108139157295227
Batch 6/64 loss: 0.14421844482421875
Batch 7/64 loss: 0.11190658807754517
Batch 8/64 loss: 0.11972194910049438
Batch 9/64 loss: 0.1365376114845276
Batch 10/64 loss: 0.13175159692764282
Batch 11/64 loss: 0.11645162105560303
Batch 12/64 loss: 0.13484764099121094
Batch 13/64 loss: 0.1324571967124939
Batch 14/64 loss: 0.11671751737594604
Batch 15/64 loss: 0.1334773302078247
Batch 16/64 loss: 0.15207552909851074
Batch 17/64 loss: 0.11750620603561401
Batch 18/64 loss: 0.12468421459197998
Batch 19/64 loss: 0.13598096370697021
Batch 20/64 loss: 0.14081573486328125
Batch 21/64 loss: 0.11297386884689331
Batch 22/64 loss: 0.11553335189819336
Batch 23/64 loss: 0.11899340152740479
Batch 24/64 loss: 0.13607323169708252
Batch 25/64 loss: 0.12558746337890625
Batch 26/64 loss: 0.12263566255569458
Batch 27/64 loss: 0.11866950988769531
Batch 28/64 loss: 0.12140446901321411
Batch 29/64 loss: 0.12444156408309937
Batch 30/64 loss: 0.13102424144744873
Batch 31/64 loss: 0.13276684284210205
Batch 32/64 loss: 0.12105345726013184
Batch 33/64 loss: 0.11610817909240723
Batch 34/64 loss: 0.11136108636856079
Batch 35/64 loss: 0.13247621059417725
Batch 36/64 loss: 0.1402137279510498
Batch 37/64 loss: 0.12736964225769043
Batch 38/64 loss: 0.10933780670166016
Batch 39/64 loss: 0.1388866901397705
Batch 40/64 loss: 0.14079779386520386
Batch 41/64 loss: 0.13507157564163208
Batch 42/64 loss: 0.13567298650741577
Batch 43/64 loss: 0.11094951629638672
Batch 44/64 loss: 0.1279645562171936
Batch 45/64 loss: 0.12640178203582764
Batch 46/64 loss: 0.1255059838294983
Batch 47/64 loss: 0.11602818965911865
Batch 48/64 loss: 0.1194237470626831
Batch 49/64 loss: 0.11269855499267578
Batch 50/64 loss: 0.1309768557548523
Batch 51/64 loss: 0.12321621179580688
Batch 52/64 loss: 0.11330527067184448
Batch 53/64 loss: 0.11983150243759155
Batch 54/64 loss: 0.12209224700927734
Batch 55/64 loss: 0.12450939416885376
Batch 56/64 loss: 0.13569188117980957
Batch 57/64 loss: 0.12361288070678711
Batch 58/64 loss: 0.14174914360046387
Batch 59/64 loss: 0.13161104917526245
Batch 60/64 loss: 0.11017775535583496
Batch 61/64 loss: 0.11257851123809814
Batch 62/64 loss: 0.14872729778289795
Batch 63/64 loss: 0.14598321914672852
Batch 64/64 loss: 0.11415159702301025
Epoch 30  Train loss: 0.12672140972287047  Val loss: 0.14544832378728284
Saving best model, epoch: 30
Epoch 31
-------------------------------
Batch 1/64 loss: 0.1385430097579956
Batch 2/64 loss: 0.11597806215286255
Batch 3/64 loss: 0.1256403923034668
Batch 4/64 loss: 0.12075293064117432
Batch 5/64 loss: 0.13164609670639038
Batch 6/64 loss: 0.1040310263633728
Batch 7/64 loss: 0.10108721256256104
Batch 8/64 loss: 0.1357203722000122
Batch 9/64 loss: 0.13103532791137695
Batch 10/64 loss: 0.12631642818450928
Batch 11/64 loss: 0.09278029203414917
Batch 12/64 loss: 0.11403685808181763
Batch 13/64 loss: 0.11934769153594971
Batch 14/64 loss: 0.11593437194824219
Batch 15/64 loss: 0.1263156533241272
Batch 16/64 loss: 0.12685376405715942
Batch 17/64 loss: 0.09687811136245728
Batch 18/64 loss: 0.1133190393447876
Batch 19/64 loss: 0.10803985595703125
Batch 20/64 loss: 0.12551891803741455
Batch 21/64 loss: 0.1300099492073059
Batch 22/64 loss: 0.1263061761856079
Batch 23/64 loss: 0.11683708429336548
Batch 24/64 loss: 0.11907607316970825
Batch 25/64 loss: 0.12779802083969116
Batch 26/64 loss: 0.09137588739395142
Batch 27/64 loss: 0.12098020315170288
Batch 28/64 loss: 0.12096583843231201
Batch 29/64 loss: 0.14710426330566406
Batch 30/64 loss: 0.12255328893661499
Batch 31/64 loss: 0.13434189558029175
Batch 32/64 loss: 0.09794914722442627
Batch 33/64 loss: 0.1273277997970581
Batch 34/64 loss: 0.10654902458190918
Batch 35/64 loss: 0.11870306730270386
Batch 36/64 loss: 0.1332959532737732
Batch 37/64 loss: 0.13882946968078613
Batch 38/64 loss: 0.09791970252990723
Batch 39/64 loss: 0.12090712785720825
Batch 40/64 loss: 0.11330121755599976
Batch 41/64 loss: 0.11875313520431519
Batch 42/64 loss: 0.11776483058929443
Batch 43/64 loss: 0.11051404476165771
Batch 44/64 loss: 0.11820542812347412
Batch 45/64 loss: 0.13720422983169556
Batch 46/64 loss: 0.12496316432952881
Batch 47/64 loss: 0.1074671745300293
Batch 48/64 loss: 0.12679731845855713
Batch 49/64 loss: 0.12446528673171997
Batch 50/64 loss: 0.10588514804840088
Batch 51/64 loss: 0.12562763690948486
Batch 52/64 loss: 0.14352869987487793
Batch 53/64 loss: 0.11627674102783203
Batch 54/64 loss: 0.1126943826675415
Batch 55/64 loss: 0.11666887998580933
Batch 56/64 loss: 0.11283695697784424
Batch 57/64 loss: 0.10247933864593506
Batch 58/64 loss: 0.1285492181777954
Batch 59/64 loss: 0.13410460948944092
Batch 60/64 loss: 0.11335378885269165
Batch 61/64 loss: 0.1320304274559021
Batch 62/64 loss: 0.12244713306427002
Batch 63/64 loss: 0.1321675181388855
Batch 64/64 loss: 0.12938404083251953
Epoch 31  Train loss: 0.12021536827087402  Val loss: 0.13864262980693803
Saving best model, epoch: 31
Epoch 32
-------------------------------
Batch 1/64 loss: 0.11058062314987183
Batch 2/64 loss: 0.12334465980529785
Batch 3/64 loss: 0.11243736743927002
Batch 4/64 loss: 0.10386800765991211
Batch 5/64 loss: 0.1318347454071045
Batch 6/64 loss: 0.12963402271270752
Batch 7/64 loss: 0.11063683032989502
Batch 8/64 loss: 0.1209602952003479
Batch 9/64 loss: 0.11328518390655518
Batch 10/64 loss: 0.12838232517242432
Batch 11/64 loss: 0.10780423879623413
Batch 12/64 loss: 0.09899449348449707
Batch 13/64 loss: 0.12381190061569214
Batch 14/64 loss: 0.12259471416473389
Batch 15/64 loss: 0.09793126583099365
Batch 16/64 loss: 0.13225889205932617
Batch 17/64 loss: 0.11709225177764893
Batch 18/64 loss: 0.15094292163848877
Batch 19/64 loss: 0.11557531356811523
Batch 20/64 loss: 0.11719381809234619
Batch 21/64 loss: 0.1232861876487732
Batch 22/64 loss: 0.1154600977897644
Batch 23/64 loss: 0.11748719215393066
Batch 24/64 loss: 0.1319177746772766
Batch 25/64 loss: 0.11381745338439941
Batch 26/64 loss: 0.1186223030090332
Batch 27/64 loss: 0.12580525875091553
Batch 28/64 loss: 0.12086808681488037
Batch 29/64 loss: 0.13383322954177856
Batch 30/64 loss: 0.10772895812988281
Batch 31/64 loss: 0.1322076916694641
Batch 32/64 loss: 0.12310701608657837
Batch 33/64 loss: 0.1353505253791809
Batch 34/64 loss: 0.11312675476074219
Batch 35/64 loss: 0.12434524297714233
Batch 36/64 loss: 0.13099104166030884
Batch 37/64 loss: 0.09990543127059937
Batch 38/64 loss: 0.10562527179718018
Batch 39/64 loss: 0.12034022808074951
Batch 40/64 loss: 0.11495637893676758
Batch 41/64 loss: 0.1374780535697937
Batch 42/64 loss: 0.11348903179168701
Batch 43/64 loss: 0.11176633834838867
Batch 44/64 loss: 0.11326175928115845
Batch 45/64 loss: 0.10984766483306885
Batch 46/64 loss: 0.10648316144943237
Batch 47/64 loss: 0.12001729011535645
Batch 48/64 loss: 0.12561333179473877
Batch 49/64 loss: 0.09581094980239868
Batch 50/64 loss: 0.13832533359527588
Batch 51/64 loss: 0.11017948389053345
Batch 52/64 loss: 0.1141587495803833
Batch 53/64 loss: 0.10933464765548706
Batch 54/64 loss: 0.10360729694366455
Batch 55/64 loss: 0.09062397480010986
Batch 56/64 loss: 0.11109709739685059
Batch 57/64 loss: 0.1054842472076416
Batch 58/64 loss: 0.12697094678878784
Batch 59/64 loss: 0.12414485216140747
Batch 60/64 loss: 0.1239171028137207
Batch 61/64 loss: 0.11412334442138672
Batch 62/64 loss: 0.13243871927261353
Batch 63/64 loss: 0.11820948123931885
Batch 64/64 loss: 0.1330224871635437
Epoch 32  Train loss: 0.11818189363853604  Val loss: 0.1365063981092263
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: 0.1558937430381775
Batch 2/64 loss: 0.12156504392623901
Batch 3/64 loss: 0.0986180305480957
Batch 4/64 loss: 0.11514997482299805
Batch 5/64 loss: 0.13079464435577393
Batch 6/64 loss: 0.12609583139419556
Batch 7/64 loss: 0.102935791015625
Batch 8/64 loss: 0.12534981966018677
Batch 9/64 loss: 0.1071854829788208
Batch 10/64 loss: 0.10652142763137817
Batch 11/64 loss: 0.11226576566696167
Batch 12/64 loss: 0.09285628795623779
Batch 13/64 loss: 0.11426156759262085
Batch 14/64 loss: 0.0984845757484436
Batch 15/64 loss: 0.12201523780822754
Batch 16/64 loss: 0.10491973161697388
Batch 17/64 loss: 0.10994070768356323
Batch 18/64 loss: 0.12063568830490112
Batch 19/64 loss: 0.10824060440063477
Batch 20/64 loss: 0.10042154788970947
Batch 21/64 loss: 0.10282784700393677
Batch 22/64 loss: 0.09260261058807373
Batch 23/64 loss: 0.11159497499465942
Batch 24/64 loss: 0.13308924436569214
Batch 25/64 loss: 0.10248196125030518
Batch 26/64 loss: 0.1087997555732727
Batch 27/64 loss: 0.10064762830734253
Batch 28/64 loss: 0.10297995805740356
Batch 29/64 loss: 0.1276710033416748
Batch 30/64 loss: 0.1050676703453064
Batch 31/64 loss: 0.10198509693145752
Batch 32/64 loss: 0.10449588298797607
Batch 33/64 loss: 0.1195034384727478
Batch 34/64 loss: 0.12780487537384033
Batch 35/64 loss: 0.11749935150146484
Batch 36/64 loss: 0.13458847999572754
Batch 37/64 loss: 0.11021941900253296
Batch 38/64 loss: 0.11048233509063721
Batch 39/64 loss: 0.10469973087310791
Batch 40/64 loss: 0.1346588134765625
Batch 41/64 loss: 0.09290575981140137
Batch 42/64 loss: 0.10121405124664307
Batch 43/64 loss: 0.11110258102416992
Batch 44/64 loss: 0.12664729356765747
Batch 45/64 loss: 0.11724722385406494
Batch 46/64 loss: 0.10113584995269775
Batch 47/64 loss: 0.08984905481338501
Batch 48/64 loss: 0.12427997589111328
Batch 49/64 loss: 0.12107080221176147
Batch 50/64 loss: 0.12164908647537231
Batch 51/64 loss: 0.13182199001312256
Batch 52/64 loss: 0.11429566144943237
Batch 53/64 loss: 0.09414941072463989
Batch 54/64 loss: 0.11855959892272949
Batch 55/64 loss: 0.11433571577072144
Batch 56/64 loss: 0.10766381025314331
Batch 57/64 loss: 0.12427258491516113
Batch 58/64 loss: 0.12171280384063721
Batch 59/64 loss: 0.12660515308380127
Batch 60/64 loss: 0.12702983617782593
Batch 61/64 loss: 0.11004340648651123
Batch 62/64 loss: 0.11309540271759033
Batch 63/64 loss: 0.08476805686950684
Batch 64/64 loss: 0.12751883268356323
Epoch 33  Train loss: 0.11323836574367449  Val loss: 0.13023832286756062
Saving best model, epoch: 33
Epoch 34
-------------------------------
Batch 1/64 loss: 0.12223613262176514
Batch 2/64 loss: 0.09088188409805298
Batch 3/64 loss: 0.10290777683258057
Batch 4/64 loss: 0.10737186670303345
Batch 5/64 loss: 0.12564492225646973
Batch 6/64 loss: 0.115825355052948
Batch 7/64 loss: 0.10299503803253174
Batch 8/64 loss: 0.10756808519363403
Batch 9/64 loss: 0.095120370388031
Batch 10/64 loss: 0.11534464359283447
Batch 11/64 loss: 0.09957170486450195
Batch 12/64 loss: 0.12164217233657837
Batch 13/64 loss: 0.10246437788009644
Batch 14/64 loss: 0.11798810958862305
Batch 15/64 loss: 0.09592956304550171
Batch 16/64 loss: 0.13247954845428467
Batch 17/64 loss: 0.14466243982315063
Batch 18/64 loss: 0.12857836484909058
Batch 19/64 loss: 0.1339278221130371
Batch 20/64 loss: 0.11257213354110718
Batch 21/64 loss: 0.13823121786117554
Batch 22/64 loss: 0.10780292749404907
Batch 23/64 loss: 0.12192344665527344
Batch 24/64 loss: 0.11774182319641113
Batch 25/64 loss: 0.11855953931808472
Batch 26/64 loss: 0.13759875297546387
Batch 27/64 loss: 0.10194802284240723
Batch 28/64 loss: 0.1078411340713501
Batch 29/64 loss: 0.12310504913330078
Batch 30/64 loss: 0.1262340545654297
Batch 31/64 loss: 0.12282425165176392
Batch 32/64 loss: 0.09814894199371338
Batch 33/64 loss: 0.09758889675140381
Batch 34/64 loss: 0.10033631324768066
Batch 35/64 loss: 0.11156415939331055
Batch 36/64 loss: 0.09941458702087402
Batch 37/64 loss: 0.0963587760925293
Batch 38/64 loss: 0.12097090482711792
Batch 39/64 loss: 0.08919614553451538
Batch 40/64 loss: 0.09053736925125122
Batch 41/64 loss: 0.11298155784606934
Batch 42/64 loss: 0.11125451326370239
Batch 43/64 loss: 0.0961046814918518
Batch 44/64 loss: 0.09516173601150513
Batch 45/64 loss: 0.13987815380096436
Batch 46/64 loss: 0.1103852391242981
Batch 47/64 loss: 0.13376551866531372
Batch 48/64 loss: 0.12691974639892578
Batch 49/64 loss: 0.12979435920715332
Batch 50/64 loss: 0.11402767896652222
Batch 51/64 loss: 0.13563144207000732
Batch 52/64 loss: 0.09812533855438232
Batch 53/64 loss: 0.08804523944854736
Batch 54/64 loss: 0.12081605195999146
Batch 55/64 loss: 0.11818915605545044
Batch 56/64 loss: 0.1281207799911499
Batch 57/64 loss: 0.08911710977554321
Batch 58/64 loss: 0.1025436520576477
Batch 59/64 loss: 0.09701365232467651
Batch 60/64 loss: 0.11247247457504272
Batch 61/64 loss: 0.09079247713088989
Batch 62/64 loss: 0.10250675678253174
Batch 63/64 loss: 0.09359985589981079
Batch 64/64 loss: 0.13286662101745605
Epoch 34  Train loss: 0.11216526685976515  Val loss: 0.1315392841588181
Epoch 35
-------------------------------
Batch 1/64 loss: 0.10516279935836792
Batch 2/64 loss: 0.11666524410247803
Batch 3/64 loss: 0.1311659812927246
Batch 4/64 loss: 0.1215810775756836
Batch 5/64 loss: 0.09651064872741699
Batch 6/64 loss: 0.11374318599700928
Batch 7/64 loss: 0.1113288402557373
Batch 8/64 loss: 0.11840826272964478
Batch 9/64 loss: 0.10951751470565796
Batch 10/64 loss: 0.11244237422943115
Batch 11/64 loss: 0.1213955283164978
Batch 12/64 loss: 0.09863787889480591
Batch 13/64 loss: 0.11028534173965454
Batch 14/64 loss: 0.10245919227600098
Batch 15/64 loss: 0.1008756160736084
Batch 16/64 loss: 0.09754693508148193
Batch 17/64 loss: 0.09672778844833374
Batch 18/64 loss: 0.09989488124847412
Batch 19/64 loss: 0.10177373886108398
Batch 20/64 loss: 0.10673010349273682
Batch 21/64 loss: 0.1170964241027832
Batch 22/64 loss: 0.09531933069229126
Batch 23/64 loss: 0.11836177110671997
Batch 24/64 loss: 0.09157609939575195
Batch 25/64 loss: 0.11040431261062622
Batch 26/64 loss: 0.10083335638046265
Batch 27/64 loss: 0.09810912609100342
Batch 28/64 loss: 0.11509191989898682
Batch 29/64 loss: 0.12679952383041382
Batch 30/64 loss: 0.11907738447189331
Batch 31/64 loss: 0.10919278860092163
Batch 32/64 loss: 0.09959286451339722
Batch 33/64 loss: 0.10782355070114136
Batch 34/64 loss: 0.12600845098495483
Batch 35/64 loss: 0.12999552488327026
Batch 36/64 loss: 0.11180490255355835
Batch 37/64 loss: 0.13640069961547852
Batch 38/64 loss: 0.09220612049102783
Batch 39/64 loss: 0.11741292476654053
Batch 40/64 loss: 0.14354932308197021
Batch 41/64 loss: 0.11305618286132812
Batch 42/64 loss: 0.10682868957519531
Batch 43/64 loss: 0.11981308460235596
Batch 44/64 loss: 0.09769511222839355
Batch 45/64 loss: 0.12895715236663818
Batch 46/64 loss: 0.08250349760055542
Batch 47/64 loss: 0.12652552127838135
Batch 48/64 loss: 0.09616565704345703
Batch 49/64 loss: 0.10747063159942627
Batch 50/64 loss: 0.10787999629974365
Batch 51/64 loss: 0.10967642068862915
Batch 52/64 loss: 0.10865384340286255
Batch 53/64 loss: 0.10022240877151489
Batch 54/64 loss: 0.10499465465545654
Batch 55/64 loss: 0.11333906650543213
Batch 56/64 loss: 0.07590895891189575
Batch 57/64 loss: 0.11281818151473999
Batch 58/64 loss: 0.09619241952896118
Batch 59/64 loss: 0.09826403856277466
Batch 60/64 loss: 0.09719967842102051
Batch 61/64 loss: 0.13064169883728027
Batch 62/64 loss: 0.1161642074584961
Batch 63/64 loss: 0.096133291721344
Batch 64/64 loss: 0.09837812185287476
Epoch 35  Train loss: 0.10918270302753823  Val loss: 0.13607367166538828
Epoch 36
-------------------------------
Batch 1/64 loss: 0.10265320539474487
Batch 2/64 loss: 0.10382473468780518
Batch 3/64 loss: 0.15334439277648926
Batch 4/64 loss: 0.09233349561691284
Batch 5/64 loss: 0.11439144611358643
Batch 6/64 loss: 0.11511749029159546
Batch 7/64 loss: 0.09676331281661987
Batch 8/64 loss: 0.10745024681091309
Batch 9/64 loss: 0.11399489641189575
Batch 10/64 loss: 0.11640322208404541
Batch 11/64 loss: 0.12025731801986694
Batch 12/64 loss: 0.1109778881072998
Batch 13/64 loss: 0.09591251611709595
Batch 14/64 loss: 0.09869784116744995
Batch 15/64 loss: 0.10048621892929077
Batch 16/64 loss: 0.10559886693954468
Batch 17/64 loss: 0.10790061950683594
Batch 18/64 loss: 0.11106479167938232
Batch 19/64 loss: 0.08108049631118774
Batch 20/64 loss: 0.10292339324951172
Batch 21/64 loss: 0.08972156047821045
Batch 22/64 loss: 0.1077842116355896
Batch 23/64 loss: 0.09576785564422607
Batch 24/64 loss: 0.13097405433654785
Batch 25/64 loss: 0.09212398529052734
Batch 26/64 loss: 0.09090173244476318
Batch 27/64 loss: 0.11683666706085205
Batch 28/64 loss: 0.11176061630249023
Batch 29/64 loss: 0.10920816659927368
Batch 30/64 loss: 0.10207438468933105
Batch 31/64 loss: 0.12425029277801514
Batch 32/64 loss: 0.10907143354415894
Batch 33/64 loss: 0.1110333800315857
Batch 34/64 loss: 0.08699691295623779
Batch 35/64 loss: 0.11288738250732422
Batch 36/64 loss: 0.0954277515411377
Batch 37/64 loss: 0.0966801643371582
Batch 38/64 loss: 0.12180745601654053
Batch 39/64 loss: 0.13220572471618652
Batch 40/64 loss: 0.09922027587890625
Batch 41/64 loss: 0.10016810894012451
Batch 42/64 loss: 0.10488909482955933
Batch 43/64 loss: 0.10574197769165039
Batch 44/64 loss: 0.11166638135910034
Batch 45/64 loss: 0.10924088954925537
Batch 46/64 loss: 0.11428338289260864
Batch 47/64 loss: 0.10578417778015137
Batch 48/64 loss: 0.10930538177490234
Batch 49/64 loss: 0.10705769062042236
Batch 50/64 loss: 0.08789938688278198
Batch 51/64 loss: 0.10927039384841919
Batch 52/64 loss: 0.0934140682220459
Batch 53/64 loss: 0.10327315330505371
Batch 54/64 loss: 0.10005748271942139
Batch 55/64 loss: 0.11298871040344238
Batch 56/64 loss: 0.11130374670028687
Batch 57/64 loss: 0.11411905288696289
Batch 58/64 loss: 0.10097700357437134
Batch 59/64 loss: 0.10725343227386475
Batch 60/64 loss: 0.09659683704376221
Batch 61/64 loss: 0.10537838935852051
Batch 62/64 loss: 0.11530768871307373
Batch 63/64 loss: 0.0902603268623352
Batch 64/64 loss: 0.11190766096115112
Epoch 36  Train loss: 0.10647965342390771  Val loss: 0.1299164997752999
Saving best model, epoch: 36
Epoch 37
-------------------------------
Batch 1/64 loss: 0.11012762784957886
Batch 2/64 loss: 0.09643566608428955
Batch 3/64 loss: 0.12308323383331299
Batch 4/64 loss: 0.08912307024002075
Batch 5/64 loss: 0.08655595779418945
Batch 6/64 loss: 0.10321998596191406
Batch 7/64 loss: 0.0897817611694336
Batch 8/64 loss: 0.10746753215789795
Batch 9/64 loss: 0.12314015626907349
Batch 10/64 loss: 0.12453877925872803
Batch 11/64 loss: 0.11943107843399048
Batch 12/64 loss: 0.09762829542160034
Batch 13/64 loss: 0.09185808897018433
Batch 14/64 loss: 0.1067647933959961
Batch 15/64 loss: 0.09369027614593506
Batch 16/64 loss: 0.09392333030700684
Batch 17/64 loss: 0.09270745515823364
Batch 18/64 loss: 0.10227185487747192
Batch 19/64 loss: 0.09016454219818115
Batch 20/64 loss: 0.10639190673828125
Batch 21/64 loss: 0.08748650550842285
Batch 22/64 loss: 0.11411088705062866
Batch 23/64 loss: 0.07686442136764526
Batch 24/64 loss: 0.125981867313385
Batch 25/64 loss: 0.11630415916442871
Batch 26/64 loss: 0.11681967973709106
Batch 27/64 loss: 0.08886617422103882
Batch 28/64 loss: 0.09790974855422974
Batch 29/64 loss: 0.10047626495361328
Batch 30/64 loss: 0.08445340394973755
Batch 31/64 loss: 0.12331366539001465
Batch 32/64 loss: 0.125110924243927
Batch 33/64 loss: 0.09987211227416992
Batch 34/64 loss: 0.0935019850730896
Batch 35/64 loss: 0.11751741170883179
Batch 36/64 loss: 0.08749139308929443
Batch 37/64 loss: 0.09775328636169434
Batch 38/64 loss: 0.13601183891296387
Batch 39/64 loss: 0.10598665475845337
Batch 40/64 loss: 0.09521734714508057
Batch 41/64 loss: 0.1092684268951416
Batch 42/64 loss: 0.08859270811080933
Batch 43/64 loss: 0.09500622749328613
Batch 44/64 loss: 0.12963414192199707
Batch 45/64 loss: 0.12083423137664795
Batch 46/64 loss: 0.09294718503952026
Batch 47/64 loss: 0.09637278318405151
Batch 48/64 loss: 0.10181057453155518
Batch 49/64 loss: 0.09357452392578125
Batch 50/64 loss: 0.08255124092102051
Batch 51/64 loss: 0.0967903733253479
Batch 52/64 loss: 0.08550995588302612
Batch 53/64 loss: 0.11382901668548584
Batch 54/64 loss: 0.09273552894592285
Batch 55/64 loss: 0.0826902985572815
Batch 56/64 loss: 0.09359079599380493
Batch 57/64 loss: 0.08714640140533447
Batch 58/64 loss: 0.10075646638870239
Batch 59/64 loss: 0.1237289309501648
Batch 60/64 loss: 0.0888904333114624
Batch 61/64 loss: 0.12441438436508179
Batch 62/64 loss: 0.08753132820129395
Batch 63/64 loss: 0.08359026908874512
Batch 64/64 loss: 0.09311503171920776
Epoch 37  Train loss: 0.10166255095425775  Val loss: 0.12040151845138918
Saving best model, epoch: 37
Epoch 38
-------------------------------
Batch 1/64 loss: 0.0964931845664978
Batch 2/64 loss: 0.11828124523162842
Batch 3/64 loss: 0.10263782739639282
Batch 4/64 loss: 0.106742262840271
Batch 5/64 loss: 0.1176639199256897
Batch 6/64 loss: 0.08603596687316895
Batch 7/64 loss: 0.12668859958648682
Batch 8/64 loss: 0.0967627763748169
Batch 9/64 loss: 0.05933767557144165
Batch 10/64 loss: 0.09958219528198242
Batch 11/64 loss: 0.10476082563400269
Batch 12/64 loss: 0.08782529830932617
Batch 13/64 loss: 0.10638678073883057
Batch 14/64 loss: 0.1008368730545044
Batch 15/64 loss: 0.08553099632263184
Batch 16/64 loss: 0.11882489919662476
Batch 17/64 loss: 0.08286291360855103
Batch 18/64 loss: 0.08912217617034912
Batch 19/64 loss: 0.08280569314956665
Batch 20/64 loss: 0.11456984281539917
Batch 21/64 loss: 0.07731354236602783
Batch 22/64 loss: 0.107613205909729
Batch 23/64 loss: 0.1256313920021057
Batch 24/64 loss: 0.10928905010223389
Batch 25/64 loss: 0.09840726852416992
Batch 26/64 loss: 0.1025230884552002
Batch 27/64 loss: 0.07866966724395752
Batch 28/64 loss: 0.09248274564743042
Batch 29/64 loss: 0.09671950340270996
Batch 30/64 loss: 0.08452939987182617
Batch 31/64 loss: 0.10159695148468018
Batch 32/64 loss: 0.090151846408844
Batch 33/64 loss: 0.08563685417175293
Batch 34/64 loss: 0.10193300247192383
Batch 35/64 loss: 0.11518216133117676
Batch 36/64 loss: 0.10400986671447754
Batch 37/64 loss: 0.09203439950942993
Batch 38/64 loss: 0.0943613052368164
Batch 39/64 loss: 0.09101402759552002
Batch 40/64 loss: 0.08120334148406982
Batch 41/64 loss: 0.10651969909667969
Batch 42/64 loss: 0.1004418134689331
Batch 43/64 loss: 0.092792809009552
Batch 44/64 loss: 0.08998364210128784
Batch 45/64 loss: 0.0995606780052185
Batch 46/64 loss: 0.10825181007385254
Batch 47/64 loss: 0.10923570394515991
Batch 48/64 loss: 0.08750462532043457
Batch 49/64 loss: 0.11028099060058594
Batch 50/64 loss: 0.13268399238586426
Batch 51/64 loss: 0.09395921230316162
Batch 52/64 loss: 0.0858299732208252
Batch 53/64 loss: 0.10076022148132324
Batch 54/64 loss: 0.11929762363433838
Batch 55/64 loss: 0.08513021469116211
Batch 56/64 loss: 0.09698820114135742
Batch 57/64 loss: 0.10366368293762207
Batch 58/64 loss: 0.11846262216567993
Batch 59/64 loss: 0.101978600025177
Batch 60/64 loss: 0.07694977521896362
Batch 61/64 loss: 0.08411997556686401
Batch 62/64 loss: 0.08780479431152344
Batch 63/64 loss: 0.09279054403305054
Batch 64/64 loss: 0.1261361837387085
Epoch 38  Train loss: 0.0987238888647042  Val loss: 0.11520640874646373
Saving best model, epoch: 38
Epoch 39
-------------------------------
Batch 1/64 loss: 0.08442944288253784
Batch 2/64 loss: 0.08498001098632812
Batch 3/64 loss: 0.11756706237792969
Batch 4/64 loss: 0.09380501508712769
Batch 5/64 loss: 0.08846056461334229
Batch 6/64 loss: 0.08308517932891846
Batch 7/64 loss: 0.11064600944519043
Batch 8/64 loss: 0.11073589324951172
Batch 9/64 loss: 0.11012279987335205
Batch 10/64 loss: 0.08933889865875244
Batch 11/64 loss: 0.08272600173950195
Batch 12/64 loss: 0.11549419164657593
Batch 13/64 loss: 0.09018081426620483
Batch 14/64 loss: 0.08104562759399414
Batch 15/64 loss: 0.09616172313690186
Batch 16/64 loss: 0.07827591896057129
Batch 17/64 loss: 0.10594940185546875
Batch 18/64 loss: 0.08240264654159546
Batch 19/64 loss: 0.08501672744750977
Batch 20/64 loss: 0.10012543201446533
Batch 21/64 loss: 0.0830565094947815
Batch 22/64 loss: 0.08512771129608154
Batch 23/64 loss: 0.09874957799911499
Batch 24/64 loss: 0.11139988899230957
Batch 25/64 loss: 0.07805460691452026
Batch 26/64 loss: 0.08263921737670898
Batch 27/64 loss: 0.08362323045730591
Batch 28/64 loss: 0.09871494770050049
Batch 29/64 loss: 0.08231520652770996
Batch 30/64 loss: 0.09966719150543213
Batch 31/64 loss: 0.0908358097076416
Batch 32/64 loss: 0.08598244190216064
Batch 33/64 loss: 0.07780468463897705
Batch 34/64 loss: 0.09016537666320801
Batch 35/64 loss: 0.08485054969787598
Batch 36/64 loss: 0.11454510688781738
Batch 37/64 loss: 0.10333132743835449
Batch 38/64 loss: 0.09995782375335693
Batch 39/64 loss: 0.10169363021850586
Batch 40/64 loss: 0.09395712614059448
Batch 41/64 loss: 0.09780842065811157
Batch 42/64 loss: 0.06370365619659424
Batch 43/64 loss: 0.08115530014038086
Batch 44/64 loss: 0.1150815486907959
Batch 45/64 loss: 0.11323654651641846
Batch 46/64 loss: 0.10275280475616455
Batch 47/64 loss: 0.11589640378952026
Batch 48/64 loss: 0.09086191654205322
Batch 49/64 loss: 0.09012341499328613
Batch 50/64 loss: 0.08392584323883057
Batch 51/64 loss: 0.1050233244895935
Batch 52/64 loss: 0.09939020872116089
Batch 53/64 loss: 0.12176907062530518
Batch 54/64 loss: 0.09116673469543457
Batch 55/64 loss: 0.10155844688415527
Batch 56/64 loss: 0.10806232690811157
Batch 57/64 loss: 0.08305329084396362
Batch 58/64 loss: 0.09707045555114746
Batch 59/64 loss: 0.09009432792663574
Batch 60/64 loss: 0.12820327281951904
Batch 61/64 loss: 0.1272798776626587
Batch 62/64 loss: 0.08756780624389648
Batch 63/64 loss: 0.093830406665802
Batch 64/64 loss: 0.08962500095367432
Epoch 39  Train loss: 0.0955742036595064  Val loss: 0.12001732042974622
Epoch 40
-------------------------------
Batch 1/64 loss: 0.11117434501647949
Batch 2/64 loss: 0.08947885036468506
Batch 3/64 loss: 0.12298440933227539
Batch 4/64 loss: 0.086417555809021
Batch 5/64 loss: 0.0857076644897461
Batch 6/64 loss: 0.09870302677154541
Batch 7/64 loss: 0.09363281726837158
Batch 8/64 loss: 0.1022534966468811
Batch 9/64 loss: 0.09886538982391357
Batch 10/64 loss: 0.08839350938796997
Batch 11/64 loss: 0.10192584991455078
Batch 12/64 loss: 0.07395714521408081
Batch 13/64 loss: 0.09216421842575073
Batch 14/64 loss: 0.10005319118499756
Batch 15/64 loss: 0.09177005290985107
Batch 16/64 loss: 0.11762100458145142
Batch 17/64 loss: 0.08252954483032227
Batch 18/64 loss: 0.08230912685394287
Batch 19/64 loss: 0.09623593091964722
Batch 20/64 loss: 0.10239619016647339
Batch 21/64 loss: 0.0752570629119873
Batch 22/64 loss: 0.13796699047088623
Batch 23/64 loss: 0.10578852891921997
Batch 24/64 loss: 0.08448541164398193
Batch 25/64 loss: 0.09727656841278076
Batch 26/64 loss: 0.0952143669128418
Batch 27/64 loss: 0.10536009073257446
Batch 28/64 loss: 0.09569025039672852
Batch 29/64 loss: 0.11110925674438477
Batch 30/64 loss: 0.09994471073150635
Batch 31/64 loss: 0.09327059984207153
Batch 32/64 loss: 0.07023638486862183
Batch 33/64 loss: 0.07518643140792847
Batch 34/64 loss: 0.11049515008926392
Batch 35/64 loss: 0.09817969799041748
Batch 36/64 loss: 0.09048044681549072
Batch 37/64 loss: 0.09773129224777222
Batch 38/64 loss: 0.07295417785644531
Batch 39/64 loss: 0.09454244375228882
Batch 40/64 loss: 0.07686430215835571
Batch 41/64 loss: 0.11483591794967651
Batch 42/64 loss: 0.12804853916168213
Batch 43/64 loss: 0.08454912900924683
Batch 44/64 loss: 0.07509756088256836
Batch 45/64 loss: 0.11128449440002441
Batch 46/64 loss: 0.06523627042770386
Batch 47/64 loss: 0.09686398506164551
Batch 48/64 loss: 0.07999062538146973
Batch 49/64 loss: 0.11868226528167725
Batch 50/64 loss: 0.1087113618850708
Batch 51/64 loss: 0.09973227977752686
Batch 52/64 loss: 0.09170836210250854
Batch 53/64 loss: 0.08509516716003418
Batch 54/64 loss: 0.09093677997589111
Batch 55/64 loss: 0.09174716472625732
Batch 56/64 loss: 0.09335118532180786
Batch 57/64 loss: 0.09469592571258545
Batch 58/64 loss: 0.09626281261444092
Batch 59/64 loss: 0.09977555274963379
Batch 60/64 loss: 0.1144413948059082
Batch 61/64 loss: 0.09342795610427856
Batch 62/64 loss: 0.08816057443618774
Batch 63/64 loss: 0.10204899311065674
Batch 64/64 loss: 0.11629319190979004
Epoch 40  Train loss: 0.09597665375354243  Val loss: 0.11717002928461816
Epoch 41
-------------------------------
Batch 1/64 loss: 0.10381793975830078
Batch 2/64 loss: 0.11635929346084595
Batch 3/64 loss: 0.09053415060043335
Batch 4/64 loss: 0.08678197860717773
Batch 5/64 loss: 0.09380292892456055
Batch 6/64 loss: 0.07792818546295166
Batch 7/64 loss: 0.0998961329460144
Batch 8/64 loss: 0.10220456123352051
Batch 9/64 loss: 0.08731091022491455
Batch 10/64 loss: 0.08756673336029053
Batch 11/64 loss: 0.0805138349533081
Batch 12/64 loss: 0.10529321432113647
Batch 13/64 loss: 0.08410441875457764
Batch 14/64 loss: 0.09258365631103516
Batch 15/64 loss: 0.11867642402648926
Batch 16/64 loss: 0.10044068098068237
Batch 17/64 loss: 0.10060131549835205
Batch 18/64 loss: 0.10448330640792847
Batch 19/64 loss: 0.09221625328063965
Batch 20/64 loss: 0.11175519227981567
Batch 21/64 loss: 0.08091270923614502
Batch 22/64 loss: 0.11750566959381104
Batch 23/64 loss: 0.09143853187561035
Batch 24/64 loss: 0.08713006973266602
Batch 25/64 loss: 0.09274381399154663
Batch 26/64 loss: 0.06894266605377197
Batch 27/64 loss: 0.10712695121765137
Batch 28/64 loss: 0.09596973657608032
Batch 29/64 loss: 0.09086096286773682
Batch 30/64 loss: 0.07425886392593384
Batch 31/64 loss: 0.11179405450820923
Batch 32/64 loss: 0.1172899603843689
Batch 33/64 loss: 0.0795333981513977
Batch 34/64 loss: 0.05894404649734497
Batch 35/64 loss: 0.09398031234741211
Batch 36/64 loss: 0.12114489078521729
Batch 37/64 loss: 0.06934887170791626
Batch 38/64 loss: 0.08915400505065918
Batch 39/64 loss: 0.08256685733795166
Batch 40/64 loss: 0.08018606901168823
Batch 41/64 loss: 0.09715569019317627
Batch 42/64 loss: 0.07597959041595459
Batch 43/64 loss: 0.09199035167694092
Batch 44/64 loss: 0.09209299087524414
Batch 45/64 loss: 0.1129942536354065
Batch 46/64 loss: 0.10973703861236572
Batch 47/64 loss: 0.06741935014724731
Batch 48/64 loss: 0.10578399896621704
Batch 49/64 loss: 0.11239838600158691
Batch 50/64 loss: 0.08715349435806274
Batch 51/64 loss: 0.08071368932723999
Batch 52/64 loss: 0.08496832847595215
Batch 53/64 loss: 0.07166081666946411
Batch 54/64 loss: 0.07397782802581787
Batch 55/64 loss: 0.10302716493606567
Batch 56/64 loss: 0.08428490161895752
Batch 57/64 loss: 0.07176977396011353
Batch 58/64 loss: 0.08892720937728882
Batch 59/64 loss: 0.1059766411781311
Batch 60/64 loss: 0.08548241853713989
Batch 61/64 loss: 0.09635549783706665
Batch 62/64 loss: 0.10854268074035645
Batch 63/64 loss: 0.10151094198226929
Batch 64/64 loss: 0.08851462602615356
Epoch 41  Train loss: 0.09292537347943175  Val loss: 0.11698964862889032
Epoch 42
-------------------------------
Batch 1/64 loss: 0.06506186723709106
Batch 2/64 loss: 0.08378410339355469
Batch 3/64 loss: 0.10054504871368408
Batch 4/64 loss: 0.07906055450439453
Batch 5/64 loss: 0.09023821353912354
Batch 6/64 loss: 0.08547896146774292
Batch 7/64 loss: 0.09436732530593872
Batch 8/64 loss: 0.06821125745773315
Batch 9/64 loss: 0.09401977062225342
Batch 10/64 loss: 0.10683190822601318
Batch 11/64 loss: 0.08902466297149658
Batch 12/64 loss: 0.0900108814239502
Batch 13/64 loss: 0.09897130727767944
Batch 14/64 loss: 0.07678306102752686
Batch 15/64 loss: 0.06923097372055054
Batch 16/64 loss: 0.09207803010940552
Batch 17/64 loss: 0.07847553491592407
Batch 18/64 loss: 0.07416355609893799
Batch 19/64 loss: 0.07890021800994873
Batch 20/64 loss: 0.0739372968673706
Batch 21/64 loss: 0.10227036476135254
Batch 22/64 loss: 0.09050053358078003
Batch 23/64 loss: 0.07800918817520142
Batch 24/64 loss: 0.07930541038513184
Batch 25/64 loss: 0.11399894952774048
Batch 26/64 loss: 0.09300661087036133
Batch 27/64 loss: 0.08921343088150024
Batch 28/64 loss: 0.09912991523742676
Batch 29/64 loss: 0.08716559410095215
Batch 30/64 loss: 0.10357892513275146
Batch 31/64 loss: 0.08236086368560791
Batch 32/64 loss: 0.09835362434387207
Batch 33/64 loss: 0.07616496086120605
Batch 34/64 loss: 0.09995245933532715
Batch 35/64 loss: 0.12804192304611206
Batch 36/64 loss: 0.09441572427749634
Batch 37/64 loss: 0.098172128200531
Batch 38/64 loss: 0.1258692741394043
Batch 39/64 loss: 0.11768901348114014
Batch 40/64 loss: 0.0998070240020752
Batch 41/64 loss: 0.10538220405578613
Batch 42/64 loss: 0.06826448440551758
Batch 43/64 loss: 0.08465129137039185
Batch 44/64 loss: 0.08414566516876221
Batch 45/64 loss: 0.10415178537368774
Batch 46/64 loss: 0.0872308611869812
Batch 47/64 loss: 0.10209262371063232
Batch 48/64 loss: 0.07966345548629761
Batch 49/64 loss: 0.08009016513824463
Batch 50/64 loss: 0.06673341989517212
Batch 51/64 loss: 0.10412782430648804
Batch 52/64 loss: 0.09532064199447632
Batch 53/64 loss: 0.10332810878753662
Batch 54/64 loss: 0.08178955316543579
Batch 55/64 loss: 0.10022538900375366
Batch 56/64 loss: 0.07925915718078613
Batch 57/64 loss: 0.10986137390136719
Batch 58/64 loss: 0.08180034160614014
Batch 59/64 loss: 0.09912323951721191
Batch 60/64 loss: 0.0841640830039978
Batch 61/64 loss: 0.11120545864105225
Batch 62/64 loss: 0.09192317724227905
Batch 63/64 loss: 0.10249239206314087
Batch 64/64 loss: 0.09554076194763184
Epoch 42  Train loss: 0.0913703955856024  Val loss: 0.11438685921868917
Saving best model, epoch: 42
Epoch 43
-------------------------------
Batch 1/64 loss: 0.09333103895187378
Batch 2/64 loss: 0.10154891014099121
Batch 3/64 loss: 0.10243940353393555
Batch 4/64 loss: 0.09298908710479736
Batch 5/64 loss: 0.07134747505187988
Batch 6/64 loss: 0.09388715028762817
Batch 7/64 loss: 0.0915982723236084
Batch 8/64 loss: 0.11309170722961426
Batch 9/64 loss: 0.08156239986419678
Batch 10/64 loss: 0.05500221252441406
Batch 11/64 loss: 0.09022200107574463
Batch 12/64 loss: 0.09080034494400024
Batch 13/64 loss: 0.07601171731948853
Batch 14/64 loss: 0.1023932695388794
Batch 15/64 loss: 0.08246797323226929
Batch 16/64 loss: 0.0896652340888977
Batch 17/64 loss: 0.0912327766418457
Batch 18/64 loss: 0.0757298469543457
Batch 19/64 loss: 0.08596622943878174
Batch 20/64 loss: 0.12038171291351318
Batch 21/64 loss: 0.08347141742706299
Batch 22/64 loss: 0.06884407997131348
Batch 23/64 loss: 0.09109187126159668
Batch 24/64 loss: 0.11046838760375977
Batch 25/64 loss: 0.08103394508361816
Batch 26/64 loss: 0.08138257265090942
Batch 27/64 loss: 0.07639849185943604
Batch 28/64 loss: 0.0822027325630188
Batch 29/64 loss: 0.07835584878921509
Batch 30/64 loss: 0.07604068517684937
Batch 31/64 loss: 0.08439207077026367
Batch 32/64 loss: 0.08764290809631348
Batch 33/64 loss: 0.09033596515655518
Batch 34/64 loss: 0.08129477500915527
Batch 35/64 loss: 0.08694690465927124
Batch 36/64 loss: 0.0863613486289978
Batch 37/64 loss: 0.10106891393661499
Batch 38/64 loss: 0.08490884304046631
Batch 39/64 loss: 0.08408766984939575
Batch 40/64 loss: 0.08280569314956665
Batch 41/64 loss: 0.0961759090423584
Batch 42/64 loss: 0.11834561824798584
Batch 43/64 loss: 0.07824927568435669
Batch 44/64 loss: 0.08377188444137573
Batch 45/64 loss: 0.09845328330993652
Batch 46/64 loss: 0.10448122024536133
Batch 47/64 loss: 0.11616450548171997
Batch 48/64 loss: 0.10474973917007446
Batch 49/64 loss: 0.07903921604156494
Batch 50/64 loss: 0.0975039005279541
Batch 51/64 loss: 0.0845566987991333
Batch 52/64 loss: 0.07501792907714844
Batch 53/64 loss: 0.10836482048034668
Batch 54/64 loss: 0.08499062061309814
Batch 55/64 loss: 0.06831979751586914
Batch 56/64 loss: 0.0850909948348999
Batch 57/64 loss: 0.07824790477752686
Batch 58/64 loss: 0.07843178510665894
Batch 59/64 loss: 0.08395373821258545
Batch 60/64 loss: 0.0784749984741211
Batch 61/64 loss: 0.11239314079284668
Batch 62/64 loss: 0.06144833564758301
Batch 63/64 loss: 0.1063808798789978
Batch 64/64 loss: 0.09279870986938477
Epoch 43  Train loss: 0.08867465281019024  Val loss: 0.11266079596227796
Saving best model, epoch: 43
Epoch 44
-------------------------------
Batch 1/64 loss: 0.0514909029006958
Batch 2/64 loss: 0.09221172332763672
Batch 3/64 loss: 0.0717044472694397
Batch 4/64 loss: 0.09492415189743042
Batch 5/64 loss: 0.06874150037765503
Batch 6/64 loss: 0.08961695432662964
Batch 7/64 loss: 0.09190797805786133
Batch 8/64 loss: 0.0820845365524292
Batch 9/64 loss: 0.057415664196014404
Batch 10/64 loss: 0.06576311588287354
Batch 11/64 loss: 0.08656460046768188
Batch 12/64 loss: 0.08770108222961426
Batch 13/64 loss: 0.09681594371795654
Batch 14/64 loss: 0.10003459453582764
Batch 15/64 loss: 0.09376740455627441
Batch 16/64 loss: 0.07129055261611938
Batch 17/64 loss: 0.07819163799285889
Batch 18/64 loss: 0.10327821969985962
Batch 19/64 loss: 0.09560590982437134
Batch 20/64 loss: 0.0877189040184021
Batch 21/64 loss: 0.07127302885055542
Batch 22/64 loss: 0.09458959102630615
Batch 23/64 loss: 0.09030085802078247
Batch 24/64 loss: 0.08099812269210815
Batch 25/64 loss: 0.09292680025100708
Batch 26/64 loss: 0.10593724250793457
Batch 27/64 loss: 0.08155608177185059
Batch 28/64 loss: 0.09229391813278198
Batch 29/64 loss: 0.10950237512588501
Batch 30/64 loss: 0.08545690774917603
Batch 31/64 loss: 0.08814561367034912
Batch 32/64 loss: 0.10502243041992188
Batch 33/64 loss: 0.06728267669677734
Batch 34/64 loss: 0.09470802545547485
Batch 35/64 loss: 0.0967782735824585
Batch 36/64 loss: 0.10082733631134033
Batch 37/64 loss: 0.08529138565063477
Batch 38/64 loss: 0.07172912359237671
Batch 39/64 loss: 0.06802225112915039
Batch 40/64 loss: 0.07669293880462646
Batch 41/64 loss: 0.09898751974105835
Batch 42/64 loss: 0.07882809638977051
Batch 43/64 loss: 0.05546605587005615
Batch 44/64 loss: 0.07619035243988037
Batch 45/64 loss: 0.07124269008636475
Batch 46/64 loss: 0.0917431116104126
Batch 47/64 loss: 0.08337336778640747
Batch 48/64 loss: 0.0902787446975708
Batch 49/64 loss: 0.08174347877502441
Batch 50/64 loss: 0.09884464740753174
Batch 51/64 loss: 0.0734872817993164
Batch 52/64 loss: 0.08590668439865112
Batch 53/64 loss: 0.09742999076843262
Batch 54/64 loss: 0.07919710874557495
Batch 55/64 loss: 0.0858001708984375
Batch 56/64 loss: 0.10722982883453369
Batch 57/64 loss: 0.08084428310394287
Batch 58/64 loss: 0.08708924055099487
Batch 59/64 loss: 0.09751665592193604
Batch 60/64 loss: 0.12033587694168091
Batch 61/64 loss: 0.08443963527679443
Batch 62/64 loss: 0.08598875999450684
Batch 63/64 loss: 0.10527646541595459
Batch 64/64 loss: 0.0743558406829834
Epoch 44  Train loss: 0.08626151739382276  Val loss: 0.11062429205248855
Saving best model, epoch: 44
Epoch 45
-------------------------------
Batch 1/64 loss: 0.06748008728027344
Batch 2/64 loss: 0.08354264497756958
Batch 3/64 loss: 0.06600821018218994
Batch 4/64 loss: 0.08326852321624756
Batch 5/64 loss: 0.1007426381111145
Batch 6/64 loss: 0.07128727436065674
Batch 7/64 loss: 0.08192551136016846
Batch 8/64 loss: 0.10848093032836914
Batch 9/64 loss: 0.06843626499176025
Batch 10/64 loss: 0.067901611328125
Batch 11/64 loss: 0.06835973262786865
Batch 12/64 loss: 0.10129231214523315
Batch 13/64 loss: 0.09739851951599121
Batch 14/64 loss: 0.05445051193237305
Batch 15/64 loss: 0.07760000228881836
Batch 16/64 loss: 0.08020776510238647
Batch 17/64 loss: 0.07085007429122925
Batch 18/64 loss: 0.09085738658905029
Batch 19/64 loss: 0.07131069898605347
Batch 20/64 loss: 0.057392001152038574
Batch 21/64 loss: 0.08690762519836426
Batch 22/64 loss: 0.10081279277801514
Batch 23/64 loss: 0.08683490753173828
Batch 24/64 loss: 0.07273072004318237
Batch 25/64 loss: 0.08824992179870605
Batch 26/64 loss: 0.08239585161209106
Batch 27/64 loss: 0.08946007490158081
Batch 28/64 loss: 0.07884544134140015
Batch 29/64 loss: 0.07976651191711426
Batch 30/64 loss: 0.08143174648284912
Batch 31/64 loss: 0.09492498636245728
Batch 32/64 loss: 0.08101862668991089
Batch 33/64 loss: 0.0767548680305481
Batch 34/64 loss: 0.09012949466705322
Batch 35/64 loss: 0.06490099430084229
Batch 36/64 loss: 0.09978866577148438
Batch 37/64 loss: 0.07596981525421143
Batch 38/64 loss: 0.07615506649017334
Batch 39/64 loss: 0.07533454895019531
Batch 40/64 loss: 0.06558680534362793
Batch 41/64 loss: 0.07161468267440796
Batch 42/64 loss: 0.09298515319824219
Batch 43/64 loss: 0.07664114236831665
Batch 44/64 loss: 0.08999496698379517
Batch 45/64 loss: 0.09208488464355469
Batch 46/64 loss: 0.08672839403152466
Batch 47/64 loss: 0.05707579851150513
Batch 48/64 loss: 0.08469271659851074
Batch 49/64 loss: 0.09013146162033081
Batch 50/64 loss: 0.08725279569625854
Batch 51/64 loss: 0.05729329586029053
Batch 52/64 loss: 0.08747702836990356
Batch 53/64 loss: 0.10375720262527466
Batch 54/64 loss: 0.07330173254013062
Batch 55/64 loss: 0.11015409231185913
Batch 56/64 loss: 0.09080320596694946
Batch 57/64 loss: 0.08689069747924805
Batch 58/64 loss: 0.09151124954223633
Batch 59/64 loss: 0.09996038675308228
Batch 60/64 loss: 0.06895405054092407
Batch 61/64 loss: 0.09853744506835938
Batch 62/64 loss: 0.10135674476623535
Batch 63/64 loss: 0.0840369462966919
Batch 64/64 loss: 0.1189456582069397
Epoch 45  Train loss: 0.08296843112683763  Val loss: 0.10989878984661036
Saving best model, epoch: 45
Epoch 46
-------------------------------
Batch 1/64 loss: 0.0863843560218811
Batch 2/64 loss: 0.08451223373413086
Batch 3/64 loss: 0.08328688144683838
Batch 4/64 loss: 0.09956800937652588
Batch 5/64 loss: 0.08159512281417847
Batch 6/64 loss: 0.08829689025878906
Batch 7/64 loss: 0.10722982883453369
Batch 8/64 loss: 0.07914948463439941
Batch 9/64 loss: 0.08703124523162842
Batch 10/64 loss: 0.07854825258255005
Batch 11/64 loss: 0.07238024473190308
Batch 12/64 loss: 0.04683947563171387
Batch 13/64 loss: 0.059628069400787354
Batch 14/64 loss: 0.07037729024887085
Batch 15/64 loss: 0.07730025053024292
Batch 16/64 loss: 0.09517580270767212
Batch 17/64 loss: 0.11204028129577637
Batch 18/64 loss: 0.057917118072509766
Batch 19/64 loss: 0.09842425584793091
Batch 20/64 loss: 0.06043112277984619
Batch 21/64 loss: 0.09129542112350464
Batch 22/64 loss: 0.08754396438598633
Batch 23/64 loss: 0.0728139877319336
Batch 24/64 loss: 0.07822805643081665
Batch 25/64 loss: 0.07161855697631836
Batch 26/64 loss: 0.075833261013031
Batch 27/64 loss: 0.08068805932998657
Batch 28/64 loss: 0.0618973970413208
Batch 29/64 loss: 0.0833902359008789
Batch 30/64 loss: 0.07830601930618286
Batch 31/64 loss: 0.08446413278579712
Batch 32/64 loss: 0.07991969585418701
Batch 33/64 loss: 0.0680696964263916
Batch 34/64 loss: 0.0590672492980957
Batch 35/64 loss: 0.1044207215309143
Batch 36/64 loss: 0.10940521955490112
Batch 37/64 loss: 0.07981163263320923
Batch 38/64 loss: 0.07553434371948242
Batch 39/64 loss: 0.09563970565795898
Batch 40/64 loss: 0.0883219838142395
Batch 41/64 loss: 0.07662451267242432
Batch 42/64 loss: 0.07729458808898926
Batch 43/64 loss: 0.07325458526611328
Batch 44/64 loss: 0.0954628586769104
Batch 45/64 loss: 0.09643268585205078
Batch 46/64 loss: 0.08707308769226074
Batch 47/64 loss: 0.06022524833679199
Batch 48/64 loss: 0.09447509050369263
Batch 49/64 loss: 0.10655128955841064
Batch 50/64 loss: 0.0839390754699707
Batch 51/64 loss: 0.07219153642654419
Batch 52/64 loss: 0.09254103899002075
Batch 53/64 loss: 0.0850057601928711
Batch 54/64 loss: 0.06608343124389648
Batch 55/64 loss: 0.07339680194854736
Batch 56/64 loss: 0.106758713722229
Batch 57/64 loss: 0.10128676891326904
Batch 58/64 loss: 0.06490743160247803
Batch 59/64 loss: 0.09202390909194946
Batch 60/64 loss: 0.07747441530227661
Batch 61/64 loss: 0.09562474489212036
Batch 62/64 loss: 0.0761149525642395
Batch 63/64 loss: 0.10097432136535645
Batch 64/64 loss: 0.07848244905471802
Epoch 46  Train loss: 0.08261904693117329  Val loss: 0.11094804318090484
Epoch 47
-------------------------------
Batch 1/64 loss: 0.09538817405700684
Batch 2/64 loss: 0.08569979667663574
Batch 3/64 loss: 0.09048575162887573
Batch 4/64 loss: 0.09140729904174805
Batch 5/64 loss: 0.09075397253036499
Batch 6/64 loss: 0.08031213283538818
Batch 7/64 loss: 0.0910375714302063
Batch 8/64 loss: 0.06836682558059692
Batch 9/64 loss: 0.10013389587402344
Batch 10/64 loss: 0.07086300849914551
Batch 11/64 loss: 0.10079234838485718
Batch 12/64 loss: 0.05317354202270508
Batch 13/64 loss: 0.07777410745620728
Batch 14/64 loss: 0.10304594039916992
Batch 15/64 loss: 0.07143515348434448
Batch 16/64 loss: 0.07845807075500488
Batch 17/64 loss: 0.07248413562774658
Batch 18/64 loss: 0.07208770513534546
Batch 19/64 loss: 0.07134568691253662
Batch 20/64 loss: 0.0866585373878479
Batch 21/64 loss: 0.08547580242156982
Batch 22/64 loss: 0.06746143102645874
Batch 23/64 loss: 0.0858420729637146
Batch 24/64 loss: 0.09565955400466919
Batch 25/64 loss: 0.09484356641769409
Batch 26/64 loss: 0.06494605541229248
Batch 27/64 loss: 0.0746389627456665
Batch 28/64 loss: 0.09298670291900635
Batch 29/64 loss: 0.06849205493927002
Batch 30/64 loss: 0.07127833366394043
Batch 31/64 loss: 0.08434581756591797
Batch 32/64 loss: 0.06551504135131836
Batch 33/64 loss: 0.08381545543670654
Batch 34/64 loss: 0.09883207082748413
Batch 35/64 loss: 0.10869157314300537
Batch 36/64 loss: 0.08274310827255249
Batch 37/64 loss: 0.05757129192352295
Batch 38/64 loss: 0.06121957302093506
Batch 39/64 loss: 0.09421968460083008
Batch 40/64 loss: 0.0827629566192627
Batch 41/64 loss: 0.10360497236251831
Batch 42/64 loss: 0.07369232177734375
Batch 43/64 loss: 0.07640624046325684
Batch 44/64 loss: 0.08902764320373535
Batch 45/64 loss: 0.07869958877563477
Batch 46/64 loss: 0.0713152289390564
Batch 47/64 loss: 0.0804857611656189
Batch 48/64 loss: 0.07955992221832275
Batch 49/64 loss: 0.08999383449554443
Batch 50/64 loss: 0.09428095817565918
Batch 51/64 loss: 0.07135605812072754
Batch 52/64 loss: 0.09399759769439697
Batch 53/64 loss: 0.0836493968963623
Batch 54/64 loss: 0.07040834426879883
Batch 55/64 loss: 0.07238924503326416
Batch 56/64 loss: 0.06599986553192139
Batch 57/64 loss: 0.07384973764419556
Batch 58/64 loss: 0.06668943166732788
Batch 59/64 loss: 0.06228995323181152
Batch 60/64 loss: 0.07175058126449585
Batch 61/64 loss: 0.08984380960464478
Batch 62/64 loss: 0.04620051383972168
Batch 63/64 loss: 0.08253920078277588
Batch 64/64 loss: 0.07163423299789429
Epoch 47  Train loss: 0.08023216701021381  Val loss: 0.11216222994106333
Epoch 48
-------------------------------
Batch 1/64 loss: 0.08187979459762573
Batch 2/64 loss: 0.10884737968444824
Batch 3/64 loss: 0.07581979036331177
Batch 4/64 loss: 0.08130824565887451
Batch 5/64 loss: 0.08327782154083252
Batch 6/64 loss: 0.061939239501953125
Batch 7/64 loss: 0.09066987037658691
Batch 8/64 loss: 0.07796019315719604
Batch 9/64 loss: 0.05711650848388672
Batch 10/64 loss: 0.06899625062942505
Batch 11/64 loss: 0.08599632978439331
Batch 12/64 loss: 0.07472407817840576
Batch 13/64 loss: 0.08418601751327515
Batch 14/64 loss: 0.11749470233917236
Batch 15/64 loss: 0.07113897800445557
Batch 16/64 loss: 0.06584995985031128
Batch 17/64 loss: 0.08514446020126343
Batch 18/64 loss: 0.07531183958053589
Batch 19/64 loss: 0.063912034034729
Batch 20/64 loss: 0.07547879219055176
Batch 21/64 loss: 0.07595646381378174
Batch 22/64 loss: 0.07264232635498047
Batch 23/64 loss: 0.09810113906860352
Batch 24/64 loss: 0.08966606855392456
Batch 25/64 loss: 0.11107653379440308
Batch 26/64 loss: 0.08708441257476807
Batch 27/64 loss: 0.10049015283584595
Batch 28/64 loss: 0.085124671459198
Batch 29/64 loss: 0.09018325805664062
Batch 30/64 loss: 0.06850922107696533
Batch 31/64 loss: 0.07147878408432007
Batch 32/64 loss: 0.07391142845153809
Batch 33/64 loss: 0.05554455518722534
Batch 34/64 loss: 0.05148577690124512
Batch 35/64 loss: 0.05943518877029419
Batch 36/64 loss: 0.06776618957519531
Batch 37/64 loss: 0.09152746200561523
Batch 38/64 loss: 0.07288920879364014
Batch 39/64 loss: 0.06241714954376221
Batch 40/64 loss: 0.05581855773925781
Batch 41/64 loss: 0.081950843334198
Batch 42/64 loss: 0.08745092153549194
Batch 43/64 loss: 0.05683410167694092
Batch 44/64 loss: 0.11861652135848999
Batch 45/64 loss: 0.06242263317108154
Batch 46/64 loss: 0.08974170684814453
Batch 47/64 loss: 0.10724776983261108
Batch 48/64 loss: 0.06075704097747803
Batch 49/64 loss: 0.08132690191268921
Batch 50/64 loss: 0.07415008544921875
Batch 51/64 loss: 0.07137566804885864
Batch 52/64 loss: 0.06727206707000732
Batch 53/64 loss: 0.08506053686141968
Batch 54/64 loss: 0.08999907970428467
Batch 55/64 loss: 0.07977598905563354
Batch 56/64 loss: 0.07978028059005737
Batch 57/64 loss: 0.07728207111358643
Batch 58/64 loss: 0.11036014556884766
Batch 59/64 loss: 0.08563053607940674
Batch 60/64 loss: 0.05662614107131958
Batch 61/64 loss: 0.08117955923080444
Batch 62/64 loss: 0.09250134229660034
Batch 63/64 loss: 0.052347004413604736
Batch 64/64 loss: 0.0663098692893982
Epoch 48  Train loss: 0.07886403425067079  Val loss: 0.10428323041122804
Saving best model, epoch: 48
Epoch 49
-------------------------------
Batch 1/64 loss: 0.08011478185653687
Batch 2/64 loss: 0.08017903566360474
Batch 3/64 loss: 0.09908270835876465
Batch 4/64 loss: 0.0747496485710144
Batch 5/64 loss: 0.06145739555358887
Batch 6/64 loss: 0.07398146390914917
Batch 7/64 loss: 0.09744846820831299
Batch 8/64 loss: 0.06457710266113281
Batch 9/64 loss: 0.04961526393890381
Batch 10/64 loss: 0.05307036638259888
Batch 11/64 loss: 0.09296494722366333
Batch 12/64 loss: 0.06607651710510254
Batch 13/64 loss: 0.07899630069732666
Batch 14/64 loss: 0.07099878787994385
Batch 15/64 loss: 0.0888335108757019
Batch 16/64 loss: 0.08384382724761963
Batch 17/64 loss: 0.08820134401321411
Batch 18/64 loss: 0.08867055177688599
Batch 19/64 loss: 0.08199191093444824
Batch 20/64 loss: 0.07519185543060303
Batch 21/64 loss: 0.07841551303863525
Batch 22/64 loss: 0.06943720579147339
Batch 23/64 loss: 0.08097094297409058
Batch 24/64 loss: 0.06368577480316162
Batch 25/64 loss: 0.06878435611724854
Batch 26/64 loss: 0.06586283445358276
Batch 27/64 loss: 0.07364004850387573
Batch 28/64 loss: 0.07203280925750732
Batch 29/64 loss: 0.06591922044754028
Batch 30/64 loss: 0.0817108154296875
Batch 31/64 loss: 0.08248817920684814
Batch 32/64 loss: 0.07927083969116211
Batch 33/64 loss: 0.056803226470947266
Batch 34/64 loss: 0.08662128448486328
Batch 35/64 loss: 0.0815771222114563
Batch 36/64 loss: 0.08042991161346436
Batch 37/64 loss: 0.11180615425109863
Batch 38/64 loss: 0.06503802537918091
Batch 39/64 loss: 0.09115397930145264
Batch 40/64 loss: 0.06656879186630249
Batch 41/64 loss: 0.08015018701553345
Batch 42/64 loss: 0.09380221366882324
Batch 43/64 loss: 0.07718980312347412
Batch 44/64 loss: 0.06445026397705078
Batch 45/64 loss: 0.10751116275787354
Batch 46/64 loss: 0.08117341995239258
Batch 47/64 loss: 0.054329752922058105
Batch 48/64 loss: 0.09713530540466309
Batch 49/64 loss: 0.06176590919494629
Batch 50/64 loss: 0.08228003978729248
Batch 51/64 loss: 0.08284473419189453
Batch 52/64 loss: 0.0954633355140686
Batch 53/64 loss: 0.06524091958999634
Batch 54/64 loss: 0.09711039066314697
Batch 55/64 loss: 0.08230644464492798
Batch 56/64 loss: 0.08489429950714111
Batch 57/64 loss: 0.08597654104232788
Batch 58/64 loss: 0.07224106788635254
Batch 59/64 loss: 0.07313388586044312
Batch 60/64 loss: 0.08679187297821045
Batch 61/64 loss: 0.09923684597015381
Batch 62/64 loss: 0.06450450420379639
Batch 63/64 loss: 0.07377868890762329
Batch 64/64 loss: 0.07915204763412476
Epoch 49  Train loss: 0.07835197565602321  Val loss: 0.10005411270148155
Saving best model, epoch: 49
Epoch 50
-------------------------------
Batch 1/64 loss: 0.07497823238372803
Batch 2/64 loss: 0.06453955173492432
Batch 3/64 loss: 0.09525704383850098
Batch 4/64 loss: 0.05800437927246094
Batch 5/64 loss: 0.09542369842529297
Batch 6/64 loss: 0.09228354692459106
Batch 7/64 loss: 0.08220571279525757
Batch 8/64 loss: 0.061980366706848145
Batch 9/64 loss: 0.08103322982788086
Batch 10/64 loss: 0.0914616584777832
Batch 11/64 loss: 0.05605536699295044
Batch 12/64 loss: 0.06857550144195557
Batch 13/64 loss: 0.07655632495880127
Batch 14/64 loss: 0.07608646154403687
Batch 15/64 loss: 0.06135678291320801
Batch 16/64 loss: 0.0759897232055664
Batch 17/64 loss: 0.08172845840454102
Batch 18/64 loss: 0.08678686618804932
Batch 19/64 loss: 0.04922163486480713
Batch 20/64 loss: 0.07622015476226807
Batch 21/64 loss: 0.10166478157043457
Batch 22/64 loss: 0.07492899894714355
Batch 23/64 loss: 0.04603219032287598
Batch 24/64 loss: 0.08021414279937744
Batch 25/64 loss: 0.06458109617233276
Batch 26/64 loss: 0.07401680946350098
Batch 27/64 loss: 0.07233738899230957
Batch 28/64 loss: 0.06160581111907959
Batch 29/64 loss: 0.0714348554611206
Batch 30/64 loss: 0.07811599969863892
Batch 31/64 loss: 0.08370524644851685
Batch 32/64 loss: 0.08980041742324829
Batch 33/64 loss: 0.10050088167190552
Batch 34/64 loss: 0.08257091045379639
Batch 35/64 loss: 0.0828406810760498
Batch 36/64 loss: 0.062065303325653076
Batch 37/64 loss: 0.10825175046920776
Batch 38/64 loss: 0.07599085569381714
Batch 39/64 loss: 0.07961064577102661
Batch 40/64 loss: 0.07956069707870483
Batch 41/64 loss: 0.08817756175994873
Batch 42/64 loss: 0.0665547251701355
Batch 43/64 loss: 0.0815545916557312
Batch 44/64 loss: 0.05361741781234741
Batch 45/64 loss: 0.0642271637916565
Batch 46/64 loss: 0.08687913417816162
Batch 47/64 loss: 0.08073502779006958
Batch 48/64 loss: 0.09851551055908203
Batch 49/64 loss: 0.08549880981445312
Batch 50/64 loss: 0.07619768381118774
Batch 51/64 loss: 0.05009573698043823
Batch 52/64 loss: 0.06808280944824219
Batch 53/64 loss: 0.07315295934677124
Batch 54/64 loss: 0.07621252536773682
Batch 55/64 loss: 0.0823202133178711
Batch 56/64 loss: 0.06905233860015869
Batch 57/64 loss: 0.09705793857574463
Batch 58/64 loss: 0.08905923366546631
Batch 59/64 loss: 0.09994041919708252
Batch 60/64 loss: 0.1125112771987915
Batch 61/64 loss: 0.07164192199707031
Batch 62/64 loss: 0.04386484622955322
Batch 63/64 loss: 0.06351613998413086
Batch 64/64 loss: 0.05660945177078247
Epoch 50  Train loss: 0.0768077997600331  Val loss: 0.09610243515460352
Saving best model, epoch: 50
Epoch 51
-------------------------------
Batch 1/64 loss: 0.06835967302322388
Batch 2/64 loss: 0.04697507619857788
Batch 3/64 loss: 0.048981428146362305
Batch 4/64 loss: 0.07296240329742432
Batch 5/64 loss: 0.057450830936431885
Batch 6/64 loss: 0.05057418346405029
Batch 7/64 loss: 0.06585294008255005
Batch 8/64 loss: 0.07669508457183838
Batch 9/64 loss: 0.08681535720825195
Batch 10/64 loss: 0.07370412349700928
Batch 11/64 loss: 0.07238578796386719
Batch 12/64 loss: 0.055550217628479004
Batch 13/64 loss: 0.06139647960662842
Batch 14/64 loss: 0.10283219814300537
Batch 15/64 loss: 0.04363834857940674
Batch 16/64 loss: 0.06954503059387207
Batch 17/64 loss: 0.06682628393173218
Batch 18/64 loss: 0.12306123971939087
Batch 19/64 loss: 0.07924890518188477
Batch 20/64 loss: 0.07288819551467896
Batch 21/64 loss: 0.08472907543182373
Batch 22/64 loss: 0.10808950662612915
Batch 23/64 loss: 0.10021644830703735
Batch 24/64 loss: 0.08128941059112549
Batch 25/64 loss: 0.0924566388130188
Batch 26/64 loss: 0.07003265619277954
Batch 27/64 loss: 0.054400861263275146
Batch 28/64 loss: 0.08494925498962402
Batch 29/64 loss: 0.062309324741363525
Batch 30/64 loss: 0.058353304862976074
Batch 31/64 loss: 0.07477998733520508
Batch 32/64 loss: 0.08105385303497314
Batch 33/64 loss: 0.07241672277450562
Batch 34/64 loss: 0.07905733585357666
Batch 35/64 loss: 0.05471336841583252
Batch 36/64 loss: 0.09652090072631836
Batch 37/64 loss: 0.07180547714233398
Batch 38/64 loss: 0.09469044208526611
Batch 39/64 loss: 0.06670570373535156
Batch 40/64 loss: 0.06553620100021362
Batch 41/64 loss: 0.07414877414703369
Batch 42/64 loss: 0.06078338623046875
Batch 43/64 loss: 0.06513702869415283
Batch 44/64 loss: 0.06247591972351074
Batch 45/64 loss: 0.04290270805358887
Batch 46/64 loss: 0.08594387769699097
Batch 47/64 loss: 0.08286374807357788
Batch 48/64 loss: 0.08359086513519287
Batch 49/64 loss: 0.07776319980621338
Batch 50/64 loss: 0.08865737915039062
Batch 51/64 loss: 0.08506578207015991
Batch 52/64 loss: 0.050003886222839355
Batch 53/64 loss: 0.06800466775894165
Batch 54/64 loss: 0.08094102144241333
Batch 55/64 loss: 0.07426726818084717
Batch 56/64 loss: 0.07802045345306396
Batch 57/64 loss: 0.07439130544662476
Batch 58/64 loss: 0.10277044773101807
Batch 59/64 loss: 0.06703734397888184
Batch 60/64 loss: 0.0800817608833313
Batch 61/64 loss: 0.07686185836791992
Batch 62/64 loss: 0.07373416423797607
Batch 63/64 loss: 0.08689248561859131
Batch 64/64 loss: 0.06426215171813965
Epoch 51  Train loss: 0.07404527383692125  Val loss: 0.09653415938013608
Epoch 52
-------------------------------
Batch 1/64 loss: 0.05296969413757324
Batch 2/64 loss: 0.08708107471466064
Batch 3/64 loss: 0.08014053106307983
Batch 4/64 loss: 0.0657157301902771
Batch 5/64 loss: 0.06653732061386108
Batch 6/64 loss: 0.07597637176513672
Batch 7/64 loss: 0.058826327323913574
Batch 8/64 loss: 0.0696173906326294
Batch 9/64 loss: 0.07140582799911499
Batch 10/64 loss: 0.05749458074569702
Batch 11/64 loss: 0.08264702558517456
Batch 12/64 loss: 0.07391154766082764
Batch 13/64 loss: 0.08916580677032471
Batch 14/64 loss: 0.06360900402069092
Batch 15/64 loss: 0.05068659782409668
Batch 16/64 loss: 0.069050133228302
Batch 17/64 loss: 0.051803648471832275
Batch 18/64 loss: 0.10780787467956543
Batch 19/64 loss: 0.09374886751174927
Batch 20/64 loss: 0.07295101881027222
Batch 21/64 loss: 0.091605544090271
Batch 22/64 loss: 0.07361429929733276
Batch 23/64 loss: 0.07557123899459839
Batch 24/64 loss: 0.071624755859375
Batch 25/64 loss: 0.07375609874725342
Batch 26/64 loss: 0.053501784801483154
Batch 27/64 loss: 0.07087564468383789
Batch 28/64 loss: 0.06505244970321655
Batch 29/64 loss: 0.06275171041488647
Batch 30/64 loss: 0.04903680086135864
Batch 31/64 loss: 0.07992953062057495
Batch 32/64 loss: 0.05804574489593506
Batch 33/64 loss: 0.06147778034210205
Batch 34/64 loss: 0.08273762464523315
Batch 35/64 loss: 0.07669055461883545
Batch 36/64 loss: 0.08113688230514526
Batch 37/64 loss: 0.06000471115112305
Batch 38/64 loss: 0.09918045997619629
Batch 39/64 loss: 0.05312842130661011
Batch 40/64 loss: 0.07239413261413574
Batch 41/64 loss: 0.07300233840942383
Batch 42/64 loss: 0.08094298839569092
Batch 43/64 loss: 0.08461976051330566
Batch 44/64 loss: 0.07634389400482178
Batch 45/64 loss: 0.06608033180236816
Batch 46/64 loss: 0.0651932954788208
Batch 47/64 loss: 0.06443935632705688
Batch 48/64 loss: 0.06062430143356323
Batch 49/64 loss: 0.08387339115142822
Batch 50/64 loss: 0.07606279850006104
Batch 51/64 loss: 0.09513455629348755
Batch 52/64 loss: 0.06777012348175049
Batch 53/64 loss: 0.08660870790481567
Batch 54/64 loss: 0.06696093082427979
Batch 55/64 loss: 0.08744663000106812
Batch 56/64 loss: 0.14288544654846191
Batch 57/64 loss: 0.08933365345001221
Batch 58/64 loss: 0.07534569501876831
Batch 59/64 loss: 0.07253134250640869
Batch 60/64 loss: 0.059390366077423096
Batch 61/64 loss: 0.0604856014251709
Batch 62/64 loss: 0.04716646671295166
Batch 63/64 loss: 0.08097350597381592
Batch 64/64 loss: 0.07720589637756348
Epoch 52  Train loss: 0.07332364624621822  Val loss: 0.10359251417245242
Epoch 53
-------------------------------
Batch 1/64 loss: 0.07483899593353271
Batch 2/64 loss: 0.07480907440185547
Batch 3/64 loss: 0.09554952383041382
Batch 4/64 loss: 0.0653340220451355
Batch 5/64 loss: 0.09162360429763794
Batch 6/64 loss: 0.10616511106491089
Batch 7/64 loss: 0.07031536102294922
Batch 8/64 loss: 0.06985998153686523
Batch 9/64 loss: 0.062039196491241455
Batch 10/64 loss: 0.0545346736907959
Batch 11/64 loss: 0.09339898824691772
Batch 12/64 loss: 0.06856679916381836
Batch 13/64 loss: 0.08837103843688965
Batch 14/64 loss: 0.10057926177978516
Batch 15/64 loss: 0.06653308868408203
Batch 16/64 loss: 0.05430656671524048
Batch 17/64 loss: 0.05867612361907959
Batch 18/64 loss: 0.04051351547241211
Batch 19/64 loss: 0.067221999168396
Batch 20/64 loss: 0.060328125953674316
Batch 21/64 loss: 0.08046376705169678
Batch 22/64 loss: 0.07301634550094604
Batch 23/64 loss: 0.05430328845977783
Batch 24/64 loss: 0.0635189414024353
Batch 25/64 loss: 0.06917649507522583
Batch 26/64 loss: 0.05968201160430908
Batch 27/64 loss: 0.0402262806892395
Batch 28/64 loss: 0.07456868886947632
Batch 29/64 loss: 0.062062978744506836
Batch 30/64 loss: 0.05604928731918335
Batch 31/64 loss: 0.06159830093383789
Batch 32/64 loss: 0.09282034635543823
Batch 33/64 loss: 0.03972905874252319
Batch 34/64 loss: 0.06964176893234253
Batch 35/64 loss: 0.10072445869445801
Batch 36/64 loss: 0.05782037973403931
Batch 37/64 loss: 0.06296432018280029
Batch 38/64 loss: 0.0626218318939209
Batch 39/64 loss: 0.06956923007965088
Batch 40/64 loss: 0.057132601737976074
Batch 41/64 loss: 0.06571978330612183
Batch 42/64 loss: 0.06870347261428833
Batch 43/64 loss: 0.08961844444274902
Batch 44/64 loss: 0.052458763122558594
Batch 45/64 loss: 0.0773734450340271
Batch 46/64 loss: 0.07955843210220337
Batch 47/64 loss: 0.05050933361053467
Batch 48/64 loss: 0.08605188131332397
Batch 49/64 loss: 0.06180918216705322
Batch 50/64 loss: 0.09853202104568481
Batch 51/64 loss: 0.07613539695739746
Batch 52/64 loss: 0.07288283109664917
Batch 53/64 loss: 0.070259690284729
Batch 54/64 loss: 0.0794680118560791
Batch 55/64 loss: 0.09313595294952393
Batch 56/64 loss: 0.0661274790763855
Batch 57/64 loss: 0.09171479940414429
Batch 58/64 loss: 0.08557206392288208
Batch 59/64 loss: 0.07084137201309204
Batch 60/64 loss: 0.08071792125701904
Batch 61/64 loss: 0.07841348648071289
Batch 62/64 loss: 0.0644228458404541
Batch 63/64 loss: 0.11226505041122437
Batch 64/64 loss: 0.06828194856643677
Epoch 53  Train loss: 0.07207464400459739  Val loss: 0.1137839187051832
Epoch 54
-------------------------------
Batch 1/64 loss: 0.0696142315864563
Batch 2/64 loss: 0.07108008861541748
Batch 3/64 loss: 0.07403331995010376
Batch 4/64 loss: 0.07847362756729126
Batch 5/64 loss: 0.06213951110839844
Batch 6/64 loss: 0.0803828239440918
Batch 7/64 loss: 0.06286048889160156
Batch 8/64 loss: 0.07700014114379883
Batch 9/64 loss: 0.08556908369064331
Batch 10/64 loss: 0.06126159429550171
Batch 11/64 loss: 0.05463296175003052
Batch 12/64 loss: 0.07369589805603027
Batch 13/64 loss: 0.08433997631072998
Batch 14/64 loss: 0.09349948167800903
Batch 15/64 loss: 0.07635217905044556
Batch 16/64 loss: 0.07416635751724243
Batch 17/64 loss: 0.06565314531326294
Batch 18/64 loss: 0.0760621428489685
Batch 19/64 loss: 0.0833694338798523
Batch 20/64 loss: 0.08016085624694824
Batch 21/64 loss: 0.06954550743103027
Batch 22/64 loss: 0.08079361915588379
Batch 23/64 loss: 0.08220827579498291
Batch 24/64 loss: 0.0814359188079834
Batch 25/64 loss: 0.07576119899749756
Batch 26/64 loss: 0.0628892183303833
Batch 27/64 loss: 0.08227711915969849
Batch 28/64 loss: 0.06838315725326538
Batch 29/64 loss: 0.06775510311126709
Batch 30/64 loss: 0.08654189109802246
Batch 31/64 loss: 0.060913920402526855
Batch 32/64 loss: 0.09113442897796631
Batch 33/64 loss: 0.06201577186584473
Batch 34/64 loss: 0.06193745136260986
Batch 35/64 loss: 0.06820470094680786
Batch 36/64 loss: 0.06104075908660889
Batch 37/64 loss: 0.06058531999588013
Batch 38/64 loss: 0.06200224161148071
Batch 39/64 loss: 0.07417809963226318
Batch 40/64 loss: 0.07284283638000488
Batch 41/64 loss: 0.06948763132095337
Batch 42/64 loss: 0.0659104585647583
Batch 43/64 loss: 0.07741695642471313
Batch 44/64 loss: 0.08206301927566528
Batch 45/64 loss: 0.07798945903778076
Batch 46/64 loss: 0.06925845146179199
Batch 47/64 loss: 0.07986783981323242
Batch 48/64 loss: 0.08624112606048584
Batch 49/64 loss: 0.04482084512710571
Batch 50/64 loss: 0.06847846508026123
Batch 51/64 loss: 0.06619763374328613
Batch 52/64 loss: 0.046373188495635986
Batch 53/64 loss: 0.05895209312438965
Batch 54/64 loss: 0.0665239691734314
Batch 55/64 loss: 0.07376271486282349
Batch 56/64 loss: 0.07118964195251465
Batch 57/64 loss: 0.08176898956298828
Batch 58/64 loss: 0.07041805982589722
Batch 59/64 loss: 0.06481218338012695
Batch 60/64 loss: 0.07337766885757446
Batch 61/64 loss: 0.08574056625366211
Batch 62/64 loss: 0.07031774520874023
Batch 63/64 loss: 0.090137779712677
Batch 64/64 loss: 0.0411067008972168
Epoch 54  Train loss: 0.07198008462494494  Val loss: 0.09801437682712201
Epoch 55
-------------------------------
Batch 1/64 loss: 0.054658591747283936
Batch 2/64 loss: 0.05009603500366211
Batch 3/64 loss: 0.03957533836364746
Batch 4/64 loss: 0.0860358476638794
Batch 5/64 loss: 0.08335733413696289
Batch 6/64 loss: 0.07257574796676636
Batch 7/64 loss: 0.06704908609390259
Batch 8/64 loss: 0.06776732206344604
Batch 9/64 loss: 0.05187416076660156
Batch 10/64 loss: 0.0565028190612793
Batch 11/64 loss: 0.08262747526168823
Batch 12/64 loss: 0.04450035095214844
Batch 13/64 loss: 0.06633609533309937
Batch 14/64 loss: 0.04845505952835083
Batch 15/64 loss: 0.06339049339294434
Batch 16/64 loss: 0.0602191686630249
Batch 17/64 loss: 0.06023693084716797
Batch 18/64 loss: 0.05075347423553467
Batch 19/64 loss: 0.06622779369354248
Batch 20/64 loss: 0.10854935646057129
Batch 21/64 loss: 0.09234970808029175
Batch 22/64 loss: 0.06195175647735596
Batch 23/64 loss: 0.05707824230194092
Batch 24/64 loss: 0.07184135913848877
Batch 25/64 loss: 0.0706719160079956
Batch 26/64 loss: 0.07215911149978638
Batch 27/64 loss: 0.07604378461837769
Batch 28/64 loss: 0.04132258892059326
Batch 29/64 loss: 0.10518026351928711
Batch 30/64 loss: 0.06392598152160645
Batch 31/64 loss: 0.052689552307128906
Batch 32/64 loss: 0.07903033494949341
Batch 33/64 loss: 0.07898175716400146
Batch 34/64 loss: 0.04870009422302246
Batch 35/64 loss: 0.06927293539047241
Batch 36/64 loss: 0.06525683403015137
Batch 37/64 loss: 0.0703461766242981
Batch 38/64 loss: 0.07519584894180298
Batch 39/64 loss: 0.05518120527267456
Batch 40/64 loss: 0.07945871353149414
Batch 41/64 loss: 0.04336661100387573
Batch 42/64 loss: 0.09586077928543091
Batch 43/64 loss: 0.0840221643447876
Batch 44/64 loss: 0.05584758520126343
Batch 45/64 loss: 0.057835936546325684
Batch 46/64 loss: 0.07763016223907471
Batch 47/64 loss: 0.07516723871231079
Batch 48/64 loss: 0.06865900754928589
Batch 49/64 loss: 0.0799856185913086
Batch 50/64 loss: 0.06051403284072876
Batch 51/64 loss: 0.05551415681838989
Batch 52/64 loss: 0.05861067771911621
Batch 53/64 loss: 0.05965179204940796
Batch 54/64 loss: 0.0611950159072876
Batch 55/64 loss: 0.06945770978927612
Batch 56/64 loss: 0.06063193082809448
Batch 57/64 loss: 0.06343591213226318
Batch 58/64 loss: 0.07961684465408325
Batch 59/64 loss: 0.09715104103088379
Batch 60/64 loss: 0.0716516375541687
Batch 61/64 loss: 0.06553506851196289
Batch 62/64 loss: 0.09052306413650513
Batch 63/64 loss: 0.0772925615310669
Batch 64/64 loss: 0.06496644020080566
Epoch 55  Train loss: 0.06784796901777679  Val loss: 0.10094616544205708
Epoch 56
-------------------------------
Batch 1/64 loss: 0.06689000129699707
Batch 2/64 loss: 0.08509039878845215
Batch 3/64 loss: 0.04645615816116333
Batch 4/64 loss: 0.03480958938598633
Batch 5/64 loss: 0.054094016551971436
Batch 6/64 loss: 0.06984037160873413
Batch 7/64 loss: 0.0475689172744751
Batch 8/64 loss: 0.05683445930480957
Batch 9/64 loss: 0.08060407638549805
Batch 10/64 loss: 0.0812196135520935
Batch 11/64 loss: 0.05657762289047241
Batch 12/64 loss: 0.058890342712402344
Batch 13/64 loss: 0.046114444732666016
Batch 14/64 loss: 0.04858565330505371
Batch 15/64 loss: 0.0745776891708374
Batch 16/64 loss: 0.05567842721939087
Batch 17/64 loss: 0.07206207513809204
Batch 18/64 loss: 0.07834732532501221
Batch 19/64 loss: 0.06857866048812866
Batch 20/64 loss: 0.06039869785308838
Batch 21/64 loss: 0.07955151796340942
Batch 22/64 loss: 0.08277535438537598
Batch 23/64 loss: 0.062431275844573975
Batch 24/64 loss: 0.06374073028564453
Batch 25/64 loss: 0.07474088668823242
Batch 26/64 loss: 0.0939716100692749
Batch 27/64 loss: 0.06314939260482788
Batch 28/64 loss: 0.06376588344573975
Batch 29/64 loss: 0.05698966979980469
Batch 30/64 loss: 0.09174370765686035
Batch 31/64 loss: 0.06150120496749878
Batch 32/64 loss: 0.05806994438171387
Batch 33/64 loss: 0.08254152536392212
Batch 34/64 loss: 0.0498080849647522
Batch 35/64 loss: 0.06678241491317749
Batch 36/64 loss: 0.08008038997650146
Batch 37/64 loss: 0.06269991397857666
Batch 38/64 loss: 0.08466863632202148
Batch 39/64 loss: 0.03957647085189819
Batch 40/64 loss: 0.07032561302185059
Batch 41/64 loss: 0.059446752071380615
Batch 42/64 loss: 0.0805472731590271
Batch 43/64 loss: 0.059431254863739014
Batch 44/64 loss: 0.07682693004608154
Batch 45/64 loss: 0.05912965536117554
Batch 46/64 loss: 0.09615534543991089
Batch 47/64 loss: 0.06098341941833496
Batch 48/64 loss: 0.06972384452819824
Batch 49/64 loss: 0.04822009801864624
Batch 50/64 loss: 0.06332552433013916
Batch 51/64 loss: 0.06873965263366699
Batch 52/64 loss: 0.08306372165679932
Batch 53/64 loss: 0.06489616632461548
Batch 54/64 loss: 0.07971882820129395
Batch 55/64 loss: 0.07125252485275269
Batch 56/64 loss: 0.07147496938705444
Batch 57/64 loss: 0.07523584365844727
Batch 58/64 loss: 0.06538951396942139
Batch 59/64 loss: 0.07543724775314331
Batch 60/64 loss: 0.059510886669158936
Batch 61/64 loss: 0.06030970811843872
Batch 62/64 loss: 0.08316338062286377
Batch 63/64 loss: 0.08206003904342651
Batch 64/64 loss: 0.05768853425979614
Epoch 56  Train loss: 0.06728536077574188  Val loss: 0.09378934193312917
Saving best model, epoch: 56
Epoch 57
-------------------------------
Batch 1/64 loss: 0.05006134510040283
Batch 2/64 loss: 0.08521062135696411
Batch 3/64 loss: 0.060112595558166504
Batch 4/64 loss: 0.05473220348358154
Batch 5/64 loss: 0.07704341411590576
Batch 6/64 loss: 0.07052302360534668
Batch 7/64 loss: 0.06628251075744629
Batch 8/64 loss: 0.07525146007537842
Batch 9/64 loss: 0.07264864444732666
Batch 10/64 loss: 0.049654245376586914
Batch 11/64 loss: 0.08746832609176636
Batch 12/64 loss: 0.04744458198547363
Batch 13/64 loss: 0.06963735818862915
Batch 14/64 loss: 0.05195528268814087
Batch 15/64 loss: 0.08326101303100586
Batch 16/64 loss: 0.054633140563964844
Batch 17/64 loss: 0.05721724033355713
Batch 18/64 loss: 0.06803154945373535
Batch 19/64 loss: 0.052190542221069336
Batch 20/64 loss: 0.06118285655975342
Batch 21/64 loss: 0.07317519187927246
Batch 22/64 loss: 0.05715137720108032
Batch 23/64 loss: 0.03736460208892822
Batch 24/64 loss: 0.05876976251602173
Batch 25/64 loss: 0.05584454536437988
Batch 26/64 loss: 0.04060477018356323
Batch 27/64 loss: 0.05323004722595215
Batch 28/64 loss: 0.03983020782470703
Batch 29/64 loss: 0.08370822668075562
Batch 30/64 loss: 0.05967062711715698
Batch 31/64 loss: 0.07767856121063232
Batch 32/64 loss: 0.07027292251586914
Batch 33/64 loss: 0.05729013681411743
Batch 34/64 loss: 0.037146568298339844
Batch 35/64 loss: 0.07023346424102783
Batch 36/64 loss: 0.06222653388977051
Batch 37/64 loss: 0.08904498815536499
Batch 38/64 loss: 0.08527266979217529
Batch 39/64 loss: 0.0733986496925354
Batch 40/64 loss: 0.05593794584274292
Batch 41/64 loss: 0.06293332576751709
Batch 42/64 loss: 0.05462378263473511
Batch 43/64 loss: 0.06311166286468506
Batch 44/64 loss: 0.07937628030776978
Batch 45/64 loss: 0.0728103518486023
Batch 46/64 loss: 0.06308960914611816
Batch 47/64 loss: 0.08179807662963867
Batch 48/64 loss: 0.0707511305809021
Batch 49/64 loss: 0.0845533013343811
Batch 50/64 loss: 0.0611652135848999
Batch 51/64 loss: 0.07167339324951172
Batch 52/64 loss: 0.05639159679412842
Batch 53/64 loss: 0.07950276136398315
Batch 54/64 loss: 0.09327542781829834
Batch 55/64 loss: 0.07355725765228271
Batch 56/64 loss: 0.0660557746887207
Batch 57/64 loss: 0.07850134372711182
Batch 58/64 loss: 0.07528841495513916
Batch 59/64 loss: 0.07647955417633057
Batch 60/64 loss: 0.055540502071380615
Batch 61/64 loss: 0.05239737033843994
Batch 62/64 loss: 0.05440175533294678
Batch 63/64 loss: 0.07427561283111572
Batch 64/64 loss: 0.04468578100204468
Epoch 57  Train loss: 0.06537194649378458  Val loss: 0.09502872486704404
Epoch 58
-------------------------------
Batch 1/64 loss: 0.08587092161178589
Batch 2/64 loss: 0.0648651123046875
Batch 3/64 loss: 0.06060904264450073
Batch 4/64 loss: 0.04874014854431152
Batch 5/64 loss: 0.053709447383880615
Batch 6/64 loss: 0.08112013339996338
Batch 7/64 loss: 0.05456280708312988
Batch 8/64 loss: 0.06913876533508301
Batch 9/64 loss: 0.05931973457336426
Batch 10/64 loss: 0.08172380924224854
Batch 11/64 loss: 0.0415194034576416
Batch 12/64 loss: 0.034324586391448975
Batch 13/64 loss: 0.0644761323928833
Batch 14/64 loss: 0.05709528923034668
Batch 15/64 loss: 0.050479769706726074
Batch 16/64 loss: 0.07973051071166992
Batch 17/64 loss: 0.07039272785186768
Batch 18/64 loss: 0.054283201694488525
Batch 19/64 loss: 0.06797099113464355
Batch 20/64 loss: 0.05380690097808838
Batch 21/64 loss: 0.07275903224945068
Batch 22/64 loss: 0.06069833040237427
Batch 23/64 loss: 0.04492610692977905
Batch 24/64 loss: 0.053246915340423584
Batch 25/64 loss: 0.07386976480484009
Batch 26/64 loss: 0.05308544635772705
Batch 27/64 loss: 0.08466958999633789
Batch 28/64 loss: 0.09209936857223511
Batch 29/64 loss: 0.04810452461242676
Batch 30/64 loss: 0.0600321888923645
Batch 31/64 loss: 0.06521344184875488
Batch 32/64 loss: 0.07005524635314941
Batch 33/64 loss: 0.07042163610458374
Batch 34/64 loss: 0.06151050329208374
Batch 35/64 loss: 0.06324160099029541
Batch 36/64 loss: 0.03944474458694458
Batch 37/64 loss: 0.05118662118911743
Batch 38/64 loss: 0.04471343755722046
Batch 39/64 loss: 0.08850520849227905
Batch 40/64 loss: 0.07539194822311401
Batch 41/64 loss: 0.051641762256622314
Batch 42/64 loss: 0.07650387287139893
Batch 43/64 loss: 0.05944979190826416
Batch 44/64 loss: 0.08794784545898438
Batch 45/64 loss: 0.07414031028747559
Batch 46/64 loss: 0.048870205879211426
Batch 47/64 loss: 0.06125235557556152
Batch 48/64 loss: 0.05082935094833374
Batch 49/64 loss: 0.08058559894561768
Batch 50/64 loss: 0.08860695362091064
Batch 51/64 loss: 0.06085205078125
Batch 52/64 loss: 0.06075942516326904
Batch 53/64 loss: 0.04741257429122925
Batch 54/64 loss: 0.07364678382873535
Batch 55/64 loss: 0.06869739294052124
Batch 56/64 loss: 0.07369762659072876
Batch 57/64 loss: 0.07136517763137817
Batch 58/64 loss: 0.04961496591567993
Batch 59/64 loss: 0.07532191276550293
Batch 60/64 loss: 0.05613994598388672
Batch 61/64 loss: 0.059094786643981934
Batch 62/64 loss: 0.0652998685836792
Batch 63/64 loss: 0.06167256832122803
Batch 64/64 loss: 0.0599595308303833
Epoch 58  Train loss: 0.0636123587103451  Val loss: 0.09095279129919727
Saving best model, epoch: 58
Epoch 59
-------------------------------
Batch 1/64 loss: 0.06492149829864502
Batch 2/64 loss: 0.07063984870910645
Batch 3/64 loss: 0.04422593116760254
Batch 4/64 loss: 0.07661402225494385
Batch 5/64 loss: 0.05119389295578003
Batch 6/64 loss: 0.054910361766815186
Batch 7/64 loss: 0.07150381803512573
Batch 8/64 loss: 0.07741928100585938
Batch 9/64 loss: 0.04907858371734619
Batch 10/64 loss: 0.0582619309425354
Batch 11/64 loss: 0.06425005197525024
Batch 12/64 loss: 0.062171339988708496
Batch 13/64 loss: 0.0551871657371521
Batch 14/64 loss: 0.05593216419219971
Batch 15/64 loss: 0.08795231580734253
Batch 16/64 loss: 0.07115626335144043
Batch 17/64 loss: 0.03584641218185425
Batch 18/64 loss: 0.07447755336761475
Batch 19/64 loss: 0.05765944719314575
Batch 20/64 loss: 0.07851046323776245
Batch 21/64 loss: 0.07808190584182739
Batch 22/64 loss: 0.07206380367279053
Batch 23/64 loss: 0.04050642251968384
Batch 24/64 loss: 0.0779263973236084
Batch 25/64 loss: 0.05463266372680664
Batch 26/64 loss: 0.05335342884063721
Batch 27/64 loss: 0.049758315086364746
Batch 28/64 loss: 0.05941784381866455
Batch 29/64 loss: 0.053307294845581055
Batch 30/64 loss: 0.023081541061401367
Batch 31/64 loss: 0.06399041414260864
Batch 32/64 loss: 0.08365505933761597
Batch 33/64 loss: 0.07330203056335449
Batch 34/64 loss: 0.06484287977218628
Batch 35/64 loss: 0.06282258033752441
Batch 36/64 loss: 0.08215606212615967
Batch 37/64 loss: 0.04926341772079468
Batch 38/64 loss: 0.07758826017379761
Batch 39/64 loss: 0.07663577795028687
Batch 40/64 loss: 0.07014650106430054
Batch 41/64 loss: 0.057308077812194824
Batch 42/64 loss: 0.05045241117477417
Batch 43/64 loss: 0.05163013935089111
Batch 44/64 loss: 0.07522928714752197
Batch 45/64 loss: 0.07530879974365234
Batch 46/64 loss: 0.05856722593307495
Batch 47/64 loss: 0.08814281225204468
Batch 48/64 loss: 0.04767310619354248
Batch 49/64 loss: 0.07165437936782837
Batch 50/64 loss: 0.058291614055633545
Batch 51/64 loss: 0.06367325782775879
Batch 52/64 loss: 0.06880313158035278
Batch 53/64 loss: 0.0617256760597229
Batch 54/64 loss: 0.06389814615249634
Batch 55/64 loss: 0.05055046081542969
Batch 56/64 loss: 0.0531541109085083
Batch 57/64 loss: 0.04557323455810547
Batch 58/64 loss: 0.04552304744720459
Batch 59/64 loss: 0.06971704959869385
Batch 60/64 loss: 0.06442832946777344
Batch 61/64 loss: 0.050874531269073486
Batch 62/64 loss: 0.06620359420776367
Batch 63/64 loss: 0.0595705509185791
Batch 64/64 loss: 0.063129723072052
Epoch 59  Train loss: 0.06233325822680604  Val loss: 0.094993887488375
Epoch 60
-------------------------------
Batch 1/64 loss: 0.0736737847328186
Batch 2/64 loss: 0.032000958919525146
Batch 3/64 loss: 0.03896522521972656
Batch 4/64 loss: 0.06413358449935913
Batch 5/64 loss: 0.04875755310058594
Batch 6/64 loss: 0.06140601634979248
Batch 7/64 loss: 0.046279966831207275
Batch 8/64 loss: 0.05995148420333862
Batch 9/64 loss: 0.057213008403778076
Batch 10/64 loss: 0.07497745752334595
Batch 11/64 loss: 0.06373792886734009
Batch 12/64 loss: 0.07498288154602051
Batch 13/64 loss: 0.06557852029800415
Batch 14/64 loss: 0.048736751079559326
Batch 15/64 loss: 0.043272554874420166
Batch 16/64 loss: 0.042892515659332275
Batch 17/64 loss: 0.06853282451629639
Batch 18/64 loss: 0.04157334566116333
Batch 19/64 loss: 0.0665099024772644
Batch 20/64 loss: 0.05315905809402466
Batch 21/64 loss: 0.0857689380645752
Batch 22/64 loss: 0.026023566722869873
Batch 23/64 loss: 0.04029357433319092
Batch 24/64 loss: 0.06240880489349365
Batch 25/64 loss: 0.06218552589416504
Batch 26/64 loss: 0.059355735778808594
Batch 27/64 loss: 0.05336242914199829
Batch 28/64 loss: 0.05121815204620361
Batch 29/64 loss: 0.04656451940536499
Batch 30/64 loss: 0.08104062080383301
Batch 31/64 loss: 0.04900413751602173
Batch 32/64 loss: 0.0624125599861145
Batch 33/64 loss: 0.062347352504730225
Batch 34/64 loss: 0.05343484878540039
Batch 35/64 loss: 0.06935447454452515
Batch 36/64 loss: 0.06267982721328735
Batch 37/64 loss: 0.06887412071228027
Batch 38/64 loss: 0.06529653072357178
Batch 39/64 loss: 0.07321560382843018
Batch 40/64 loss: 0.06204688549041748
Batch 41/64 loss: 0.0453038215637207
Batch 42/64 loss: 0.050245821475982666
Batch 43/64 loss: 0.059039950370788574
Batch 44/64 loss: 0.07935607433319092
Batch 45/64 loss: 0.07637417316436768
Batch 46/64 loss: 0.0681726336479187
Batch 47/64 loss: 0.06571054458618164
Batch 48/64 loss: 0.07265722751617432
Batch 49/64 loss: 0.05381488800048828
Batch 50/64 loss: 0.06664884090423584
Batch 51/64 loss: 0.08551603555679321
Batch 52/64 loss: 0.08468443155288696
Batch 53/64 loss: 0.06429648399353027
Batch 54/64 loss: 0.08111804723739624
Batch 55/64 loss: 0.07112908363342285
Batch 56/64 loss: 0.07030415534973145
Batch 57/64 loss: 0.060253143310546875
Batch 58/64 loss: 0.07263368368148804
Batch 59/64 loss: 0.06310898065567017
Batch 60/64 loss: 0.06065088510513306
Batch 61/64 loss: 0.08521521091461182
Batch 62/64 loss: 0.06896799802780151
Batch 63/64 loss: 0.05869555473327637
Batch 64/64 loss: 0.07769745588302612
Epoch 60  Train loss: 0.06188853792115754  Val loss: 0.09376915828468874
Epoch 61
-------------------------------
Batch 1/64 loss: 0.07776349782943726
Batch 2/64 loss: 0.05849313735961914
Batch 3/64 loss: 0.07282334566116333
Batch 4/64 loss: 0.06888794898986816
Batch 5/64 loss: 0.06661033630371094
Batch 6/64 loss: 0.070293128490448
Batch 7/64 loss: 0.07413017749786377
Batch 8/64 loss: 0.055410683155059814
Batch 9/64 loss: 0.04624950885772705
Batch 10/64 loss: 0.06091785430908203
Batch 11/64 loss: 0.06036090850830078
Batch 12/64 loss: 0.037697434425354004
Batch 13/64 loss: 0.06598776578903198
Batch 14/64 loss: 0.030692756175994873
Batch 15/64 loss: 0.060253679752349854
Batch 16/64 loss: 0.06870251893997192
Batch 17/64 loss: 0.07096731662750244
Batch 18/64 loss: 0.05709046125411987
Batch 19/64 loss: 0.0663071870803833
Batch 20/64 loss: 0.09454059600830078
Batch 21/64 loss: 0.0666164755821228
Batch 22/64 loss: 0.0702660083770752
Batch 23/64 loss: 0.06076502799987793
Batch 24/64 loss: 0.039374589920043945
Batch 25/64 loss: 0.05968809127807617
Batch 26/64 loss: 0.0527416467666626
Batch 27/64 loss: 0.07383579015731812
Batch 28/64 loss: 0.079559326171875
Batch 29/64 loss: 0.05876791477203369
Batch 30/64 loss: 0.04825782775878906
Batch 31/64 loss: 0.049098968505859375
Batch 32/64 loss: 0.04943817853927612
Batch 33/64 loss: 0.053583621978759766
Batch 34/64 loss: 0.057528674602508545
Batch 35/64 loss: 0.04800361394882202
Batch 36/64 loss: 0.08555775880813599
Batch 37/64 loss: 0.0497857928276062
Batch 38/64 loss: 0.05410736799240112
Batch 39/64 loss: 0.05653160810470581
Batch 40/64 loss: 0.038390517234802246
Batch 41/64 loss: 0.060788869857788086
Batch 42/64 loss: 0.04324394464492798
Batch 43/64 loss: 0.06450468301773071
Batch 44/64 loss: 0.03316378593444824
Batch 45/64 loss: 0.03732037544250488
Batch 46/64 loss: 0.08413183689117432
Batch 47/64 loss: 0.0609356164932251
Batch 48/64 loss: 0.03922617435455322
Batch 49/64 loss: 0.08308625221252441
Batch 50/64 loss: 0.03920614719390869
Batch 51/64 loss: 0.0600014328956604
Batch 52/64 loss: 0.05779498815536499
Batch 53/64 loss: 0.06317943334579468
Batch 54/64 loss: 0.0544353723526001
Batch 55/64 loss: 0.04212594032287598
Batch 56/64 loss: 0.07304251194000244
Batch 57/64 loss: 0.08192390203475952
Batch 58/64 loss: 0.03536057472229004
Batch 59/64 loss: 0.06277638673782349
Batch 60/64 loss: 0.07705628871917725
Batch 61/64 loss: 0.04935258626937866
Batch 62/64 loss: 0.09447985887527466
Batch 63/64 loss: 0.04899930953979492
Batch 64/64 loss: 0.06337052583694458
Epoch 61  Train loss: 0.059760677814483645  Val loss: 0.08947206690549031
Saving best model, epoch: 61
Epoch 62
-------------------------------
Batch 1/64 loss: 0.061688244342803955
Batch 2/64 loss: 0.06930392980575562
Batch 3/64 loss: 0.04682344198226929
Batch 4/64 loss: 0.03896439075469971
Batch 5/64 loss: 0.04266619682312012
Batch 6/64 loss: 0.061222612857818604
Batch 7/64 loss: 0.06265681982040405
Batch 8/64 loss: 0.054686903953552246
Batch 9/64 loss: 0.04741710424423218
Batch 10/64 loss: 0.042109131813049316
Batch 11/64 loss: 0.04836457967758179
Batch 12/64 loss: 0.04694110155105591
Batch 13/64 loss: 0.05537295341491699
Batch 14/64 loss: 0.058594465255737305
Batch 15/64 loss: 0.07062047719955444
Batch 16/64 loss: 0.07310378551483154
Batch 17/64 loss: 0.058446526527404785
Batch 18/64 loss: 0.06168341636657715
Batch 19/64 loss: 0.04885381460189819
Batch 20/64 loss: 0.06826192140579224
Batch 21/64 loss: 0.05448746681213379
Batch 22/64 loss: 0.06334298849105835
Batch 23/64 loss: 0.05082434415817261
Batch 24/64 loss: 0.05066549777984619
Batch 25/64 loss: 0.09177076816558838
Batch 26/64 loss: 0.04760432243347168
Batch 27/64 loss: 0.07324045896530151
Batch 28/64 loss: 0.07825297117233276
Batch 29/64 loss: 0.07215142250061035
Batch 30/64 loss: 0.06809687614440918
Batch 31/64 loss: 0.05095177888870239
Batch 32/64 loss: 0.03755927085876465
Batch 33/64 loss: 0.06854444742202759
Batch 34/64 loss: 0.06218600273132324
Batch 35/64 loss: 0.06775212287902832
Batch 36/64 loss: 0.06299340724945068
Batch 37/64 loss: 0.06176251173019409
Batch 38/64 loss: 0.0486411452293396
Batch 39/64 loss: 0.03559386730194092
Batch 40/64 loss: 0.0594027042388916
Batch 41/64 loss: 0.07901906967163086
Batch 42/64 loss: 0.045646846294403076
Batch 43/64 loss: 0.0663144588470459
Batch 44/64 loss: 0.05819964408874512
Batch 45/64 loss: 0.046015143394470215
Batch 46/64 loss: 0.0762529969215393
Batch 47/64 loss: 0.06331473588943481
Batch 48/64 loss: 0.05767470598220825
Batch 49/64 loss: 0.052514612674713135
Batch 50/64 loss: 0.08229422569274902
Batch 51/64 loss: 0.052456676959991455
Batch 52/64 loss: 0.04966616630554199
Batch 53/64 loss: 0.06783348321914673
Batch 54/64 loss: 0.06189239025115967
Batch 55/64 loss: 0.05107969045639038
Batch 56/64 loss: 0.04140782356262207
Batch 57/64 loss: 0.047096192836761475
Batch 58/64 loss: 0.04544639587402344
Batch 59/64 loss: 0.06404751539230347
Batch 60/64 loss: 0.07891011238098145
Batch 61/64 loss: 0.0597914457321167
Batch 62/64 loss: 0.0555453896522522
Batch 63/64 loss: 0.04883772134780884
Batch 64/64 loss: 0.050878822803497314
Epoch 62  Train loss: 0.0582434941740597  Val loss: 0.092303737946802
Epoch 63
-------------------------------
Batch 1/64 loss: 0.040848612785339355
Batch 2/64 loss: 0.05017101764678955
Batch 3/64 loss: 0.04782146215438843
Batch 4/64 loss: 0.05197346210479736
Batch 5/64 loss: 0.053394973278045654
Batch 6/64 loss: 0.056469857692718506
Batch 7/64 loss: 0.0214574933052063
Batch 8/64 loss: 0.07533615827560425
Batch 9/64 loss: 0.06326091289520264
Batch 10/64 loss: 0.048735082149505615
Batch 11/64 loss: 0.05264580249786377
Batch 12/64 loss: 0.08351534605026245
Batch 13/64 loss: 0.07611721754074097
Batch 14/64 loss: 0.06370341777801514
Batch 15/64 loss: 0.08884286880493164
Batch 16/64 loss: 0.05958837270736694
Batch 17/64 loss: 0.04476743936538696
Batch 18/64 loss: 0.0350726842880249
Batch 19/64 loss: 0.05624282360076904
Batch 20/64 loss: 0.04349881410598755
Batch 21/64 loss: 0.05851799249649048
Batch 22/64 loss: 0.07217156887054443
Batch 23/64 loss: 0.05700594186782837
Batch 24/64 loss: 0.0749887228012085
Batch 25/64 loss: 0.043531060218811035
Batch 26/64 loss: 0.06715834140777588
Batch 27/64 loss: 0.045543670654296875
Batch 28/64 loss: 0.050419747829437256
Batch 29/64 loss: 0.03764539957046509
Batch 30/64 loss: 0.0543784499168396
Batch 31/64 loss: 0.05422896146774292
Batch 32/64 loss: 0.07526969909667969
Batch 33/64 loss: 0.04707568883895874
Batch 34/64 loss: 0.06129336357116699
Batch 35/64 loss: 0.06741392612457275
Batch 36/64 loss: 0.0583530068397522
Batch 37/64 loss: 0.04460984468460083
Batch 38/64 loss: 0.035790324211120605
Batch 39/64 loss: 0.07816272974014282
Batch 40/64 loss: 0.05655038356781006
Batch 41/64 loss: 0.057181715965270996
Batch 42/64 loss: 0.040744781494140625
Batch 43/64 loss: 0.052734196186065674
Batch 44/64 loss: 0.05925559997558594
Batch 45/64 loss: 0.0692368745803833
Batch 46/64 loss: 0.048597872257232666
Batch 47/64 loss: 0.06364649534225464
Batch 48/64 loss: 0.04504120349884033
Batch 49/64 loss: 0.04975605010986328
Batch 50/64 loss: 0.06114685535430908
Batch 51/64 loss: 0.05369305610656738
Batch 52/64 loss: 0.06726521253585815
Batch 53/64 loss: 0.041064321994781494
Batch 54/64 loss: 0.0612332820892334
Batch 55/64 loss: 0.03999859094619751
Batch 56/64 loss: 0.05227077007293701
Batch 57/64 loss: 0.052062928676605225
Batch 58/64 loss: 0.05886411666870117
Batch 59/64 loss: 0.05699622631072998
Batch 60/64 loss: 0.07249563932418823
Batch 61/64 loss: 0.041146695613861084
Batch 62/64 loss: 0.09058541059494019
Batch 63/64 loss: 0.0607297420501709
Batch 64/64 loss: 0.07350760698318481
Epoch 63  Train loss: 0.056540407620224296  Val loss: 0.08939990391026657
Saving best model, epoch: 63
Epoch 64
-------------------------------
Batch 1/64 loss: 0.04788690805435181
Batch 2/64 loss: 0.055886805057525635
Batch 3/64 loss: 0.06528925895690918
Batch 4/64 loss: 0.02445775270462036
Batch 5/64 loss: 0.044612109661102295
Batch 6/64 loss: 0.06541979312896729
Batch 7/64 loss: 0.052808403968811035
Batch 8/64 loss: 0.031206846237182617
Batch 9/64 loss: 0.0371822714805603
Batch 10/64 loss: 0.05087292194366455
Batch 11/64 loss: 0.051134586334228516
Batch 12/64 loss: 0.04241502285003662
Batch 13/64 loss: 0.01443415880203247
Batch 14/64 loss: 0.055907607078552246
Batch 15/64 loss: 0.02467632293701172
Batch 16/64 loss: 0.05837947130203247
Batch 17/64 loss: 0.03356963396072388
Batch 18/64 loss: 0.06185293197631836
Batch 19/64 loss: 0.0826493501663208
Batch 20/64 loss: 0.05242156982421875
Batch 21/64 loss: 0.08052635192871094
Batch 22/64 loss: 0.05897790193557739
Batch 23/64 loss: 0.025196850299835205
Batch 24/64 loss: 0.071735680103302
Batch 25/64 loss: 0.08211344480514526
Batch 26/64 loss: 0.043959617614746094
Batch 27/64 loss: 0.05130445957183838
Batch 28/64 loss: 0.07051295042037964
Batch 29/64 loss: 0.06562286615371704
Batch 30/64 loss: 0.062103271484375
Batch 31/64 loss: 0.06743776798248291
Batch 32/64 loss: 0.05107051134109497
Batch 33/64 loss: 0.07520467042922974
Batch 34/64 loss: 0.05071616172790527
Batch 35/64 loss: 0.051230430603027344
Batch 36/64 loss: 0.054108619689941406
Batch 37/64 loss: 0.04429274797439575
Batch 38/64 loss: 0.06172078847885132
Batch 39/64 loss: 0.06616687774658203
Batch 40/64 loss: 0.052688002586364746
Batch 41/64 loss: 0.0711168646812439
Batch 42/64 loss: 0.045421481132507324
Batch 43/64 loss: 0.07191795110702515
Batch 44/64 loss: 0.061419010162353516
Batch 45/64 loss: 0.0452272891998291
Batch 46/64 loss: 0.08988773822784424
Batch 47/64 loss: 0.05592423677444458
Batch 48/64 loss: 0.06460124254226685
Batch 49/64 loss: 0.050682783126831055
Batch 50/64 loss: 0.056713998317718506
Batch 51/64 loss: 0.0664600133895874
Batch 52/64 loss: 0.042140841484069824
Batch 53/64 loss: 0.04748737812042236
Batch 54/64 loss: 0.08498883247375488
Batch 55/64 loss: 0.05352175235748291
Batch 56/64 loss: 0.03728020191192627
Batch 57/64 loss: 0.07776975631713867
Batch 58/64 loss: 0.09281527996063232
Batch 59/64 loss: 0.06863152980804443
Batch 60/64 loss: 0.06980979442596436
Batch 61/64 loss: 0.046085476875305176
Batch 62/64 loss: 0.042670369148254395
Batch 63/64 loss: 0.05257314443588257
Batch 64/64 loss: 0.0685739517211914
Epoch 64  Train loss: 0.05625617457371132  Val loss: 0.0862135127237982
Saving best model, epoch: 64
Epoch 65
-------------------------------
Batch 1/64 loss: 0.03494131565093994
Batch 2/64 loss: 0.06473559141159058
Batch 3/64 loss: 0.05111849308013916
Batch 4/64 loss: 0.052415549755096436
Batch 5/64 loss: 0.06321984529495239
Batch 6/64 loss: 0.08212363719940186
Batch 7/64 loss: 0.07633358240127563
Batch 8/64 loss: 0.040391743183135986
Batch 9/64 loss: 0.0556255578994751
Batch 10/64 loss: 0.055474698543548584
Batch 11/64 loss: 0.06079679727554321
Batch 12/64 loss: 0.07130932807922363
Batch 13/64 loss: 0.0862201452255249
Batch 14/64 loss: 0.07206195592880249
Batch 15/64 loss: 0.04149293899536133
Batch 16/64 loss: 0.04137641191482544
Batch 17/64 loss: 0.06037425994873047
Batch 18/64 loss: 0.048414766788482666
Batch 19/64 loss: 0.03593945503234863
Batch 20/64 loss: 0.07457906007766724
Batch 21/64 loss: 0.027708470821380615
Batch 22/64 loss: 0.04365283250808716
Batch 23/64 loss: 0.054627954959869385
Batch 24/64 loss: 0.045095622539520264
Batch 25/64 loss: 0.03536689281463623
Batch 26/64 loss: 0.056127727031707764
Batch 27/64 loss: 0.06959253549575806
Batch 28/64 loss: 0.08076667785644531
Batch 29/64 loss: 0.033851027488708496
Batch 30/64 loss: 0.06746464967727661
Batch 31/64 loss: 0.037827491760253906
Batch 32/64 loss: 0.044760704040527344
Batch 33/64 loss: 0.05819827318191528
Batch 34/64 loss: 0.03601747751235962
Batch 35/64 loss: 0.042313218116760254
Batch 36/64 loss: 0.05338418483734131
Batch 37/64 loss: 0.07902467250823975
Batch 38/64 loss: 0.04015690088272095
Batch 39/64 loss: 0.0592842698097229
Batch 40/64 loss: 0.06309795379638672
Batch 41/64 loss: 0.04967820644378662
Batch 42/64 loss: 0.07194769382476807
Batch 43/64 loss: 0.046121835708618164
Batch 44/64 loss: 0.06572163105010986
Batch 45/64 loss: 0.049118995666503906
Batch 46/64 loss: 0.06602370738983154
Batch 47/64 loss: 0.06560498476028442
Batch 48/64 loss: 0.05222821235656738
Batch 49/64 loss: 0.0431441068649292
Batch 50/64 loss: 0.05200767517089844
Batch 51/64 loss: 0.022986531257629395
Batch 52/64 loss: 0.07225477695465088
Batch 53/64 loss: 0.05738019943237305
Batch 54/64 loss: 0.06415826082229614
Batch 55/64 loss: 0.0491679310798645
Batch 56/64 loss: 0.04606527090072632
Batch 57/64 loss: 0.06219673156738281
Batch 58/64 loss: 0.05784863233566284
Batch 59/64 loss: 0.05031180381774902
Batch 60/64 loss: 0.04951369762420654
Batch 61/64 loss: 0.026776671409606934
Batch 62/64 loss: 0.05187910795211792
Batch 63/64 loss: 0.07972925901412964
Batch 64/64 loss: 0.08257675170898438
Epoch 65  Train loss: 0.05507550052568024  Val loss: 0.084699638930383
Saving best model, epoch: 65
Epoch 66
-------------------------------
Batch 1/64 loss: 0.05074024200439453
Batch 2/64 loss: 0.06171149015426636
Batch 3/64 loss: 0.04528588056564331
Batch 4/64 loss: 0.03582650423049927
Batch 5/64 loss: 0.06303781270980835
Batch 6/64 loss: 0.0526309609413147
Batch 7/64 loss: 0.0317801833152771
Batch 8/64 loss: 0.03798192739486694
Batch 9/64 loss: 0.04322254657745361
Batch 10/64 loss: 0.03296083211898804
Batch 11/64 loss: 0.04177218675613403
Batch 12/64 loss: 0.06562274694442749
Batch 13/64 loss: 0.04212099313735962
Batch 14/64 loss: 0.05111551284790039
Batch 15/64 loss: 0.06475210189819336
Batch 16/64 loss: 0.05060577392578125
Batch 17/64 loss: 0.04693007469177246
Batch 18/64 loss: 0.06426060199737549
Batch 19/64 loss: 0.05368220806121826
Batch 20/64 loss: 0.05349797010421753
Batch 21/64 loss: 0.03165990114212036
Batch 22/64 loss: 0.059015870094299316
Batch 23/64 loss: 0.051451802253723145
Batch 24/64 loss: 0.04636722803115845
Batch 25/64 loss: 0.0342678427696228
Batch 26/64 loss: 0.052747368812561035
Batch 27/64 loss: 0.06294548511505127
Batch 28/64 loss: 0.04146212339401245
Batch 29/64 loss: 0.07316809892654419
Batch 30/64 loss: 0.047873616218566895
Batch 31/64 loss: 0.05346477031707764
Batch 32/64 loss: 0.04485523700714111
Batch 33/64 loss: 0.045630812644958496
Batch 34/64 loss: 0.07086461782455444
Batch 35/64 loss: 0.043339550495147705
Batch 36/64 loss: 0.06368380784988403
Batch 37/64 loss: 0.05161041021347046
Batch 38/64 loss: 0.050073325634002686
Batch 39/64 loss: 0.0489078164100647
Batch 40/64 loss: 0.0440140962600708
Batch 41/64 loss: 0.07741373777389526
Batch 42/64 loss: 0.04434412717819214
Batch 43/64 loss: 0.04263925552368164
Batch 44/64 loss: 0.04750227928161621
Batch 45/64 loss: 0.0660855770111084
Batch 46/64 loss: 0.09067106246948242
Batch 47/64 loss: 0.0498354434967041
Batch 48/64 loss: 0.06749629974365234
Batch 49/64 loss: 0.04194766283035278
Batch 50/64 loss: 0.052822887897491455
Batch 51/64 loss: 0.07259368896484375
Batch 52/64 loss: 0.04902297258377075
Batch 53/64 loss: 0.057694196701049805
Batch 54/64 loss: 0.0531618595123291
Batch 55/64 loss: 0.04342019557952881
Batch 56/64 loss: 0.057083189487457275
Batch 57/64 loss: 0.053553223609924316
Batch 58/64 loss: 0.02130866050720215
Batch 59/64 loss: 0.07653820514678955
Batch 60/64 loss: 0.05987602472305298
Batch 61/64 loss: 0.05704808235168457
Batch 62/64 loss: 0.051965296268463135
Batch 63/64 loss: 0.0719183087348938
Batch 64/64 loss: 0.03073298931121826
Epoch 66  Train loss: 0.05232838135139615  Val loss: 0.0921857150149919
Epoch 67
-------------------------------
Batch 1/64 loss: 0.06106823682785034
Batch 2/64 loss: 0.039170026779174805
Batch 3/64 loss: 0.03294241428375244
Batch 4/64 loss: 0.04365682601928711
Batch 5/64 loss: 0.05188387632369995
Batch 6/64 loss: 0.039910852909088135
Batch 7/64 loss: 0.05310171842575073
Batch 8/64 loss: 0.04497635364532471
Batch 9/64 loss: 0.06596130132675171
Batch 10/64 loss: 0.040140390396118164
Batch 11/64 loss: 0.06410175561904907
Batch 12/64 loss: 0.059905409812927246
Batch 13/64 loss: 0.04640793800354004
Batch 14/64 loss: 0.03145182132720947
Batch 15/64 loss: 0.05872619152069092
Batch 16/64 loss: 0.05190783739089966
Batch 17/64 loss: 0.05142104625701904
Batch 18/64 loss: 0.022657275199890137
Batch 19/64 loss: 0.0652359127998352
Batch 20/64 loss: 0.054387032985687256
Batch 21/64 loss: 0.05138504505157471
Batch 22/64 loss: 0.03784763813018799
Batch 23/64 loss: 0.05010432004928589
Batch 24/64 loss: 0.0452384352684021
Batch 25/64 loss: 0.04009932279586792
Batch 26/64 loss: 0.056310176849365234
Batch 27/64 loss: 0.0615004301071167
Batch 28/64 loss: 0.037646710872650146
Batch 29/64 loss: 0.036549270153045654
Batch 30/64 loss: 0.06013989448547363
Batch 31/64 loss: 0.061787188053131104
Batch 32/64 loss: 0.03969860076904297
Batch 33/64 loss: 0.06061309576034546
Batch 34/64 loss: 0.04109025001525879
Batch 35/64 loss: 0.0473361611366272
Batch 36/64 loss: 0.036279261112213135
Batch 37/64 loss: 0.0625801682472229
Batch 38/64 loss: 0.07530218362808228
Batch 39/64 loss: 0.04686868190765381
Batch 40/64 loss: 0.05117344856262207
Batch 41/64 loss: 0.059103965759277344
Batch 42/64 loss: 0.04812312126159668
Batch 43/64 loss: 0.06612050533294678
Batch 44/64 loss: 0.06797873973846436
Batch 45/64 loss: 0.03981202840805054
Batch 46/64 loss: 0.07398521900177002
Batch 47/64 loss: 0.043945908546447754
Batch 48/64 loss: 0.048940062522888184
Batch 49/64 loss: 0.05450636148452759
Batch 50/64 loss: 0.08766037225723267
Batch 51/64 loss: 0.04087018966674805
Batch 52/64 loss: 0.060028135776519775
Batch 53/64 loss: 0.06675881147384644
Batch 54/64 loss: 0.06834781169891357
Batch 55/64 loss: 0.06479388475418091
Batch 56/64 loss: 0.055968642234802246
Batch 57/64 loss: 0.0625448226928711
Batch 58/64 loss: 0.053314805030822754
Batch 59/64 loss: 0.03701591491699219
Batch 60/64 loss: 0.03226238489151001
Batch 61/64 loss: 0.05722653865814209
Batch 62/64 loss: 0.04984700679779053
Batch 63/64 loss: 0.03581041097640991
Batch 64/64 loss: 0.055608272552490234
Epoch 67  Train loss: 0.051689981946758196  Val loss: 0.08532589960753713
Epoch 68
-------------------------------
Batch 1/64 loss: 0.05996745824813843
Batch 2/64 loss: 0.05972975492477417
Batch 3/64 loss: 0.03148680925369263
Batch 4/64 loss: 0.039864182472229004
Batch 5/64 loss: 0.05798208713531494
Batch 6/64 loss: 0.049812495708465576
Batch 7/64 loss: 0.04387533664703369
Batch 8/64 loss: 0.06708818674087524
Batch 9/64 loss: 0.05688345432281494
Batch 10/64 loss: 0.06411510705947876
Batch 11/64 loss: 0.06337219476699829
Batch 12/64 loss: 0.04663205146789551
Batch 13/64 loss: 0.047625064849853516
Batch 14/64 loss: 0.04591476917266846
Batch 15/64 loss: 0.04084497690200806
Batch 16/64 loss: 0.04020214080810547
Batch 17/64 loss: 0.06028813123703003
Batch 18/64 loss: 0.06271308660507202
Batch 19/64 loss: 0.015663206577301025
Batch 20/64 loss: 0.05906069278717041
Batch 21/64 loss: 0.04145228862762451
Batch 22/64 loss: 0.0660698413848877
Batch 23/64 loss: 0.033809006214141846
Batch 24/64 loss: 0.03275656700134277
Batch 25/64 loss: 0.04831349849700928
Batch 26/64 loss: 0.05996906757354736
Batch 27/64 loss: 0.023329854011535645
Batch 28/64 loss: 0.02148383855819702
Batch 29/64 loss: 0.05625653266906738
Batch 30/64 loss: 0.059166014194488525
Batch 31/64 loss: 0.060103416442871094
Batch 32/64 loss: 0.04900538921356201
Batch 33/64 loss: 0.02667558193206787
Batch 34/64 loss: 0.07302945852279663
Batch 35/64 loss: 0.06631159782409668
Batch 36/64 loss: 0.03573143482208252
Batch 37/64 loss: 0.06498253345489502
Batch 38/64 loss: 0.02937185764312744
Batch 39/64 loss: 0.053497254848480225
Batch 40/64 loss: 0.04277646541595459
Batch 41/64 loss: 0.06507879495620728
Batch 42/64 loss: 0.0546756386756897
Batch 43/64 loss: 0.07372909784317017
Batch 44/64 loss: 0.0578877329826355
Batch 45/64 loss: 0.06483161449432373
Batch 46/64 loss: 0.027005434036254883
Batch 47/64 loss: 0.06293773651123047
Batch 48/64 loss: 0.03416788578033447
Batch 49/64 loss: 0.05644577741622925
Batch 50/64 loss: 0.01896435022354126
Batch 51/64 loss: 0.042448997497558594
Batch 52/64 loss: 0.07774865627288818
Batch 53/64 loss: 0.03929513692855835
Batch 54/64 loss: 0.05736881494522095
Batch 55/64 loss: 0.06925690174102783
Batch 56/64 loss: 0.05253243446350098
Batch 57/64 loss: 0.05734354257583618
Batch 58/64 loss: 0.0678180456161499
Batch 59/64 loss: 0.08797961473464966
Batch 60/64 loss: 0.050002217292785645
Batch 61/64 loss: 0.05943799018859863
Batch 62/64 loss: 0.051997363567352295
Batch 63/64 loss: 0.04155075550079346
Batch 64/64 loss: 0.06897854804992676
Epoch 68  Train loss: 0.051442370695226336  Val loss: 0.08621134708837136
Epoch 69
-------------------------------
Batch 1/64 loss: 0.05127310752868652
Batch 2/64 loss: 0.07471823692321777
Batch 3/64 loss: 0.06162106990814209
Batch 4/64 loss: 0.0385974645614624
Batch 5/64 loss: 0.07642114162445068
Batch 6/64 loss: 0.04510033130645752
Batch 7/64 loss: 0.036812782287597656
Batch 8/64 loss: 0.023157179355621338
Batch 9/64 loss: 0.046922504901885986
Batch 10/64 loss: 0.039616167545318604
Batch 11/64 loss: 0.022587358951568604
Batch 12/64 loss: 0.05548393726348877
Batch 13/64 loss: 0.05243480205535889
Batch 14/64 loss: 0.06240898370742798
Batch 15/64 loss: 0.034795939922332764
Batch 16/64 loss: 0.0514567494392395
Batch 17/64 loss: 0.0620189905166626
Batch 18/64 loss: 0.0276261568069458
Batch 19/64 loss: 0.03480643033981323
Batch 20/64 loss: 0.033332884311676025
Batch 21/64 loss: 0.060527026653289795
Batch 22/64 loss: 0.04525744915008545
Batch 23/64 loss: 0.02931314706802368
Batch 24/64 loss: 0.0593448281288147
Batch 25/64 loss: 0.07122039794921875
Batch 26/64 loss: 0.06634765863418579
Batch 27/64 loss: 0.0589945912361145
Batch 28/64 loss: 0.05464458465576172
Batch 29/64 loss: 0.04715299606323242
Batch 30/64 loss: 0.07048934698104858
Batch 31/64 loss: 0.06906670331954956
Batch 32/64 loss: 0.03232276439666748
Batch 33/64 loss: 0.03336566686630249
Batch 34/64 loss: 0.04637753963470459
Batch 35/64 loss: 0.05307352542877197
Batch 36/64 loss: 0.06573706865310669
Batch 37/64 loss: 0.02581346035003662
Batch 38/64 loss: 0.045432865619659424
Batch 39/64 loss: 0.029712259769439697
Batch 40/64 loss: 0.026563823223114014
Batch 41/64 loss: 0.054308414459228516
Batch 42/64 loss: 0.0588451623916626
Batch 43/64 loss: 0.04735952615737915
Batch 44/64 loss: 0.08064937591552734
Batch 45/64 loss: 0.05401289463043213
Batch 46/64 loss: 0.04132276773452759
Batch 47/64 loss: 0.02950185537338257
Batch 48/64 loss: 0.07005834579467773
Batch 49/64 loss: 0.06803852319717407
Batch 50/64 loss: 0.05887007713317871
Batch 51/64 loss: 0.03647899627685547
Batch 52/64 loss: 0.04486393928527832
Batch 53/64 loss: 0.031881093978881836
Batch 54/64 loss: 0.06366932392120361
Batch 55/64 loss: 0.07160782814025879
Batch 56/64 loss: 0.04372692108154297
Batch 57/64 loss: 0.058884620666503906
Batch 58/64 loss: 0.03209328651428223
Batch 59/64 loss: 0.0886678695678711
Batch 60/64 loss: 0.060802578926086426
Batch 61/64 loss: 0.052194416522979736
Batch 62/64 loss: 0.04602867364883423
Batch 63/64 loss: 0.06197226047515869
Batch 64/64 loss: 0.06430590152740479
Epoch 69  Train loss: 0.05060420550552069  Val loss: 0.08496947509726298
Epoch 70
-------------------------------
Batch 1/64 loss: 0.0764913558959961
Batch 2/64 loss: 0.061265528202056885
Batch 3/64 loss: 0.039817214012145996
Batch 4/64 loss: 0.027258992195129395
Batch 5/64 loss: 0.03452521562576294
Batch 6/64 loss: 0.036377668380737305
Batch 7/64 loss: 0.03372633457183838
Batch 8/64 loss: 0.05582284927368164
Batch 9/64 loss: 0.05932873487472534
Batch 10/64 loss: 0.03944522142410278
Batch 11/64 loss: 0.03833073377609253
Batch 12/64 loss: 0.07246428728103638
Batch 13/64 loss: 0.06328350305557251
Batch 14/64 loss: 0.03297299146652222
Batch 15/64 loss: 0.037314534187316895
Batch 16/64 loss: 0.06895369291305542
Batch 17/64 loss: 0.06819748878479004
Batch 18/64 loss: 0.04369759559631348
Batch 19/64 loss: 0.06655639410018921
Batch 20/64 loss: 0.053822457790374756
Batch 21/64 loss: 0.037525296211242676
Batch 22/64 loss: 0.03285151720046997
Batch 23/64 loss: 0.047675251960754395
Batch 24/64 loss: 0.0556255578994751
Batch 25/64 loss: 0.028330087661743164
Batch 26/64 loss: 0.03491818904876709
Batch 27/64 loss: 0.06060749292373657
Batch 28/64 loss: 0.04157441854476929
Batch 29/64 loss: 0.06377553939819336
Batch 30/64 loss: 0.018418431282043457
Batch 31/64 loss: 0.04478096961975098
Batch 32/64 loss: 0.025902867317199707
Batch 33/64 loss: 0.05808919668197632
Batch 34/64 loss: 0.056056201457977295
Batch 35/64 loss: 0.03352463245391846
Batch 36/64 loss: 0.019894182682037354
Batch 37/64 loss: 0.07384204864501953
Batch 38/64 loss: 0.05975443124771118
Batch 39/64 loss: 0.04891413450241089
Batch 40/64 loss: 0.04681187868118286
Batch 41/64 loss: 0.06897354125976562
Batch 42/64 loss: 0.06127774715423584
Batch 43/64 loss: 0.0360180139541626
Batch 44/64 loss: 0.05528378486633301
Batch 45/64 loss: 0.03948265314102173
Batch 46/64 loss: 0.0554807186126709
Batch 47/64 loss: 0.06191939115524292
Batch 48/64 loss: 0.036940574645996094
Batch 49/64 loss: 0.05373692512512207
Batch 50/64 loss: 0.041291236877441406
Batch 51/64 loss: 0.048054277896881104
Batch 52/64 loss: 0.02399766445159912
Batch 53/64 loss: 0.03678256273269653
Batch 54/64 loss: 0.056729793548583984
Batch 55/64 loss: 0.0685773491859436
Batch 56/64 loss: 0.04021894931793213
Batch 57/64 loss: 0.06001150608062744
Batch 58/64 loss: 0.039987921714782715
Batch 59/64 loss: 0.05017220973968506
Batch 60/64 loss: 0.05229610204696655
Batch 61/64 loss: 0.04807627201080322
Batch 62/64 loss: 0.05390268564224243
Batch 63/64 loss: 0.03035140037536621
Batch 64/64 loss: 0.04283249378204346
Epoch 70  Train loss: 0.047846474834516935  Val loss: 0.0801576642236349
Saving best model, epoch: 70
Epoch 71
-------------------------------
Batch 1/64 loss: 0.061879754066467285
Batch 2/64 loss: 0.0246085524559021
Batch 3/64 loss: 0.0365222692489624
Batch 4/64 loss: 0.03362685441970825
Batch 5/64 loss: 0.0417022705078125
Batch 6/64 loss: 0.050725579261779785
Batch 7/64 loss: 0.039148032665252686
Batch 8/64 loss: 0.03854811191558838
Batch 9/64 loss: 0.0463719367980957
Batch 10/64 loss: 0.05046558380126953
Batch 11/64 loss: 0.06847107410430908
Batch 12/64 loss: 0.047205567359924316
Batch 13/64 loss: 0.0249212384223938
Batch 14/64 loss: 0.06522154808044434
Batch 15/64 loss: 0.048145413398742676
Batch 16/64 loss: 0.04547351598739624
Batch 17/64 loss: 0.05828213691711426
Batch 18/64 loss: 0.03927958011627197
Batch 19/64 loss: 0.05018949508666992
Batch 20/64 loss: 0.045661330223083496
Batch 21/64 loss: 0.04120439291000366
Batch 22/64 loss: 0.05864220857620239
Batch 23/64 loss: 0.06472790241241455
Batch 24/64 loss: 0.0352289080619812
Batch 25/64 loss: 0.05952811241149902
Batch 26/64 loss: 0.030006110668182373
Batch 27/64 loss: 0.05097132921218872
Batch 28/64 loss: 0.05369997024536133
Batch 29/64 loss: 0.04810059070587158
Batch 30/64 loss: 0.0383836030960083
Batch 31/64 loss: 0.05869400501251221
Batch 32/64 loss: 0.0253714919090271
Batch 33/64 loss: 0.05278205871582031
Batch 34/64 loss: 0.04420936107635498
Batch 35/64 loss: 0.027463436126708984
Batch 36/64 loss: 0.05953824520111084
Batch 37/64 loss: 0.06646561622619629
Batch 38/64 loss: 0.05480372905731201
Batch 39/64 loss: 0.0348895788192749
Batch 40/64 loss: 0.03317070007324219
Batch 41/64 loss: 0.03416109085083008
Batch 42/64 loss: 0.09049844741821289
Batch 43/64 loss: 0.02383124828338623
Batch 44/64 loss: 0.056915998458862305
Batch 45/64 loss: 0.03935420513153076
Batch 46/64 loss: 0.034186482429504395
Batch 47/64 loss: 0.042790770530700684
Batch 48/64 loss: 0.039895713329315186
Batch 49/64 loss: 0.034355103969573975
Batch 50/64 loss: 0.049707651138305664
Batch 51/64 loss: 0.05283457040786743
Batch 52/64 loss: 0.028139591217041016
Batch 53/64 loss: 0.05490058660507202
Batch 54/64 loss: 0.03209489583969116
Batch 55/64 loss: 0.031943678855895996
Batch 56/64 loss: 0.06264960765838623
Batch 57/64 loss: 0.048852622509002686
Batch 58/64 loss: 0.042009055614471436
Batch 59/64 loss: 0.033698856830596924
Batch 60/64 loss: 0.053095996379852295
Batch 61/64 loss: 0.04518646001815796
Batch 62/64 loss: 0.06570601463317871
Batch 63/64 loss: 0.06807553768157959
Batch 64/64 loss: 0.05751579999923706
Epoch 71  Train loss: 0.0464682702924691  Val loss: 0.0820244896862515
Epoch 72
-------------------------------
Batch 1/64 loss: 0.018983125686645508
Batch 2/64 loss: 0.06945192813873291
Batch 3/64 loss: 0.0512431263923645
Batch 4/64 loss: 0.03410142660140991
Batch 5/64 loss: 0.05719727277755737
Batch 6/64 loss: 0.060297608375549316
Batch 7/64 loss: 0.027723073959350586
Batch 8/64 loss: 0.050634145736694336
Batch 9/64 loss: 0.03807985782623291
Batch 10/64 loss: 0.0418890118598938
Batch 11/64 loss: 0.049216389656066895
Batch 12/64 loss: 0.05389970541000366
Batch 13/64 loss: 0.05241018533706665
Batch 14/64 loss: 0.05894869565963745
Batch 15/64 loss: 0.044423043727874756
Batch 16/64 loss: 0.06760752201080322
Batch 17/64 loss: 0.057152509689331055
Batch 18/64 loss: 0.05608099699020386
Batch 19/64 loss: 0.049613773822784424
Batch 20/64 loss: 0.02941673994064331
Batch 21/64 loss: 0.04383683204650879
Batch 22/64 loss: 0.05836218595504761
Batch 23/64 loss: 0.047395169734954834
Batch 24/64 loss: 0.0491068959236145
Batch 25/64 loss: 0.041870057582855225
Batch 26/64 loss: 0.050760626792907715
Batch 27/64 loss: 0.043773531913757324
Batch 28/64 loss: 0.03802168369293213
Batch 29/64 loss: 0.060788869857788086
Batch 30/64 loss: 0.05687844753265381
Batch 31/64 loss: 0.03561091423034668
Batch 32/64 loss: 0.05930393934249878
Batch 33/64 loss: 0.03242301940917969
Batch 34/64 loss: 0.02268970012664795
Batch 35/64 loss: 0.028454065322875977
Batch 36/64 loss: 0.05878949165344238
Batch 37/64 loss: 0.03157991170883179
Batch 38/64 loss: 0.04176288843154907
Batch 39/64 loss: 0.05328357219696045
Batch 40/64 loss: 0.04484736919403076
Batch 41/64 loss: 0.04385519027709961
Batch 42/64 loss: 0.03246062994003296
Batch 43/64 loss: 0.030901670455932617
Batch 44/64 loss: 0.0665321946144104
Batch 45/64 loss: 0.03623175621032715
Batch 46/64 loss: 0.031838297843933105
Batch 47/64 loss: 0.04714345932006836
Batch 48/64 loss: 0.044831275939941406
Batch 49/64 loss: 0.03514379262924194
Batch 50/64 loss: 0.0352177619934082
Batch 51/64 loss: 0.03291064500808716
Batch 52/64 loss: 0.07254225015640259
Batch 53/64 loss: 0.041900694370269775
Batch 54/64 loss: 0.04076945781707764
Batch 55/64 loss: 0.03923749923706055
Batch 56/64 loss: 0.0498577356338501
Batch 57/64 loss: 0.0572582483291626
Batch 58/64 loss: 0.04953598976135254
Batch 59/64 loss: 0.06504201889038086
Batch 60/64 loss: 0.03652751445770264
Batch 61/64 loss: 0.036959707736968994
Batch 62/64 loss: 0.05047231912612915
Batch 63/64 loss: 0.03862041234970093
Batch 64/64 loss: 0.05367612838745117
Epoch 72  Train loss: 0.045865991068821325  Val loss: 0.0804219575682047
Epoch 73
-------------------------------
Batch 1/64 loss: 0.02135777473449707
Batch 2/64 loss: 0.029007256031036377
Batch 3/64 loss: 0.030402302742004395
Batch 4/64 loss: 0.054355621337890625
Batch 5/64 loss: 0.039041996002197266
Batch 6/64 loss: 0.06872910261154175
Batch 7/64 loss: 0.05694001913070679
Batch 8/64 loss: 0.056738972663879395
Batch 9/64 loss: 0.03776627779006958
Batch 10/64 loss: 0.04096341133117676
Batch 11/64 loss: 0.04632973670959473
Batch 12/64 loss: 0.055500149726867676
Batch 13/64 loss: 0.044035911560058594
Batch 14/64 loss: 0.04627704620361328
Batch 15/64 loss: 0.05434983968734741
Batch 16/64 loss: 0.07250672578811646
Batch 17/64 loss: 0.026846468448638916
Batch 18/64 loss: 0.08531457185745239
Batch 19/64 loss: 0.03356832265853882
Batch 20/64 loss: 0.03783905506134033
Batch 21/64 loss: 0.06666982173919678
Batch 22/64 loss: 0.03661906719207764
Batch 23/64 loss: 0.02539139986038208
Batch 24/64 loss: 0.04587209224700928
Batch 25/64 loss: 0.04600250720977783
Batch 26/64 loss: 0.06194770336151123
Batch 27/64 loss: 0.051014244556427
Batch 28/64 loss: 0.038058578968048096
Batch 29/64 loss: 0.06367158889770508
Batch 30/64 loss: 0.052373647689819336
Batch 31/64 loss: 0.05744969844818115
Batch 32/64 loss: 0.02707219123840332
Batch 33/64 loss: 0.05330014228820801
Batch 34/64 loss: 0.025735020637512207
Batch 35/64 loss: 0.034021615982055664
Batch 36/64 loss: 0.05493569374084473
Batch 37/64 loss: 0.04564857482910156
Batch 38/64 loss: 0.03283512592315674
Batch 39/64 loss: 0.0813748836517334
Batch 40/64 loss: 0.013906359672546387
Batch 41/64 loss: 0.054143309593200684
Batch 42/64 loss: 0.04714834690093994
Batch 43/64 loss: 0.02443838119506836
Batch 44/64 loss: 0.06499648094177246
Batch 45/64 loss: 0.05600762367248535
Batch 46/64 loss: 0.06459009647369385
Batch 47/64 loss: 0.05442327260971069
Batch 48/64 loss: 0.02258586883544922
Batch 49/64 loss: 0.024639248847961426
Batch 50/64 loss: 0.033839523792266846
Batch 51/64 loss: 0.05260199308395386
Batch 52/64 loss: 0.03328746557235718
Batch 53/64 loss: 0.037978172302246094
Batch 54/64 loss: 0.04961347579956055
Batch 55/64 loss: 0.05024135112762451
Batch 56/64 loss: 0.03768259286880493
Batch 57/64 loss: 0.0306549072265625
Batch 58/64 loss: 0.04263937473297119
Batch 59/64 loss: 0.022140681743621826
Batch 60/64 loss: 0.04213589429855347
Batch 61/64 loss: 0.0683584213256836
Batch 62/64 loss: 0.04706394672393799
Batch 63/64 loss: 0.054033637046813965
Batch 64/64 loss: 0.054107487201690674
Epoch 73  Train loss: 0.04560933650708666  Val loss: 0.07995388065416788
Saving best model, epoch: 73
Epoch 74
-------------------------------
Batch 1/64 loss: 0.042677462100982666
Batch 2/64 loss: 0.037450551986694336
Batch 3/64 loss: 0.03183555603027344
Batch 4/64 loss: 0.031764328479766846
Batch 5/64 loss: 0.026411831378936768
Batch 6/64 loss: 0.028862595558166504
Batch 7/64 loss: 0.03994840383529663
Batch 8/64 loss: 0.041894376277923584
Batch 9/64 loss: 0.040988922119140625
Batch 10/64 loss: 0.04438894987106323
Batch 11/64 loss: 0.01961570978164673
Batch 12/64 loss: 0.038897156715393066
Batch 13/64 loss: 0.06400728225708008
Batch 14/64 loss: 0.046347081661224365
Batch 15/64 loss: 0.05515778064727783
Batch 16/64 loss: 0.05692929029464722
Batch 17/64 loss: 0.034445345401763916
Batch 18/64 loss: 0.03673219680786133
Batch 19/64 loss: 0.03696030378341675
Batch 20/64 loss: 0.015291810035705566
Batch 21/64 loss: 0.03917419910430908
Batch 22/64 loss: 0.06998038291931152
Batch 23/64 loss: 0.01949399709701538
Batch 24/64 loss: 0.04229378700256348
Batch 25/64 loss: 0.06330031156539917
Batch 26/64 loss: 0.04350411891937256
Batch 27/64 loss: 0.04191499948501587
Batch 28/64 loss: 0.05350184440612793
Batch 29/64 loss: 0.048067331314086914
Batch 30/64 loss: 0.038498520851135254
Batch 31/64 loss: 0.04664963483810425
Batch 32/64 loss: 0.05187368392944336
Batch 33/64 loss: 0.03181189298629761
Batch 34/64 loss: 0.03931760787963867
Batch 35/64 loss: 0.06059587001800537
Batch 36/64 loss: 0.0579647421836853
Batch 37/64 loss: 0.042203426361083984
Batch 38/64 loss: 0.04438048601150513
Batch 39/64 loss: 0.04371356964111328
Batch 40/64 loss: 0.020968198776245117
Batch 41/64 loss: 0.039036333560943604
Batch 42/64 loss: 0.05237549543380737
Batch 43/64 loss: 0.051471829414367676
Batch 44/64 loss: 0.03315359354019165
Batch 45/64 loss: 0.057692646980285645
Batch 46/64 loss: 0.03773021697998047
Batch 47/64 loss: 0.05077916383743286
Batch 48/64 loss: 0.07528775930404663
Batch 49/64 loss: 0.031032204627990723
Batch 50/64 loss: 0.052697956562042236
Batch 51/64 loss: 0.04384207725524902
Batch 52/64 loss: 0.038171470165252686
Batch 53/64 loss: 0.06815868616104126
Batch 54/64 loss: 0.058456480503082275
Batch 55/64 loss: 0.02942270040512085
Batch 56/64 loss: 0.03342723846435547
Batch 57/64 loss: 0.03892552852630615
Batch 58/64 loss: 0.05723506212234497
Batch 59/64 loss: 0.06757622957229614
Batch 60/64 loss: 0.05317211151123047
Batch 61/64 loss: 0.033166706562042236
Batch 62/64 loss: 0.046423912048339844
Batch 63/64 loss: 0.04854273796081543
Batch 64/64 loss: 0.052118778228759766
Epoch 74  Train loss: 0.04402639632131539  Val loss: 0.08053878820229232
Epoch 75
-------------------------------
Batch 1/64 loss: 0.024554133415222168
Batch 2/64 loss: 0.01804882287979126
Batch 3/64 loss: 0.0372769832611084
Batch 4/64 loss: 0.046051621437072754
Batch 5/64 loss: 0.06562662124633789
Batch 6/64 loss: 0.048260390758514404
Batch 7/64 loss: 0.04630434513092041
Batch 8/64 loss: 0.08151501417160034
Batch 9/64 loss: 0.027257084846496582
Batch 10/64 loss: 0.032788991928100586
Batch 11/64 loss: 0.04453301429748535
Batch 12/64 loss: 0.044176340103149414
Batch 13/64 loss: 0.03568840026855469
Batch 14/64 loss: 0.05063927173614502
Batch 15/64 loss: 0.029276251792907715
Batch 16/64 loss: 0.026853203773498535
Batch 17/64 loss: 0.042669475078582764
Batch 18/64 loss: 0.025332212448120117
Batch 19/64 loss: 0.03164798021316528
Batch 20/64 loss: 0.07222336530685425
Batch 21/64 loss: 0.051260411739349365
Batch 22/64 loss: 0.0188712477684021
Batch 23/64 loss: 0.04944545030593872
Batch 24/64 loss: 0.015222907066345215
Batch 25/64 loss: 0.04598182439804077
Batch 26/64 loss: 0.03711062669754028
Batch 27/64 loss: 0.04704707860946655
Batch 28/64 loss: 0.03286385536193848
Batch 29/64 loss: 0.034211814403533936
Batch 30/64 loss: 0.0455666184425354
Batch 31/64 loss: 0.04597681760787964
Batch 32/64 loss: 0.02235013246536255
Batch 33/64 loss: 0.02952730655670166
Batch 34/64 loss: 0.04221761226654053
Batch 35/64 loss: 0.025105535984039307
Batch 36/64 loss: 0.01258629560470581
Batch 37/64 loss: 0.03725159168243408
Batch 38/64 loss: 0.05911147594451904
Batch 39/64 loss: 0.04878199100494385
Batch 40/64 loss: 0.043121933937072754
Batch 41/64 loss: 0.031219303607940674
Batch 42/64 loss: 0.05267918109893799
Batch 43/64 loss: 0.05921655893325806
Batch 44/64 loss: 0.027543306350708008
Batch 45/64 loss: 0.03269881010055542
Batch 46/64 loss: 0.0692245364189148
Batch 47/64 loss: 0.06804710626602173
Batch 48/64 loss: 0.046015143394470215
Batch 49/64 loss: 0.06007516384124756
Batch 50/64 loss: 0.025766313076019287
Batch 51/64 loss: 0.05582928657531738
Batch 52/64 loss: 0.04788792133331299
Batch 53/64 loss: 0.050849318504333496
Batch 54/64 loss: 0.047492146492004395
Batch 55/64 loss: 0.05739092826843262
Batch 56/64 loss: 0.05074256658554077
Batch 57/64 loss: 0.03695797920227051
Batch 58/64 loss: 0.04246276617050171
Batch 59/64 loss: 0.053626179695129395
Batch 60/64 loss: 0.03906816244125366
Batch 61/64 loss: 0.04362732172012329
Batch 62/64 loss: 0.04529672861099243
Batch 63/64 loss: 0.03731304407119751
Batch 64/64 loss: 0.05501747131347656
Epoch 75  Train loss: 0.04230006161858054  Val loss: 0.07951223399630937
Saving best model, epoch: 75
Epoch 76
-------------------------------
Batch 1/64 loss: 0.048293471336364746
Batch 2/64 loss: 0.036800503730773926
Batch 3/64 loss: 0.030906617641448975
Batch 4/64 loss: 0.05054318904876709
Batch 5/64 loss: 0.051631927490234375
Batch 6/64 loss: 0.046469807624816895
Batch 7/64 loss: 0.05169564485549927
Batch 8/64 loss: 0.04946631193161011
Batch 9/64 loss: 0.06257975101470947
Batch 10/64 loss: 0.03328031301498413
Batch 11/64 loss: 0.037011921405792236
Batch 12/64 loss: 0.0505412220954895
Batch 13/64 loss: 0.04571479558944702
Batch 14/64 loss: 0.03740125894546509
Batch 15/64 loss: 0.04141080379486084
Batch 16/64 loss: 0.043054819107055664
Batch 17/64 loss: 0.043247222900390625
Batch 18/64 loss: 0.055541276931762695
Batch 19/64 loss: 0.0472831130027771
Batch 20/64 loss: 0.02452826499938965
Batch 21/64 loss: 0.028142035007476807
Batch 22/64 loss: 0.03345906734466553
Batch 23/64 loss: 0.043236613273620605
Batch 24/64 loss: 0.05376708507537842
Batch 25/64 loss: 0.04961264133453369
Batch 26/64 loss: 0.0472409725189209
Batch 27/64 loss: 0.04421412944793701
Batch 28/64 loss: 0.05835944414138794
Batch 29/64 loss: 0.028086960315704346
Batch 30/64 loss: 0.06303322315216064
Batch 31/64 loss: 0.027642369270324707
Batch 32/64 loss: 0.04393815994262695
Batch 33/64 loss: 0.04764431715011597
Batch 34/64 loss: 0.0452038049697876
Batch 35/64 loss: 0.03999674320220947
Batch 36/64 loss: 0.05345231294631958
Batch 37/64 loss: 0.019051790237426758
Batch 38/64 loss: 0.04905170202255249
Batch 39/64 loss: 0.046969592571258545
Batch 40/64 loss: 0.07010352611541748
Batch 41/64 loss: 0.0360720157623291
Batch 42/64 loss: 0.044134557247161865
Batch 43/64 loss: 0.022308290004730225
Batch 44/64 loss: 0.049598872661590576
Batch 45/64 loss: 0.025127410888671875
Batch 46/64 loss: 0.0467454195022583
Batch 47/64 loss: 0.03143727779388428
Batch 48/64 loss: 0.04842191934585571
Batch 49/64 loss: 0.02696239948272705
Batch 50/64 loss: 0.03595393896102905
Batch 51/64 loss: 0.02736443281173706
Batch 52/64 loss: 0.04250621795654297
Batch 53/64 loss: 0.035751163959503174
Batch 54/64 loss: 0.05530071258544922
Batch 55/64 loss: 0.032315731048583984
Batch 56/64 loss: 0.036766648292541504
Batch 57/64 loss: 0.04296189546585083
Batch 58/64 loss: 0.06575864553451538
Batch 59/64 loss: 0.04705536365509033
Batch 60/64 loss: 0.04641461372375488
Batch 61/64 loss: 0.035386741161346436
Batch 62/64 loss: 0.04169267416000366
Batch 63/64 loss: 0.05802345275878906
Batch 64/64 loss: 0.058825671672821045
Epoch 76  Train loss: 0.04325942549051023  Val loss: 0.08470475468848579
Epoch 77
-------------------------------
Batch 1/64 loss: 0.03045874834060669
Batch 2/64 loss: 0.0330241322517395
Batch 3/64 loss: 0.05602210760116577
Batch 4/64 loss: 0.045983850955963135
Batch 5/64 loss: 0.03198838233947754
Batch 6/64 loss: 0.04773300886154175
Batch 7/64 loss: 0.011180520057678223
Batch 8/64 loss: 0.04549521207809448
Batch 9/64 loss: 0.07285290956497192
Batch 10/64 loss: 0.01980668306350708
Batch 11/64 loss: 0.042014122009277344
Batch 12/64 loss: 0.037146568298339844
Batch 13/64 loss: 0.03096592426300049
Batch 14/64 loss: 0.037424564361572266
Batch 15/64 loss: 0.0626334547996521
Batch 16/64 loss: 0.03154975175857544
Batch 17/64 loss: 0.020658433437347412
Batch 18/64 loss: 0.04788714647293091
Batch 19/64 loss: 0.03602015972137451
Batch 20/64 loss: 0.039804816246032715
Batch 21/64 loss: 0.051757097244262695
Batch 22/64 loss: 0.032434165477752686
Batch 23/64 loss: 0.012737751007080078
Batch 24/64 loss: 0.0646810531616211
Batch 25/64 loss: 0.04305589199066162
Batch 26/64 loss: 0.03737145662307739
Batch 27/64 loss: 0.019839167594909668
Batch 28/64 loss: 0.06704723834991455
Batch 29/64 loss: 0.039285123348236084
Batch 30/64 loss: 0.04075223207473755
Batch 31/64 loss: 0.03162026405334473
Batch 32/64 loss: 0.05085045099258423
Batch 33/64 loss: 0.07007557153701782
Batch 34/64 loss: 0.018395721912384033
Batch 35/64 loss: 0.05046170949935913
Batch 36/64 loss: 0.0517311692237854
Batch 37/64 loss: 0.0385739803314209
Batch 38/64 loss: 0.033440470695495605
Batch 39/64 loss: 0.06342238187789917
Batch 40/64 loss: 0.01955181360244751
Batch 41/64 loss: 0.04537147283554077
Batch 42/64 loss: 0.03552979230880737
Batch 43/64 loss: 0.04491424560546875
Batch 44/64 loss: 0.029800891876220703
Batch 45/64 loss: 0.030765116214752197
Batch 46/64 loss: 0.05444753170013428
Batch 47/64 loss: 0.036397695541381836
Batch 48/64 loss: 0.02214103937149048
Batch 49/64 loss: 0.04498481750488281
Batch 50/64 loss: 0.06529843807220459
Batch 51/64 loss: 0.052288711071014404
Batch 52/64 loss: 0.05816704034805298
Batch 53/64 loss: 0.05044150352478027
Batch 54/64 loss: 0.034873247146606445
Batch 55/64 loss: 0.0344851016998291
Batch 56/64 loss: 0.049728989601135254
Batch 57/64 loss: 0.03920698165893555
Batch 58/64 loss: 0.032654643058776855
Batch 59/64 loss: 0.026362955570220947
Batch 60/64 loss: 0.041055500507354736
Batch 61/64 loss: 0.025776267051696777
Batch 62/64 loss: 0.06200838088989258
Batch 63/64 loss: 0.031692445278167725
Batch 64/64 loss: 0.015979290008544922
Epoch 77  Train loss: 0.04044098012587603  Val loss: 0.08145808374758848
Epoch 78
-------------------------------
Batch 1/64 loss: 0.03256803750991821
Batch 2/64 loss: 0.05127483606338501
Batch 3/64 loss: 0.03420710563659668
Batch 4/64 loss: 0.018490314483642578
Batch 5/64 loss: 0.06424093246459961
Batch 6/64 loss: 0.03948259353637695
Batch 7/64 loss: 0.03982645273208618
Batch 8/64 loss: 0.034355998039245605
Batch 9/64 loss: 0.02423882484436035
Batch 10/64 loss: 0.04191964864730835
Batch 11/64 loss: 0.02506864070892334
Batch 12/64 loss: 0.025341808795928955
Batch 13/64 loss: 0.060565292835235596
Batch 14/64 loss: 0.06278526782989502
Batch 15/64 loss: 0.03572624921798706
Batch 16/64 loss: 0.04254317283630371
Batch 17/64 loss: 0.06974786520004272
Batch 18/64 loss: 0.04997318983078003
Batch 19/64 loss: 0.016597211360931396
Batch 20/64 loss: 0.04020726680755615
Batch 21/64 loss: 0.02488774061203003
Batch 22/64 loss: 0.043251633644104004
Batch 23/64 loss: 0.03883570432662964
Batch 24/64 loss: 0.042826294898986816
Batch 25/64 loss: 0.04154294729232788
Batch 26/64 loss: 0.048088908195495605
Batch 27/64 loss: 0.06447529792785645
Batch 28/64 loss: 0.043934524059295654
Batch 29/64 loss: 0.02563774585723877
Batch 30/64 loss: 0.03203630447387695
Batch 31/64 loss: 0.029379725456237793
Batch 32/64 loss: 0.04271167516708374
Batch 33/64 loss: 0.0419582724571228
Batch 34/64 loss: 0.018098771572113037
Batch 35/64 loss: 0.02539646625518799
Batch 36/64 loss: 0.06956547498703003
Batch 37/64 loss: 0.03518718481063843
Batch 38/64 loss: 0.05847930908203125
Batch 39/64 loss: 0.04795408248901367
Batch 40/64 loss: 0.05448943376541138
Batch 41/64 loss: 0.05661118030548096
Batch 42/64 loss: 0.02994668483734131
Batch 43/64 loss: 0.04066741466522217
Batch 44/64 loss: 0.042867958545684814
Batch 45/64 loss: 0.04660487174987793
Batch 46/64 loss: 0.033793866634368896
Batch 47/64 loss: 0.03170132637023926
Batch 48/64 loss: 0.025871694087982178
Batch 49/64 loss: 0.05184054374694824
Batch 50/64 loss: 0.037118613719940186
Batch 51/64 loss: 0.033045828342437744
Batch 52/64 loss: 0.02980417013168335
Batch 53/64 loss: 0.051536738872528076
Batch 54/64 loss: 0.041885972023010254
Batch 55/64 loss: 0.03427565097808838
Batch 56/64 loss: 0.03794991970062256
Batch 57/64 loss: 0.036415934562683105
Batch 58/64 loss: 0.034681081771850586
Batch 59/64 loss: 0.04365336894989014
Batch 60/64 loss: 0.03270775079727173
Batch 61/64 loss: 0.039049386978149414
Batch 62/64 loss: 0.029716074466705322
Batch 63/64 loss: 0.03723543882369995
Batch 64/64 loss: 0.027221620082855225
Epoch 78  Train loss: 0.039800719887602566  Val loss: 0.0771257289496484
Saving best model, epoch: 78
Epoch 79
-------------------------------
Batch 1/64 loss: 0.05262380838394165
Batch 2/64 loss: 0.04036235809326172
Batch 3/64 loss: 0.05985915660858154
Batch 4/64 loss: 0.034558892250061035
Batch 5/64 loss: 0.01953744888305664
Batch 6/64 loss: 0.02601104974746704
Batch 7/64 loss: 0.030804932117462158
Batch 8/64 loss: 0.01640409231185913
Batch 9/64 loss: 0.014352798461914062
Batch 10/64 loss: 0.047161221504211426
Batch 11/64 loss: 0.02810359001159668
Batch 12/64 loss: 0.02997732162475586
Batch 13/64 loss: 0.009514808654785156
Batch 14/64 loss: 0.034174203872680664
Batch 15/64 loss: 0.023760437965393066
Batch 16/64 loss: 0.042099595069885254
Batch 17/64 loss: 0.034304022789001465
Batch 18/64 loss: 0.012733936309814453
Batch 19/64 loss: 0.04540139436721802
Batch 20/64 loss: 0.04329913854598999
Batch 21/64 loss: 0.053080737590789795
Batch 22/64 loss: 0.047028541564941406
Batch 23/64 loss: 0.04383838176727295
Batch 24/64 loss: 0.03495526313781738
Batch 25/64 loss: 0.029888451099395752
Batch 26/64 loss: 0.05428612232208252
Batch 27/64 loss: 0.041786909103393555
Batch 28/64 loss: 0.03458106517791748
Batch 29/64 loss: 0.020345568656921387
Batch 30/64 loss: 0.055634260177612305
Batch 31/64 loss: -0.0035553574562072754
Batch 32/64 loss: 0.03609490394592285
Batch 33/64 loss: 0.03823554515838623
Batch 34/64 loss: 0.03779172897338867
Batch 35/64 loss: 0.06627953052520752
Batch 36/64 loss: 0.03370261192321777
Batch 37/64 loss: 0.047778964042663574
Batch 38/64 loss: 0.08203685283660889
Batch 39/64 loss: 0.0446474552154541
Batch 40/64 loss: 0.04445457458496094
Batch 41/64 loss: 0.04535478353500366
Batch 42/64 loss: 0.023414552211761475
Batch 43/64 loss: 0.054072439670562744
Batch 44/64 loss: 0.040447235107421875
Batch 45/64 loss: 0.019449710845947266
Batch 46/64 loss: 0.03853046894073486
Batch 47/64 loss: 0.05137312412261963
Batch 48/64 loss: 0.03926360607147217
Batch 49/64 loss: 0.012193858623504639
Batch 50/64 loss: 0.09452372789382935
Batch 51/64 loss: 0.041723430156707764
Batch 52/64 loss: 0.05230015516281128
Batch 53/64 loss: 0.052016377449035645
Batch 54/64 loss: 0.02950948476791382
Batch 55/64 loss: 0.04683077335357666
Batch 56/64 loss: 0.046048521995544434
Batch 57/64 loss: 0.044273197650909424
Batch 58/64 loss: 0.02600342035293579
Batch 59/64 loss: 0.053519487380981445
Batch 60/64 loss: 0.04499673843383789
Batch 61/64 loss: 0.03333503007888794
Batch 62/64 loss: 0.029076576232910156
Batch 63/64 loss: 0.02743685245513916
Batch 64/64 loss: 0.05453085899353027
Epoch 79  Train loss: 0.038816125720155005  Val loss: 0.07586002370336212
Saving best model, epoch: 79
Epoch 80
-------------------------------
Batch 1/64 loss: 0.025488555431365967
Batch 2/64 loss: 0.02567809820175171
Batch 3/64 loss: 0.021285951137542725
Batch 4/64 loss: 0.03143119812011719
Batch 5/64 loss: 0.07510644197463989
Batch 6/64 loss: 0.02606302499771118
Batch 7/64 loss: 0.04417616128921509
Batch 8/64 loss: 0.031248033046722412
Batch 9/64 loss: 0.06154567003250122
Batch 10/64 loss: 0.017302334308624268
Batch 11/64 loss: 0.03176027536392212
Batch 12/64 loss: 0.04747885465621948
Batch 13/64 loss: 0.027100086212158203
Batch 14/64 loss: 0.04888343811035156
Batch 15/64 loss: 0.03951239585876465
Batch 16/64 loss: 0.022744357585906982
Batch 17/64 loss: 0.035198092460632324
Batch 18/64 loss: 0.04520237445831299
Batch 19/64 loss: 0.024371981620788574
Batch 20/64 loss: 0.02109050750732422
Batch 21/64 loss: 0.03811955451965332
Batch 22/64 loss: 0.036384522914886475
Batch 23/64 loss: 0.03828781843185425
Batch 24/64 loss: 0.029101431369781494
Batch 25/64 loss: 0.0406268835067749
Batch 26/64 loss: 0.04673582315444946
Batch 27/64 loss: 0.028857409954071045
Batch 28/64 loss: 0.027264118194580078
Batch 29/64 loss: 0.04913616180419922
Batch 30/64 loss: 0.02039414644241333
Batch 31/64 loss: 0.03872966766357422
Batch 32/64 loss: 0.06028240919113159
Batch 33/64 loss: 0.04136502742767334
Batch 34/64 loss: 0.032253921031951904
Batch 35/64 loss: 0.041537463665008545
Batch 36/64 loss: 0.021843135356903076
Batch 37/64 loss: 0.023573577404022217
Batch 38/64 loss: 0.01727062463760376
Batch 39/64 loss: 0.040928006172180176
Batch 40/64 loss: 0.02490401268005371
Batch 41/64 loss: 0.05376631021499634
Batch 42/64 loss: 0.06592410802841187
Batch 43/64 loss: 0.06183856725692749
Batch 44/64 loss: 0.04480689764022827
Batch 45/64 loss: 0.015934646129608154
Batch 46/64 loss: 0.01975804567337036
Batch 47/64 loss: 0.04051166772842407
Batch 48/64 loss: 0.05916249752044678
Batch 49/64 loss: 0.026685118675231934
Batch 50/64 loss: 0.04849362373352051
Batch 51/64 loss: 0.03962075710296631
Batch 52/64 loss: 0.030983567237854004
Batch 53/64 loss: 0.03865385055541992
Batch 54/64 loss: 0.027537524700164795
Batch 55/64 loss: 0.031173110008239746
Batch 56/64 loss: 0.021373093128204346
Batch 57/64 loss: 0.05672484636306763
Batch 58/64 loss: 0.023535728454589844
Batch 59/64 loss: 0.047776639461517334
Batch 60/64 loss: 0.06843924522399902
Batch 61/64 loss: 0.07318419218063354
Batch 62/64 loss: 0.020917415618896484
Batch 63/64 loss: 0.02586972713470459
Batch 64/64 loss: 0.03428763151168823
Epoch 80  Train loss: 0.037155301196902406  Val loss: 0.07092245296923975
Saving best model, epoch: 80
Epoch 81
-------------------------------
Batch 1/64 loss: 0.031205475330352783
Batch 2/64 loss: 0.057004690170288086
Batch 3/64 loss: 0.025104403495788574
Batch 4/64 loss: 0.049448490142822266
Batch 5/64 loss: 0.04105353355407715
Batch 6/64 loss: 0.014685273170471191
Batch 7/64 loss: 0.06263077259063721
Batch 8/64 loss: 0.03194129467010498
Batch 9/64 loss: 0.06661200523376465
Batch 10/64 loss: 0.021497607231140137
Batch 11/64 loss: 0.03491693735122681
Batch 12/64 loss: 0.011575877666473389
Batch 13/64 loss: 0.03853476047515869
Batch 14/64 loss: 0.036150455474853516
Batch 15/64 loss: 0.031193971633911133
Batch 16/64 loss: 0.02181488275527954
Batch 17/64 loss: 0.03778254985809326
Batch 18/64 loss: 0.021719515323638916
Batch 19/64 loss: 0.016490161418914795
Batch 20/64 loss: 0.026553034782409668
Batch 21/64 loss: 0.042549967765808105
Batch 22/64 loss: 0.026152074337005615
Batch 23/64 loss: 0.020495355129241943
Batch 24/64 loss: 0.028554141521453857
Batch 25/64 loss: 0.035153090953826904
Batch 26/64 loss: 0.01862466335296631
Batch 27/64 loss: 0.03173726797103882
Batch 28/64 loss: 0.01561272144317627
Batch 29/64 loss: 0.0405268669128418
Batch 30/64 loss: 0.04789614677429199
Batch 31/64 loss: 0.04000598192214966
Batch 32/64 loss: 0.018084168434143066
Batch 33/64 loss: 0.014566898345947266
Batch 34/64 loss: 0.023972153663635254
Batch 35/64 loss: 0.014316856861114502
Batch 36/64 loss: 0.011957406997680664
Batch 37/64 loss: 0.04890596866607666
Batch 38/64 loss: 0.023433148860931396
Batch 39/64 loss: 0.04175400733947754
Batch 40/64 loss: 0.03909492492675781
Batch 41/64 loss: 0.05618351697921753
Batch 42/64 loss: 0.03281450271606445
Batch 43/64 loss: 0.039983391761779785
Batch 44/64 loss: 0.030094146728515625
Batch 45/64 loss: 0.04275047779083252
Batch 46/64 loss: 0.037844717502593994
Batch 47/64 loss: 0.040985822677612305
Batch 48/64 loss: 0.023890316486358643
Batch 49/64 loss: 0.03769087791442871
Batch 50/64 loss: 0.020154595375061035
Batch 51/64 loss: 0.04217350482940674
Batch 52/64 loss: 0.036636412143707275
Batch 53/64 loss: 0.07095545530319214
Batch 54/64 loss: 0.021630704402923584
Batch 55/64 loss: 0.024871468544006348
Batch 56/64 loss: 0.014683246612548828
Batch 57/64 loss: 0.034341514110565186
Batch 58/64 loss: 0.04227447509765625
Batch 59/64 loss: 0.035890281200408936
Batch 60/64 loss: 0.03341364860534668
Batch 61/64 loss: 0.03386157751083374
Batch 62/64 loss: 0.049006879329681396
Batch 63/64 loss: 0.04987090826034546
Batch 64/64 loss: 0.03468680381774902
Epoch 81  Train loss: 0.033558071360868566  Val loss: 0.07568779167850402
Epoch 82
-------------------------------
Batch 1/64 loss: 0.0076746344566345215
Batch 2/64 loss: 0.03231477737426758
Batch 3/64 loss: 0.016416966915130615
Batch 4/64 loss: 0.03518259525299072
Batch 5/64 loss: 0.03513944149017334
Batch 6/64 loss: 0.03346902132034302
Batch 7/64 loss: 0.028611421585083008
Batch 8/64 loss: 0.019045710563659668
Batch 9/64 loss: 0.02060532569885254
Batch 10/64 loss: 0.037825703620910645
Batch 11/64 loss: 0.020319879055023193
Batch 12/64 loss: 0.060063302516937256
Batch 13/64 loss: 0.049021363258361816
Batch 14/64 loss: 0.0194857120513916
Batch 15/64 loss: 0.010786950588226318
Batch 16/64 loss: 0.04763615131378174
Batch 17/64 loss: 0.019235193729400635
Batch 18/64 loss: 0.012792885303497314
Batch 19/64 loss: 0.059669435024261475
Batch 20/64 loss: 0.03625082969665527
Batch 21/64 loss: 0.046463072299957275
Batch 22/64 loss: 0.06641215085983276
Batch 23/64 loss: 0.03191101551055908
Batch 24/64 loss: 0.03006458282470703
Batch 25/64 loss: 0.06483983993530273
Batch 26/64 loss: 0.0277712345123291
Batch 27/64 loss: 0.03871893882751465
Batch 28/64 loss: 0.030820906162261963
Batch 29/64 loss: 0.025084733963012695
Batch 30/64 loss: 0.048236966133117676
Batch 31/64 loss: 0.008554220199584961
Batch 32/64 loss: 0.03689306974411011
Batch 33/64 loss: 0.05634850263595581
Batch 34/64 loss: 0.031183302402496338
Batch 35/64 loss: 0.02485901117324829
Batch 36/64 loss: 0.0303269624710083
Batch 37/64 loss: 0.022032439708709717
Batch 38/64 loss: 0.03994697332382202
Batch 39/64 loss: 0.029993414878845215
Batch 40/64 loss: 0.03339827060699463
Batch 41/64 loss: 0.06120002269744873
Batch 42/64 loss: 0.033809006214141846
Batch 43/64 loss: 0.04418182373046875
Batch 44/64 loss: 0.07263165712356567
Batch 45/64 loss: 0.027450621128082275
Batch 46/64 loss: 0.037501394748687744
Batch 47/64 loss: 0.017044425010681152
Batch 48/64 loss: 0.042802393436431885
Batch 49/64 loss: 0.015095293521881104
Batch 50/64 loss: 0.03871166706085205
Batch 51/64 loss: 0.030914783477783203
Batch 52/64 loss: 0.038571953773498535
Batch 53/64 loss: 0.044422030448913574
Batch 54/64 loss: 0.04156088829040527
Batch 55/64 loss: 0.046036481857299805
Batch 56/64 loss: 0.04484361410140991
Batch 57/64 loss: 0.027626514434814453
Batch 58/64 loss: 0.04632312059402466
Batch 59/64 loss: 0.04366040229797363
Batch 60/64 loss: 0.02488476037979126
Batch 61/64 loss: 0.026123106479644775
Batch 62/64 loss: 0.03332781791687012
Batch 63/64 loss: 0.01947098970413208
Batch 64/64 loss: 0.044507503509521484
Epoch 82  Train loss: 0.034776193020390526  Val loss: 0.07544138853492606
Epoch 83
-------------------------------
Batch 1/64 loss: 0.05621802806854248
Batch 2/64 loss: 0.034027099609375
Batch 3/64 loss: 0.04318517446517944
Batch 4/64 loss: 0.017898619174957275
Batch 5/64 loss: 0.03964400291442871
Batch 6/64 loss: 0.04113250970840454
Batch 7/64 loss: 0.01495516300201416
Batch 8/64 loss: 0.013673663139343262
Batch 9/64 loss: 0.047465503215789795
Batch 10/64 loss: 0.01969432830810547
Batch 11/64 loss: 0.01703101396560669
Batch 12/64 loss: 0.032608628273010254
Batch 13/64 loss: 0.025956332683563232
Batch 14/64 loss: 0.03820878267288208
Batch 15/64 loss: 0.029599547386169434
Batch 16/64 loss: 0.06617206335067749
Batch 17/64 loss: 0.0452113151550293
Batch 18/64 loss: 0.009875059127807617
Batch 19/64 loss: 0.02797800302505493
Batch 20/64 loss: 0.02374410629272461
Batch 21/64 loss: 0.011143147945404053
Batch 22/64 loss: 0.03322339057922363
Batch 23/64 loss: 0.0423160195350647
Batch 24/64 loss: 0.04276132583618164
Batch 25/64 loss: 0.05734598636627197
Batch 26/64 loss: 0.03674036264419556
Batch 27/64 loss: 0.010438680648803711
Batch 28/64 loss: 0.01736307144165039
Batch 29/64 loss: 0.014713048934936523
Batch 30/64 loss: 0.0338020920753479
Batch 31/64 loss: 0.029986262321472168
Batch 32/64 loss: 0.04741102457046509
Batch 33/64 loss: 0.02924060821533203
Batch 34/64 loss: 0.04078710079193115
Batch 35/64 loss: 0.020352661609649658
Batch 36/64 loss: 0.05288076400756836
Batch 37/64 loss: 0.031543612480163574
Batch 38/64 loss: 0.024712085723876953
Batch 39/64 loss: 0.0251997709274292
Batch 40/64 loss: 0.03132760524749756
Batch 41/64 loss: 0.02953171730041504
Batch 42/64 loss: 0.06230229139328003
Batch 43/64 loss: 0.018442988395690918
Batch 44/64 loss: 0.024885892868041992
Batch 45/64 loss: 0.033408522605895996
Batch 46/64 loss: 0.04194331169128418
Batch 47/64 loss: 0.038828134536743164
Batch 48/64 loss: 0.041781485080718994
Batch 49/64 loss: 0.06274628639221191
Batch 50/64 loss: 0.05738425254821777
Batch 51/64 loss: 0.02514350414276123
Batch 52/64 loss: 0.04981589317321777
Batch 53/64 loss: 0.04423022270202637
Batch 54/64 loss: 0.03224623203277588
Batch 55/64 loss: 0.021275877952575684
Batch 56/64 loss: 0.03106522560119629
Batch 57/64 loss: 0.04177647829055786
Batch 58/64 loss: 0.027595102787017822
Batch 59/64 loss: 0.05127990245819092
Batch 60/64 loss: 0.06450152397155762
Batch 61/64 loss: 0.026606082916259766
Batch 62/64 loss: 0.04215419292449951
Batch 63/64 loss: 0.05333667993545532
Batch 64/64 loss: 0.015350520610809326
Epoch 83  Train loss: 0.034688035413330674  Val loss: 0.07082107017949685
Saving best model, epoch: 83
Epoch 84
-------------------------------
Batch 1/64 loss: 0.04435157775878906
Batch 2/64 loss: 0.018463850021362305
Batch 3/64 loss: 0.02954709529876709
Batch 4/64 loss: 0.04067838191986084
Batch 5/64 loss: 0.029497802257537842
Batch 6/64 loss: 0.012117087841033936
Batch 7/64 loss: 0.01845550537109375
Batch 8/64 loss: 0.04859006404876709
Batch 9/64 loss: 0.030317187309265137
Batch 10/64 loss: 0.027047038078308105
Batch 11/64 loss: 0.030072927474975586
Batch 12/64 loss: 0.029343128204345703
Batch 13/64 loss: 0.027599215507507324
Batch 14/64 loss: 0.029737651348114014
Batch 15/64 loss: 0.03618699312210083
Batch 16/64 loss: 0.03687411546707153
Batch 17/64 loss: 0.027997255325317383
Batch 18/64 loss: 0.05737870931625366
Batch 19/64 loss: 0.010308921337127686
Batch 20/64 loss: 0.021661996841430664
Batch 21/64 loss: 0.04768425226211548
Batch 22/64 loss: 0.010895788669586182
Batch 23/64 loss: 0.020702004432678223
Batch 24/64 loss: 0.027244091033935547
Batch 25/64 loss: 0.03899437189102173
Batch 26/64 loss: 0.010664761066436768
Batch 27/64 loss: 0.013557255268096924
Batch 28/64 loss: 0.038419246673583984
Batch 29/64 loss: 0.034500956535339355
Batch 30/64 loss: 0.0430791974067688
Batch 31/64 loss: 0.005008816719055176
Batch 32/64 loss: 0.027886927127838135
Batch 33/64 loss: 0.028081536293029785
Batch 34/64 loss: 0.04229158163070679
Batch 35/64 loss: 0.039566755294799805
Batch 36/64 loss: 0.006322324275970459
Batch 37/64 loss: 0.04298955202102661
Batch 38/64 loss: 0.04172360897064209
Batch 39/64 loss: 0.0343778133392334
Batch 40/64 loss: 0.0322878360748291
Batch 41/64 loss: 0.057769954204559326
Batch 42/64 loss: 0.03501826524734497
Batch 43/64 loss: 0.022441446781158447
Batch 44/64 loss: 0.012078404426574707
Batch 45/64 loss: 0.03622347116470337
Batch 46/64 loss: 0.06159466505050659
Batch 47/64 loss: 0.03918945789337158
Batch 48/64 loss: 0.0438687801361084
Batch 49/64 loss: 0.015745460987091064
Batch 50/64 loss: 0.017369866371154785
Batch 51/64 loss: 0.028464019298553467
Batch 52/64 loss: 0.012071371078491211
Batch 53/64 loss: 0.019363999366760254
Batch 54/64 loss: 0.019872009754180908
Batch 55/64 loss: 0.017707228660583496
Batch 56/64 loss: 0.04864919185638428
Batch 57/64 loss: 0.04961961507797241
Batch 58/64 loss: 0.06535464525222778
Batch 59/64 loss: 0.03449404239654541
Batch 60/64 loss: 0.024951577186584473
Batch 61/64 loss: 0.04234546422958374
Batch 62/64 loss: 0.02068197727203369
Batch 63/64 loss: 0.027741074562072754
Batch 64/64 loss: 0.027380704879760742
Epoch 84  Train loss: 0.03083383055294261  Val loss: 0.06715270620850763
Saving best model, epoch: 84
Epoch 85
-------------------------------
Batch 1/64 loss: 0.026221752166748047
Batch 2/64 loss: 0.012506961822509766
Batch 3/64 loss: 0.023475170135498047
Batch 4/64 loss: 0.03044956922531128
Batch 5/64 loss: 0.03217208385467529
Batch 6/64 loss: 0.003913223743438721
Batch 7/64 loss: 0.019244909286499023
Batch 8/64 loss: 0.02615731954574585
Batch 9/64 loss: 0.017908573150634766
Batch 10/64 loss: 0.015385150909423828
Batch 11/64 loss: 0.024371027946472168
Batch 12/64 loss: 0.032971203327178955
Batch 13/64 loss: 0.0288161039352417
Batch 14/64 loss: 0.07411313056945801
Batch 15/64 loss: 0.023371994495391846
Batch 16/64 loss: 0.03722488880157471
Batch 17/64 loss: 0.03797847032546997
Batch 18/64 loss: 0.006011903285980225
Batch 19/64 loss: 0.029600679874420166
Batch 20/64 loss: 0.012694835662841797
Batch 21/64 loss: 0.03444790840148926
Batch 22/64 loss: 0.005401551723480225
Batch 23/64 loss: 0.014141678810119629
Batch 24/64 loss: 0.01847785711288452
Batch 25/64 loss: 0.04911959171295166
Batch 26/64 loss: 0.032894015312194824
Batch 27/64 loss: 0.030468404293060303
Batch 28/64 loss: 0.060441017150878906
Batch 29/64 loss: 0.01142340898513794
Batch 30/64 loss: 0.03842341899871826
Batch 31/64 loss: 0.0067212581634521484
Batch 32/64 loss: 0.05665695667266846
Batch 33/64 loss: 0.02648162841796875
Batch 34/64 loss: 0.0561596155166626
Batch 35/64 loss: 0.06268548965454102
Batch 36/64 loss: 0.02250361442565918
Batch 37/64 loss: 0.019635796546936035
Batch 38/64 loss: 0.015426576137542725
Batch 39/64 loss: 0.027140498161315918
Batch 40/64 loss: 0.017234444618225098
Batch 41/64 loss: 0.034635722637176514
Batch 42/64 loss: 0.019505679607391357
Batch 43/64 loss: 0.041790544986724854
Batch 44/64 loss: 0.025208592414855957
Batch 45/64 loss: 0.007681012153625488
Batch 46/64 loss: 0.038020551204681396
Batch 47/64 loss: 0.028755247592926025
Batch 48/64 loss: 0.02203524112701416
Batch 49/64 loss: 0.03497737646102905
Batch 50/64 loss: 0.06030845642089844
Batch 51/64 loss: 0.04972183704376221
Batch 52/64 loss: 0.03992760181427002
Batch 53/64 loss: 0.04533308744430542
Batch 54/64 loss: 0.024810791015625
Batch 55/64 loss: 0.029999852180480957
Batch 56/64 loss: 0.04681837558746338
Batch 57/64 loss: 0.026270687580108643
Batch 58/64 loss: 0.04277372360229492
Batch 59/64 loss: 0.028464913368225098
Batch 60/64 loss: 0.018835008144378662
Batch 61/64 loss: 0.015317261219024658
Batch 62/64 loss: 0.020736515522003174
Batch 63/64 loss: 0.026597976684570312
Batch 64/64 loss: -0.0008920431137084961
Epoch 85  Train loss: 0.028993422844830682  Val loss: 0.06638652926048462
Saving best model, epoch: 85
Epoch 86
-------------------------------
Batch 1/64 loss: 0.007854938507080078
Batch 2/64 loss: 0.03024667501449585
Batch 3/64 loss: 0.021213948726654053
Batch 4/64 loss: 0.005365252494812012
Batch 5/64 loss: 0.059046268463134766
Batch 6/64 loss: 0.036324262619018555
Batch 7/64 loss: 0.028363585472106934
Batch 8/64 loss: 0.002669394016265869
Batch 9/64 loss: 0.020453810691833496
Batch 10/64 loss: 0.030713438987731934
Batch 11/64 loss: 0.044054508209228516
Batch 12/64 loss: 0.01724386215209961
Batch 13/64 loss: 0.04906076192855835
Batch 14/64 loss: 0.03400290012359619
Batch 15/64 loss: 0.033176302909851074
Batch 16/64 loss: 0.01863110065460205
Batch 17/64 loss: 0.01612168550491333
Batch 18/64 loss: 0.047309815883636475
Batch 19/64 loss: 0.016479551792144775
Batch 20/64 loss: 0.055770039558410645
Batch 21/64 loss: 0.01874983310699463
Batch 22/64 loss: 0.010816454887390137
Batch 23/64 loss: 0.04741746187210083
Batch 24/64 loss: 0.04093366861343384
Batch 25/64 loss: 0.020013153553009033
Batch 26/64 loss: 0.06214094161987305
Batch 27/64 loss: 0.032136380672454834
Batch 28/64 loss: 0.01597416400909424
Batch 29/64 loss: 0.029182851314544678
Batch 30/64 loss: 0.02240431308746338
Batch 31/64 loss: 0.006721794605255127
Batch 32/64 loss: 0.019079267978668213
Batch 33/64 loss: 0.05516326427459717
Batch 34/64 loss: 0.05986583232879639
Batch 35/64 loss: 0.014824628829956055
Batch 36/64 loss: 0.012182354927062988
Batch 37/64 loss: 0.02019214630126953
Batch 38/64 loss: 0.024337291717529297
Batch 39/64 loss: 0.032941222190856934
Batch 40/64 loss: 0.030675649642944336
Batch 41/64 loss: 0.019116520881652832
Batch 42/64 loss: 0.0040683746337890625
Batch 43/64 loss: 0.0172806978225708
Batch 44/64 loss: 0.031400322914123535
Batch 45/64 loss: 0.03756725788116455
Batch 46/64 loss: 0.027868211269378662
Batch 47/64 loss: 0.010427713394165039
Batch 48/64 loss: 0.03065711259841919
Batch 49/64 loss: 0.024975955486297607
Batch 50/64 loss: 0.046454429626464844
Batch 51/64 loss: 0.026276767253875732
Batch 52/64 loss: 0.029072165489196777
Batch 53/64 loss: 0.031544625759124756
Batch 54/64 loss: 0.015020608901977539
Batch 55/64 loss: 0.024393796920776367
Batch 56/64 loss: 0.046202898025512695
Batch 57/64 loss: 0.03779369592666626
Batch 58/64 loss: 0.04943704605102539
Batch 59/64 loss: 0.019570350646972656
Batch 60/64 loss: 0.01616913080215454
Batch 61/64 loss: 0.04669886827468872
Batch 62/64 loss: 0.038817644119262695
Batch 63/64 loss: 0.04203200340270996
Batch 64/64 loss: 0.016355693340301514
Epoch 86  Train loss: 0.028783807801265344  Val loss: 0.06950989375819046
Epoch 87
-------------------------------
Batch 1/64 loss: 0.01436305046081543
Batch 2/64 loss: 0.022388160228729248
Batch 3/64 loss: 0.014904558658599854
Batch 4/64 loss: 0.019060492515563965
Batch 5/64 loss: 0.022815406322479248
Batch 6/64 loss: 0.04314905405044556
Batch 7/64 loss: 0.019604861736297607
Batch 8/64 loss: 0.024715840816497803
Batch 9/64 loss: 0.010506391525268555
Batch 10/64 loss: 0.05095362663269043
Batch 11/64 loss: 0.03594076633453369
Batch 12/64 loss: 0.03770279884338379
Batch 13/64 loss: 0.030938386917114258
Batch 14/64 loss: 0.03393930196762085
Batch 15/64 loss: 0.02031153440475464
Batch 16/64 loss: 0.03287553787231445
Batch 17/64 loss: 0.029229581356048584
Batch 18/64 loss: 0.037133991718292236
Batch 19/64 loss: 0.027150332927703857
Batch 20/64 loss: 0.04137825965881348
Batch 21/64 loss: 0.03677850961685181
Batch 22/64 loss: 0.020839989185333252
Batch 23/64 loss: 0.013566136360168457
Batch 24/64 loss: 0.021411418914794922
Batch 25/64 loss: 0.015898823738098145
Batch 26/64 loss: 0.018653392791748047
Batch 27/64 loss: 0.03441929817199707
Batch 28/64 loss: 0.05094945430755615
Batch 29/64 loss: 0.02203667163848877
Batch 30/64 loss: 0.007035374641418457
Batch 31/64 loss: 0.04220271110534668
Batch 32/64 loss: 0.014789700508117676
Batch 33/64 loss: 0.012903213500976562
Batch 34/64 loss: 0.017166733741760254
Batch 35/64 loss: 0.028979897499084473
Batch 36/64 loss: 0.02426755428314209
Batch 37/64 loss: 0.019496917724609375
Batch 38/64 loss: 0.024038374423980713
Batch 39/64 loss: 0.028905034065246582
Batch 40/64 loss: 0.01416856050491333
Batch 41/64 loss: 0.03372544050216675
Batch 42/64 loss: 0.045784831047058105
Batch 43/64 loss: 0.0077558159828186035
Batch 44/64 loss: 0.04549205303192139
Batch 45/64 loss: 0.020041227340698242
Batch 46/64 loss: 0.02293097972869873
Batch 47/64 loss: 0.04163402318954468
Batch 48/64 loss: 0.033416152000427246
Batch 49/64 loss: 0.039894819259643555
Batch 50/64 loss: 0.022336840629577637
Batch 51/64 loss: 0.008654952049255371
Batch 52/64 loss: 0.0474819540977478
Batch 53/64 loss: 0.03249853849411011
Batch 54/64 loss: 0.04498523473739624
Batch 55/64 loss: 0.012838602066040039
Batch 56/64 loss: 0.027350902557373047
Batch 57/64 loss: 0.0058942437171936035
Batch 58/64 loss: 0.03892403841018677
Batch 59/64 loss: 0.055889904499053955
Batch 60/64 loss: 0.031655967235565186
Batch 61/64 loss: 0.03184574842453003
Batch 62/64 loss: 0.015923738479614258
Batch 63/64 loss: 0.048709988594055176
Batch 64/64 loss: 0.008513092994689941
Epoch 87  Train loss: 0.027601890470467363  Val loss: 0.07185096101662547
Epoch 88
-------------------------------
Batch 1/64 loss: 0.031190335750579834
Batch 2/64 loss: 0.03794217109680176
Batch 3/64 loss: 0.032885611057281494
Batch 4/64 loss: 0.040430545806884766
Batch 5/64 loss: 0.03848999738693237
Batch 6/64 loss: 0.040899813175201416
Batch 7/64 loss: 0.03902095556259155
Batch 8/64 loss: 0.026891052722930908
Batch 9/64 loss: 0.021213233470916748
Batch 10/64 loss: 0.022384047508239746
Batch 11/64 loss: 0.029282569885253906
Batch 12/64 loss: 0.013040661811828613
Batch 13/64 loss: 0.028140366077423096
Batch 14/64 loss: 0.024611234664916992
Batch 15/64 loss: 0.012556076049804688
Batch 16/64 loss: 0.024295449256896973
Batch 17/64 loss: 0.014408230781555176
Batch 18/64 loss: 0.06260192394256592
Batch 19/64 loss: 0.02363145351409912
Batch 20/64 loss: 0.021450519561767578
Batch 21/64 loss: 0.0322575569152832
Batch 22/64 loss: 0.02213984727859497
Batch 23/64 loss: 0.03466641902923584
Batch 24/64 loss: 0.03028714656829834
Batch 25/64 loss: 0.013301372528076172
Batch 26/64 loss: 0.019324898719787598
Batch 27/64 loss: 0.057476699352264404
Batch 28/64 loss: 0.03663212060928345
Batch 29/64 loss: 0.0318225622177124
Batch 30/64 loss: 0.020669639110565186
Batch 31/64 loss: 0.0397799015045166
Batch 32/64 loss: 0.03256118297576904
Batch 33/64 loss: 0.02597755193710327
Batch 34/64 loss: 0.03368043899536133
Batch 35/64 loss: 0.016165614128112793
Batch 36/64 loss: 0.02903449535369873
Batch 37/64 loss: 0.027028203010559082
Batch 38/64 loss: 0.020843148231506348
Batch 39/64 loss: 0.01100313663482666
Batch 40/64 loss: 0.027131736278533936
Batch 41/64 loss: 0.01432710886001587
Batch 42/64 loss: 0.02936452627182007
Batch 43/64 loss: 0.04033529758453369
Batch 44/64 loss: 0.023280203342437744
Batch 45/64 loss: 0.02752065658569336
Batch 46/64 loss: 0.021058499813079834
Batch 47/64 loss: 0.030720233917236328
Batch 48/64 loss: 0.02517879009246826
Batch 49/64 loss: 0.008508145809173584
Batch 50/64 loss: 0.019198954105377197
Batch 51/64 loss: 0.04060572385787964
Batch 52/64 loss: 0.047299087047576904
Batch 53/64 loss: 0.015833377838134766
Batch 54/64 loss: 0.04576241970062256
Batch 55/64 loss: 0.019167065620422363
Batch 56/64 loss: 0.04068589210510254
Batch 57/64 loss: 0.02998119592666626
Batch 58/64 loss: 0.02099233865737915
Batch 59/64 loss: 0.014575004577636719
Batch 60/64 loss: 0.041466712951660156
Batch 61/64 loss: 0.035018324851989746
Batch 62/64 loss: 0.01701796054840088
Batch 63/64 loss: 0.03660023212432861
Batch 64/64 loss: 0.02919715642929077
Epoch 88  Train loss: 0.02844777317608104  Val loss: 0.07037317117874566
Epoch 89
-------------------------------
Batch 1/64 loss: 0.01972353458404541
Batch 2/64 loss: 0.05091142654418945
Batch 3/64 loss: 0.036742448806762695
Batch 4/64 loss: 0.027693867683410645
Batch 5/64 loss: 0.015102267265319824
Batch 6/64 loss: 0.034158945083618164
Batch 7/64 loss: 0.025680720806121826
Batch 8/64 loss: 0.02142965793609619
Batch 9/64 loss: 0.0335729718208313
Batch 10/64 loss: 0.035399436950683594
Batch 11/64 loss: 0.015448808670043945
Batch 12/64 loss: 0.04139739274978638
Batch 13/64 loss: 0.028023600578308105
Batch 14/64 loss: 0.0373075008392334
Batch 15/64 loss: 0.005814075469970703
Batch 16/64 loss: 0.0034937262535095215
Batch 17/64 loss: 0.02556246519088745
Batch 18/64 loss: 0.002135753631591797
Batch 19/64 loss: 0.008206725120544434
Batch 20/64 loss: 0.03982734680175781
Batch 21/64 loss: 0.011931777000427246
Batch 22/64 loss: 0.06311553716659546
Batch 23/64 loss: 0.013237357139587402
Batch 24/64 loss: 0.07228732109069824
Batch 25/64 loss: 0.021437406539916992
Batch 26/64 loss: 0.06172281503677368
Batch 27/64 loss: 0.023210644721984863
Batch 28/64 loss: 0.040186166763305664
Batch 29/64 loss: 0.025521039962768555
Batch 30/64 loss: 0.05382895469665527
Batch 31/64 loss: 0.02748173475265503
Batch 32/64 loss: 0.05510920286178589
Batch 33/64 loss: 0.02151811122894287
Batch 34/64 loss: 0.03674793243408203
Batch 35/64 loss: 0.024055063724517822
Batch 36/64 loss: 0.01655888557434082
Batch 37/64 loss: 0.017996907234191895
Batch 38/64 loss: 0.026734352111816406
Batch 39/64 loss: -0.000256955623626709
Batch 40/64 loss: 0.021112442016601562
Batch 41/64 loss: 0.02071923017501831
Batch 42/64 loss: 0.010606646537780762
Batch 43/64 loss: 0.012256860733032227
Batch 44/64 loss: 0.016300976276397705
Batch 45/64 loss: 0.018791913986206055
Batch 46/64 loss: 0.028960108757019043
Batch 47/64 loss: 0.03262460231781006
Batch 48/64 loss: 0.027892649173736572
Batch 49/64 loss: 0.006123960018157959
Batch 50/64 loss: 0.010308384895324707
Batch 51/64 loss: 0.03303098678588867
Batch 52/64 loss: 0.06061208248138428
Batch 53/64 loss: 0.04017162322998047
Batch 54/64 loss: 0.04227977991104126
Batch 55/64 loss: 0.021575927734375
Batch 56/64 loss: 0.018164336681365967
Batch 57/64 loss: 0.03109961748123169
Batch 58/64 loss: 0.007417500019073486
Batch 59/64 loss: 0.006381630897521973
Batch 60/64 loss: 0.021592676639556885
Batch 61/64 loss: 0.04640930891036987
Batch 62/64 loss: 0.010611534118652344
Batch 63/64 loss: 0.02158135175704956
Batch 64/64 loss: 0.02469426393508911
Epoch 89  Train loss: 0.026748294222588634  Val loss: 0.06684894025120948
Epoch 90
-------------------------------
Batch 1/64 loss: 0.016994714736938477
Batch 2/64 loss: 0.0253143310546875
Batch 3/64 loss: 0.03916114568710327
Batch 4/64 loss: 0.020816266536712646
Batch 5/64 loss: 0.03938549757003784
Batch 6/64 loss: 0.03948509693145752
Batch 7/64 loss: 0.016454637050628662
Batch 8/64 loss: 0.01806032657623291
Batch 9/64 loss: 0.008682072162628174
Batch 10/64 loss: 0.030958175659179688
Batch 11/64 loss: 0.04755127429962158
Batch 12/64 loss: 0.03336745500564575
Batch 13/64 loss: 0.03611069917678833
Batch 14/64 loss: 0.029596984386444092
Batch 15/64 loss: 0.024798035621643066
Batch 16/64 loss: 0.028444528579711914
Batch 17/64 loss: 0.028798818588256836
Batch 18/64 loss: 0.016283392906188965
Batch 19/64 loss: 0.043111324310302734
Batch 20/64 loss: 0.02684628963470459
Batch 21/64 loss: -0.003294527530670166
Batch 22/64 loss: 0.006667912006378174
Batch 23/64 loss: 0.026181280612945557
Batch 24/64 loss: 0.03921210765838623
Batch 25/64 loss: 0.02421635389328003
Batch 26/64 loss: 0.028887808322906494
Batch 27/64 loss: 0.013764917850494385
Batch 28/64 loss: 0.03380417823791504
Batch 29/64 loss: 0.03448575735092163
Batch 30/64 loss: 0.022648632526397705
Batch 31/64 loss: 0.016196608543395996
Batch 32/64 loss: 0.02360546588897705
Batch 33/64 loss: 0.011635959148406982
Batch 34/64 loss: 0.0032761096954345703
Batch 35/64 loss: 0.03933060169219971
Batch 36/64 loss: 0.023076236248016357
Batch 37/64 loss: 0.02428227663040161
Batch 38/64 loss: 0.020824730396270752
Batch 39/64 loss: 0.012713313102722168
Batch 40/64 loss: 0.014772891998291016
Batch 41/64 loss: 0.01360166072845459
Batch 42/64 loss: 0.041157066822052
Batch 43/64 loss: 0.05799424648284912
Batch 44/64 loss: 0.022700488567352295
Batch 45/64 loss: 0.03387993574142456
Batch 46/64 loss: 0.02732408046722412
Batch 47/64 loss: 0.0015597939491271973
Batch 48/64 loss: 0.015364527702331543
Batch 49/64 loss: 0.02939695119857788
Batch 50/64 loss: 0.03493452072143555
Batch 51/64 loss: 0.01938915252685547
Batch 52/64 loss: 0.04020357131958008
Batch 53/64 loss: 0.025548458099365234
Batch 54/64 loss: 0.036536455154418945
Batch 55/64 loss: 0.02673470973968506
Batch 56/64 loss: 0.027839481830596924
Batch 57/64 loss: 0.036526620388031006
Batch 58/64 loss: 0.04140084981918335
Batch 59/64 loss: 0.05294930934906006
Batch 60/64 loss: 0.03184330463409424
Batch 61/64 loss: 0.013177812099456787
Batch 62/64 loss: 0.04064756631851196
Batch 63/64 loss: 0.011548519134521484
Batch 64/64 loss: 0.03127443790435791
Epoch 90  Train loss: 0.026544699481889315  Val loss: 0.06792610699368506
Epoch 91
-------------------------------
Batch 1/64 loss: 0.03710734844207764
Batch 2/64 loss: 0.053648948669433594
Batch 3/64 loss: 0.035732924938201904
Batch 4/64 loss: 0.030107080936431885
Batch 5/64 loss: 0.0392490029335022
Batch 6/64 loss: 0.011289060115814209
Batch 7/64 loss: 0.02608811855316162
Batch 8/64 loss: 0.028616368770599365
Batch 9/64 loss: 0.021220028400421143
Batch 10/64 loss: 0.023413538932800293
Batch 11/64 loss: 0.051899492740631104
Batch 12/64 loss: 0.04183143377304077
Batch 13/64 loss: 0.024324357509613037
Batch 14/64 loss: 0.0490109920501709
Batch 15/64 loss: 0.015386700630187988
Batch 16/64 loss: 0.018411338329315186
Batch 17/64 loss: 0.017468631267547607
Batch 18/64 loss: 0.025803327560424805
Batch 19/64 loss: 0.02988821268081665
Batch 20/64 loss: 0.033816635608673096
Batch 21/64 loss: 0.020771443843841553
Batch 22/64 loss: 0.018108069896697998
Batch 23/64 loss: 0.018630027770996094
Batch 24/64 loss: 0.020836949348449707
Batch 25/64 loss: -0.00010848045349121094
Batch 26/64 loss: 0.02917766571044922
Batch 27/64 loss: 0.028516173362731934
Batch 28/64 loss: 0.03179931640625
Batch 29/64 loss: 0.020934462547302246
Batch 30/64 loss: 0.04224371910095215
Batch 31/64 loss: 0.004081666469573975
Batch 32/64 loss: 0.03339189291000366
Batch 33/64 loss: 0.024942755699157715
Batch 34/64 loss: 0.039528489112854004
Batch 35/64 loss: 0.020273327827453613
Batch 36/64 loss: 0.0359271764755249
Batch 37/64 loss: 0.02655559778213501
Batch 38/64 loss: 0.025487959384918213
Batch 39/64 loss: 0.040137290954589844
Batch 40/64 loss: 0.003502964973449707
Batch 41/64 loss: 0.017995893955230713
Batch 42/64 loss: 0.021133482456207275
Batch 43/64 loss: 0.033215999603271484
Batch 44/64 loss: -0.00245743989944458
Batch 45/64 loss: 0.021211624145507812
Batch 46/64 loss: 0.0034720897674560547
Batch 47/64 loss: 0.03483593463897705
Batch 48/64 loss: 0.009421348571777344
Batch 49/64 loss: 0.045471906661987305
Batch 50/64 loss: 0.017509758472442627
Batch 51/64 loss: 0.03624916076660156
Batch 52/64 loss: 0.0317150354385376
Batch 53/64 loss: 0.03396642208099365
Batch 54/64 loss: 0.028517544269561768
Batch 55/64 loss: 0.01685464382171631
Batch 56/64 loss: -0.000797271728515625
Batch 57/64 loss: 0.02376335859298706
Batch 58/64 loss: 0.024265289306640625
Batch 59/64 loss: 0.017964482307434082
Batch 60/64 loss: 0.020816445350646973
Batch 61/64 loss: 0.03357064723968506
Batch 62/64 loss: 0.026515305042266846
Batch 63/64 loss: 0.03961217403411865
Batch 64/64 loss: 0.007927477359771729
Epoch 91  Train loss: 0.02572268948835485  Val loss: 0.07348982656944249
Epoch 92
-------------------------------
Batch 1/64 loss: 0.03789997100830078
Batch 2/64 loss: 0.020014464855194092
Batch 3/64 loss: 0.0243605375289917
Batch 4/64 loss: 0.014205396175384521
Batch 5/64 loss: 0.00799185037612915
Batch 6/64 loss: 0.03627359867095947
Batch 7/64 loss: 0.018098831176757812
Batch 8/64 loss: 0.04411739110946655
Batch 9/64 loss: 0.012614130973815918
Batch 10/64 loss: 0.04301273822784424
Batch 11/64 loss: 0.024000883102416992
Batch 12/64 loss: 0.022985756397247314
Batch 13/64 loss: 0.006304144859313965
Batch 14/64 loss: 0.050995707511901855
Batch 15/64 loss: 0.023395061492919922
Batch 16/64 loss: 0.023043274879455566
Batch 17/64 loss: 0.012801885604858398
Batch 18/64 loss: 0.014551758766174316
Batch 19/64 loss: 0.03072202205657959
Batch 20/64 loss: 0.022429466247558594
Batch 21/64 loss: 0.050141990184783936
Batch 22/64 loss: 0.018261849880218506
Batch 23/64 loss: 0.01293414831161499
Batch 24/64 loss: 0.01622021198272705
Batch 25/64 loss: 0.008023262023925781
Batch 26/64 loss: 0.026890575885772705
Batch 27/64 loss: 0.01523125171661377
Batch 28/64 loss: 0.015048503875732422
Batch 29/64 loss: 0.015081703662872314
Batch 30/64 loss: 0.021522164344787598
Batch 31/64 loss: 0.009046077728271484
Batch 32/64 loss: 0.0284804105758667
Batch 33/64 loss: 0.024741053581237793
Batch 34/64 loss: 0.004307150840759277
Batch 35/64 loss: 0.034771621227264404
Batch 36/64 loss: 0.011032819747924805
Batch 37/64 loss: 0.04832291603088379
Batch 38/64 loss: 0.03502577543258667
Batch 39/64 loss: 0.04635775089263916
Batch 40/64 loss: 0.010717034339904785
Batch 41/64 loss: 0.044505298137664795
Batch 42/64 loss: 0.012288808822631836
Batch 43/64 loss: 0.03061729669570923
Batch 44/64 loss: 0.008671283721923828
Batch 45/64 loss: 0.025247514247894287
Batch 46/64 loss: 0.03691220283508301
Batch 47/64 loss: 0.028143644332885742
Batch 48/64 loss: 0.029800117015838623
Batch 49/64 loss: 0.06076407432556152
Batch 50/64 loss: 0.0282973051071167
Batch 51/64 loss: 0.002537250518798828
Batch 52/64 loss: 0.03691798448562622
Batch 53/64 loss: 0.030224204063415527
Batch 54/64 loss: 0.026241779327392578
Batch 55/64 loss: 0.006500959396362305
Batch 56/64 loss: 0.02240043878555298
Batch 57/64 loss: 0.02626216411590576
Batch 58/64 loss: 0.02824002504348755
Batch 59/64 loss: 0.019877731800079346
Batch 60/64 loss: 0.035062551498413086
Batch 61/64 loss: 0.017868876457214355
Batch 62/64 loss: 0.034574270248413086
Batch 63/64 loss: 0.03714549541473389
Batch 64/64 loss: 0.0492897629737854
Epoch 92  Train loss: 0.025224246932011026  Val loss: 0.06948645889144583
Epoch 93
-------------------------------
Batch 1/64 loss: 0.02717965841293335
Batch 2/64 loss: 0.011454284191131592
Batch 3/64 loss: 0.039664268493652344
Batch 4/64 loss: 0.008508920669555664
Batch 5/64 loss: 0.014723479747772217
Batch 6/64 loss: -0.002963542938232422
Batch 7/64 loss: 0.010704636573791504
Batch 8/64 loss: 0.013860940933227539
Batch 9/64 loss: 0.05953770875930786
Batch 10/64 loss: 0.026937365531921387
Batch 11/64 loss: 0.039794325828552246
Batch 12/64 loss: 0.023810863494873047
Batch 13/64 loss: 0.06167173385620117
Batch 14/64 loss: 0.04126054048538208
Batch 15/64 loss: 0.015418648719787598
Batch 16/64 loss: -0.0008791685104370117
Batch 17/64 loss: 0.004816532135009766
Batch 18/64 loss: 0.02183741331100464
Batch 19/64 loss: 0.03331148624420166
Batch 20/64 loss: 0.019417881965637207
Batch 21/64 loss: 0.020528197288513184
Batch 22/64 loss: 0.025947868824005127
Batch 23/64 loss: 0.018273472785949707
Batch 24/64 loss: 0.03894150257110596
Batch 25/64 loss: 0.0035767555236816406
Batch 26/64 loss: 0.02079176902770996
Batch 27/64 loss: 0.020688116550445557
Batch 28/64 loss: 0.013768017292022705
Batch 29/64 loss: 0.023511290550231934
Batch 30/64 loss: -0.0005748271942138672
Batch 31/64 loss: 0.007664918899536133
Batch 32/64 loss: 0.011996150016784668
Batch 33/64 loss: 0.0388866662979126
Batch 34/64 loss: 0.021448135375976562
Batch 35/64 loss: 0.03199601173400879
Batch 36/64 loss: 0.02117025852203369
Batch 37/64 loss: 0.00035446882247924805
Batch 38/64 loss: 0.012037873268127441
Batch 39/64 loss: 0.02227950096130371
Batch 40/64 loss: 0.0339241623878479
Batch 41/64 loss: 0.03279435634613037
Batch 42/64 loss: 0.03600972890853882
Batch 43/64 loss: 0.025395572185516357
Batch 44/64 loss: 0.026735424995422363
Batch 45/64 loss: 0.028008222579956055
Batch 46/64 loss: 0.0099295973777771
Batch 47/64 loss: 0.03212863206863403
Batch 48/64 loss: 0.02561163902282715
Batch 49/64 loss: 0.019288063049316406
Batch 50/64 loss: 0.010373353958129883
Batch 51/64 loss: 0.02149045467376709
Batch 52/64 loss: 0.0266951322555542
Batch 53/64 loss: 0.012772321701049805
Batch 54/64 loss: 0.020415306091308594
Batch 55/64 loss: 0.0496639609336853
Batch 56/64 loss: 0.021576762199401855
Batch 57/64 loss: 0.019370555877685547
Batch 58/64 loss: 0.012558281421661377
Batch 59/64 loss: 0.02559828758239746
Batch 60/64 loss: 0.021412968635559082
Batch 61/64 loss: 0.02515500783920288
Batch 62/64 loss: 0.028116226196289062
Batch 63/64 loss: 0.01509237289428711
Batch 64/64 loss: 0.048569321632385254
Epoch 93  Train loss: 0.022586627567515654  Val loss: 0.06987326620370661
Epoch 94
-------------------------------
Batch 1/64 loss: 0.011697828769683838
Batch 2/64 loss: 0.043339312076568604
Batch 3/64 loss: 0.028899669647216797
Batch 4/64 loss: 0.022400975227355957
Batch 5/64 loss: 0.026269257068634033
Batch 6/64 loss: 0.024301886558532715
Batch 7/64 loss: 0.03087836503982544
Batch 8/64 loss: 0.020202577114105225
Batch 9/64 loss: 0.03106820583343506
Batch 10/64 loss: -0.001284480094909668
Batch 11/64 loss: 0.046337783336639404
Batch 12/64 loss: 0.02724158763885498
Batch 13/64 loss: 0.01976257562637329
Batch 14/64 loss: 0.005607008934020996
Batch 15/64 loss: -0.000413358211517334
Batch 16/64 loss: 0.007355630397796631
Batch 17/64 loss: 0.027309417724609375
Batch 18/64 loss: 0.02706599235534668
Batch 19/64 loss: -0.001552879810333252
Batch 20/64 loss: 0.030816256999969482
Batch 21/64 loss: 0.020286381244659424
Batch 22/64 loss: 0.019909024238586426
Batch 23/64 loss: 0.050617218017578125
Batch 24/64 loss: 0.00323641300201416
Batch 25/64 loss: 0.02087002992630005
Batch 26/64 loss: 0.03342890739440918
Batch 27/64 loss: 0.02284914255142212
Batch 28/64 loss: 0.022755146026611328
Batch 29/64 loss: 0.0331273078918457
Batch 30/64 loss: 0.011618971824645996
Batch 31/64 loss: 0.016319453716278076
Batch 32/64 loss: 0.03783053159713745
Batch 33/64 loss: 0.0042264461517333984
Batch 34/64 loss: 0.030998587608337402
Batch 35/64 loss: 0.011054575443267822
Batch 36/64 loss: 0.02074909210205078
Batch 37/64 loss: 0.014430224895477295
Batch 38/64 loss: 0.03501957654953003
Batch 39/64 loss: 0.028064310550689697
Batch 40/64 loss: 0.02197211980819702
Batch 41/64 loss: 0.016486525535583496
Batch 42/64 loss: 0.0074310302734375
Batch 43/64 loss: 0.02749025821685791
Batch 44/64 loss: 0.016544342041015625
Batch 45/64 loss: 0.029781222343444824
Batch 46/64 loss: 0.005068063735961914
Batch 47/64 loss: 0.006397068500518799
Batch 48/64 loss: 0.0035753846168518066
Batch 49/64 loss: 0.027812957763671875
Batch 50/64 loss: 0.01918339729309082
Batch 51/64 loss: 0.02540767192840576
Batch 52/64 loss: 0.01343679428100586
Batch 53/64 loss: 0.028472483158111572
Batch 54/64 loss: 0.04568219184875488
Batch 55/64 loss: 0.018242955207824707
Batch 56/64 loss: 0.019971013069152832
Batch 57/64 loss: 0.04617297649383545
Batch 58/64 loss: 0.03010237216949463
Batch 59/64 loss: 0.007399022579193115
Batch 60/64 loss: 0.01208639144897461
Batch 61/64 loss: 0.04245811700820923
Batch 62/64 loss: 0.025524497032165527
Batch 63/64 loss: 0.03095877170562744
Batch 64/64 loss: 0.03319406509399414
Epoch 94  Train loss: 0.022231343213249655  Val loss: 0.06780398670340732
Epoch 95
-------------------------------
Batch 1/64 loss: 0.01949000358581543
Batch 2/64 loss: 0.029288649559020996
Batch 3/64 loss: 0.01760023832321167
Batch 4/64 loss: 0.013092279434204102
Batch 5/64 loss: 0.008216500282287598
Batch 6/64 loss: 0.05203026533126831
Batch 7/64 loss: -0.0020647048950195312
Batch 8/64 loss: 0.02378636598587036
Batch 9/64 loss: 0.0077266693115234375
Batch 10/64 loss: 0.008006691932678223
Batch 11/64 loss: 0.012239038944244385
Batch 12/64 loss: 0.007991433143615723
Batch 13/64 loss: 0.04940152168273926
Batch 14/64 loss: 0.024530231952667236
Batch 15/64 loss: 0.051894307136535645
Batch 16/64 loss: 0.013885498046875
Batch 17/64 loss: 0.03855264186859131
Batch 18/64 loss: 0.04023253917694092
Batch 19/64 loss: -0.006067872047424316
Batch 20/64 loss: 0.03555411100387573
Batch 21/64 loss: 0.006380319595336914
Batch 22/64 loss: 0.028847157955169678
Batch 23/64 loss: 0.031877756118774414
Batch 24/64 loss: 0.007361054420471191
Batch 25/64 loss: 0.051873207092285156
Batch 26/64 loss: 0.030398070812225342
Batch 27/64 loss: 0.030280649662017822
Batch 28/64 loss: 0.022868990898132324
Batch 29/64 loss: 0.017882227897644043
Batch 30/64 loss: 0.008726775646209717
Batch 31/64 loss: 0.0082932710647583
Batch 32/64 loss: 0.012481272220611572
Batch 33/64 loss: 0.00701296329498291
Batch 34/64 loss: 0.013226807117462158
Batch 35/64 loss: 0.01735377311706543
Batch 36/64 loss: 0.034770309925079346
Batch 37/64 loss: 0.030628442764282227
Batch 38/64 loss: 0.020484626293182373
Batch 39/64 loss: 0.020860731601715088
Batch 40/64 loss: 0.028813600540161133
Batch 41/64 loss: -0.001687169075012207
Batch 42/64 loss: 0.01615816354751587
Batch 43/64 loss: 0.0074135661125183105
Batch 44/64 loss: 0.03050631284713745
Batch 45/64 loss: 0.02914869785308838
Batch 46/64 loss: 0.02841848134994507
Batch 47/64 loss: 0.02635788917541504
Batch 48/64 loss: 0.025528669357299805
Batch 49/64 loss: 0.008025050163269043
Batch 50/64 loss: 0.026169419288635254
Batch 51/64 loss: 0.01778125762939453
Batch 52/64 loss: 0.04518866539001465
Batch 53/64 loss: 0.035906314849853516
Batch 54/64 loss: 0.015150785446166992
Batch 55/64 loss: 0.029003143310546875
Batch 56/64 loss: 0.026378095149993896
Batch 57/64 loss: 0.02615499496459961
Batch 58/64 loss: 0.024513065814971924
Batch 59/64 loss: 0.02004373073577881
Batch 60/64 loss: 0.004804849624633789
Batch 61/64 loss: 0.0034819841384887695
Batch 62/64 loss: 0.009904146194458008
Batch 63/64 loss: 0.012233912944793701
Batch 64/64 loss: 0.0020165443420410156
Epoch 95  Train loss: 0.021080860437131397  Val loss: 0.06612588675161407
Saving best model, epoch: 95
Epoch 96
-------------------------------
Batch 1/64 loss: -0.0050958991050720215
Batch 2/64 loss: 0.021518349647521973
Batch 3/64 loss: 0.023543894290924072
Batch 4/64 loss: 0.005799710750579834
Batch 5/64 loss: 0.019672691822052002
Batch 6/64 loss: 0.0007090568542480469
Batch 7/64 loss: 0.06203615665435791
Batch 8/64 loss: 0.033968567848205566
Batch 9/64 loss: 0.014368057250976562
Batch 10/64 loss: -0.0066190361976623535
Batch 11/64 loss: 0.058476269245147705
Batch 12/64 loss: 0.022025704383850098
Batch 13/64 loss: 0.0020350217819213867
Batch 14/64 loss: 0.002184927463531494
Batch 15/64 loss: 0.0214461088180542
Batch 16/64 loss: 0.016375303268432617
Batch 17/64 loss: 0.03545457124710083
Batch 18/64 loss: 0.04931318759918213
Batch 19/64 loss: 0.008092224597930908
Batch 20/64 loss: 0.031174957752227783
Batch 21/64 loss: 0.03418642282485962
Batch 22/64 loss: 0.015956401824951172
Batch 23/64 loss: 0.013100028038024902
Batch 24/64 loss: 0.01741790771484375
Batch 25/64 loss: 0.04663383960723877
Batch 26/64 loss: 0.028418421745300293
Batch 27/64 loss: 0.011616706848144531
Batch 28/64 loss: 0.022205471992492676
Batch 29/64 loss: 0.009179115295410156
Batch 30/64 loss: 0.009515821933746338
Batch 31/64 loss: 0.03394150733947754
Batch 32/64 loss: 0.01851409673690796
Batch 33/64 loss: 0.030861616134643555
Batch 34/64 loss: 0.031772077083587646
Batch 35/64 loss: 0.014409244060516357
Batch 36/64 loss: 0.021677374839782715
Batch 37/64 loss: 0.010576307773590088
Batch 38/64 loss: 0.024909615516662598
Batch 39/64 loss: 0.0072479248046875
Batch 40/64 loss: 0.02144646644592285
Batch 41/64 loss: 0.016775131225585938
Batch 42/64 loss: 0.01676112413406372
Batch 43/64 loss: 0.020050406455993652
Batch 44/64 loss: 0.023070335388183594
Batch 45/64 loss: 0.02474421262741089
Batch 46/64 loss: 0.021596789360046387
Batch 47/64 loss: 0.009141623973846436
Batch 48/64 loss: 0.02027684450149536
Batch 49/64 loss: 0.014133989810943604
Batch 50/64 loss: 0.0098341703414917
Batch 51/64 loss: 0.005952894687652588
Batch 52/64 loss: 0.028108596801757812
Batch 53/64 loss: 0.028175830841064453
Batch 54/64 loss: 0.036570727825164795
Batch 55/64 loss: 0.04668593406677246
Batch 56/64 loss: 0.033183395862579346
Batch 57/64 loss: 0.008790969848632812
Batch 58/64 loss: 0.03032541275024414
Batch 59/64 loss: 0.006567478179931641
Batch 60/64 loss: 0.0024625062942504883
Batch 61/64 loss: 0.01286017894744873
Batch 62/64 loss: 0.01548546552658081
Batch 63/64 loss: 0.01262897253036499
Batch 64/64 loss: 0.003459751605987549
Epoch 96  Train loss: 0.020185788238749783  Val loss: 0.06700374273090429
Epoch 97
-------------------------------
Batch 1/64 loss: 0.03527921438217163
Batch 2/64 loss: 0.023493945598602295
Batch 3/64 loss: 0.002964317798614502
Batch 4/64 loss: 0.024307310581207275
Batch 5/64 loss: -0.0023480653762817383
Batch 6/64 loss: 0.020979344844818115
Batch 7/64 loss: 0.007275283336639404
Batch 8/64 loss: 0.012808382511138916
Batch 9/64 loss: 0.01696544885635376
Batch 10/64 loss: -0.007814586162567139
Batch 11/64 loss: 0.010242223739624023
Batch 12/64 loss: 0.005401194095611572
Batch 13/64 loss: -0.012311220169067383
Batch 14/64 loss: 0.027355611324310303
Batch 15/64 loss: 0.011479973793029785
Batch 16/64 loss: 0.01208639144897461
Batch 17/64 loss: 0.027599215507507324
Batch 18/64 loss: 0.032103776931762695
Batch 19/64 loss: 0.013390541076660156
Batch 20/64 loss: 0.028057217597961426
Batch 21/64 loss: 0.01717197895050049
Batch 22/64 loss: 0.03741133213043213
Batch 23/64 loss: 0.02034968137741089
Batch 24/64 loss: 0.02231544256210327
Batch 25/64 loss: 0.0108109712600708
Batch 26/64 loss: -0.00014382600784301758
Batch 27/64 loss: 0.02020132541656494
Batch 28/64 loss: 0.022711098194122314
Batch 29/64 loss: 0.025801777839660645
Batch 30/64 loss: 0.033032774925231934
Batch 31/64 loss: 0.01859438419342041
Batch 32/64 loss: 0.02189880609512329
Batch 33/64 loss: 0.01805746555328369
Batch 34/64 loss: 0.01905268430709839
Batch 35/64 loss: 0.022252023220062256
Batch 36/64 loss: 0.024887681007385254
Batch 37/64 loss: 0.023555636405944824
Batch 38/64 loss: 0.013103008270263672
Batch 39/64 loss: 0.03757745027542114
Batch 40/64 loss: 0.015070497989654541
Batch 41/64 loss: 0.030199766159057617
Batch 42/64 loss: 0.011766314506530762
Batch 43/64 loss: 0.004607856273651123
Batch 44/64 loss: 0.030887722969055176
Batch 45/64 loss: 0.017078638076782227
Batch 46/64 loss: 0.03269535303115845
Batch 47/64 loss: 0.04533815383911133
Batch 48/64 loss: 0.04937940835952759
Batch 49/64 loss: 0.014915764331817627
Batch 50/64 loss: 0.01765388250350952
Batch 51/64 loss: 0.013978362083435059
Batch 52/64 loss: 0.022676467895507812
Batch 53/64 loss: 0.023494601249694824
Batch 54/64 loss: 0.029778718948364258
Batch 55/64 loss: 0.02513861656188965
Batch 56/64 loss: 0.011157393455505371
Batch 57/64 loss: 7.063150405883789e-05
Batch 58/64 loss: -0.001780390739440918
Batch 59/64 loss: 0.002893507480621338
Batch 60/64 loss: 0.01865321397781372
Batch 61/64 loss: 0.033167243003845215
Batch 62/64 loss: 0.029805004596710205
Batch 63/64 loss: 0.013080596923828125
Batch 64/64 loss: 0.00106048583984375
Epoch 97  Train loss: 0.01867388089497884  Val loss: 0.06575311623078442
Saving best model, epoch: 97
Epoch 98
-------------------------------
Batch 1/64 loss: 0.010925769805908203
Batch 2/64 loss: 0.021166205406188965
Batch 3/64 loss: 0.010360121726989746
Batch 4/64 loss: -0.00498729944229126
Batch 5/64 loss: 0.01576697826385498
Batch 6/64 loss: 0.009956717491149902
Batch 7/64 loss: -0.008161544799804688
Batch 8/64 loss: -0.002232670783996582
Batch 9/64 loss: 0.016442060470581055
Batch 10/64 loss: 0.008583307266235352
Batch 11/64 loss: 0.014243245124816895
Batch 12/64 loss: 0.03149145841598511
Batch 13/64 loss: 0.005394876003265381
Batch 14/64 loss: 0.03284364938735962
Batch 15/64 loss: 0.045013427734375
Batch 16/64 loss: 0.035416364669799805
Batch 17/64 loss: 0.017161905765533447
Batch 18/64 loss: -0.001336216926574707
Batch 19/64 loss: 0.02401834726333618
Batch 20/64 loss: 0.016592025756835938
Batch 21/64 loss: 0.023385167121887207
Batch 22/64 loss: 0.00412750244140625
Batch 23/64 loss: 0.0016184449195861816
Batch 24/64 loss: 0.003833293914794922
Batch 25/64 loss: 0.01729297637939453
Batch 26/64 loss: 0.00863337516784668
Batch 27/64 loss: 0.009907543659210205
Batch 28/64 loss: 0.04407459497451782
Batch 29/64 loss: -0.0032765865325927734
Batch 30/64 loss: 0.01344972848892212
Batch 31/64 loss: 0.032015085220336914
Batch 32/64 loss: 0.012590885162353516
Batch 33/64 loss: 0.010699748992919922
Batch 34/64 loss: 0.0273209810256958
Batch 35/64 loss: 0.024485409259796143
Batch 36/64 loss: 0.03895515203475952
Batch 37/64 loss: 0.03991687297821045
Batch 38/64 loss: 0.012829840183258057
Batch 39/64 loss: 0.01401221752166748
Batch 40/64 loss: 0.037420570850372314
Batch 41/64 loss: 0.03968524932861328
Batch 42/64 loss: 0.015184581279754639
Batch 43/64 loss: 0.03369772434234619
Batch 44/64 loss: 0.02702152729034424
Batch 45/64 loss: 0.01830732822418213
Batch 46/64 loss: 0.03272294998168945
Batch 47/64 loss: 0.00667184591293335
Batch 48/64 loss: 0.01313108205795288
Batch 49/64 loss: 0.04120349884033203
Batch 50/64 loss: 0.015856027603149414
Batch 51/64 loss: 0.021438300609588623
Batch 52/64 loss: 0.03891646862030029
Batch 53/64 loss: 0.02302229404449463
Batch 54/64 loss: 0.007692992687225342
Batch 55/64 loss: 0.021250665187835693
Batch 56/64 loss: 0.022280216217041016
Batch 57/64 loss: 0.021150171756744385
Batch 58/64 loss: 0.011736631393432617
Batch 59/64 loss: 0.02765357494354248
Batch 60/64 loss: 0.026595354080200195
Batch 61/64 loss: 0.03507906198501587
Batch 62/64 loss: 0.02173173427581787
Batch 63/64 loss: 0.02128380537033081
Batch 64/64 loss: -0.0010232925415039062
Epoch 98  Train loss: 0.019050935670441273  Val loss: 0.06846756992471177
Epoch 99
-------------------------------
Batch 1/64 loss: 0.0347328782081604
Batch 2/64 loss: 0.024218082427978516
Batch 3/64 loss: 0.02974802255630493
Batch 4/64 loss: 0.007688760757446289
Batch 5/64 loss: 0.001316666603088379
Batch 6/64 loss: 0.018078744411468506
Batch 7/64 loss: 0.03627932071685791
Batch 8/64 loss: 0.006354928016662598
Batch 9/64 loss: 0.025129258632659912
Batch 10/64 loss: 0.02327406406402588
Batch 11/64 loss: 0.023020386695861816
Batch 12/64 loss: 0.028055310249328613
Batch 13/64 loss: 0.011031568050384521
Batch 14/64 loss: 0.018094539642333984
Batch 15/64 loss: 0.006614983081817627
Batch 16/64 loss: 0.0005306005477905273
Batch 17/64 loss: 0.018017053604125977
Batch 18/64 loss: 0.006549835205078125
Batch 19/64 loss: 0.01950526237487793
Batch 20/64 loss: 0.028682351112365723
Batch 21/64 loss: 0.013662338256835938
Batch 22/64 loss: 0.009950995445251465
Batch 23/64 loss: 0.022351324558258057
Batch 24/64 loss: 0.025024890899658203
Batch 25/64 loss: 0.008919298648834229
Batch 26/64 loss: 0.02962934970855713
Batch 27/64 loss: -0.001217484474182129
Batch 28/64 loss: 0.029474973678588867
Batch 29/64 loss: 0.02539348602294922
Batch 30/64 loss: 0.02315378189086914
Batch 31/64 loss: 0.029186487197875977
Batch 32/64 loss: 0.03934258222579956
Batch 33/64 loss: 0.03418475389480591
Batch 34/64 loss: 0.02103865146636963
Batch 35/64 loss: 0.03243333101272583
Batch 36/64 loss: 0.019037246704101562
Batch 37/64 loss: 0.01803290843963623
Batch 38/64 loss: 0.018673062324523926
Batch 39/64 loss: 0.02262634038925171
Batch 40/64 loss: 0.0035085082054138184
Batch 41/64 loss: 0.02905595302581787
Batch 42/64 loss: 0.015970468521118164
Batch 43/64 loss: 0.0010486245155334473
Batch 44/64 loss: 0.01153630018234253
Batch 45/64 loss: 0.05391770601272583
Batch 46/64 loss: 0.03232431411743164
Batch 47/64 loss: 0.006860792636871338
Batch 48/64 loss: 0.0170174241065979
Batch 49/64 loss: 0.02577108144760132
Batch 50/64 loss: 0.012541890144348145
Batch 51/64 loss: 0.01664590835571289
Batch 52/64 loss: -0.004536926746368408
Batch 53/64 loss: 0.021172404289245605
Batch 54/64 loss: 0.040358781814575195
Batch 55/64 loss: 0.010519325733184814
Batch 56/64 loss: 0.03694009780883789
Batch 57/64 loss: 0.02466362714767456
Batch 58/64 loss: 0.022562623023986816
Batch 59/64 loss: 0.0145951509475708
Batch 60/64 loss: 0.006606042385101318
Batch 61/64 loss: 0.0093994140625
Batch 62/64 loss: 0.018051564693450928
Batch 63/64 loss: 0.013924360275268555
Batch 64/64 loss: 0.009966790676116943
Epoch 99  Train loss: 0.019384336705301323  Val loss: 0.06422608476324179
Saving best model, epoch: 99
Epoch 100
-------------------------------
Batch 1/64 loss: 0.023062944412231445
Batch 2/64 loss: 0.022304832935333252
Batch 3/64 loss: 0.013313651084899902
Batch 4/64 loss: 0.009373128414154053
Batch 5/64 loss: 0.018339216709136963
Batch 6/64 loss: -0.005653262138366699
Batch 7/64 loss: -0.0005513429641723633
Batch 8/64 loss: 0.008293628692626953
Batch 9/64 loss: 0.01610666513442993
Batch 10/64 loss: 0.02662724256515503
Batch 11/64 loss: 0.0065250396728515625
Batch 12/64 loss: 0.033997297286987305
Batch 13/64 loss: 0.014189600944519043
Batch 14/64 loss: -0.0006303787231445312
Batch 15/64 loss: 0.004079163074493408
Batch 16/64 loss: -0.0030027031898498535
Batch 17/64 loss: 0.013806462287902832
Batch 18/64 loss: -0.007639169692993164
Batch 19/64 loss: 0.048082709312438965
Batch 20/64 loss: 0.030499696731567383
Batch 21/64 loss: 0.037922561168670654
Batch 22/64 loss: 0.016930758953094482
Batch 23/64 loss: 0.032555341720581055
Batch 24/64 loss: 0.010386526584625244
Batch 25/64 loss: 0.019108891487121582
Batch 26/64 loss: 0.0037055015563964844
Batch 27/64 loss: 0.009277820587158203
Batch 28/64 loss: 0.007580101490020752
Batch 29/64 loss: 0.014550447463989258
Batch 30/64 loss: 0.02173483371734619
Batch 31/64 loss: 0.02855229377746582
Batch 32/64 loss: 0.035921692848205566
Batch 33/64 loss: 0.017508327960968018
Batch 34/64 loss: 0.022310853004455566
Batch 35/64 loss: 0.05158030986785889
Batch 36/64 loss: 0.02115917205810547
Batch 37/64 loss: 0.002290785312652588
Batch 38/64 loss: 0.021607160568237305
Batch 39/64 loss: 0.024685919284820557
Batch 40/64 loss: 0.04247027635574341
Batch 41/64 loss: -0.0030802488327026367
Batch 42/64 loss: 0.03481245040893555
Batch 43/64 loss: 0.010895967483520508
Batch 44/64 loss: 0.04258310794830322
Batch 45/64 loss: 0.035863399505615234
Batch 46/64 loss: 0.005879461765289307
Batch 47/64 loss: 0.036312103271484375
Batch 48/64 loss: 0.019715189933776855
Batch 49/64 loss: 0.05110478401184082
Batch 50/64 loss: 0.01752305030822754
Batch 51/64 loss: 0.013479232788085938
Batch 52/64 loss: 0.03237354755401611
Batch 53/64 loss: 0.03190338611602783
Batch 54/64 loss: 0.032404184341430664
Batch 55/64 loss: 0.03489196300506592
Batch 56/64 loss: 0.002673625946044922
Batch 57/64 loss: 0.017161965370178223
Batch 58/64 loss: 0.0509607195854187
Batch 59/64 loss: 0.009859800338745117
Batch 60/64 loss: 0.004904747009277344
Batch 61/64 loss: 0.002161264419555664
Batch 62/64 loss: 0.00909048318862915
Batch 63/64 loss: 0.009422361850738525
Batch 64/64 loss: 0.022881031036376953
Epoch 100  Train loss: 0.019341510884902057  Val loss: 0.0640208792850324
Saving best model, epoch: 100
Epoch 101
-------------------------------
Batch 1/64 loss: 0.005707502365112305
Batch 2/64 loss: -0.00788569450378418
Batch 3/64 loss: 0.022745966911315918
Batch 4/64 loss: 0.010981619358062744
Batch 5/64 loss: -0.0025388002395629883
Batch 6/64 loss: 0.058135390281677246
Batch 7/64 loss: 0.003885924816131592
Batch 8/64 loss: 0.01653808355331421
Batch 9/64 loss: 0.00991123914718628
Batch 10/64 loss: 0.01328665018081665
Batch 11/64 loss: 0.02025902271270752
Batch 12/64 loss: 0.014277458190917969
Batch 13/64 loss: 0.034467101097106934
Batch 14/64 loss: 0.04919642210006714
Batch 15/64 loss: 0.0054514408111572266
Batch 16/64 loss: 0.001315295696258545
Batch 17/64 loss: 0.01163172721862793
Batch 18/64 loss: 0.027521967887878418
Batch 19/64 loss: 0.035183608531951904
Batch 20/64 loss: -0.003877878189086914
Batch 21/64 loss: 0.010208964347839355
Batch 22/64 loss: 0.040572524070739746
Batch 23/64 loss: 0.006749153137207031
Batch 24/64 loss: 0.0008623600006103516
Batch 25/64 loss: 0.025430798530578613
Batch 26/64 loss: 0.022223174571990967
Batch 27/64 loss: 0.017757415771484375
Batch 28/64 loss: -0.00715559720993042
Batch 29/64 loss: 0.01769697666168213
Batch 30/64 loss: 0.023792386054992676
Batch 31/64 loss: 0.026784062385559082
Batch 32/64 loss: 0.014584720134735107
Batch 33/64 loss: 0.004783153533935547
Batch 34/64 loss: 0.007512509822845459
Batch 35/64 loss: -0.003869175910949707
Batch 36/64 loss: 0.017094969749450684
Batch 37/64 loss: 0.021160364151000977
Batch 38/64 loss: 0.040081560611724854
Batch 39/64 loss: 0.017879188060760498
Batch 40/64 loss: 0.029741227626800537
Batch 41/64 loss: 0.018948793411254883
Batch 42/64 loss: -0.01677417755126953
Batch 43/64 loss: 0.012527406215667725
Batch 44/64 loss: 0.024131178855895996
Batch 45/64 loss: 0.0340428352355957
Batch 46/64 loss: 0.01541358232498169
Batch 47/64 loss: -0.003828763961791992
Batch 48/64 loss: 0.02951139211654663
Batch 49/64 loss: 0.011964797973632812
Batch 50/64 loss: 0.014632701873779297
Batch 51/64 loss: 0.026265084743499756
Batch 52/64 loss: 0.0027474164962768555
Batch 53/64 loss: 0.01874697208404541
Batch 54/64 loss: 0.0214768648147583
Batch 55/64 loss: 0.004320859909057617
Batch 56/64 loss: 0.0265350341796875
Batch 57/64 loss: 0.008168041706085205
Batch 58/64 loss: 0.010126233100891113
Batch 59/64 loss: 0.0204617977142334
Batch 60/64 loss: 0.014712154865264893
Batch 61/64 loss: 0.007039964199066162
Batch 62/64 loss: 0.021698355674743652
Batch 63/64 loss: -0.0008893609046936035
Batch 64/64 loss: 0.009285330772399902
Epoch 101  Train loss: 0.015514163877449784  Val loss: 0.06388625492345017
Saving best model, epoch: 101
Epoch 102
-------------------------------
Batch 1/64 loss: 0.008321106433868408
Batch 2/64 loss: -0.021408438682556152
Batch 3/64 loss: 0.02876955270767212
Batch 4/64 loss: 0.014297008514404297
Batch 5/64 loss: 0.020990192890167236
Batch 6/64 loss: -0.004883527755737305
Batch 7/64 loss: 0.0067893266677856445
Batch 8/64 loss: 0.02621513605117798
Batch 9/64 loss: 0.022074103355407715
Batch 10/64 loss: 0.003731250762939453
Batch 11/64 loss: -0.005974233150482178
Batch 12/64 loss: 0.057619690895080566
Batch 13/64 loss: 0.019990384578704834
Batch 14/64 loss: 0.0009123086929321289
Batch 15/64 loss: 0.024923741817474365
Batch 16/64 loss: 0.03248840570449829
Batch 17/64 loss: 0.028232872486114502
Batch 18/64 loss: 0.017721056938171387
Batch 19/64 loss: 0.014656603336334229
Batch 20/64 loss: -0.0003380775451660156
Batch 21/64 loss: -0.012118995189666748
Batch 22/64 loss: -0.012826204299926758
Batch 23/64 loss: 0.008497953414916992
Batch 24/64 loss: 0.028837382793426514
Batch 25/64 loss: 0.02387791872024536
Batch 26/64 loss: -0.007725059986114502
Batch 27/64 loss: 0.018360137939453125
Batch 28/64 loss: 0.01085519790649414
Batch 29/64 loss: 0.010130763053894043
Batch 30/64 loss: 0.02717304229736328
Batch 31/64 loss: 0.0026340484619140625
Batch 32/64 loss: 0.00575101375579834
Batch 33/64 loss: 0.01688516139984131
Batch 34/64 loss: 0.00027745962142944336
Batch 35/64 loss: 0.013457238674163818
Batch 36/64 loss: 0.013107120990753174
Batch 37/64 loss: 0.0242922306060791
Batch 38/64 loss: 0.02299189567565918
Batch 39/64 loss: 0.001337885856628418
Batch 40/64 loss: 0.02589273452758789
Batch 41/64 loss: 0.03611272573471069
Batch 42/64 loss: 0.008577048778533936
Batch 43/64 loss: 0.007444024085998535
Batch 44/64 loss: 0.010435879230499268
Batch 45/64 loss: 0.03667640686035156
Batch 46/64 loss: 0.0003769397735595703
Batch 47/64 loss: 0.028383612632751465
Batch 48/64 loss: 0.03159528970718384
Batch 49/64 loss: 0.02558809518814087
Batch 50/64 loss: 0.03798115253448486
Batch 51/64 loss: 0.013027548789978027
Batch 52/64 loss: -0.004882216453552246
Batch 53/64 loss: 0.019728422164916992
Batch 54/64 loss: 0.022267043590545654
Batch 55/64 loss: 0.03896212577819824
Batch 56/64 loss: -0.0035016536712646484
Batch 57/64 loss: 0.011973798274993896
Batch 58/64 loss: 0.03645730018615723
Batch 59/64 loss: 0.02852565050125122
Batch 60/64 loss: 0.02238863706588745
Batch 61/64 loss: 0.01353687047958374
Batch 62/64 loss: 0.0075647830963134766
Batch 63/64 loss: 0.011182248592376709
Batch 64/64 loss: 0.024441957473754883
Epoch 102  Train loss: 0.015302754383461148  Val loss: 0.06362854348834847
Saving best model, epoch: 102
Epoch 103
-------------------------------
Batch 1/64 loss: -0.002045929431915283
Batch 2/64 loss: 0.004465818405151367
Batch 3/64 loss: -0.008021235466003418
Batch 4/64 loss: 0.016255617141723633
Batch 5/64 loss: -0.0013937950134277344
Batch 6/64 loss: 0.017850220203399658
Batch 7/64 loss: -0.008199155330657959
Batch 8/64 loss: 0.018860578536987305
Batch 9/64 loss: 0.016383111476898193
Batch 10/64 loss: 0.008244454860687256
Batch 11/64 loss: -0.0031096935272216797
Batch 12/64 loss: -0.008378326892852783
Batch 13/64 loss: 0.018130958080291748
Batch 14/64 loss: 0.00037670135498046875
Batch 15/64 loss: 0.02536773681640625
Batch 16/64 loss: -0.005655467510223389
Batch 17/64 loss: 0.029042482376098633
Batch 18/64 loss: 0.021833300590515137
Batch 19/64 loss: 0.009134352207183838
Batch 20/64 loss: -0.0017402172088623047
Batch 21/64 loss: 0.01190793514251709
Batch 22/64 loss: 0.00015437602996826172
Batch 23/64 loss: -0.001497507095336914
Batch 24/64 loss: 0.015206515789031982
Batch 25/64 loss: 0.0039980411529541016
Batch 26/64 loss: -0.0013761520385742188
Batch 27/64 loss: 0.015231966972351074
Batch 28/64 loss: 0.040685057640075684
Batch 29/64 loss: 0.01857435703277588
Batch 30/64 loss: 0.01785147190093994
Batch 31/64 loss: 0.035637617111206055
Batch 32/64 loss: 0.0002504587173461914
Batch 33/64 loss: 0.009248495101928711
Batch 34/64 loss: 0.03637927770614624
Batch 35/64 loss: 0.03350335359573364
Batch 36/64 loss: -0.005388617515563965
Batch 37/64 loss: 0.020666956901550293
Batch 38/64 loss: 0.012010157108306885
Batch 39/64 loss: 0.00513845682144165
Batch 40/64 loss: 0.02121257781982422
Batch 41/64 loss: 0.03196972608566284
Batch 42/64 loss: 0.0065871477127075195
Batch 43/64 loss: 0.06635797023773193
Batch 44/64 loss: 0.020875930786132812
Batch 45/64 loss: 0.038226306438446045
Batch 46/64 loss: 0.022669434547424316
Batch 47/64 loss: -0.0012764930725097656
Batch 48/64 loss: 0.014114558696746826
Batch 49/64 loss: 0.0217859148979187
Batch 50/64 loss: 0.032563209533691406
Batch 51/64 loss: 0.0272904634475708
Batch 52/64 loss: 0.0037148594856262207
Batch 53/64 loss: 0.017150044441223145
Batch 54/64 loss: 0.043412208557128906
Batch 55/64 loss: -0.010611653327941895
Batch 56/64 loss: 0.004914700984954834
Batch 57/64 loss: 0.005332469940185547
Batch 58/64 loss: 0.00966334342956543
Batch 59/64 loss: 0.027436435222625732
Batch 60/64 loss: 0.01381772756576538
Batch 61/64 loss: -0.006324350833892822
Batch 62/64 loss: 0.013743281364440918
Batch 63/64 loss: -0.02109140157699585
Batch 64/64 loss: 0.006304562091827393
Epoch 103  Train loss: 0.012923083352107628  Val loss: 0.05968406888627514
Saving best model, epoch: 103
Epoch 104
-------------------------------
Batch 1/64 loss: 0.026442229747772217
Batch 2/64 loss: -0.01385718584060669
Batch 3/64 loss: 0.016296803951263428
Batch 4/64 loss: -0.012777388095855713
Batch 5/64 loss: -0.007956504821777344
Batch 6/64 loss: -0.01109158992767334
Batch 7/64 loss: 0.022510290145874023
Batch 8/64 loss: 0.013555586338043213
Batch 9/64 loss: 0.0013935565948486328
Batch 10/64 loss: 0.013839483261108398
Batch 11/64 loss: -0.003637254238128662
Batch 12/64 loss: 0.043305933475494385
Batch 13/64 loss: -0.013111293315887451
Batch 14/64 loss: 0.003120899200439453
Batch 15/64 loss: 0.02730005979537964
Batch 16/64 loss: 0.03254246711730957
Batch 17/64 loss: 0.013160407543182373
Batch 18/64 loss: 0.014088690280914307
Batch 19/64 loss: 0.00046128034591674805
Batch 20/64 loss: -0.011195242404937744
Batch 21/64 loss: 0.03374373912811279
Batch 22/64 loss: 0.020981669425964355
Batch 23/64 loss: 0.013634443283081055
Batch 24/64 loss: -0.001537621021270752
Batch 25/64 loss: -0.000910639762878418
Batch 26/64 loss: -0.0006006360054016113
Batch 27/64 loss: 0.01101154088973999
Batch 28/64 loss: 0.00992351770401001
Batch 29/64 loss: 0.04058200120925903
Batch 30/64 loss: -0.006140589714050293
Batch 31/64 loss: 0.005119442939758301
Batch 32/64 loss: 0.034564077854156494
Batch 33/64 loss: 0.005445599555969238
Batch 34/64 loss: 0.026437878608703613
Batch 35/64 loss: 0.011667013168334961
Batch 36/64 loss: -0.006360411643981934
Batch 37/64 loss: 0.00921630859375
Batch 38/64 loss: 0.012049436569213867
Batch 39/64 loss: -0.003174006938934326
Batch 40/64 loss: 0.00836169719696045
Batch 41/64 loss: 0.03489404916763306
Batch 42/64 loss: 0.008417308330535889
Batch 43/64 loss: 0.00310516357421875
Batch 44/64 loss: 0.015272974967956543
Batch 45/64 loss: 0.0033758878707885742
Batch 46/64 loss: -0.00020062923431396484
Batch 47/64 loss: 0.008082032203674316
Batch 48/64 loss: 0.0075016021728515625
Batch 49/64 loss: -0.009097754955291748
Batch 50/64 loss: 0.008579909801483154
Batch 51/64 loss: 0.017641901969909668
Batch 52/64 loss: 0.013339042663574219
Batch 53/64 loss: 0.00816124677658081
Batch 54/64 loss: 0.0007470846176147461
Batch 55/64 loss: 0.016457319259643555
Batch 56/64 loss: 0.022910714149475098
Batch 57/64 loss: 0.014804840087890625
Batch 58/64 loss: 0.03644824028015137
Batch 59/64 loss: 0.04369354248046875
Batch 60/64 loss: 0.029361963272094727
Batch 61/64 loss: 0.032894134521484375
Batch 62/64 loss: 0.042494237422943115
Batch 63/64 loss: 0.01656782627105713
Batch 64/64 loss: 0.012630820274353027
Epoch 104  Train loss: 0.011973826557982202  Val loss: 0.06584198081616274
Epoch 105
-------------------------------
Batch 1/64 loss: 0.02001899480819702
Batch 2/64 loss: -0.0018645524978637695
Batch 3/64 loss: 0.011726319789886475
Batch 4/64 loss: 0.018699228763580322
Batch 5/64 loss: 0.006715536117553711
Batch 6/64 loss: 0.0010242462158203125
Batch 7/64 loss: -0.014353930950164795
Batch 8/64 loss: 0.01003265380859375
Batch 9/64 loss: 0.006761491298675537
Batch 10/64 loss: 0.05169016122817993
Batch 11/64 loss: 0.006523489952087402
Batch 12/64 loss: -0.007373452186584473
Batch 13/64 loss: 0.020373880863189697
Batch 14/64 loss: 0.025599002838134766
Batch 15/64 loss: 0.008810102939605713
Batch 16/64 loss: 0.04228609800338745
Batch 17/64 loss: -0.020000576972961426
Batch 18/64 loss: 0.011709332466125488
Batch 19/64 loss: -0.024426162242889404
Batch 20/64 loss: 0.02827686071395874
Batch 21/64 loss: -0.008682310581207275
Batch 22/64 loss: -0.007796287536621094
Batch 23/64 loss: 0.00806725025177002
Batch 24/64 loss: 0.0079728364944458
Batch 25/64 loss: -0.008786559104919434
Batch 26/64 loss: 0.00031048059463500977
Batch 27/64 loss: 0.004997909069061279
Batch 28/64 loss: 0.01017308235168457
Batch 29/64 loss: -0.001520395278930664
Batch 30/64 loss: 0.005432546138763428
Batch 31/64 loss: 0.01896083354949951
Batch 32/64 loss: 0.0016867518424987793
Batch 33/64 loss: 0.02174466848373413
Batch 34/64 loss: 0.014071524143218994
Batch 35/64 loss: 0.005720794200897217
Batch 36/64 loss: 0.0026649832725524902
Batch 37/64 loss: 0.0050411224365234375
Batch 38/64 loss: 0.011310160160064697
Batch 39/64 loss: 0.028929173946380615
Batch 40/64 loss: 0.005121350288391113
Batch 41/64 loss: 0.01436847448348999
Batch 42/64 loss: 0.012915074825286865
Batch 43/64 loss: 0.04066711664199829
Batch 44/64 loss: 0.015537917613983154
Batch 45/64 loss: 0.015118062496185303
Batch 46/64 loss: 0.007754802703857422
Batch 47/64 loss: 0.035835862159729004
Batch 48/64 loss: 0.039414823055267334
Batch 49/64 loss: -0.004036068916320801
Batch 50/64 loss: 0.018999338150024414
Batch 51/64 loss: 0.031796813011169434
Batch 52/64 loss: 0.0226823091506958
Batch 53/64 loss: -7.11679458618164e-05
Batch 54/64 loss: 0.02551800012588501
Batch 55/64 loss: -0.0006428360939025879
Batch 56/64 loss: 0.04662132263183594
Batch 57/64 loss: -9.28640365600586e-05
Batch 58/64 loss: 0.009054422378540039
Batch 59/64 loss: 0.024538874626159668
Batch 60/64 loss: -0.008982956409454346
Batch 61/64 loss: 0.015805184841156006
Batch 62/64 loss: 0.004984259605407715
Batch 63/64 loss: 0.022046685218811035
Batch 64/64 loss: 0.004208087921142578
Epoch 105  Train loss: 0.011304127936269722  Val loss: 0.062490922069221834
Epoch 106
-------------------------------
Batch 1/64 loss: 0.012505769729614258
Batch 2/64 loss: -0.0071523189544677734
Batch 3/64 loss: 0.0046231746673583984
Batch 4/64 loss: 0.00956803560256958
Batch 5/64 loss: -0.0063130855560302734
Batch 6/64 loss: -0.0005863308906555176
Batch 7/64 loss: 0.0022481679916381836
Batch 8/64 loss: 0.024861931800842285
Batch 9/64 loss: 0.0316959023475647
Batch 10/64 loss: -0.003651916980743408
Batch 11/64 loss: 0.006373167037963867
Batch 12/64 loss: 0.029983162879943848
Batch 13/64 loss: 0.007219195365905762
Batch 14/64 loss: 0.041434645652770996
Batch 15/64 loss: 0.019720852375030518
Batch 16/64 loss: 0.017746150493621826
Batch 17/64 loss: 0.023689448833465576
Batch 18/64 loss: -0.0017467141151428223
Batch 19/64 loss: 0.018279194831848145
Batch 20/64 loss: 0.003745734691619873
Batch 21/64 loss: -0.016288340091705322
Batch 22/64 loss: 0.021334707736968994
Batch 23/64 loss: 0.0049086809158325195
Batch 24/64 loss: 0.04486525058746338
Batch 25/64 loss: 0.000583946704864502
Batch 26/64 loss: 0.028964221477508545
Batch 27/64 loss: -0.0020145177841186523
Batch 28/64 loss: 0.007010459899902344
Batch 29/64 loss: -0.011440157890319824
Batch 30/64 loss: 0.027802705764770508
Batch 31/64 loss: -0.005060136318206787
Batch 32/64 loss: 0.006216943264007568
Batch 33/64 loss: 0.007812380790710449
Batch 34/64 loss: 0.015283048152923584
Batch 35/64 loss: 0.001950085163116455
Batch 36/64 loss: 0.011800408363342285
Batch 37/64 loss: -0.010956048965454102
Batch 38/64 loss: 0.01963794231414795
Batch 39/64 loss: 0.025565743446350098
Batch 40/64 loss: 0.02234417200088501
Batch 41/64 loss: 0.01863652467727661
Batch 42/64 loss: 2.7358531951904297e-05
Batch 43/64 loss: 0.009651660919189453
Batch 44/64 loss: 0.0008717179298400879
Batch 45/64 loss: 0.011730313301086426
Batch 46/64 loss: 0.008566141128540039
Batch 47/64 loss: -0.012002885341644287
Batch 48/64 loss: 0.013956844806671143
Batch 49/64 loss: -0.005786240100860596
Batch 50/64 loss: 0.015511155128479004
Batch 51/64 loss: 0.0007252693176269531
Batch 52/64 loss: 0.0375407338142395
Batch 53/64 loss: -0.009728729724884033
Batch 54/64 loss: 0.00726926326751709
Batch 55/64 loss: 0.01438450813293457
Batch 56/64 loss: 0.01313638687133789
Batch 57/64 loss: 0.017457544803619385
Batch 58/64 loss: 0.009892761707305908
Batch 59/64 loss: 0.013479471206665039
Batch 60/64 loss: 0.018488585948944092
Batch 61/64 loss: -0.002456068992614746
Batch 62/64 loss: -0.0013580322265625
Batch 63/64 loss: 0.01898568868637085
Batch 64/64 loss: -0.0017548799514770508
Epoch 106  Train loss: 0.009917325132033404  Val loss: 0.0634930094902458
Epoch 107
-------------------------------
Batch 1/64 loss: 0.0053688883781433105
Batch 2/64 loss: 0.0106850266456604
Batch 3/64 loss: 0.011365115642547607
Batch 4/64 loss: 0.012680351734161377
Batch 5/64 loss: 0.008285164833068848
Batch 6/64 loss: -0.011289775371551514
Batch 7/64 loss: 0.005336284637451172
Batch 8/64 loss: -0.003300309181213379
Batch 9/64 loss: 0.014754772186279297
Batch 10/64 loss: 0.012258052825927734
Batch 11/64 loss: 0.015833914279937744
Batch 12/64 loss: 0.003484487533569336
Batch 13/64 loss: 0.026071786880493164
Batch 14/64 loss: 0.0040223002433776855
Batch 15/64 loss: 0.014094233512878418
Batch 16/64 loss: 0.021141111850738525
Batch 17/64 loss: -0.004576683044433594
Batch 18/64 loss: 0.00899362564086914
Batch 19/64 loss: 0.030803799629211426
Batch 20/64 loss: 0.019944965839385986
Batch 21/64 loss: 0.022059202194213867
Batch 22/64 loss: 0.006685614585876465
Batch 23/64 loss: 0.016110599040985107
Batch 24/64 loss: -0.006716609001159668
Batch 25/64 loss: 0.01888442039489746
Batch 26/64 loss: -0.01110607385635376
Batch 27/64 loss: -0.0033376216888427734
Batch 28/64 loss: 0.016647815704345703
Batch 29/64 loss: -0.0009946823120117188
Batch 30/64 loss: 0.004374265670776367
Batch 31/64 loss: -0.000499427318572998
Batch 32/64 loss: 0.018749356269836426
Batch 33/64 loss: 0.016683101654052734
Batch 34/64 loss: 0.015079379081726074
Batch 35/64 loss: 0.0054931640625
Batch 36/64 loss: -0.01886892318725586
Batch 37/64 loss: 0.03650546073913574
Batch 38/64 loss: 0.029609322547912598
Batch 39/64 loss: 0.009208500385284424
Batch 40/64 loss: 0.015323936939239502
Batch 41/64 loss: -0.005879223346710205
Batch 42/64 loss: 0.0005023479461669922
Batch 43/64 loss: 0.0025482177734375
Batch 44/64 loss: -0.014272034168243408
Batch 45/64 loss: 0.014332890510559082
Batch 46/64 loss: 0.014736354351043701
Batch 47/64 loss: 0.02318859100341797
Batch 48/64 loss: -0.0027149319648742676
Batch 49/64 loss: -0.0006725788116455078
Batch 50/64 loss: 0.004178404808044434
Batch 51/64 loss: -0.003407001495361328
Batch 52/64 loss: 0.016839563846588135
Batch 53/64 loss: 0.0237615704536438
Batch 54/64 loss: 0.014910221099853516
Batch 55/64 loss: 0.011447906494140625
Batch 56/64 loss: 0.0061991214752197266
Batch 57/64 loss: 0.02972012758255005
Batch 58/64 loss: 0.02469050884246826
Batch 59/64 loss: 0.0005742907524108887
Batch 60/64 loss: 0.0026226043701171875
Batch 61/64 loss: 0.0037171244621276855
Batch 62/64 loss: -0.0004761219024658203
Batch 63/64 loss: 0.025252163410186768
Batch 64/64 loss: 0.020098507404327393
Epoch 107  Train loss: 0.009454461406258975  Val loss: 0.062617228612867
Epoch 108
-------------------------------
Batch 1/64 loss: 0.011060893535614014
Batch 2/64 loss: 0.001651763916015625
Batch 3/64 loss: -0.0064198970794677734
Batch 4/64 loss: 0.0022535324096679688
Batch 5/64 loss: -0.008952021598815918
Batch 6/64 loss: 0.042825162410736084
Batch 7/64 loss: 0.02343893051147461
Batch 8/64 loss: -0.005544841289520264
Batch 9/64 loss: 0.019303560256958008
Batch 10/64 loss: 0.008594155311584473
Batch 11/64 loss: 0.003111898899078369
Batch 12/64 loss: 0.024610459804534912
Batch 13/64 loss: 0.030019700527191162
Batch 14/64 loss: -0.005450069904327393
Batch 15/64 loss: 0.009665727615356445
Batch 16/64 loss: -0.005377590656280518
Batch 17/64 loss: -0.0014712214469909668
Batch 18/64 loss: 0.04480552673339844
Batch 19/64 loss: 0.013318777084350586
Batch 20/64 loss: 0.025504887104034424
Batch 21/64 loss: 0.015521883964538574
Batch 22/64 loss: -0.007962226867675781
Batch 23/64 loss: -0.011739909648895264
Batch 24/64 loss: 0.0061307549476623535
Batch 25/64 loss: 0.007422983646392822
Batch 26/64 loss: 0.010950207710266113
Batch 27/64 loss: 0.010160267353057861
Batch 28/64 loss: -0.010161042213439941
Batch 29/64 loss: 0.026908397674560547
Batch 30/64 loss: 0.012618064880371094
Batch 31/64 loss: 0.020745396614074707
Batch 32/64 loss: 0.02991640567779541
Batch 33/64 loss: 0.002307295799255371
Batch 34/64 loss: 0.015238523483276367
Batch 35/64 loss: 0.002799689769744873
Batch 36/64 loss: -0.007112324237823486
Batch 37/64 loss: 0.02283656597137451
Batch 38/64 loss: 0.02496933937072754
Batch 39/64 loss: 2.8848648071289062e-05
Batch 40/64 loss: -0.0013709664344787598
Batch 41/64 loss: 0.024849355220794678
Batch 42/64 loss: -0.0044214725494384766
Batch 43/64 loss: 0.014062821865081787
Batch 44/64 loss: 0.033690571784973145
Batch 45/64 loss: -0.002414882183074951
Batch 46/64 loss: -0.011894762516021729
Batch 47/64 loss: -0.004744410514831543
Batch 48/64 loss: 0.029980063438415527
Batch 49/64 loss: 0.0028536319732666016
Batch 50/64 loss: 0.007410585880279541
Batch 51/64 loss: 0.009276509284973145
Batch 52/64 loss: -0.013878941535949707
Batch 53/64 loss: 0.006035506725311279
Batch 54/64 loss: 0.016091585159301758
Batch 55/64 loss: 0.0012669563293457031
Batch 56/64 loss: 0.011642038822174072
Batch 57/64 loss: 0.02448958158493042
Batch 58/64 loss: 0.0006026029586791992
Batch 59/64 loss: -0.001970231533050537
Batch 60/64 loss: 0.03352165222167969
Batch 61/64 loss: 0.035377442836761475
Batch 62/64 loss: 0.0018761157989501953
Batch 63/64 loss: 0.00394439697265625
Batch 64/64 loss: 0.005564749240875244
Epoch 108  Train loss: 0.009709455218969607  Val loss: 0.06498904797629393
Epoch 109
-------------------------------
Batch 1/64 loss: 0.003457188606262207
Batch 2/64 loss: 0.007174193859100342
Batch 3/64 loss: 0.0362168550491333
Batch 4/64 loss: 0.006077408790588379
Batch 5/64 loss: 0.01171964406967163
Batch 6/64 loss: 0.0016351938247680664
Batch 7/64 loss: 0.028593003749847412
Batch 8/64 loss: 0.011211037635803223
Batch 9/64 loss: 0.02648472785949707
Batch 10/64 loss: 0.021274566650390625
Batch 11/64 loss: -0.004604458808898926
Batch 12/64 loss: 0.018370628356933594
Batch 13/64 loss: 0.019730567932128906
Batch 14/64 loss: 0.007485032081604004
Batch 15/64 loss: 0.025117456912994385
Batch 16/64 loss: -0.0003821849822998047
Batch 17/64 loss: 0.029154539108276367
Batch 18/64 loss: 0.020753085613250732
Batch 19/64 loss: -0.00431978702545166
Batch 20/64 loss: -0.009120643138885498
Batch 21/64 loss: -0.009656310081481934
Batch 22/64 loss: 0.011680185794830322
Batch 23/64 loss: 0.010696053504943848
Batch 24/64 loss: 0.0281752347946167
Batch 25/64 loss: -0.0016685128211975098
Batch 26/64 loss: 0.00822758674621582
Batch 27/64 loss: -0.019353806972503662
Batch 28/64 loss: 0.0026148557662963867
Batch 29/64 loss: 0.002186417579650879
Batch 30/64 loss: -0.00790095329284668
Batch 31/64 loss: -0.011759340763092041
Batch 32/64 loss: 0.01682257652282715
Batch 33/64 loss: -0.0009688138961791992
Batch 34/64 loss: -0.007083773612976074
Batch 35/64 loss: 0.03799080848693848
Batch 36/64 loss: 0.005287647247314453
Batch 37/64 loss: 0.0005274415016174316
Batch 38/64 loss: -0.007838845252990723
Batch 39/64 loss: 0.012615978717803955
Batch 40/64 loss: -0.0011512041091918945
Batch 41/64 loss: 0.010549545288085938
Batch 42/64 loss: 0.014791011810302734
Batch 43/64 loss: 0.005151331424713135
Batch 44/64 loss: 0.02214205265045166
Batch 45/64 loss: 0.005038797855377197
Batch 46/64 loss: -0.008270621299743652
Batch 47/64 loss: 0.03185272216796875
Batch 48/64 loss: 0.031585097312927246
Batch 49/64 loss: 0.00875699520111084
Batch 50/64 loss: -0.00418323278427124
Batch 51/64 loss: 0.003831624984741211
Batch 52/64 loss: 0.0008011460304260254
Batch 53/64 loss: 0.013191759586334229
Batch 54/64 loss: -0.01743793487548828
Batch 55/64 loss: 0.017577528953552246
Batch 56/64 loss: 0.004559040069580078
Batch 57/64 loss: -7.891654968261719e-05
Batch 58/64 loss: -0.0032318830490112305
Batch 59/64 loss: 0.0024573206901550293
Batch 60/64 loss: 0.00886315107345581
Batch 61/64 loss: 0.010809779167175293
Batch 62/64 loss: -0.00549006462097168
Batch 63/64 loss: 0.012490034103393555
Batch 64/64 loss: 0.010051429271697998
Epoch 109  Train loss: 0.007823782574896718  Val loss: 0.06273446726225496
Epoch 110
-------------------------------
Batch 1/64 loss: -0.0037047863006591797
Batch 2/64 loss: 0.030807077884674072
Batch 3/64 loss: -0.008014500141143799
Batch 4/64 loss: 0.026995420455932617
Batch 5/64 loss: 0.010293900966644287
Batch 6/64 loss: 0.0005643963813781738
Batch 7/64 loss: -0.015225768089294434
Batch 8/64 loss: -0.0033995509147644043
Batch 9/64 loss: 0.0031440258026123047
Batch 10/64 loss: -0.0038254261016845703
Batch 11/64 loss: -0.027061402797698975
Batch 12/64 loss: 0.004727959632873535
Batch 13/64 loss: 0.00035393238067626953
Batch 14/64 loss: -0.0024684667587280273
Batch 15/64 loss: 0.02628493309020996
Batch 16/64 loss: 0.008538484573364258
Batch 17/64 loss: -0.00036895275115966797
Batch 18/64 loss: -0.0224076509475708
Batch 19/64 loss: -0.011478722095489502
Batch 20/64 loss: 0.015181958675384521
Batch 21/64 loss: 0.011600852012634277
Batch 22/64 loss: 0.00848311185836792
Batch 23/64 loss: 0.018109798431396484
Batch 24/64 loss: 0.03704613447189331
Batch 25/64 loss: 0.015603959560394287
Batch 26/64 loss: 0.014284610748291016
Batch 27/64 loss: -0.012118339538574219
Batch 28/64 loss: -0.010053873062133789
Batch 29/64 loss: 0.0189286470413208
Batch 30/64 loss: 0.0036612749099731445
Batch 31/64 loss: 0.0014082789421081543
Batch 32/64 loss: 0.02510249614715576
Batch 33/64 loss: 0.0024431347846984863
Batch 34/64 loss: -0.021479010581970215
Batch 35/64 loss: -0.006886601448059082
Batch 36/64 loss: 0.002003788948059082
Batch 37/64 loss: 0.023033618927001953
Batch 38/64 loss: -0.008356034755706787
Batch 39/64 loss: -0.017862677574157715
Batch 40/64 loss: -0.0034215450286865234
Batch 41/64 loss: 0.021563053131103516
Batch 42/64 loss: 0.02998584508895874
Batch 43/64 loss: 0.0020526647567749023
Batch 44/64 loss: 0.028871893882751465
Batch 45/64 loss: 0.007393240928649902
Batch 46/64 loss: 0.013348221778869629
Batch 47/64 loss: -0.006764650344848633
Batch 48/64 loss: 0.013080954551696777
Batch 49/64 loss: 0.01385354995727539
Batch 50/64 loss: -0.0023079514503479004
Batch 51/64 loss: 0.01300954818725586
Batch 52/64 loss: 0.028906285762786865
Batch 53/64 loss: 0.027718663215637207
Batch 54/64 loss: 0.028955578804016113
Batch 55/64 loss: -0.0012419819831848145
Batch 56/64 loss: -0.005041241645812988
Batch 57/64 loss: -0.0030215978622436523
Batch 58/64 loss: 0.020032227039337158
Batch 59/64 loss: -0.001924753189086914
Batch 60/64 loss: 0.024472355842590332
Batch 61/64 loss: 0.00909280776977539
Batch 62/64 loss: 0.026899397373199463
Batch 63/64 loss: -0.005875587463378906
Batch 64/64 loss: 0.017432868480682373
Epoch 110  Train loss: 0.00669179079579372  Val loss: 0.06330754687286325
Epoch 111
-------------------------------
Batch 1/64 loss: -0.013558268547058105
Batch 2/64 loss: 0.010034263134002686
Batch 3/64 loss: 0.020139753818511963
Batch 4/64 loss: -0.02462822198867798
Batch 5/64 loss: 0.0197751522064209
Batch 6/64 loss: 0.0008774399757385254
Batch 7/64 loss: -0.009279370307922363
Batch 8/64 loss: 0.009983539581298828
Batch 9/64 loss: -0.009825527667999268
Batch 10/64 loss: -0.0011532306671142578
Batch 11/64 loss: -0.017957746982574463
Batch 12/64 loss: 0.029394447803497314
Batch 13/64 loss: 0.010976552963256836
Batch 14/64 loss: -0.017813146114349365
Batch 15/64 loss: -0.007266998291015625
Batch 16/64 loss: 0.0031328201293945312
Batch 17/64 loss: 0.009939312934875488
Batch 18/64 loss: 0.015436410903930664
Batch 19/64 loss: -0.01727461814880371
Batch 20/64 loss: -0.002236485481262207
Batch 21/64 loss: -0.004879415035247803
Batch 22/64 loss: 0.01869148015975952
Batch 23/64 loss: 0.014043271541595459
Batch 24/64 loss: 0.020679354667663574
Batch 25/64 loss: -0.007752656936645508
Batch 26/64 loss: -0.0048983097076416016
Batch 27/64 loss: 0.01218712329864502
Batch 28/64 loss: 0.027929306030273438
Batch 29/64 loss: 0.0031462907791137695
Batch 30/64 loss: 0.03589367866516113
Batch 31/64 loss: 0.022177815437316895
Batch 32/64 loss: -0.013078808784484863
Batch 33/64 loss: 0.014712929725646973
Batch 34/64 loss: 0.04228568077087402
Batch 35/64 loss: 0.02402442693710327
Batch 36/64 loss: 0.013395071029663086
Batch 37/64 loss: 0.006215572357177734
Batch 38/64 loss: 0.006199181079864502
Batch 39/64 loss: 0.021758079528808594
Batch 40/64 loss: 0.017104923725128174
Batch 41/64 loss: -0.008122384548187256
Batch 42/64 loss: 0.00881350040435791
Batch 43/64 loss: 0.0156707763671875
Batch 44/64 loss: 0.018311619758605957
Batch 45/64 loss: 0.016099095344543457
Batch 46/64 loss: -0.014658689498901367
Batch 47/64 loss: 0.002313852310180664
Batch 48/64 loss: 0.01571136713027954
Batch 49/64 loss: -0.004621565341949463
Batch 50/64 loss: 0.006275653839111328
Batch 51/64 loss: -0.00781935453414917
Batch 52/64 loss: 0.01037442684173584
Batch 53/64 loss: -0.010145068168640137
Batch 54/64 loss: -0.015235304832458496
Batch 55/64 loss: 0.006385087966918945
Batch 56/64 loss: 0.034930288791656494
Batch 57/64 loss: 0.009712517261505127
Batch 58/64 loss: -0.0179404616355896
Batch 59/64 loss: -0.007002294063568115
Batch 60/64 loss: 0.023837268352508545
Batch 61/64 loss: -0.017148613929748535
Batch 62/64 loss: 0.017370939254760742
Batch 63/64 loss: 0.014703154563903809
Batch 64/64 loss: -0.0005311369895935059
Epoch 111  Train loss: 0.005897231896718343  Val loss: 0.06388837713556192
Epoch 112
-------------------------------
Batch 1/64 loss: -0.011303424835205078
Batch 2/64 loss: 0.0002878904342651367
Batch 3/64 loss: 0.0014521479606628418
Batch 4/64 loss: 0.016087770462036133
Batch 5/64 loss: 0.01319211721420288
Batch 6/64 loss: -0.0011176466941833496
Batch 7/64 loss: 0.022710740566253662
Batch 8/64 loss: -0.0069280266761779785
Batch 9/64 loss: 0.027206361293792725
Batch 10/64 loss: -0.009291350841522217
Batch 11/64 loss: 0.022497057914733887
Batch 12/64 loss: 0.003055751323699951
Batch 13/64 loss: -0.0038647055625915527
Batch 14/64 loss: -0.022629618644714355
Batch 15/64 loss: 0.004326343536376953
Batch 16/64 loss: -0.0128898024559021
Batch 17/64 loss: 0.01368725299835205
Batch 18/64 loss: -0.0028696060180664062
Batch 19/64 loss: -0.011235415935516357
Batch 20/64 loss: 0.004454493522644043
Batch 21/64 loss: 0.0033311843872070312
Batch 22/64 loss: 0.0015758872032165527
Batch 23/64 loss: 0.020710110664367676
Batch 24/64 loss: -0.008879482746124268
Batch 25/64 loss: -0.014564275741577148
Batch 26/64 loss: 0.024502873420715332
Batch 27/64 loss: -0.0009508132934570312
Batch 28/64 loss: -0.002171337604522705
Batch 29/64 loss: -0.011112630367279053
Batch 30/64 loss: -0.00757908821105957
Batch 31/64 loss: 0.0073511600494384766
Batch 32/64 loss: 0.012671589851379395
Batch 33/64 loss: 0.016569316387176514
Batch 34/64 loss: 0.009324371814727783
Batch 35/64 loss: -0.005958497524261475
Batch 36/64 loss: 0.014310061931610107
Batch 37/64 loss: 0.005330801010131836
Batch 38/64 loss: 0.0073032379150390625
Batch 39/64 loss: -0.0014774799346923828
Batch 40/64 loss: 0.011307120323181152
Batch 41/64 loss: -0.004206061363220215
Batch 42/64 loss: -0.004649162292480469
Batch 43/64 loss: -0.003969669342041016
Batch 44/64 loss: 0.007114768028259277
Batch 45/64 loss: -0.02242177724838257
Batch 46/64 loss: -0.004782497882843018
Batch 47/64 loss: -0.02550184726715088
Batch 48/64 loss: 0.02566295862197876
Batch 49/64 loss: 0.015361785888671875
Batch 50/64 loss: 0.0009304285049438477
Batch 51/64 loss: 0.00751185417175293
Batch 52/64 loss: -0.0014137029647827148
Batch 53/64 loss: -6.794929504394531e-05
Batch 54/64 loss: -0.006599545478820801
Batch 55/64 loss: 0.027650177478790283
Batch 56/64 loss: 0.011553943157196045
Batch 57/64 loss: 0.02775716781616211
Batch 58/64 loss: 0.0008415579795837402
Batch 59/64 loss: 0.01742565631866455
Batch 60/64 loss: 0.019353866577148438
Batch 61/64 loss: 0.01618218421936035
Batch 62/64 loss: 0.015042901039123535
Batch 63/64 loss: 0.00808173418045044
Batch 64/64 loss: 0.04428797960281372
Epoch 112  Train loss: 0.004525446190553553  Val loss: 0.05975369550927808
Epoch 113
-------------------------------
Batch 1/64 loss: 0.010271251201629639
Batch 2/64 loss: 0.009263336658477783
Batch 3/64 loss: 0.022862255573272705
Batch 4/64 loss: -0.014240622520446777
Batch 5/64 loss: -0.0010300874710083008
Batch 6/64 loss: 0.009032487869262695
Batch 7/64 loss: -0.0012764334678649902
Batch 8/64 loss: -0.005525529384613037
Batch 9/64 loss: 0.008174002170562744
Batch 10/64 loss: 0.002925872802734375
Batch 11/64 loss: -0.019838690757751465
Batch 12/64 loss: 0.008050024509429932
Batch 13/64 loss: -0.007449686527252197
Batch 14/64 loss: -0.0109025239944458
Batch 15/64 loss: -0.0040239691734313965
Batch 16/64 loss: 0.009196221828460693
Batch 17/64 loss: -0.0006820559501647949
Batch 18/64 loss: -0.006538093090057373
Batch 19/64 loss: 0.0031881332397460938
Batch 20/64 loss: 0.015805602073669434
Batch 21/64 loss: -0.011167168617248535
Batch 22/64 loss: 0.00569075345993042
Batch 23/64 loss: 0.0063152313232421875
Batch 24/64 loss: 0.02954387664794922
Batch 25/64 loss: -0.011732101440429688
Batch 26/64 loss: 0.01977384090423584
Batch 27/64 loss: 0.00667804479598999
Batch 28/64 loss: -0.0012650489807128906
Batch 29/64 loss: 0.010330736637115479
Batch 30/64 loss: -0.013718247413635254
Batch 31/64 loss: -0.022809326648712158
Batch 32/64 loss: 0.02517092227935791
Batch 33/64 loss: 0.020556628704071045
Batch 34/64 loss: 0.01850217580795288
Batch 35/64 loss: 0.0043291449546813965
Batch 36/64 loss: -0.010078608989715576
Batch 37/64 loss: -0.016176342964172363
Batch 38/64 loss: 0.0025089383125305176
Batch 39/64 loss: 0.0317724347114563
Batch 40/64 loss: 0.014636397361755371
Batch 41/64 loss: -0.004291713237762451
Batch 42/64 loss: 0.028104126453399658
Batch 43/64 loss: -0.026387274265289307
Batch 44/64 loss: 0.01658177375793457
Batch 45/64 loss: 0.0024930238723754883
Batch 46/64 loss: 0.019965648651123047
Batch 47/64 loss: -0.0025348663330078125
Batch 48/64 loss: -0.014107465744018555
Batch 49/64 loss: 0.021589577198028564
Batch 50/64 loss: 0.007721662521362305
Batch 51/64 loss: 0.022200405597686768
Batch 52/64 loss: -0.003249943256378174
Batch 53/64 loss: 0.023000597953796387
Batch 54/64 loss: 0.0202673077583313
Batch 55/64 loss: 0.005488157272338867
Batch 56/64 loss: -0.003921866416931152
Batch 57/64 loss: 0.004187464714050293
Batch 58/64 loss: -0.006164669990539551
Batch 59/64 loss: -0.009189844131469727
Batch 60/64 loss: 0.004794895648956299
Batch 61/64 loss: -0.007849156856536865
Batch 62/64 loss: 0.01631265878677368
Batch 63/64 loss: 0.010248124599456787
Batch 64/64 loss: -0.007574796676635742
Epoch 113  Train loss: 0.004011000838934207  Val loss: 0.059929610322840846
Epoch 114
-------------------------------
Batch 1/64 loss: 0.007502198219299316
Batch 2/64 loss: -0.0020052194595336914
Batch 3/64 loss: 0.03572261333465576
Batch 4/64 loss: -0.006055712699890137
Batch 5/64 loss: -0.022939443588256836
Batch 6/64 loss: -0.004210114479064941
Batch 7/64 loss: -0.004288613796234131
Batch 8/64 loss: -0.02406543493270874
Batch 9/64 loss: -0.0012142062187194824
Batch 10/64 loss: -0.00027817487716674805
Batch 11/64 loss: 0.011362075805664062
Batch 12/64 loss: 0.01491326093673706
Batch 13/64 loss: 0.01274418830871582
Batch 14/64 loss: -0.006944715976715088
Batch 15/64 loss: 0.016347169876098633
Batch 16/64 loss: -0.01212167739868164
Batch 17/64 loss: 0.005566775798797607
Batch 18/64 loss: 0.00017344951629638672
Batch 19/64 loss: -0.01161414384841919
Batch 20/64 loss: 0.018665194511413574
Batch 21/64 loss: 0.01704871654510498
Batch 22/64 loss: -0.014032602310180664
Batch 23/64 loss: 0.01939094066619873
Batch 24/64 loss: -0.0043051838874816895
Batch 25/64 loss: 0.0023320913314819336
Batch 26/64 loss: -0.01176750659942627
Batch 27/64 loss: -0.007908046245574951
Batch 28/64 loss: 0.0038070082664489746
Batch 29/64 loss: 0.0035149455070495605
Batch 30/64 loss: -0.011089444160461426
Batch 31/64 loss: 0.0035382509231567383
Batch 32/64 loss: 0.0027695894241333008
Batch 33/64 loss: -0.012477695941925049
Batch 34/64 loss: -0.019904732704162598
Batch 35/64 loss: -0.003697991371154785
Batch 36/64 loss: -0.007720291614532471
Batch 37/64 loss: 0.011695146560668945
Batch 38/64 loss: -0.0077664852142333984
Batch 39/64 loss: -0.0003165006637573242
Batch 40/64 loss: 0.00897204875946045
Batch 41/64 loss: -0.00780034065246582
Batch 42/64 loss: 0.01904386281967163
Batch 43/64 loss: 0.00245589017868042
Batch 44/64 loss: -0.010580182075500488
Batch 45/64 loss: 0.008244335651397705
Batch 46/64 loss: -0.013224482536315918
Batch 47/64 loss: 0.019917786121368408
Batch 48/64 loss: -0.0074773430824279785
Batch 49/64 loss: 0.0039771199226379395
Batch 50/64 loss: 0.0020333528518676758
Batch 51/64 loss: 0.020607709884643555
Batch 52/64 loss: -0.02119290828704834
Batch 53/64 loss: -0.010029315948486328
Batch 54/64 loss: -0.00033473968505859375
Batch 55/64 loss: 0.008402228355407715
Batch 56/64 loss: 0.0008039474487304688
Batch 57/64 loss: 0.02823960781097412
Batch 58/64 loss: -0.0022599101066589355
Batch 59/64 loss: 0.005867123603820801
Batch 60/64 loss: 0.005289196968078613
Batch 61/64 loss: 0.019007861614227295
Batch 62/64 loss: -0.011548757553100586
Batch 63/64 loss: 0.012207984924316406
Batch 64/64 loss: 0.03150862455368042
Epoch 114  Train loss: 0.0014842858501509125  Val loss: 0.062472124894460045
Epoch 115
-------------------------------
Batch 1/64 loss: -0.01028674840927124
Batch 2/64 loss: 0.026044607162475586
Batch 3/64 loss: -0.015462696552276611
Batch 4/64 loss: 0.00675046443939209
Batch 5/64 loss: -0.010416388511657715
Batch 6/64 loss: -0.006600677967071533
Batch 7/64 loss: -0.006211578845977783
Batch 8/64 loss: 0.0016559362411499023
Batch 9/64 loss: -0.02086043357849121
Batch 10/64 loss: 0.00919497013092041
Batch 11/64 loss: 0.03350704908370972
Batch 12/64 loss: 0.0015682578086853027
Batch 13/64 loss: -0.006386399269104004
Batch 14/64 loss: 0.018946528434753418
Batch 15/64 loss: 0.02028578519821167
Batch 16/64 loss: -0.018153667449951172
Batch 17/64 loss: -0.00041353702545166016
Batch 18/64 loss: -0.012149333953857422
Batch 19/64 loss: 0.0031462907791137695
Batch 20/64 loss: -0.006973683834075928
Batch 21/64 loss: -0.010976731777191162
Batch 22/64 loss: 0.011246919631958008
Batch 23/64 loss: -0.0056122541427612305
Batch 24/64 loss: -0.015099406242370605
Batch 25/64 loss: -0.00767672061920166
Batch 26/64 loss: 0.01343446969985962
Batch 27/64 loss: 0.003413081169128418
Batch 28/64 loss: 0.014005780220031738
Batch 29/64 loss: -0.007878124713897705
Batch 30/64 loss: 0.007795989513397217
Batch 31/64 loss: 0.021457195281982422
Batch 32/64 loss: 0.03950512409210205
Batch 33/64 loss: 0.004582762718200684
Batch 34/64 loss: 0.024547815322875977
Batch 35/64 loss: 0.01947033405303955
Batch 36/64 loss: 0.016895711421966553
Batch 37/64 loss: 0.015028655529022217
Batch 38/64 loss: 0.00964266061782837
Batch 39/64 loss: -0.024521827697753906
Batch 40/64 loss: -0.012058615684509277
Batch 41/64 loss: 0.02871394157409668
Batch 42/64 loss: -0.017304301261901855
Batch 43/64 loss: -0.010792255401611328
Batch 44/64 loss: -0.0022736191749572754
Batch 45/64 loss: 0.009187459945678711
Batch 46/64 loss: -0.0006771683692932129
Batch 47/64 loss: 0.014829397201538086
Batch 48/64 loss: -0.0011987686157226562
Batch 49/64 loss: 0.024394333362579346
Batch 50/64 loss: 0.008923113346099854
Batch 51/64 loss: 0.012808263301849365
Batch 52/64 loss: 0.005077362060546875
Batch 53/64 loss: 0.020244956016540527
Batch 54/64 loss: -0.014774441719055176
Batch 55/64 loss: 0.0017145276069641113
Batch 56/64 loss: 0.0014511346817016602
Batch 57/64 loss: 0.005939900875091553
Batch 58/64 loss: 0.0072190165519714355
Batch 59/64 loss: -0.0038574934005737305
Batch 60/64 loss: -0.0031118392944335938
Batch 61/64 loss: -0.0006000399589538574
Batch 62/64 loss: 0.017946064472198486
Batch 63/64 loss: -0.00906062126159668
Batch 64/64 loss: -0.00505375862121582
Epoch 115  Train loss: 0.003378763385847503  Val loss: 0.0655214299041381
Epoch 116
-------------------------------
Batch 1/64 loss: -0.003766775131225586
Batch 2/64 loss: 0.0018030405044555664
Batch 3/64 loss: -0.013235628604888916
Batch 4/64 loss: 0.005062460899353027
Batch 5/64 loss: -0.0010524392127990723
Batch 6/64 loss: 0.005886971950531006
Batch 7/64 loss: -0.017820119857788086
Batch 8/64 loss: -0.005671143531799316
Batch 9/64 loss: 0.017200469970703125
Batch 10/64 loss: 0.007456719875335693
Batch 11/64 loss: 0.014079689979553223
Batch 12/64 loss: -0.013272881507873535
Batch 13/64 loss: -0.019899368286132812
Batch 14/64 loss: -0.00813382863998413
Batch 15/64 loss: -0.0030096769332885742
Batch 16/64 loss: 0.023623168468475342
Batch 17/64 loss: 0.00342714786529541
Batch 18/64 loss: 0.025793135166168213
Batch 19/64 loss: 0.00843900442123413
Batch 20/64 loss: -0.027502000331878662
Batch 21/64 loss: 0.007945537567138672
Batch 22/64 loss: 0.038800716400146484
Batch 23/64 loss: -0.004391670227050781
Batch 24/64 loss: -6.568431854248047e-05
Batch 25/64 loss: -0.001964092254638672
Batch 26/64 loss: -0.006758511066436768
Batch 27/64 loss: -0.011507689952850342
Batch 28/64 loss: 0.0003097057342529297
Batch 29/64 loss: -0.002272486686706543
Batch 30/64 loss: -0.02255767583847046
Batch 31/64 loss: -0.017768144607543945
Batch 32/64 loss: -0.006279945373535156
Batch 33/64 loss: 0.020082533359527588
Batch 34/64 loss: -0.007808387279510498
Batch 35/64 loss: -0.014399409294128418
Batch 36/64 loss: -0.015987932682037354
Batch 37/64 loss: -0.007929027080535889
Batch 38/64 loss: -0.004985213279724121
Batch 39/64 loss: 0.0010919570922851562
Batch 40/64 loss: 0.04506218433380127
Batch 41/64 loss: -0.005008280277252197
Batch 42/64 loss: 0.009792625904083252
Batch 43/64 loss: 0.01283937692642212
Batch 44/64 loss: 0.015421390533447266
Batch 45/64 loss: 0.010953426361083984
Batch 46/64 loss: -0.005432248115539551
Batch 47/64 loss: -0.0025864839553833008
Batch 48/64 loss: 0.0012902617454528809
Batch 49/64 loss: -0.0009628534317016602
Batch 50/64 loss: -0.007906556129455566
Batch 51/64 loss: -0.006140470504760742
Batch 52/64 loss: -0.0013428926467895508
Batch 53/64 loss: 0.02028578519821167
Batch 54/64 loss: 0.004094541072845459
Batch 55/64 loss: -0.0032756924629211426
Batch 56/64 loss: 0.03113633394241333
Batch 57/64 loss: -0.0016236305236816406
Batch 58/64 loss: 0.020351529121398926
Batch 59/64 loss: -0.0003566741943359375
Batch 60/64 loss: 0.00414121150970459
Batch 61/64 loss: 0.016410231590270996
Batch 62/64 loss: 0.006333410739898682
Batch 63/64 loss: -0.0044525861740112305
Batch 64/64 loss: -0.026180922985076904
Epoch 116  Train loss: 0.0012917768721487008  Val loss: 0.05662161029900882
Saving best model, epoch: 116
Epoch 117
-------------------------------
Batch 1/64 loss: 0.0013599991798400879
Batch 2/64 loss: -0.009535670280456543
Batch 3/64 loss: -0.023954153060913086
Batch 4/64 loss: 0.009769916534423828
Batch 5/64 loss: 0.0036922097206115723
Batch 6/64 loss: 0.01135176420211792
Batch 7/64 loss: 0.006016254425048828
Batch 8/64 loss: -0.002906322479248047
Batch 9/64 loss: 0.007078886032104492
Batch 10/64 loss: 0.006479382514953613
Batch 11/64 loss: -0.011891841888427734
Batch 12/64 loss: -0.013798773288726807
Batch 13/64 loss: -0.016808152198791504
Batch 14/64 loss: -0.0027570128440856934
Batch 15/64 loss: 0.006726324558258057
Batch 16/64 loss: 0.027127504348754883
Batch 17/64 loss: -0.008869767189025879
Batch 18/64 loss: 0.006998538970947266
Batch 19/64 loss: 0.011179089546203613
Batch 20/64 loss: -0.028153419494628906
Batch 21/64 loss: 0.021829307079315186
Batch 22/64 loss: -0.0029148459434509277
Batch 23/64 loss: 0.019763946533203125
Batch 24/64 loss: -0.012021422386169434
Batch 25/64 loss: -0.007440388202667236
Batch 26/64 loss: -0.01248013973236084
Batch 27/64 loss: -0.003117084503173828
Batch 28/64 loss: 0.0013103485107421875
Batch 29/64 loss: 0.008792340755462646
Batch 30/64 loss: -0.0066928863525390625
Batch 31/64 loss: 0.005697011947631836
Batch 32/64 loss: 0.01611042022705078
Batch 33/64 loss: -0.0029942989349365234
Batch 34/64 loss: -0.006024181842803955
Batch 35/64 loss: 1.3113021850585938e-06
Batch 36/64 loss: 0.009523868560791016
Batch 37/64 loss: -0.009795904159545898
Batch 38/64 loss: -0.0007551312446594238
Batch 39/64 loss: 0.041837990283966064
Batch 40/64 loss: 0.0004209280014038086
Batch 41/64 loss: -0.009167313575744629
Batch 42/64 loss: -0.011408448219299316
Batch 43/64 loss: -0.012358367443084717
Batch 44/64 loss: 0.006394624710083008
Batch 45/64 loss: -0.013522088527679443
Batch 46/64 loss: -0.01517629623413086
Batch 47/64 loss: 0.009235203266143799
Batch 48/64 loss: 0.007276296615600586
Batch 49/64 loss: -0.019648373126983643
Batch 50/64 loss: -0.004627704620361328
Batch 51/64 loss: 0.01941382884979248
Batch 52/64 loss: 0.009808242321014404
Batch 53/64 loss: -0.014348089694976807
Batch 54/64 loss: -0.004963397979736328
Batch 55/64 loss: -0.0050574541091918945
Batch 56/64 loss: 0.001385033130645752
Batch 57/64 loss: 0.027160346508026123
Batch 58/64 loss: 0.006626307964324951
Batch 59/64 loss: -0.011571705341339111
Batch 60/64 loss: 0.015474438667297363
Batch 61/64 loss: -0.017645657062530518
Batch 62/64 loss: 0.021877825260162354
Batch 63/64 loss: 0.010483801364898682
Batch 64/64 loss: -0.02163773775100708
Epoch 117  Train loss: 0.0003069599469502767  Val loss: 0.055100522704960145
Saving best model, epoch: 117
Epoch 118
-------------------------------
Batch 1/64 loss: -0.005169510841369629
Batch 2/64 loss: 0.011397957801818848
Batch 3/64 loss: -0.00308990478515625
Batch 4/64 loss: 0.0046656131744384766
Batch 5/64 loss: 0.026808857917785645
Batch 6/64 loss: 0.0008056759834289551
Batch 7/64 loss: 0.026271581649780273
Batch 8/64 loss: -0.0036274194717407227
Batch 9/64 loss: -0.023554325103759766
Batch 10/64 loss: -0.0017821192741394043
Batch 11/64 loss: 0.00251924991607666
Batch 12/64 loss: 0.005664706230163574
Batch 13/64 loss: -0.008302569389343262
Batch 14/64 loss: 0.0019528865814208984
Batch 15/64 loss: 0.0043038129806518555
Batch 16/64 loss: 0.0022531747817993164
Batch 17/64 loss: -0.009321093559265137
Batch 18/64 loss: -0.006999194622039795
Batch 19/64 loss: 0.01441490650177002
Batch 20/64 loss: 0.028233349323272705
Batch 21/64 loss: 0.026704907417297363
Batch 22/64 loss: -0.011974334716796875
Batch 23/64 loss: 0.0005395412445068359
Batch 24/64 loss: 0.008198142051696777
Batch 25/64 loss: -0.014307558536529541
Batch 26/64 loss: 0.026351451873779297
Batch 27/64 loss: -0.022953510284423828
Batch 28/64 loss: -0.004559934139251709
Batch 29/64 loss: 0.00023990869522094727
Batch 30/64 loss: 0.04745358228683472
Batch 31/64 loss: 0.012604236602783203
Batch 32/64 loss: 0.004773557186126709
Batch 33/64 loss: 0.0587693452835083
Batch 34/64 loss: -0.005116164684295654
Batch 35/64 loss: 0.033375680446624756
Batch 36/64 loss: -0.0047182440757751465
Batch 37/64 loss: -0.02106642723083496
Batch 38/64 loss: 0.004003584384918213
Batch 39/64 loss: -0.004416465759277344
Batch 40/64 loss: -0.0069803595542907715
Batch 41/64 loss: 0.02110922336578369
Batch 42/64 loss: -0.0031408071517944336
Batch 43/64 loss: -0.0008167028427124023
Batch 44/64 loss: 0.02071547508239746
Batch 45/64 loss: 0.0021547675132751465
Batch 46/64 loss: 0.007508397102355957
Batch 47/64 loss: -0.010645806789398193
Batch 48/64 loss: -0.011416018009185791
Batch 49/64 loss: -0.007927775382995605
Batch 50/64 loss: -0.009372889995574951
Batch 51/64 loss: -0.009252071380615234
Batch 52/64 loss: -0.015790879726409912
Batch 53/64 loss: 0.010082244873046875
Batch 54/64 loss: 0.015549182891845703
Batch 55/64 loss: 0.008427023887634277
Batch 56/64 loss: 0.0015886425971984863
Batch 57/64 loss: -0.016991019248962402
Batch 58/64 loss: 0.02528989315032959
Batch 59/64 loss: 0.0035376548767089844
Batch 60/64 loss: -0.0031478404998779297
Batch 61/64 loss: -0.007332980632781982
Batch 62/64 loss: -0.009212970733642578
Batch 63/64 loss: 0.00870215892791748
Batch 64/64 loss: -0.016311347484588623
Epoch 118  Train loss: 0.003164705341937495  Val loss: 0.056089080076447057
Epoch 119
-------------------------------
Batch 1/64 loss: 0.0031238794326782227
Batch 2/64 loss: -0.00905764102935791
Batch 3/64 loss: 0.0014379024505615234
Batch 4/64 loss: -0.010126948356628418
Batch 5/64 loss: -0.0147208571434021
Batch 6/64 loss: 0.0037536025047302246
Batch 7/64 loss: -0.021322011947631836
Batch 8/64 loss: -0.002790689468383789
Batch 9/64 loss: -0.012878775596618652
Batch 10/64 loss: 0.00015598535537719727
Batch 11/64 loss: 0.02110886573791504
Batch 12/64 loss: 0.000666499137878418
Batch 13/64 loss: -0.016173958778381348
Batch 14/64 loss: -0.009124279022216797
Batch 15/64 loss: 0.01562201976776123
Batch 16/64 loss: 0.0076642632484436035
Batch 17/64 loss: 0.014855742454528809
Batch 18/64 loss: 0.0028821825981140137
Batch 19/64 loss: -0.016950607299804688
Batch 20/64 loss: -0.01748347282409668
Batch 21/64 loss: 0.01304161548614502
Batch 22/64 loss: -0.02270418405532837
Batch 23/64 loss: -0.01594012975692749
Batch 24/64 loss: -0.008853435516357422
Batch 25/64 loss: -0.012145519256591797
Batch 26/64 loss: -0.022740423679351807
Batch 27/64 loss: -0.009647130966186523
Batch 28/64 loss: -0.010578036308288574
Batch 29/64 loss: 0.0026551485061645508
Batch 30/64 loss: -0.0008046627044677734
Batch 31/64 loss: 0.00873631238937378
Batch 32/64 loss: 0.02717036008834839
Batch 33/64 loss: -0.005114912986755371
Batch 34/64 loss: -0.005478560924530029
Batch 35/64 loss: -0.011575102806091309
Batch 36/64 loss: 0.02027827501296997
Batch 37/64 loss: 0.012852191925048828
Batch 38/64 loss: 0.02169853448867798
Batch 39/64 loss: 0.02026665210723877
Batch 40/64 loss: 0.02264404296875
Batch 41/64 loss: 0.005481541156768799
Batch 42/64 loss: -0.00128173828125
Batch 43/64 loss: -0.023350656032562256
Batch 44/64 loss: 0.0025516152381896973
Batch 45/64 loss: 0.01712965965270996
Batch 46/64 loss: 0.0021005868911743164
Batch 47/64 loss: 0.01111757755279541
Batch 48/64 loss: 0.007153749465942383
Batch 49/64 loss: -0.004288911819458008
Batch 50/64 loss: 0.0014740228652954102
Batch 51/64 loss: -0.005018174648284912
Batch 52/64 loss: 0.004264235496520996
Batch 53/64 loss: 0.009238898754119873
Batch 54/64 loss: 0.009503722190856934
Batch 55/64 loss: -0.0035886168479919434
Batch 56/64 loss: -0.0029579997062683105
Batch 57/64 loss: -0.005767881870269775
Batch 58/64 loss: 0.021564483642578125
Batch 59/64 loss: -0.0023885369300842285
Batch 60/64 loss: 0.01290208101272583
Batch 61/64 loss: -0.01402050256729126
Batch 62/64 loss: 0.008099019527435303
Batch 63/64 loss: -0.02285975217819214
Batch 64/64 loss: -0.00025206804275512695
Epoch 119  Train loss: -0.00013690812914979224  Val loss: 0.05699880098559193
Epoch 120
-------------------------------
Batch 1/64 loss: -0.007286489009857178
Batch 2/64 loss: -0.018500149250030518
Batch 3/64 loss: 0.004826188087463379
Batch 4/64 loss: -0.037055790424346924
Batch 5/64 loss: 0.01412808895111084
Batch 6/64 loss: -0.007885396480560303
Batch 7/64 loss: 0.01163417100906372
Batch 8/64 loss: 0.018996477127075195
Batch 9/64 loss: 0.0016425848007202148
Batch 10/64 loss: 0.004282236099243164
Batch 11/64 loss: -0.0018521547317504883
Batch 12/64 loss: -0.015492081642150879
Batch 13/64 loss: -0.005795121192932129
Batch 14/64 loss: -0.01823633909225464
Batch 15/64 loss: 0.00999462604522705
Batch 16/64 loss: -0.019709885120391846
Batch 17/64 loss: -0.007813453674316406
Batch 18/64 loss: -0.0031568408012390137
Batch 19/64 loss: 0.0019276142120361328
Batch 20/64 loss: -0.012107253074645996
Batch 21/64 loss: -0.007500588893890381
Batch 22/64 loss: -0.0005881190299987793
Batch 23/64 loss: 0.018114924430847168
Batch 24/64 loss: 0.006632685661315918
Batch 25/64 loss: -0.009773850440979004
Batch 26/64 loss: -0.01667994260787964
Batch 27/64 loss: -0.009492099285125732
Batch 28/64 loss: -0.00393301248550415
Batch 29/64 loss: -0.021475493907928467
Batch 30/64 loss: 0.03639626502990723
Batch 31/64 loss: 0.004800379276275635
Batch 32/64 loss: -0.012553095817565918
Batch 33/64 loss: 0.0019137859344482422
Batch 34/64 loss: -0.018527746200561523
Batch 35/64 loss: 0.007075965404510498
Batch 36/64 loss: 0.020027875900268555
Batch 37/64 loss: 0.0037611722946166992
Batch 38/64 loss: 0.011123478412628174
Batch 39/64 loss: -0.00845038890838623
Batch 40/64 loss: -0.019895315170288086
Batch 41/64 loss: -0.0052825212478637695
Batch 42/64 loss: 0.0038406848907470703
Batch 43/64 loss: 0.015264034271240234
Batch 44/64 loss: 0.012056350708007812
Batch 45/64 loss: 0.018772006034851074
Batch 46/64 loss: -0.01775979995727539
Batch 47/64 loss: -0.0037736892700195312
Batch 48/64 loss: -0.0003898739814758301
Batch 49/64 loss: 0.001955866813659668
Batch 50/64 loss: 0.02330923080444336
Batch 51/64 loss: 0.0023049116134643555
Batch 52/64 loss: 0.018616676330566406
Batch 53/64 loss: -0.0024771690368652344
Batch 54/64 loss: -0.007464051246643066
Batch 55/64 loss: -0.004560589790344238
Batch 56/64 loss: -0.008619844913482666
Batch 57/64 loss: 0.0004476308822631836
Batch 58/64 loss: -0.006567180156707764
Batch 59/64 loss: -0.0020053982734680176
Batch 60/64 loss: -0.004076242446899414
Batch 61/64 loss: 0.023171305656433105
Batch 62/64 loss: -0.003000915050506592
Batch 63/64 loss: -0.0017715692520141602
Batch 64/64 loss: 0.002542257308959961
Epoch 120  Train loss: -0.0008248712502273859  Val loss: 0.059027405538919456
Epoch 121
-------------------------------
Batch 1/64 loss: -0.005761265754699707
Batch 2/64 loss: 0.008406281471252441
Batch 3/64 loss: -0.012552917003631592
Batch 4/64 loss: -0.00337827205657959
Batch 5/64 loss: -0.005905747413635254
Batch 6/64 loss: 0.004302859306335449
Batch 7/64 loss: -0.018413782119750977
Batch 8/64 loss: -0.013154268264770508
Batch 9/64 loss: -0.014801561832427979
Batch 10/64 loss: 0.006721854209899902
Batch 11/64 loss: -0.0001932978630065918
Batch 12/64 loss: -0.01150047779083252
Batch 13/64 loss: -0.016721606254577637
Batch 14/64 loss: -0.010135233402252197
Batch 15/64 loss: -0.014451026916503906
Batch 16/64 loss: -0.016344785690307617
Batch 17/64 loss: -0.006680786609649658
Batch 18/64 loss: 0.004494428634643555
Batch 19/64 loss: 0.012576282024383545
Batch 20/64 loss: 0.005260288715362549
Batch 21/64 loss: 0.0010372400283813477
Batch 22/64 loss: -0.0075958967208862305
Batch 23/64 loss: -0.03398007154464722
Batch 24/64 loss: -0.001961052417755127
Batch 25/64 loss: -0.0035052895545959473
Batch 26/64 loss: -0.004312753677368164
Batch 27/64 loss: 0.005098879337310791
Batch 28/64 loss: 0.02041327953338623
Batch 29/64 loss: -0.005106091499328613
Batch 30/64 loss: 0.0009084343910217285
Batch 31/64 loss: 0.012467920780181885
Batch 32/64 loss: -0.0022715330123901367
Batch 33/64 loss: 0.011998355388641357
Batch 34/64 loss: -0.00041985511779785156
Batch 35/64 loss: 0.013101577758789062
Batch 36/64 loss: 0.013285696506500244
Batch 37/64 loss: -0.011828184127807617
Batch 38/64 loss: -0.0010544657707214355
Batch 39/64 loss: -0.0010302066802978516
Batch 40/64 loss: -0.0016492009162902832
Batch 41/64 loss: -0.005256175994873047
Batch 42/64 loss: 0.0006293058395385742
Batch 43/64 loss: -0.010698795318603516
Batch 44/64 loss: -0.006040632724761963
Batch 45/64 loss: 0.008684992790222168
Batch 46/64 loss: 0.0034589767456054688
Batch 47/64 loss: -0.004753053188323975
Batch 48/64 loss: 0.0032597780227661133
Batch 49/64 loss: 0.0007385015487670898
Batch 50/64 loss: 0.02204829454421997
Batch 51/64 loss: 0.009643256664276123
Batch 52/64 loss: 0.02411627769470215
Batch 53/64 loss: 0.027221381664276123
Batch 54/64 loss: -0.0077054500579833984
Batch 55/64 loss: -0.0024343132972717285
Batch 56/64 loss: 0.0057822465896606445
Batch 57/64 loss: 0.025700151920318604
Batch 58/64 loss: 0.0032660365104675293
Batch 59/64 loss: -0.012928903102874756
Batch 60/64 loss: -0.0030978918075561523
Batch 61/64 loss: 0.0134810209274292
Batch 62/64 loss: 0.009268999099731445
Batch 63/64 loss: -0.012952089309692383
Batch 64/64 loss: -0.010864794254302979
Epoch 121  Train loss: -0.0003349479506997501  Val loss: 0.058654985067361
Epoch 122
-------------------------------
Batch 1/64 loss: 0.00866711139678955
Batch 2/64 loss: 0.0070157647132873535
Batch 3/64 loss: -0.020590126514434814
Batch 4/64 loss: -0.003840029239654541
Batch 5/64 loss: 0.0047719478607177734
Batch 6/64 loss: 0.00013756752014160156
Batch 7/64 loss: -0.0036647915840148926
Batch 8/64 loss: 0.03003484010696411
Batch 9/64 loss: -0.006629765033721924
Batch 10/64 loss: -0.008555710315704346
Batch 11/64 loss: -0.006248056888580322
Batch 12/64 loss: -0.008567869663238525
Batch 13/64 loss: 0.018165528774261475
Batch 14/64 loss: -0.00960087776184082
Batch 15/64 loss: -0.012735724449157715
Batch 16/64 loss: 0.0013290643692016602
Batch 17/64 loss: 0.019039392471313477
Batch 18/64 loss: 0.0006518959999084473
Batch 19/64 loss: -0.03164529800415039
Batch 20/64 loss: -0.0240287184715271
Batch 21/64 loss: -0.014581918716430664
Batch 22/64 loss: -0.0011574625968933105
Batch 23/64 loss: -0.005538344383239746
Batch 24/64 loss: 0.00551915168762207
Batch 25/64 loss: 0.00862109661102295
Batch 26/64 loss: -0.014737546443939209
Batch 27/64 loss: 0.002101898193359375
Batch 28/64 loss: -0.0003496408462524414
Batch 29/64 loss: -0.020005643367767334
Batch 30/64 loss: 0.008957207202911377
Batch 31/64 loss: 0.003741025924682617
Batch 32/64 loss: -0.004965484142303467
Batch 33/64 loss: 0.019412100315093994
Batch 34/64 loss: 0.0032141804695129395
Batch 35/64 loss: -0.027184605598449707
Batch 36/64 loss: -0.008590281009674072
Batch 37/64 loss: 0.006830453872680664
Batch 38/64 loss: -0.004953265190124512
Batch 39/64 loss: -0.00209963321685791
Batch 40/64 loss: -0.0016274452209472656
Batch 41/64 loss: -0.022524237632751465
Batch 42/64 loss: 0.021235227584838867
Batch 43/64 loss: -0.010515749454498291
Batch 44/64 loss: -0.014996469020843506
Batch 45/64 loss: -0.034758567810058594
Batch 46/64 loss: -0.009482324123382568
Batch 47/64 loss: 0.022595345973968506
Batch 48/64 loss: -0.022505462169647217
Batch 49/64 loss: 0.017046451568603516
Batch 50/64 loss: 0.0030661821365356445
Batch 51/64 loss: 0.010536909103393555
Batch 52/64 loss: -0.025781214237213135
Batch 53/64 loss: 0.009532690048217773
Batch 54/64 loss: -0.012279987335205078
Batch 55/64 loss: -0.034538984298706055
Batch 56/64 loss: 0.014335393905639648
Batch 57/64 loss: 0.0008039474487304688
Batch 58/64 loss: 0.027411580085754395
Batch 59/64 loss: 0.030137062072753906
Batch 60/64 loss: 0.0024200081825256348
Batch 61/64 loss: 0.04330778121948242
Batch 62/64 loss: -0.0005831718444824219
Batch 63/64 loss: -0.017852783203125
Batch 64/64 loss: -0.0018576383590698242
Epoch 122  Train loss: -0.001544652733148313  Val loss: 0.05669120230625585
Epoch 123
-------------------------------
Batch 1/64 loss: -0.008762359619140625
Batch 2/64 loss: 0.019343852996826172
Batch 3/64 loss: -0.007062733173370361
Batch 4/64 loss: 0.016614019870758057
Batch 5/64 loss: -0.009576082229614258
Batch 6/64 loss: 0.017409563064575195
Batch 7/64 loss: -0.0033954381942749023
Batch 8/64 loss: 0.002904653549194336
Batch 9/64 loss: 0.007243990898132324
Batch 10/64 loss: 0.0029281973838806152
Batch 11/64 loss: -0.03053724765777588
Batch 12/64 loss: -0.0058953166007995605
Batch 13/64 loss: 0.012324154376983643
Batch 14/64 loss: -0.02361243963241577
Batch 15/64 loss: -0.003043532371520996
Batch 16/64 loss: -0.0017080903053283691
Batch 17/64 loss: 0.010316967964172363
Batch 18/64 loss: -0.014164328575134277
Batch 19/64 loss: 0.00041925907135009766
Batch 20/64 loss: -0.028718888759613037
Batch 21/64 loss: 0.0008543729782104492
Batch 22/64 loss: 0.006404101848602295
Batch 23/64 loss: 0.003168165683746338
Batch 24/64 loss: 0.007909119129180908
Batch 25/64 loss: 0.014586031436920166
Batch 26/64 loss: 0.017847657203674316
Batch 27/64 loss: 0.011159300804138184
Batch 28/64 loss: -0.020922839641571045
Batch 29/64 loss: -0.01687932014465332
Batch 30/64 loss: 0.003641366958618164
Batch 31/64 loss: -0.023796439170837402
Batch 32/64 loss: 0.0070792436599731445
Batch 33/64 loss: -0.01443701982498169
Batch 34/64 loss: -0.0009490847587585449
Batch 35/64 loss: -0.025414228439331055
Batch 36/64 loss: -0.0061092376708984375
Batch 37/64 loss: -0.015667319297790527
Batch 38/64 loss: 0.006580233573913574
Batch 39/64 loss: -0.007597804069519043
Batch 40/64 loss: -0.008174479007720947
Batch 41/64 loss: -0.017948269844055176
Batch 42/64 loss: 0.004387974739074707
Batch 43/64 loss: 0.03780907392501831
Batch 44/64 loss: -0.01777052879333496
Batch 45/64 loss: -0.010715246200561523
Batch 46/64 loss: -0.011270403861999512
Batch 47/64 loss: -0.006228089332580566
Batch 48/64 loss: 0.016619086265563965
Batch 49/64 loss: -0.005278050899505615
Batch 50/64 loss: -0.017113327980041504
Batch 51/64 loss: -0.017493069171905518
Batch 52/64 loss: -0.01276928186416626
Batch 53/64 loss: 0.004865288734436035
Batch 54/64 loss: -0.00487285852432251
Batch 55/64 loss: -0.005909562110900879
Batch 56/64 loss: -0.00894308090209961
Batch 57/64 loss: 0.021846473217010498
Batch 58/64 loss: 0.002950429916381836
Batch 59/64 loss: -0.021887660026550293
Batch 60/64 loss: 0.0002345442771911621
Batch 61/64 loss: 0.005656719207763672
Batch 62/64 loss: -0.003078281879425049
Batch 63/64 loss: 0.0040912628173828125
Batch 64/64 loss: -0.0028420090675354004
Epoch 123  Train loss: -0.0027080524201486627  Val loss: 0.0572296069659728
Epoch 124
-------------------------------
Batch 1/64 loss: -0.025933504104614258
Batch 2/64 loss: 0.0003018379211425781
Batch 3/64 loss: -0.013431251049041748
Batch 4/64 loss: -0.009573221206665039
Batch 5/64 loss: -0.010158121585845947
Batch 6/64 loss: 0.00687021017074585
Batch 7/64 loss: -0.009858012199401855
Batch 8/64 loss: -0.02892923355102539
Batch 9/64 loss: -0.004618525505065918
Batch 10/64 loss: 0.013812363147735596
Batch 11/64 loss: -0.018487989902496338
Batch 12/64 loss: -0.009089052677154541
Batch 13/64 loss: -0.001964569091796875
Batch 14/64 loss: -0.006108641624450684
Batch 15/64 loss: 0.0048754215240478516
Batch 16/64 loss: -0.006509244441986084
Batch 17/64 loss: -0.0240403413772583
Batch 18/64 loss: -0.005255460739135742
Batch 19/64 loss: -0.0240323543548584
Batch 20/64 loss: 0.009924829006195068
Batch 21/64 loss: -0.0014184117317199707
Batch 22/64 loss: 0.0031464695930480957
Batch 23/64 loss: 0.021546661853790283
Batch 24/64 loss: -0.013331711292266846
Batch 25/64 loss: -0.0017145872116088867
Batch 26/64 loss: 0.004464626312255859
Batch 27/64 loss: 0.03928542137145996
Batch 28/64 loss: 0.012451648712158203
Batch 29/64 loss: 0.0004767179489135742
Batch 30/64 loss: -0.005095243453979492
Batch 31/64 loss: -0.020902395248413086
Batch 32/64 loss: -0.013540029525756836
Batch 33/64 loss: -0.011879682540893555
Batch 34/64 loss: 0.020318984985351562
Batch 35/64 loss: -0.0008467435836791992
Batch 36/64 loss: -0.0038202404975891113
Batch 37/64 loss: -0.02456909418106079
Batch 38/64 loss: -0.02415609359741211
Batch 39/64 loss: -0.012701928615570068
Batch 40/64 loss: -0.0046308040618896484
Batch 41/64 loss: -0.018402159214019775
Batch 42/64 loss: 0.012461364269256592
Batch 43/64 loss: -0.008139610290527344
Batch 44/64 loss: -0.00896388292312622
Batch 45/64 loss: -0.006049692630767822
Batch 46/64 loss: -0.022831261157989502
Batch 47/64 loss: 0.02618539333343506
Batch 48/64 loss: 0.0064560770988464355
Batch 49/64 loss: -0.0231093168258667
Batch 50/64 loss: 0.006742715835571289
Batch 51/64 loss: -0.011530697345733643
Batch 52/64 loss: -0.0066378116607666016
Batch 53/64 loss: 0.0012781620025634766
Batch 54/64 loss: -0.005538582801818848
Batch 55/64 loss: 0.017914891242980957
Batch 56/64 loss: -0.005103051662445068
Batch 57/64 loss: -0.0012767314910888672
Batch 58/64 loss: -0.006116032600402832
Batch 59/64 loss: -0.010728716850280762
Batch 60/64 loss: -0.01661050319671631
Batch 61/64 loss: 0.01740562915802002
Batch 62/64 loss: -0.004936099052429199
Batch 63/64 loss: 0.017940282821655273
Batch 64/64 loss: 0.010995626449584961
Epoch 124  Train loss: -0.00377198761584712  Val loss: 0.05359226506190611
Saving best model, epoch: 124
Epoch 125
-------------------------------
Batch 1/64 loss: 0.0009441971778869629
Batch 2/64 loss: -0.008783698081970215
Batch 3/64 loss: -0.009903430938720703
Batch 4/64 loss: -0.019208073616027832
Batch 5/64 loss: -0.006047427654266357
Batch 6/64 loss: -0.005518555641174316
Batch 7/64 loss: 0.0009430646896362305
Batch 8/64 loss: 0.011247575283050537
Batch 9/64 loss: 0.03577113151550293
Batch 10/64 loss: -0.0023854970932006836
Batch 11/64 loss: 0.0014074444770812988
Batch 12/64 loss: -0.013565123081207275
Batch 13/64 loss: -0.014314234256744385
Batch 14/64 loss: -0.015483617782592773
Batch 15/64 loss: 0.009374022483825684
Batch 16/64 loss: -0.02440941333770752
Batch 17/64 loss: -0.017948448657989502
Batch 18/64 loss: -0.011668622493743896
Batch 19/64 loss: 0.007363379001617432
Batch 20/64 loss: 0.024173736572265625
Batch 21/64 loss: -0.009334981441497803
Batch 22/64 loss: -0.001131892204284668
Batch 23/64 loss: 0.026567578315734863
Batch 24/64 loss: 0.00494009256362915
Batch 25/64 loss: 0.005527377128601074
Batch 26/64 loss: 0.003142237663269043
Batch 27/64 loss: -0.0011107921600341797
Batch 28/64 loss: -0.0006237030029296875
Batch 29/64 loss: -0.012425482273101807
Batch 30/64 loss: -0.011434078216552734
Batch 31/64 loss: -0.012689352035522461
Batch 32/64 loss: -0.0173490047454834
Batch 33/64 loss: -0.007202565670013428
Batch 34/64 loss: 0.004136085510253906
Batch 35/64 loss: -0.010827422142028809
Batch 36/64 loss: -0.0034984350204467773
Batch 37/64 loss: 0.005170166492462158
Batch 38/64 loss: -0.01272130012512207
Batch 39/64 loss: -0.022671818733215332
Batch 40/64 loss: 0.003267347812652588
Batch 41/64 loss: -0.018681883811950684
Batch 42/64 loss: -0.009776949882507324
Batch 43/64 loss: -0.0016312003135681152
Batch 44/64 loss: 0.0008594393730163574
Batch 45/64 loss: 0.0093117356300354
Batch 46/64 loss: 0.0025070905685424805
Batch 47/64 loss: -0.026198267936706543
Batch 48/64 loss: -0.015343427658081055
Batch 49/64 loss: -0.017377495765686035
Batch 50/64 loss: 0.009956836700439453
Batch 51/64 loss: -0.01180356740951538
Batch 52/64 loss: -0.010800957679748535
Batch 53/64 loss: 0.0006097555160522461
Batch 54/64 loss: -0.034357666969299316
Batch 55/64 loss: 0.01367497444152832
Batch 56/64 loss: -0.00920778512954712
Batch 57/64 loss: 0.0155906081199646
Batch 58/64 loss: -0.01102536916732788
Batch 59/64 loss: 0.023257136344909668
Batch 60/64 loss: 0.00671696662902832
Batch 61/64 loss: -0.011964619159698486
Batch 62/64 loss: -0.026402950286865234
Batch 63/64 loss: -0.029842138290405273
Batch 64/64 loss: -0.013640105724334717
Epoch 125  Train loss: -0.004555942731745103  Val loss: 0.053646912279817244
Epoch 126
-------------------------------
Batch 1/64 loss: -0.005057096481323242
Batch 2/64 loss: -0.004158377647399902
Batch 3/64 loss: 0.003105342388153076
Batch 4/64 loss: -0.01901876926422119
Batch 5/64 loss: 0.006794095039367676
Batch 6/64 loss: -0.043356120586395264
Batch 7/64 loss: 0.008530139923095703
Batch 8/64 loss: -0.018145263195037842
Batch 9/64 loss: -0.002826213836669922
Batch 10/64 loss: -0.0007228851318359375
Batch 11/64 loss: -0.012845873832702637
Batch 12/64 loss: -0.01194155216217041
Batch 13/64 loss: -0.017445683479309082
Batch 14/64 loss: -0.02247762680053711
Batch 15/64 loss: 0.003580331802368164
Batch 16/64 loss: -0.0012842416763305664
Batch 17/64 loss: -0.019359052181243896
Batch 18/64 loss: 0.0037022829055786133
Batch 19/64 loss: 0.0014911890029907227
Batch 20/64 loss: -0.014452993869781494
Batch 21/64 loss: -0.0123063325881958
Batch 22/64 loss: -0.009777307510375977
Batch 23/64 loss: 0.013101160526275635
Batch 24/64 loss: -0.01692783832550049
Batch 25/64 loss: 0.0022008419036865234
Batch 26/64 loss: 0.012620925903320312
Batch 27/64 loss: -0.004278838634490967
Batch 28/64 loss: 0.0026971101760864258
Batch 29/64 loss: 0.06120520830154419
Batch 30/64 loss: -0.022855043411254883
Batch 31/64 loss: -0.01852583885192871
Batch 32/64 loss: -0.020745396614074707
Batch 33/64 loss: 0.0010012388229370117
Batch 34/64 loss: -0.01323777437210083
Batch 35/64 loss: -0.010318994522094727
Batch 36/64 loss: -0.011122345924377441
Batch 37/64 loss: -0.025945961475372314
Batch 38/64 loss: -0.006876170635223389
Batch 39/64 loss: -0.00937575101852417
Batch 40/64 loss: -0.0016779899597167969
Batch 41/64 loss: -0.012361586093902588
Batch 42/64 loss: -0.0013583898544311523
Batch 43/64 loss: -0.010408878326416016
Batch 44/64 loss: -0.00811922550201416
Batch 45/64 loss: -0.0008294582366943359
Batch 46/64 loss: 0.01884758472442627
Batch 47/64 loss: 0.010896027088165283
Batch 48/64 loss: 0.009700357913970947
Batch 49/64 loss: -0.01301950216293335
Batch 50/64 loss: 0.013625621795654297
Batch 51/64 loss: -0.011536896228790283
Batch 52/64 loss: -0.005264937877655029
Batch 53/64 loss: -0.019284307956695557
Batch 54/64 loss: -0.0027734041213989258
Batch 55/64 loss: -0.021565675735473633
Batch 56/64 loss: 0.01586127281188965
Batch 57/64 loss: -0.01382225751876831
Batch 58/64 loss: -0.0027431249618530273
Batch 59/64 loss: 0.016953647136688232
Batch 60/64 loss: -0.015074729919433594
Batch 61/64 loss: -0.005817770957946777
Batch 62/64 loss: -0.004407644271850586
Batch 63/64 loss: -0.010696887969970703
Batch 64/64 loss: -0.00633394718170166
Epoch 126  Train loss: -0.005254652453403847  Val loss: 0.05553495044151122
Epoch 127
-------------------------------
Batch 1/64 loss: 0.003269791603088379
Batch 2/64 loss: -0.011525511741638184
Batch 3/64 loss: 0.003209531307220459
Batch 4/64 loss: -0.0013607144355773926
Batch 5/64 loss: 0.004125416278839111
Batch 6/64 loss: -0.0075955986976623535
Batch 7/64 loss: -0.009178459644317627
Batch 8/64 loss: -0.006659984588623047
Batch 9/64 loss: -0.028663694858551025
Batch 10/64 loss: -0.03238636255264282
Batch 11/64 loss: -0.028681814670562744
Batch 12/64 loss: 0.025651931762695312
Batch 13/64 loss: -0.008911728858947754
Batch 14/64 loss: -0.005218088626861572
Batch 15/64 loss: 0.0007606744766235352
Batch 16/64 loss: 0.00580209493637085
Batch 17/64 loss: 0.013480603694915771
Batch 18/64 loss: -0.0084572434425354
Batch 19/64 loss: -0.01678091287612915
Batch 20/64 loss: -0.011455237865447998
Batch 21/64 loss: -0.011337518692016602
Batch 22/64 loss: 0.008565366268157959
Batch 23/64 loss: 0.011768639087677002
Batch 24/64 loss: -0.00631946325302124
Batch 25/64 loss: 0.0008001327514648438
Batch 26/64 loss: -0.005444526672363281
Batch 27/64 loss: -0.008777260780334473
Batch 28/64 loss: -0.010695099830627441
Batch 29/64 loss: -0.019107818603515625
Batch 30/64 loss: -0.013315558433532715
Batch 31/64 loss: -0.01939237117767334
Batch 32/64 loss: -0.018443703651428223
Batch 33/64 loss: -0.00759124755859375
Batch 34/64 loss: -0.0016562342643737793
Batch 35/64 loss: -0.022348105907440186
Batch 36/64 loss: -0.0007880330085754395
Batch 37/64 loss: 0.004875898361206055
Batch 38/64 loss: -0.016393661499023438
Batch 39/64 loss: 0.010688185691833496
Batch 40/64 loss: 0.011687278747558594
Batch 41/64 loss: -0.007184505462646484
Batch 42/64 loss: 0.0037633180618286133
Batch 43/64 loss: -0.019637107849121094
Batch 44/64 loss: 0.021088361740112305
Batch 45/64 loss: 0.0027208328247070312
Batch 46/64 loss: 0.009605884552001953
Batch 47/64 loss: 0.010402321815490723
Batch 48/64 loss: -0.004181861877441406
Batch 49/64 loss: -0.008065521717071533
Batch 50/64 loss: -0.01705843210220337
Batch 51/64 loss: 0.0119284987449646
Batch 52/64 loss: -0.010487914085388184
Batch 53/64 loss: -0.027366578578948975
Batch 54/64 loss: -0.013010025024414062
Batch 55/64 loss: -0.017759621143341064
Batch 56/64 loss: -0.00715869665145874
Batch 57/64 loss: -0.0173528790473938
Batch 58/64 loss: -0.02249765396118164
Batch 59/64 loss: 0.002923905849456787
Batch 60/64 loss: -0.005245208740234375
Batch 61/64 loss: -0.013158977031707764
Batch 62/64 loss: 0.010586977005004883
Batch 63/64 loss: 0.01264810562133789
Batch 64/64 loss: -0.01565331220626831
Epoch 127  Train loss: -0.0054907791754778695  Val loss: 0.05598099739690827
Epoch 128
-------------------------------
Batch 1/64 loss: -0.01897716522216797
Batch 2/64 loss: 0.01367330551147461
Batch 3/64 loss: -0.004734814167022705
Batch 4/64 loss: -0.007429242134094238
Batch 5/64 loss: -0.005950510501861572
Batch 6/64 loss: -0.01812213659286499
Batch 7/64 loss: -0.017265677452087402
Batch 8/64 loss: -0.014979541301727295
Batch 9/64 loss: -0.028661012649536133
Batch 10/64 loss: 0.0006632208824157715
Batch 11/64 loss: -0.01072627305984497
Batch 12/64 loss: -0.014996707439422607
Batch 13/64 loss: -0.004108428955078125
Batch 14/64 loss: -0.011682868003845215
Batch 15/64 loss: -0.01432114839553833
Batch 16/64 loss: 0.003333747386932373
Batch 17/64 loss: -0.01214742660522461
Batch 18/64 loss: 0.0014061927795410156
Batch 19/64 loss: 0.006459057331085205
Batch 20/64 loss: -0.01504981517791748
Batch 21/64 loss: -0.03641664981842041
Batch 22/64 loss: 0.012394368648529053
Batch 23/64 loss: -0.007233798503875732
Batch 24/64 loss: -0.00017893314361572266
Batch 25/64 loss: -0.02402663230895996
Batch 26/64 loss: -0.004116177558898926
Batch 27/64 loss: -0.008374333381652832
Batch 28/64 loss: -0.0008409619331359863
Batch 29/64 loss: -0.00010198354721069336
Batch 30/64 loss: -0.029338538646697998
Batch 31/64 loss: -0.020947635173797607
Batch 32/64 loss: -0.023006737232208252
Batch 33/64 loss: -0.008632302284240723
Batch 34/64 loss: 0.0009642839431762695
Batch 35/64 loss: 7.909536361694336e-05
Batch 36/64 loss: -0.01091766357421875
Batch 37/64 loss: -0.010604619979858398
Batch 38/64 loss: -0.001218557357788086
Batch 39/64 loss: -0.022903025150299072
Batch 40/64 loss: -0.002051234245300293
Batch 41/64 loss: 0.007949113845825195
Batch 42/64 loss: -0.022501826286315918
Batch 43/64 loss: 0.0075272321701049805
Batch 44/64 loss: -0.022260308265686035
Batch 45/64 loss: -0.0012115836143493652
Batch 46/64 loss: -0.014088928699493408
Batch 47/64 loss: -0.017463088035583496
Batch 48/64 loss: -0.019064128398895264
Batch 49/64 loss: 0.011915147304534912
Batch 50/64 loss: -0.03378230333328247
Batch 51/64 loss: 0.0057375431060791016
Batch 52/64 loss: -0.0017578601837158203
Batch 53/64 loss: -0.01239776611328125
Batch 54/64 loss: 0.002761244773864746
Batch 55/64 loss: -0.0025498270988464355
Batch 56/64 loss: -0.00814729928970337
Batch 57/64 loss: -0.00489199161529541
Batch 58/64 loss: -0.0061449408531188965
Batch 59/64 loss: -0.014684200286865234
Batch 60/64 loss: 0.0013770461082458496
Batch 61/64 loss: -0.012985944747924805
Batch 62/64 loss: 0.014371156692504883
Batch 63/64 loss: -0.019623517990112305
Batch 64/64 loss: -0.008525192737579346
Epoch 128  Train loss: -0.008461179686527626  Val loss: 0.056914759870247335
Epoch 129
-------------------------------
Batch 1/64 loss: -0.03618234395980835
Batch 2/64 loss: -0.007118046283721924
Batch 3/64 loss: -0.0229148268699646
Batch 4/64 loss: 0.0072536468505859375
Batch 5/64 loss: -0.020880818367004395
Batch 6/64 loss: 0.004129350185394287
Batch 7/64 loss: -0.01575249433517456
Batch 8/64 loss: 0.004609465599060059
Batch 9/64 loss: 0.008000671863555908
Batch 10/64 loss: -0.021868109703063965
Batch 11/64 loss: -0.021743059158325195
Batch 12/64 loss: -0.01953190565109253
Batch 13/64 loss: 0.00866466760635376
Batch 14/64 loss: -0.03652834892272949
Batch 15/64 loss: 0.0035058259963989258
Batch 16/64 loss: -0.0029886364936828613
Batch 17/64 loss: -0.009142756462097168
Batch 18/64 loss: 0.003319859504699707
Batch 19/64 loss: -0.01709270477294922
Batch 20/64 loss: -0.004000544548034668
Batch 21/64 loss: 0.003793954849243164
Batch 22/64 loss: -0.012558579444885254
Batch 23/64 loss: -0.027200758457183838
Batch 24/64 loss: -0.020863890647888184
Batch 25/64 loss: -0.006954073905944824
Batch 26/64 loss: -0.0028963685035705566
Batch 27/64 loss: 0.002381265163421631
Batch 28/64 loss: 0.0013211369514465332
Batch 29/64 loss: -0.011411607265472412
Batch 30/64 loss: -0.0019301176071166992
Batch 31/64 loss: -0.017556309700012207
Batch 32/64 loss: -0.010711312294006348
Batch 33/64 loss: -0.007601439952850342
Batch 34/64 loss: 0.02201181650161743
Batch 35/64 loss: -0.002870798110961914
Batch 36/64 loss: 0.010648071765899658
Batch 37/64 loss: -0.009310662746429443
Batch 38/64 loss: 0.03444337844848633
Batch 39/64 loss: 0.005677580833435059
Batch 40/64 loss: 0.006785154342651367
Batch 41/64 loss: -0.023383796215057373
Batch 42/64 loss: -0.004138171672821045
Batch 43/64 loss: -0.01491999626159668
Batch 44/64 loss: 0.006730914115905762
Batch 45/64 loss: -0.024473965167999268
Batch 46/64 loss: -0.02274322509765625
Batch 47/64 loss: -0.0012364387512207031
Batch 48/64 loss: -0.006539463996887207
Batch 49/64 loss: 0.0104789137840271
Batch 50/64 loss: 0.010204315185546875
Batch 51/64 loss: -0.021421074867248535
Batch 52/64 loss: -0.015804052352905273
Batch 53/64 loss: -0.010386109352111816
Batch 54/64 loss: -0.026985526084899902
Batch 55/64 loss: -0.015272080898284912
Batch 56/64 loss: -0.015402793884277344
Batch 57/64 loss: -0.025101304054260254
Batch 58/64 loss: -0.0032854080200195312
Batch 59/64 loss: -0.020003795623779297
Batch 60/64 loss: -0.0009663701057434082
Batch 61/64 loss: -0.02454143762588501
Batch 62/64 loss: -0.005400300025939941
Batch 63/64 loss: -0.023325800895690918
Batch 64/64 loss: 0.029785096645355225
Epoch 129  Train loss: -0.007790475499396231  Val loss: 0.05679980340282532
Epoch 130
-------------------------------
Batch 1/64 loss: -0.02007657289505005
Batch 2/64 loss: 0.013060688972473145
Batch 3/64 loss: -0.0029714107513427734
Batch 4/64 loss: -0.020647644996643066
Batch 5/64 loss: -0.026680052280426025
Batch 6/64 loss: -0.02982616424560547
Batch 7/64 loss: -0.016084492206573486
Batch 8/64 loss: 0.0043950676918029785
Batch 9/64 loss: 0.001839756965637207
Batch 10/64 loss: 0.01267009973526001
Batch 11/64 loss: -0.007678985595703125
Batch 12/64 loss: -0.0159340500831604
Batch 13/64 loss: -0.0075762271881103516
Batch 14/64 loss: -0.009260416030883789
Batch 15/64 loss: 0.005405604839324951
Batch 16/64 loss: 0.007769405841827393
Batch 17/64 loss: -0.04079866409301758
Batch 18/64 loss: -0.005111217498779297
Batch 19/64 loss: -0.008255302906036377
Batch 20/64 loss: -0.019830822944641113
Batch 21/64 loss: -0.020942628383636475
Batch 22/64 loss: -0.01450282335281372
Batch 23/64 loss: 0.0010467171669006348
Batch 24/64 loss: -0.02431356906890869
Batch 25/64 loss: -0.019087374210357666
Batch 26/64 loss: -0.009140253067016602
Batch 27/64 loss: 0.0005475878715515137
Batch 28/64 loss: -0.00022834539413452148
Batch 29/64 loss: 0.01171642541885376
Batch 30/64 loss: -0.01137775182723999
Batch 31/64 loss: -0.025014281272888184
Batch 32/64 loss: -0.02226412296295166
Batch 33/64 loss: 0.023595333099365234
Batch 34/64 loss: -8.094310760498047e-05
Batch 35/64 loss: -0.009500205516815186
Batch 36/64 loss: -0.0059868693351745605
Batch 37/64 loss: -0.008348226547241211
Batch 38/64 loss: -0.022374391555786133
Batch 39/64 loss: -0.01584315299987793
Batch 40/64 loss: -0.02149975299835205
Batch 41/64 loss: 0.016919732093811035
Batch 42/64 loss: -0.004891872406005859
Batch 43/64 loss: -0.007906675338745117
Batch 44/64 loss: 0.033910930156707764
Batch 45/64 loss: -0.017480134963989258
Batch 46/64 loss: -0.020441174507141113
Batch 47/64 loss: -0.03037053346633911
Batch 48/64 loss: 0.010008692741394043
Batch 49/64 loss: 0.011327505111694336
Batch 50/64 loss: -0.013064920902252197
Batch 51/64 loss: -0.02229464054107666
Batch 52/64 loss: -0.010179698467254639
Batch 53/64 loss: 0.009751379489898682
Batch 54/64 loss: 0.02248847484588623
Batch 55/64 loss: 0.016816675662994385
Batch 56/64 loss: -0.0090409517288208
Batch 57/64 loss: 0.03015190362930298
Batch 58/64 loss: -0.006521642208099365
Batch 59/64 loss: -0.005104184150695801
Batch 60/64 loss: 0.013186931610107422
Batch 61/64 loss: -0.01630115509033203
Batch 62/64 loss: -0.028733491897583008
Batch 63/64 loss: -0.013792574405670166
Batch 64/64 loss: 0.025531470775604248
Epoch 130  Train loss: -0.00629965253904754  Val loss: 0.057860736379918364
Epoch 131
-------------------------------
Batch 1/64 loss: -0.018543362617492676
Batch 2/64 loss: -0.009589672088623047
Batch 3/64 loss: 0.0014342069625854492
Batch 4/64 loss: -0.016538500785827637
Batch 5/64 loss: -0.020661473274230957
Batch 6/64 loss: -0.027782082557678223
Batch 7/64 loss: 0.00930100679397583
Batch 8/64 loss: -0.014781057834625244
Batch 9/64 loss: -0.02465033531188965
Batch 10/64 loss: 0.007245004177093506
Batch 11/64 loss: -0.0009142160415649414
Batch 12/64 loss: -0.018360376358032227
Batch 13/64 loss: -0.008148431777954102
Batch 14/64 loss: -0.028849422931671143
Batch 15/64 loss: -0.010944068431854248
Batch 16/64 loss: -0.007543325424194336
Batch 17/64 loss: -0.02169090509414673
Batch 18/64 loss: 0.006328105926513672
Batch 19/64 loss: -0.015854835510253906
Batch 20/64 loss: -0.024219751358032227
Batch 21/64 loss: -0.015008985996246338
Batch 22/64 loss: -0.024132907390594482
Batch 23/64 loss: 0.0036319494247436523
Batch 24/64 loss: -0.0001658797264099121
Batch 25/64 loss: 0.007351875305175781
Batch 26/64 loss: 0.0033628344535827637
Batch 27/64 loss: -0.017327189445495605
Batch 28/64 loss: -0.020084798336029053
Batch 29/64 loss: -0.010555267333984375
Batch 30/64 loss: -0.015272259712219238
Batch 31/64 loss: -0.009237468242645264
Batch 32/64 loss: -0.018331527709960938
Batch 33/64 loss: -0.016544580459594727
Batch 34/64 loss: 0.008277654647827148
Batch 35/64 loss: -0.008039593696594238
Batch 36/64 loss: -0.006693482398986816
Batch 37/64 loss: 0.03318661451339722
Batch 38/64 loss: -0.01631152629852295
Batch 39/64 loss: 0.02406299114227295
Batch 40/64 loss: 0.01561880111694336
Batch 41/64 loss: -0.016427576541900635
Batch 42/64 loss: -0.009799480438232422
Batch 43/64 loss: -0.022381961345672607
Batch 44/64 loss: -0.01964735984802246
Batch 45/64 loss: -0.010643601417541504
Batch 46/64 loss: -0.023460805416107178
Batch 47/64 loss: -0.009210824966430664
Batch 48/64 loss: 0.01751774549484253
Batch 49/64 loss: 0.00813215970993042
Batch 50/64 loss: -0.015389561653137207
Batch 51/64 loss: -0.0006937384605407715
Batch 52/64 loss: -0.025364458560943604
Batch 53/64 loss: -0.027584731578826904
Batch 54/64 loss: -0.022791385650634766
Batch 55/64 loss: 0.0010280013084411621
Batch 56/64 loss: 0.00010895729064941406
Batch 57/64 loss: -0.01174914836883545
Batch 58/64 loss: 0.011711537837982178
Batch 59/64 loss: -0.040008604526519775
Batch 60/64 loss: 0.012286543846130371
Batch 61/64 loss: -0.005283236503601074
Batch 62/64 loss: 0.0001633167266845703
Batch 63/64 loss: -0.008645117282867432
Batch 64/64 loss: -0.023165225982666016
Epoch 131  Train loss: -0.008823270423739564  Val loss: 0.053587444459449796
Saving best model, epoch: 131
Epoch 132
-------------------------------
Batch 1/64 loss: -0.0008984804153442383
Batch 2/64 loss: -0.006425082683563232
Batch 3/64 loss: -0.023776531219482422
Batch 4/64 loss: -0.03700757026672363
Batch 5/64 loss: -0.005199015140533447
Batch 6/64 loss: 0.00873953104019165
Batch 7/64 loss: -0.0035755038261413574
Batch 8/64 loss: -0.025870144367218018
Batch 9/64 loss: 0.012651205062866211
Batch 10/64 loss: -0.008020937442779541
Batch 11/64 loss: -0.010175764560699463
Batch 12/64 loss: 0.006024360656738281
Batch 13/64 loss: -0.018330931663513184
Batch 14/64 loss: -0.027734577655792236
Batch 15/64 loss: -0.01977473497390747
Batch 16/64 loss: -0.020603179931640625
Batch 17/64 loss: -0.023425936698913574
Batch 18/64 loss: -0.02320164442062378
Batch 19/64 loss: 0.008520722389221191
Batch 20/64 loss: -0.025770246982574463
Batch 21/64 loss: -0.012639105319976807
Batch 22/64 loss: -0.03200387954711914
Batch 23/64 loss: -0.006567955017089844
Batch 24/64 loss: -0.013532042503356934
Batch 25/64 loss: -0.00984036922454834
Batch 26/64 loss: 0.0037587881088256836
Batch 27/64 loss: 0.003992259502410889
Batch 28/64 loss: -0.016747713088989258
Batch 29/64 loss: -0.0063332319259643555
Batch 30/64 loss: -0.0325581431388855
Batch 31/64 loss: 0.0009682178497314453
Batch 32/64 loss: -0.02895987033843994
Batch 33/64 loss: 0.006021022796630859
Batch 34/64 loss: -0.02485203742980957
Batch 35/64 loss: -0.014270782470703125
Batch 36/64 loss: 0.00041472911834716797
Batch 37/64 loss: -0.027613401412963867
Batch 38/64 loss: -0.034096479415893555
Batch 39/64 loss: 0.004615485668182373
Batch 40/64 loss: 0.024968326091766357
Batch 41/64 loss: 0.0023195743560791016
Batch 42/64 loss: -0.015317976474761963
Batch 43/64 loss: -0.005893290042877197
Batch 44/64 loss: 0.026084542274475098
Batch 45/64 loss: -0.016324996948242188
Batch 46/64 loss: -0.009094655513763428
Batch 47/64 loss: -0.0014535188674926758
Batch 48/64 loss: 0.01851600408554077
Batch 49/64 loss: -0.01408076286315918
Batch 50/64 loss: -0.014959275722503662
Batch 51/64 loss: -0.019928157329559326
Batch 52/64 loss: -0.011994779109954834
Batch 53/64 loss: -0.02308487892150879
Batch 54/64 loss: -0.016010284423828125
Batch 55/64 loss: 0.00036340951919555664
Batch 56/64 loss: 0.002192676067352295
Batch 57/64 loss: -0.012110888957977295
Batch 58/64 loss: -0.01840430498123169
Batch 59/64 loss: -0.007756471633911133
Batch 60/64 loss: -0.00762939453125
Batch 61/64 loss: 0.0004839897155761719
Batch 62/64 loss: -0.035051584243774414
Batch 63/64 loss: -0.011765539646148682
Batch 64/64 loss: 0.01511317491531372
Epoch 132  Train loss: -0.010018765926361084  Val loss: 0.05435238773470482
Epoch 133
-------------------------------
Batch 1/64 loss: 0.0010346174240112305
Batch 2/64 loss: -0.007626891136169434
Batch 3/64 loss: -0.016619086265563965
Batch 4/64 loss: -0.004891157150268555
Batch 5/64 loss: -0.02402365207672119
Batch 6/64 loss: 0.004734396934509277
Batch 7/64 loss: -0.009419500827789307
Batch 8/64 loss: -0.029741406440734863
Batch 9/64 loss: -0.016181588172912598
Batch 10/64 loss: -0.019459426403045654
Batch 11/64 loss: 0.0053130388259887695
Batch 12/64 loss: -0.010328114032745361
Batch 13/64 loss: 0.005915045738220215
Batch 14/64 loss: 0.0035602450370788574
Batch 15/64 loss: 0.02299022674560547
Batch 16/64 loss: 0.0023370981216430664
Batch 17/64 loss: -0.012676537036895752
Batch 18/64 loss: -0.008479654788970947
Batch 19/64 loss: -0.0277559757232666
Batch 20/64 loss: -0.010271668434143066
Batch 21/64 loss: -0.017322540283203125
Batch 22/64 loss: -0.00852203369140625
Batch 23/64 loss: 0.00977790355682373
Batch 24/64 loss: -0.011436641216278076
Batch 25/64 loss: -0.008435606956481934
Batch 26/64 loss: -0.02522367238998413
Batch 27/64 loss: -0.016392230987548828
Batch 28/64 loss: -0.01957601308822632
Batch 29/64 loss: 0.02570486068725586
Batch 30/64 loss: -0.0010598301887512207
Batch 31/64 loss: -0.015200138092041016
Batch 32/64 loss: 0.00314253568649292
Batch 33/64 loss: -0.02428072690963745
Batch 34/64 loss: -0.0010657906532287598
Batch 35/64 loss: -0.0013677477836608887
Batch 36/64 loss: -0.0037961602210998535
Batch 37/64 loss: 0.00012981891632080078
Batch 38/64 loss: -0.01469200849533081
Batch 39/64 loss: -0.019310593605041504
Batch 40/64 loss: -0.012437880039215088
Batch 41/64 loss: -0.0024239420890808105
Batch 42/64 loss: -0.004239559173583984
Batch 43/64 loss: 0.009431421756744385
Batch 44/64 loss: -0.008962154388427734
Batch 45/64 loss: -0.016356825828552246
Batch 46/64 loss: -0.020222187042236328
Batch 47/64 loss: 0.0038222074508666992
Batch 48/64 loss: -0.023822307586669922
Batch 49/64 loss: -0.01744741201400757
Batch 50/64 loss: -0.009241044521331787
Batch 51/64 loss: -0.01732456684112549
Batch 52/64 loss: -0.019997060298919678
Batch 53/64 loss: -0.0022063851356506348
Batch 54/64 loss: 0.003958523273468018
Batch 55/64 loss: -0.01615750789642334
Batch 56/64 loss: -0.012244582176208496
Batch 57/64 loss: -0.029249846935272217
Batch 58/64 loss: -0.007509946823120117
Batch 59/64 loss: 0.023569762706756592
Batch 60/64 loss: 0.0020676255226135254
Batch 61/64 loss: -0.01637899875640869
Batch 62/64 loss: 0.011049151420593262
Batch 63/64 loss: -0.022625207901000977
Batch 64/64 loss: 0.0023937225341796875
Epoch 133  Train loss: -0.007900706459494198  Val loss: 0.056160021893347255
Epoch 134
-------------------------------
Batch 1/64 loss: -0.007291972637176514
Batch 2/64 loss: -0.012562870979309082
Batch 3/64 loss: -0.0002281665802001953
Batch 4/64 loss: 0.0012090802192687988
Batch 5/64 loss: -0.029988467693328857
Batch 6/64 loss: -0.015662729740142822
Batch 7/64 loss: -0.023826301097869873
Batch 8/64 loss: 0.010003328323364258
Batch 9/64 loss: -0.02330958843231201
Batch 10/64 loss: 0.0009286999702453613
Batch 11/64 loss: -0.01089489459991455
Batch 12/64 loss: 0.008249998092651367
Batch 13/64 loss: -0.010064005851745605
Batch 14/64 loss: -0.014300644397735596
Batch 15/64 loss: -0.011552572250366211
Batch 16/64 loss: -0.032637596130371094
Batch 17/64 loss: -0.019334077835083008
Batch 18/64 loss: -0.009502708911895752
Batch 19/64 loss: -0.03800708055496216
Batch 20/64 loss: -0.0046994686126708984
Batch 21/64 loss: -0.010668516159057617
Batch 22/64 loss: 0.0017381906509399414
Batch 23/64 loss: -0.012738525867462158
Batch 24/64 loss: -0.016405224800109863
Batch 25/64 loss: -0.024036288261413574
Batch 26/64 loss: -0.016545474529266357
Batch 27/64 loss: -0.022612810134887695
Batch 28/64 loss: -0.02978593111038208
Batch 29/64 loss: -0.03517651557922363
Batch 30/64 loss: -0.02518361806869507
Batch 31/64 loss: -0.010324239730834961
Batch 32/64 loss: -0.009024262428283691
Batch 33/64 loss: -0.025240063667297363
Batch 34/64 loss: -0.025092244148254395
Batch 35/64 loss: -0.008626341819763184
Batch 36/64 loss: 0.0071266889572143555
Batch 37/64 loss: 0.024534642696380615
Batch 38/64 loss: -0.009953200817108154
Batch 39/64 loss: -0.005174160003662109
Batch 40/64 loss: 0.01821821928024292
Batch 41/64 loss: -0.0040770769119262695
Batch 42/64 loss: -0.002431631088256836
Batch 43/64 loss: -0.014543712139129639
Batch 44/64 loss: -0.01711750030517578
Batch 45/64 loss: -0.005391061305999756
Batch 46/64 loss: 0.0008176565170288086
Batch 47/64 loss: -0.019983410835266113
Batch 48/64 loss: -0.004633724689483643
Batch 49/64 loss: -0.0048868656158447266
Batch 50/64 loss: -0.033014655113220215
Batch 51/64 loss: -0.022068381309509277
Batch 52/64 loss: -0.03256946802139282
Batch 53/64 loss: -0.00812000036239624
Batch 54/64 loss: -0.01719486713409424
Batch 55/64 loss: -0.021033644676208496
Batch 56/64 loss: -0.012269556522369385
Batch 57/64 loss: -0.012135446071624756
Batch 58/64 loss: -0.004915416240692139
Batch 59/64 loss: 0.01589888334274292
Batch 60/64 loss: 0.003856956958770752
Batch 61/64 loss: -0.018834471702575684
Batch 62/64 loss: -0.006309449672698975
Batch 63/64 loss: -0.016340911388397217
Batch 64/64 loss: 0.00510561466217041
Epoch 134  Train loss: -0.01148094523186777  Val loss: 0.05072910289993811
Saving best model, epoch: 134
Epoch 135
-------------------------------
Batch 1/64 loss: 0.000916600227355957
Batch 2/64 loss: -0.03077465295791626
Batch 3/64 loss: -0.014767646789550781
Batch 4/64 loss: -0.008722364902496338
Batch 5/64 loss: -0.014811694622039795
Batch 6/64 loss: -0.023226261138916016
Batch 7/64 loss: -0.0127907395362854
Batch 8/64 loss: -0.022328078746795654
Batch 9/64 loss: 0.001587212085723877
Batch 10/64 loss: -0.04638850688934326
Batch 11/64 loss: -0.009743988513946533
Batch 12/64 loss: -0.006811797618865967
Batch 13/64 loss: -0.022035956382751465
Batch 14/64 loss: -0.016922414302825928
Batch 15/64 loss: -0.010512709617614746
Batch 16/64 loss: -0.014107465744018555
Batch 17/64 loss: -0.03236949443817139
Batch 18/64 loss: -0.01947641372680664
Batch 19/64 loss: 0.019884943962097168
Batch 20/64 loss: -0.032956063747406006
Batch 21/64 loss: -0.010823369026184082
Batch 22/64 loss: -0.02094632387161255
Batch 23/64 loss: -0.005368232727050781
Batch 24/64 loss: -0.02471601963043213
Batch 25/64 loss: -0.018660366535186768
Batch 26/64 loss: -0.003264009952545166
Batch 27/64 loss: -0.00688624382019043
Batch 28/64 loss: -0.03423607349395752
Batch 29/64 loss: -0.03278648853302002
Batch 30/64 loss: 0.008313894271850586
Batch 31/64 loss: -0.0024049878120422363
Batch 32/64 loss: -0.016149401664733887
Batch 33/64 loss: -0.01979649066925049
Batch 34/64 loss: -0.0007714629173278809
Batch 35/64 loss: -0.004042983055114746
Batch 36/64 loss: -0.016855299472808838
Batch 37/64 loss: -0.022983074188232422
Batch 38/64 loss: -0.010038435459136963
Batch 39/64 loss: -0.012173712253570557
Batch 40/64 loss: -0.006103515625
Batch 41/64 loss: -0.006039142608642578
Batch 42/64 loss: 0.007245779037475586
Batch 43/64 loss: -0.014786720275878906
Batch 44/64 loss: -0.007728457450866699
Batch 45/64 loss: -0.015240132808685303
Batch 46/64 loss: -0.03147768974304199
Batch 47/64 loss: 0.003199160099029541
Batch 48/64 loss: -0.006043076515197754
Batch 49/64 loss: -0.02396976947784424
Batch 50/64 loss: -0.0240059494972229
Batch 51/64 loss: 0.006786167621612549
Batch 52/64 loss: 0.007825136184692383
Batch 53/64 loss: -0.01751810312271118
Batch 54/64 loss: 0.009480953216552734
Batch 55/64 loss: -0.007808506488800049
Batch 56/64 loss: 0.0033870339393615723
Batch 57/64 loss: -0.03320944309234619
Batch 58/64 loss: -0.009005844593048096
Batch 59/64 loss: -0.019313454627990723
Batch 60/64 loss: -0.01565605401992798
Batch 61/64 loss: -0.0018371939659118652
Batch 62/64 loss: -0.010734260082244873
Batch 63/64 loss: -0.013001561164855957
Batch 64/64 loss: 0.022114455699920654
Epoch 135  Train loss: -0.012233966705845851  Val loss: 0.05096759296364801
Epoch 136
-------------------------------
Batch 1/64 loss: -0.015585601329803467
Batch 2/64 loss: -0.0194891095161438
Batch 3/64 loss: 0.015367627143859863
Batch 4/64 loss: -0.03839695453643799
Batch 5/64 loss: -0.011906564235687256
Batch 6/64 loss: -0.012225627899169922
Batch 7/64 loss: -0.02576655149459839
Batch 8/64 loss: -0.03489738702774048
Batch 9/64 loss: -0.013141214847564697
Batch 10/64 loss: -0.012282073497772217
Batch 11/64 loss: 0.0009705424308776855
Batch 12/64 loss: -0.013361990451812744
Batch 13/64 loss: -0.03519415855407715
Batch 14/64 loss: -0.025739550590515137
Batch 15/64 loss: -0.0218694806098938
Batch 16/64 loss: 0.010578274726867676
Batch 17/64 loss: 0.01590752601623535
Batch 18/64 loss: 0.0016499757766723633
Batch 19/64 loss: -0.03509509563446045
Batch 20/64 loss: -0.02873462438583374
Batch 21/64 loss: -0.021275997161865234
Batch 22/64 loss: -0.02809321880340576
Batch 23/64 loss: -0.01772838830947876
Batch 24/64 loss: -0.01770728826522827
Batch 25/64 loss: -0.0032085180282592773
Batch 26/64 loss: -0.01954251527786255
Batch 27/64 loss: -0.008917510509490967
Batch 28/64 loss: 0.007846832275390625
Batch 29/64 loss: 0.00542140007019043
Batch 30/64 loss: -0.01372748613357544
Batch 31/64 loss: -0.015173017978668213
Batch 32/64 loss: -0.01532292366027832
Batch 33/64 loss: -0.03489750623703003
Batch 34/64 loss: -0.01363670825958252
Batch 35/64 loss: 0.006487667560577393
Batch 36/64 loss: -0.006882429122924805
Batch 37/64 loss: -0.0051062703132629395
Batch 38/64 loss: -0.011931180953979492
Batch 39/64 loss: -0.017234623432159424
Batch 40/64 loss: -0.02329176664352417
Batch 41/64 loss: -0.017795264720916748
Batch 42/64 loss: 0.00437772274017334
Batch 43/64 loss: -0.01443713903427124
Batch 44/64 loss: -0.006708681583404541
Batch 45/64 loss: -0.0048285722732543945
Batch 46/64 loss: -0.02950417995452881
Batch 47/64 loss: 0.0025835037231445312
Batch 48/64 loss: -0.036624908447265625
Batch 49/64 loss: -0.02926456928253174
Batch 50/64 loss: -0.018506646156311035
Batch 51/64 loss: 0.0017685294151306152
Batch 52/64 loss: -0.03453397750854492
Batch 53/64 loss: 0.01581883430480957
Batch 54/64 loss: 0.004631519317626953
Batch 55/64 loss: -0.006883502006530762
Batch 56/64 loss: -0.018207252025604248
Batch 57/64 loss: -0.04165911674499512
Batch 58/64 loss: 0.0054253339767456055
Batch 59/64 loss: -0.02670055627822876
Batch 60/64 loss: -0.0010308623313903809
Batch 61/64 loss: -0.008825540542602539
Batch 62/64 loss: -0.03640735149383545
Batch 63/64 loss: -0.0024830102920532227
Batch 64/64 loss: -0.025592923164367676
Epoch 136  Train loss: -0.013680374388601266  Val loss: 0.05133911711243829
Epoch 137
-------------------------------
Batch 1/64 loss: -0.009322166442871094
Batch 2/64 loss: -0.020342230796813965
Batch 3/64 loss: -0.011421799659729004
Batch 4/64 loss: -0.009499549865722656
Batch 5/64 loss: -0.036405205726623535
Batch 6/64 loss: -0.023478150367736816
Batch 7/64 loss: -0.02839028835296631
Batch 8/64 loss: -0.031682729721069336
Batch 9/64 loss: -0.04190492630004883
Batch 10/64 loss: 0.005972564220428467
Batch 11/64 loss: -0.01883310079574585
Batch 12/64 loss: -0.0016152262687683105
Batch 13/64 loss: -0.029906153678894043
Batch 14/64 loss: -0.010930299758911133
Batch 15/64 loss: -0.018553614616394043
Batch 16/64 loss: -0.026630938053131104
Batch 17/64 loss: -0.03281897306442261
Batch 18/64 loss: -0.032742440700531006
Batch 19/64 loss: 0.005013585090637207
Batch 20/64 loss: -0.013440608978271484
Batch 21/64 loss: -0.03213357925415039
Batch 22/64 loss: -0.0040912628173828125
Batch 23/64 loss: -0.02671074867248535
Batch 24/64 loss: -0.009588062763214111
Batch 25/64 loss: -0.012810766696929932
Batch 26/64 loss: -0.016168296337127686
Batch 27/64 loss: -0.025920510292053223
Batch 28/64 loss: -0.037749290466308594
Batch 29/64 loss: -0.020081520080566406
Batch 30/64 loss: -0.015894412994384766
Batch 31/64 loss: 0.013655781745910645
Batch 32/64 loss: -0.023962795734405518
Batch 33/64 loss: -0.01835620403289795
Batch 34/64 loss: 0.010678350925445557
Batch 35/64 loss: -0.006379663944244385
Batch 36/64 loss: 0.01111602783203125
Batch 37/64 loss: -0.0043585896492004395
Batch 38/64 loss: 0.013787686824798584
Batch 39/64 loss: -0.008617281913757324
Batch 40/64 loss: -0.021026909351348877
Batch 41/64 loss: -0.016089141368865967
Batch 42/64 loss: 0.012474894523620605
Batch 43/64 loss: -0.000668942928314209
Batch 44/64 loss: -0.011032342910766602
Batch 45/64 loss: -0.03594815731048584
Batch 46/64 loss: -0.017868220806121826
Batch 47/64 loss: -0.028462886810302734
Batch 48/64 loss: -0.026555299758911133
Batch 49/64 loss: 0.005610227584838867
Batch 50/64 loss: -0.016652941703796387
Batch 51/64 loss: -0.008503735065460205
Batch 52/64 loss: -0.008162081241607666
Batch 53/64 loss: 0.008727967739105225
Batch 54/64 loss: -0.03432595729827881
Batch 55/64 loss: -0.019415855407714844
Batch 56/64 loss: -0.02669304609298706
Batch 57/64 loss: -0.0073738694190979
Batch 58/64 loss: -0.017627716064453125
Batch 59/64 loss: -0.012359082698822021
Batch 60/64 loss: 0.020643949508666992
Batch 61/64 loss: -0.017727553844451904
Batch 62/64 loss: -0.019568979740142822
Batch 63/64 loss: -0.013701558113098145
Batch 64/64 loss: -0.015599489212036133
Epoch 137  Train loss: -0.014502341139550302  Val loss: 0.058355419291663414
Epoch 138
-------------------------------
Batch 1/64 loss: -0.01638185977935791
Batch 2/64 loss: -0.009480774402618408
Batch 3/64 loss: 0.019724488258361816
Batch 4/64 loss: -0.014547944068908691
Batch 5/64 loss: -0.018124520778656006
Batch 6/64 loss: -0.013437271118164062
Batch 7/64 loss: 0.0005551576614379883
Batch 8/64 loss: -0.01737034320831299
Batch 9/64 loss: -0.020298540592193604
Batch 10/64 loss: -0.007936537265777588
Batch 11/64 loss: -0.023130714893341064
Batch 12/64 loss: -0.0016604065895080566
Batch 13/64 loss: -0.027763664722442627
Batch 14/64 loss: -0.03710758686065674
Batch 15/64 loss: -0.01280224323272705
Batch 16/64 loss: -0.014643549919128418
Batch 17/64 loss: -0.027736246585845947
Batch 18/64 loss: -0.01276695728302002
Batch 19/64 loss: -0.016157925128936768
Batch 20/64 loss: -0.017116546630859375
Batch 21/64 loss: -0.03152620792388916
Batch 22/64 loss: -0.025231659412384033
Batch 23/64 loss: -0.036153972148895264
Batch 24/64 loss: 0.016857445240020752
Batch 25/64 loss: 0.00011175870895385742
Batch 26/64 loss: -0.032563209533691406
Batch 27/64 loss: -0.04924309253692627
Batch 28/64 loss: 0.01449209451675415
Batch 29/64 loss: 0.01943385601043701
Batch 30/64 loss: -0.0123177170753479
Batch 31/64 loss: -0.02982199192047119
Batch 32/64 loss: -0.022118985652923584
Batch 33/64 loss: 0.004254698753356934
Batch 34/64 loss: -0.010170221328735352
Batch 35/64 loss: -0.02229619026184082
Batch 36/64 loss: -0.01335364580154419
Batch 37/64 loss: -0.0031482577323913574
Batch 38/64 loss: -0.03635859489440918
Batch 39/64 loss: 0.004549682140350342
Batch 40/64 loss: -0.008343398571014404
Batch 41/64 loss: -0.013634800910949707
Batch 42/64 loss: -0.03670382499694824
Batch 43/64 loss: -0.02425980567932129
Batch 44/64 loss: -0.023594677448272705
Batch 45/64 loss: -0.031304121017456055
Batch 46/64 loss: -0.025899171829223633
Batch 47/64 loss: -0.02756589651107788
Batch 48/64 loss: -0.018067479133605957
Batch 49/64 loss: -0.007474243640899658
Batch 50/64 loss: -0.005312800407409668
Batch 51/64 loss: -0.01491403579711914
Batch 52/64 loss: 0.004317224025726318
Batch 53/64 loss: -0.012871980667114258
Batch 54/64 loss: -0.01881355047225952
Batch 55/64 loss: -0.022855103015899658
Batch 56/64 loss: -0.00807565450668335
Batch 57/64 loss: 0.001258552074432373
Batch 58/64 loss: -0.010197162628173828
Batch 59/64 loss: -0.03324902057647705
Batch 60/64 loss: -0.0241316556930542
Batch 61/64 loss: 0.007432758808135986
Batch 62/64 loss: -0.0020347237586975098
Batch 63/64 loss: 0.0035039186477661133
Batch 64/64 loss: 0.021950185298919678
Epoch 138  Train loss: -0.013946920983931597  Val loss: 0.05313105333302029
Epoch 139
-------------------------------
Batch 1/64 loss: -0.03921985626220703
Batch 2/64 loss: -0.030028223991394043
Batch 3/64 loss: -0.013622581958770752
Batch 4/64 loss: -0.021030306816101074
Batch 5/64 loss: -0.006362557411193848
Batch 6/64 loss: -0.005563855171203613
Batch 7/64 loss: 0.012077510356903076
Batch 8/64 loss: 0.0005778074264526367
Batch 9/64 loss: -0.025356173515319824
Batch 10/64 loss: -0.021547675132751465
Batch 11/64 loss: -0.026651382446289062
Batch 12/64 loss: 0.015523850917816162
Batch 13/64 loss: -0.021158576011657715
Batch 14/64 loss: -0.0226285457611084
Batch 15/64 loss: -0.01697075366973877
Batch 16/64 loss: -0.001346290111541748
Batch 17/64 loss: -0.026400983333587646
Batch 18/64 loss: 0.0028694868087768555
Batch 19/64 loss: -0.00446009635925293
Batch 20/64 loss: -0.01081240177154541
Batch 21/64 loss: -0.022317051887512207
Batch 22/64 loss: -0.044904351234436035
Batch 23/64 loss: -0.033184707164764404
Batch 24/64 loss: -0.015002131462097168
Batch 25/64 loss: -0.00868600606918335
Batch 26/64 loss: 0.0035030245780944824
Batch 27/64 loss: -0.024737238883972168
Batch 28/64 loss: -0.013333261013031006
Batch 29/64 loss: -0.011973261833190918
Batch 30/64 loss: -0.006536662578582764
Batch 31/64 loss: 0.007608175277709961
Batch 32/64 loss: -0.026955068111419678
Batch 33/64 loss: -0.03508400917053223
Batch 34/64 loss: -0.03873974084854126
Batch 35/64 loss: -0.027374446392059326
Batch 36/64 loss: -0.026673495769500732
Batch 37/64 loss: 0.00812464952468872
Batch 38/64 loss: 0.0019858479499816895
Batch 39/64 loss: -0.006378650665283203
Batch 40/64 loss: -0.02491319179534912
Batch 41/64 loss: -0.0104445219039917
Batch 42/64 loss: 0.003373861312866211
Batch 43/64 loss: 0.020860671997070312
Batch 44/64 loss: -0.02016836404800415
Batch 45/64 loss: -0.016520202159881592
Batch 46/64 loss: 0.0033757686614990234
Batch 47/64 loss: -0.01415717601776123
Batch 48/64 loss: -0.015239357948303223
Batch 49/64 loss: 0.00032591819763183594
Batch 50/64 loss: -0.020235657691955566
Batch 51/64 loss: 0.011453211307525635
Batch 52/64 loss: -0.04018205404281616
Batch 53/64 loss: -0.017354249954223633
Batch 54/64 loss: -0.008280456066131592
Batch 55/64 loss: -0.03860360383987427
Batch 56/64 loss: -0.004343569278717041
Batch 57/64 loss: -0.022383153438568115
Batch 58/64 loss: -0.01949894428253174
Batch 59/64 loss: -0.010975182056427002
Batch 60/64 loss: -0.010524392127990723
Batch 61/64 loss: -0.009012937545776367
Batch 62/64 loss: -0.016509950160980225
Batch 63/64 loss: -0.011950016021728516
Batch 64/64 loss: -0.007718205451965332
Epoch 139  Train loss: -0.013811234399384144  Val loss: 0.04911964537761465
Saving best model, epoch: 139
Epoch 140
-------------------------------
Batch 1/64 loss: -0.030579090118408203
Batch 2/64 loss: 0.004859328269958496
Batch 3/64 loss: -0.015298426151275635
Batch 4/64 loss: -0.0256805419921875
Batch 5/64 loss: -0.006551504135131836
Batch 6/64 loss: -0.023436665534973145
Batch 7/64 loss: -0.029664456844329834
Batch 8/64 loss: -0.013925909996032715
Batch 9/64 loss: -0.027484595775604248
Batch 10/64 loss: -0.0360754132270813
Batch 11/64 loss: -0.022393107414245605
Batch 12/64 loss: -0.027758479118347168
Batch 13/64 loss: 0.007787942886352539
Batch 14/64 loss: -0.02580893039703369
Batch 15/64 loss: -0.043331682682037354
Batch 16/64 loss: -0.0328679084777832
Batch 17/64 loss: -0.017506957054138184
Batch 18/64 loss: -0.037164509296417236
Batch 19/64 loss: -0.02107793092727661
Batch 20/64 loss: -0.012958288192749023
Batch 21/64 loss: -0.0175173282623291
Batch 22/64 loss: 0.007092773914337158
Batch 23/64 loss: -0.020767569541931152
Batch 24/64 loss: 0.0034505128860473633
Batch 25/64 loss: 0.006895005702972412
Batch 26/64 loss: -0.016852855682373047
Batch 27/64 loss: 0.0057489871978759766
Batch 28/64 loss: -0.011433601379394531
Batch 29/64 loss: -0.03938835859298706
Batch 30/64 loss: 0.016710638999938965
Batch 31/64 loss: -0.03301131725311279
Batch 32/64 loss: -0.01669132709503174
Batch 33/64 loss: 0.0009279251098632812
Batch 34/64 loss: 0.003495454788208008
Batch 35/64 loss: -0.004550755023956299
Batch 36/64 loss: -0.02512061595916748
Batch 37/64 loss: -0.01524055004119873
Batch 38/64 loss: -0.010074377059936523
Batch 39/64 loss: -0.025014638900756836
Batch 40/64 loss: 0.0015377998352050781
Batch 41/64 loss: -0.0020952224731445312
Batch 42/64 loss: 0.002450227737426758
Batch 43/64 loss: -0.016437530517578125
Batch 44/64 loss: 0.005507528781890869
Batch 45/64 loss: -0.014795482158660889
Batch 46/64 loss: -0.011934101581573486
Batch 47/64 loss: -0.02151590585708618
Batch 48/64 loss: -0.03255051374435425
Batch 49/64 loss: -0.01929706335067749
Batch 50/64 loss: -0.01947033405303955
Batch 51/64 loss: -0.03158050775527954
Batch 52/64 loss: -0.017907261848449707
Batch 53/64 loss: -0.01746654510498047
Batch 54/64 loss: -0.016164422035217285
Batch 55/64 loss: -0.010540962219238281
Batch 56/64 loss: -0.01937335729598999
Batch 57/64 loss: -0.01837080717086792
Batch 58/64 loss: -0.031214594841003418
Batch 59/64 loss: -0.009088635444641113
Batch 60/64 loss: -0.014140605926513672
Batch 61/64 loss: -0.016354620456695557
Batch 62/64 loss: -0.02154994010925293
Batch 63/64 loss: -0.015115797519683838
Batch 64/64 loss: -0.001597762107849121
Epoch 140  Train loss: -0.015638056455873975  Val loss: 0.05421596458277751
Epoch 141
-------------------------------
Batch 1/64 loss: 0.0010228157043457031
Batch 2/64 loss: -0.028260529041290283
Batch 3/64 loss: -0.022542119026184082
Batch 4/64 loss: -0.012686550617218018
Batch 5/64 loss: -0.029459655284881592
Batch 6/64 loss: -0.018997490406036377
Batch 7/64 loss: -0.00778043270111084
Batch 8/64 loss: -0.023238182067871094
Batch 9/64 loss: -0.019225656986236572
Batch 10/64 loss: -0.005757272243499756
Batch 11/64 loss: -0.035237014293670654
Batch 12/64 loss: -0.010053753852844238
Batch 13/64 loss: -0.03143191337585449
Batch 14/64 loss: -0.021048545837402344
Batch 15/64 loss: -0.02050071954727173
Batch 16/64 loss: -0.008409500122070312
Batch 17/64 loss: -0.02809053659439087
Batch 18/64 loss: -0.028633058071136475
Batch 19/64 loss: -0.000838935375213623
Batch 20/64 loss: -0.02287161350250244
Batch 21/64 loss: -0.01286160945892334
Batch 22/64 loss: -0.019752979278564453
Batch 23/64 loss: -0.03489947319030762
Batch 24/64 loss: -0.009412765502929688
Batch 25/64 loss: -0.027418136596679688
Batch 26/64 loss: -0.005753874778747559
Batch 27/64 loss: -0.006871402263641357
Batch 28/64 loss: -0.014963209629058838
Batch 29/64 loss: -0.015343725681304932
Batch 30/64 loss: -0.022537291049957275
Batch 31/64 loss: -0.02929610013961792
Batch 32/64 loss: -0.00891345739364624
Batch 33/64 loss: -0.018149137496948242
Batch 34/64 loss: -0.02605658769607544
Batch 35/64 loss: -0.02561497688293457
Batch 36/64 loss: 0.010603785514831543
Batch 37/64 loss: -0.01521676778793335
Batch 38/64 loss: -0.025470852851867676
Batch 39/64 loss: -0.017583131790161133
Batch 40/64 loss: 0.017319023609161377
Batch 41/64 loss: -0.01687443256378174
Batch 42/64 loss: -0.03333425521850586
Batch 43/64 loss: -0.03226655721664429
Batch 44/64 loss: -0.03729051351547241
Batch 45/64 loss: -0.001749277114868164
Batch 46/64 loss: -0.02760171890258789
Batch 47/64 loss: -0.022694647312164307
Batch 48/64 loss: -0.015619993209838867
Batch 49/64 loss: 0.0029201507568359375
Batch 50/64 loss: -0.011233925819396973
Batch 51/64 loss: -0.007347583770751953
Batch 52/64 loss: -0.02870643138885498
Batch 53/64 loss: -0.01690906286239624
Batch 54/64 loss: -0.009243130683898926
Batch 55/64 loss: 0.0008663535118103027
Batch 56/64 loss: 0.0010181069374084473
Batch 57/64 loss: -0.026194453239440918
Batch 58/64 loss: -0.01272439956665039
Batch 59/64 loss: -0.030254840850830078
Batch 60/64 loss: -0.023035109043121338
Batch 61/64 loss: -0.009854257106781006
Batch 62/64 loss: -0.011839628219604492
Batch 63/64 loss: -0.014290153980255127
Batch 64/64 loss: -0.01655513048171997
Epoch 141  Train loss: -0.01692406967574475  Val loss: 0.05133941472600825
Epoch 142
-------------------------------
Batch 1/64 loss: -0.04935896396636963
Batch 2/64 loss: -0.031051039695739746
Batch 3/64 loss: -0.03269404172897339
Batch 4/64 loss: -0.023531019687652588
Batch 5/64 loss: 0.007536053657531738
Batch 6/64 loss: -0.027339398860931396
Batch 7/64 loss: -0.012478828430175781
Batch 8/64 loss: -0.04117894172668457
Batch 9/64 loss: -0.0113753080368042
Batch 10/64 loss: -0.00627058744430542
Batch 11/64 loss: -0.04266202449798584
Batch 12/64 loss: -0.016840815544128418
Batch 13/64 loss: -0.03086632490158081
Batch 14/64 loss: -0.024560987949371338
Batch 15/64 loss: -0.01708430051803589
Batch 16/64 loss: -0.03664165735244751
Batch 17/64 loss: -0.03412967920303345
Batch 18/64 loss: -0.00023984909057617188
Batch 19/64 loss: -0.03030228614807129
Batch 20/64 loss: -0.035509705543518066
Batch 21/64 loss: -0.0034849047660827637
Batch 22/64 loss: -0.006845712661743164
Batch 23/64 loss: 0.006863236427307129
Batch 24/64 loss: -0.009927868843078613
Batch 25/64 loss: -0.03151369094848633
Batch 26/64 loss: -0.022740602493286133
Batch 27/64 loss: -0.02555328607559204
Batch 28/64 loss: 0.02648550271987915
Batch 29/64 loss: -0.019491970539093018
Batch 30/64 loss: -0.008628308773040771
Batch 31/64 loss: -0.026847422122955322
Batch 32/64 loss: -0.0019423365592956543
Batch 33/64 loss: -0.020760416984558105
Batch 34/64 loss: -0.021199703216552734
Batch 35/64 loss: 0.0014119744300842285
Batch 36/64 loss: -0.036386966705322266
Batch 37/64 loss: -0.020745933055877686
Batch 38/64 loss: -0.022095918655395508
Batch 39/64 loss: -0.024705171585083008
Batch 40/64 loss: 0.0007312893867492676
Batch 41/64 loss: -0.0037589669227600098
Batch 42/64 loss: -0.031752169132232666
Batch 43/64 loss: -0.026553869247436523
Batch 44/64 loss: 0.0057288408279418945
Batch 45/64 loss: -0.020896553993225098
Batch 46/64 loss: -0.009780168533325195
Batch 47/64 loss: -0.009705305099487305
Batch 48/64 loss: -0.010379612445831299
Batch 49/64 loss: -0.027280092239379883
Batch 50/64 loss: -0.017009258270263672
Batch 51/64 loss: -0.012676715850830078
Batch 52/64 loss: -0.0212438702583313
Batch 53/64 loss: -0.01219397783279419
Batch 54/64 loss: -0.022273898124694824
Batch 55/64 loss: -0.017801403999328613
Batch 56/64 loss: -0.028891324996948242
Batch 57/64 loss: -0.03022664785385132
Batch 58/64 loss: -0.01606518030166626
Batch 59/64 loss: -0.016430199146270752
Batch 60/64 loss: -0.024077773094177246
Batch 61/64 loss: -0.03386712074279785
Batch 62/64 loss: -0.004488706588745117
Batch 63/64 loss: -0.023688077926635742
Batch 64/64 loss: -0.00116729736328125
Epoch 142  Train loss: -0.01851208537232642  Val loss: 0.051414533169408846
Epoch 143
-------------------------------
Batch 1/64 loss: -0.008924722671508789
Batch 2/64 loss: -0.03562009334564209
Batch 3/64 loss: -0.012442290782928467
Batch 4/64 loss: -0.02770984172821045
Batch 5/64 loss: -0.02440941333770752
Batch 6/64 loss: -0.03167283535003662
Batch 7/64 loss: -0.031830549240112305
Batch 8/64 loss: -0.009512603282928467
Batch 9/64 loss: -0.012717664241790771
Batch 10/64 loss: 0.0011659860610961914
Batch 11/64 loss: -0.014996647834777832
Batch 12/64 loss: -0.035034239292144775
Batch 13/64 loss: -0.04551297426223755
Batch 14/64 loss: -0.026020288467407227
Batch 15/64 loss: -0.015465259552001953
Batch 16/64 loss: -0.010578751564025879
Batch 17/64 loss: -0.020609915256500244
Batch 18/64 loss: -0.01564931869506836
Batch 19/64 loss: 0.01443248987197876
Batch 20/64 loss: -0.02087712287902832
Batch 21/64 loss: -0.024874746799468994
Batch 22/64 loss: 0.001791834831237793
Batch 23/64 loss: -0.026932477951049805
Batch 24/64 loss: -0.02317345142364502
Batch 25/64 loss: -0.004104018211364746
Batch 26/64 loss: -0.012939214706420898
Batch 27/64 loss: -0.0026012659072875977
Batch 28/64 loss: -0.018508613109588623
Batch 29/64 loss: -0.0027937889099121094
Batch 30/64 loss: 0.010867774486541748
Batch 31/64 loss: -0.013135433197021484
Batch 32/64 loss: -0.043202757835388184
Batch 33/64 loss: -0.024669945240020752
Batch 34/64 loss: -0.02700197696685791
Batch 35/64 loss: -0.03051060438156128
Batch 36/64 loss: -0.0038918256759643555
Batch 37/64 loss: -0.006678521633148193
Batch 38/64 loss: -0.006653428077697754
Batch 39/64 loss: -0.007360219955444336
Batch 40/64 loss: -0.025817155838012695
Batch 41/64 loss: -0.02872645854949951
Batch 42/64 loss: -0.012202858924865723
Batch 43/64 loss: -0.013906478881835938
Batch 44/64 loss: -0.012325048446655273
Batch 45/64 loss: -0.006402194499969482
Batch 46/64 loss: -0.0030513405799865723
Batch 47/64 loss: -0.02358776330947876
Batch 48/64 loss: -0.02753615379333496
Batch 49/64 loss: -0.006166279315948486
Batch 50/64 loss: -0.045443832874298096
Batch 51/64 loss: -0.02294832468032837
Batch 52/64 loss: -0.0004779696464538574
Batch 53/64 loss: -0.00170058012008667
Batch 54/64 loss: -0.023290634155273438
Batch 55/64 loss: 0.002919435501098633
Batch 56/64 loss: -0.03210115432739258
Batch 57/64 loss: -0.029079914093017578
Batch 58/64 loss: -0.02375054359436035
Batch 59/64 loss: -0.024281322956085205
Batch 60/64 loss: -0.020955026149749756
Batch 61/64 loss: 0.0008141398429870605
Batch 62/64 loss: -0.017438173294067383
Batch 63/64 loss: -0.025681793689727783
Batch 64/64 loss: -0.04032766819000244
Epoch 143  Train loss: -0.017313598651511997  Val loss: 0.05349023563345683
Epoch 144
-------------------------------
Batch 1/64 loss: -0.03400981426239014
Batch 2/64 loss: -0.02675074338912964
Batch 3/64 loss: -0.01819753646850586
Batch 4/64 loss: -0.0034332871437072754
Batch 5/64 loss: -0.020419716835021973
Batch 6/64 loss: -0.027378380298614502
Batch 7/64 loss: -0.026416659355163574
Batch 8/64 loss: -0.03282827138900757
Batch 9/64 loss: -0.030171513557434082
Batch 10/64 loss: -0.041307032108306885
Batch 11/64 loss: -0.01837557554244995
Batch 12/64 loss: -0.03453099727630615
Batch 13/64 loss: -0.0411609411239624
Batch 14/64 loss: 0.0015318989753723145
Batch 15/64 loss: -0.036953628063201904
Batch 16/64 loss: -0.046561598777770996
Batch 17/64 loss: 0.0012947320938110352
Batch 18/64 loss: -0.03681063652038574
Batch 19/64 loss: -0.013685286045074463
Batch 20/64 loss: 0.0004544854164123535
Batch 21/64 loss: -0.004815101623535156
Batch 22/64 loss: -0.0036106109619140625
Batch 23/64 loss: 0.011939823627471924
Batch 24/64 loss: -0.031502366065979004
Batch 25/64 loss: -0.020564556121826172
Batch 26/64 loss: -0.020622611045837402
Batch 27/64 loss: -0.006331443786621094
Batch 28/64 loss: -0.007784128189086914
Batch 29/64 loss: -0.014034271240234375
Batch 30/64 loss: -0.04194951057434082
Batch 31/64 loss: -0.018718600273132324
Batch 32/64 loss: -0.02035146951675415
Batch 33/64 loss: -0.0229455828666687
Batch 34/64 loss: -0.040656447410583496
Batch 35/64 loss: -0.03396952152252197
Batch 36/64 loss: -0.029687881469726562
Batch 37/64 loss: -0.027577579021453857
Batch 38/64 loss: -0.019602417945861816
Batch 39/64 loss: -0.00426870584487915
Batch 40/64 loss: 0.008393824100494385
Batch 41/64 loss: -0.03327834606170654
Batch 42/64 loss: -0.003438115119934082
Batch 43/64 loss: -0.009041190147399902
Batch 44/64 loss: -0.03863489627838135
Batch 45/64 loss: -0.010028958320617676
Batch 46/64 loss: -0.021344542503356934
Batch 47/64 loss: -0.0409432053565979
Batch 48/64 loss: 0.015712618827819824
Batch 49/64 loss: -0.025745630264282227
Batch 50/64 loss: -0.001765429973602295
Batch 51/64 loss: 0.01684504747390747
Batch 52/64 loss: 0.008159041404724121
Batch 53/64 loss: -0.009342074394226074
Batch 54/64 loss: -0.005034208297729492
Batch 55/64 loss: -0.004536747932434082
Batch 56/64 loss: -0.028104305267333984
Batch 57/64 loss: -0.02329486608505249
Batch 58/64 loss: -0.009315967559814453
Batch 59/64 loss: -0.0069388747215271
Batch 60/64 loss: -0.031780898571014404
Batch 61/64 loss: -0.013734102249145508
Batch 62/64 loss: 0.0037914514541625977
Batch 63/64 loss: -0.024466216564178467
Batch 64/64 loss: -0.002109527587890625
Epoch 144  Train loss: -0.017760191711724972  Val loss: 0.048880780685398584
Saving best model, epoch: 144
Epoch 145
-------------------------------
Batch 1/64 loss: -0.030991315841674805
Batch 2/64 loss: -0.03917050361633301
Batch 3/64 loss: -0.03367120027542114
Batch 4/64 loss: -0.029182791709899902
Batch 5/64 loss: -0.028179287910461426
Batch 6/64 loss: -0.022906839847564697
Batch 7/64 loss: -0.034006714820861816
Batch 8/64 loss: -0.009622931480407715
Batch 9/64 loss: 0.0008594393730163574
Batch 10/64 loss: -0.034664034843444824
Batch 11/64 loss: -0.03948962688446045
Batch 12/64 loss: 0.0050956010818481445
Batch 13/64 loss: 0.01564854383468628
Batch 14/64 loss: -0.0033270716667175293
Batch 15/64 loss: -0.02783668041229248
Batch 16/64 loss: -0.00945049524307251
Batch 17/64 loss: -0.02210855484008789
Batch 18/64 loss: -0.024837613105773926
Batch 19/64 loss: -0.029683291912078857
Batch 20/64 loss: -0.017398953437805176
Batch 21/64 loss: -0.03403109312057495
Batch 22/64 loss: -0.017992913722991943
Batch 23/64 loss: 0.0028856992721557617
Batch 24/64 loss: 0.01027822494506836
Batch 25/64 loss: -0.03048419952392578
Batch 26/64 loss: -0.005435824394226074
Batch 27/64 loss: -0.022758185863494873
Batch 28/64 loss: -0.03491222858428955
Batch 29/64 loss: -0.04005753993988037
Batch 30/64 loss: -0.016444802284240723
Batch 31/64 loss: -8.970499038696289e-05
Batch 32/64 loss: -0.009719312191009521
Batch 33/64 loss: -0.038385212421417236
Batch 34/64 loss: 0.005092024803161621
Batch 35/64 loss: 0.007917821407318115
Batch 36/64 loss: -0.02586674690246582
Batch 37/64 loss: -0.024139225482940674
Batch 38/64 loss: -0.01348942518234253
Batch 39/64 loss: -0.014882981777191162
Batch 40/64 loss: -0.027263283729553223
Batch 41/64 loss: -0.005582094192504883
Batch 42/64 loss: -0.0043059587478637695
Batch 43/64 loss: -0.013028740882873535
Batch 44/64 loss: -0.006552994251251221
Batch 45/64 loss: -0.03293734788894653
Batch 46/64 loss: -0.025969088077545166
Batch 47/64 loss: -0.026488900184631348
Batch 48/64 loss: 0.007915377616882324
Batch 49/64 loss: 0.010584354400634766
Batch 50/64 loss: -0.009526729583740234
Batch 51/64 loss: -0.042916834354400635
Batch 52/64 loss: -0.05187225341796875
Batch 53/64 loss: -0.02505314350128174
Batch 54/64 loss: 0.009564459323883057
Batch 55/64 loss: -0.010084390640258789
Batch 56/64 loss: -0.025284528732299805
Batch 57/64 loss: -0.034513235092163086
Batch 58/64 loss: -0.002166271209716797
Batch 59/64 loss: -0.025932013988494873
Batch 60/64 loss: -0.0307466983795166
Batch 61/64 loss: -0.008891105651855469
Batch 62/64 loss: -0.0025795698165893555
Batch 63/64 loss: -0.021142899990081787
Batch 64/64 loss: -0.018048226833343506
Epoch 145  Train loss: -0.01781568644093532  Val loss: 0.050732233679990996
Epoch 146
-------------------------------
Batch 1/64 loss: -0.018369197845458984
Batch 2/64 loss: -0.013338923454284668
Batch 3/64 loss: -0.026511311531066895
Batch 4/64 loss: 0.0019500255584716797
Batch 5/64 loss: -0.027824759483337402
Batch 6/64 loss: -0.006454169750213623
Batch 7/64 loss: -0.01604360342025757
Batch 8/64 loss: -0.007194399833679199
Batch 9/64 loss: -0.01665031909942627
Batch 10/64 loss: -0.008814871311187744
Batch 11/64 loss: -0.023746252059936523
Batch 12/64 loss: -0.01408243179321289
Batch 13/64 loss: -0.04574716091156006
Batch 14/64 loss: -0.03086918592453003
Batch 15/64 loss: 0.007436096668243408
Batch 16/64 loss: -0.021014153957366943
Batch 17/64 loss: -0.016623377799987793
Batch 18/64 loss: -0.0035970211029052734
Batch 19/64 loss: -0.02713954448699951
Batch 20/64 loss: -0.024399876594543457
Batch 21/64 loss: -0.02899867296218872
Batch 22/64 loss: -0.029507935047149658
Batch 23/64 loss: -0.004452347755432129
Batch 24/64 loss: -0.01702558994293213
Batch 25/64 loss: -0.0246354341506958
Batch 26/64 loss: -0.0321197509765625
Batch 27/64 loss: -0.021106958389282227
Batch 28/64 loss: -0.03107970952987671
Batch 29/64 loss: -0.009397506713867188
Batch 30/64 loss: -0.04203671216964722
Batch 31/64 loss: 0.006134748458862305
Batch 32/64 loss: -0.031200110912322998
Batch 33/64 loss: -0.02350085973739624
Batch 34/64 loss: -0.030966341495513916
Batch 35/64 loss: -0.013732314109802246
Batch 36/64 loss: -0.005842745304107666
Batch 37/64 loss: -0.017018616199493408
Batch 38/64 loss: -0.006020724773406982
Batch 39/64 loss: 0.02896714210510254
Batch 40/64 loss: -0.01006227731704712
Batch 41/64 loss: -0.041558921337127686
Batch 42/64 loss: -0.013776421546936035
Batch 43/64 loss: -0.0218888521194458
Batch 44/64 loss: -0.020556509494781494
Batch 45/64 loss: -0.0065351128578186035
Batch 46/64 loss: -0.025094270706176758
Batch 47/64 loss: -0.018081605434417725
Batch 48/64 loss: -0.009783148765563965
Batch 49/64 loss: -0.012913286685943604
Batch 50/64 loss: -0.026137053966522217
Batch 51/64 loss: -0.027514755725860596
Batch 52/64 loss: -0.038235604763031006
Batch 53/64 loss: -0.02820742130279541
Batch 54/64 loss: -0.02031642198562622
Batch 55/64 loss: -0.004979372024536133
Batch 56/64 loss: -0.022364675998687744
Batch 57/64 loss: -0.0034362077713012695
Batch 58/64 loss: -0.025030076503753662
Batch 59/64 loss: -0.020130813121795654
Batch 60/64 loss: -0.010897517204284668
Batch 61/64 loss: -0.03288954496383667
Batch 62/64 loss: -0.04537695646286011
Batch 63/64 loss: -0.02808135747909546
Batch 64/64 loss: -0.009681642055511475
Epoch 146  Train loss: -0.018724459526585597  Val loss: 0.051556255399566334
Epoch 147
-------------------------------
Batch 1/64 loss: -0.030230164527893066
Batch 2/64 loss: -0.03067302703857422
Batch 3/64 loss: -0.030946671962738037
Batch 4/64 loss: -0.015527904033660889
Batch 5/64 loss: -0.012372255325317383
Batch 6/64 loss: -0.03370189666748047
Batch 7/64 loss: -0.04682624340057373
Batch 8/64 loss: -0.04206883907318115
Batch 9/64 loss: -0.034667789936065674
Batch 10/64 loss: -0.01957792043685913
Batch 11/64 loss: -0.01413583755493164
Batch 12/64 loss: -0.001995086669921875
Batch 13/64 loss: -0.0020312070846557617
Batch 14/64 loss: -0.018072903156280518
Batch 15/64 loss: -0.02855813503265381
Batch 16/64 loss: -0.01976555585861206
Batch 17/64 loss: -0.014265477657318115
Batch 18/64 loss: -0.029709935188293457
Batch 19/64 loss: -0.022179901599884033
Batch 20/64 loss: -0.018509626388549805
Batch 21/64 loss: -0.013282299041748047
Batch 22/64 loss: -0.0014055967330932617
Batch 23/64 loss: 0.0019965767860412598
Batch 24/64 loss: -0.017679035663604736
Batch 25/64 loss: -0.010804176330566406
Batch 26/64 loss: -0.020534753799438477
Batch 27/64 loss: 0.0049253106117248535
Batch 28/64 loss: -0.011209726333618164
Batch 29/64 loss: -0.024479150772094727
Batch 30/64 loss: -0.01921379566192627
Batch 31/64 loss: -0.0256231427192688
Batch 32/64 loss: -0.019564390182495117
Batch 33/64 loss: -0.020504474639892578
Batch 34/64 loss: -0.014256119728088379
Batch 35/64 loss: -0.01775383949279785
Batch 36/64 loss: -0.032922983169555664
Batch 37/64 loss: 0.002671658992767334
Batch 38/64 loss: -0.017684996128082275
Batch 39/64 loss: -0.02700972557067871
Batch 40/64 loss: -0.03039383888244629
Batch 41/64 loss: 0.02159249782562256
Batch 42/64 loss: -0.01329505443572998
Batch 43/64 loss: -0.010280966758728027
Batch 44/64 loss: -0.03099358081817627
Batch 45/64 loss: -0.028517484664916992
Batch 46/64 loss: -0.0016877055168151855
Batch 47/64 loss: -0.006048262119293213
Batch 48/64 loss: -0.02562272548675537
Batch 49/64 loss: -0.012045979499816895
Batch 50/64 loss: -0.019877135753631592
Batch 51/64 loss: -0.019195258617401123
Batch 52/64 loss: -0.031021058559417725
Batch 53/64 loss: -0.018633544445037842
Batch 54/64 loss: -0.03317803144454956
Batch 55/64 loss: 0.0010552406311035156
Batch 56/64 loss: -0.028269469738006592
Batch 57/64 loss: -0.020177841186523438
Batch 58/64 loss: 0.011038541793823242
Batch 59/64 loss: -0.002033531665802002
Batch 60/64 loss: -0.0265694260597229
Batch 61/64 loss: -0.02424997091293335
Batch 62/64 loss: -0.023625969886779785
Batch 63/64 loss: -0.015141665935516357
Batch 64/64 loss: -0.01512002944946289
Epoch 147  Train loss: -0.018018247566971124  Val loss: 0.052521915779900306
Epoch 148
-------------------------------
Batch 1/64 loss: -0.026498079299926758
Batch 2/64 loss: -0.018515288829803467
Batch 3/64 loss: -0.021570682525634766
Batch 4/64 loss: -0.023052215576171875
Batch 5/64 loss: -0.021550774574279785
Batch 6/64 loss: -0.034791529178619385
Batch 7/64 loss: -0.020112156867980957
Batch 8/64 loss: -0.021076440811157227
Batch 9/64 loss: -0.019346177577972412
Batch 10/64 loss: -0.02032935619354248
Batch 11/64 loss: -0.03180187940597534
Batch 12/64 loss: -0.025141417980194092
Batch 13/64 loss: -0.005582094192504883
Batch 14/64 loss: -0.00201416015625
Batch 15/64 loss: -0.016349315643310547
Batch 16/64 loss: 0.004250228404998779
Batch 17/64 loss: -0.031789541244506836
Batch 18/64 loss: -0.024143576622009277
Batch 19/64 loss: -0.01908642053604126
Batch 20/64 loss: -0.03289598226547241
Batch 21/64 loss: -0.028701603412628174
Batch 22/64 loss: -0.03295618295669556
Batch 23/64 loss: -0.032764434814453125
Batch 24/64 loss: -0.032176971435546875
Batch 25/64 loss: -0.034695208072662354
Batch 26/64 loss: -0.006464481353759766
Batch 27/64 loss: -0.022017300128936768
Batch 28/64 loss: 0.005947232246398926
Batch 29/64 loss: -0.0167539119720459
Batch 30/64 loss: -0.01786649227142334
Batch 31/64 loss: -0.01411527395248413
Batch 32/64 loss: -0.0017991065979003906
Batch 33/64 loss: -0.03954869508743286
Batch 34/64 loss: -0.03575211763381958
Batch 35/64 loss: -0.0357014536857605
Batch 36/64 loss: -0.0049988627433776855
Batch 37/64 loss: -0.0226019024848938
Batch 38/64 loss: -0.014482676982879639
Batch 39/64 loss: -0.02250981330871582
Batch 40/64 loss: -0.04929506778717041
Batch 41/64 loss: 0.0032464265823364258
Batch 42/64 loss: -0.01691204309463501
Batch 43/64 loss: 0.0015808343887329102
Batch 44/64 loss: -0.033103764057159424
Batch 45/64 loss: -0.008844137191772461
Batch 46/64 loss: -0.014800786972045898
Batch 47/64 loss: -0.023498713970184326
Batch 48/64 loss: -0.005405843257904053
Batch 49/64 loss: -0.017229080200195312
Batch 50/64 loss: -0.006475687026977539
Batch 51/64 loss: -0.03390216827392578
Batch 52/64 loss: -0.036769747734069824
Batch 53/64 loss: -0.045144736766815186
Batch 54/64 loss: -0.030281484127044678
Batch 55/64 loss: -0.021743595600128174
Batch 56/64 loss: -0.017341017723083496
Batch 57/64 loss: 0.0112229585647583
Batch 58/64 loss: -0.026744186878204346
Batch 59/64 loss: -0.022712349891662598
Batch 60/64 loss: -0.01980900764465332
Batch 61/64 loss: -0.019106924533843994
Batch 62/64 loss: -0.0036301016807556152
Batch 63/64 loss: -0.020079314708709717
Batch 64/64 loss: 0.030260205268859863
Epoch 148  Train loss: -0.01969318436641319  Val loss: 0.05366969825475896
Epoch 149
-------------------------------
Batch 1/64 loss: -0.019573330879211426
Batch 2/64 loss: -0.0016359686851501465
Batch 3/64 loss: -0.02526092529296875
Batch 4/64 loss: -0.02309572696685791
Batch 5/64 loss: -0.029806017875671387
Batch 6/64 loss: -0.051982760429382324
Batch 7/64 loss: -0.02746450901031494
Batch 8/64 loss: -0.020567238330841064
Batch 9/64 loss: -0.03356671333312988
Batch 10/64 loss: -0.03312712907791138
Batch 11/64 loss: -0.02441275119781494
Batch 12/64 loss: -0.0005402565002441406
Batch 13/64 loss: -0.025064170360565186
Batch 14/64 loss: 0.001968979835510254
Batch 15/64 loss: -0.009318172931671143
Batch 16/64 loss: -0.016647636890411377
Batch 17/64 loss: -0.02849137783050537
Batch 18/64 loss: -0.03361237049102783
Batch 19/64 loss: -0.036034345626831055
Batch 20/64 loss: -0.01394343376159668
Batch 21/64 loss: 0.01455533504486084
Batch 22/64 loss: -0.0184134840965271
Batch 23/64 loss: -0.04344373941421509
Batch 24/64 loss: -0.013991773128509521
Batch 25/64 loss: -0.033528923988342285
Batch 26/64 loss: -0.03335833549499512
Batch 27/64 loss: -0.01752924919128418
Batch 28/64 loss: -0.012633323669433594
Batch 29/64 loss: -0.02011650800704956
Batch 30/64 loss: -0.04423397779464722
Batch 31/64 loss: -0.026633501052856445
Batch 32/64 loss: -0.018638253211975098
Batch 33/64 loss: -0.023002147674560547
Batch 34/64 loss: -0.03594505786895752
Batch 35/64 loss: -0.024747192859649658
Batch 36/64 loss: -0.017389774322509766
Batch 37/64 loss: 0.0014483332633972168
Batch 38/64 loss: -0.019378483295440674
Batch 39/64 loss: -0.01105797290802002
Batch 40/64 loss: -0.005663692951202393
Batch 41/64 loss: -0.04066026210784912
Batch 42/64 loss: -0.023325204849243164
Batch 43/64 loss: -0.007635176181793213
Batch 44/64 loss: -0.02835381031036377
Batch 45/64 loss: -0.033251941204071045
Batch 46/64 loss: -0.007305920124053955
Batch 47/64 loss: -0.004581332206726074
Batch 48/64 loss: 0.007734179496765137
Batch 49/64 loss: -0.026771068572998047
Batch 50/64 loss: -0.030070602893829346
Batch 51/64 loss: -0.025164484977722168
Batch 52/64 loss: -0.026951730251312256
Batch 53/64 loss: -0.032080650329589844
Batch 54/64 loss: -0.04756057262420654
Batch 55/64 loss: -0.04165160655975342
Batch 56/64 loss: -0.016064047813415527
Batch 57/64 loss: -0.02247941493988037
Batch 58/64 loss: -0.0024303197860717773
Batch 59/64 loss: -0.01947653293609619
Batch 60/64 loss: -0.01923227310180664
Batch 61/64 loss: -0.020930767059326172
Batch 62/64 loss: -0.029871046543121338
Batch 63/64 loss: -0.032372891902923584
Batch 64/64 loss: -0.021173179149627686
Epoch 149  Train loss: -0.02199599906509998  Val loss: 0.05134113527245538
Epoch 150
-------------------------------
Batch 1/64 loss: -0.03994220495223999
Batch 2/64 loss: -0.023444652557373047
Batch 3/64 loss: -0.029729843139648438
Batch 4/64 loss: -0.01770472526550293
Batch 5/64 loss: -0.013036608695983887
Batch 6/64 loss: -0.016798019409179688
Batch 7/64 loss: -0.0161360502243042
Batch 8/64 loss: -0.02636408805847168
Batch 9/64 loss: -0.02429068088531494
Batch 10/64 loss: -0.01864558458328247
Batch 11/64 loss: -0.03881251811981201
Batch 12/64 loss: -0.04271632432937622
Batch 13/64 loss: -0.021439850330352783
Batch 14/64 loss: -0.018801212310791016
Batch 15/64 loss: -0.022066235542297363
Batch 16/64 loss: -0.044523417949676514
Batch 17/64 loss: -0.028024911880493164
Batch 18/64 loss: -0.034040629863739014
Batch 19/64 loss: -0.02646273374557495
Batch 20/64 loss: -0.019925177097320557
Batch 21/64 loss: -0.0310479998588562
Batch 22/64 loss: -0.02397388219833374
Batch 23/64 loss: -0.032671988010406494
Batch 24/64 loss: -0.013294517993927002
Batch 25/64 loss: -0.03583258390426636
Batch 26/64 loss: -0.025643110275268555
Batch 27/64 loss: -0.02865004539489746
Batch 28/64 loss: -0.012663722038269043
Batch 29/64 loss: -0.03692209720611572
Batch 30/64 loss: -0.01400744915008545
Batch 31/64 loss: -0.03326314687728882
Batch 32/64 loss: -0.0242842435836792
Batch 33/64 loss: -0.012861967086791992
Batch 34/64 loss: -0.00831538438796997
Batch 35/64 loss: 0.0032628774642944336
Batch 36/64 loss: -0.019809961318969727
Batch 37/64 loss: -0.018961012363433838
Batch 38/64 loss: -0.03281545639038086
Batch 39/64 loss: -0.016877710819244385
Batch 40/64 loss: -0.041154444217681885
Batch 41/64 loss: -0.0007472634315490723
Batch 42/64 loss: -0.023505568504333496
Batch 43/64 loss: -0.0230829119682312
Batch 44/64 loss: -0.020445525646209717
Batch 45/64 loss: -0.0033309459686279297
Batch 46/64 loss: -0.02622354030609131
Batch 47/64 loss: -0.014588415622711182
Batch 48/64 loss: -0.02154320478439331
Batch 49/64 loss: 0.007251620292663574
Batch 50/64 loss: 0.001996457576751709
Batch 51/64 loss: -0.026586294174194336
Batch 52/64 loss: -0.027338266372680664
Batch 53/64 loss: -0.015663325786590576
Batch 54/64 loss: -0.006931185722351074
Batch 55/64 loss: -0.01039046049118042
Batch 56/64 loss: -0.02559661865234375
Batch 57/64 loss: -0.024287760257720947
Batch 58/64 loss: -0.034750938415527344
Batch 59/64 loss: -0.030805766582489014
Batch 60/64 loss: -0.0531347393989563
Batch 61/64 loss: -0.0070185065269470215
Batch 62/64 loss: -0.011111199855804443
Batch 63/64 loss: -0.02889847755432129
Batch 64/64 loss: -0.03196263313293457
Epoch 150  Train loss: -0.022484676510679955  Val loss: 0.05237780178535435
Epoch 151
-------------------------------
Batch 1/64 loss: -0.018583357334136963
Batch 2/64 loss: -0.029206573963165283
Batch 3/64 loss: -0.004950463771820068
Batch 4/64 loss: -0.020078659057617188
Batch 5/64 loss: -0.015901684761047363
Batch 6/64 loss: -0.02717268466949463
Batch 7/64 loss: -0.046260952949523926
Batch 8/64 loss: -0.015407323837280273
Batch 9/64 loss: -0.03820228576660156
Batch 10/64 loss: -0.05746889114379883
Batch 11/64 loss: -0.008808732032775879
Batch 12/64 loss: -0.023020446300506592
Batch 13/64 loss: -0.01599419116973877
Batch 14/64 loss: -0.0129472017288208
Batch 15/64 loss: -0.02265387773513794
Batch 16/64 loss: -0.020857393741607666
Batch 17/64 loss: -0.024661779403686523
Batch 18/64 loss: -0.03362375497817993
Batch 19/64 loss: -0.028487801551818848
Batch 20/64 loss: -0.02625906467437744
Batch 21/64 loss: -0.045572638511657715
Batch 22/64 loss: -0.016130924224853516
Batch 23/64 loss: -0.03285062313079834
Batch 24/64 loss: -0.05137050151824951
Batch 25/64 loss: -0.01851975917816162
Batch 26/64 loss: -0.02869546413421631
Batch 27/64 loss: -0.03880113363265991
Batch 28/64 loss: -0.015412867069244385
Batch 29/64 loss: -0.0327109694480896
Batch 30/64 loss: -0.021001219749450684
Batch 31/64 loss: -0.01794254779815674
Batch 32/64 loss: -0.016394197940826416
Batch 33/64 loss: -0.028024733066558838
Batch 34/64 loss: -0.020492255687713623
Batch 35/64 loss: 0.0022079944610595703
Batch 36/64 loss: -0.014748930931091309
Batch 37/64 loss: -0.030502796173095703
Batch 38/64 loss: -0.021538376808166504
Batch 39/64 loss: -0.022223949432373047
Batch 40/64 loss: -0.030172884464263916
Batch 41/64 loss: -0.025528311729431152
Batch 42/64 loss: -0.018448293209075928
Batch 43/64 loss: -0.0016153454780578613
Batch 44/64 loss: 0.016924023628234863
Batch 45/64 loss: -0.023198306560516357
Batch 46/64 loss: -0.011720716953277588
Batch 47/64 loss: 0.0028036832809448242
Batch 48/64 loss: -0.041341423988342285
Batch 49/64 loss: -0.03573966026306152
Batch 50/64 loss: -0.031403541564941406
Batch 51/64 loss: -0.02385425567626953
Batch 52/64 loss: -0.029388904571533203
Batch 53/64 loss: -0.009575128555297852
Batch 54/64 loss: -0.024378955364227295
Batch 55/64 loss: -0.010720968246459961
Batch 56/64 loss: -0.024039268493652344
Batch 57/64 loss: -0.018409013748168945
Batch 58/64 loss: -0.04397314786911011
Batch 59/64 loss: -0.03742605447769165
Batch 60/64 loss: -0.03425723314285278
Batch 61/64 loss: -0.021446406841278076
Batch 62/64 loss: -0.009457051753997803
Batch 63/64 loss: -0.032452404499053955
Batch 64/64 loss: -0.00862884521484375
Epoch 151  Train loss: -0.0233186544156542  Val loss: 0.05195999493713641
Epoch 152
-------------------------------
Batch 1/64 loss: -0.011597990989685059
Batch 2/64 loss: -0.025589346885681152
Batch 3/64 loss: -0.01745051145553589
Batch 4/64 loss: -0.03181999921798706
Batch 5/64 loss: -0.03875100612640381
Batch 6/64 loss: -0.04145139455795288
Batch 7/64 loss: -0.012557268142700195
Batch 8/64 loss: -0.024951577186584473
Batch 9/64 loss: -0.016187846660614014
Batch 10/64 loss: -0.02973085641860962
Batch 11/64 loss: -0.03647470474243164
Batch 12/64 loss: -0.020396053791046143
Batch 13/64 loss: -0.03295010328292847
Batch 14/64 loss: -0.024444282054901123
Batch 15/64 loss: -0.03898131847381592
Batch 16/64 loss: -0.03149700164794922
Batch 17/64 loss: -0.005757331848144531
Batch 18/64 loss: -0.028114259243011475
Batch 19/64 loss: -0.016561686992645264
Batch 20/64 loss: -0.028937995433807373
Batch 21/64 loss: -0.05233198404312134
Batch 22/64 loss: -0.033808350563049316
Batch 23/64 loss: -0.03814917802810669
Batch 24/64 loss: -0.03652673959732056
Batch 25/64 loss: -0.030437171459197998
Batch 26/64 loss: -0.045316874980926514
Batch 27/64 loss: -0.04187887907028198
Batch 28/64 loss: -0.048046112060546875
Batch 29/64 loss: -0.015404224395751953
Batch 30/64 loss: -0.01997697353363037
Batch 31/64 loss: -0.04120028018951416
Batch 32/64 loss: -0.02684420347213745
Batch 33/64 loss: -0.011840760707855225
Batch 34/64 loss: -0.032580792903900146
Batch 35/64 loss: -0.04049551486968994
Batch 36/64 loss: -0.03094428777694702
Batch 37/64 loss: -0.02399975061416626
Batch 38/64 loss: 0.0006220340728759766
Batch 39/64 loss: -0.02052837610244751
Batch 40/64 loss: -0.013797879219055176
Batch 41/64 loss: -0.03351813554763794
Batch 42/64 loss: 0.004130423069000244
Batch 43/64 loss: -0.03671097755432129
Batch 44/64 loss: -0.02606046199798584
Batch 45/64 loss: -0.024140119552612305
Batch 46/64 loss: 0.03611689805984497
Batch 47/64 loss: 0.0019890666007995605
Batch 48/64 loss: -0.011852264404296875
Batch 49/64 loss: -0.020354747772216797
Batch 50/64 loss: -0.03768754005432129
Batch 51/64 loss: -0.03140676021575928
Batch 52/64 loss: -0.03538155555725098
Batch 53/64 loss: -0.01736283302307129
Batch 54/64 loss: -0.027746081352233887
Batch 55/64 loss: -0.02470242977142334
Batch 56/64 loss: -0.029654204845428467
Batch 57/64 loss: -0.038487374782562256
Batch 58/64 loss: -0.0152815580368042
Batch 59/64 loss: -0.02694082260131836
Batch 60/64 loss: -0.013742327690124512
Batch 61/64 loss: -0.019902944564819336
Batch 62/64 loss: -0.0026327967643737793
Batch 63/64 loss: 0.004027664661407471
Batch 64/64 loss: -0.016670823097229004
Epoch 152  Train loss: -0.024431275853923722  Val loss: 0.05117298811162051
Epoch 153
-------------------------------
Batch 1/64 loss: 0.006894171237945557
Batch 2/64 loss: -0.014474451541900635
Batch 3/64 loss: -0.024644672870635986
Batch 4/64 loss: -0.021830499172210693
Batch 5/64 loss: -0.006385326385498047
Batch 6/64 loss: -0.033867478370666504
Batch 7/64 loss: 0.003491342067718506
Batch 8/64 loss: -0.04499506950378418
Batch 9/64 loss: -0.030754804611206055
Batch 10/64 loss: -0.015995323657989502
Batch 11/64 loss: -0.023410499095916748
Batch 12/64 loss: -0.017054080963134766
Batch 13/64 loss: -0.04283934831619263
Batch 14/64 loss: -0.018658578395843506
Batch 15/64 loss: -0.0161474347114563
Batch 16/64 loss: -0.02610623836517334
Batch 17/64 loss: -0.020781517028808594
Batch 18/64 loss: -0.05207717418670654
Batch 19/64 loss: -0.024556398391723633
Batch 20/64 loss: -0.01520770788192749
Batch 21/64 loss: 0.009476065635681152
Batch 22/64 loss: -0.013765573501586914
Batch 23/64 loss: -0.04471343755722046
Batch 24/64 loss: 0.0038573145866394043
Batch 25/64 loss: -0.038386523723602295
Batch 26/64 loss: -0.023198485374450684
Batch 27/64 loss: -0.01705867052078247
Batch 28/64 loss: -0.03012681007385254
Batch 29/64 loss: -0.014741122722625732
Batch 30/64 loss: -0.0074800848960876465
Batch 31/64 loss: -0.028849899768829346
Batch 32/64 loss: -0.015531539916992188
Batch 33/64 loss: -0.02499455213546753
Batch 34/64 loss: -0.022699475288391113
Batch 35/64 loss: -0.012376189231872559
Batch 36/64 loss: -0.0002868175506591797
Batch 37/64 loss: -0.02614492177963257
Batch 38/64 loss: -0.024013280868530273
Batch 39/64 loss: -0.009219586849212646
Batch 40/64 loss: -0.04723250865936279
Batch 41/64 loss: -0.028379380702972412
Batch 42/64 loss: -0.044373393058776855
Batch 43/64 loss: -0.03346753120422363
Batch 44/64 loss: -0.013257980346679688
Batch 45/64 loss: -0.013390183448791504
Batch 46/64 loss: -0.016103804111480713
Batch 47/64 loss: -0.013028860092163086
Batch 48/64 loss: -0.02571803331375122
Batch 49/64 loss: -0.04335898160934448
Batch 50/64 loss: -0.03156524896621704
Batch 51/64 loss: -0.03600442409515381
Batch 52/64 loss: -0.019705891609191895
Batch 53/64 loss: -0.033030688762664795
Batch 54/64 loss: -0.03170657157897949
Batch 55/64 loss: -0.007724761962890625
Batch 56/64 loss: -0.05333983898162842
Batch 57/64 loss: -0.033840298652648926
Batch 58/64 loss: -0.017861366271972656
Batch 59/64 loss: -0.036592960357666016
Batch 60/64 loss: -0.04878312349319458
Batch 61/64 loss: -0.0229874849319458
Batch 62/64 loss: -0.027834951877593994
Batch 63/64 loss: -0.04492276906967163
Batch 64/64 loss: -0.035920023918151855
Epoch 153  Train loss: -0.02401263900831634  Val loss: 0.051971955397694385
Epoch 154
-------------------------------
Batch 1/64 loss: 0.007505893707275391
Batch 2/64 loss: -0.04158461093902588
Batch 3/64 loss: -0.04665577411651611
Batch 4/64 loss: -0.03335016965866089
Batch 5/64 loss: -0.024857819080352783
Batch 6/64 loss: -0.022073209285736084
Batch 7/64 loss: -0.012764334678649902
Batch 8/64 loss: -0.027894914150238037
Batch 9/64 loss: -0.045196712017059326
Batch 10/64 loss: -0.026279330253601074
Batch 11/64 loss: -0.02492225170135498
Batch 12/64 loss: -0.02314627170562744
Batch 13/64 loss: -0.02423793077468872
Batch 14/64 loss: -0.03336739540100098
Batch 15/64 loss: -0.031039535999298096
Batch 16/64 loss: -0.024842441082000732
Batch 17/64 loss: -0.0333409309387207
Batch 18/64 loss: -0.008289635181427002
Batch 19/64 loss: -0.04630470275878906
Batch 20/64 loss: -0.015242040157318115
Batch 21/64 loss: -0.001395583152770996
Batch 22/64 loss: -0.022151291370391846
Batch 23/64 loss: -0.036590397357940674
Batch 24/64 loss: -0.01862257719039917
Batch 25/64 loss: -0.02466350793838501
Batch 26/64 loss: -0.03931933641433716
Batch 27/64 loss: -0.02410048246383667
Batch 28/64 loss: -0.036287009716033936
Batch 29/64 loss: -0.010044276714324951
Batch 30/64 loss: -0.03381454944610596
Batch 31/64 loss: -0.031023502349853516
Batch 32/64 loss: -0.022142231464385986
Batch 33/64 loss: -0.015153884887695312
Batch 34/64 loss: -0.03710430860519409
Batch 35/64 loss: -0.02292776107788086
Batch 36/64 loss: -0.028738677501678467
Batch 37/64 loss: -0.03405416011810303
Batch 38/64 loss: -0.043945252895355225
Batch 39/64 loss: -0.022779226303100586
Batch 40/64 loss: -0.028519928455352783
Batch 41/64 loss: -0.04503506422042847
Batch 42/64 loss: -0.025331199169158936
Batch 43/64 loss: -0.02139371633529663
Batch 44/64 loss: 0.004288315773010254
Batch 45/64 loss: -0.01560664176940918
Batch 46/64 loss: -0.012172818183898926
Batch 47/64 loss: -0.005070805549621582
Batch 48/64 loss: -0.023641645908355713
Batch 49/64 loss: -0.05113792419433594
Batch 50/64 loss: -0.011889815330505371
Batch 51/64 loss: -0.014642655849456787
Batch 52/64 loss: -0.027824580669403076
Batch 53/64 loss: -0.020251572132110596
Batch 54/64 loss: -0.020469725131988525
Batch 55/64 loss: -0.007085740566253662
Batch 56/64 loss: -0.010854244232177734
Batch 57/64 loss: -0.030402004718780518
Batch 58/64 loss: -0.03718900680541992
Batch 59/64 loss: -0.016571640968322754
Batch 60/64 loss: -0.030089616775512695
Batch 61/64 loss: -0.008294939994812012
Batch 62/64 loss: -0.013066411018371582
Batch 63/64 loss: 0.0017808079719543457
Batch 64/64 loss: -0.03133738040924072
Epoch 154  Train loss: -0.024105376355788286  Val loss: 0.04994116800347554
Epoch 155
-------------------------------
Batch 1/64 loss: -0.03947049379348755
Batch 2/64 loss: -0.004388689994812012
Batch 3/64 loss: -0.040892183780670166
Batch 4/64 loss: -0.0250779390335083
Batch 5/64 loss: -0.04511898756027222
Batch 6/64 loss: -0.0433083176612854
Batch 7/64 loss: -0.030246257781982422
Batch 8/64 loss: -0.02388167381286621
Batch 9/64 loss: -0.03860604763031006
Batch 10/64 loss: -0.055021584033966064
Batch 11/64 loss: -0.021060824394226074
Batch 12/64 loss: -0.024155855178833008
Batch 13/64 loss: -0.004649758338928223
Batch 14/64 loss: -0.016515910625457764
Batch 15/64 loss: -0.03644406795501709
Batch 16/64 loss: -0.025839507579803467
Batch 17/64 loss: 0.00041663646697998047
Batch 18/64 loss: -0.012935340404510498
Batch 19/64 loss: -0.026149749755859375
Batch 20/64 loss: -0.040507376194000244
Batch 21/64 loss: -0.02391338348388672
Batch 22/64 loss: -0.03442293405532837
Batch 23/64 loss: -0.030960559844970703
Batch 24/64 loss: 0.006817340850830078
Batch 25/64 loss: -0.027791976928710938
Batch 26/64 loss: -0.022455155849456787
Batch 27/64 loss: -0.03149539232254028
Batch 28/64 loss: 0.006674408912658691
Batch 29/64 loss: -0.03542792797088623
Batch 30/64 loss: -0.030575931072235107
Batch 31/64 loss: -0.03013443946838379
Batch 32/64 loss: -0.01007753610610962
Batch 33/64 loss: -0.038538217544555664
Batch 34/64 loss: -0.04546231031417847
Batch 35/64 loss: -0.01657778024673462
Batch 36/64 loss: -0.0409734845161438
Batch 37/64 loss: -0.0425296425819397
Batch 38/64 loss: -0.04830223321914673
Batch 39/64 loss: -0.03829813003540039
Batch 40/64 loss: -0.03011488914489746
Batch 41/64 loss: -0.0015543699264526367
Batch 42/64 loss: -0.009015917778015137
Batch 43/64 loss: -0.016685187816619873
Batch 44/64 loss: -0.03635966777801514
Batch 45/64 loss: -0.02078855037689209
Batch 46/64 loss: -0.016660869121551514
Batch 47/64 loss: -0.015794038772583008
Batch 48/64 loss: -0.019835948944091797
Batch 49/64 loss: -0.025500118732452393
Batch 50/64 loss: -0.04386627674102783
Batch 51/64 loss: -0.010554850101470947
Batch 52/64 loss: -0.024616658687591553
Batch 53/64 loss: -0.03444051742553711
Batch 54/64 loss: -0.02443552017211914
Batch 55/64 loss: -0.047340571880340576
Batch 56/64 loss: -0.011287093162536621
Batch 57/64 loss: -0.013080358505249023
Batch 58/64 loss: -0.015567421913146973
Batch 59/64 loss: -0.04570269584655762
Batch 60/64 loss: -0.023220300674438477
Batch 61/64 loss: -0.015190482139587402
Batch 62/64 loss: -0.02412593364715576
Batch 63/64 loss: -0.01520758867263794
Batch 64/64 loss: -0.035644710063934326
Epoch 155  Train loss: -0.025976134515276143  Val loss: 0.050269036768228324
Epoch 156
-------------------------------
Batch 1/64 loss: -0.030653417110443115
Batch 2/64 loss: -0.014664173126220703
Batch 3/64 loss: -0.03049832582473755
Batch 4/64 loss: -0.03861665725708008
Batch 5/64 loss: -0.04814964532852173
Batch 6/64 loss: -0.028264999389648438
Batch 7/64 loss: -0.03778481483459473
Batch 8/64 loss: -0.02002251148223877
Batch 9/64 loss: 0.0015285015106201172
Batch 10/64 loss: -0.04550713300704956
Batch 11/64 loss: -0.018198132514953613
Batch 12/64 loss: -0.036626577377319336
Batch 13/64 loss: -0.0500752329826355
Batch 14/64 loss: -0.05197793245315552
Batch 15/64 loss: 0.0023868680000305176
Batch 16/64 loss: 0.01163703203201294
Batch 17/64 loss: -0.03327280282974243
Batch 18/64 loss: -0.035645484924316406
Batch 19/64 loss: -0.021435320377349854
Batch 20/64 loss: -0.036731839179992676
Batch 21/64 loss: -0.022442519664764404
Batch 22/64 loss: -0.014599025249481201
Batch 23/64 loss: -0.020581603050231934
Batch 24/64 loss: -0.026184678077697754
Batch 25/64 loss: -0.05052924156188965
Batch 26/64 loss: -0.04527878761291504
Batch 27/64 loss: -0.016541779041290283
Batch 28/64 loss: -0.03585845232009888
Batch 29/64 loss: -0.027927100658416748
Batch 30/64 loss: -0.023039817810058594
Batch 31/64 loss: -0.017669856548309326
Batch 32/64 loss: -0.04916757345199585
Batch 33/64 loss: -0.019931375980377197
Batch 34/64 loss: -0.01247185468673706
Batch 35/64 loss: -0.0289422869682312
Batch 36/64 loss: -0.021304786205291748
Batch 37/64 loss: -0.023932993412017822
Batch 38/64 loss: -0.0030565857887268066
Batch 39/64 loss: -0.02123105525970459
Batch 40/64 loss: -0.02294546365737915
Batch 41/64 loss: -0.03996330499649048
Batch 42/64 loss: -0.019895732402801514
Batch 43/64 loss: -0.0076555609703063965
Batch 44/64 loss: -0.031087815761566162
Batch 45/64 loss: -0.02493494749069214
Batch 46/64 loss: -0.011585354804992676
Batch 47/64 loss: -0.03117614984512329
Batch 48/64 loss: -0.018954694271087646
Batch 49/64 loss: -0.0228651762008667
Batch 50/64 loss: -0.01224905252456665
Batch 51/64 loss: -0.01322484016418457
Batch 52/64 loss: -0.02091735601425171
Batch 53/64 loss: -0.009940743446350098
Batch 54/64 loss: -0.0473780632019043
Batch 55/64 loss: -0.010390520095825195
Batch 56/64 loss: -0.03630709648132324
Batch 57/64 loss: -0.04415631294250488
Batch 58/64 loss: -0.014810681343078613
Batch 59/64 loss: -0.03909730911254883
Batch 60/64 loss: -0.017018377780914307
Batch 61/64 loss: -0.016121268272399902
Batch 62/64 loss: -0.03251224756240845
Batch 63/64 loss: -0.029936611652374268
Batch 64/64 loss: -0.011023640632629395
Epoch 156  Train loss: -0.025516178561191934  Val loss: 0.04821017312839678
Saving best model, epoch: 156
Epoch 157
-------------------------------
Batch 1/64 loss: -0.02079874277114868
Batch 2/64 loss: -0.03629034757614136
Batch 3/64 loss: -0.024158239364624023
Batch 4/64 loss: -0.012893259525299072
Batch 5/64 loss: -0.01861107349395752
Batch 6/64 loss: -0.025857150554656982
Batch 7/64 loss: 0.006181478500366211
Batch 8/64 loss: -0.03240787982940674
Batch 9/64 loss: -0.022725164890289307
Batch 10/64 loss: -0.03245735168457031
Batch 11/64 loss: -0.04165083169937134
Batch 12/64 loss: -0.029126107692718506
Batch 13/64 loss: -0.026788413524627686
Batch 14/64 loss: -0.011098682880401611
Batch 15/64 loss: -0.006357550621032715
Batch 16/64 loss: -0.021828889846801758
Batch 17/64 loss: -0.04040640592575073
Batch 18/64 loss: -0.04785299301147461
Batch 19/64 loss: -0.03103315830230713
Batch 20/64 loss: -0.012749195098876953
Batch 21/64 loss: -0.021692097187042236
Batch 22/64 loss: -0.023452401161193848
Batch 23/64 loss: -0.023043394088745117
Batch 24/64 loss: -0.011110901832580566
Batch 25/64 loss: -0.014752328395843506
Batch 26/64 loss: -0.029346823692321777
Batch 27/64 loss: -0.017997682094573975
Batch 28/64 loss: -0.05294078588485718
Batch 29/64 loss: -0.05334913730621338
Batch 30/64 loss: -0.005375027656555176
Batch 31/64 loss: -0.020961523056030273
Batch 32/64 loss: -0.02650439739227295
Batch 33/64 loss: -0.016281723976135254
Batch 34/64 loss: -0.032670557498931885
Batch 35/64 loss: -0.043227553367614746
Batch 36/64 loss: -0.03868287801742554
Batch 37/64 loss: -0.05401211977005005
Batch 38/64 loss: -0.034266531467437744
Batch 39/64 loss: -0.02997744083404541
Batch 40/64 loss: -0.03788942098617554
Batch 41/64 loss: -0.02294766902923584
Batch 42/64 loss: -0.01227635145187378
Batch 43/64 loss: -0.0083770751953125
Batch 44/64 loss: -0.05010426044464111
Batch 45/64 loss: 0.00502169132232666
Batch 46/64 loss: -0.03176236152648926
Batch 47/64 loss: 0.006581723690032959
Batch 48/64 loss: -0.01857924461364746
Batch 49/64 loss: -0.040273070335388184
Batch 50/64 loss: -0.009516417980194092
Batch 51/64 loss: -0.016331732273101807
Batch 52/64 loss: -0.03026413917541504
Batch 53/64 loss: -0.016167283058166504
Batch 54/64 loss: 0.006495177745819092
Batch 55/64 loss: -0.02375882863998413
Batch 56/64 loss: -0.023289024829864502
Batch 57/64 loss: -0.03085547685623169
Batch 58/64 loss: -0.046334803104400635
Batch 59/64 loss: -0.025978505611419678
Batch 60/64 loss: -0.033409714698791504
Batch 61/64 loss: -0.04001510143280029
Batch 62/64 loss: -0.05206197500228882
Batch 63/64 loss: -0.023630797863006592
Batch 64/64 loss: -0.0235559344291687
Epoch 157  Train loss: -0.02559916669247197  Val loss: 0.04813276123754757
Saving best model, epoch: 157
Epoch 158
-------------------------------
Batch 1/64 loss: -0.03621751070022583
Batch 2/64 loss: -0.03790003061294556
Batch 3/64 loss: -0.016070544719696045
Batch 4/64 loss: -0.05236417055130005
Batch 5/64 loss: -0.008825123310089111
Batch 6/64 loss: -0.018297970294952393
Batch 7/64 loss: -0.034292519092559814
Batch 8/64 loss: -0.01255035400390625
Batch 9/64 loss: -0.004036903381347656
Batch 10/64 loss: -0.05056488513946533
Batch 11/64 loss: -0.01139146089553833
Batch 12/64 loss: -0.022953033447265625
Batch 13/64 loss: 0.0008665323257446289
Batch 14/64 loss: -0.0023154020309448242
Batch 15/64 loss: -0.016389966011047363
Batch 16/64 loss: -0.0048899054527282715
Batch 17/64 loss: -0.025493204593658447
Batch 18/64 loss: -0.045928001403808594
Batch 19/64 loss: -0.014381170272827148
Batch 20/64 loss: -0.016962170600891113
Batch 21/64 loss: -0.045381784439086914
Batch 22/64 loss: -0.03745931386947632
Batch 23/64 loss: -0.05014336109161377
Batch 24/64 loss: -0.037466466426849365
Batch 25/64 loss: -0.024721980094909668
Batch 26/64 loss: -0.02907794713973999
Batch 27/64 loss: -0.019670724868774414
Batch 28/64 loss: -0.041704773902893066
Batch 29/64 loss: -0.04819643497467041
Batch 30/64 loss: -0.01100766658782959
Batch 31/64 loss: -0.03804999589920044
Batch 32/64 loss: -0.04451262950897217
Batch 33/64 loss: -0.02603602409362793
Batch 34/64 loss: -0.008113622665405273
Batch 35/64 loss: -0.02413541078567505
Batch 36/64 loss: -0.014388501644134521
Batch 37/64 loss: -0.0033981800079345703
Batch 38/64 loss: -0.04352307319641113
Batch 39/64 loss: -0.009515166282653809
Batch 40/64 loss: 0.0029993057250976562
Batch 41/64 loss: -0.020126044750213623
Batch 42/64 loss: -0.014939546585083008
Batch 43/64 loss: -0.0293847918510437
Batch 44/64 loss: -0.027245938777923584
Batch 45/64 loss: -0.045753657817840576
Batch 46/64 loss: -0.006499230861663818
Batch 47/64 loss: -0.03507894277572632
Batch 48/64 loss: -0.023860633373260498
Batch 49/64 loss: -0.03026372194290161
Batch 50/64 loss: -0.04031836986541748
Batch 51/64 loss: -0.027362704277038574
Batch 52/64 loss: -0.023033976554870605
Batch 53/64 loss: -0.006556987762451172
Batch 54/64 loss: -0.009487390518188477
Batch 55/64 loss: -0.017495572566986084
Batch 56/64 loss: -0.023767530918121338
Batch 57/64 loss: -0.036120593547821045
Batch 58/64 loss: -0.029431641101837158
Batch 59/64 loss: -0.020670652389526367
Batch 60/64 loss: -0.018712997436523438
Batch 61/64 loss: -0.037473857402801514
Batch 62/64 loss: -0.02858877182006836
Batch 63/64 loss: -0.03802156448364258
Batch 64/64 loss: -0.026686668395996094
Epoch 158  Train loss: -0.025014457515641755  Val loss: 0.04915212294490067
Epoch 159
-------------------------------
Batch 1/64 loss: -0.028146088123321533
Batch 2/64 loss: -0.04733556509017944
Batch 3/64 loss: -0.03811466693878174
Batch 4/64 loss: -0.035105347633361816
Batch 5/64 loss: -0.02886909246444702
Batch 6/64 loss: -0.004726111888885498
Batch 7/64 loss: -0.037699222564697266
Batch 8/64 loss: -0.04134649038314819
Batch 9/64 loss: -0.025841712951660156
Batch 10/64 loss: -0.014170229434967041
Batch 11/64 loss: -0.035902976989746094
Batch 12/64 loss: -0.04713630676269531
Batch 13/64 loss: -0.02692890167236328
Batch 14/64 loss: -0.02180558443069458
Batch 15/64 loss: -0.01825261116027832
Batch 16/64 loss: -0.016466796398162842
Batch 17/64 loss: -0.01780635118484497
Batch 18/64 loss: -0.029572010040283203
Batch 19/64 loss: -0.055607497692108154
Batch 20/64 loss: -0.026880919933319092
Batch 21/64 loss: -0.03384488821029663
Batch 22/64 loss: 0.004874765872955322
Batch 23/64 loss: 0.006348550319671631
Batch 24/64 loss: -0.03588134050369263
Batch 25/64 loss: -0.014519929885864258
Batch 26/64 loss: -0.011767268180847168
Batch 27/64 loss: -0.03279322385787964
Batch 28/64 loss: -0.03712618350982666
Batch 29/64 loss: -0.023459196090698242
Batch 30/64 loss: -0.001634359359741211
Batch 31/64 loss: -0.022597670555114746
Batch 32/64 loss: -0.02047628164291382
Batch 33/64 loss: -0.028649628162384033
Batch 34/64 loss: -0.044237732887268066
Batch 35/64 loss: -0.013457536697387695
Batch 36/64 loss: -0.023038387298583984
Batch 37/64 loss: -0.03372621536254883
Batch 38/64 loss: -0.03274303674697876
Batch 39/64 loss: -0.019938766956329346
Batch 40/64 loss: -0.030554473400115967
Batch 41/64 loss: -0.026599526405334473
Batch 42/64 loss: -0.020211398601531982
Batch 43/64 loss: -0.00760650634765625
Batch 44/64 loss: -0.016872823238372803
Batch 45/64 loss: -0.04814136028289795
Batch 46/64 loss: -0.05615532398223877
Batch 47/64 loss: -0.0001728534698486328
Batch 48/64 loss: -0.04200965166091919
Batch 49/64 loss: -0.024570167064666748
Batch 50/64 loss: -0.0339963436126709
Batch 51/64 loss: -0.01700538396835327
Batch 52/64 loss: -0.03802520036697388
Batch 53/64 loss: -0.007678568363189697
Batch 54/64 loss: -0.021553218364715576
Batch 55/64 loss: -0.026714742183685303
Batch 56/64 loss: -0.03653222322463989
Batch 57/64 loss: -0.01038283109664917
Batch 58/64 loss: -0.03713786602020264
Batch 59/64 loss: -0.01590287685394287
Batch 60/64 loss: -0.030297279357910156
Batch 61/64 loss: -0.030898571014404297
Batch 62/64 loss: -0.007794439792633057
Batch 63/64 loss: -0.03578805923461914
Batch 64/64 loss: -0.024037659168243408
Epoch 159  Train loss: -0.02599235679589066  Val loss: 0.04665467091852037
Saving best model, epoch: 159
Epoch 160
-------------------------------
Batch 1/64 loss: -0.012712478637695312
Batch 2/64 loss: -0.034184813499450684
Batch 3/64 loss: -0.017880678176879883
Batch 4/64 loss: -0.030985116958618164
Batch 5/64 loss: -0.03200709819793701
Batch 6/64 loss: -0.02939927577972412
Batch 7/64 loss: -0.05215716361999512
Batch 8/64 loss: -0.03292191028594971
Batch 9/64 loss: -0.032888054847717285
Batch 10/64 loss: -0.037770211696624756
Batch 11/64 loss: -0.052877068519592285
Batch 12/64 loss: -0.04570138454437256
Batch 13/64 loss: -0.053835272789001465
Batch 14/64 loss: -0.047585487365722656
Batch 15/64 loss: -0.029437899589538574
Batch 16/64 loss: -0.028470635414123535
Batch 17/64 loss: -0.0399017333984375
Batch 18/64 loss: -0.027448534965515137
Batch 19/64 loss: -0.023736000061035156
Batch 20/64 loss: -0.021180272102355957
Batch 21/64 loss: -0.03498488664627075
Batch 22/64 loss: -0.024867773056030273
Batch 23/64 loss: -0.02819281816482544
Batch 24/64 loss: -0.006278097629547119
Batch 25/64 loss: -0.03432190418243408
Batch 26/64 loss: -0.028643310070037842
Batch 27/64 loss: -0.030122995376586914
Batch 28/64 loss: -0.006247043609619141
Batch 29/64 loss: -0.008414208889007568
Batch 30/64 loss: -0.035442054271698
Batch 31/64 loss: -0.027908504009246826
Batch 32/64 loss: -0.03469443321228027
Batch 33/64 loss: -0.01114886999130249
Batch 34/64 loss: -0.029207825660705566
Batch 35/64 loss: -0.019334852695465088
Batch 36/64 loss: -0.0411418080329895
Batch 37/64 loss: -0.022458374500274658
Batch 38/64 loss: -0.0017482638359069824
Batch 39/64 loss: -0.03200793266296387
Batch 40/64 loss: -0.03742408752441406
Batch 41/64 loss: -0.035487473011016846
Batch 42/64 loss: -0.029103398323059082
Batch 43/64 loss: -0.04268592596054077
Batch 44/64 loss: -0.023843884468078613
Batch 45/64 loss: 0.004017353057861328
Batch 46/64 loss: -0.007626831531524658
Batch 47/64 loss: -0.05646824836730957
Batch 48/64 loss: 0.01864326000213623
Batch 49/64 loss: -0.012827634811401367
Batch 50/64 loss: -0.044735074043273926
Batch 51/64 loss: -0.03497248888015747
Batch 52/64 loss: -0.01951509714126587
Batch 53/64 loss: -0.04411494731903076
Batch 54/64 loss: -0.048925936222076416
Batch 55/64 loss: -0.002011537551879883
Batch 56/64 loss: -0.028763115406036377
Batch 57/64 loss: -0.04839438199996948
Batch 58/64 loss: -0.032006800174713135
Batch 59/64 loss: -0.038706064224243164
Batch 60/64 loss: -0.029626905918121338
Batch 61/64 loss: -0.03040093183517456
Batch 62/64 loss: -0.026539623737335205
Batch 63/64 loss: -0.04466652870178223
Batch 64/64 loss: -0.01467353105545044
Epoch 160  Train loss: -0.028979427440493713  Val loss: 0.04462423582666928
Saving best model, epoch: 160
Epoch 161
-------------------------------
Batch 1/64 loss: -0.03320199251174927
Batch 2/64 loss: -0.048630475997924805
Batch 3/64 loss: -0.04725527763366699
Batch 4/64 loss: -0.02951180934906006
Batch 5/64 loss: -0.028351783752441406
Batch 6/64 loss: -0.03884178400039673
Batch 7/64 loss: -0.02262169122695923
Batch 8/64 loss: -0.03664052486419678
Batch 9/64 loss: -0.02997511625289917
Batch 10/64 loss: -0.03819894790649414
Batch 11/64 loss: -0.015622198581695557
Batch 12/64 loss: -0.027373433113098145
Batch 13/64 loss: -0.02768617868423462
Batch 14/64 loss: -0.02758634090423584
Batch 15/64 loss: -0.0038551688194274902
Batch 16/64 loss: -0.02616661787033081
Batch 17/64 loss: -0.024295508861541748
Batch 18/64 loss: -0.03640389442443848
Batch 19/64 loss: -0.04383605718612671
Batch 20/64 loss: -0.042427778244018555
Batch 21/64 loss: -0.002956211566925049
Batch 22/64 loss: -0.03627419471740723
Batch 23/64 loss: -0.0214083194732666
Batch 24/64 loss: -0.015317082405090332
Batch 25/64 loss: -0.025814533233642578
Batch 26/64 loss: -0.03024768829345703
Batch 27/64 loss: -0.02744591236114502
Batch 28/64 loss: -0.03981757164001465
Batch 29/64 loss: -0.05135369300842285
Batch 30/64 loss: -0.024768054485321045
Batch 31/64 loss: -0.014014482498168945
Batch 32/64 loss: -0.028621196746826172
Batch 33/64 loss: -0.036677420139312744
Batch 34/64 loss: -0.010261356830596924
Batch 35/64 loss: -0.03859025239944458
Batch 36/64 loss: -0.04794842004776001
Batch 37/64 loss: -0.03241539001464844
Batch 38/64 loss: -0.029665827751159668
Batch 39/64 loss: -0.034563302993774414
Batch 40/64 loss: -0.029444336891174316
Batch 41/64 loss: -0.04537820816040039
Batch 42/64 loss: -0.037225306034088135
Batch 43/64 loss: -0.02264845371246338
Batch 44/64 loss: -0.02882373332977295
Batch 45/64 loss: -0.03205263614654541
Batch 46/64 loss: -0.03437250852584839
Batch 47/64 loss: -0.04091989994049072
Batch 48/64 loss: -0.021199047565460205
Batch 49/64 loss: -0.04799157381057739
Batch 50/64 loss: -0.02656400203704834
Batch 51/64 loss: -0.037097156047821045
Batch 52/64 loss: -0.043193280696868896
Batch 53/64 loss: -0.01778191328048706
Batch 54/64 loss: -0.03279376029968262
Batch 55/64 loss: -0.015888512134552002
Batch 56/64 loss: -0.017427563667297363
Batch 57/64 loss: -0.013503491878509521
Batch 58/64 loss: -0.033977508544921875
Batch 59/64 loss: -0.004085540771484375
Batch 60/64 loss: -0.03602832555770874
Batch 61/64 loss: -0.038626790046691895
Batch 62/64 loss: -0.032191455364227295
Batch 63/64 loss: -0.024169325828552246
Batch 64/64 loss: -0.02436518669128418
Epoch 161  Train loss: -0.029934144487567975  Val loss: 0.04759539361671893
Epoch 162
-------------------------------
Batch 1/64 loss: -0.05024498701095581
Batch 2/64 loss: -0.03626447916030884
Batch 3/64 loss: -0.03989166021347046
Batch 4/64 loss: -0.03469955921173096
Batch 5/64 loss: -0.05995452404022217
Batch 6/64 loss: -0.04538083076477051
Batch 7/64 loss: -0.02288496494293213
Batch 8/64 loss: -0.0219118595123291
Batch 9/64 loss: -0.011217951774597168
Batch 10/64 loss: -0.027649521827697754
Batch 11/64 loss: -0.04461348056793213
Batch 12/64 loss: -0.029413163661956787
Batch 13/64 loss: -0.03300619125366211
Batch 14/64 loss: -0.05836421251296997
Batch 15/64 loss: -0.03241443634033203
Batch 16/64 loss: -0.04634004831314087
Batch 17/64 loss: -0.009549736976623535
Batch 18/64 loss: -0.04266911745071411
Batch 19/64 loss: -0.027875900268554688
Batch 20/64 loss: -0.05797755718231201
Batch 21/64 loss: -0.024488985538482666
Batch 22/64 loss: -0.03138601779937744
Batch 23/64 loss: -0.010172069072723389
Batch 24/64 loss: 0.011873841285705566
Batch 25/64 loss: -0.03386712074279785
Batch 26/64 loss: -0.027737736701965332
Batch 27/64 loss: -0.04229736328125
Batch 28/64 loss: -0.0113602876663208
Batch 29/64 loss: -0.015376627445220947
Batch 30/64 loss: -0.022843480110168457
Batch 31/64 loss: -0.023707032203674316
Batch 32/64 loss: -0.022627592086791992
Batch 33/64 loss: -0.04473334550857544
Batch 34/64 loss: -0.0515863299369812
Batch 35/64 loss: -0.01868605613708496
Batch 36/64 loss: -0.039392828941345215
Batch 37/64 loss: -0.022857725620269775
Batch 38/64 loss: -0.03308260440826416
Batch 39/64 loss: -0.027875959873199463
Batch 40/64 loss: -0.01814514398574829
Batch 41/64 loss: -0.042201220989227295
Batch 42/64 loss: -0.022120654582977295
Batch 43/64 loss: -0.02446514368057251
Batch 44/64 loss: -0.01874411106109619
Batch 45/64 loss: -0.050657451152801514
Batch 46/64 loss: -0.019184529781341553
Batch 47/64 loss: -0.04365944862365723
Batch 48/64 loss: -0.01996481418609619
Batch 49/64 loss: -0.03502810001373291
Batch 50/64 loss: -0.03442943096160889
Batch 51/64 loss: -0.043932199478149414
Batch 52/64 loss: -0.02792513370513916
Batch 53/64 loss: -0.03902876377105713
Batch 54/64 loss: -0.030517518520355225
Batch 55/64 loss: -0.029695570468902588
Batch 56/64 loss: -0.026755154132843018
Batch 57/64 loss: -0.03898143768310547
Batch 58/64 loss: -0.0009937882423400879
Batch 59/64 loss: -0.047429442405700684
Batch 60/64 loss: -0.02490234375
Batch 61/64 loss: -0.009949445724487305
Batch 62/64 loss: -0.03909271955490112
Batch 63/64 loss: -0.01256418228149414
Batch 64/64 loss: -0.01782548427581787
Epoch 162  Train loss: -0.030404178301493327  Val loss: 0.04993635170238534
Epoch 163
-------------------------------
Batch 1/64 loss: -0.055510103702545166
Batch 2/64 loss: -0.016465723514556885
Batch 3/64 loss: -0.03707069158554077
Batch 4/64 loss: 0.0007382631301879883
Batch 5/64 loss: -0.03216522932052612
Batch 6/64 loss: -0.033337414264678955
Batch 7/64 loss: -0.023032784461975098
Batch 8/64 loss: -0.010190606117248535
Batch 9/64 loss: -0.02283787727355957
Batch 10/64 loss: -0.039655447006225586
Batch 11/64 loss: -0.027456581592559814
Batch 12/64 loss: -0.03146713972091675
Batch 13/64 loss: -0.03046661615371704
Batch 14/64 loss: -0.03471869230270386
Batch 15/64 loss: -0.012630999088287354
Batch 16/64 loss: -0.022240161895751953
Batch 17/64 loss: -0.025281965732574463
Batch 18/64 loss: -0.014910697937011719
Batch 19/64 loss: -0.03531241416931152
Batch 20/64 loss: -0.04962563514709473
Batch 21/64 loss: -0.041266560554504395
Batch 22/64 loss: -0.021134614944458008
Batch 23/64 loss: -0.03880959749221802
Batch 24/64 loss: -0.03172767162322998
Batch 25/64 loss: -0.03782248497009277
Batch 26/64 loss: -0.04465615749359131
Batch 27/64 loss: -0.03737509250640869
Batch 28/64 loss: -0.014074146747589111
Batch 29/64 loss: -0.02852606773376465
Batch 30/64 loss: -0.0414431095123291
Batch 31/64 loss: -0.036762893199920654
Batch 32/64 loss: -0.0320436954498291
Batch 33/64 loss: -0.045605480670928955
Batch 34/64 loss: -0.040406644344329834
Batch 35/64 loss: -0.011491477489471436
Batch 36/64 loss: -0.03179788589477539
Batch 37/64 loss: 0.01503133773803711
Batch 38/64 loss: -0.054207026958465576
Batch 39/64 loss: -0.02072221040725708
Batch 40/64 loss: -0.02127450704574585
Batch 41/64 loss: -0.028461158275604248
Batch 42/64 loss: -0.03577607870101929
Batch 43/64 loss: -0.017974913120269775
Batch 44/64 loss: -0.04138517379760742
Batch 45/64 loss: -0.027454018592834473
Batch 46/64 loss: -0.05333971977233887
Batch 47/64 loss: -0.032845139503479004
Batch 48/64 loss: -0.027812600135803223
Batch 49/64 loss: -0.01947927474975586
Batch 50/64 loss: -0.03957009315490723
Batch 51/64 loss: -0.018212080001831055
Batch 52/64 loss: -0.04430443048477173
Batch 53/64 loss: -0.05731093883514404
Batch 54/64 loss: -0.00891423225402832
Batch 55/64 loss: -0.03900212049484253
Batch 56/64 loss: -0.055426716804504395
Batch 57/64 loss: -0.023278892040252686
Batch 58/64 loss: -0.02847045660018921
Batch 59/64 loss: -0.036320507526397705
Batch 60/64 loss: -0.02993851900100708
Batch 61/64 loss: -0.035887181758880615
Batch 62/64 loss: -0.041168928146362305
Batch 63/64 loss: 0.0025969743728637695
Batch 64/64 loss: -0.04767584800720215
Epoch 163  Train loss: -0.030513687694773955  Val loss: 0.04563022602055081
Epoch 164
-------------------------------
Batch 1/64 loss: -0.020463883876800537
Batch 2/64 loss: -0.06358706951141357
Batch 3/64 loss: -0.013975799083709717
Batch 4/64 loss: -0.0469738245010376
Batch 5/64 loss: -0.032984375953674316
Batch 6/64 loss: -0.03215128183364868
Batch 7/64 loss: -0.02069878578186035
Batch 8/64 loss: -0.032302796840667725
Batch 9/64 loss: -0.05858492851257324
Batch 10/64 loss: -0.05147099494934082
Batch 11/64 loss: 0.0061959028244018555
Batch 12/64 loss: -0.05577218532562256
Batch 13/64 loss: -0.029606938362121582
Batch 14/64 loss: -0.05615729093551636
Batch 15/64 loss: -0.05331063270568848
Batch 16/64 loss: -0.05624949932098389
Batch 17/64 loss: -0.04581332206726074
Batch 18/64 loss: -0.020907282829284668
Batch 19/64 loss: -0.0060541629791259766
Batch 20/64 loss: -0.011832833290100098
Batch 21/64 loss: -0.02535533905029297
Batch 22/64 loss: -0.04217654466629028
Batch 23/64 loss: -0.03636282682418823
Batch 24/64 loss: -0.016163408756256104
Batch 25/64 loss: -0.028959989547729492
Batch 26/64 loss: -0.0535469651222229
Batch 27/64 loss: -0.029730796813964844
Batch 28/64 loss: -0.026033997535705566
Batch 29/64 loss: -0.05500298738479614
Batch 30/64 loss: -0.0425906777381897
Batch 31/64 loss: -0.03214019536972046
Batch 32/64 loss: -0.04327589273452759
Batch 33/64 loss: -0.04605048894882202
Batch 34/64 loss: -0.01707470417022705
Batch 35/64 loss: -0.0441514253616333
Batch 36/64 loss: -0.028763949871063232
Batch 37/64 loss: -0.03198957443237305
Batch 38/64 loss: -0.018529176712036133
Batch 39/64 loss: -0.016342103481292725
Batch 40/64 loss: -0.017957031726837158
Batch 41/64 loss: -0.0062752366065979
Batch 42/64 loss: -0.022215664386749268
Batch 43/64 loss: -0.03646194934844971
Batch 44/64 loss: -0.0402836799621582
Batch 45/64 loss: 0.003788292407989502
Batch 46/64 loss: -0.03425103425979614
Batch 47/64 loss: -0.02252042293548584
Batch 48/64 loss: -0.028600215911865234
Batch 49/64 loss: -0.015182971954345703
Batch 50/64 loss: -0.026926755905151367
Batch 51/64 loss: -0.006939291954040527
Batch 52/64 loss: -0.017516136169433594
Batch 53/64 loss: -0.023929595947265625
Batch 54/64 loss: -0.05072420835494995
Batch 55/64 loss: -0.009960651397705078
Batch 56/64 loss: -0.04102402925491333
Batch 57/64 loss: -0.020434856414794922
Batch 58/64 loss: -0.02491748332977295
Batch 59/64 loss: -0.04064643383026123
Batch 60/64 loss: -0.04884451627731323
Batch 61/64 loss: -0.036580562591552734
Batch 62/64 loss: -0.023221373558044434
Batch 63/64 loss: -0.012829840183258057
Batch 64/64 loss: -0.018886864185333252
Epoch 164  Train loss: -0.03067551103292727  Val loss: 0.049260862709320695
Epoch 165
-------------------------------
Batch 1/64 loss: -0.049663662910461426
Batch 2/64 loss: -0.029713332653045654
Batch 3/64 loss: -0.04482382535934448
Batch 4/64 loss: -0.017407715320587158
Batch 5/64 loss: -0.0404585599899292
Batch 6/64 loss: -0.04694455862045288
Batch 7/64 loss: -0.04204869270324707
Batch 8/64 loss: -0.03282725811004639
Batch 9/64 loss: 0.006375908851623535
Batch 10/64 loss: -0.01282811164855957
Batch 11/64 loss: -0.0455859899520874
Batch 12/64 loss: -0.023108601570129395
Batch 13/64 loss: -0.038749098777770996
Batch 14/64 loss: -0.039054036140441895
Batch 15/64 loss: -0.011616289615631104
Batch 16/64 loss: -0.023446083068847656
Batch 17/64 loss: -0.03771233558654785
Batch 18/64 loss: -0.03278076648712158
Batch 19/64 loss: -0.045479774475097656
Batch 20/64 loss: -0.013685882091522217
Batch 21/64 loss: -0.04443317651748657
Batch 22/64 loss: -0.05847620964050293
Batch 23/64 loss: -0.023495018482208252
Batch 24/64 loss: -0.024832844734191895
Batch 25/64 loss: -0.04172921180725098
Batch 26/64 loss: -0.02831822633743286
Batch 27/64 loss: 0.0061531662940979
Batch 28/64 loss: -0.04864048957824707
Batch 29/64 loss: -0.04141652584075928
Batch 30/64 loss: -0.03631776571273804
Batch 31/64 loss: -0.010160565376281738
Batch 32/64 loss: -0.047111332416534424
Batch 33/64 loss: -0.02701359987258911
Batch 34/64 loss: -0.029790520668029785
Batch 35/64 loss: -0.04541313648223877
Batch 36/64 loss: -0.058767080307006836
Batch 37/64 loss: -0.04906350374221802
Batch 38/64 loss: -0.028364360332489014
Batch 39/64 loss: -0.006100594997406006
Batch 40/64 loss: -0.020292997360229492
Batch 41/64 loss: -0.03621697425842285
Batch 42/64 loss: -0.02143758535385132
Batch 43/64 loss: -0.02915191650390625
Batch 44/64 loss: -0.027065634727478027
Batch 45/64 loss: -0.038771212100982666
Batch 46/64 loss: 0.0006292462348937988
Batch 47/64 loss: -0.04495549201965332
Batch 48/64 loss: -0.04467761516571045
Batch 49/64 loss: -0.035847008228302
Batch 50/64 loss: -0.03240489959716797
Batch 51/64 loss: -0.00652623176574707
Batch 52/64 loss: -0.031668126583099365
Batch 53/64 loss: -0.024749040603637695
Batch 54/64 loss: -0.034750282764434814
Batch 55/64 loss: -0.004822671413421631
Batch 56/64 loss: -0.021044790744781494
Batch 57/64 loss: -0.011176466941833496
Batch 58/64 loss: -0.019991040229797363
Batch 59/64 loss: -0.0014605522155761719
Batch 60/64 loss: -0.03975564241409302
Batch 61/64 loss: -0.01968628168106079
Batch 62/64 loss: -0.03630346059799194
Batch 63/64 loss: -0.04333162307739258
Batch 64/64 loss: -0.03226596117019653
Epoch 165  Train loss: -0.03003148915720921  Val loss: 0.04766353852150776
Epoch 166
-------------------------------
Batch 1/64 loss: -0.03236585855484009
Batch 2/64 loss: -0.029801547527313232
Batch 3/64 loss: -0.04986453056335449
Batch 4/64 loss: -0.02962172031402588
Batch 5/64 loss: -0.033282577991485596
Batch 6/64 loss: -0.047838807106018066
Batch 7/64 loss: -0.03120279312133789
Batch 8/64 loss: -0.032112181186676025
Batch 9/64 loss: -0.01457667350769043
Batch 10/64 loss: -0.021568775177001953
Batch 11/64 loss: -0.0354921817779541
Batch 12/64 loss: -0.049533069133758545
Batch 13/64 loss: -0.0384446382522583
Batch 14/64 loss: -0.021502196788787842
Batch 15/64 loss: -0.04513317346572876
Batch 16/64 loss: -0.049968838691711426
Batch 17/64 loss: -0.028563261032104492
Batch 18/64 loss: -0.03122609853744507
Batch 19/64 loss: -0.03162336349487305
Batch 20/64 loss: -0.03285485506057739
Batch 21/64 loss: -0.0236702561378479
Batch 22/64 loss: -0.020365357398986816
Batch 23/64 loss: -0.02965790033340454
Batch 24/64 loss: -0.021459102630615234
Batch 25/64 loss: -0.03871041536331177
Batch 26/64 loss: -0.022507309913635254
Batch 27/64 loss: -0.05288565158843994
Batch 28/64 loss: -0.015806913375854492
Batch 29/64 loss: -0.0416032075881958
Batch 30/64 loss: -0.027981281280517578
Batch 31/64 loss: -0.01896500587463379
Batch 32/64 loss: -0.029367566108703613
Batch 33/64 loss: -0.03566563129425049
Batch 34/64 loss: -0.026214241981506348
Batch 35/64 loss: -0.042170822620391846
Batch 36/64 loss: -0.025013089179992676
Batch 37/64 loss: -0.03055095672607422
Batch 38/64 loss: -0.04229629039764404
Batch 39/64 loss: -0.041779398918151855
Batch 40/64 loss: -0.05171489715576172
Batch 41/64 loss: -0.038771867752075195
Batch 42/64 loss: -0.008529365062713623
Batch 43/64 loss: -0.026333332061767578
Batch 44/64 loss: -0.03568696975708008
Batch 45/64 loss: -0.04047757387161255
Batch 46/64 loss: -0.04167795181274414
Batch 47/64 loss: -0.0423811674118042
Batch 48/64 loss: -0.02994215488433838
Batch 49/64 loss: -0.008828043937683105
Batch 50/64 loss: -0.03947383165359497
Batch 51/64 loss: -0.013967156410217285
Batch 52/64 loss: -0.03325784206390381
Batch 53/64 loss: -0.034809112548828125
Batch 54/64 loss: -0.008730053901672363
Batch 55/64 loss: -0.04086792469024658
Batch 56/64 loss: -0.015344202518463135
Batch 57/64 loss: -0.02895057201385498
Batch 58/64 loss: -0.025468885898590088
Batch 59/64 loss: -0.04755592346191406
Batch 60/64 loss: -0.041341185569763184
Batch 61/64 loss: -0.028668642044067383
Batch 62/64 loss: -0.029524505138397217
Batch 63/64 loss: -0.007778048515319824
Batch 64/64 loss: -0.0018311142921447754
Epoch 166  Train loss: -0.03128944448396272  Val loss: 0.04331463288605418
Saving best model, epoch: 166
Epoch 167
-------------------------------
Batch 1/64 loss: -0.05292057991027832
Batch 2/64 loss: -0.03443491458892822
Batch 3/64 loss: -0.01919609308242798
Batch 4/64 loss: -0.02593761682510376
Batch 5/64 loss: -0.04504746198654175
Batch 6/64 loss: -0.03421574831008911
Batch 7/64 loss: -0.0475466251373291
Batch 8/64 loss: -0.04785102605819702
Batch 9/64 loss: -0.03076016902923584
Batch 10/64 loss: -0.014068543910980225
Batch 11/64 loss: -0.00955730676651001
Batch 12/64 loss: -0.04470580816268921
Batch 13/64 loss: -0.04298478364944458
Batch 14/64 loss: -0.014261424541473389
Batch 15/64 loss: -0.029239952564239502
Batch 16/64 loss: -0.06145590543746948
Batch 17/64 loss: -0.04444706439971924
Batch 18/64 loss: -0.04065251350402832
Batch 19/64 loss: -0.009309947490692139
Batch 20/64 loss: -0.03298395872116089
Batch 21/64 loss: -0.035990893840789795
Batch 22/64 loss: -0.0323030948638916
Batch 23/64 loss: -0.026596486568450928
Batch 24/64 loss: -0.041500866413116455
Batch 25/64 loss: -0.013476252555847168
Batch 26/64 loss: -0.026385068893432617
Batch 27/64 loss: -0.03243684768676758
Batch 28/64 loss: -0.0432247519493103
Batch 29/64 loss: -0.03614640235900879
Batch 30/64 loss: -0.04062080383300781
Batch 31/64 loss: -0.03532689809799194
Batch 32/64 loss: -0.057950735092163086
Batch 33/64 loss: -0.05560934543609619
Batch 34/64 loss: -0.040555715560913086
Batch 35/64 loss: -0.018553495407104492
Batch 36/64 loss: -0.03154611587524414
Batch 37/64 loss: -0.012083292007446289
Batch 38/64 loss: -0.043294429779052734
Batch 39/64 loss: -0.031081676483154297
Batch 40/64 loss: -0.01736360788345337
Batch 41/64 loss: -0.017687559127807617
Batch 42/64 loss: -0.0018146038055419922
Batch 43/64 loss: -0.030041754245758057
Batch 44/64 loss: -0.016783297061920166
Batch 45/64 loss: -0.04280543327331543
Batch 46/64 loss: -0.0652698278427124
Batch 47/64 loss: -0.01953965425491333
Batch 48/64 loss: -0.024588346481323242
Batch 49/64 loss: -0.017388641834259033
Batch 50/64 loss: -0.018839359283447266
Batch 51/64 loss: -0.005585193634033203
Batch 52/64 loss: -0.03688526153564453
Batch 53/64 loss: -0.02609539031982422
Batch 54/64 loss: -0.0330660343170166
Batch 55/64 loss: -0.0037641525268554688
Batch 56/64 loss: -0.036485910415649414
Batch 57/64 loss: -0.020264863967895508
Batch 58/64 loss: -0.034680187702178955
Batch 59/64 loss: -0.04125547409057617
Batch 60/64 loss: -0.027061104774475098
Batch 61/64 loss: -0.011925935745239258
Batch 62/64 loss: -0.038101017475128174
Batch 63/64 loss: -0.03383684158325195
Batch 64/64 loss: -0.0396081805229187
Epoch 167  Train loss: -0.03117013655456842  Val loss: 0.047577445859351926
Epoch 168
-------------------------------
Batch 1/64 loss: -0.03920912742614746
Batch 2/64 loss: -0.06270194053649902
Batch 3/64 loss: -0.025568366050720215
Batch 4/64 loss: -0.010601043701171875
Batch 5/64 loss: -0.028936386108398438
Batch 6/64 loss: -0.03927558660507202
Batch 7/64 loss: -0.039391517639160156
Batch 8/64 loss: -0.04911071062088013
Batch 9/64 loss: -0.052997469902038574
Batch 10/64 loss: -0.05034744739532471
Batch 11/64 loss: -0.03297138214111328
Batch 12/64 loss: -0.04651671648025513
Batch 13/64 loss: -0.039861440658569336
Batch 14/64 loss: -0.006547152996063232
Batch 15/64 loss: -0.022057175636291504
Batch 16/64 loss: -0.03343474864959717
Batch 17/64 loss: -0.020521700382232666
Batch 18/64 loss: -0.04064738750457764
Batch 19/64 loss: -0.04728138446807861
Batch 20/64 loss: -0.02005094289779663
Batch 21/64 loss: -0.04313606023788452
Batch 22/64 loss: -0.03541088104248047
Batch 23/64 loss: -0.028379082679748535
Batch 24/64 loss: -0.03816103935241699
Batch 25/64 loss: -0.03855246305465698
Batch 26/64 loss: -0.037913620471954346
Batch 27/64 loss: -0.018492043018341064
Batch 28/64 loss: -0.02192699909210205
Batch 29/64 loss: -0.023047029972076416
Batch 30/64 loss: -0.010500192642211914
Batch 31/64 loss: -0.02840965986251831
Batch 32/64 loss: -0.0035498738288879395
Batch 33/64 loss: -0.021367669105529785
Batch 34/64 loss: -0.042073071002960205
Batch 35/64 loss: -0.027303338050842285
Batch 36/64 loss: -0.007177531719207764
Batch 37/64 loss: -0.03055793046951294
Batch 38/64 loss: -0.03294789791107178
Batch 39/64 loss: -0.02546614408493042
Batch 40/64 loss: -0.02343076467514038
Batch 41/64 loss: -0.02707970142364502
Batch 42/64 loss: -0.02415567636489868
Batch 43/64 loss: 0.0006502270698547363
Batch 44/64 loss: -0.037951529026031494
Batch 45/64 loss: -0.03270065784454346
Batch 46/64 loss: -0.01600325107574463
Batch 47/64 loss: -0.026904940605163574
Batch 48/64 loss: -0.03210538625717163
Batch 49/64 loss: -0.006871640682220459
Batch 50/64 loss: -0.03521043062210083
Batch 51/64 loss: -0.03863602876663208
Batch 52/64 loss: -0.03684818744659424
Batch 53/64 loss: -0.023834645748138428
Batch 54/64 loss: -0.029004573822021484
Batch 55/64 loss: -0.04722648859024048
Batch 56/64 loss: -0.017141997814178467
Batch 57/64 loss: -0.012062251567840576
Batch 58/64 loss: -0.038185954093933105
Batch 59/64 loss: -0.04098087549209595
Batch 60/64 loss: -0.0332639217376709
Batch 61/64 loss: -0.012909293174743652
Batch 62/64 loss: -0.03472018241882324
Batch 63/64 loss: -0.03142523765563965
Batch 64/64 loss: -0.027173161506652832
Epoch 168  Train loss: -0.029816241825328153  Val loss: 0.05275967440654322
Epoch 169
-------------------------------
Batch 1/64 loss: -0.020234882831573486
Batch 2/64 loss: -0.02603667974472046
Batch 3/64 loss: -0.04150986671447754
Batch 4/64 loss: -0.036910414695739746
Batch 5/64 loss: -0.0642290711402893
Batch 6/64 loss: -0.01783311367034912
Batch 7/64 loss: -0.009937882423400879
Batch 8/64 loss: -0.01918494701385498
Batch 9/64 loss: -0.04699498414993286
Batch 10/64 loss: -0.03840821981430054
Batch 11/64 loss: -0.029261231422424316
Batch 12/64 loss: -0.042669713497161865
Batch 13/64 loss: -0.05132138729095459
Batch 14/64 loss: -0.024099409580230713
Batch 15/64 loss: -0.029912114143371582
Batch 16/64 loss: -0.03530764579772949
Batch 17/64 loss: -0.057122647762298584
Batch 18/64 loss: -0.028725743293762207
Batch 19/64 loss: -0.03283393383026123
Batch 20/64 loss: -0.04795515537261963
Batch 21/64 loss: -0.006337165832519531
Batch 22/64 loss: -0.04406023025512695
Batch 23/64 loss: -0.02931034564971924
Batch 24/64 loss: -0.02514505386352539
Batch 25/64 loss: -0.030319809913635254
Batch 26/64 loss: -0.03910261392593384
Batch 27/64 loss: -0.04328358173370361
Batch 28/64 loss: -0.027286946773529053
Batch 29/64 loss: -0.031124234199523926
Batch 30/64 loss: -0.0593562126159668
Batch 31/64 loss: -0.0533633828163147
Batch 32/64 loss: -0.053540945053100586
Batch 33/64 loss: -0.03977102041244507
Batch 34/64 loss: -0.01809823513031006
Batch 35/64 loss: -0.04196435213088989
Batch 36/64 loss: -0.012197792530059814
Batch 37/64 loss: -0.033113718032836914
Batch 38/64 loss: -0.047878801822662354
Batch 39/64 loss: -0.02861839532852173
Batch 40/64 loss: -0.028283238410949707
Batch 41/64 loss: -0.01664799451828003
Batch 42/64 loss: -0.050439655780792236
Batch 43/64 loss: -0.03443711996078491
Batch 44/64 loss: -0.023291707038879395
Batch 45/64 loss: -0.039965033531188965
Batch 46/64 loss: -0.0327797532081604
Batch 47/64 loss: -0.048029541969299316
Batch 48/64 loss: -0.021039605140686035
Batch 49/64 loss: -0.03041297197341919
Batch 50/64 loss: -0.03146857023239136
Batch 51/64 loss: -0.014424443244934082
Batch 52/64 loss: -0.00011247396469116211
Batch 53/64 loss: -0.03442394733428955
Batch 54/64 loss: -0.025599658489227295
Batch 55/64 loss: -0.05947732925415039
Batch 56/64 loss: -0.053530752658843994
Batch 57/64 loss: -0.027678191661834717
Batch 58/64 loss: -0.030499160289764404
Batch 59/64 loss: -0.025833308696746826
Batch 60/64 loss: -0.04240274429321289
Batch 61/64 loss: -0.03405207395553589
Batch 62/64 loss: -0.015289545059204102
Batch 63/64 loss: -0.018625855445861816
Batch 64/64 loss: -0.03530478477478027
Epoch 169  Train loss: -0.03340525720633712  Val loss: 0.0460045722751683
Epoch 170
-------------------------------
Batch 1/64 loss: -0.05589240789413452
Batch 2/64 loss: -0.03506845235824585
Batch 3/64 loss: -0.04082232713699341
Batch 4/64 loss: -0.05970031023025513
Batch 5/64 loss: -0.039224863052368164
Batch 6/64 loss: -0.04946941137313843
Batch 7/64 loss: -0.04491877555847168
Batch 8/64 loss: -0.0469132661819458
Batch 9/64 loss: -0.036436617374420166
Batch 10/64 loss: -0.06370300054550171
Batch 11/64 loss: -0.024646282196044922
Batch 12/64 loss: -0.0631711483001709
Batch 13/64 loss: -0.04429548978805542
Batch 14/64 loss: -0.04639887809753418
Batch 15/64 loss: -0.031645774841308594
Batch 16/64 loss: -0.038933396339416504
Batch 17/64 loss: -0.036284804344177246
Batch 18/64 loss: -0.04842174053192139
Batch 19/64 loss: 0.02030879259109497
Batch 20/64 loss: -0.03753244876861572
Batch 21/64 loss: -0.04290211200714111
Batch 22/64 loss: -0.03871864080429077
Batch 23/64 loss: -0.04443645477294922
Batch 24/64 loss: -0.03351271152496338
Batch 25/64 loss: -0.052847087383270264
Batch 26/64 loss: -0.047303199768066406
Batch 27/64 loss: -0.05140399932861328
Batch 28/64 loss: -0.017201006412506104
Batch 29/64 loss: -0.037499964237213135
Batch 30/64 loss: -0.04861617088317871
Batch 31/64 loss: -0.04195362329483032
Batch 32/64 loss: -0.02410578727722168
Batch 33/64 loss: -0.02623540163040161
Batch 34/64 loss: -0.04456806182861328
Batch 35/64 loss: -0.046634137630462646
Batch 36/64 loss: -0.021553993225097656
Batch 37/64 loss: -0.050836265087127686
Batch 38/64 loss: -0.049445271492004395
Batch 39/64 loss: -0.029156029224395752
Batch 40/64 loss: -0.03239375352859497
Batch 41/64 loss: -0.041961610317230225
Batch 42/64 loss: -0.01872915029525757
Batch 43/64 loss: -0.04869234561920166
Batch 44/64 loss: -0.03547859191894531
Batch 45/64 loss: -0.031979918479919434
Batch 46/64 loss: -0.017063379287719727
Batch 47/64 loss: 0.01502305269241333
Batch 48/64 loss: -0.019863009452819824
Batch 49/64 loss: -0.027362525463104248
Batch 50/64 loss: -0.02138519287109375
Batch 51/64 loss: -0.029796183109283447
Batch 52/64 loss: -0.04232978820800781
Batch 53/64 loss: -0.03438061475753784
Batch 54/64 loss: -0.04790925979614258
Batch 55/64 loss: -0.022036612033843994
Batch 56/64 loss: -0.021332144737243652
Batch 57/64 loss: -0.04128611087799072
Batch 58/64 loss: -0.0417943000793457
Batch 59/64 loss: -0.032833993434906006
Batch 60/64 loss: -0.057772278785705566
Batch 61/64 loss: -0.04402279853820801
Batch 62/64 loss: -0.03888058662414551
Batch 63/64 loss: -0.019124627113342285
Batch 64/64 loss: -0.026835322380065918
Epoch 170  Train loss: -0.03679392524794036  Val loss: 0.04557720738178266
Epoch 171
-------------------------------
Batch 1/64 loss: -0.05443793535232544
Batch 2/64 loss: -0.05716139078140259
Batch 3/64 loss: -0.04423266649246216
Batch 4/64 loss: -0.04197460412979126
Batch 5/64 loss: -0.03490126132965088
Batch 6/64 loss: -0.03344160318374634
Batch 7/64 loss: -0.02678501605987549
Batch 8/64 loss: -0.05148196220397949
Batch 9/64 loss: -0.03161817789077759
Batch 10/64 loss: -0.018257081508636475
Batch 11/64 loss: -0.030593812465667725
Batch 12/64 loss: -0.03316092491149902
Batch 13/64 loss: -0.03481316566467285
Batch 14/64 loss: -0.031654536724090576
Batch 15/64 loss: -0.035403430461883545
Batch 16/64 loss: -0.020617008209228516
Batch 17/64 loss: -0.054386913776397705
Batch 18/64 loss: -0.03829169273376465
Batch 19/64 loss: -0.03420436382293701
Batch 20/64 loss: -0.06227070093154907
Batch 21/64 loss: -0.05283939838409424
Batch 22/64 loss: -0.046472907066345215
Batch 23/64 loss: -0.040832698345184326
Batch 24/64 loss: -0.037231385707855225
Batch 25/64 loss: -0.041937410831451416
Batch 26/64 loss: -0.025446414947509766
Batch 27/64 loss: -0.027913689613342285
Batch 28/64 loss: -0.03183108568191528
Batch 29/64 loss: -0.04613262414932251
Batch 30/64 loss: -0.022710025310516357
Batch 31/64 loss: -0.05287790298461914
Batch 32/64 loss: -0.04439413547515869
Batch 33/64 loss: -0.04996466636657715
Batch 34/64 loss: -0.0377158522605896
Batch 35/64 loss: -0.048371076583862305
Batch 36/64 loss: -0.051340699195861816
Batch 37/64 loss: -0.017889201641082764
Batch 38/64 loss: -0.024712324142456055
Batch 39/64 loss: -0.012971758842468262
Batch 40/64 loss: -0.031108081340789795
Batch 41/64 loss: -0.004937291145324707
Batch 42/64 loss: -0.056803226470947266
Batch 43/64 loss: -0.012449443340301514
Batch 44/64 loss: -0.02916109561920166
Batch 45/64 loss: -0.05264025926589966
Batch 46/64 loss: -0.00533139705657959
Batch 47/64 loss: -0.03420102596282959
Batch 48/64 loss: -0.031988680362701416
Batch 49/64 loss: -0.05248302221298218
Batch 50/64 loss: -0.02855241298675537
Batch 51/64 loss: -0.03422093391418457
Batch 52/64 loss: -0.04988771677017212
Batch 53/64 loss: -0.02473527193069458
Batch 54/64 loss: -0.042417824268341064
Batch 55/64 loss: -0.04060018062591553
Batch 56/64 loss: -0.042184531688690186
Batch 57/64 loss: -0.008476495742797852
Batch 58/64 loss: -0.04670673608779907
Batch 59/64 loss: -0.05362439155578613
Batch 60/64 loss: -0.008396387100219727
Batch 61/64 loss: -0.033319950103759766
Batch 62/64 loss: -0.03396117687225342
Batch 63/64 loss: -0.03943896293640137
Batch 64/64 loss: -0.024907946586608887
Epoch 171  Train loss: -0.03607185821907193  Val loss: 0.04623001404234634
Epoch 172
-------------------------------
Batch 1/64 loss: -0.019284963607788086
Batch 2/64 loss: -0.02037978172302246
Batch 3/64 loss: -0.04050469398498535
Batch 4/64 loss: -0.01624166965484619
Batch 5/64 loss: -0.019162893295288086
Batch 6/64 loss: -0.04322582483291626
Batch 7/64 loss: -0.018284320831298828
Batch 8/64 loss: -0.053915202617645264
Batch 9/64 loss: -0.035775184631347656
Batch 10/64 loss: -0.04965871572494507
Batch 11/64 loss: -0.04090869426727295
Batch 12/64 loss: -0.03952294588088989
Batch 13/64 loss: -0.04806870222091675
Batch 14/64 loss: -0.04873764514923096
Batch 15/64 loss: -0.03511732816696167
Batch 16/64 loss: -0.013102829456329346
Batch 17/64 loss: -0.0515209436416626
Batch 18/64 loss: -0.01932770013809204
Batch 19/64 loss: -0.030143320560455322
Batch 20/64 loss: -0.04786348342895508
Batch 21/64 loss: -0.047198474407196045
Batch 22/64 loss: -0.05131882429122925
Batch 23/64 loss: -0.034089744091033936
Batch 24/64 loss: -0.04657059907913208
Batch 25/64 loss: -0.049936652183532715
Batch 26/64 loss: -0.03508007526397705
Batch 27/64 loss: -0.017659902572631836
Batch 28/64 loss: -0.05575793981552124
Batch 29/64 loss: -0.027960240840911865
Batch 30/64 loss: -0.03380894660949707
Batch 31/64 loss: -0.04245954751968384
Batch 32/64 loss: -0.014790058135986328
Batch 33/64 loss: -0.045421302318573
Batch 34/64 loss: -0.03676176071166992
Batch 35/64 loss: -0.023136258125305176
Batch 36/64 loss: -0.03159826993942261
Batch 37/64 loss: -0.03712618350982666
Batch 38/64 loss: -0.02625107765197754
Batch 39/64 loss: -0.06071114540100098
Batch 40/64 loss: -0.04122191667556763
Batch 41/64 loss: -0.03630024194717407
Batch 42/64 loss: -0.04621720314025879
Batch 43/64 loss: -0.02368772029876709
Batch 44/64 loss: -0.02379024028778076
Batch 45/64 loss: -0.008261382579803467
Batch 46/64 loss: -0.028269827365875244
Batch 47/64 loss: -0.05524086952209473
Batch 48/64 loss: -0.03803980350494385
Batch 49/64 loss: -0.041003406047821045
Batch 50/64 loss: -0.044129014015197754
Batch 51/64 loss: -0.05119180679321289
Batch 52/64 loss: -0.0519598126411438
Batch 53/64 loss: -0.04162335395812988
Batch 54/64 loss: -0.0628470778465271
Batch 55/64 loss: -0.030599892139434814
Batch 56/64 loss: -0.0496671199798584
Batch 57/64 loss: -0.037661194801330566
Batch 58/64 loss: -0.04031568765640259
Batch 59/64 loss: -0.017241239547729492
Batch 60/64 loss: -0.028031587600708008
Batch 61/64 loss: -0.013944268226623535
Batch 62/64 loss: -0.047545015811920166
Batch 63/64 loss: -0.036508262157440186
Batch 64/64 loss: -0.03043234348297119
Epoch 172  Train loss: -0.03649421252456366  Val loss: 0.04474920997095272
Epoch 173
-------------------------------
Batch 1/64 loss: -0.055368900299072266
Batch 2/64 loss: -0.03821563720703125
Batch 3/64 loss: -0.046927690505981445
Batch 4/64 loss: -0.03799015283584595
Batch 5/64 loss: -0.038182616233825684
Batch 6/64 loss: -0.06211799383163452
Batch 7/64 loss: -0.02854984998703003
Batch 8/64 loss: -0.04440641403198242
Batch 9/64 loss: -0.02870631217956543
Batch 10/64 loss: -0.02345430850982666
Batch 11/64 loss: -0.04024624824523926
Batch 12/64 loss: -0.01779460906982422
Batch 13/64 loss: -0.042634546756744385
Batch 14/64 loss: -0.031981050968170166
Batch 15/64 loss: -0.0377083420753479
Batch 16/64 loss: -0.042153120040893555
Batch 17/64 loss: -0.038328468799591064
Batch 18/64 loss: -0.026903867721557617
Batch 19/64 loss: -0.06496387720108032
Batch 20/64 loss: -0.0010532140731811523
Batch 21/64 loss: -0.010286808013916016
Batch 22/64 loss: -0.03528314828872681
Batch 23/64 loss: -0.013085901737213135
Batch 24/64 loss: -0.05433839559555054
Batch 25/64 loss: -0.056096553802490234
Batch 26/64 loss: -0.016514837741851807
Batch 27/64 loss: -0.039439618587493896
Batch 28/64 loss: -0.014568924903869629
Batch 29/64 loss: -0.031517624855041504
Batch 30/64 loss: -0.015604257583618164
Batch 31/64 loss: -0.04364651441574097
Batch 32/64 loss: -0.01680225133895874
Batch 33/64 loss: -0.04066568613052368
Batch 34/64 loss: -0.0074800848960876465
Batch 35/64 loss: -0.03146922588348389
Batch 36/64 loss: -0.022558927536010742
Batch 37/64 loss: -0.03614377975463867
Batch 38/64 loss: -0.022301554679870605
Batch 39/64 loss: -0.037993550300598145
Batch 40/64 loss: -0.05746227502822876
Batch 41/64 loss: -0.03166007995605469
Batch 42/64 loss: -0.017642974853515625
Batch 43/64 loss: -0.04033631086349487
Batch 44/64 loss: -0.037286996841430664
Batch 45/64 loss: -0.03011035919189453
Batch 46/64 loss: -0.05824410915374756
Batch 47/64 loss: -0.023359358310699463
Batch 48/64 loss: -0.044719696044921875
Batch 49/64 loss: -0.021267414093017578
Batch 50/64 loss: -0.012976288795471191
Batch 51/64 loss: -0.05017709732055664
Batch 52/64 loss: -0.04037719964981079
Batch 53/64 loss: -0.0592954158782959
Batch 54/64 loss: -0.04086112976074219
Batch 55/64 loss: -0.009370386600494385
Batch 56/64 loss: -0.01291590929031372
Batch 57/64 loss: -0.03877687454223633
Batch 58/64 loss: -0.05164879560470581
Batch 59/64 loss: -0.06159484386444092
Batch 60/64 loss: -0.03508716821670532
Batch 61/64 loss: -0.05558574199676514
Batch 62/64 loss: -0.02982032299041748
Batch 63/64 loss: -0.005572497844696045
Batch 64/64 loss: -0.02285599708557129
Epoch 173  Train loss: -0.034145507625505035  Val loss: 0.046305600515345935
Epoch 174
-------------------------------
Batch 1/64 loss: -0.03228801488876343
Batch 2/64 loss: -0.04325908422470093
Batch 3/64 loss: -0.024437367916107178
Batch 4/64 loss: -0.03155708312988281
Batch 5/64 loss: -0.026948094367980957
Batch 6/64 loss: -0.04051315784454346
Batch 7/64 loss: -0.044812798500061035
Batch 8/64 loss: -0.034879446029663086
Batch 9/64 loss: -0.042659640312194824
Batch 10/64 loss: -0.043740272521972656
Batch 11/64 loss: -0.039935946464538574
Batch 12/64 loss: -0.04886144399642944
Batch 13/64 loss: -0.037028968334198
Batch 14/64 loss: -0.009363889694213867
Batch 15/64 loss: -0.04485940933227539
Batch 16/64 loss: -0.02821660041809082
Batch 17/64 loss: -0.020106971263885498
Batch 18/64 loss: -0.06744503974914551
Batch 19/64 loss: -0.04140162467956543
Batch 20/64 loss: -0.059661030769348145
Batch 21/64 loss: -0.025044381618499756
Batch 22/64 loss: -0.03071039915084839
Batch 23/64 loss: -0.03492844104766846
Batch 24/64 loss: -0.038688719272613525
Batch 25/64 loss: -0.03165799379348755
Batch 26/64 loss: -0.06022202968597412
Batch 27/64 loss: -0.04964125156402588
Batch 28/64 loss: -0.045234858989715576
Batch 29/64 loss: -0.03397321701049805
Batch 30/64 loss: -0.060415029525756836
Batch 31/64 loss: -0.054262518882751465
Batch 32/64 loss: -0.03058081865310669
Batch 33/64 loss: -0.045913755893707275
Batch 34/64 loss: -0.03415226936340332
Batch 35/64 loss: -0.054443955421447754
Batch 36/64 loss: -0.0361061692237854
Batch 37/64 loss: -0.03583860397338867
Batch 38/64 loss: -0.03216284513473511
Batch 39/64 loss: -0.039864420890808105
Batch 40/64 loss: -0.03900337219238281
Batch 41/64 loss: -0.007006466388702393
Batch 42/64 loss: -0.045200347900390625
Batch 43/64 loss: -0.021587848663330078
Batch 44/64 loss: -0.056619226932525635
Batch 45/64 loss: -0.024309635162353516
Batch 46/64 loss: -0.0417783260345459
Batch 47/64 loss: -0.03606283664703369
Batch 48/64 loss: -0.0450778603553772
Batch 49/64 loss: -0.026460707187652588
Batch 50/64 loss: -0.007515549659729004
Batch 51/64 loss: -0.028016865253448486
Batch 52/64 loss: -0.0481228232383728
Batch 53/64 loss: -0.041296184062957764
Batch 54/64 loss: -0.03760474920272827
Batch 55/64 loss: -0.03874552249908447
Batch 56/64 loss: -0.043897926807403564
Batch 57/64 loss: -0.05106925964355469
Batch 58/64 loss: -0.014506280422210693
Batch 59/64 loss: -0.049622535705566406
Batch 60/64 loss: -0.032009243965148926
Batch 61/64 loss: -0.04222613573074341
Batch 62/64 loss: -0.046600162982940674
Batch 63/64 loss: -0.04147934913635254
Batch 64/64 loss: -0.04259830713272095
Epoch 174  Train loss: -0.03817389034757427  Val loss: 0.04690406515016589
Epoch 175
-------------------------------
Batch 1/64 loss: -0.04060530662536621
Batch 2/64 loss: -0.047170400619506836
Batch 3/64 loss: -0.05591529607772827
Batch 4/64 loss: -0.039833903312683105
Batch 5/64 loss: -0.041249096393585205
Batch 6/64 loss: -0.028902947902679443
Batch 7/64 loss: -0.051050662994384766
Batch 8/64 loss: -0.01786506175994873
Batch 9/64 loss: -0.04184913635253906
Batch 10/64 loss: -0.04986405372619629
Batch 11/64 loss: -0.0283166766166687
Batch 12/64 loss: -0.03721308708190918
Batch 13/64 loss: -0.026204347610473633
Batch 14/64 loss: -0.050299882888793945
Batch 15/64 loss: -0.04615300893783569
Batch 16/64 loss: -0.0246884822845459
Batch 17/64 loss: -0.01690089702606201
Batch 18/64 loss: -0.03897362947463989
Batch 19/64 loss: -0.016298532485961914
Batch 20/64 loss: -0.0462644100189209
Batch 21/64 loss: -0.052725791931152344
Batch 22/64 loss: -0.02463376522064209
Batch 23/64 loss: -0.030011117458343506
Batch 24/64 loss: -0.0521429181098938
Batch 25/64 loss: -0.05566048622131348
Batch 26/64 loss: -0.048395395278930664
Batch 27/64 loss: -0.029922306537628174
Batch 28/64 loss: -0.013754427433013916
Batch 29/64 loss: -0.040341854095458984
Batch 30/64 loss: -0.03977799415588379
Batch 31/64 loss: -0.03346151113510132
Batch 32/64 loss: -0.042916297912597656
Batch 33/64 loss: -0.014822006225585938
Batch 34/64 loss: -0.04313385486602783
Batch 35/64 loss: -0.057686448097229004
Batch 36/64 loss: -0.05680346488952637
Batch 37/64 loss: -0.054588139057159424
Batch 38/64 loss: -0.05495631694793701
Batch 39/64 loss: -0.06981021165847778
Batch 40/64 loss: -0.036808133125305176
Batch 41/64 loss: -0.030818581581115723
Batch 42/64 loss: -0.046675026416778564
Batch 43/64 loss: -0.036861538887023926
Batch 44/64 loss: -0.035056233406066895
Batch 45/64 loss: -0.007582902908325195
Batch 46/64 loss: -0.02635979652404785
Batch 47/64 loss: -0.01032865047454834
Batch 48/64 loss: -0.03756272792816162
Batch 49/64 loss: -0.006838321685791016
Batch 50/64 loss: -0.04880326986312866
Batch 51/64 loss: -0.04207026958465576
Batch 52/64 loss: -0.043547987937927246
Batch 53/64 loss: -0.03737086057662964
Batch 54/64 loss: -0.03541082143783569
Batch 55/64 loss: -0.02747577428817749
Batch 56/64 loss: -0.04943650960922241
Batch 57/64 loss: -0.02786839008331299
Batch 58/64 loss: -0.039113402366638184
Batch 59/64 loss: -0.05374932289123535
Batch 60/64 loss: -0.031249523162841797
Batch 61/64 loss: -0.04639405012130737
Batch 62/64 loss: -0.03820878267288208
Batch 63/64 loss: -0.02460002899169922
Batch 64/64 loss: -0.03450828790664673
Epoch 175  Train loss: -0.03776055293924668  Val loss: 0.04537374784856318
Epoch 176
-------------------------------
Batch 1/64 loss: -0.06775975227355957
Batch 2/64 loss: -0.055649757385253906
Batch 3/64 loss: -0.030459702014923096
Batch 4/64 loss: -0.04780113697052002
Batch 5/64 loss: -0.024761080741882324
Batch 6/64 loss: -0.053507983684539795
Batch 7/64 loss: -0.06248384714126587
Batch 8/64 loss: -0.03290587663650513
Batch 9/64 loss: -0.04756975173950195
Batch 10/64 loss: -0.05324256420135498
Batch 11/64 loss: -0.03083270788192749
Batch 12/64 loss: -0.05047661066055298
Batch 13/64 loss: -0.03166604042053223
Batch 14/64 loss: -0.048688530921936035
Batch 15/64 loss: -0.03829103708267212
Batch 16/64 loss: -0.017258942127227783
Batch 17/64 loss: -0.040044546127319336
Batch 18/64 loss: -0.012940526008605957
Batch 19/64 loss: -0.04290032386779785
Batch 20/64 loss: -0.03741341829299927
Batch 21/64 loss: -0.03533804416656494
Batch 22/64 loss: -0.026801109313964844
Batch 23/64 loss: -0.035832762718200684
Batch 24/64 loss: -0.037938475608825684
Batch 25/64 loss: -0.039824068546295166
Batch 26/64 loss: -0.03717672824859619
Batch 27/64 loss: -0.04827094078063965
Batch 28/64 loss: -0.026739299297332764
Batch 29/64 loss: -0.055008649826049805
Batch 30/64 loss: -0.0313495397567749
Batch 31/64 loss: -0.033986687660217285
Batch 32/64 loss: -0.03238862752914429
Batch 33/64 loss: -0.03006291389465332
Batch 34/64 loss: -0.028772413730621338
Batch 35/64 loss: -0.039847731590270996
Batch 36/64 loss: -0.05850344896316528
Batch 37/64 loss: -0.0333896279335022
Batch 38/64 loss: -0.038815975189208984
Batch 39/64 loss: -0.06147503852844238
Batch 40/64 loss: -0.06034576892852783
Batch 41/64 loss: -0.03309941291809082
Batch 42/64 loss: 0.0032427310943603516
Batch 43/64 loss: -0.040007174015045166
Batch 44/64 loss: -0.039882779121398926
Batch 45/64 loss: -0.04670614004135132
Batch 46/64 loss: -0.03700566291809082
Batch 47/64 loss: -0.03575688600540161
Batch 48/64 loss: -0.020893335342407227
Batch 49/64 loss: -0.03149765729904175
Batch 50/64 loss: -0.05866849422454834
Batch 51/64 loss: -0.040513694286346436
Batch 52/64 loss: -0.049548327922821045
Batch 53/64 loss: -0.04707467555999756
Batch 54/64 loss: -0.04741322994232178
Batch 55/64 loss: -0.0754515528678894
Batch 56/64 loss: -0.05302226543426514
Batch 57/64 loss: -0.038564205169677734
Batch 58/64 loss: -0.030489683151245117
Batch 59/64 loss: -0.0284157395362854
Batch 60/64 loss: -0.03912550210952759
Batch 61/64 loss: -2.956390380859375e-05
Batch 62/64 loss: -0.05286651849746704
Batch 63/64 loss: -0.015807747840881348
Batch 64/64 loss: -0.0004096031188964844
Epoch 176  Train loss: -0.038861595415601545  Val loss: 0.047973371453301605
Epoch 177
-------------------------------
Batch 1/64 loss: -0.04424172639846802
Batch 2/64 loss: -0.046649396419525146
Batch 3/64 loss: -0.049391329288482666
Batch 4/64 loss: -0.008909463882446289
Batch 5/64 loss: -0.03615754842758179
Batch 6/64 loss: -0.04704773426055908
Batch 7/64 loss: -0.040334880352020264
Batch 8/64 loss: -0.025395631790161133
Batch 9/64 loss: -0.03973954916000366
Batch 10/64 loss: -0.032115399837493896
Batch 11/64 loss: -0.05574154853820801
Batch 12/64 loss: -0.0425267219543457
Batch 13/64 loss: -0.009001970291137695
Batch 14/64 loss: -0.04035818576812744
Batch 15/64 loss: -0.022347092628479004
Batch 16/64 loss: -0.04648059606552124
Batch 17/64 loss: -0.051157236099243164
Batch 18/64 loss: -0.030124247074127197
Batch 19/64 loss: -0.03915572166442871
Batch 20/64 loss: -0.03387939929962158
Batch 21/64 loss: -0.015820860862731934
Batch 22/64 loss: -0.05115705728530884
Batch 23/64 loss: -0.033120155334472656
Batch 24/64 loss: -0.030237913131713867
Batch 25/64 loss: -0.024706900119781494
Batch 26/64 loss: -0.03801620006561279
Batch 27/64 loss: -0.05387789011001587
Batch 28/64 loss: -0.03413891792297363
Batch 29/64 loss: -0.022992432117462158
Batch 30/64 loss: -0.053652822971343994
Batch 31/64 loss: -0.02930593490600586
Batch 32/64 loss: -0.02320486307144165
Batch 33/64 loss: -0.07034409046173096
Batch 34/64 loss: -0.03864622116088867
Batch 35/64 loss: -0.03736066818237305
Batch 36/64 loss: -0.04017263650894165
Batch 37/64 loss: -0.0467451810836792
Batch 38/64 loss: -0.03692770004272461
Batch 39/64 loss: -0.03422689437866211
Batch 40/64 loss: -0.032152771949768066
Batch 41/64 loss: -0.04548990726470947
Batch 42/64 loss: -0.05327868461608887
Batch 43/64 loss: -0.024505138397216797
Batch 44/64 loss: -0.06492030620574951
Batch 45/64 loss: -0.05911797285079956
Batch 46/64 loss: -0.054624974727630615
Batch 47/64 loss: -0.05207669734954834
Batch 48/64 loss: -0.042105674743652344
Batch 49/64 loss: -0.03596371412277222
Batch 50/64 loss: -0.05112510919570923
Batch 51/64 loss: -0.047704994678497314
Batch 52/64 loss: -0.03826779127120972
Batch 53/64 loss: -0.04136228561401367
Batch 54/64 loss: -0.0450206995010376
Batch 55/64 loss: -0.028464913368225098
Batch 56/64 loss: -0.050859034061431885
Batch 57/64 loss: -0.052446603775024414
Batch 58/64 loss: -0.02093672752380371
Batch 59/64 loss: -0.02913069725036621
Batch 60/64 loss: -0.027758240699768066
Batch 61/64 loss: 0.009865343570709229
Batch 62/64 loss: -0.02580118179321289
Batch 63/64 loss: -0.06483656167984009
Batch 64/64 loss: -0.013209879398345947
Epoch 177  Train loss: -0.03832789752997604  Val loss: 0.04933266479944445
Epoch 178
-------------------------------
Batch 1/64 loss: -0.03626185655593872
Batch 2/64 loss: -0.0250665545463562
Batch 3/64 loss: -0.042994022369384766
Batch 4/64 loss: -0.03124082088470459
Batch 5/64 loss: -0.04385662078857422
Batch 6/64 loss: -0.04478013515472412
Batch 7/64 loss: -0.03357285261154175
Batch 8/64 loss: -0.05258446931838989
Batch 9/64 loss: -0.021489739418029785
Batch 10/64 loss: -0.0474621057510376
Batch 11/64 loss: -0.05938690900802612
Batch 12/64 loss: -0.037120521068573
Batch 13/64 loss: -0.04274106025695801
Batch 14/64 loss: -0.03445100784301758
Batch 15/64 loss: -0.03883695602416992
Batch 16/64 loss: -0.0517008900642395
Batch 17/64 loss: -0.04599416255950928
Batch 18/64 loss: -0.050288259983062744
Batch 19/64 loss: -0.04185706377029419
Batch 20/64 loss: -0.03328096866607666
Batch 21/64 loss: -0.03953719139099121
Batch 22/64 loss: -0.029789328575134277
Batch 23/64 loss: -0.04072529077529907
Batch 24/64 loss: -0.022473812103271484
Batch 25/64 loss: -0.03011190891265869
Batch 26/64 loss: -0.030935287475585938
Batch 27/64 loss: -0.027524948120117188
Batch 28/64 loss: -0.046563148498535156
Batch 29/64 loss: -0.03830397129058838
Batch 30/64 loss: -0.01239079236984253
Batch 31/64 loss: -0.04725140333175659
Batch 32/64 loss: -0.010007202625274658
Batch 33/64 loss: -0.07520073652267456
Batch 34/64 loss: -0.03825545310974121
Batch 35/64 loss: -0.034409284591674805
Batch 36/64 loss: -0.03535652160644531
Batch 37/64 loss: -0.039490342140197754
Batch 38/64 loss: -0.04927271604537964
Batch 39/64 loss: -0.04399174451828003
Batch 40/64 loss: -0.05625009536743164
Batch 41/64 loss: -0.03368610143661499
Batch 42/64 loss: -0.05652165412902832
Batch 43/64 loss: -0.03901594877243042
Batch 44/64 loss: -0.05046111345291138
Batch 45/64 loss: -0.04465746879577637
Batch 46/64 loss: -0.04186749458312988
Batch 47/64 loss: -0.048924148082733154
Batch 48/64 loss: -0.039842307567596436
Batch 49/64 loss: -0.019872188568115234
Batch 50/64 loss: -0.025405704975128174
Batch 51/64 loss: -0.034191787242889404
Batch 52/64 loss: -0.04841351509094238
Batch 53/64 loss: -0.03759276866912842
Batch 54/64 loss: -0.05006706714630127
Batch 55/64 loss: -0.023757100105285645
Batch 56/64 loss: -0.03649336099624634
Batch 57/64 loss: -0.026015400886535645
Batch 58/64 loss: -0.05252641439437866
Batch 59/64 loss: -0.035375893115997314
Batch 60/64 loss: -0.048080503940582275
Batch 61/64 loss: -0.03490865230560303
Batch 62/64 loss: -0.03302192687988281
Batch 63/64 loss: -0.05155819654464722
Batch 64/64 loss: -0.03918057680130005
Epoch 178  Train loss: -0.03928549500072703  Val loss: 0.04720069186384326
Epoch 179
-------------------------------
Batch 1/64 loss: -0.05370962619781494
Batch 2/64 loss: -0.04980027675628662
Batch 3/64 loss: -0.05909597873687744
Batch 4/64 loss: -0.03937739133834839
Batch 5/64 loss: -0.03706777095794678
Batch 6/64 loss: -0.04495847225189209
Batch 7/64 loss: -0.033919692039489746
Batch 8/64 loss: -0.03444749116897583
Batch 9/64 loss: -0.06339311599731445
Batch 10/64 loss: -0.0323372483253479
Batch 11/64 loss: -0.036205291748046875
Batch 12/64 loss: -0.03828561305999756
Batch 13/64 loss: -0.06454038619995117
Batch 14/64 loss: -0.032990455627441406
Batch 15/64 loss: -0.015528082847595215
Batch 16/64 loss: -0.06324911117553711
Batch 17/64 loss: -0.043094098567962646
Batch 18/64 loss: -0.06921303272247314
Batch 19/64 loss: -0.05885124206542969
Batch 20/64 loss: -0.0576174259185791
Batch 21/64 loss: -0.04897904396057129
Batch 22/64 loss: -0.046600282192230225
Batch 23/64 loss: -0.05295282602310181
Batch 24/64 loss: -0.031466007232666016
Batch 25/64 loss: -0.03552430868148804
Batch 26/64 loss: -0.05203789472579956
Batch 27/64 loss: -0.026875555515289307
Batch 28/64 loss: -0.05104285478591919
Batch 29/64 loss: -0.055324435234069824
Batch 30/64 loss: -0.04519510269165039
Batch 31/64 loss: -0.050627827644348145
Batch 32/64 loss: -0.023481130599975586
Batch 33/64 loss: -0.06085360050201416
Batch 34/64 loss: -0.04870307445526123
Batch 35/64 loss: -0.030362725257873535
Batch 36/64 loss: -0.04391336441040039
Batch 37/64 loss: -0.05507051944732666
Batch 38/64 loss: -0.034767985343933105
Batch 39/64 loss: -0.012208521366119385
Batch 40/64 loss: -0.05665856599807739
Batch 41/64 loss: -0.02157902717590332
Batch 42/64 loss: -0.04492652416229248
Batch 43/64 loss: -0.0457768440246582
Batch 44/64 loss: -0.03217041492462158
Batch 45/64 loss: -0.04326075315475464
Batch 46/64 loss: -0.033292293548583984
Batch 47/64 loss: -0.05946362018585205
Batch 48/64 loss: -0.05633765459060669
Batch 49/64 loss: -0.04339611530303955
Batch 50/64 loss: -0.05012667179107666
Batch 51/64 loss: -0.026049494743347168
Batch 52/64 loss: -0.04802757501602173
Batch 53/64 loss: -0.01973867416381836
Batch 54/64 loss: -0.006508469581604004
Batch 55/64 loss: -0.011547446250915527
Batch 56/64 loss: -0.030810296535491943
Batch 57/64 loss: 0.00798487663269043
Batch 58/64 loss: -0.004584074020385742
Batch 59/64 loss: -0.03687208890914917
Batch 60/64 loss: -0.03257840871810913
Batch 61/64 loss: -0.003590703010559082
Batch 62/64 loss: -0.04658025503158569
Batch 63/64 loss: -0.03206557035446167
Batch 64/64 loss: -0.0441896915435791
Epoch 179  Train loss: -0.039917926227345184  Val loss: 0.047075942209905776
Epoch 180
-------------------------------
Batch 1/64 loss: -0.03307974338531494
Batch 2/64 loss: -0.035655856132507324
Batch 3/64 loss: -0.03462111949920654
Batch 4/64 loss: -0.01932394504547119
Batch 5/64 loss: -0.02822655439376831
Batch 6/64 loss: -0.04092609882354736
Batch 7/64 loss: -0.052365660667419434
Batch 8/64 loss: -0.05092865228652954
Batch 9/64 loss: -0.04562288522720337
Batch 10/64 loss: -0.03798532485961914
Batch 11/64 loss: -0.03734666109085083
Batch 12/64 loss: -0.03987377882003784
Batch 13/64 loss: -0.06538903713226318
Batch 14/64 loss: -0.015440642833709717
Batch 15/64 loss: -0.06391561031341553
Batch 16/64 loss: -0.03556126356124878
Batch 17/64 loss: -0.04730886220932007
Batch 18/64 loss: -0.05333983898162842
Batch 19/64 loss: -0.038776934146881104
Batch 20/64 loss: -0.032958805561065674
Batch 21/64 loss: -0.045896828174591064
Batch 22/64 loss: -0.04123079776763916
Batch 23/64 loss: -0.03349381685256958
Batch 24/64 loss: -0.04840540885925293
Batch 25/64 loss: -0.05284130573272705
Batch 26/64 loss: -0.052565693855285645
Batch 27/64 loss: -0.026660799980163574
Batch 28/64 loss: -0.03001117706298828
Batch 29/64 loss: -0.05851536989212036
Batch 30/64 loss: -0.03282839059829712
Batch 31/64 loss: -0.06597328186035156
Batch 32/64 loss: -0.05863767862319946
Batch 33/64 loss: -0.05128777027130127
Batch 34/64 loss: -0.046611785888671875
Batch 35/64 loss: -0.03780829906463623
Batch 36/64 loss: -0.04943084716796875
Batch 37/64 loss: -0.061271071434020996
Batch 38/64 loss: -0.059304773807525635
Batch 39/64 loss: -0.046928226947784424
Batch 40/64 loss: -0.016627252101898193
Batch 41/64 loss: -0.023551523685455322
Batch 42/64 loss: -0.029264450073242188
Batch 43/64 loss: -0.04024088382720947
Batch 44/64 loss: -0.032197415828704834
Batch 45/64 loss: -0.02727222442626953
Batch 46/64 loss: -0.028123795986175537
Batch 47/64 loss: -0.027560830116271973
Batch 48/64 loss: -0.039703190326690674
Batch 49/64 loss: -0.04348701238632202
Batch 50/64 loss: -0.049431681632995605
Batch 51/64 loss: -0.03911876678466797
Batch 52/64 loss: -0.0517849326133728
Batch 53/64 loss: -0.018008172512054443
Batch 54/64 loss: -0.04028141498565674
Batch 55/64 loss: -0.018955111503601074
Batch 56/64 loss: -0.025255322456359863
Batch 57/64 loss: -0.014491021633148193
Batch 58/64 loss: -0.034532904624938965
Batch 59/64 loss: -0.04383820295333862
Batch 60/64 loss: -0.036129891872406006
Batch 61/64 loss: -0.03658777475357056
Batch 62/64 loss: -0.03527641296386719
Batch 63/64 loss: -0.035829782485961914
Batch 64/64 loss: -0.052268266677856445
Epoch 180  Train loss: -0.039766302295759615  Val loss: 0.04828265934056023
Epoch 181
-------------------------------
Batch 1/64 loss: -0.04113173484802246
Batch 2/64 loss: -0.020015597343444824
Batch 3/64 loss: -0.03867948055267334
Batch 4/64 loss: -0.028280258178710938
Batch 5/64 loss: -0.022195816040039062
Batch 6/64 loss: -0.03367769718170166
Batch 7/64 loss: -0.025560498237609863
Batch 8/64 loss: -0.03325927257537842
Batch 9/64 loss: -0.05738019943237305
Batch 10/64 loss: -0.051666438579559326
Batch 11/64 loss: -0.052044034004211426
Batch 12/64 loss: -0.0146942138671875
Batch 13/64 loss: -0.04057574272155762
Batch 14/64 loss: -0.04496574401855469
Batch 15/64 loss: -0.04102212190628052
Batch 16/64 loss: -0.030475199222564697
Batch 17/64 loss: -0.05359923839569092
Batch 18/64 loss: -0.03372877836227417
Batch 19/64 loss: -0.05532562732696533
Batch 20/64 loss: -0.028847038745880127
Batch 21/64 loss: -0.024704933166503906
Batch 22/64 loss: -0.05039149522781372
Batch 23/64 loss: -0.051968276500701904
Batch 24/64 loss: -0.050943195819854736
Batch 25/64 loss: -0.03524768352508545
Batch 26/64 loss: -0.049898624420166016
Batch 27/64 loss: -0.04720860719680786
Batch 28/64 loss: -0.051896095275878906
Batch 29/64 loss: -0.04279446601867676
Batch 30/64 loss: -0.046141862869262695
Batch 31/64 loss: -0.012750804424285889
Batch 32/64 loss: -0.04674637317657471
Batch 33/64 loss: -0.02896338701248169
Batch 34/64 loss: -0.05207467079162598
Batch 35/64 loss: -0.04075700044631958
Batch 36/64 loss: -0.030677318572998047
Batch 37/64 loss: -0.03458750247955322
Batch 38/64 loss: -0.059577107429504395
Batch 39/64 loss: -0.054998695850372314
Batch 40/64 loss: -0.059975385665893555
Batch 41/64 loss: -0.029998838901519775
Batch 42/64 loss: -0.05883479118347168
Batch 43/64 loss: -0.04905658960342407
Batch 44/64 loss: -0.051510751247406006
Batch 45/64 loss: -0.04576534032821655
Batch 46/64 loss: -0.032489120960235596
Batch 47/64 loss: -0.04748481512069702
Batch 48/64 loss: -0.05755239725112915
Batch 49/64 loss: -0.028406977653503418
Batch 50/64 loss: -0.035011887550354004
Batch 51/64 loss: -0.050832509994506836
Batch 52/64 loss: -0.014596700668334961
Batch 53/64 loss: -0.018220901489257812
Batch 54/64 loss: -0.022420525550842285
Batch 55/64 loss: -0.050957322120666504
Batch 56/64 loss: -0.04035747051239014
Batch 57/64 loss: -0.05283540487289429
Batch 58/64 loss: -0.0452580451965332
Batch 59/64 loss: -0.04673624038696289
Batch 60/64 loss: -0.04543060064315796
Batch 61/64 loss: -0.04504746198654175
Batch 62/64 loss: -0.02514946460723877
Batch 63/64 loss: -0.021346747875213623
Batch 64/64 loss: -0.054813921451568604
Epoch 181  Train loss: -0.04040532649732104  Val loss: 0.04539375780374324
Epoch 182
-------------------------------
Batch 1/64 loss: -0.03621029853820801
Batch 2/64 loss: -0.03632509708404541
Batch 3/64 loss: -0.05642056465148926
Batch 4/64 loss: -0.06312745809555054
Batch 5/64 loss: -0.025989532470703125
Batch 6/64 loss: -0.045590758323669434
Batch 7/64 loss: -0.03207683563232422
Batch 8/64 loss: -0.06671029329299927
Batch 9/64 loss: -0.048747897148132324
Batch 10/64 loss: -0.05057799816131592
Batch 11/64 loss: -0.04998767375946045
Batch 12/64 loss: -0.06337374448776245
Batch 13/64 loss: -0.05610954761505127
Batch 14/64 loss: -0.03963857889175415
Batch 15/64 loss: -0.0240628719329834
Batch 16/64 loss: -0.05535614490509033
Batch 17/64 loss: -0.060365259647369385
Batch 18/64 loss: -0.04653525352478027
Batch 19/64 loss: -0.04147684574127197
Batch 20/64 loss: -0.04014170169830322
Batch 21/64 loss: -0.043414175510406494
Batch 22/64 loss: -0.04555559158325195
Batch 23/64 loss: -0.03418409824371338
Batch 24/64 loss: -0.05444115400314331
Batch 25/64 loss: -0.048770010471343994
Batch 26/64 loss: -0.01850813627243042
Batch 27/64 loss: -0.04946935176849365
Batch 28/64 loss: -0.06135737895965576
Batch 29/64 loss: -0.044353365898132324
Batch 30/64 loss: -0.005341708660125732
Batch 31/64 loss: -0.04833108186721802
Batch 32/64 loss: -0.03378736972808838
Batch 33/64 loss: -0.045015156269073486
Batch 34/64 loss: -0.05797719955444336
Batch 35/64 loss: -0.04371613264083862
Batch 36/64 loss: -0.016862154006958008
Batch 37/64 loss: -0.041498661041259766
Batch 38/64 loss: -0.0608903169631958
Batch 39/64 loss: -0.043693602085113525
Batch 40/64 loss: -0.037161409854888916
Batch 41/64 loss: 0.0010082721710205078
Batch 42/64 loss: -0.05779021978378296
Batch 43/64 loss: -0.04571533203125
Batch 44/64 loss: -0.05833953619003296
Batch 45/64 loss: -0.03441739082336426
Batch 46/64 loss: -0.0462610125541687
Batch 47/64 loss: -0.04729163646697998
Batch 48/64 loss: -0.04938167333602905
Batch 49/64 loss: -0.03220045566558838
Batch 50/64 loss: -0.0010988116264343262
Batch 51/64 loss: -0.025339484214782715
Batch 52/64 loss: -0.029736459255218506
Batch 53/64 loss: -0.033242106437683105
Batch 54/64 loss: -0.07508206367492676
Batch 55/64 loss: -0.04691159725189209
Batch 56/64 loss: -0.026122868061065674
Batch 57/64 loss: -0.030182719230651855
Batch 58/64 loss: -0.058369219303131104
Batch 59/64 loss: -0.0542224645614624
Batch 60/64 loss: -0.03797316551208496
Batch 61/64 loss: -0.019934892654418945
Batch 62/64 loss: -0.029822826385498047
Batch 63/64 loss: -0.05931413173675537
Batch 64/64 loss: -0.052138328552246094
Epoch 182  Train loss: -0.04250977179583381  Val loss: 0.04599502344721371
Epoch 183
-------------------------------
Batch 1/64 loss: -0.05012226104736328
Batch 2/64 loss: -0.050493836402893066
Batch 3/64 loss: -0.05696314573287964
Batch 4/64 loss: -0.06803733110427856
Batch 5/64 loss: -0.013645350933074951
Batch 6/64 loss: -0.03817760944366455
Batch 7/64 loss: -0.05724942684173584
Batch 8/64 loss: -0.05555015802383423
Batch 9/64 loss: -0.05177813768386841
Batch 10/64 loss: -0.04486161470413208
Batch 11/64 loss: -0.029816389083862305
Batch 12/64 loss: -0.047415733337402344
Batch 13/64 loss: -0.06451606750488281
Batch 14/64 loss: -0.03066384792327881
Batch 15/64 loss: -0.017822086811065674
Batch 16/64 loss: -0.03705775737762451
Batch 17/64 loss: -0.025069773197174072
Batch 18/64 loss: -0.06869572401046753
Batch 19/64 loss: -0.05676072835922241
Batch 20/64 loss: -0.06579232215881348
Batch 21/64 loss: -0.040456295013427734
Batch 22/64 loss: -0.04276400804519653
Batch 23/64 loss: -0.043348610401153564
Batch 24/64 loss: -0.05179721117019653
Batch 25/64 loss: -0.03134942054748535
Batch 26/64 loss: -0.050525963306427
Batch 27/64 loss: -0.03671067953109741
Batch 28/64 loss: -0.04882204532623291
Batch 29/64 loss: -0.04317045211791992
Batch 30/64 loss: -0.0556522011756897
Batch 31/64 loss: -0.051166653633117676
Batch 32/64 loss: -0.052487969398498535
Batch 33/64 loss: -0.017892181873321533
Batch 34/64 loss: -0.03751957416534424
Batch 35/64 loss: -0.03652334213256836
Batch 36/64 loss: -0.06699925661087036
Batch 37/64 loss: -0.06835824251174927
Batch 38/64 loss: -0.03040766716003418
Batch 39/64 loss: -0.05058181285858154
Batch 40/64 loss: -0.013239860534667969
Batch 41/64 loss: -0.03776359558105469
Batch 42/64 loss: -0.05702471733093262
Batch 43/64 loss: -0.030900299549102783
Batch 44/64 loss: -0.03299051523208618
Batch 45/64 loss: -0.049765825271606445
Batch 46/64 loss: -0.05821728706359863
Batch 47/64 loss: -0.05052536725997925
Batch 48/64 loss: -0.041257262229919434
Batch 49/64 loss: -0.029406070709228516
Batch 50/64 loss: -0.03450751304626465
Batch 51/64 loss: -0.05682194232940674
Batch 52/64 loss: -0.01717376708984375
Batch 53/64 loss: -0.046792685985565186
Batch 54/64 loss: -0.06246054172515869
Batch 55/64 loss: -0.04464888572692871
Batch 56/64 loss: -0.04516589641571045
Batch 57/64 loss: -0.03024148941040039
Batch 58/64 loss: -0.018409132957458496
Batch 59/64 loss: -0.054027020931243896
Batch 60/64 loss: -0.028686165809631348
Batch 61/64 loss: -0.04509544372558594
Batch 62/64 loss: -0.0341496467590332
Batch 63/64 loss: 0.00930774211883545
Batch 64/64 loss: -0.036564648151397705
Epoch 183  Train loss: -0.04273583631889493  Val loss: 0.04451572772153874
Epoch 184
-------------------------------
Batch 1/64 loss: -0.05440568923950195
Batch 2/64 loss: -0.049347758293151855
Batch 3/64 loss: -0.04605269432067871
Batch 4/64 loss: -0.04117631912231445
Batch 5/64 loss: -0.06412369012832642
Batch 6/64 loss: -0.02719634771347046
Batch 7/64 loss: -0.050360679626464844
Batch 8/64 loss: -0.030933618545532227
Batch 9/64 loss: -0.04931020736694336
Batch 10/64 loss: -0.04920804500579834
Batch 11/64 loss: -0.0604320764541626
Batch 12/64 loss: -0.053375422954559326
Batch 13/64 loss: -0.05695939064025879
Batch 14/64 loss: -0.0040378570556640625
Batch 15/64 loss: -0.053527891635894775
Batch 16/64 loss: -0.04554760456085205
Batch 17/64 loss: -0.03809696435928345
Batch 18/64 loss: -0.03680455684661865
Batch 19/64 loss: -0.07895082235336304
Batch 20/64 loss: -0.06010401248931885
Batch 21/64 loss: -0.04545038938522339
Batch 22/64 loss: -0.05906534194946289
Batch 23/64 loss: -0.03884768486022949
Batch 24/64 loss: -0.057840943336486816
Batch 25/64 loss: -0.05650508403778076
Batch 26/64 loss: -0.044122517108917236
Batch 27/64 loss: -0.07168686389923096
Batch 28/64 loss: -0.04146963357925415
Batch 29/64 loss: -0.04694974422454834
Batch 30/64 loss: -0.0144081711769104
Batch 31/64 loss: -0.045150160789489746
Batch 32/64 loss: -0.0446399450302124
Batch 33/64 loss: -0.046874284744262695
Batch 34/64 loss: -0.047669827938079834
Batch 35/64 loss: -0.02418452501296997
Batch 36/64 loss: -0.05480009317398071
Batch 37/64 loss: -0.05987948179244995
Batch 38/64 loss: -0.04150944948196411
Batch 39/64 loss: -0.044247984886169434
Batch 40/64 loss: -0.041333794593811035
Batch 41/64 loss: -0.042260169982910156
Batch 42/64 loss: -0.03966885805130005
Batch 43/64 loss: -0.06786775588989258
Batch 44/64 loss: -0.06176924705505371
Batch 45/64 loss: -0.03539079427719116
Batch 46/64 loss: -0.03992241621017456
Batch 47/64 loss: -0.05239272117614746
Batch 48/64 loss: -0.037155866622924805
Batch 49/64 loss: -0.03864312171936035
Batch 50/64 loss: -0.013923168182373047
Batch 51/64 loss: -0.02787601947784424
Batch 52/64 loss: -0.03739279508590698
Batch 53/64 loss: -0.032990097999572754
Batch 54/64 loss: -0.0366787314414978
Batch 55/64 loss: -0.03871917724609375
Batch 56/64 loss: -0.04711616039276123
Batch 57/64 loss: -0.044644296169281006
Batch 58/64 loss: -0.04306405782699585
Batch 59/64 loss: -0.027193427085876465
Batch 60/64 loss: -0.041808128356933594
Batch 61/64 loss: -0.04233270883560181
Batch 62/64 loss: -0.015731751918792725
Batch 63/64 loss: -0.026497483253479004
Batch 64/64 loss: -0.04229629039764404
Epoch 184  Train loss: -0.04391135281207515  Val loss: 0.04932078217313052
Epoch 185
-------------------------------
Batch 1/64 loss: -0.03542912006378174
Batch 2/64 loss: -0.04495668411254883
Batch 3/64 loss: -0.0540999174118042
Batch 4/64 loss: -0.06518274545669556
Batch 5/64 loss: -0.04739367961883545
Batch 6/64 loss: -0.05740076303482056
Batch 7/64 loss: -0.022243857383728027
Batch 8/64 loss: -0.0545499324798584
Batch 9/64 loss: -0.0199739933013916
Batch 10/64 loss: -0.0478702187538147
Batch 11/64 loss: -0.042798519134521484
Batch 12/64 loss: -0.06933856010437012
Batch 13/64 loss: -0.03387010097503662
Batch 14/64 loss: -0.05044668912887573
Batch 15/64 loss: -0.03936225175857544
Batch 16/64 loss: -0.03973346948623657
Batch 17/64 loss: -0.05931901931762695
Batch 18/64 loss: -0.045845210552215576
Batch 19/64 loss: -0.05059182643890381
Batch 20/64 loss: -0.012858867645263672
Batch 21/64 loss: -0.03965890407562256
Batch 22/64 loss: -0.03960698843002319
Batch 23/64 loss: -0.007111847400665283
Batch 24/64 loss: -0.05844008922576904
Batch 25/64 loss: -0.039745867252349854
Batch 26/64 loss: -0.03477597236633301
Batch 27/64 loss: -0.05800783634185791
Batch 28/64 loss: -0.04038172960281372
Batch 29/64 loss: -0.05527991056442261
Batch 30/64 loss: -0.03215157985687256
Batch 31/64 loss: -0.02227717638015747
Batch 32/64 loss: -0.03753399848937988
Batch 33/64 loss: -0.039109110832214355
Batch 34/64 loss: -0.054808080196380615
Batch 35/64 loss: -0.031094491481781006
Batch 36/64 loss: -0.03900456428527832
Batch 37/64 loss: -0.042388916015625
Batch 38/64 loss: -0.05320417881011963
Batch 39/64 loss: -0.04500687122344971
Batch 40/64 loss: -0.01742953062057495
Batch 41/64 loss: -0.03624165058135986
Batch 42/64 loss: -0.04396510124206543
Batch 43/64 loss: -0.045930564403533936
Batch 44/64 loss: -0.05482363700866699
Batch 45/64 loss: -0.05444866418838501
Batch 46/64 loss: -0.02392137050628662
Batch 47/64 loss: -0.06103646755218506
Batch 48/64 loss: -0.022374868392944336
Batch 49/64 loss: -0.03940379619598389
Batch 50/64 loss: -0.0433502197265625
Batch 51/64 loss: -0.043165743350982666
Batch 52/64 loss: -0.03335696458816528
Batch 53/64 loss: -0.03527474403381348
Batch 54/64 loss: -0.03632289171218872
Batch 55/64 loss: -0.03753042221069336
Batch 56/64 loss: -0.030743718147277832
Batch 57/64 loss: -0.03049832582473755
Batch 58/64 loss: -0.010009288787841797
Batch 59/64 loss: -0.04717737436294556
Batch 60/64 loss: -0.0362170934677124
Batch 61/64 loss: -0.03733062744140625
Batch 62/64 loss: -0.028686881065368652
Batch 63/64 loss: -0.04200989007949829
Batch 64/64 loss: -0.04216045141220093
Epoch 185  Train loss: -0.04056037196926042  Val loss: 0.04892683131588284
Epoch 186
-------------------------------
Batch 1/64 loss: -0.05419731140136719
Batch 2/64 loss: -0.055644094944000244
Batch 3/64 loss: -0.05260753631591797
Batch 4/64 loss: -0.021630465984344482
Batch 5/64 loss: -0.040537238121032715
Batch 6/64 loss: -0.01915431022644043
Batch 7/64 loss: -0.06301009654998779
Batch 8/64 loss: -0.016544103622436523
Batch 9/64 loss: -0.04812884330749512
Batch 10/64 loss: -0.05050915479660034
Batch 11/64 loss: -0.05379056930541992
Batch 12/64 loss: -0.04903054237365723
Batch 13/64 loss: -0.04077720642089844
Batch 14/64 loss: -0.050398051738739014
Batch 15/64 loss: -0.0444188117980957
Batch 16/64 loss: -0.05594116449356079
Batch 17/64 loss: -0.06984972953796387
Batch 18/64 loss: -0.05954819917678833
Batch 19/64 loss: -0.05542337894439697
Batch 20/64 loss: -0.04507339000701904
Batch 21/64 loss: -0.047286808490753174
Batch 22/64 loss: -0.04117119312286377
Batch 23/64 loss: -0.0654306411743164
Batch 24/64 loss: -0.044798076152801514
Batch 25/64 loss: -0.030038952827453613
Batch 26/64 loss: -0.05218851566314697
Batch 27/64 loss: -0.051774024963378906
Batch 28/64 loss: -0.0681273341178894
Batch 29/64 loss: -0.04797482490539551
Batch 30/64 loss: -0.03371107578277588
Batch 31/64 loss: -0.052859723567962646
Batch 32/64 loss: -0.04843395948410034
Batch 33/64 loss: -0.048973798751831055
Batch 34/64 loss: -0.0319787859916687
Batch 35/64 loss: -0.030184030532836914
Batch 36/64 loss: 0.016229987144470215
Batch 37/64 loss: -0.058528244495391846
Batch 38/64 loss: -0.054782092571258545
Batch 39/64 loss: -0.048034727573394775
Batch 40/64 loss: -0.03629779815673828
Batch 41/64 loss: -0.052357256412506104
Batch 42/64 loss: -0.013894081115722656
Batch 43/64 loss: -0.06612229347229004
Batch 44/64 loss: -0.05984795093536377
Batch 45/64 loss: -0.026160001754760742
Batch 46/64 loss: -0.06797832250595093
Batch 47/64 loss: -0.040543437004089355
Batch 48/64 loss: -0.045611441135406494
Batch 49/64 loss: -0.016185402870178223
Batch 50/64 loss: -0.01809835433959961
Batch 51/64 loss: -0.044585466384887695
Batch 52/64 loss: -0.03382652997970581
Batch 53/64 loss: -0.04271972179412842
Batch 54/64 loss: -0.05444896221160889
Batch 55/64 loss: -0.02927231788635254
Batch 56/64 loss: -0.05061447620391846
Batch 57/64 loss: -0.036621928215026855
Batch 58/64 loss: -0.06064826250076294
Batch 59/64 loss: -0.04815024137496948
Batch 60/64 loss: -0.0320202112197876
Batch 61/64 loss: -0.048521220684051514
Batch 62/64 loss: -0.035065531730651855
Batch 63/64 loss: -0.052341341972351074
Batch 64/64 loss: -0.036917150020599365
Epoch 186  Train loss: -0.04432755194458307  Val loss: 0.04613700130141478
Epoch 187
-------------------------------
Batch 1/64 loss: -0.05031228065490723
Batch 2/64 loss: -0.037831902503967285
Batch 3/64 loss: -0.030726969242095947
Batch 4/64 loss: -0.0371326208114624
Batch 5/64 loss: -0.05326145887374878
Batch 6/64 loss: -0.05809378623962402
Batch 7/64 loss: -0.05794411897659302
Batch 8/64 loss: -0.026259422302246094
Batch 9/64 loss: -0.02417987585067749
Batch 10/64 loss: -0.040047645568847656
Batch 11/64 loss: -0.01227426528930664
Batch 12/64 loss: -0.032010555267333984
Batch 13/64 loss: -0.026446402072906494
Batch 14/64 loss: -0.07605910301208496
Batch 15/64 loss: -0.039556264877319336
Batch 16/64 loss: -0.04540657997131348
Batch 17/64 loss: -0.05798017978668213
Batch 18/64 loss: -0.04781132936477661
Batch 19/64 loss: -0.016214489936828613
Batch 20/64 loss: -0.02790379524230957
Batch 21/64 loss: -0.05543893575668335
Batch 22/64 loss: -0.05361676216125488
Batch 23/64 loss: -0.054563283920288086
Batch 24/64 loss: -0.05302995443344116
Batch 25/64 loss: -0.05409574508666992
Batch 26/64 loss: -0.06492739915847778
Batch 27/64 loss: -0.04426330327987671
Batch 28/64 loss: -0.04260975122451782
Batch 29/64 loss: -0.04861140251159668
Batch 30/64 loss: -0.04179567098617554
Batch 31/64 loss: -0.05106031894683838
Batch 32/64 loss: -0.06873530149459839
Batch 33/64 loss: -0.051994502544403076
Batch 34/64 loss: -0.052188754081726074
Batch 35/64 loss: -0.030314147472381592
Batch 36/64 loss: -0.038845062255859375
Batch 37/64 loss: -0.06919503211975098
Batch 38/64 loss: -0.06555497646331787
Batch 39/64 loss: -0.03523409366607666
Batch 40/64 loss: -0.037584125995635986
Batch 41/64 loss: -0.03520071506500244
Batch 42/64 loss: -0.031435370445251465
Batch 43/64 loss: -0.058576226234436035
Batch 44/64 loss: -0.049449801445007324
Batch 45/64 loss: -0.05340111255645752
Batch 46/64 loss: -0.06340348720550537
Batch 47/64 loss: -0.055050551891326904
Batch 48/64 loss: -0.04234731197357178
Batch 49/64 loss: -0.0328676700592041
Batch 50/64 loss: -0.05135619640350342
Batch 51/64 loss: -0.017336606979370117
Batch 52/64 loss: -0.0887761116027832
Batch 53/64 loss: -0.06591331958770752
Batch 54/64 loss: -0.04776787757873535
Batch 55/64 loss: -0.06733262538909912
Batch 56/64 loss: -0.018983900547027588
Batch 57/64 loss: -0.06847834587097168
Batch 58/64 loss: -0.05784040689468384
Batch 59/64 loss: -0.04854089021682739
Batch 60/64 loss: -0.026308059692382812
Batch 61/64 loss: -0.039034366607666016
Batch 62/64 loss: -0.04011273384094238
Batch 63/64 loss: -0.02575969696044922
Batch 64/64 loss: -0.0362817645072937
Epoch 187  Train loss: -0.04586033376992917  Val loss: 0.043443372773960286
Epoch 188
-------------------------------
Batch 1/64 loss: -0.0430794358253479
Batch 2/64 loss: -0.06835448741912842
Batch 3/64 loss: -0.03734481334686279
Batch 4/64 loss: -0.03724426031112671
Batch 5/64 loss: -0.05654418468475342
Batch 6/64 loss: -0.04191935062408447
Batch 7/64 loss: -0.052014708518981934
Batch 8/64 loss: -0.026566922664642334
Batch 9/64 loss: -0.035284578800201416
Batch 10/64 loss: -0.06478947401046753
Batch 11/64 loss: -0.0684581995010376
Batch 12/64 loss: -0.045439064502716064
Batch 13/64 loss: -0.03420168161392212
Batch 14/64 loss: -0.06607043743133545
Batch 15/64 loss: -0.031732916831970215
Batch 16/64 loss: -0.05027437210083008
Batch 17/64 loss: -0.03590524196624756
Batch 18/64 loss: -0.04682052135467529
Batch 19/64 loss: -0.05409562587738037
Batch 20/64 loss: -0.05385953187942505
Batch 21/64 loss: -0.031331419944763184
Batch 22/64 loss: -0.052280426025390625
Batch 23/64 loss: -0.05571967363357544
Batch 24/64 loss: -0.07350492477416992
Batch 25/64 loss: -0.04374963045120239
Batch 26/64 loss: -0.040051817893981934
Batch 27/64 loss: -0.05152249336242676
Batch 28/64 loss: -0.065513014793396
Batch 29/64 loss: -0.05520594120025635
Batch 30/64 loss: -0.00471419095993042
Batch 31/64 loss: -0.06391727924346924
Batch 32/64 loss: -0.05588185787200928
Batch 33/64 loss: -0.06808912754058838
Batch 34/64 loss: -0.05373811721801758
Batch 35/64 loss: -0.04446244239807129
Batch 36/64 loss: -0.05580449104309082
Batch 37/64 loss: -0.03362172842025757
Batch 38/64 loss: -0.0665234923362732
Batch 39/64 loss: -0.03893977403640747
Batch 40/64 loss: -0.05902892351150513
Batch 41/64 loss: -0.050182998180389404
Batch 42/64 loss: -0.04978621006011963
Batch 43/64 loss: -0.016196906566619873
Batch 44/64 loss: -0.061000704765319824
Batch 45/64 loss: -0.02981865406036377
Batch 46/64 loss: -0.05489146709442139
Batch 47/64 loss: -0.043874382972717285
Batch 48/64 loss: -0.024909615516662598
Batch 49/64 loss: -0.04324054718017578
Batch 50/64 loss: -0.05704760551452637
Batch 51/64 loss: -0.04647564888000488
Batch 52/64 loss: -0.06188744306564331
Batch 53/64 loss: -0.03145378828048706
Batch 54/64 loss: -0.028218209743499756
Batch 55/64 loss: -0.03864330053329468
Batch 56/64 loss: -0.0357893705368042
Batch 57/64 loss: -0.043797194957733154
Batch 58/64 loss: -0.0509413480758667
Batch 59/64 loss: -0.04974585771560669
Batch 60/64 loss: -0.04961979389190674
Batch 61/64 loss: -0.019379615783691406
Batch 62/64 loss: -0.04018980264663696
Batch 63/64 loss: -0.05504190921783447
Batch 64/64 loss: -0.06190413236618042
Epoch 188  Train loss: -0.04693585961472754  Val loss: 0.04583158710158568
Epoch 189
-------------------------------
Batch 1/64 loss: -0.04181098937988281
Batch 2/64 loss: -0.05577659606933594
Batch 3/64 loss: -0.013943076133728027
Batch 4/64 loss: -0.04235714673995972
Batch 5/64 loss: -0.06810367107391357
Batch 6/64 loss: -0.044997453689575195
Batch 7/64 loss: -0.035009682178497314
Batch 8/64 loss: -0.044744253158569336
Batch 9/64 loss: -0.05189412832260132
Batch 10/64 loss: -0.06369304656982422
Batch 11/64 loss: -0.04428136348724365
Batch 12/64 loss: -0.040405988693237305
Batch 13/64 loss: -0.040478646755218506
Batch 14/64 loss: -0.03559279441833496
Batch 15/64 loss: -0.047928810119628906
Batch 16/64 loss: -0.04819077253341675
Batch 17/64 loss: -0.0641409158706665
Batch 18/64 loss: -0.046011149883270264
Batch 19/64 loss: -0.06323850154876709
Batch 20/64 loss: -0.0752750039100647
Batch 21/64 loss: -0.055974721908569336
Batch 22/64 loss: -0.04574102163314819
Batch 23/64 loss: -0.03563123941421509
Batch 24/64 loss: -0.06861376762390137
Batch 25/64 loss: -0.0384177565574646
Batch 26/64 loss: -0.0594099760055542
Batch 27/64 loss: -0.02932286262512207
Batch 28/64 loss: -0.05462014675140381
Batch 29/64 loss: -0.03227794170379639
Batch 30/64 loss: -0.016966044902801514
Batch 31/64 loss: -0.05333501100540161
Batch 32/64 loss: -0.04947352409362793
Batch 33/64 loss: -0.03560078144073486
Batch 34/64 loss: -0.04839247465133667
Batch 35/64 loss: -0.046672523021698
Batch 36/64 loss: -0.035089969635009766
Batch 37/64 loss: -0.047657012939453125
Batch 38/64 loss: -0.05894327163696289
Batch 39/64 loss: -0.026146650314331055
Batch 40/64 loss: -0.028585970401763916
Batch 41/64 loss: -0.05058026313781738
Batch 42/64 loss: -0.050708651542663574
Batch 43/64 loss: -0.04157763719558716
Batch 44/64 loss: -0.05923616886138916
Batch 45/64 loss: -0.03557717800140381
Batch 46/64 loss: -0.05784499645233154
Batch 47/64 loss: -0.04281705617904663
Batch 48/64 loss: -0.0182340145111084
Batch 49/64 loss: -0.03960782289505005
Batch 50/64 loss: -0.035514891147613525
Batch 51/64 loss: -0.020154178142547607
Batch 52/64 loss: -0.041136085987091064
Batch 53/64 loss: -0.031023025512695312
Batch 54/64 loss: -0.03798210620880127
Batch 55/64 loss: -0.049317359924316406
Batch 56/64 loss: -0.039757609367370605
Batch 57/64 loss: -0.05311894416809082
Batch 58/64 loss: -0.033890485763549805
Batch 59/64 loss: -0.05126690864562988
Batch 60/64 loss: -0.0637427568435669
Batch 61/64 loss: -0.0400160551071167
Batch 62/64 loss: -0.06178969144821167
Batch 63/64 loss: -0.054925620555877686
Batch 64/64 loss: -0.0621066689491272
Epoch 189  Train loss: -0.04535131244098439  Val loss: 0.04823735550916482
Epoch 190
-------------------------------
Batch 1/64 loss: -0.04039973020553589
Batch 2/64 loss: -0.05455970764160156
Batch 3/64 loss: -0.048792243003845215
Batch 4/64 loss: -0.0406796932220459
Batch 5/64 loss: -0.044165074825286865
Batch 6/64 loss: -0.056098341941833496
Batch 7/64 loss: -0.0532534122467041
Batch 8/64 loss: -0.047443926334381104
Batch 9/64 loss: -0.04355138540267944
Batch 10/64 loss: -0.01231616735458374
Batch 11/64 loss: -0.05050027370452881
Batch 12/64 loss: -0.016139626502990723
Batch 13/64 loss: -0.03608691692352295
Batch 14/64 loss: -0.043152689933776855
Batch 15/64 loss: -0.0658419132232666
Batch 16/64 loss: -0.06564617156982422
Batch 17/64 loss: -0.05015277862548828
Batch 18/64 loss: -0.04569816589355469
Batch 19/64 loss: -0.04579800367355347
Batch 20/64 loss: -0.03305625915527344
Batch 21/64 loss: -0.05646759271621704
Batch 22/64 loss: -0.059380531311035156
Batch 23/64 loss: -0.04121553897857666
Batch 24/64 loss: -0.029116153717041016
Batch 25/64 loss: -0.0643661618232727
Batch 26/64 loss: -0.03878295421600342
Batch 27/64 loss: -0.0390700101852417
Batch 28/64 loss: -0.04415649175643921
Batch 29/64 loss: -0.05010443925857544
Batch 30/64 loss: -0.0590510368347168
Batch 31/64 loss: -0.053018927574157715
Batch 32/64 loss: -0.04832017421722412
Batch 33/64 loss: -0.06257736682891846
Batch 34/64 loss: -0.03509718179702759
Batch 35/64 loss: -0.048221707344055176
Batch 36/64 loss: -0.0377269983291626
Batch 37/64 loss: -0.03883516788482666
Batch 38/64 loss: -0.048746585845947266
Batch 39/64 loss: -0.05473661422729492
Batch 40/64 loss: -0.053975820541381836
Batch 41/64 loss: -0.0675082802772522
Batch 42/64 loss: -0.032729923725128174
Batch 43/64 loss: -0.053215205669403076
Batch 44/64 loss: -0.07205486297607422
Batch 45/64 loss: -0.02668130397796631
Batch 46/64 loss: -0.042833566665649414
Batch 47/64 loss: -0.06706535816192627
Batch 48/64 loss: -0.024174928665161133
Batch 49/64 loss: -0.019656002521514893
Batch 50/64 loss: -0.057946085929870605
Batch 51/64 loss: -0.049175143241882324
Batch 52/64 loss: -0.04468876123428345
Batch 53/64 loss: -0.05790114402770996
Batch 54/64 loss: -0.03545016050338745
Batch 55/64 loss: -0.018728554248809814
Batch 56/64 loss: -0.022088706493377686
Batch 57/64 loss: -0.05511486530303955
Batch 58/64 loss: -0.03292036056518555
Batch 59/64 loss: -0.03712821006774902
Batch 60/64 loss: -0.031645357608795166
Batch 61/64 loss: -0.04020315408706665
Batch 62/64 loss: -0.052728116512298584
Batch 63/64 loss: -0.04557842016220093
Batch 64/64 loss: -0.027519822120666504
Epoch 190  Train loss: -0.04492794158411961  Val loss: 0.04915997314289263
Epoch 191
-------------------------------
Batch 1/64 loss: -0.05608105659484863
Batch 2/64 loss: -0.0652661919593811
Batch 3/64 loss: -0.0428767204284668
Batch 4/64 loss: -0.0395359992980957
Batch 5/64 loss: -0.05415689945220947
Batch 6/64 loss: -0.048095762729644775
Batch 7/64 loss: -0.04672640562057495
Batch 8/64 loss: -0.06694698333740234
Batch 9/64 loss: -0.04867565631866455
Batch 10/64 loss: -0.035668373107910156
Batch 11/64 loss: -0.0643923282623291
Batch 12/64 loss: -0.03619962930679321
Batch 13/64 loss: -0.055561184883117676
Batch 14/64 loss: -0.04652273654937744
Batch 15/64 loss: -0.0656934380531311
Batch 16/64 loss: -0.04295116662979126
Batch 17/64 loss: -0.01502162218093872
Batch 18/64 loss: -0.0453067421913147
Batch 19/64 loss: -0.05212831497192383
Batch 20/64 loss: -0.04040437936782837
Batch 21/64 loss: -0.06401759386062622
Batch 22/64 loss: -0.041980743408203125
Batch 23/64 loss: -0.02721649408340454
Batch 24/64 loss: -0.049781978130340576
Batch 25/64 loss: -0.048890113830566406
Batch 26/64 loss: -0.0506097674369812
Batch 27/64 loss: -0.061979055404663086
Batch 28/64 loss: -0.04413074254989624
Batch 29/64 loss: -0.038769662380218506
Batch 30/64 loss: -0.048162102699279785
Batch 31/64 loss: -0.034035444259643555
Batch 32/64 loss: -0.0013313889503479004
Batch 33/64 loss: -0.03593510389328003
Batch 34/64 loss: -0.04877817630767822
Batch 35/64 loss: -0.05110001564025879
Batch 36/64 loss: -0.05192089080810547
Batch 37/64 loss: -0.04686051607131958
Batch 38/64 loss: -0.0721137523651123
Batch 39/64 loss: -0.04115700721740723
Batch 40/64 loss: -0.03973078727722168
Batch 41/64 loss: -0.06309080123901367
Batch 42/64 loss: -0.05761706829071045
Batch 43/64 loss: -0.05917191505432129
Batch 44/64 loss: -0.05345594882965088
Batch 45/64 loss: -0.01336526870727539
Batch 46/64 loss: -0.03309214115142822
Batch 47/64 loss: -0.04206317663192749
Batch 48/64 loss: -0.04495656490325928
Batch 49/64 loss: -0.050902724266052246
Batch 50/64 loss: -0.03506767749786377
Batch 51/64 loss: -0.05029034614562988
Batch 52/64 loss: -0.05785435438156128
Batch 53/64 loss: -0.029214322566986084
Batch 54/64 loss: -0.03943431377410889
Batch 55/64 loss: -0.03230094909667969
Batch 56/64 loss: -0.04271417856216431
Batch 57/64 loss: -0.05972486734390259
Batch 58/64 loss: -0.04294097423553467
Batch 59/64 loss: -0.04457765817642212
Batch 60/64 loss: -0.054570555686950684
Batch 61/64 loss: -0.02897191047668457
Batch 62/64 loss: -0.052276670932769775
Batch 63/64 loss: -0.024078845977783203
Batch 64/64 loss: -0.03016209602355957
Epoch 191  Train loss: -0.045506944843367036  Val loss: 0.0445019416383042
Epoch 192
-------------------------------
Batch 1/64 loss: -0.029835999011993408
Batch 2/64 loss: -0.07393813133239746
Batch 3/64 loss: -0.04139256477355957
Batch 4/64 loss: -0.030176401138305664
Batch 5/64 loss: -0.07383954524993896
Batch 6/64 loss: -0.044225335121154785
Batch 7/64 loss: -0.04437243938446045
Batch 8/64 loss: -0.03961271047592163
Batch 9/64 loss: -0.01590806245803833
Batch 10/64 loss: -0.07082194089889526
Batch 11/64 loss: -0.05879789590835571
Batch 12/64 loss: -0.03481823205947876
Batch 13/64 loss: -0.05123341083526611
Batch 14/64 loss: -0.04386448860168457
Batch 15/64 loss: -0.05893200635910034
Batch 16/64 loss: -0.03829425573348999
Batch 17/64 loss: -0.033968210220336914
Batch 18/64 loss: -0.03808969259262085
Batch 19/64 loss: -0.06360745429992676
Batch 20/64 loss: -0.04605531692504883
Batch 21/64 loss: -0.06785547733306885
Batch 22/64 loss: -0.06962168216705322
Batch 23/64 loss: -0.07374507188796997
Batch 24/64 loss: -0.0317387580871582
Batch 25/64 loss: -0.043097496032714844
Batch 26/64 loss: -0.03014904260635376
Batch 27/64 loss: -0.04645228385925293
Batch 28/64 loss: -0.04409945011138916
Batch 29/64 loss: -0.04451775550842285
Batch 30/64 loss: -0.04332268238067627
Batch 31/64 loss: -0.03706115484237671
Batch 32/64 loss: -0.06349313259124756
Batch 33/64 loss: -0.040632009506225586
Batch 34/64 loss: -0.043669283390045166
Batch 35/64 loss: -0.02178800106048584
Batch 36/64 loss: -0.07120639085769653
Batch 37/64 loss: -0.05331099033355713
Batch 38/64 loss: -0.0614585280418396
Batch 39/64 loss: -0.03036355972290039
Batch 40/64 loss: -0.06423574686050415
Batch 41/64 loss: -0.04352688789367676
Batch 42/64 loss: -0.02288675308227539
Batch 43/64 loss: -0.03303182125091553
Batch 44/64 loss: -0.06593388319015503
Batch 45/64 loss: -0.05698883533477783
Batch 46/64 loss: -0.01679253578186035
Batch 47/64 loss: -0.057964205741882324
Batch 48/64 loss: -0.023834526538848877
Batch 49/64 loss: -0.041960060596466064
Batch 50/64 loss: -0.05159074068069458
Batch 51/64 loss: -0.04298168420791626
Batch 52/64 loss: -0.06220424175262451
Batch 53/64 loss: -0.05489301681518555
Batch 54/64 loss: -0.01433718204498291
Batch 55/64 loss: -0.04326784610748291
Batch 56/64 loss: -0.05097156763076782
Batch 57/64 loss: -0.048896610736846924
Batch 58/64 loss: -0.05651259422302246
Batch 59/64 loss: -0.044081807136535645
Batch 60/64 loss: -0.04555511474609375
Batch 61/64 loss: -0.037486732006073
Batch 62/64 loss: -0.050571322441101074
Batch 63/64 loss: -0.04891926050186157
Batch 64/64 loss: -0.03404033184051514
Epoch 192  Train loss: -0.0463423069785623  Val loss: 0.05062638033706298
Epoch 193
-------------------------------
Batch 1/64 loss: -0.022902846336364746
Batch 2/64 loss: -0.009281635284423828
Batch 3/64 loss: -0.028508901596069336
Batch 4/64 loss: -0.05481010675430298
Batch 5/64 loss: -0.04299628734588623
Batch 6/64 loss: -0.05228686332702637
Batch 7/64 loss: -0.04483437538146973
Batch 8/64 loss: -0.07961130142211914
Batch 9/64 loss: -0.06192415952682495
Batch 10/64 loss: -0.05317908525466919
Batch 11/64 loss: -0.0650792121887207
Batch 12/64 loss: -0.056639790534973145
Batch 13/64 loss: -0.05290937423706055
Batch 14/64 loss: -0.056633830070495605
Batch 15/64 loss: -0.06605768203735352
Batch 16/64 loss: -0.057837069034576416
Batch 17/64 loss: -0.05995035171508789
Batch 18/64 loss: -0.0377199649810791
Batch 19/64 loss: -0.03911769390106201
Batch 20/64 loss: -0.06529629230499268
Batch 21/64 loss: -0.04128587245941162
Batch 22/64 loss: -0.04038459062576294
Batch 23/64 loss: -0.044917285442352295
Batch 24/64 loss: -0.037175655364990234
Batch 25/64 loss: -0.06346070766448975
Batch 26/64 loss: -0.07662010192871094
Batch 27/64 loss: -0.05193144083023071
Batch 28/64 loss: -0.05650436878204346
Batch 29/64 loss: -0.028886258602142334
Batch 30/64 loss: -0.04955524206161499
Batch 31/64 loss: -0.04897964000701904
Batch 32/64 loss: -0.04142183065414429
Batch 33/64 loss: -0.04877114295959473
Batch 34/64 loss: -0.05881470441818237
Batch 35/64 loss: -0.060619473457336426
Batch 36/64 loss: -0.034100234508514404
Batch 37/64 loss: -0.0603564977645874
Batch 38/64 loss: -0.04225754737854004
Batch 39/64 loss: -0.06888741254806519
Batch 40/64 loss: -0.056869447231292725
Batch 41/64 loss: -0.06453007459640503
Batch 42/64 loss: -0.04226112365722656
Batch 43/64 loss: -0.04263252019882202
Batch 44/64 loss: -0.02694118022918701
Batch 45/64 loss: -0.04580056667327881
Batch 46/64 loss: -0.01195824146270752
Batch 47/64 loss: -0.028841853141784668
Batch 48/64 loss: -0.027636170387268066
Batch 49/64 loss: -0.03503096103668213
Batch 50/64 loss: -0.047707557678222656
Batch 51/64 loss: -0.05835294723510742
Batch 52/64 loss: -0.03816324472427368
Batch 53/64 loss: -0.04621767997741699
Batch 54/64 loss: -0.056697845458984375
Batch 55/64 loss: -0.061908185482025146
Batch 56/64 loss: -0.03514289855957031
Batch 57/64 loss: -0.06493234634399414
Batch 58/64 loss: -0.05030500888824463
Batch 59/64 loss: -0.05035066604614258
Batch 60/64 loss: -0.033376097679138184
Batch 61/64 loss: -0.04716771841049194
Batch 62/64 loss: -0.037837326526641846
Batch 63/64 loss: -0.055749714374542236
Batch 64/64 loss: -0.05747842788696289
Epoch 193  Train loss: -0.04818865925657983  Val loss: 0.05039666362644471
Epoch 194
-------------------------------
Batch 1/64 loss: -0.04716891050338745
Batch 2/64 loss: -0.05926567316055298
Batch 3/64 loss: -0.05281132459640503
Batch 4/64 loss: -0.031443119049072266
Batch 5/64 loss: -0.04873812198638916
Batch 6/64 loss: -0.04459184408187866
Batch 7/64 loss: -0.07085210084915161
Batch 8/64 loss: -0.06391894817352295
Batch 9/64 loss: -0.05424010753631592
Batch 10/64 loss: -0.05644881725311279
Batch 11/64 loss: -0.04933232069015503
Batch 12/64 loss: -0.06332230567932129
Batch 13/64 loss: -0.05012977123260498
Batch 14/64 loss: -0.052143216133117676
Batch 15/64 loss: -0.03200113773345947
Batch 16/64 loss: -0.046198487281799316
Batch 17/64 loss: -0.062496304512023926
Batch 18/64 loss: -0.06673872470855713
Batch 19/64 loss: -0.039762020111083984
Batch 20/64 loss: -0.012535333633422852
Batch 21/64 loss: -0.03980743885040283
Batch 22/64 loss: -0.04444092512130737
Batch 23/64 loss: -0.0551682710647583
Batch 24/64 loss: -0.01928567886352539
Batch 25/64 loss: -0.052638351917266846
Batch 26/64 loss: -0.04844546318054199
Batch 27/64 loss: -0.035609662532806396
Batch 28/64 loss: -0.061010122299194336
Batch 29/64 loss: -0.059391915798187256
Batch 30/64 loss: -0.08508181571960449
Batch 31/64 loss: -0.05404365062713623
Batch 32/64 loss: -0.06089425086975098
Batch 33/64 loss: -0.06289595365524292
Batch 34/64 loss: -0.024721980094909668
Batch 35/64 loss: -0.06271350383758545
Batch 36/64 loss: -0.02401578426361084
Batch 37/64 loss: -0.055190205574035645
Batch 38/64 loss: -0.054960668087005615
Batch 39/64 loss: -0.0641716718673706
Batch 40/64 loss: -0.02445274591445923
Batch 41/64 loss: -0.05528664588928223
Batch 42/64 loss: -0.041408538818359375
Batch 43/64 loss: -0.05413156747817993
Batch 44/64 loss: -0.02337658405303955
Batch 45/64 loss: -0.04115092754364014
Batch 46/64 loss: -0.04510897397994995
Batch 47/64 loss: -0.04966151714324951
Batch 48/64 loss: -0.0564914345741272
Batch 49/64 loss: -0.05037569999694824
Batch 50/64 loss: -0.045904576778411865
Batch 51/64 loss: -0.03192049264907837
Batch 52/64 loss: -0.0690927505493164
Batch 53/64 loss: -0.04583090543746948
Batch 54/64 loss: -0.049328744411468506
Batch 55/64 loss: -0.013831675052642822
Batch 56/64 loss: -0.06751775741577148
Batch 57/64 loss: -0.058329999446868896
Batch 58/64 loss: -0.04170882701873779
Batch 59/64 loss: -0.0342748761177063
Batch 60/64 loss: -0.03515404462814331
Batch 61/64 loss: -0.042520105838775635
Batch 62/64 loss: -0.04540562629699707
Batch 63/64 loss: -0.030972778797149658
Batch 64/64 loss: -0.011654853820800781
Epoch 194  Train loss: -0.04753889944039139  Val loss: 0.043455397345356105
Epoch 195
-------------------------------
Batch 1/64 loss: -0.03257995843887329
Batch 2/64 loss: -0.04671436548233032
Batch 3/64 loss: -0.047848403453826904
Batch 4/64 loss: -0.036504149436950684
Batch 5/64 loss: -0.0567055344581604
Batch 6/64 loss: -0.06978827714920044
Batch 7/64 loss: -0.0699620246887207
Batch 8/64 loss: -0.04950308799743652
Batch 9/64 loss: -0.04294610023498535
Batch 10/64 loss: -0.05644506216049194
Batch 11/64 loss: -0.05535769462585449
Batch 12/64 loss: -0.0559382438659668
Batch 13/64 loss: -0.03260582685470581
Batch 14/64 loss: -0.03868412971496582
Batch 15/64 loss: -0.06826043128967285
Batch 16/64 loss: -0.0676921010017395
Batch 17/64 loss: -0.05803924798965454
Batch 18/64 loss: -0.0726240873336792
Batch 19/64 loss: -0.0720704197883606
Batch 20/64 loss: -0.06163579225540161
Batch 21/64 loss: -0.06641846895217896
Batch 22/64 loss: -0.04736131429672241
Batch 23/64 loss: -0.07002174854278564
Batch 24/64 loss: -0.044198811054229736
Batch 25/64 loss: -0.05220222473144531
Batch 26/64 loss: -0.05414879322052002
Batch 27/64 loss: -0.038113415241241455
Batch 28/64 loss: -0.0674857497215271
Batch 29/64 loss: -0.035835981369018555
Batch 30/64 loss: -0.057874977588653564
Batch 31/64 loss: -0.033796072006225586
Batch 32/64 loss: -0.06919169425964355
Batch 33/64 loss: -0.04673624038696289
Batch 34/64 loss: -0.03820490837097168
Batch 35/64 loss: -0.05705451965332031
Batch 36/64 loss: -0.07521164417266846
Batch 37/64 loss: -0.02940422296524048
Batch 38/64 loss: -0.058493852615356445
Batch 39/64 loss: -0.06505918502807617
Batch 40/64 loss: -0.05299019813537598
Batch 41/64 loss: -0.04564261436462402
Batch 42/64 loss: -0.035079896450042725
Batch 43/64 loss: -0.010030269622802734
Batch 44/64 loss: -0.040755271911621094
Batch 45/64 loss: -0.030688941478729248
Batch 46/64 loss: -0.07400155067443848
Batch 47/64 loss: -0.04417407512664795
Batch 48/64 loss: -0.05444324016571045
Batch 49/64 loss: -0.04006320238113403
Batch 50/64 loss: -0.05256575345993042
Batch 51/64 loss: -0.05616617202758789
Batch 52/64 loss: -0.0653800368309021
Batch 53/64 loss: -0.04736894369125366
Batch 54/64 loss: -0.048309147357940674
Batch 55/64 loss: -0.06716489791870117
Batch 56/64 loss: -0.06416600942611694
Batch 57/64 loss: -0.059479355812072754
Batch 58/64 loss: -0.0390128493309021
Batch 59/64 loss: -0.02439701557159424
Batch 60/64 loss: -0.0002905130386352539
Batch 61/64 loss: -0.03707611560821533
Batch 62/64 loss: -0.055533647537231445
Batch 63/64 loss: -0.04101979732513428
Batch 64/64 loss: -0.04914349317550659
Epoch 195  Train loss: -0.05053138662787045  Val loss: 0.04410796722595634
Epoch 196
-------------------------------
Batch 1/64 loss: -0.05401206016540527
Batch 2/64 loss: -0.07120591402053833
Batch 3/64 loss: -0.03790116310119629
Batch 4/64 loss: -0.05722153186798096
Batch 5/64 loss: -0.05010610818862915
Batch 6/64 loss: -0.03240078687667847
Batch 7/64 loss: -0.06329584121704102
Batch 8/64 loss: -0.05000990629196167
Batch 9/64 loss: -0.045217812061309814
Batch 10/64 loss: -0.07865512371063232
Batch 11/64 loss: -0.05523592233657837
Batch 12/64 loss: -0.03254646062850952
Batch 13/64 loss: -0.05253249406814575
Batch 14/64 loss: -0.06734836101531982
Batch 15/64 loss: -0.03862738609313965
Batch 16/64 loss: -0.035086214542388916
Batch 17/64 loss: -0.046280503273010254
Batch 18/64 loss: -0.004476308822631836
Batch 19/64 loss: -0.07019174098968506
Batch 20/64 loss: -0.059925079345703125
Batch 21/64 loss: -0.05358576774597168
Batch 22/64 loss: -0.05019509792327881
Batch 23/64 loss: -0.0704050064086914
Batch 24/64 loss: -0.034532248973846436
Batch 25/64 loss: -0.04970657825469971
Batch 26/64 loss: -0.07052946090698242
Batch 27/64 loss: -0.04803341627120972
Batch 28/64 loss: -0.057346999645233154
Batch 29/64 loss: -0.059911906719207764
Batch 30/64 loss: -0.043276190757751465
Batch 31/64 loss: -0.049091339111328125
Batch 32/64 loss: -0.05890810489654541
Batch 33/64 loss: -0.0384068489074707
Batch 34/64 loss: -0.05564284324645996
Batch 35/64 loss: -0.047135233879089355
Batch 36/64 loss: -0.04985851049423218
Batch 37/64 loss: -0.04304260015487671
Batch 38/64 loss: -0.04323697090148926
Batch 39/64 loss: -0.07572269439697266
Batch 40/64 loss: -0.06076294183731079
Batch 41/64 loss: -0.05961012840270996
Batch 42/64 loss: -0.030006766319274902
Batch 43/64 loss: -0.03735017776489258
Batch 44/64 loss: -0.04651772975921631
Batch 45/64 loss: -0.054006874561309814
Batch 46/64 loss: -0.051539599895477295
Batch 47/64 loss: -0.046627819538116455
Batch 48/64 loss: -0.050087690353393555
Batch 49/64 loss: -0.05004918575286865
Batch 50/64 loss: -0.05674630403518677
Batch 51/64 loss: -0.03744381666183472
Batch 52/64 loss: -0.053292810916900635
Batch 53/64 loss: -0.03437924385070801
Batch 54/64 loss: -0.047909438610076904
Batch 55/64 loss: -0.03048419952392578
Batch 56/64 loss: -0.05227196216583252
Batch 57/64 loss: -0.02752208709716797
Batch 58/64 loss: -0.03748679161071777
Batch 59/64 loss: -0.026401996612548828
Batch 60/64 loss: -0.05997908115386963
Batch 61/64 loss: -0.07777869701385498
Batch 62/64 loss: -0.05456221103668213
Batch 63/64 loss: -0.049675583839416504
Batch 64/64 loss: -0.07277601957321167
Epoch 196  Train loss: -0.05000658339145137  Val loss: 0.041840248296350954
Saving best model, epoch: 196
Epoch 197
-------------------------------
Batch 1/64 loss: -0.09189695119857788
Batch 2/64 loss: -0.03707396984100342
Batch 3/64 loss: -0.056618690490722656
Batch 4/64 loss: -0.05009239912033081
Batch 5/64 loss: -0.041169822216033936
Batch 6/64 loss: -0.0697329044342041
Batch 7/64 loss: -0.076255202293396
Batch 8/64 loss: -0.047008633613586426
Batch 9/64 loss: -0.06938409805297852
Batch 10/64 loss: -0.05054676532745361
Batch 11/64 loss: -0.039759159088134766
Batch 12/64 loss: -0.05073326826095581
Batch 13/64 loss: -0.06388568878173828
Batch 14/64 loss: -0.06328284740447998
Batch 15/64 loss: -0.04954618215560913
Batch 16/64 loss: -0.05237150192260742
Batch 17/64 loss: -0.059221625328063965
Batch 18/64 loss: -0.053528666496276855
Batch 19/64 loss: -0.07208168506622314
Batch 20/64 loss: -0.05414050817489624
Batch 21/64 loss: -0.05568206310272217
Batch 22/64 loss: -0.049906373023986816
Batch 23/64 loss: -0.03018587827682495
Batch 24/64 loss: -0.03247988224029541
Batch 25/64 loss: -0.03510528802871704
Batch 26/64 loss: -0.06662887334823608
Batch 27/64 loss: -0.04185599088668823
Batch 28/64 loss: -0.06973505020141602
Batch 29/64 loss: -0.055700480937957764
Batch 30/64 loss: -0.04591202735900879
Batch 31/64 loss: -0.0772860050201416
Batch 32/64 loss: -0.06672722101211548
Batch 33/64 loss: -0.05777478218078613
Batch 34/64 loss: -0.042946577072143555
Batch 35/64 loss: -0.035163164138793945
Batch 36/64 loss: -0.023001134395599365
Batch 37/64 loss: -0.032443344593048096
Batch 38/64 loss: -0.05721825361251831
Batch 39/64 loss: -0.056264281272888184
Batch 40/64 loss: -0.019658982753753662
Batch 41/64 loss: -0.061950087547302246
Batch 42/64 loss: -0.06715208292007446
Batch 43/64 loss: -0.07734942436218262
Batch 44/64 loss: -0.04425722360610962
Batch 45/64 loss: -0.05039060115814209
Batch 46/64 loss: -0.029236257076263428
Batch 47/64 loss: -0.05567294359207153
Batch 48/64 loss: -0.05419337749481201
Batch 49/64 loss: -0.05218738317489624
Batch 50/64 loss: -0.03320735692977905
Batch 51/64 loss: -0.0595242977142334
Batch 52/64 loss: -0.04876554012298584
Batch 53/64 loss: -0.04344189167022705
Batch 54/64 loss: -0.06542468070983887
Batch 55/64 loss: -0.045035600662231445
Batch 56/64 loss: -0.06224977970123291
Batch 57/64 loss: -0.04771411418914795
Batch 58/64 loss: -0.05656301975250244
Batch 59/64 loss: -0.049125492572784424
Batch 60/64 loss: -0.021440327167510986
Batch 61/64 loss: -0.07269465923309326
Batch 62/64 loss: -0.04881000518798828
Batch 63/64 loss: -0.024061381816864014
Batch 64/64 loss: -0.012387514114379883
Epoch 197  Train loss: -0.051446970771340764  Val loss: 0.04782126019500785
Epoch 198
-------------------------------
Batch 1/64 loss: -0.053802311420440674
Batch 2/64 loss: -0.05684906244277954
Batch 3/64 loss: -0.0701722502708435
Batch 4/64 loss: -0.05936145782470703
Batch 5/64 loss: -0.0456470251083374
Batch 6/64 loss: -0.045038819313049316
Batch 7/64 loss: -0.029939591884613037
Batch 8/64 loss: -0.06305849552154541
Batch 9/64 loss: -0.04972708225250244
Batch 10/64 loss: -0.03594160079956055
Batch 11/64 loss: -0.04395949840545654
Batch 12/64 loss: -0.03508216142654419
Batch 13/64 loss: -0.04252469539642334
Batch 14/64 loss: -0.07355606555938721
Batch 15/64 loss: -0.03889232873916626
Batch 16/64 loss: -0.03827929496765137
Batch 17/64 loss: -0.05752396583557129
Batch 18/64 loss: -0.013421714305877686
Batch 19/64 loss: -0.04887479543685913
Batch 20/64 loss: -0.055467307567596436
Batch 21/64 loss: -0.05064207315444946
Batch 22/64 loss: -0.05793774127960205
Batch 23/64 loss: -0.05056929588317871
Batch 24/64 loss: -0.06216639280319214
Batch 25/64 loss: -0.05896902084350586
Batch 26/64 loss: -0.059115052223205566
Batch 27/64 loss: -0.06253451108932495
Batch 28/64 loss: -0.04551500082015991
Batch 29/64 loss: -0.07908093929290771
Batch 30/64 loss: -0.05531132221221924
Batch 31/64 loss: -0.02260512113571167
Batch 32/64 loss: -0.04975825548171997
Batch 33/64 loss: -0.05008399486541748
Batch 34/64 loss: -0.04336923360824585
Batch 35/64 loss: -0.06047189235687256
Batch 36/64 loss: -0.0462837815284729
Batch 37/64 loss: -0.039139628410339355
Batch 38/64 loss: -0.013598024845123291
Batch 39/64 loss: -0.027526021003723145
Batch 40/64 loss: -0.045008301734924316
Batch 41/64 loss: -0.03906530141830444
Batch 42/64 loss: -0.05804014205932617
Batch 43/64 loss: -0.060749948024749756
Batch 44/64 loss: -0.04040646553039551
Batch 45/64 loss: -0.06269615888595581
Batch 46/64 loss: -0.058962464332580566
Batch 47/64 loss: -0.05546975135803223
Batch 48/64 loss: -0.07180094718933105
Batch 49/64 loss: -0.03871774673461914
Batch 50/64 loss: -0.05441689491271973
Batch 51/64 loss: -0.059419989585876465
Batch 52/64 loss: -0.033351242542266846
Batch 53/64 loss: -0.040798962116241455
Batch 54/64 loss: -0.0552448034286499
Batch 55/64 loss: -0.02710139751434326
Batch 56/64 loss: -0.05023193359375
Batch 57/64 loss: -0.05226922035217285
Batch 58/64 loss: -0.05002140998840332
Batch 59/64 loss: -0.033545613288879395
Batch 60/64 loss: -0.043877243995666504
Batch 61/64 loss: -0.0460735559463501
Batch 62/64 loss: -0.020844340324401855
Batch 63/64 loss: -0.05777263641357422
Batch 64/64 loss: -0.07325869798660278
Epoch 198  Train loss: -0.04866866387572943  Val loss: 0.04393220336986162
Epoch 199
-------------------------------
Batch 1/64 loss: -0.028508424758911133
Batch 2/64 loss: -0.05919349193572998
Batch 3/64 loss: -0.05763334035873413
Batch 4/64 loss: -0.01932239532470703
Batch 5/64 loss: -0.05229461193084717
Batch 6/64 loss: -0.06666392087936401
Batch 7/64 loss: -0.046085894107818604
Batch 8/64 loss: -0.04473459720611572
Batch 9/64 loss: -0.05434596538543701
Batch 10/64 loss: -0.06412190198898315
Batch 11/64 loss: -0.06561362743377686
Batch 12/64 loss: -0.08946883678436279
Batch 13/64 loss: -0.06190776824951172
Batch 14/64 loss: -0.044835686683654785
Batch 15/64 loss: -0.05894869565963745
Batch 16/64 loss: -0.06534737348556519
Batch 17/64 loss: -0.07048040628433228
Batch 18/64 loss: -0.07454389333724976
Batch 19/64 loss: -0.04375797510147095
Batch 20/64 loss: -0.03668588399887085
Batch 21/64 loss: -0.0709223747253418
Batch 22/64 loss: -0.048261165618896484
Batch 23/64 loss: -0.06829315423965454
Batch 24/64 loss: -0.041285932064056396
Batch 25/64 loss: -0.03510504961013794
Batch 26/64 loss: -0.05391806364059448
Batch 27/64 loss: -0.04813039302825928
Batch 28/64 loss: -0.06985193490982056
Batch 29/64 loss: -0.06575500965118408
Batch 30/64 loss: -0.05479472875595093
Batch 31/64 loss: -0.061981260776519775
Batch 32/64 loss: -0.0702815055847168
Batch 33/64 loss: -0.055616915225982666
Batch 34/64 loss: -0.0257033109664917
Batch 35/64 loss: -0.046746253967285156
Batch 36/64 loss: -0.03056412935256958
Batch 37/64 loss: -0.05677837133407593
Batch 38/64 loss: -0.0725749135017395
Batch 39/64 loss: -0.054715514183044434
Batch 40/64 loss: -0.04421430826187134
Batch 41/64 loss: -0.04356485605239868
Batch 42/64 loss: -0.030382871627807617
Batch 43/64 loss: -0.07891887426376343
Batch 44/64 loss: -0.05665731430053711
Batch 45/64 loss: -0.03249019384384155
Batch 46/64 loss: -0.042829811573028564
Batch 47/64 loss: -0.053403258323669434
Batch 48/64 loss: -0.04773557186126709
Batch 49/64 loss: -0.06108379364013672
Batch 50/64 loss: -0.046776771545410156
Batch 51/64 loss: -0.0623631477355957
Batch 52/64 loss: -0.04893696308135986
Batch 53/64 loss: -0.04809772968292236
Batch 54/64 loss: -0.020228207111358643
Batch 55/64 loss: -0.05489981174468994
Batch 56/64 loss: -0.051743268966674805
Batch 57/64 loss: -0.041510164737701416
Batch 58/64 loss: -0.03734028339385986
Batch 59/64 loss: -0.052791714668273926
Batch 60/64 loss: -0.05973106622695923
Batch 61/64 loss: -0.054357171058654785
Batch 62/64 loss: -0.06307142972946167
Batch 63/64 loss: -0.06406712532043457
Batch 64/64 loss: -0.05393671989440918
Epoch 199  Train loss: -0.05291643797182569  Val loss: 0.04633314093363654
Epoch 200
-------------------------------
Batch 1/64 loss: -0.048670053482055664
Batch 2/64 loss: -0.05766528844833374
Batch 3/64 loss: -0.015291154384613037
Batch 4/64 loss: -0.062387943267822266
Batch 5/64 loss: -0.06859230995178223
Batch 6/64 loss: -0.04348486661911011
Batch 7/64 loss: -0.05394601821899414
Batch 8/64 loss: -0.06928378343582153
Batch 9/64 loss: -0.04655921459197998
Batch 10/64 loss: -0.049284160137176514
Batch 11/64 loss: -0.03922462463378906
Batch 12/64 loss: -0.05367743968963623
Batch 13/64 loss: -0.04626023769378662
Batch 14/64 loss: -0.05960726737976074
Batch 15/64 loss: -0.04083406925201416
Batch 16/64 loss: -0.03937935829162598
Batch 17/64 loss: -0.019578397274017334
Batch 18/64 loss: -0.0477524995803833
Batch 19/64 loss: -0.06743079423904419
Batch 20/64 loss: -0.04182076454162598
Batch 21/64 loss: -0.04564821720123291
Batch 22/64 loss: -0.03842568397521973
Batch 23/64 loss: -0.0492396354675293
Batch 24/64 loss: -0.06304198503494263
Batch 25/64 loss: -0.032765746116638184
Batch 26/64 loss: -0.0506361722946167
Batch 27/64 loss: -0.04868340492248535
Batch 28/64 loss: -0.06507962942123413
Batch 29/64 loss: -0.026777803897857666
Batch 30/64 loss: -0.058340251445770264
Batch 31/64 loss: -0.05790853500366211
Batch 32/64 loss: -0.07412248849868774
Batch 33/64 loss: -0.050012052059173584
Batch 34/64 loss: -0.06781512498855591
Batch 35/64 loss: -0.05794334411621094
Batch 36/64 loss: -0.05365931987762451
Batch 37/64 loss: -0.041117310523986816
Batch 38/64 loss: -0.07209587097167969
Batch 39/64 loss: -0.030824601650238037
Batch 40/64 loss: -0.06687730550765991
Batch 41/64 loss: -0.06650561094284058
Batch 42/64 loss: -0.04948914051055908
Batch 43/64 loss: -0.04699289798736572
Batch 44/64 loss: -0.039914488792419434
Batch 45/64 loss: -0.04374194145202637
Batch 46/64 loss: -0.02494502067565918
Batch 47/64 loss: -0.03769475221633911
Batch 48/64 loss: -0.05572783946990967
Batch 49/64 loss: -0.04736447334289551
Batch 50/64 loss: -0.06704050302505493
Batch 51/64 loss: -0.05667316913604736
Batch 52/64 loss: -0.07717877626419067
Batch 53/64 loss: -0.0624239444732666
Batch 54/64 loss: -0.03684842586517334
Batch 55/64 loss: -0.0563545823097229
Batch 56/64 loss: -0.07212710380554199
Batch 57/64 loss: -0.05741894245147705
Batch 58/64 loss: -0.0478631854057312
Batch 59/64 loss: -0.04154175519943237
Batch 60/64 loss: -0.06179821491241455
Batch 61/64 loss: -0.042356252670288086
Batch 62/64 loss: -0.07485699653625488
Batch 63/64 loss: -0.054146528244018555
Batch 64/64 loss: -0.05159711837768555
Epoch 200  Train loss: -0.05147368019702388  Val loss: 0.04236529004532857
Epoch 201
-------------------------------
Batch 1/64 loss: -0.05611294507980347
Batch 2/64 loss: -0.046502649784088135
Batch 3/64 loss: -0.07817625999450684
Batch 4/64 loss: -0.06479406356811523
Batch 5/64 loss: -0.03591442108154297
Batch 6/64 loss: -0.04931682348251343
Batch 7/64 loss: -0.049538493156433105
Batch 8/64 loss: -0.05687737464904785
Batch 9/64 loss: -0.06541800498962402
Batch 10/64 loss: -0.02774512767791748
Batch 11/64 loss: -0.05571335554122925
Batch 12/64 loss: -0.047521233558654785
Batch 13/64 loss: -0.0638379454612732
Batch 14/64 loss: -0.05153411626815796
Batch 15/64 loss: -0.07462531328201294
Batch 16/64 loss: -0.0741126537322998
Batch 17/64 loss: -0.04658156633377075
Batch 18/64 loss: -0.06806808710098267
Batch 19/64 loss: -0.057996392250061035
Batch 20/64 loss: -0.0507432222366333
Batch 21/64 loss: -0.07145392894744873
Batch 22/64 loss: -0.06276482343673706
Batch 23/64 loss: -0.07354497909545898
Batch 24/64 loss: -0.06955152750015259
Batch 25/64 loss: -0.03640949726104736
Batch 26/64 loss: -0.05613243579864502
Batch 27/64 loss: -0.05303239822387695
Batch 28/64 loss: -0.037093281745910645
Batch 29/64 loss: -0.06034582853317261
Batch 30/64 loss: -0.06541681289672852
Batch 31/64 loss: -0.03279048204421997
Batch 32/64 loss: -0.022096455097198486
Batch 33/64 loss: -0.03568392992019653
Batch 34/64 loss: -0.033470332622528076
Batch 35/64 loss: -0.02065330743789673
Batch 36/64 loss: -0.059372663497924805
Batch 37/64 loss: -0.03338003158569336
Batch 38/64 loss: -0.045794010162353516
Batch 39/64 loss: -0.06455612182617188
Batch 40/64 loss: -0.06745290756225586
Batch 41/64 loss: -0.04169100522994995
Batch 42/64 loss: -0.04543185234069824
Batch 43/64 loss: -0.0599672794342041
Batch 44/64 loss: -0.05037343502044678
Batch 45/64 loss: -0.03735780715942383
Batch 46/64 loss: -0.046160995960235596
Batch 47/64 loss: -0.03967726230621338
Batch 48/64 loss: -0.04175442457199097
Batch 49/64 loss: -0.059790074825286865
Batch 50/64 loss: -0.07459551095962524
Batch 51/64 loss: -0.03761047124862671
Batch 52/64 loss: -0.05695253610610962
Batch 53/64 loss: -0.057385146617889404
Batch 54/64 loss: -0.043019890785217285
Batch 55/64 loss: -0.045475661754608154
Batch 56/64 loss: -0.0425187349319458
Batch 57/64 loss: -0.04736047983169556
Batch 58/64 loss: -0.05920600891113281
Batch 59/64 loss: -0.04261672496795654
Batch 60/64 loss: -0.0563700795173645
Batch 61/64 loss: -0.05309617519378662
Batch 62/64 loss: -0.027384400367736816
Batch 63/64 loss: -0.06638163328170776
Batch 64/64 loss: -0.03339916467666626
Epoch 201  Train loss: -0.05144082775302962  Val loss: 0.045343268163425404
Epoch 202
-------------------------------
Batch 1/64 loss: -0.027849197387695312
Batch 2/64 loss: -0.03447115421295166
Batch 3/64 loss: -0.040361106395721436
Batch 4/64 loss: -0.055803775787353516
Batch 5/64 loss: -0.052795350551605225
Batch 6/64 loss: -0.05876624584197998
Batch 7/64 loss: -0.05936455726623535
Batch 8/64 loss: -0.06110560894012451
Batch 9/64 loss: -0.08190637826919556
Batch 10/64 loss: -0.06522762775421143
Batch 11/64 loss: -0.04860496520996094
Batch 12/64 loss: -0.06987267732620239
Batch 13/64 loss: -0.060532331466674805
Batch 14/64 loss: -0.050770103931427
Batch 15/64 loss: -0.0497516393661499
Batch 16/64 loss: -0.03210395574569702
Batch 17/64 loss: -0.06716430187225342
Batch 18/64 loss: -0.04761993885040283
Batch 19/64 loss: -0.06021338701248169
Batch 20/64 loss: -0.04438871145248413
Batch 21/64 loss: -0.056681156158447266
Batch 22/64 loss: -0.05362868309020996
Batch 23/64 loss: -0.043228209018707275
Batch 24/64 loss: -0.033963799476623535
Batch 25/64 loss: -0.042401909828186035
Batch 26/64 loss: -0.07190936803817749
Batch 27/64 loss: -0.041595280170440674
Batch 28/64 loss: -0.05642282962799072
Batch 29/64 loss: -0.036056458950042725
Batch 30/64 loss: -0.0782577395439148
Batch 31/64 loss: -0.06814360618591309
Batch 32/64 loss: -0.03649294376373291
Batch 33/64 loss: -0.038890302181243896
Batch 34/64 loss: -0.05062079429626465
Batch 35/64 loss: -0.032959699630737305
Batch 36/64 loss: -0.06185954809188843
Batch 37/64 loss: -0.06400877237319946
Batch 38/64 loss: -0.050819337368011475
Batch 39/64 loss: -0.05166923999786377
Batch 40/64 loss: -0.05806630849838257
Batch 41/64 loss: -0.05693185329437256
Batch 42/64 loss: -0.042842745780944824
Batch 43/64 loss: -0.053345322608947754
Batch 44/64 loss: -0.06370818614959717
Batch 45/64 loss: -0.038984596729278564
Batch 46/64 loss: -0.029156804084777832
Batch 47/64 loss: -0.03650104999542236
Batch 48/64 loss: -0.042124271392822266
Batch 49/64 loss: -0.042474985122680664
Batch 50/64 loss: -0.05126368999481201
Batch 51/64 loss: -0.05209052562713623
Batch 52/64 loss: -0.05659186840057373
Batch 53/64 loss: -0.0647587776184082
Batch 54/64 loss: -0.047766029834747314
Batch 55/64 loss: -0.07547175884246826
Batch 56/64 loss: -0.052619338035583496
Batch 57/64 loss: -0.031195461750030518
Batch 58/64 loss: -0.01896291971206665
Batch 59/64 loss: -0.029003381729125977
Batch 60/64 loss: -0.04917645454406738
Batch 61/64 loss: -0.04231745004653931
Batch 62/64 loss: -0.042223334312438965
Batch 63/64 loss: -0.06933629512786865
Batch 64/64 loss: -0.03366011381149292
Epoch 202  Train loss: -0.050359861757241046  Val loss: 0.047031236268400736
Epoch 203
-------------------------------
Batch 1/64 loss: -0.06934303045272827
Batch 2/64 loss: -0.051934242248535156
Batch 3/64 loss: -0.0710831880569458
Batch 4/64 loss: -0.04004091024398804
Batch 5/64 loss: -0.05270332098007202
Batch 6/64 loss: -0.06243431568145752
Batch 7/64 loss: -0.04958295822143555
Batch 8/64 loss: -0.05034792423248291
Batch 9/64 loss: -0.07732325792312622
Batch 10/64 loss: -0.04289257526397705
Batch 11/64 loss: -0.05491369962692261
Batch 12/64 loss: -0.031109213829040527
Batch 13/64 loss: -0.043710410594940186
Batch 14/64 loss: -0.06841570138931274
Batch 15/64 loss: -0.025485455989837646
Batch 16/64 loss: -0.05685555934906006
Batch 17/64 loss: -0.06099361181259155
Batch 18/64 loss: -0.052909016609191895
Batch 19/64 loss: -0.07584244012832642
Batch 20/64 loss: -0.050059497356414795
Batch 21/64 loss: -0.05814552307128906
Batch 22/64 loss: -0.06947505474090576
Batch 23/64 loss: -0.0566975474357605
Batch 24/64 loss: -0.05470609664916992
Batch 25/64 loss: -0.07856786251068115
Batch 26/64 loss: -0.07050812244415283
Batch 27/64 loss: -0.04634052515029907
Batch 28/64 loss: -0.04551374912261963
Batch 29/64 loss: -0.04605942964553833
Batch 30/64 loss: -0.027033567428588867
Batch 31/64 loss: -0.04465055465698242
Batch 32/64 loss: -0.057103753089904785
Batch 33/64 loss: -0.06653493642807007
Batch 34/64 loss: -0.04440122842788696
Batch 35/64 loss: -0.06766313314437866
Batch 36/64 loss: -0.04502594470977783
Batch 37/64 loss: -0.05869448184967041
Batch 38/64 loss: -0.06758779287338257
Batch 39/64 loss: -0.07580041885375977
Batch 40/64 loss: -0.05862069129943848
Batch 41/64 loss: -0.04501771926879883
Batch 42/64 loss: -0.06380730867385864
Batch 43/64 loss: -0.059967875480651855
Batch 44/64 loss: -0.044589877128601074
Batch 45/64 loss: -0.04778099060058594
Batch 46/64 loss: -0.044288456439971924
Batch 47/64 loss: -0.08153623342514038
Batch 48/64 loss: -0.05493849515914917
Batch 49/64 loss: -0.05934625864028931
Batch 50/64 loss: -0.05261051654815674
Batch 51/64 loss: -0.037311553955078125
Batch 52/64 loss: -0.04631805419921875
Batch 53/64 loss: -0.06257247924804688
Batch 54/64 loss: -0.06982159614562988
Batch 55/64 loss: -0.03605848550796509
Batch 56/64 loss: -0.05854618549346924
Batch 57/64 loss: -0.05044978857040405
Batch 58/64 loss: -0.057348549365997314
Batch 59/64 loss: -0.07092267274856567
Batch 60/64 loss: -0.06387722492218018
Batch 61/64 loss: -0.025104939937591553
Batch 62/64 loss: -0.048339664936065674
Batch 63/64 loss: -0.023151814937591553
Batch 64/64 loss: -0.03787881135940552
Epoch 203  Train loss: -0.05426237793529735  Val loss: 0.04763293040986733
Epoch 204
-------------------------------
Batch 1/64 loss: -0.09350395202636719
Batch 2/64 loss: -0.04033702611923218
Batch 3/64 loss: -0.04887795448303223
Batch 4/64 loss: -0.038599252700805664
Batch 5/64 loss: -0.03164076805114746
Batch 6/64 loss: -0.07548731565475464
Batch 7/64 loss: -0.03809309005737305
Batch 8/64 loss: -0.053171753883361816
Batch 9/64 loss: -0.0532149076461792
Batch 10/64 loss: -0.04827755689620972
Batch 11/64 loss: -0.046170592308044434
Batch 12/64 loss: -0.06110024452209473
Batch 13/64 loss: -0.05439406633377075
Batch 14/64 loss: -0.059272170066833496
Batch 15/64 loss: -0.08055007457733154
Batch 16/64 loss: -0.03541523218154907
Batch 17/64 loss: -0.06735342741012573
Batch 18/64 loss: -0.06573820114135742
Batch 19/64 loss: -0.03191721439361572
Batch 20/64 loss: -0.06899279356002808
Batch 21/64 loss: -0.07380527257919312
Batch 22/64 loss: -0.05152851343154907
Batch 23/64 loss: -0.03694051504135132
Batch 24/64 loss: -0.05203956365585327
Batch 25/64 loss: -0.06543654203414917
Batch 26/64 loss: -0.05305922031402588
Batch 27/64 loss: -0.05587202310562134
Batch 28/64 loss: -0.053803443908691406
Batch 29/64 loss: -0.06550627946853638
Batch 30/64 loss: -0.022543907165527344
Batch 31/64 loss: -0.060416340827941895
Batch 32/64 loss: -0.05673408508300781
Batch 33/64 loss: -0.036037564277648926
Batch 34/64 loss: -0.04689836502075195
Batch 35/64 loss: -0.067737877368927
Batch 36/64 loss: -0.027959764003753662
Batch 37/64 loss: -0.04094541072845459
Batch 38/64 loss: -0.05066639184951782
Batch 39/64 loss: -0.05604022741317749
Batch 40/64 loss: -0.03819429874420166
Batch 41/64 loss: -0.07666885852813721
Batch 42/64 loss: -0.056523680686950684
Batch 43/64 loss: -0.04817795753479004
Batch 44/64 loss: -0.06167256832122803
Batch 45/64 loss: -0.05686509609222412
Batch 46/64 loss: -0.06167513132095337
Batch 47/64 loss: -0.03907465934753418
Batch 48/64 loss: -0.03397184610366821
Batch 49/64 loss: -0.06769305467605591
Batch 50/64 loss: -0.041075825691223145
Batch 51/64 loss: -0.03648245334625244
Batch 52/64 loss: -0.046743035316467285
Batch 53/64 loss: -0.061724185943603516
Batch 54/64 loss: -0.04704725742340088
Batch 55/64 loss: -0.040520668029785156
Batch 56/64 loss: -0.04818427562713623
Batch 57/64 loss: -0.04517972469329834
Batch 58/64 loss: -0.045963406562805176
Batch 59/64 loss: -0.04311293363571167
Batch 60/64 loss: -0.062342286109924316
Batch 61/64 loss: -0.05093181133270264
Batch 62/64 loss: -0.06004399061203003
Batch 63/64 loss: -0.059943974018096924
Batch 64/64 loss: -0.0533602237701416
Epoch 204  Train loss: -0.052328032138300874  Val loss: 0.04525859044589538
Epoch 205
-------------------------------
Batch 1/64 loss: -0.05708390474319458
Batch 2/64 loss: -0.04523104429244995
Batch 3/64 loss: -0.07131516933441162
Batch 4/64 loss: -0.06329631805419922
Batch 5/64 loss: -0.04993337392807007
Batch 6/64 loss: -0.05639368295669556
Batch 7/64 loss: -0.04176592826843262
Batch 8/64 loss: -0.043766140937805176
Batch 9/64 loss: -0.07873541116714478
Batch 10/64 loss: -0.050075650215148926
Batch 11/64 loss: -0.029258251190185547
Batch 12/64 loss: -0.06791776418685913
Batch 13/64 loss: -0.05951756238937378
Batch 14/64 loss: -0.04729360342025757
Batch 15/64 loss: -0.05869168043136597
Batch 16/64 loss: -0.053578078746795654
Batch 17/64 loss: -0.04523998498916626
Batch 18/64 loss: -0.04432022571563721
Batch 19/64 loss: -0.04542344808578491
Batch 20/64 loss: -0.03289377689361572
Batch 21/64 loss: -0.05474746227264404
Batch 22/64 loss: -0.06899666786193848
Batch 23/64 loss: -0.07075619697570801
Batch 24/64 loss: -0.03932672739028931
Batch 25/64 loss: -0.07407152652740479
Batch 26/64 loss: -0.06548196077346802
Batch 27/64 loss: -0.06591296195983887
Batch 28/64 loss: -0.047262609004974365
Batch 29/64 loss: -0.06010007858276367
Batch 30/64 loss: -0.05446344614028931
Batch 31/64 loss: -0.06981748342514038
Batch 32/64 loss: -0.04126882553100586
Batch 33/64 loss: -0.05188089609146118
Batch 34/64 loss: -0.06328260898590088
Batch 35/64 loss: -0.04553169012069702
Batch 36/64 loss: -0.05635106563568115
Batch 37/64 loss: -0.028466343879699707
Batch 38/64 loss: -0.05477398633956909
Batch 39/64 loss: -0.051842570304870605
Batch 40/64 loss: -0.039177775382995605
Batch 41/64 loss: -0.06140494346618652
Batch 42/64 loss: -0.07742071151733398
Batch 43/64 loss: -0.06097602844238281
Batch 44/64 loss: -0.07901930809020996
Batch 45/64 loss: -0.06559634208679199
Batch 46/64 loss: -0.06952112913131714
Batch 47/64 loss: -0.06331455707550049
Batch 48/64 loss: -0.05892777442932129
Batch 49/64 loss: -0.038127899169921875
Batch 50/64 loss: -0.05336356163024902
Batch 51/64 loss: -0.06410205364227295
Batch 52/64 loss: -0.04389965534210205
Batch 53/64 loss: -0.06033360958099365
Batch 54/64 loss: -0.0624348521232605
Batch 55/64 loss: -0.0512315034866333
Batch 56/64 loss: -0.02150404453277588
Batch 57/64 loss: -0.061396002769470215
Batch 58/64 loss: -0.03769791126251221
Batch 59/64 loss: -0.0614737868309021
Batch 60/64 loss: -0.07362627983093262
Batch 61/64 loss: -0.059689104557037354
Batch 62/64 loss: -0.033493101596832275
Batch 63/64 loss: -0.025269150733947754
Batch 64/64 loss: -0.06223654747009277
Epoch 205  Train loss: -0.05452148400101007  Val loss: 0.04791353371544802
Epoch 206
-------------------------------
Batch 1/64 loss: -0.03353935480117798
Batch 2/64 loss: -0.045842111110687256
Batch 3/64 loss: -0.0677192211151123
Batch 4/64 loss: -0.07947123050689697
Batch 5/64 loss: -0.06602144241333008
Batch 6/64 loss: -0.059736669063568115
Batch 7/64 loss: -0.08048522472381592
Batch 8/64 loss: -0.07236391305923462
Batch 9/64 loss: -0.06322896480560303
Batch 10/64 loss: -0.051406025886535645
Batch 11/64 loss: -0.06393575668334961
Batch 12/64 loss: -0.011982500553131104
Batch 13/64 loss: -0.05693000555038452
Batch 14/64 loss: -0.0629623532295227
Batch 15/64 loss: -0.056255221366882324
Batch 16/64 loss: -0.03464788198471069
Batch 17/64 loss: -0.07122427225112915
Batch 18/64 loss: -0.07019758224487305
Batch 19/64 loss: -0.06131809949874878
Batch 20/64 loss: -0.0667925477027893
Batch 21/64 loss: -0.05689191818237305
Batch 22/64 loss: -0.025555729866027832
Batch 23/64 loss: -0.03201359510421753
Batch 24/64 loss: -0.07293379306793213
Batch 25/64 loss: -0.04110628366470337
Batch 26/64 loss: -0.038040339946746826
Batch 27/64 loss: -0.07479256391525269
Batch 28/64 loss: -0.03343844413757324
Batch 29/64 loss: -0.032710134983062744
Batch 30/64 loss: -0.047077953815460205
Batch 31/64 loss: -0.046353816986083984
Batch 32/64 loss: -0.05897468328475952
Batch 33/64 loss: -0.05340445041656494
Batch 34/64 loss: -0.06035435199737549
Batch 35/64 loss: -0.044735610485076904
Batch 36/64 loss: -0.056600749492645264
Batch 37/64 loss: -0.04831874370574951
Batch 38/64 loss: -0.051695048809051514
Batch 39/64 loss: -0.05564028024673462
Batch 40/64 loss: -0.07266789674758911
Batch 41/64 loss: -0.061721861362457275
Batch 42/64 loss: -0.04005134105682373
Batch 43/64 loss: -0.07255452871322632
Batch 44/64 loss: -0.059202730655670166
Batch 45/64 loss: -0.06142234802246094
Batch 46/64 loss: -0.06794339418411255
Batch 47/64 loss: -0.06813198328018188
Batch 48/64 loss: -0.07087182998657227
Batch 49/64 loss: -0.06891322135925293
Batch 50/64 loss: -0.04422497749328613
Batch 51/64 loss: -0.051434993743896484
Batch 52/64 loss: -0.06401926279067993
Batch 53/64 loss: -0.06783050298690796
Batch 54/64 loss: -0.035131633281707764
Batch 55/64 loss: -0.0499308705329895
Batch 56/64 loss: -0.05226093530654907
Batch 57/64 loss: -0.06774652004241943
Batch 58/64 loss: -0.04430609941482544
Batch 59/64 loss: -0.07475316524505615
Batch 60/64 loss: -0.07059335708618164
Batch 61/64 loss: -0.04275941848754883
Batch 62/64 loss: -0.038930535316467285
Batch 63/64 loss: -0.06050074100494385
Batch 64/64 loss: -0.04167526960372925
Epoch 206  Train loss: -0.055621325034721225  Val loss: 0.045896278009381904
Epoch 207
-------------------------------
Batch 1/64 loss: -0.04582482576370239
Batch 2/64 loss: -0.04864656925201416
Batch 3/64 loss: -0.06924700736999512
Batch 4/64 loss: -0.06364566087722778
Batch 5/64 loss: -0.06552433967590332
Batch 6/64 loss: -0.049580395221710205
Batch 7/64 loss: -0.03215444087982178
Batch 8/64 loss: -0.08431065082550049
Batch 9/64 loss: -0.08334821462631226
Batch 10/64 loss: -0.03278815746307373
Batch 11/64 loss: -0.06425493955612183
Batch 12/64 loss: -0.07209354639053345
Batch 13/64 loss: -0.09121674299240112
Batch 14/64 loss: -0.06961339712142944
Batch 15/64 loss: -0.05855846405029297
Batch 16/64 loss: -0.06600314378738403
Batch 17/64 loss: -0.06431466341018677
Batch 18/64 loss: -0.06577515602111816
Batch 19/64 loss: -0.028598129749298096
Batch 20/64 loss: -0.07699441909790039
Batch 21/64 loss: -0.06759065389633179
Batch 22/64 loss: -0.05478793382644653
Batch 23/64 loss: -0.06605511903762817
Batch 24/64 loss: -0.05641353130340576
Batch 25/64 loss: -0.04679977893829346
Batch 26/64 loss: -0.03902542591094971
Batch 27/64 loss: -0.03326272964477539
Batch 28/64 loss: -0.04046517610549927
Batch 29/64 loss: -0.03277856111526489
Batch 30/64 loss: -0.06539428234100342
Batch 31/64 loss: -0.0539013147354126
Batch 32/64 loss: -0.042414307594299316
Batch 33/64 loss: -0.05440247058868408
Batch 34/64 loss: -0.04625648260116577
Batch 35/64 loss: -0.044560909271240234
Batch 36/64 loss: -0.047167181968688965
Batch 37/64 loss: -0.0761176347732544
Batch 38/64 loss: -0.07234495878219604
Batch 39/64 loss: -0.05512440204620361
Batch 40/64 loss: -0.058555424213409424
Batch 41/64 loss: -0.047149479389190674
Batch 42/64 loss: -0.012572050094604492
Batch 43/64 loss: -0.051593899726867676
Batch 44/64 loss: -0.06770062446594238
Batch 45/64 loss: -0.05712515115737915
Batch 46/64 loss: -0.059027016162872314
Batch 47/64 loss: -0.05005401372909546
Batch 48/64 loss: -0.07225072383880615
Batch 49/64 loss: -0.039822936058044434
Batch 50/64 loss: -0.07647192478179932
Batch 51/64 loss: -0.0221329927444458
Batch 52/64 loss: -0.044017016887664795
Batch 53/64 loss: -0.05534881353378296
Batch 54/64 loss: -0.01368027925491333
Batch 55/64 loss: -0.058110952377319336
Batch 56/64 loss: -0.04763960838317871
Batch 57/64 loss: -0.05046367645263672
Batch 58/64 loss: -0.05495607852935791
Batch 59/64 loss: -0.03562110662460327
Batch 60/64 loss: -0.05569052696228027
Batch 61/64 loss: -0.053669095039367676
Batch 62/64 loss: -0.0630643367767334
Batch 63/64 loss: -0.08066880702972412
Batch 64/64 loss: -0.024779200553894043
Epoch 207  Train loss: -0.054483555344974295  Val loss: 0.047044498609103695
Epoch 208
-------------------------------
Batch 1/64 loss: -0.06522488594055176
Batch 2/64 loss: -0.07906085252761841
Batch 3/64 loss: -0.027290940284729004
Batch 4/64 loss: -0.04870295524597168
Batch 5/64 loss: -0.06187891960144043
Batch 6/64 loss: -0.0532604455947876
Batch 7/64 loss: -0.05396479368209839
Batch 8/64 loss: -0.03183484077453613
Batch 9/64 loss: -0.06221449375152588
Batch 10/64 loss: -0.02741342782974243
Batch 11/64 loss: -0.0402948260307312
Batch 12/64 loss: -0.059346556663513184
Batch 13/64 loss: -0.057639360427856445
Batch 14/64 loss: -0.04108923673629761
Batch 15/64 loss: -0.053659915924072266
Batch 16/64 loss: -0.07603186368942261
Batch 17/64 loss: -0.06366145610809326
Batch 18/64 loss: -0.04860544204711914
Batch 19/64 loss: -0.040993452072143555
Batch 20/64 loss: -0.05866819620132446
Batch 21/64 loss: -0.03265583515167236
Batch 22/64 loss: -0.039489805698394775
Batch 23/64 loss: -0.06221538782119751
Batch 24/64 loss: -0.025738656520843506
Batch 25/64 loss: -0.05256921052932739
Batch 26/64 loss: -0.056809425354003906
Batch 27/64 loss: -0.0498729944229126
Batch 28/64 loss: -0.06585633754730225
Batch 29/64 loss: -0.06291311979293823
Batch 30/64 loss: -0.049059927463531494
Batch 31/64 loss: -0.07237827777862549
Batch 32/64 loss: -0.05624973773956299
Batch 33/64 loss: -0.056945979595184326
Batch 34/64 loss: -0.03473317623138428
Batch 35/64 loss: -0.06918436288833618
Batch 36/64 loss: -0.056916236877441406
Batch 37/64 loss: -0.05965179204940796
Batch 38/64 loss: -0.06342530250549316
Batch 39/64 loss: -0.08122044801712036
Batch 40/64 loss: -0.028429090976715088
Batch 41/64 loss: -0.053286194801330566
Batch 42/64 loss: -0.06630510091781616
Batch 43/64 loss: -0.0742291808128357
Batch 44/64 loss: -0.05094707012176514
Batch 45/64 loss: -0.06202197074890137
Batch 46/64 loss: -0.06740444898605347
Batch 47/64 loss: -0.06513798236846924
Batch 48/64 loss: -0.06627321243286133
Batch 49/64 loss: -0.06255900859832764
Batch 50/64 loss: -0.0535544753074646
Batch 51/64 loss: -0.06764274835586548
Batch 52/64 loss: -0.04239046573638916
Batch 53/64 loss: -0.06167393922805786
Batch 54/64 loss: -0.06504607200622559
Batch 55/64 loss: -0.05480426549911499
Batch 56/64 loss: -0.05124109983444214
Batch 57/64 loss: -0.03600078821182251
Batch 58/64 loss: -0.060596585273742676
Batch 59/64 loss: -0.05200546979904175
Batch 60/64 loss: -0.08344566822052002
Batch 61/64 loss: -0.06727206707000732
Batch 62/64 loss: -0.07427752017974854
Batch 63/64 loss: -0.035028696060180664
Batch 64/64 loss: -0.054411470890045166
Epoch 208  Train loss: -0.055546738820917464  Val loss: 0.04393729004253637
Epoch 209
-------------------------------
Batch 1/64 loss: -0.0569685697555542
Batch 2/64 loss: -0.0729602575302124
Batch 3/64 loss: -0.06039237976074219
Batch 4/64 loss: -0.07730764150619507
Batch 5/64 loss: -0.07755565643310547
Batch 6/64 loss: -0.08212006092071533
Batch 7/64 loss: -0.031079113483428955
Batch 8/64 loss: -0.04227405786514282
Batch 9/64 loss: -0.08205258846282959
Batch 10/64 loss: -0.06468778848648071
Batch 11/64 loss: -0.07168066501617432
Batch 12/64 loss: -0.06330227851867676
Batch 13/64 loss: -0.06887084245681763
Batch 14/64 loss: -0.0571860671043396
Batch 15/64 loss: -0.06483191251754761
Batch 16/64 loss: -0.04442673921585083
Batch 17/64 loss: -0.07162141799926758
Batch 18/64 loss: -0.02329474687576294
Batch 19/64 loss: -0.029226601123809814
Batch 20/64 loss: -0.060839951038360596
Batch 21/64 loss: -0.035996437072753906
Batch 22/64 loss: -0.05535531044006348
Batch 23/64 loss: -0.06032019853591919
Batch 24/64 loss: -0.05393582582473755
Batch 25/64 loss: -0.027005493640899658
Batch 26/64 loss: -0.04412931203842163
Batch 27/64 loss: -0.039586544036865234
Batch 28/64 loss: -0.034601449966430664
Batch 29/64 loss: -0.05495309829711914
Batch 30/64 loss: -0.07030785083770752
Batch 31/64 loss: -0.05025959014892578
Batch 32/64 loss: -0.06076759099960327
Batch 33/64 loss: -0.03461194038391113
Batch 34/64 loss: -0.03982400894165039
Batch 35/64 loss: -0.045054614543914795
Batch 36/64 loss: -0.045584022998809814
Batch 37/64 loss: -0.051611125469207764
Batch 38/64 loss: -0.06631672382354736
Batch 39/64 loss: -0.058085620403289795
Batch 40/64 loss: -0.03453594446182251
Batch 41/64 loss: -0.03431057929992676
Batch 42/64 loss: -0.04913419485092163
Batch 43/64 loss: -0.06569933891296387
Batch 44/64 loss: -0.06351321935653687
Batch 45/64 loss: -0.05767422914505005
Batch 46/64 loss: -0.07319271564483643
Batch 47/64 loss: -0.06497782468795776
Batch 48/64 loss: -0.05592387914657593
Batch 49/64 loss: -0.07398974895477295
Batch 50/64 loss: -0.06123119592666626
Batch 51/64 loss: -0.05843597650527954
Batch 52/64 loss: -0.056392550468444824
Batch 53/64 loss: -0.05389606952667236
Batch 54/64 loss: -0.050703465938568115
Batch 55/64 loss: -0.06212270259857178
Batch 56/64 loss: -0.054611027240753174
Batch 57/64 loss: -0.0640750527381897
Batch 58/64 loss: -0.0629759430885315
Batch 59/64 loss: -0.024342119693756104
Batch 60/64 loss: -0.04659879207611084
Batch 61/64 loss: -0.08387726545333862
Batch 62/64 loss: -0.07395565509796143
Batch 63/64 loss: -0.07015854120254517
Batch 64/64 loss: -0.08699756860733032
Epoch 209  Train loss: -0.0563539184776007  Val loss: 0.047319188765234145
Epoch 210
-------------------------------
Batch 1/64 loss: -0.04367130994796753
Batch 2/64 loss: -0.06380605697631836
Batch 3/64 loss: -0.05651450157165527
Batch 4/64 loss: -0.044321656227111816
Batch 5/64 loss: -0.04704487323760986
Batch 6/64 loss: -0.07159167528152466
Batch 7/64 loss: -0.056063830852508545
Batch 8/64 loss: -0.0671006441116333
Batch 9/64 loss: -0.06972110271453857
Batch 10/64 loss: -0.07831466197967529
Batch 11/64 loss: -0.07967859506607056
Batch 12/64 loss: -0.07456827163696289
Batch 13/64 loss: -0.03602176904678345
Batch 14/64 loss: -0.05275928974151611
Batch 15/64 loss: -0.050726115703582764
Batch 16/64 loss: -0.0631023645401001
Batch 17/64 loss: -0.08645820617675781
Batch 18/64 loss: -0.07126688957214355
Batch 19/64 loss: -0.05925208330154419
Batch 20/64 loss: -0.06827682256698608
Batch 21/64 loss: -0.038507163524627686
Batch 22/64 loss: -0.02463585138320923
Batch 23/64 loss: -0.04033815860748291
Batch 24/64 loss: -0.07200992107391357
Batch 25/64 loss: -0.07514464855194092
Batch 26/64 loss: -0.036791980266571045
Batch 27/64 loss: -0.06398457288742065
Batch 28/64 loss: -0.035419344902038574
Batch 29/64 loss: -0.05583113431930542
Batch 30/64 loss: -0.05851989984512329
Batch 31/64 loss: -0.05027991533279419
Batch 32/64 loss: -0.06700927019119263
Batch 33/64 loss: -0.06473326683044434
Batch 34/64 loss: -0.06412804126739502
Batch 35/64 loss: -0.06040221452713013
Batch 36/64 loss: -0.02418607473373413
Batch 37/64 loss: -0.03012937307357788
Batch 38/64 loss: -0.05534195899963379
Batch 39/64 loss: -0.08596837520599365
Batch 40/64 loss: -0.05131340026855469
Batch 41/64 loss: -0.06731009483337402
Batch 42/64 loss: -0.05086827278137207
Batch 43/64 loss: -0.0244295597076416
Batch 44/64 loss: -0.053255319595336914
Batch 45/64 loss: -0.054251015186309814
Batch 46/64 loss: -0.032766103744506836
Batch 47/64 loss: -0.08171933889389038
Batch 48/64 loss: -0.06817102432250977
Batch 49/64 loss: -0.06315350532531738
Batch 50/64 loss: -0.052990734577178955
Batch 51/64 loss: -0.053705811500549316
Batch 52/64 loss: -0.0583416223526001
Batch 53/64 loss: -0.04287773370742798
Batch 54/64 loss: -0.06396162509918213
Batch 55/64 loss: -0.05182230472564697
Batch 56/64 loss: -0.06184917688369751
Batch 57/64 loss: -0.06120884418487549
Batch 58/64 loss: -0.05687999725341797
Batch 59/64 loss: -0.06178349256515503
Batch 60/64 loss: -0.0749632716178894
Batch 61/64 loss: -0.045593321323394775
Batch 62/64 loss: -0.07191121578216553
Batch 63/64 loss: -0.04358309507369995
Batch 64/64 loss: -0.05786627531051636
Epoch 210  Train loss: -0.05703108193827611  Val loss: 0.04483818721115794
Epoch 211
-------------------------------
Batch 1/64 loss: -0.07304024696350098
Batch 2/64 loss: -0.050222933292388916
Batch 3/64 loss: -0.07884275913238525
Batch 4/64 loss: -0.07047796249389648
Batch 5/64 loss: -0.039304375648498535
Batch 6/64 loss: -0.0677710771560669
Batch 7/64 loss: -0.052071988582611084
Batch 8/64 loss: -0.07195568084716797
Batch 9/64 loss: -0.05360817909240723
Batch 10/64 loss: -0.05695587396621704
Batch 11/64 loss: -0.061823248863220215
Batch 12/64 loss: -0.03084862232208252
Batch 13/64 loss: -0.06211668252944946
Batch 14/64 loss: -0.07051151990890503
Batch 15/64 loss: -0.06684505939483643
Batch 16/64 loss: -0.0592571496963501
Batch 17/64 loss: -0.045463740825653076
Batch 18/64 loss: -0.07033950090408325
Batch 19/64 loss: -0.05612140893936157
Batch 20/64 loss: -0.043643414974212646
Batch 21/64 loss: -0.02032679319381714
Batch 22/64 loss: -0.07492697238922119
Batch 23/64 loss: -0.058430254459381104
Batch 24/64 loss: -0.06869399547576904
Batch 25/64 loss: -0.04785341024398804
Batch 26/64 loss: -0.05548280477523804
Batch 27/64 loss: -0.055369555950164795
Batch 28/64 loss: -0.06018674373626709
Batch 29/64 loss: -0.07735306024551392
Batch 30/64 loss: -0.05521869659423828
Batch 31/64 loss: -0.07130354642868042
Batch 32/64 loss: -0.03851747512817383
Batch 33/64 loss: -0.05274099111557007
Batch 34/64 loss: -0.04361480474472046
Batch 35/64 loss: -0.047466397285461426
Batch 36/64 loss: -0.04605305194854736
Batch 37/64 loss: -0.06779468059539795
Batch 38/64 loss: -0.04383587837219238
Batch 39/64 loss: -0.02766185998916626
Batch 40/64 loss: -0.04875993728637695
Batch 41/64 loss: -0.04783743619918823
Batch 42/64 loss: -0.07240831851959229
Batch 43/64 loss: -0.05023616552352905
Batch 44/64 loss: -0.04367119073867798
Batch 45/64 loss: -0.046770572662353516
Batch 46/64 loss: -0.048004150390625
Batch 47/64 loss: -0.05481678247451782
Batch 48/64 loss: -0.06667017936706543
Batch 49/64 loss: -0.07923877239227295
Batch 50/64 loss: -0.07755255699157715
Batch 51/64 loss: -0.05582684278488159
Batch 52/64 loss: -0.06815224885940552
Batch 53/64 loss: -0.056797683238983154
Batch 54/64 loss: -0.032044827938079834
Batch 55/64 loss: -0.059863388538360596
Batch 56/64 loss: -0.05728119611740112
Batch 57/64 loss: -0.07776147127151489
Batch 58/64 loss: -0.04984074831008911
Batch 59/64 loss: -0.05656778812408447
Batch 60/64 loss: -0.07768881320953369
Batch 61/64 loss: -0.05316585302352905
Batch 62/64 loss: -0.05461609363555908
Batch 63/64 loss: -0.059604108333587646
Batch 64/64 loss: -0.04677319526672363
Epoch 211  Train loss: -0.05685191247977463  Val loss: 0.04376790572687523
Epoch 212
-------------------------------
Batch 1/64 loss: -0.06002622842788696
Batch 2/64 loss: -0.042682945728302
Batch 3/64 loss: -0.053267598152160645
Batch 4/64 loss: -0.049192607402801514
Batch 5/64 loss: -0.0628085732460022
Batch 6/64 loss: -0.05152827501296997
Batch 7/64 loss: -0.06416618824005127
Batch 8/64 loss: -0.06615036725997925
Batch 9/64 loss: -0.05610060691833496
Batch 10/64 loss: -0.06411963701248169
Batch 11/64 loss: -0.02990126609802246
Batch 12/64 loss: -0.06602925062179565
Batch 13/64 loss: -0.06787562370300293
Batch 14/64 loss: -0.054678261280059814
Batch 15/64 loss: -0.06594264507293701
Batch 16/64 loss: -0.05201774835586548
Batch 17/64 loss: -0.03757542371749878
Batch 18/64 loss: -0.06651222705841064
Batch 19/64 loss: -0.048603594303131104
Batch 20/64 loss: -0.06154412031173706
Batch 21/64 loss: -0.06087440252304077
Batch 22/64 loss: -0.06035876274108887
Batch 23/64 loss: -0.04803764820098877
Batch 24/64 loss: -0.05021512508392334
Batch 25/64 loss: -0.050170302391052246
Batch 26/64 loss: -0.06825768947601318
Batch 27/64 loss: -0.05529510974884033
Batch 28/64 loss: -0.05163681507110596
Batch 29/64 loss: -0.04850733280181885
Batch 30/64 loss: -0.08827471733093262
Batch 31/64 loss: -0.057875990867614746
Batch 32/64 loss: -0.04009813070297241
Batch 33/64 loss: -0.05702090263366699
Batch 34/64 loss: -0.07033926248550415
Batch 35/64 loss: -0.04359471797943115
Batch 36/64 loss: -0.044802069664001465
Batch 37/64 loss: -0.025799274444580078
Batch 38/64 loss: -0.07108426094055176
Batch 39/64 loss: -0.05114305019378662
Batch 40/64 loss: -0.06050705909729004
Batch 41/64 loss: -0.06131356954574585
Batch 42/64 loss: -0.048019468784332275
Batch 43/64 loss: -0.051418423652648926
Batch 44/64 loss: -0.06107306480407715
Batch 45/64 loss: -0.0712575912475586
Batch 46/64 loss: -0.05510431528091431
Batch 47/64 loss: -0.03154653310775757
Batch 48/64 loss: -0.06667101383209229
Batch 49/64 loss: -0.0567777156829834
Batch 50/64 loss: -0.05888336896896362
Batch 51/64 loss: -0.07527601718902588
Batch 52/64 loss: -0.028248369693756104
Batch 53/64 loss: -0.06361138820648193
Batch 54/64 loss: -0.0641477108001709
Batch 55/64 loss: -0.07114017009735107
Batch 56/64 loss: -0.06952816247940063
Batch 57/64 loss: -0.054249346256256104
Batch 58/64 loss: -0.04407244920730591
Batch 59/64 loss: -0.06550377607345581
Batch 60/64 loss: -0.00043380260467529297
Batch 61/64 loss: -0.07623744010925293
Batch 62/64 loss: -0.029279112815856934
Batch 63/64 loss: -0.039831578731536865
Batch 64/64 loss: -0.04933607578277588
Epoch 212  Train loss: -0.05498419228722067  Val loss: 0.04835300879789792
Epoch 213
-------------------------------
Batch 1/64 loss: -0.05563032627105713
Batch 2/64 loss: -0.07574295997619629
Batch 3/64 loss: -0.06509512662887573
Batch 4/64 loss: -0.05656921863555908
Batch 5/64 loss: -0.07756698131561279
Batch 6/64 loss: -0.05068725347518921
Batch 7/64 loss: -0.07077819108963013
Batch 8/64 loss: -0.049085140228271484
Batch 9/64 loss: -0.05739247798919678
Batch 10/64 loss: -0.07860040664672852
Batch 11/64 loss: -0.049417734146118164
Batch 12/64 loss: -0.06810712814331055
Batch 13/64 loss: -0.05183124542236328
Batch 14/64 loss: -0.06898307800292969
Batch 15/64 loss: -0.08977389335632324
Batch 16/64 loss: -0.05113077163696289
Batch 17/64 loss: -0.06124669313430786
Batch 18/64 loss: -0.0469631552696228
Batch 19/64 loss: -0.07670813798904419
Batch 20/64 loss: -0.06063413619995117
Batch 21/64 loss: -0.08044123649597168
Batch 22/64 loss: -0.05816972255706787
Batch 23/64 loss: -0.06427699327468872
Batch 24/64 loss: -0.05536305904388428
Batch 25/64 loss: -0.04019886255264282
Batch 26/64 loss: -0.05619657039642334
Batch 27/64 loss: -0.05505573749542236
Batch 28/64 loss: -0.03677690029144287
Batch 29/64 loss: -0.06608825922012329
Batch 30/64 loss: -0.06508630514144897
Batch 31/64 loss: -0.06471419334411621
Batch 32/64 loss: -0.0689200758934021
Batch 33/64 loss: -0.06028175354003906
Batch 34/64 loss: -0.065848708152771
Batch 35/64 loss: -0.0546453595161438
Batch 36/64 loss: -0.06056642532348633
Batch 37/64 loss: -0.07821935415267944
Batch 38/64 loss: -0.05202966928482056
Batch 39/64 loss: -0.04945957660675049
Batch 40/64 loss: -0.07137483358383179
Batch 41/64 loss: -0.018207192420959473
Batch 42/64 loss: -0.06823569536209106
Batch 43/64 loss: -0.04924666881561279
Batch 44/64 loss: -0.028612732887268066
Batch 45/64 loss: -0.048589229583740234
Batch 46/64 loss: -0.053185224533081055
Batch 47/64 loss: -0.08637326955795288
Batch 48/64 loss: -0.0394744873046875
Batch 49/64 loss: -0.019225478172302246
Batch 50/64 loss: -0.029714882373809814
Batch 51/64 loss: -0.07568895816802979
Batch 52/64 loss: -0.06532174348831177
Batch 53/64 loss: -0.04849499464035034
Batch 54/64 loss: -0.05814158916473389
Batch 55/64 loss: -0.05236703157424927
Batch 56/64 loss: -0.06772327423095703
Batch 57/64 loss: -0.07336556911468506
Batch 58/64 loss: -0.04581761360168457
Batch 59/64 loss: -0.04361516237258911
Batch 60/64 loss: -0.05892711877822876
Batch 61/64 loss: -0.046679019927978516
Batch 62/64 loss: -0.059303104877471924
Batch 63/64 loss: -0.04813814163208008
Batch 64/64 loss: -0.04137146472930908
Epoch 213  Train loss: -0.057743285216537175  Val loss: 0.041015523815482756
Saving best model, epoch: 213
Epoch 214
-------------------------------
Batch 1/64 loss: -0.08320319652557373
Batch 2/64 loss: -0.07161378860473633
Batch 3/64 loss: -0.04853677749633789
Batch 4/64 loss: -0.0396881103515625
Batch 5/64 loss: -0.06005549430847168
Batch 6/64 loss: -0.05674189329147339
Batch 7/64 loss: -0.06914359331130981
Batch 8/64 loss: -0.07843667268753052
Batch 9/64 loss: -0.06968080997467041
Batch 10/64 loss: -0.04560798406600952
Batch 11/64 loss: -0.0641137957572937
Batch 12/64 loss: -0.04800540208816528
Batch 13/64 loss: -0.05717170238494873
Batch 14/64 loss: -0.062204718589782715
Batch 15/64 loss: -0.04699152708053589
Batch 16/64 loss: -0.06641364097595215
Batch 17/64 loss: -0.05344504117965698
Batch 18/64 loss: -0.06842845678329468
Batch 19/64 loss: -0.05704373121261597
Batch 20/64 loss: -0.043200790882110596
Batch 21/64 loss: -0.0658988356590271
Batch 22/64 loss: -0.028457820415496826
Batch 23/64 loss: -0.06607842445373535
Batch 24/64 loss: -0.0642898678779602
Batch 25/64 loss: -0.08914041519165039
Batch 26/64 loss: -0.05586409568786621
Batch 27/64 loss: -0.06983298063278198
Batch 28/64 loss: -0.056485652923583984
Batch 29/64 loss: -0.05663806200027466
Batch 30/64 loss: -0.04952031373977661
Batch 31/64 loss: -0.03537076711654663
Batch 32/64 loss: -0.07792383432388306
Batch 33/64 loss: -0.05620616674423218
Batch 34/64 loss: -0.06616419553756714
Batch 35/64 loss: -0.03584104776382446
Batch 36/64 loss: -0.0609973669052124
Batch 37/64 loss: -0.0720442533493042
Batch 38/64 loss: -0.06900680065155029
Batch 39/64 loss: -0.06781113147735596
Batch 40/64 loss: -0.07310402393341064
Batch 41/64 loss: -0.052473247051239014
Batch 42/64 loss: -0.04684656858444214
Batch 43/64 loss: -0.05759340524673462
Batch 44/64 loss: -0.062255859375
Batch 45/64 loss: -0.06674343347549438
Batch 46/64 loss: -0.04256319999694824
Batch 47/64 loss: -0.06988340616226196
Batch 48/64 loss: -0.0659860372543335
Batch 49/64 loss: -0.04893302917480469
Batch 50/64 loss: -0.06372976303100586
Batch 51/64 loss: -0.06202995777130127
Batch 52/64 loss: -0.0772354006767273
Batch 53/64 loss: -0.061309993267059326
Batch 54/64 loss: -0.033240437507629395
Batch 55/64 loss: -0.02949124574661255
Batch 56/64 loss: -0.06451642513275146
Batch 57/64 loss: -0.06991684436798096
Batch 58/64 loss: -0.06585729122161865
Batch 59/64 loss: -0.07126951217651367
Batch 60/64 loss: -0.030789971351623535
Batch 61/64 loss: -0.05270862579345703
Batch 62/64 loss: -0.06740748882293701
Batch 63/64 loss: -0.04405558109283447
Batch 64/64 loss: -0.06754887104034424
Epoch 214  Train loss: -0.059041593121547324  Val loss: 0.04526104201975557
Epoch 215
-------------------------------
Batch 1/64 loss: -0.06993281841278076
Batch 2/64 loss: -0.054868996143341064
Batch 3/64 loss: -0.06006133556365967
Batch 4/64 loss: -0.07105660438537598
Batch 5/64 loss: -0.06014758348464966
Batch 6/64 loss: -0.06166541576385498
Batch 7/64 loss: -0.07620865106582642
Batch 8/64 loss: -0.04514580965042114
Batch 9/64 loss: -0.07230174541473389
Batch 10/64 loss: -0.06222134828567505
Batch 11/64 loss: -0.06826174259185791
Batch 12/64 loss: -0.03895813226699829
Batch 13/64 loss: -0.040784358978271484
Batch 14/64 loss: -0.06992971897125244
Batch 15/64 loss: -0.06198173761367798
Batch 16/64 loss: -0.04614663124084473
Batch 17/64 loss: -0.06076657772064209
Batch 18/64 loss: -0.06545311212539673
Batch 19/64 loss: -0.0873708724975586
Batch 20/64 loss: -0.05344414710998535
Batch 21/64 loss: -0.04857170581817627
Batch 22/64 loss: -0.05996406078338623
Batch 23/64 loss: -0.06250005960464478
Batch 24/64 loss: -0.06795668601989746
Batch 25/64 loss: -0.04846245050430298
Batch 26/64 loss: -0.029805779457092285
Batch 27/64 loss: -0.06708347797393799
Batch 28/64 loss: -0.0672406554222107
Batch 29/64 loss: -0.05028116703033447
Batch 30/64 loss: -0.04002511501312256
Batch 31/64 loss: -0.06836205720901489
Batch 32/64 loss: -0.052466630935668945
Batch 33/64 loss: -0.08688211441040039
Batch 34/64 loss: -0.04718482494354248
Batch 35/64 loss: -0.05700451135635376
Batch 36/64 loss: -0.04945695400238037
Batch 37/64 loss: -0.05295056104660034
Batch 38/64 loss: -0.05074828863143921
Batch 39/64 loss: -0.0678454041481018
Batch 40/64 loss: -0.04354947805404663
Batch 41/64 loss: -0.04602682590484619
Batch 42/64 loss: -0.04379552602767944
Batch 43/64 loss: -0.05053579807281494
Batch 44/64 loss: -0.05847907066345215
Batch 45/64 loss: -0.043854594230651855
Batch 46/64 loss: -0.050713419914245605
Batch 47/64 loss: -0.05846834182739258
Batch 48/64 loss: -0.05828481912612915
Batch 49/64 loss: -0.06385016441345215
Batch 50/64 loss: -0.033559322357177734
Batch 51/64 loss: -0.04410821199417114
Batch 52/64 loss: -0.059575676918029785
Batch 53/64 loss: -0.04999333620071411
Batch 54/64 loss: -0.07980650663375854
Batch 55/64 loss: -0.07490164041519165
Batch 56/64 loss: -0.040149986743927
Batch 57/64 loss: -0.07213133573532104
Batch 58/64 loss: -0.08594071865081787
Batch 59/64 loss: -0.04944878816604614
Batch 60/64 loss: -0.05793964862823486
Batch 61/64 loss: -0.0635342001914978
Batch 62/64 loss: -0.05975210666656494
Batch 63/64 loss: -0.06647634506225586
Batch 64/64 loss: -0.06421935558319092
Epoch 215  Train loss: -0.058110434868756465  Val loss: 0.047116598312797416
Epoch 216
-------------------------------
Batch 1/64 loss: -0.06598508358001709
Batch 2/64 loss: -0.056009769439697266
Batch 3/64 loss: -0.09019160270690918
Batch 4/64 loss: -0.05190640687942505
Batch 5/64 loss: -0.04429745674133301
Batch 6/64 loss: -0.05936276912689209
Batch 7/64 loss: -0.08684295415878296
Batch 8/64 loss: -0.04671895503997803
Batch 9/64 loss: -0.04632461071014404
Batch 10/64 loss: -0.0717359185218811
Batch 11/64 loss: -0.06087207794189453
Batch 12/64 loss: -0.08259284496307373
Batch 13/64 loss: -0.03961282968521118
Batch 14/64 loss: -0.05868273973464966
Batch 15/64 loss: -0.06779700517654419
Batch 16/64 loss: -0.050139427185058594
Batch 17/64 loss: -0.061156272888183594
Batch 18/64 loss: -0.055348992347717285
Batch 19/64 loss: -0.0722501277923584
Batch 20/64 loss: -0.05858433246612549
Batch 21/64 loss: -0.0605165958404541
Batch 22/64 loss: -0.05858498811721802
Batch 23/64 loss: -0.05897241830825806
Batch 24/64 loss: -0.06874251365661621
Batch 25/64 loss: -0.037607669830322266
Batch 26/64 loss: -0.07463794946670532
Batch 27/64 loss: -0.03269517421722412
Batch 28/64 loss: -0.052692413330078125
Batch 29/64 loss: -0.06919693946838379
Batch 30/64 loss: -0.06093311309814453
Batch 31/64 loss: -0.06736409664154053
Batch 32/64 loss: -0.054911255836486816
Batch 33/64 loss: -0.06782412528991699
Batch 34/64 loss: -0.0629122257232666
Batch 35/64 loss: -0.07847404479980469
Batch 36/64 loss: -0.05315154790878296
Batch 37/64 loss: -0.03780865669250488
Batch 38/64 loss: -0.08803093433380127
Batch 39/64 loss: -0.056021809577941895
Batch 40/64 loss: -0.04419994354248047
Batch 41/64 loss: -0.04910707473754883
Batch 42/64 loss: -0.019817650318145752
Batch 43/64 loss: -0.039231181144714355
Batch 44/64 loss: -0.055259764194488525
Batch 45/64 loss: -0.09101396799087524
Batch 46/64 loss: -0.06208086013793945
Batch 47/64 loss: -0.06459689140319824
Batch 48/64 loss: -0.054160475730895996
Batch 49/64 loss: -0.06780904531478882
Batch 50/64 loss: -0.06338846683502197
Batch 51/64 loss: -0.054999470710754395
Batch 52/64 loss: -0.08302426338195801
Batch 53/64 loss: -0.0822291374206543
Batch 54/64 loss: -0.05460500717163086
Batch 55/64 loss: -0.05821162462234497
Batch 56/64 loss: -0.06530344486236572
Batch 57/64 loss: -0.07647228240966797
Batch 58/64 loss: -0.03210306167602539
Batch 59/64 loss: -0.07013261318206787
Batch 60/64 loss: -0.061067819595336914
Batch 61/64 loss: -0.06541585922241211
Batch 62/64 loss: -0.04636561870574951
Batch 63/64 loss: -0.057302236557006836
Batch 64/64 loss: -0.04186534881591797
Epoch 216  Train loss: -0.05987117524240531  Val loss: 0.041459656458130406
Epoch 217
-------------------------------
Batch 1/64 loss: -0.07072389125823975
Batch 2/64 loss: -0.07738059759140015
Batch 3/64 loss: -0.03570050001144409
Batch 4/64 loss: -0.055934786796569824
Batch 5/64 loss: -0.08099567890167236
Batch 6/64 loss: -0.06693822145462036
Batch 7/64 loss: -0.07402157783508301
Batch 8/64 loss: -0.03326821327209473
Batch 9/64 loss: -0.04030156135559082
Batch 10/64 loss: -0.08646076917648315
Batch 11/64 loss: -0.06275194883346558
Batch 12/64 loss: -0.07684528827667236
Batch 13/64 loss: -0.05506652593612671
Batch 14/64 loss: -0.06150537729263306
Batch 15/64 loss: -0.049851059913635254
Batch 16/64 loss: -0.05233651399612427
Batch 17/64 loss: -0.036443233489990234
Batch 18/64 loss: -0.05733776092529297
Batch 19/64 loss: -0.05760061740875244
Batch 20/64 loss: -0.06694889068603516
Batch 21/64 loss: -0.06387060880661011
Batch 22/64 loss: -0.07956081628799438
Batch 23/64 loss: -0.05958956480026245
Batch 24/64 loss: -0.09389805793762207
Batch 25/64 loss: -0.05050075054168701
Batch 26/64 loss: -0.039809465408325195
Batch 27/64 loss: -0.06759291887283325
Batch 28/64 loss: -0.06019484996795654
Batch 29/64 loss: -0.06458902359008789
Batch 30/64 loss: -0.08297628164291382
Batch 31/64 loss: -0.06462597846984863
Batch 32/64 loss: -0.0630180835723877
Batch 33/64 loss: -0.07932418584823608
Batch 34/64 loss: -0.029132604598999023
Batch 35/64 loss: -0.07504725456237793
Batch 36/64 loss: -0.05853837728500366
Batch 37/64 loss: -0.06077909469604492
Batch 38/64 loss: -0.07202941179275513
Batch 39/64 loss: -0.07605087757110596
Batch 40/64 loss: -0.07153522968292236
Batch 41/64 loss: -0.04086148738861084
Batch 42/64 loss: -0.07073849439620972
Batch 43/64 loss: -0.0484616756439209
Batch 44/64 loss: -0.08789104223251343
Batch 45/64 loss: -0.06014454364776611
Batch 46/64 loss: -0.07405596971511841
Batch 47/64 loss: -0.045359015464782715
Batch 48/64 loss: -0.06214916706085205
Batch 49/64 loss: -0.07038998603820801
Batch 50/64 loss: -0.050880372524261475
Batch 51/64 loss: -0.040968894958496094
Batch 52/64 loss: -0.06794166564941406
Batch 53/64 loss: -0.07261234521865845
Batch 54/64 loss: -0.05476212501525879
Batch 55/64 loss: -0.0638655424118042
Batch 56/64 loss: -0.06057453155517578
Batch 57/64 loss: -0.060274362564086914
Batch 58/64 loss: -0.011488497257232666
Batch 59/64 loss: -0.0360223650932312
Batch 60/64 loss: -0.08117997646331787
Batch 61/64 loss: -0.03376847505569458
Batch 62/64 loss: -0.06459945440292358
Batch 63/64 loss: -0.05099344253540039
Batch 64/64 loss: -0.07733279466629028
Epoch 217  Train loss: -0.06084799182181265  Val loss: 0.04545608091190508
Epoch 218
-------------------------------
Batch 1/64 loss: -0.08079999685287476
Batch 2/64 loss: -0.0859990119934082
Batch 3/64 loss: -0.04582470655441284
Batch 4/64 loss: -0.0778493881225586
Batch 5/64 loss: -0.06571763753890991
Batch 6/64 loss: -0.08368593454360962
Batch 7/64 loss: -0.06360739469528198
Batch 8/64 loss: -0.03440570831298828
Batch 9/64 loss: -0.0828484296798706
Batch 10/64 loss: -0.04705357551574707
Batch 11/64 loss: -0.06648671627044678
Batch 12/64 loss: -0.06692934036254883
Batch 13/64 loss: -0.041852355003356934
Batch 14/64 loss: -0.07632899284362793
Batch 15/64 loss: -0.07728135585784912
Batch 16/64 loss: -0.04419523477554321
Batch 17/64 loss: -0.08133441209793091
Batch 18/64 loss: -0.05864238739013672
Batch 19/64 loss: -0.0521998405456543
Batch 20/64 loss: -0.04758244752883911
Batch 21/64 loss: -0.05824214220046997
Batch 22/64 loss: -0.06777834892272949
Batch 23/64 loss: -0.052897095680236816
Batch 24/64 loss: -0.05236983299255371
Batch 25/64 loss: -0.07601964473724365
Batch 26/64 loss: -0.07265621423721313
Batch 27/64 loss: -0.04265153408050537
Batch 28/64 loss: -0.07259166240692139
Batch 29/64 loss: -0.07595950365066528
Batch 30/64 loss: -0.07201731204986572
Batch 31/64 loss: -0.07682353258132935
Batch 32/64 loss: -0.0736393928527832
Batch 33/64 loss: -0.04672574996948242
Batch 34/64 loss: -0.04666328430175781
Batch 35/64 loss: -0.06131696701049805
Batch 36/64 loss: -0.07054406404495239
Batch 37/64 loss: -0.0756637454032898
Batch 38/64 loss: -0.06287932395935059
Batch 39/64 loss: -0.061295270919799805
Batch 40/64 loss: -0.06387495994567871
Batch 41/64 loss: -0.0803181529045105
Batch 42/64 loss: -0.048243582248687744
Batch 43/64 loss: -0.05951780080795288
Batch 44/64 loss: -0.02305537462234497
Batch 45/64 loss: -0.057969748973846436
Batch 46/64 loss: -0.08197236061096191
Batch 47/64 loss: -0.05404937267303467
Batch 48/64 loss: -0.03867065906524658
Batch 49/64 loss: -0.08231949806213379
Batch 50/64 loss: -0.061295151710510254
Batch 51/64 loss: -0.0591282844543457
Batch 52/64 loss: -0.07764589786529541
Batch 53/64 loss: -0.06991434097290039
Batch 54/64 loss: -0.04407453536987305
Batch 55/64 loss: -0.038173556327819824
Batch 56/64 loss: -0.0690041184425354
Batch 57/64 loss: -0.07621335983276367
Batch 58/64 loss: -0.0775877833366394
Batch 59/64 loss: -0.03756517171859741
Batch 60/64 loss: -0.061356544494628906
Batch 61/64 loss: -0.05753183364868164
Batch 62/64 loss: -0.05803710222244263
Batch 63/64 loss: -0.03420466184616089
Batch 64/64 loss: -0.06689774990081787
Epoch 218  Train loss: -0.062137359263850195  Val loss: 0.04421416447334683
Epoch 219
-------------------------------
Batch 1/64 loss: -0.04253697395324707
Batch 2/64 loss: -0.057660818099975586
Batch 3/64 loss: -0.06803590059280396
Batch 4/64 loss: -0.066494882106781
Batch 5/64 loss: -0.06324142217636108
Batch 6/64 loss: -0.06551891565322876
Batch 7/64 loss: -0.04302215576171875
Batch 8/64 loss: -0.060094356536865234
Batch 9/64 loss: -0.056508421897888184
Batch 10/64 loss: -0.0635000467300415
Batch 11/64 loss: -0.09506380558013916
Batch 12/64 loss: -0.07305741310119629
Batch 13/64 loss: -0.06560271978378296
Batch 14/64 loss: -0.057525634765625
Batch 15/64 loss: -0.06841576099395752
Batch 16/64 loss: -0.08903121948242188
Batch 17/64 loss: -0.06713080406188965
Batch 18/64 loss: -0.07517176866531372
Batch 19/64 loss: -0.052228450775146484
Batch 20/64 loss: -0.09218102693557739
Batch 21/64 loss: -0.08195751905441284
Batch 22/64 loss: -0.053299784660339355
Batch 23/64 loss: -0.07210493087768555
Batch 24/64 loss: -0.03905218839645386
Batch 25/64 loss: -0.06773984432220459
Batch 26/64 loss: -0.0754886269569397
Batch 27/64 loss: -0.06150233745574951
Batch 28/64 loss: -0.08215022087097168
Batch 29/64 loss: -0.07752764225006104
Batch 30/64 loss: -0.049356698989868164
Batch 31/64 loss: -0.018707454204559326
Batch 32/64 loss: -0.060364365577697754
Batch 33/64 loss: -0.07280939817428589
Batch 34/64 loss: -0.05692654848098755
Batch 35/64 loss: -0.07740485668182373
Batch 36/64 loss: -0.018146157264709473
Batch 37/64 loss: -0.043650031089782715
Batch 38/64 loss: -0.07545769214630127
Batch 39/64 loss: -0.05416131019592285
Batch 40/64 loss: -0.06570035219192505
Batch 41/64 loss: -0.06231260299682617
Batch 42/64 loss: -0.03663909435272217
Batch 43/64 loss: -0.07929003238677979
Batch 44/64 loss: -0.06376582384109497
Batch 45/64 loss: -0.06279397010803223
Batch 46/64 loss: -0.05941438674926758
Batch 47/64 loss: -0.06890028715133667
Batch 48/64 loss: -0.06508243083953857
Batch 49/64 loss: -0.058645784854888916
Batch 50/64 loss: -0.04532665014266968
Batch 51/64 loss: -0.044668614864349365
Batch 52/64 loss: -0.08285778760910034
Batch 53/64 loss: -0.06786167621612549
Batch 54/64 loss: -0.05267131328582764
Batch 55/64 loss: -0.05144357681274414
Batch 56/64 loss: -0.0771021842956543
Batch 57/64 loss: -0.054627954959869385
Batch 58/64 loss: -0.07247656583786011
Batch 59/64 loss: -0.06709903478622437
Batch 60/64 loss: -0.06228363513946533
Batch 61/64 loss: -0.038146793842315674
Batch 62/64 loss: -0.04500770568847656
Batch 63/64 loss: -0.03859370946884155
Batch 64/64 loss: -0.023588240146636963
Epoch 219  Train loss: -0.06118012947194716  Val loss: 0.04920329506864253
Epoch 220
-------------------------------
Batch 1/64 loss: -0.04403740167617798
Batch 2/64 loss: -0.02893221378326416
Batch 3/64 loss: -0.06964707374572754
Batch 4/64 loss: -0.07523393630981445
Batch 5/64 loss: -0.05554705858230591
Batch 6/64 loss: -0.05708128213882446
Batch 7/64 loss: -0.06573408842086792
Batch 8/64 loss: -0.06578463315963745
Batch 9/64 loss: -0.04960906505584717
Batch 10/64 loss: -0.05814969539642334
Batch 11/64 loss: -0.04862523078918457
Batch 12/64 loss: -0.07083725929260254
Batch 13/64 loss: -0.06914836168289185
Batch 14/64 loss: -0.05151069164276123
Batch 15/64 loss: -0.08884608745574951
Batch 16/64 loss: -0.0863577127456665
Batch 17/64 loss: -0.08671605587005615
Batch 18/64 loss: -0.06122779846191406
Batch 19/64 loss: -0.09278380870819092
Batch 20/64 loss: -0.013356149196624756
Batch 21/64 loss: -0.08356988430023193
Batch 22/64 loss: -0.048339247703552246
Batch 23/64 loss: -0.07127690315246582
Batch 24/64 loss: -0.0709376335144043
Batch 25/64 loss: -0.07701367139816284
Batch 26/64 loss: -0.05800604820251465
Batch 27/64 loss: -0.07161706686019897
Batch 28/64 loss: -0.03330415487289429
Batch 29/64 loss: -0.03917604684829712
Batch 30/64 loss: -0.06275707483291626
Batch 31/64 loss: -0.08216136693954468
Batch 32/64 loss: -0.07687234878540039
Batch 33/64 loss: -0.05006366968154907
Batch 34/64 loss: -0.052963316440582275
Batch 35/64 loss: -0.06069636344909668
Batch 36/64 loss: -0.07812589406967163
Batch 37/64 loss: -0.08894753456115723
Batch 38/64 loss: -0.06596088409423828
Batch 39/64 loss: -0.08926326036453247
Batch 40/64 loss: -0.06354433298110962
Batch 41/64 loss: -0.07078510522842407
Batch 42/64 loss: -0.06668031215667725
Batch 43/64 loss: -0.05883675813674927
Batch 44/64 loss: -0.06746792793273926
Batch 45/64 loss: -0.05069953203201294
Batch 46/64 loss: -0.025483787059783936
Batch 47/64 loss: -0.04955792427062988
Batch 48/64 loss: -0.04822760820388794
Batch 49/64 loss: -0.06191813945770264
Batch 50/64 loss: -0.052267491817474365
Batch 51/64 loss: -0.06711256504058838
Batch 52/64 loss: -0.0585627555847168
Batch 53/64 loss: -0.05210298299789429
Batch 54/64 loss: -0.05184578895568848
Batch 55/64 loss: -0.06381833553314209
Batch 56/64 loss: -0.0608670711517334
Batch 57/64 loss: -0.048683226108551025
Batch 58/64 loss: -0.058515191078186035
Batch 59/64 loss: -0.06939005851745605
Batch 60/64 loss: -0.06490814685821533
Batch 61/64 loss: -0.060530006885528564
Batch 62/64 loss: -0.07447928190231323
Batch 63/64 loss: -0.0952802300453186
Batch 64/64 loss: -0.030311524868011475
Epoch 220  Train loss: -0.06218848906311334  Val loss: 0.04680876182936311
Epoch 221
-------------------------------
Batch 1/64 loss: -0.06726503372192383
Batch 2/64 loss: -0.07258230447769165
Batch 3/64 loss: -0.06351405382156372
Batch 4/64 loss: -0.05587303638458252
Batch 5/64 loss: -0.07651644945144653
Batch 6/64 loss: -0.05436432361602783
Batch 7/64 loss: -0.06131124496459961
Batch 8/64 loss: -0.08504527807235718
Batch 9/64 loss: -0.09551233053207397
Batch 10/64 loss: -0.05163013935089111
Batch 11/64 loss: -0.06711697578430176
Batch 12/64 loss: -0.04779207706451416
Batch 13/64 loss: -0.06563740968704224
Batch 14/64 loss: -0.06451892852783203
Batch 15/64 loss: -0.06060093641281128
Batch 16/64 loss: -0.07692199945449829
Batch 17/64 loss: -0.04860192537307739
Batch 18/64 loss: -0.06353354454040527
Batch 19/64 loss: -0.016615688800811768
Batch 20/64 loss: -0.06567847728729248
Batch 21/64 loss: -0.04550361633300781
Batch 22/64 loss: -0.07297170162200928
Batch 23/64 loss: -0.06354475021362305
Batch 24/64 loss: -0.06964254379272461
Batch 25/64 loss: -0.06420958042144775
Batch 26/64 loss: -0.06840604543685913
Batch 27/64 loss: -0.07042795419692993
Batch 28/64 loss: -0.03284329175949097
Batch 29/64 loss: -0.06312620639801025
Batch 30/64 loss: -0.047732770442962646
Batch 31/64 loss: -0.06207716464996338
Batch 32/64 loss: -0.07047992944717407
Batch 33/64 loss: -0.03218454122543335
Batch 34/64 loss: -0.06344997882843018
Batch 35/64 loss: -0.0687631368637085
Batch 36/64 loss: -0.06668245792388916
Batch 37/64 loss: -0.08665621280670166
Batch 38/64 loss: -0.038536131381988525
Batch 39/64 loss: -0.07701915502548218
Batch 40/64 loss: -0.04424828290939331
Batch 41/64 loss: -0.05138742923736572
Batch 42/64 loss: -0.0597461462020874
Batch 43/64 loss: -0.061023592948913574
Batch 44/64 loss: -0.08380818367004395
Batch 45/64 loss: -0.04573476314544678
Batch 46/64 loss: -0.045946359634399414
Batch 47/64 loss: -0.04084664583206177
Batch 48/64 loss: -0.03674501180648804
Batch 49/64 loss: -0.06325554847717285
Batch 50/64 loss: -0.08494651317596436
Batch 51/64 loss: -0.04006361961364746
Batch 52/64 loss: -0.05952328443527222
Batch 53/64 loss: -0.06635117530822754
Batch 54/64 loss: -0.0498805046081543
Batch 55/64 loss: -0.06527543067932129
Batch 56/64 loss: -0.0780172348022461
Batch 57/64 loss: -0.05811721086502075
Batch 58/64 loss: -0.06882286071777344
Batch 59/64 loss: -0.06804710626602173
Batch 60/64 loss: -0.07869142293930054
Batch 61/64 loss: -0.07165271043777466
Batch 62/64 loss: -0.040679097175598145
Batch 63/64 loss: -0.07614761590957642
Batch 64/64 loss: -0.08393657207489014
Epoch 221  Train loss: -0.06159685499527875  Val loss: 0.04587820121103136
Epoch 222
-------------------------------
Batch 1/64 loss: -0.03664731979370117
Batch 2/64 loss: -0.07476752996444702
Batch 3/64 loss: -0.05008190870285034
Batch 4/64 loss: -0.07049554586410522
Batch 5/64 loss: -0.08559834957122803
Batch 6/64 loss: -0.0362546443939209
Batch 7/64 loss: -0.05487656593322754
Batch 8/64 loss: -0.05106264352798462
Batch 9/64 loss: -0.08485198020935059
Batch 10/64 loss: -0.04233580827713013
Batch 11/64 loss: -0.06655818223953247
Batch 12/64 loss: -0.03455758094787598
Batch 13/64 loss: -0.0348508358001709
Batch 14/64 loss: -0.0583498477935791
Batch 15/64 loss: -0.05954653024673462
Batch 16/64 loss: -0.05805438756942749
Batch 17/64 loss: -0.06140202283859253
Batch 18/64 loss: -0.06847858428955078
Batch 19/64 loss: -0.061045050621032715
Batch 20/64 loss: -0.08891797065734863
Batch 21/64 loss: -0.0432552695274353
Batch 22/64 loss: -0.06536668539047241
Batch 23/64 loss: -0.06204646825790405
Batch 24/64 loss: -0.06008630990982056
Batch 25/64 loss: -0.06306040287017822
Batch 26/64 loss: -0.06769019365310669
Batch 27/64 loss: -0.07786095142364502
Batch 28/64 loss: -0.07527035474777222
Batch 29/64 loss: -0.051466941833496094
Batch 30/64 loss: -0.06231820583343506
Batch 31/64 loss: -0.07604634761810303
Batch 32/64 loss: -0.07831323146820068
Batch 33/64 loss: -0.09118449687957764
Batch 34/64 loss: -0.037883460521698
Batch 35/64 loss: -0.06749242544174194
Batch 36/64 loss: -0.08772444725036621
Batch 37/64 loss: -0.025221407413482666
Batch 38/64 loss: -0.05872464179992676
Batch 39/64 loss: -0.07329869270324707
Batch 40/64 loss: -0.06062614917755127
Batch 41/64 loss: -0.05791682004928589
Batch 42/64 loss: -0.04551726579666138
Batch 43/64 loss: -0.06650340557098389
Batch 44/64 loss: -0.05632936954498291
Batch 45/64 loss: -0.08790159225463867
Batch 46/64 loss: -0.060874879360198975
Batch 47/64 loss: -0.07882344722747803
Batch 48/64 loss: -0.07447516918182373
Batch 49/64 loss: -0.07309836149215698
Batch 50/64 loss: -0.04367905855178833
Batch 51/64 loss: -0.0699150562286377
Batch 52/64 loss: -0.06448233127593994
Batch 53/64 loss: -0.07547962665557861
Batch 54/64 loss: -0.058004915714263916
Batch 55/64 loss: -0.07491856813430786
Batch 56/64 loss: -0.07979428768157959
Batch 57/64 loss: -0.04986250400543213
Batch 58/64 loss: -0.0680815577507019
Batch 59/64 loss: -0.07568943500518799
Batch 60/64 loss: -0.0512351393699646
Batch 61/64 loss: -0.05202782154083252
Batch 62/64 loss: -0.04656416177749634
Batch 63/64 loss: -0.07379215955734253
Batch 64/64 loss: -0.04779118299484253
Epoch 222  Train loss: -0.06250165794409958  Val loss: 0.04390296555057015
Epoch 223
-------------------------------
Batch 1/64 loss: -0.07287216186523438
Batch 2/64 loss: -0.05699002742767334
Batch 3/64 loss: -0.08171463012695312
Batch 4/64 loss: -0.0651889443397522
Batch 5/64 loss: -0.04894286394119263
Batch 6/64 loss: -0.08559656143188477
Batch 7/64 loss: -0.06371504068374634
Batch 8/64 loss: -0.0742986798286438
Batch 9/64 loss: -0.054463744163513184
Batch 10/64 loss: -0.07129108905792236
Batch 11/64 loss: -0.03786969184875488
Batch 12/64 loss: -0.0979013442993164
Batch 13/64 loss: -0.07581931352615356
Batch 14/64 loss: -0.08790040016174316
Batch 15/64 loss: -0.06896865367889404
Batch 16/64 loss: -0.03728795051574707
Batch 17/64 loss: -0.04565167427062988
Batch 18/64 loss: -0.07613009214401245
Batch 19/64 loss: -0.053879737854003906
Batch 20/64 loss: -0.06461077928543091
Batch 21/64 loss: -0.07789289951324463
Batch 22/64 loss: -0.07043635845184326
Batch 23/64 loss: -0.08515006303787231
Batch 24/64 loss: -0.06835931539535522
Batch 25/64 loss: -0.0286293625831604
Batch 26/64 loss: -0.046488165855407715
Batch 27/64 loss: -0.07001429796218872
Batch 28/64 loss: -0.03382551670074463
Batch 29/64 loss: -0.08233082294464111
Batch 30/64 loss: -0.07063573598861694
Batch 31/64 loss: -0.07135593891143799
Batch 32/64 loss: -0.062137484550476074
Batch 33/64 loss: -0.045852839946746826
Batch 34/64 loss: -0.058097124099731445
Batch 35/64 loss: -0.07588636875152588
Batch 36/64 loss: -0.06129658222198486
Batch 37/64 loss: -0.06986206769943237
Batch 38/64 loss: -0.07307654619216919
Batch 39/64 loss: -0.07347822189331055
Batch 40/64 loss: -0.07727539539337158
Batch 41/64 loss: -0.018633663654327393
Batch 42/64 loss: -0.07432126998901367
Batch 43/64 loss: -0.058444082736968994
Batch 44/64 loss: -0.05818796157836914
Batch 45/64 loss: -0.07206988334655762
Batch 46/64 loss: -0.05788475275039673
Batch 47/64 loss: -0.044744789600372314
Batch 48/64 loss: -0.05744808912277222
Batch 49/64 loss: -0.045134663581848145
Batch 50/64 loss: -0.05740201473236084
Batch 51/64 loss: -0.052056729793548584
Batch 52/64 loss: -0.05457305908203125
Batch 53/64 loss: -0.05395829677581787
Batch 54/64 loss: -0.06192070245742798
Batch 55/64 loss: -0.04463207721710205
Batch 56/64 loss: -0.08789002895355225
Batch 57/64 loss: -0.07692921161651611
Batch 58/64 loss: -0.05219531059265137
Batch 59/64 loss: -0.053188323974609375
Batch 60/64 loss: -0.04138338565826416
Batch 61/64 loss: -0.05634874105453491
Batch 62/64 loss: -0.05311858654022217
Batch 63/64 loss: -0.033400118350982666
Batch 64/64 loss: -0.08089423179626465
Epoch 223  Train loss: -0.0619562494988535  Val loss: 0.048548390570375105
Epoch 224
-------------------------------
Batch 1/64 loss: -0.07249295711517334
Batch 2/64 loss: -0.04789459705352783
Batch 3/64 loss: -0.08059972524642944
Batch 4/64 loss: -0.042612552642822266
Batch 5/64 loss: -0.03457629680633545
Batch 6/64 loss: -0.06747221946716309
Batch 7/64 loss: -0.03830277919769287
Batch 8/64 loss: -0.07396572828292847
Batch 9/64 loss: -0.05428123474121094
Batch 10/64 loss: -0.0540732741355896
Batch 11/64 loss: -0.08474624156951904
Batch 12/64 loss: -0.08442747592926025
Batch 13/64 loss: -0.073356032371521
Batch 14/64 loss: -0.055894315242767334
Batch 15/64 loss: -0.06111776828765869
Batch 16/64 loss: -0.045176684856414795
Batch 17/64 loss: -0.03239130973815918
Batch 18/64 loss: -0.08226799964904785
Batch 19/64 loss: -0.06634342670440674
Batch 20/64 loss: -0.07269680500030518
Batch 21/64 loss: -0.07386338710784912
Batch 22/64 loss: -0.06942325830459595
Batch 23/64 loss: -0.08534431457519531
Batch 24/64 loss: -0.06515473127365112
Batch 25/64 loss: -0.08374190330505371
Batch 26/64 loss: -0.05604755878448486
Batch 27/64 loss: -0.0724073052406311
Batch 28/64 loss: -0.06925070285797119
Batch 29/64 loss: -0.049261510372161865
Batch 30/64 loss: -0.04415786266326904
Batch 31/64 loss: -0.0776556134223938
Batch 32/64 loss: -0.06323230266571045
Batch 33/64 loss: -0.06607890129089355
Batch 34/64 loss: -0.05225253105163574
Batch 35/64 loss: -0.06133401393890381
Batch 36/64 loss: -0.062297284603118896
Batch 37/64 loss: -0.06899547576904297
Batch 38/64 loss: -0.07318830490112305
Batch 39/64 loss: -0.06575661897659302
Batch 40/64 loss: -0.04990023374557495
Batch 41/64 loss: -0.053730130195617676
Batch 42/64 loss: -0.08593744039535522
Batch 43/64 loss: -0.06900614500045776
Batch 44/64 loss: -0.07732534408569336
Batch 45/64 loss: -0.05349928140640259
Batch 46/64 loss: -0.07205510139465332
Batch 47/64 loss: -0.06474626064300537
Batch 48/64 loss: -0.04842352867126465
Batch 49/64 loss: -0.07240355014801025
Batch 50/64 loss: -0.06250733137130737
Batch 51/64 loss: -0.06374615430831909
Batch 52/64 loss: -0.040048837661743164
Batch 53/64 loss: -0.074043869972229
Batch 54/64 loss: -0.07408183813095093
Batch 55/64 loss: -0.0641564130783081
Batch 56/64 loss: -0.0459059476852417
Batch 57/64 loss: -0.04892146587371826
Batch 58/64 loss: -0.06764674186706543
Batch 59/64 loss: -0.07685106992721558
Batch 60/64 loss: -0.06509315967559814
Batch 61/64 loss: -0.07821178436279297
Batch 62/64 loss: -0.0693168044090271
Batch 63/64 loss: -0.06416290998458862
Batch 64/64 loss: -0.0720563530921936
Epoch 224  Train loss: -0.06399837825812546  Val loss: 0.04443889651511543
Epoch 225
-------------------------------
Batch 1/64 loss: -0.07317572832107544
Batch 2/64 loss: -0.07730048894882202
Batch 3/64 loss: -0.050974249839782715
Batch 4/64 loss: -0.08087670803070068
Batch 5/64 loss: -0.06483709812164307
Batch 6/64 loss: -0.07076162099838257
Batch 7/64 loss: -0.08715766668319702
Batch 8/64 loss: -0.07357984781265259
Batch 9/64 loss: -0.07232481241226196
Batch 10/64 loss: -0.07277381420135498
Batch 11/64 loss: -0.06159400939941406
Batch 12/64 loss: -0.07256019115447998
Batch 13/64 loss: -0.05618083477020264
Batch 14/64 loss: -0.05470627546310425
Batch 15/64 loss: -0.06796926259994507
Batch 16/64 loss: -0.04804950952529907
Batch 17/64 loss: -0.07903444766998291
Batch 18/64 loss: -0.06574571132659912
Batch 19/64 loss: -0.052268147468566895
Batch 20/64 loss: -0.04478490352630615
Batch 21/64 loss: -0.04446357488632202
Batch 22/64 loss: -0.054895102977752686
Batch 23/64 loss: -0.048001885414123535
Batch 24/64 loss: -0.07270640134811401
Batch 25/64 loss: -0.054858624935150146
Batch 26/64 loss: -0.055123746395111084
Batch 27/64 loss: -0.06000876426696777
Batch 28/64 loss: -0.06448549032211304
Batch 29/64 loss: -0.07825493812561035
Batch 30/64 loss: -0.08090990781784058
Batch 31/64 loss: -0.05381578207015991
Batch 32/64 loss: -0.07496148347854614
Batch 33/64 loss: -0.08163106441497803
Batch 34/64 loss: -0.06272488832473755
Batch 35/64 loss: -0.08612775802612305
Batch 36/64 loss: -0.07474267482757568
Batch 37/64 loss: -0.07232093811035156
Batch 38/64 loss: -0.07571685314178467
Batch 39/64 loss: -0.07745105028152466
Batch 40/64 loss: -0.03807675838470459
Batch 41/64 loss: -0.06587833166122437
Batch 42/64 loss: -0.07104277610778809
Batch 43/64 loss: -0.06733620166778564
Batch 44/64 loss: -0.05045884847640991
Batch 45/64 loss: -0.05788236856460571
Batch 46/64 loss: -0.0613216757774353
Batch 47/64 loss: -0.08010637760162354
Batch 48/64 loss: -0.06420332193374634
Batch 49/64 loss: -0.07375627756118774
Batch 50/64 loss: -0.09797024726867676
Batch 51/64 loss: -0.08411532640457153
Batch 52/64 loss: -0.07314956188201904
Batch 53/64 loss: -0.06599938869476318
Batch 54/64 loss: -0.05779021978378296
Batch 55/64 loss: -0.06362038850784302
Batch 56/64 loss: -0.07733899354934692
Batch 57/64 loss: -0.06869977712631226
Batch 58/64 loss: -0.023795604705810547
Batch 59/64 loss: -0.047877728939056396
Batch 60/64 loss: -0.06197798252105713
Batch 61/64 loss: -0.03770172595977783
Batch 62/64 loss: -0.028979599475860596
Batch 63/64 loss: -0.07563334703445435
Batch 64/64 loss: -0.036556899547576904
Epoch 225  Train loss: -0.0646586158696343  Val loss: 0.04470650457434638
Epoch 226
-------------------------------
Batch 1/64 loss: -0.06786239147186279
Batch 2/64 loss: -0.0816996693611145
Batch 3/64 loss: -0.07149815559387207
Batch 4/64 loss: -0.07774299383163452
Batch 5/64 loss: -0.07876324653625488
Batch 6/64 loss: -0.06966638565063477
Batch 7/64 loss: -0.06674730777740479
Batch 8/64 loss: -0.07030993700027466
Batch 9/64 loss: -0.07099008560180664
Batch 10/64 loss: -0.04536694288253784
Batch 11/64 loss: -0.059105873107910156
Batch 12/64 loss: -0.0270155668258667
Batch 13/64 loss: -0.07464271783828735
Batch 14/64 loss: -0.0639600157737732
Batch 15/64 loss: -0.044776737689971924
Batch 16/64 loss: -0.06457966566085815
Batch 17/64 loss: -0.05410325527191162
Batch 18/64 loss: -0.08382993936538696
Batch 19/64 loss: -0.07504725456237793
Batch 20/64 loss: -0.061808109283447266
Batch 21/64 loss: -0.05536586046218872
Batch 22/64 loss: -0.061008453369140625
Batch 23/64 loss: -0.07261627912521362
Batch 24/64 loss: -0.055793702602386475
Batch 25/64 loss: -0.08326280117034912
Batch 26/64 loss: -0.06798380613327026
Batch 27/64 loss: -0.05040937662124634
Batch 28/64 loss: -0.06269121170043945
Batch 29/64 loss: -0.051297903060913086
Batch 30/64 loss: -0.062290847301483154
Batch 31/64 loss: -0.043856680393218994
Batch 32/64 loss: -0.04923450946807861
Batch 33/64 loss: -0.04595893621444702
Batch 34/64 loss: -0.07285445928573608
Batch 35/64 loss: -0.06899130344390869
Batch 36/64 loss: -0.05243837833404541
Batch 37/64 loss: -0.07716667652130127
Batch 38/64 loss: -0.07874661684036255
Batch 39/64 loss: -0.06753742694854736
Batch 40/64 loss: -0.07309651374816895
Batch 41/64 loss: -0.029322803020477295
Batch 42/64 loss: -0.07535791397094727
Batch 43/64 loss: -0.0477217435836792
Batch 44/64 loss: -0.07143574953079224
Batch 45/64 loss: -0.05764472484588623
Batch 46/64 loss: -0.08674097061157227
Batch 47/64 loss: -0.06927043199539185
Batch 48/64 loss: -0.0593419075012207
Batch 49/64 loss: -0.04011613130569458
Batch 50/64 loss: -0.0437278151512146
Batch 51/64 loss: -0.0690426230430603
Batch 52/64 loss: -0.06733673810958862
Batch 53/64 loss: -0.0561489462852478
Batch 54/64 loss: -0.05511784553527832
Batch 55/64 loss: -0.07126140594482422
Batch 56/64 loss: -0.06476885080337524
Batch 57/64 loss: -0.08718633651733398
Batch 58/64 loss: -0.07623755931854248
Batch 59/64 loss: -0.057728707790374756
Batch 60/64 loss: -0.056478023529052734
Batch 61/64 loss: -0.07169240713119507
Batch 62/64 loss: -0.09149014949798584
Batch 63/64 loss: -0.04248744249343872
Batch 64/64 loss: -0.02036285400390625
Epoch 226  Train loss: -0.06313799783295276  Val loss: 0.045083635041803835
Epoch 227
-------------------------------
Batch 1/64 loss: -0.05409824848175049
Batch 2/64 loss: -0.05300939083099365
Batch 3/64 loss: -0.06539785861968994
Batch 4/64 loss: -0.07301062345504761
Batch 5/64 loss: -0.06840252876281738
Batch 6/64 loss: -0.07714557647705078
Batch 7/64 loss: -0.08289498090744019
Batch 8/64 loss: -0.06063270568847656
Batch 9/64 loss: -0.05818676948547363
Batch 10/64 loss: -0.08055609464645386
Batch 11/64 loss: -0.06942665576934814
Batch 12/64 loss: -0.06710934638977051
Batch 13/64 loss: -0.06670570373535156
Batch 14/64 loss: -0.05566084384918213
Batch 15/64 loss: -0.08820343017578125
Batch 16/64 loss: -0.08431017398834229
Batch 17/64 loss: -0.07273662090301514
Batch 18/64 loss: -0.041013360023498535
Batch 19/64 loss: -0.0743265151977539
Batch 20/64 loss: -0.09100610017776489
Batch 21/64 loss: -0.06225287914276123
Batch 22/64 loss: -0.0629691481590271
Batch 23/64 loss: -0.043306708335876465
Batch 24/64 loss: -0.07284307479858398
Batch 25/64 loss: -0.05025506019592285
Batch 26/64 loss: -0.065956711769104
Batch 27/64 loss: -0.08119899034500122
Batch 28/64 loss: -0.08326166868209839
Batch 29/64 loss: -0.06822144985198975
Batch 30/64 loss: -0.07306170463562012
Batch 31/64 loss: -0.048403143882751465
Batch 32/64 loss: -0.061132609844207764
Batch 33/64 loss: -0.0758325457572937
Batch 34/64 loss: -0.07257348299026489
Batch 35/64 loss: -0.08099347352981567
Batch 36/64 loss: -0.0614924430847168
Batch 37/64 loss: -0.059637367725372314
Batch 38/64 loss: -0.044869184494018555
Batch 39/64 loss: -0.06323856115341187
Batch 40/64 loss: -0.08141958713531494
Batch 41/64 loss: -0.05787205696105957
Batch 42/64 loss: -0.06958812475204468
Batch 43/64 loss: -0.028233051300048828
Batch 44/64 loss: -0.05141949653625488
Batch 45/64 loss: -0.051970481872558594
Batch 46/64 loss: -0.07748883962631226
Batch 47/64 loss: -0.058680593967437744
Batch 48/64 loss: -0.06828540563583374
Batch 49/64 loss: -0.07741814851760864
Batch 50/64 loss: -0.05110651254653931
Batch 51/64 loss: -0.06269657611846924
Batch 52/64 loss: -0.04579287767410278
Batch 53/64 loss: -0.07077866792678833
Batch 54/64 loss: -0.06327927112579346
Batch 55/64 loss: -0.036196231842041016
Batch 56/64 loss: -0.06343233585357666
Batch 57/64 loss: -0.06426221132278442
Batch 58/64 loss: -0.0754135251045227
Batch 59/64 loss: -0.07978218793869019
Batch 60/64 loss: -0.06161034107208252
Batch 61/64 loss: -0.06307756900787354
Batch 62/64 loss: -0.06623399257659912
Batch 63/64 loss: -0.08173501491546631
Batch 64/64 loss: -0.07466310262680054
Epoch 227  Train loss: -0.06555457512537638  Val loss: 0.042821737499171515
Epoch 228
-------------------------------
Batch 1/64 loss: -0.06746619939804077
Batch 2/64 loss: -0.06810307502746582
Batch 3/64 loss: -0.08858567476272583
Batch 4/64 loss: -0.05723148584365845
Batch 5/64 loss: -0.07714724540710449
Batch 6/64 loss: -0.07584047317504883
Batch 7/64 loss: -0.07346981763839722
Batch 8/64 loss: -0.06537151336669922
Batch 9/64 loss: -0.0653260350227356
Batch 10/64 loss: -0.07641351222991943
Batch 11/64 loss: -0.07507014274597168
Batch 12/64 loss: -0.07020127773284912
Batch 13/64 loss: -0.049689650535583496
Batch 14/64 loss: -0.06542348861694336
Batch 15/64 loss: -0.04213672876358032
Batch 16/64 loss: -0.05464279651641846
Batch 17/64 loss: -0.05402863025665283
Batch 18/64 loss: -0.05827617645263672
Batch 19/64 loss: -0.07151669263839722
Batch 20/64 loss: -0.0839877724647522
Batch 21/64 loss: -0.023841023445129395
Batch 22/64 loss: -0.09141206741333008
Batch 23/64 loss: -0.06717079877853394
Batch 24/64 loss: -0.060039401054382324
Batch 25/64 loss: -0.052993178367614746
Batch 26/64 loss: -0.0722501277923584
Batch 27/64 loss: -0.05104488134384155
Batch 28/64 loss: -0.07249367237091064
Batch 29/64 loss: -0.0598447322845459
Batch 30/64 loss: -0.05881953239440918
Batch 31/64 loss: -0.05389910936355591
Batch 32/64 loss: -0.054786086082458496
Batch 33/64 loss: -0.05236160755157471
Batch 34/64 loss: -0.05816584825515747
Batch 35/64 loss: -0.07165658473968506
Batch 36/64 loss: -0.0667760968208313
Batch 37/64 loss: -0.06156611442565918
Batch 38/64 loss: -0.06348001956939697
Batch 39/64 loss: -0.05185377597808838
Batch 40/64 loss: -0.07404303550720215
Batch 41/64 loss: -0.07031059265136719
Batch 42/64 loss: -0.09661680459976196
Batch 43/64 loss: -0.07638716697692871
Batch 44/64 loss: -0.03960561752319336
Batch 45/64 loss: -0.07141768932342529
Batch 46/64 loss: -0.07842880487442017
Batch 47/64 loss: -0.06485122442245483
Batch 48/64 loss: -0.06705605983734131
Batch 49/64 loss: -0.07729458808898926
Batch 50/64 loss: -0.07639384269714355
Batch 51/64 loss: -0.0902361273765564
Batch 52/64 loss: -0.07813131809234619
Batch 53/64 loss: -0.05377459526062012
Batch 54/64 loss: -0.04741990566253662
Batch 55/64 loss: -0.046076953411102295
Batch 56/64 loss: -0.08296024799346924
Batch 57/64 loss: -0.04807543754577637
Batch 58/64 loss: -0.05101138353347778
Batch 59/64 loss: -0.052454590797424316
Batch 60/64 loss: -0.06971055269241333
Batch 61/64 loss: -0.056526899337768555
Batch 62/64 loss: -0.08205562829971313
Batch 63/64 loss: -0.08223819732666016
Batch 64/64 loss: -0.07488173246383667
Epoch 228  Train loss: -0.06546857427148257  Val loss: 0.043893472435548135
Epoch 229
-------------------------------
Batch 1/64 loss: -0.08133608102798462
Batch 2/64 loss: -0.08817815780639648
Batch 3/64 loss: -0.09572112560272217
Batch 4/64 loss: -0.07833540439605713
Batch 5/64 loss: -0.0827837586402893
Batch 6/64 loss: -0.07905369997024536
Batch 7/64 loss: -0.10030889511108398
Batch 8/64 loss: -0.06291007995605469
Batch 9/64 loss: -0.08630621433258057
Batch 10/64 loss: -0.09168994426727295
Batch 11/64 loss: -0.07251083850860596
Batch 12/64 loss: -0.06374573707580566
Batch 13/64 loss: -0.07077556848526001
Batch 14/64 loss: -0.08039617538452148
Batch 15/64 loss: -0.07371735572814941
Batch 16/64 loss: -0.06067901849746704
Batch 17/64 loss: -0.06327497959136963
Batch 18/64 loss: -0.08383333683013916
Batch 19/64 loss: -0.053845763206481934
Batch 20/64 loss: -0.047582268714904785
Batch 21/64 loss: -0.07711297273635864
Batch 22/64 loss: -0.022327840328216553
Batch 23/64 loss: -0.07279860973358154
Batch 24/64 loss: -0.061171650886535645
Batch 25/64 loss: -0.04452389478683472
Batch 26/64 loss: -0.06527519226074219
Batch 27/64 loss: -0.04216563701629639
Batch 28/64 loss: -0.0635785460472107
Batch 29/64 loss: -0.06804388761520386
Batch 30/64 loss: -0.05440783500671387
Batch 31/64 loss: -0.06981641054153442
Batch 32/64 loss: -0.07009667158126831
Batch 33/64 loss: -0.06691044569015503
Batch 34/64 loss: -0.07458233833312988
Batch 35/64 loss: -0.07870984077453613
Batch 36/64 loss: -0.08563590049743652
Batch 37/64 loss: -0.05899852514266968
Batch 38/64 loss: -0.09074223041534424
Batch 39/64 loss: -0.0600738525390625
Batch 40/64 loss: -0.04952901601791382
Batch 41/64 loss: -0.06728494167327881
Batch 42/64 loss: -0.06935405731201172
Batch 43/64 loss: -0.06291788816452026
Batch 44/64 loss: -0.07737743854522705
Batch 45/64 loss: -0.033930838108062744
Batch 46/64 loss: -0.03991711139678955
Batch 47/64 loss: -0.08673954010009766
Batch 48/64 loss: -0.06656122207641602
Batch 49/64 loss: -0.04692387580871582
Batch 50/64 loss: -0.04730498790740967
Batch 51/64 loss: -0.06549584865570068
Batch 52/64 loss: -0.052906334400177
Batch 53/64 loss: -0.055252134799957275
Batch 54/64 loss: -0.054990947246551514
Batch 55/64 loss: -0.047615647315979004
Batch 56/64 loss: -0.0553288459777832
Batch 57/64 loss: -0.059617698192596436
Batch 58/64 loss: -0.06654864549636841
Batch 59/64 loss: -0.06944334506988525
Batch 60/64 loss: -0.04782700538635254
Batch 61/64 loss: -0.048302650451660156
Batch 62/64 loss: -0.08201181888580322
Batch 63/64 loss: -0.08394372463226318
Batch 64/64 loss: -0.07949405908584595
Epoch 229  Train loss: -0.06652082812552358  Val loss: 0.04315023221510792
Epoch 230
-------------------------------
Batch 1/64 loss: -0.06356251239776611
Batch 2/64 loss: -0.09008437395095825
Batch 3/64 loss: -0.06710076332092285
Batch 4/64 loss: -0.07114309072494507
Batch 5/64 loss: -0.07706344127655029
Batch 6/64 loss: -0.06258946657180786
Batch 7/64 loss: -0.07399386167526245
Batch 8/64 loss: -0.06393826007843018
Batch 9/64 loss: -0.062456727027893066
Batch 10/64 loss: -0.05662888288497925
Batch 11/64 loss: -0.05783402919769287
Batch 12/64 loss: -0.09418457746505737
Batch 13/64 loss: -0.06773209571838379
Batch 14/64 loss: -0.0559614896774292
Batch 15/64 loss: -0.09641677141189575
Batch 16/64 loss: -0.05992835760116577
Batch 17/64 loss: -0.07431501150131226
Batch 18/64 loss: -0.06439828872680664
Batch 19/64 loss: -0.08595418930053711
Batch 20/64 loss: -0.07372921705245972
Batch 21/64 loss: -0.09098482131958008
Batch 22/64 loss: -0.06997841596603394
Batch 23/64 loss: -0.07404088973999023
Batch 24/64 loss: -0.08783072233200073
Batch 25/64 loss: -0.07344961166381836
Batch 26/64 loss: -0.06408417224884033
Batch 27/64 loss: -0.059469640254974365
Batch 28/64 loss: -0.03917968273162842
Batch 29/64 loss: -0.07124441862106323
Batch 30/64 loss: -0.05354511737823486
Batch 31/64 loss: -0.08162271976470947
Batch 32/64 loss: -0.056028664112091064
Batch 33/64 loss: -0.09108537435531616
Batch 34/64 loss: -0.069061279296875
Batch 35/64 loss: -0.06801825761795044
Batch 36/64 loss: -0.0622134804725647
Batch 37/64 loss: -0.07058960199356079
Batch 38/64 loss: -0.06356966495513916
Batch 39/64 loss: -0.07080459594726562
Batch 40/64 loss: -0.07031679153442383
Batch 41/64 loss: -0.05676865577697754
Batch 42/64 loss: -0.0816880464553833
Batch 43/64 loss: -0.0341566801071167
Batch 44/64 loss: -0.04968482255935669
Batch 45/64 loss: -0.0749618411064148
Batch 46/64 loss: -0.05899667739868164
Batch 47/64 loss: -0.0644073486328125
Batch 48/64 loss: -0.08454287052154541
Batch 49/64 loss: -0.09365534782409668
Batch 50/64 loss: -0.03597754240036011
Batch 51/64 loss: -0.05858832597732544
Batch 52/64 loss: -0.06961029767990112
Batch 53/64 loss: -0.06001460552215576
Batch 54/64 loss: -0.037665605545043945
Batch 55/64 loss: -0.07147902250289917
Batch 56/64 loss: -0.061500608921051025
Batch 57/64 loss: -0.07500535249710083
Batch 58/64 loss: -0.06685787439346313
Batch 59/64 loss: -0.05868422985076904
Batch 60/64 loss: -0.06878453493118286
Batch 61/64 loss: -0.08017498254776001
Batch 62/64 loss: -0.08176511526107788
Batch 63/64 loss: -0.07364797592163086
Batch 64/64 loss: -0.0198056697845459
Epoch 230  Train loss: -0.0677585246516209  Val loss: 0.04840081172300778
Epoch 231
-------------------------------
Batch 1/64 loss: -0.06452608108520508
Batch 2/64 loss: -0.07726079225540161
Batch 3/64 loss: -0.059530556201934814
Batch 4/64 loss: -0.06381767988204956
Batch 5/64 loss: -0.07042354345321655
Batch 6/64 loss: -0.05876427888870239
Batch 7/64 loss: -0.06975400447845459
Batch 8/64 loss: -0.05216854810714722
Batch 9/64 loss: -0.060661911964416504
Batch 10/64 loss: -0.06500917673110962
Batch 11/64 loss: -0.05818629264831543
Batch 12/64 loss: -0.06341636180877686
Batch 13/64 loss: -0.07318294048309326
Batch 14/64 loss: -0.07890141010284424
Batch 15/64 loss: -0.026238679885864258
Batch 16/64 loss: -0.08005404472351074
Batch 17/64 loss: -0.10533308982849121
Batch 18/64 loss: -0.0412898063659668
Batch 19/64 loss: -0.08463746309280396
Batch 20/64 loss: -0.07919418811798096
Batch 21/64 loss: -0.08212852478027344
Batch 22/64 loss: -0.0902329683303833
Batch 23/64 loss: -0.06756126880645752
Batch 24/64 loss: -0.06554096937179565
Batch 25/64 loss: -0.055591046810150146
Batch 26/64 loss: -0.04363381862640381
Batch 27/64 loss: -0.058294057846069336
Batch 28/64 loss: -0.06586331129074097
Batch 29/64 loss: -0.045094072818756104
Batch 30/64 loss: -0.061523258686065674
Batch 31/64 loss: -0.08215856552124023
Batch 32/64 loss: -0.060637831687927246
Batch 33/64 loss: -0.07057851552963257
Batch 34/64 loss: -0.04176652431488037
Batch 35/64 loss: -0.07121825218200684
Batch 36/64 loss: -0.06481307744979858
Batch 37/64 loss: -0.0825616717338562
Batch 38/64 loss: -0.059760093688964844
Batch 39/64 loss: -0.030754387378692627
Batch 40/64 loss: -0.04568982124328613
Batch 41/64 loss: -0.049048662185668945
Batch 42/64 loss: -0.0628085732460022
Batch 43/64 loss: -0.07707232236862183
Batch 44/64 loss: -0.07170265913009644
Batch 45/64 loss: -0.07849633693695068
Batch 46/64 loss: -0.0731278657913208
Batch 47/64 loss: -0.0941510796546936
Batch 48/64 loss: -0.07109284400939941
Batch 49/64 loss: -0.09566724300384521
Batch 50/64 loss: -0.07327550649642944
Batch 51/64 loss: -0.07989668846130371
Batch 52/64 loss: -0.049389421939849854
Batch 53/64 loss: -0.06473422050476074
Batch 54/64 loss: -0.04729181528091431
Batch 55/64 loss: -0.07234662771224976
Batch 56/64 loss: -0.08174699544906616
Batch 57/64 loss: -0.07837498188018799
Batch 58/64 loss: -0.06823337078094482
Batch 59/64 loss: -0.06263422966003418
Batch 60/64 loss: -0.08061742782592773
Batch 61/64 loss: -0.06800895929336548
Batch 62/64 loss: -0.06218445301055908
Batch 63/64 loss: -0.05519038438796997
Batch 64/64 loss: -0.07794153690338135
Epoch 231  Train loss: -0.06671845819435868  Val loss: 0.04542798766565487
Epoch 232
-------------------------------
Batch 1/64 loss: -0.06325805187225342
Batch 2/64 loss: -0.09556448459625244
Batch 3/64 loss: -0.06594264507293701
Batch 4/64 loss: -0.08300459384918213
Batch 5/64 loss: -0.0469285249710083
Batch 6/64 loss: -0.08354365825653076
Batch 7/64 loss: -0.05714142322540283
Batch 8/64 loss: -0.0634908676147461
Batch 9/64 loss: -0.06555831432342529
Batch 10/64 loss: -0.08059811592102051
Batch 11/64 loss: -0.03972524404525757
Batch 12/64 loss: -0.07474839687347412
Batch 13/64 loss: -0.05594003200531006
Batch 14/64 loss: -0.08083009719848633
Batch 15/64 loss: -0.0463942289352417
Batch 16/64 loss: -0.06947481632232666
Batch 17/64 loss: -0.060082197189331055
Batch 18/64 loss: -0.08495163917541504
Batch 19/64 loss: -0.08720260858535767
Batch 20/64 loss: -0.08101415634155273
Batch 21/64 loss: -0.07775610685348511
Batch 22/64 loss: -0.06685507297515869
Batch 23/64 loss: -0.09792494773864746
Batch 24/64 loss: -0.07146549224853516
Batch 25/64 loss: -0.06459379196166992
Batch 26/64 loss: -0.07046306133270264
Batch 27/64 loss: -0.03530120849609375
Batch 28/64 loss: -0.07440882921218872
Batch 29/64 loss: -0.06844645738601685
Batch 30/64 loss: -0.07057690620422363
Batch 31/64 loss: -0.05431121587753296
Batch 32/64 loss: -0.0784916877746582
Batch 33/64 loss: -0.038106679916381836
Batch 34/64 loss: -0.05977940559387207
Batch 35/64 loss: -0.04762756824493408
Batch 36/64 loss: -0.06519216299057007
Batch 37/64 loss: -0.05383044481277466
Batch 38/64 loss: -0.08380347490310669
Batch 39/64 loss: -0.05913281440734863
Batch 40/64 loss: -0.06877511739730835
Batch 41/64 loss: -0.06328296661376953
Batch 42/64 loss: -0.07171112298965454
Batch 43/64 loss: -0.08584713935852051
Batch 44/64 loss: -0.06419384479522705
Batch 45/64 loss: -0.08127927780151367
Batch 46/64 loss: -0.08218646049499512
Batch 47/64 loss: -0.07495748996734619
Batch 48/64 loss: -0.08085638284683228
Batch 49/64 loss: -0.04878300428390503
Batch 50/64 loss: -0.07339894771575928
Batch 51/64 loss: -0.059441566467285156
Batch 52/64 loss: -0.05811697244644165
Batch 53/64 loss: -0.06728506088256836
Batch 54/64 loss: -0.03393524885177612
Batch 55/64 loss: -0.08086782693862915
Batch 56/64 loss: -0.0702352523803711
Batch 57/64 loss: -0.0676911473274231
Batch 58/64 loss: -0.0827668309211731
Batch 59/64 loss: -0.059129178524017334
Batch 60/64 loss: -0.047012925148010254
Batch 61/64 loss: -0.07534962892532349
Batch 62/64 loss: -0.07298946380615234
Batch 63/64 loss: -0.06886768341064453
Batch 64/64 loss: -0.05925452709197998
Epoch 232  Train loss: -0.06755863311243993  Val loss: 0.04482566572956203
Epoch 233
-------------------------------
Batch 1/64 loss: -0.09177172183990479
Batch 2/64 loss: -0.052482545375823975
Batch 3/64 loss: -0.042469918727874756
Batch 4/64 loss: -0.08436417579650879
Batch 5/64 loss: -0.07330620288848877
Batch 6/64 loss: -0.08773398399353027
Batch 7/64 loss: -0.06137007474899292
Batch 8/64 loss: -0.0913277268409729
Batch 9/64 loss: -0.08175265789031982
Batch 10/64 loss: -0.07829439640045166
Batch 11/64 loss: -0.06917756795883179
Batch 12/64 loss: -0.08012139797210693
Batch 13/64 loss: -0.07161223888397217
Batch 14/64 loss: -0.08209913969039917
Batch 15/64 loss: -0.08559489250183105
Batch 16/64 loss: -0.05710864067077637
Batch 17/64 loss: -0.07400137186050415
Batch 18/64 loss: -0.08043944835662842
Batch 19/64 loss: -0.04516446590423584
Batch 20/64 loss: -0.059911370277404785
Batch 21/64 loss: -0.07769358158111572
Batch 22/64 loss: -0.06390547752380371
Batch 23/64 loss: -0.08014422655105591
Batch 24/64 loss: -0.053345322608947754
Batch 25/64 loss: -0.060995280742645264
Batch 26/64 loss: -0.06752890348434448
Batch 27/64 loss: -0.0738985538482666
Batch 28/64 loss: -0.0599554181098938
Batch 29/64 loss: -0.09230589866638184
Batch 30/64 loss: -0.07947295904159546
Batch 31/64 loss: -0.08330053091049194
Batch 32/64 loss: -0.07473164796829224
Batch 33/64 loss: -0.09388256072998047
Batch 34/64 loss: -0.06452292203903198
Batch 35/64 loss: -0.026874899864196777
Batch 36/64 loss: -0.07880938053131104
Batch 37/64 loss: -0.07834810018539429
Batch 38/64 loss: -0.06690394878387451
Batch 39/64 loss: -0.07902860641479492
Batch 40/64 loss: -0.06969302892684937
Batch 41/64 loss: -0.06479763984680176
Batch 42/64 loss: -0.09226959943771362
Batch 43/64 loss: -0.07995092868804932
Batch 44/64 loss: -0.05218106508255005
Batch 45/64 loss: -0.03848886489868164
Batch 46/64 loss: -0.0636405348777771
Batch 47/64 loss: -0.030416488647460938
Batch 48/64 loss: -0.08695262670516968
Batch 49/64 loss: -0.07704389095306396
Batch 50/64 loss: -0.033746421337127686
Batch 51/64 loss: -0.06792217493057251
Batch 52/64 loss: -0.043058693408966064
Batch 53/64 loss: -0.06814330816268921
Batch 54/64 loss: -0.09119004011154175
Batch 55/64 loss: -0.04771614074707031
Batch 56/64 loss: -0.05141019821166992
Batch 57/64 loss: -0.0530242919921875
Batch 58/64 loss: -0.05932801961898804
Batch 59/64 loss: -0.07860207557678223
Batch 60/64 loss: -0.03114902973175049
Batch 61/64 loss: -0.07426154613494873
Batch 62/64 loss: -0.07272487878799438
Batch 63/64 loss: -0.053858399391174316
Batch 64/64 loss: -0.07410866022109985
Epoch 233  Train loss: -0.06812397709079818  Val loss: 0.04644329605233628
Epoch 234
-------------------------------
Batch 1/64 loss: -0.07017409801483154
Batch 2/64 loss: -0.07700151205062866
Batch 3/64 loss: -0.0428200364112854
Batch 4/64 loss: -0.0709526538848877
Batch 5/64 loss: -0.06809377670288086
Batch 6/64 loss: -0.06392514705657959
Batch 7/64 loss: -0.08768332004547119
Batch 8/64 loss: -0.0798301100730896
Batch 9/64 loss: -0.08910483121871948
Batch 10/64 loss: -0.09130734205245972
Batch 11/64 loss: -0.08130186796188354
Batch 12/64 loss: -0.06695675849914551
Batch 13/64 loss: -0.07454037666320801
Batch 14/64 loss: -0.09143519401550293
Batch 15/64 loss: -0.053098440170288086
Batch 16/64 loss: -0.07058584690093994
Batch 17/64 loss: -0.0626077651977539
Batch 18/64 loss: -0.057899415493011475
Batch 19/64 loss: -0.07068395614624023
Batch 20/64 loss: -0.09229201078414917
Batch 21/64 loss: -0.047685980796813965
Batch 22/64 loss: -0.07074928283691406
Batch 23/64 loss: -0.056151747703552246
Batch 24/64 loss: -0.08040189743041992
Batch 25/64 loss: -0.05144155025482178
Batch 26/64 loss: -0.06965148448944092
Batch 27/64 loss: -0.08169746398925781
Batch 28/64 loss: -0.04648900032043457
Batch 29/64 loss: -0.05761593580245972
Batch 30/64 loss: -0.08166611194610596
Batch 31/64 loss: -0.07183611392974854
Batch 32/64 loss: -0.07607358694076538
Batch 33/64 loss: -0.06672924757003784
Batch 34/64 loss: -0.08125436305999756
Batch 35/64 loss: -0.05136913061141968
Batch 36/64 loss: -0.046115756034851074
Batch 37/64 loss: -0.0724794864654541
Batch 38/64 loss: -0.05964207649230957
Batch 39/64 loss: -0.09132683277130127
Batch 40/64 loss: -0.07603371143341064
Batch 41/64 loss: -0.09346568584442139
Batch 42/64 loss: -0.08571857213973999
Batch 43/64 loss: -0.07844913005828857
Batch 44/64 loss: -0.05054539442062378
Batch 45/64 loss: -0.07160001993179321
Batch 46/64 loss: -0.07548767328262329
Batch 47/64 loss: -0.06440508365631104
Batch 48/64 loss: -0.05521279573440552
Batch 49/64 loss: -0.06890106201171875
Batch 50/64 loss: -0.07019305229187012
Batch 51/64 loss: -0.04151719808578491
Batch 52/64 loss: -0.07954716682434082
Batch 53/64 loss: -0.07836967706680298
Batch 54/64 loss: -0.03648686408996582
Batch 55/64 loss: -0.07095283269882202
Batch 56/64 loss: -0.05607438087463379
Batch 57/64 loss: -0.06447798013687134
Batch 58/64 loss: -0.04646313190460205
Batch 59/64 loss: -0.07794147729873657
Batch 60/64 loss: -0.06591147184371948
Batch 61/64 loss: -0.07960522174835205
Batch 62/64 loss: -0.0701218843460083
Batch 63/64 loss: -0.07549214363098145
Batch 64/64 loss: -0.04356902837753296
Epoch 234  Train loss: -0.06883642182630652  Val loss: 0.04594882411235796
Epoch 235
-------------------------------
Batch 1/64 loss: -0.06074213981628418
Batch 2/64 loss: -0.07054096460342407
Batch 3/64 loss: -0.07147294282913208
Batch 4/64 loss: -0.046701669692993164
Batch 5/64 loss: -0.0807948112487793
Batch 6/64 loss: -0.05910855531692505
Batch 7/64 loss: -0.0848734974861145
Batch 8/64 loss: -0.07008165121078491
Batch 9/64 loss: -0.07102370262145996
Batch 10/64 loss: -0.045892536640167236
Batch 11/64 loss: -0.07817882299423218
Batch 12/64 loss: -0.06257760524749756
Batch 13/64 loss: -0.0508994460105896
Batch 14/64 loss: -0.06462723016738892
Batch 15/64 loss: -0.0636359453201294
Batch 16/64 loss: -0.08305370807647705
Batch 17/64 loss: -0.0710453987121582
Batch 18/64 loss: -0.07090258598327637
Batch 19/64 loss: -0.07016456127166748
Batch 20/64 loss: -0.09014570713043213
Batch 21/64 loss: -0.04995220899581909
Batch 22/64 loss: -0.06945836544036865
Batch 23/64 loss: -0.0661582350730896
Batch 24/64 loss: -0.06013989448547363
Batch 25/64 loss: -0.06234055757522583
Batch 26/64 loss: -0.06486505270004272
Batch 27/64 loss: -0.07927238941192627
Batch 28/64 loss: -0.06688982248306274
Batch 29/64 loss: -0.0514032244682312
Batch 30/64 loss: -0.0565410852432251
Batch 31/64 loss: -0.07301509380340576
Batch 32/64 loss: -0.07622647285461426
Batch 33/64 loss: -0.056136369705200195
Batch 34/64 loss: -0.08072108030319214
Batch 35/64 loss: -0.06958270072937012
Batch 36/64 loss: -0.10287487506866455
Batch 37/64 loss: -0.08663314580917358
Batch 38/64 loss: -0.06456071138381958
Batch 39/64 loss: -0.06767433881759644
Batch 40/64 loss: -0.08395934104919434
Batch 41/64 loss: -0.08676707744598389
Batch 42/64 loss: -0.06098949909210205
Batch 43/64 loss: -0.058343350887298584
Batch 44/64 loss: -0.06167101860046387
Batch 45/64 loss: -0.04681837558746338
Batch 46/64 loss: -0.026266515254974365
Batch 47/64 loss: -0.052354633808135986
Batch 48/64 loss: -0.06705677509307861
Batch 49/64 loss: -0.06988203525543213
Batch 50/64 loss: -0.07268399000167847
Batch 51/64 loss: -0.0516817569732666
Batch 52/64 loss: -0.07043075561523438
Batch 53/64 loss: -0.07443475723266602
Batch 54/64 loss: -0.087668776512146
Batch 55/64 loss: -0.08359330892562866
Batch 56/64 loss: -0.05953347682952881
Batch 57/64 loss: -0.07498764991760254
Batch 58/64 loss: -0.06241577863693237
Batch 59/64 loss: -0.07703971862792969
Batch 60/64 loss: -0.0456089973449707
Batch 61/64 loss: -0.06817883253097534
Batch 62/64 loss: -0.061998188495635986
Batch 63/64 loss: -0.051982998847961426
Batch 64/64 loss: -0.06820350885391235
Epoch 235  Train loss: -0.06711230348138249  Val loss: 0.046444932210076714
Epoch 236
-------------------------------
Batch 1/64 loss: -0.06728827953338623
Batch 2/64 loss: -0.04982483386993408
Batch 3/64 loss: -0.0908057689666748
Batch 4/64 loss: -0.06120288372039795
Batch 5/64 loss: -0.09119516611099243
Batch 6/64 loss: -0.074241042137146
Batch 7/64 loss: -0.08340752124786377
Batch 8/64 loss: -0.06860685348510742
Batch 9/64 loss: -0.07840430736541748
Batch 10/64 loss: -0.0614619255065918
Batch 11/64 loss: -0.07829934358596802
Batch 12/64 loss: -0.07042133808135986
Batch 13/64 loss: -0.07706248760223389
Batch 14/64 loss: -0.03866314888000488
Batch 15/64 loss: -0.05714273452758789
Batch 16/64 loss: -0.07216799259185791
Batch 17/64 loss: -0.07015025615692139
Batch 18/64 loss: -0.07987689971923828
Batch 19/64 loss: -0.07913714647293091
Batch 20/64 loss: -0.06265032291412354
Batch 21/64 loss: -0.061579346656799316
Batch 22/64 loss: -0.08389168977737427
Batch 23/64 loss: -0.08028733730316162
Batch 24/64 loss: -0.09146648645401001
Batch 25/64 loss: -0.07432204484939575
Batch 26/64 loss: -0.0593375563621521
Batch 27/64 loss: -0.04243576526641846
Batch 28/64 loss: -0.057206928730010986
Batch 29/64 loss: -0.06409966945648193
Batch 30/64 loss: -0.06264728307723999
Batch 31/64 loss: -0.06325817108154297
Batch 32/64 loss: -0.06286615133285522
Batch 33/64 loss: -0.07981419563293457
Batch 34/64 loss: -0.07328027486801147
Batch 35/64 loss: -0.07636934518814087
Batch 36/64 loss: -0.03682875633239746
Batch 37/64 loss: -0.06336522102355957
Batch 38/64 loss: -0.07165002822875977
Batch 39/64 loss: -0.09253251552581787
Batch 40/64 loss: -0.05150270462036133
Batch 41/64 loss: -0.07067370414733887
Batch 42/64 loss: -0.04802173376083374
Batch 43/64 loss: -0.07869911193847656
Batch 44/64 loss: -0.07813107967376709
Batch 45/64 loss: -0.08286702632904053
Batch 46/64 loss: -0.08059102296829224
Batch 47/64 loss: -0.07892245054244995
Batch 48/64 loss: -0.09864437580108643
Batch 49/64 loss: -0.07506531476974487
Batch 50/64 loss: -0.04874616861343384
Batch 51/64 loss: -0.08130931854248047
Batch 52/64 loss: -0.04630690813064575
Batch 53/64 loss: -0.0784793496131897
Batch 54/64 loss: -0.06143450736999512
Batch 55/64 loss: -0.0853073000907898
Batch 56/64 loss: -0.0702088475227356
Batch 57/64 loss: -0.04091763496398926
Batch 58/64 loss: -0.059136927127838135
Batch 59/64 loss: -0.07448673248291016
Batch 60/64 loss: -0.07274818420410156
Batch 61/64 loss: -0.07272958755493164
Batch 62/64 loss: -0.05234074592590332
Batch 63/64 loss: -0.03996926546096802
Batch 64/64 loss: -0.03722125291824341
Epoch 236  Train loss: -0.06846125429751826  Val loss: 0.04547057323849078
Epoch 237
-------------------------------
Batch 1/64 loss: -0.07290017604827881
Batch 2/64 loss: -0.06613588333129883
Batch 3/64 loss: -0.06671315431594849
Batch 4/64 loss: -0.08709335327148438
Batch 5/64 loss: -0.09729158878326416
Batch 6/64 loss: -0.08414685726165771
Batch 7/64 loss: -0.09241366386413574
Batch 8/64 loss: -0.08384144306182861
Batch 9/64 loss: -0.10783463716506958
Batch 10/64 loss: -0.04015141725540161
Batch 11/64 loss: -0.08793950080871582
Batch 12/64 loss: -0.08267354965209961
Batch 13/64 loss: -0.09825915098190308
Batch 14/64 loss: -0.07322710752487183
Batch 15/64 loss: -0.08075428009033203
Batch 16/64 loss: -0.08359432220458984
Batch 17/64 loss: -0.05627399682998657
Batch 18/64 loss: -0.09118014574050903
Batch 19/64 loss: -0.09471923112869263
Batch 20/64 loss: -0.05612415075302124
Batch 21/64 loss: -0.0648040771484375
Batch 22/64 loss: -0.05931901931762695
Batch 23/64 loss: -0.066653311252594
Batch 24/64 loss: -0.08047741651535034
Batch 25/64 loss: -0.08059811592102051
Batch 26/64 loss: -0.06249570846557617
Batch 27/64 loss: -0.07112252712249756
Batch 28/64 loss: -0.03809398412704468
Batch 29/64 loss: -0.06344592571258545
Batch 30/64 loss: -0.052956342697143555
Batch 31/64 loss: -0.08344411849975586
Batch 32/64 loss: -0.04044389724731445
Batch 33/64 loss: -0.02689385414123535
Batch 34/64 loss: -0.050913095474243164
Batch 35/64 loss: -0.061748504638671875
Batch 36/64 loss: -0.055596232414245605
Batch 37/64 loss: -0.08704423904418945
Batch 38/64 loss: -0.08053481578826904
Batch 39/64 loss: -0.06478911638259888
Batch 40/64 loss: -0.08325332403182983
Batch 41/64 loss: -0.07331544160842896
Batch 42/64 loss: -0.06293636560440063
Batch 43/64 loss: -0.03956657648086548
Batch 44/64 loss: -0.07244384288787842
Batch 45/64 loss: -0.058970749378204346
Batch 46/64 loss: -0.06250107288360596
Batch 47/64 loss: -0.07811093330383301
Batch 48/64 loss: -0.08480072021484375
Batch 49/64 loss: -0.061153531074523926
Batch 50/64 loss: -0.06888675689697266
Batch 51/64 loss: -0.06136244535446167
Batch 52/64 loss: -0.0362621545791626
Batch 53/64 loss: -0.07196342945098877
Batch 54/64 loss: -0.058595359325408936
Batch 55/64 loss: -0.05928605794906616
Batch 56/64 loss: -0.057149648666381836
Batch 57/64 loss: -0.06838172674179077
Batch 58/64 loss: -0.060542285442352295
Batch 59/64 loss: -0.056955933570861816
Batch 60/64 loss: -0.06728947162628174
Batch 61/64 loss: -0.06745707988739014
Batch 62/64 loss: -0.0808141827583313
Batch 63/64 loss: -0.06459122896194458
Batch 64/64 loss: -0.08801943063735962
Epoch 237  Train loss: -0.06929014593947168  Val loss: 0.04319346954732416
Epoch 238
-------------------------------
Batch 1/64 loss: -0.0940089225769043
Batch 2/64 loss: -0.08081918954849243
Batch 3/64 loss: -0.05540108680725098
Batch 4/64 loss: -0.0697559118270874
Batch 5/64 loss: -0.05940711498260498
Batch 6/64 loss: -0.07469666004180908
Batch 7/64 loss: -0.05802375078201294
Batch 8/64 loss: -0.0687793493270874
Batch 9/64 loss: -0.04394954442977905
Batch 10/64 loss: -0.06593436002731323
Batch 11/64 loss: -0.08460283279418945
Batch 12/64 loss: -0.06662899255752563
Batch 13/64 loss: -0.08103591203689575
Batch 14/64 loss: -0.07782602310180664
Batch 15/64 loss: -0.050753772258758545
Batch 16/64 loss: -0.08124178647994995
Batch 17/64 loss: -0.07538330554962158
Batch 18/64 loss: -0.07157039642333984
Batch 19/64 loss: -0.06487113237380981
Batch 20/64 loss: -0.09979629516601562
Batch 21/64 loss: -0.07972526550292969
Batch 22/64 loss: -0.06467431783676147
Batch 23/64 loss: -0.06583952903747559
Batch 24/64 loss: -0.07673019170761108
Batch 25/64 loss: -0.054543137550354004
Batch 26/64 loss: -0.0539630651473999
Batch 27/64 loss: -0.09341681003570557
Batch 28/64 loss: -0.07876783609390259
Batch 29/64 loss: -0.08631056547164917
Batch 30/64 loss: -0.06814408302307129
Batch 31/64 loss: -0.058824002742767334
Batch 32/64 loss: -0.059742093086242676
Batch 33/64 loss: -0.08677375316619873
Batch 34/64 loss: -0.06171596050262451
Batch 35/64 loss: -0.07925474643707275
Batch 36/64 loss: -0.08384501934051514
Batch 37/64 loss: -0.07908463478088379
Batch 38/64 loss: -0.0665212869644165
Batch 39/64 loss: -0.09044599533081055
Batch 40/64 loss: -0.06783747673034668
Batch 41/64 loss: -0.04603844881057739
Batch 42/64 loss: -0.07638931274414062
Batch 43/64 loss: -0.09312063455581665
Batch 44/64 loss: -0.08055537939071655
Batch 45/64 loss: -0.07792305946350098
Batch 46/64 loss: -0.0745323896408081
Batch 47/64 loss: -0.06858950853347778
Batch 48/64 loss: -0.024492621421813965
Batch 49/64 loss: -0.052191853523254395
Batch 50/64 loss: -0.05770760774612427
Batch 51/64 loss: -0.07006990909576416
Batch 52/64 loss: -0.05284607410430908
Batch 53/64 loss: -0.05924856662750244
Batch 54/64 loss: -0.05054640769958496
Batch 55/64 loss: -0.08204936981201172
Batch 56/64 loss: -0.09034663438796997
Batch 57/64 loss: -0.07952374219894409
Batch 58/64 loss: -0.05437147617340088
Batch 59/64 loss: -0.08000969886779785
Batch 60/64 loss: -0.06205618381500244
Batch 61/64 loss: -0.06699937582015991
Batch 62/64 loss: -0.05917125940322876
Batch 63/64 loss: -0.07766884565353394
Batch 64/64 loss: -0.04473280906677246
Epoch 238  Train loss: -0.06981402471953747  Val loss: 0.043122164981881365
Epoch 239
-------------------------------
Batch 1/64 loss: -0.04095560312271118
Batch 2/64 loss: -0.04993486404418945
Batch 3/64 loss: -0.04669159650802612
Batch 4/64 loss: -0.08790862560272217
Batch 5/64 loss: -0.062613844871521
Batch 6/64 loss: -0.07848954200744629
Batch 7/64 loss: -0.06884902715682983
Batch 8/64 loss: -0.07219833135604858
Batch 9/64 loss: -0.07034647464752197
Batch 10/64 loss: -0.08394670486450195
Batch 11/64 loss: -0.09158998727798462
Batch 12/64 loss: -0.07643085718154907
Batch 13/64 loss: -0.04716956615447998
Batch 14/64 loss: -0.10060995817184448
Batch 15/64 loss: -0.08395743370056152
Batch 16/64 loss: -0.07594895362854004
Batch 17/64 loss: -0.08627033233642578
Batch 18/64 loss: -0.07986032962799072
Batch 19/64 loss: -0.051425933837890625
Batch 20/64 loss: -0.075675368309021
Batch 21/64 loss: -0.062361061573028564
Batch 22/64 loss: -0.09504461288452148
Batch 23/64 loss: -0.050341129302978516
Batch 24/64 loss: -0.09431338310241699
Batch 25/64 loss: -0.05372011661529541
Batch 26/64 loss: -0.07373666763305664
Batch 27/64 loss: -0.08874267339706421
Batch 28/64 loss: -0.05999457836151123
Batch 29/64 loss: -0.05341297388076782
Batch 30/64 loss: -0.01059269905090332
Batch 31/64 loss: -0.045005857944488525
Batch 32/64 loss: -0.05394262075424194
Batch 33/64 loss: -0.08254480361938477
Batch 34/64 loss: -0.07038140296936035
Batch 35/64 loss: -0.07880878448486328
Batch 36/64 loss: -0.07550477981567383
Batch 37/64 loss: -0.08998262882232666
Batch 38/64 loss: -0.044078826904296875
Batch 39/64 loss: -0.06077170372009277
Batch 40/64 loss: -0.08228367567062378
Batch 41/64 loss: -0.04646807909011841
Batch 42/64 loss: -0.07242095470428467
Batch 43/64 loss: -0.07912606000900269
Batch 44/64 loss: -0.08376580476760864
Batch 45/64 loss: -0.05020183324813843
Batch 46/64 loss: -0.08560758829116821
Batch 47/64 loss: -0.06417232751846313
Batch 48/64 loss: -0.09871089458465576
Batch 49/64 loss: -0.09255141019821167
Batch 50/64 loss: -0.06527829170227051
Batch 51/64 loss: -0.07216852903366089
Batch 52/64 loss: -0.07786506414413452
Batch 53/64 loss: -0.05066704750061035
Batch 54/64 loss: -0.043419599533081055
Batch 55/64 loss: -0.05426025390625
Batch 56/64 loss: -0.06657707691192627
Batch 57/64 loss: -0.06573259830474854
Batch 58/64 loss: -0.06596934795379639
Batch 59/64 loss: -0.06803053617477417
Batch 60/64 loss: -0.09741091728210449
Batch 61/64 loss: -0.0840064287185669
Batch 62/64 loss: -0.08190518617630005
Batch 63/64 loss: -0.07603609561920166
Batch 64/64 loss: -0.0963817834854126
Epoch 239  Train loss: -0.07013453455532298  Val loss: 0.04446551009142112
Epoch 240
-------------------------------
Batch 1/64 loss: -0.06520074605941772
Batch 2/64 loss: -0.06981515884399414
Batch 3/64 loss: -0.08421671390533447
Batch 4/64 loss: -0.0801006555557251
Batch 5/64 loss: -0.08987653255462646
Batch 6/64 loss: -0.07973223924636841
Batch 7/64 loss: -0.08168268203735352
Batch 8/64 loss: -0.09762674570083618
Batch 9/64 loss: -0.08031511306762695
Batch 10/64 loss: -0.04738748073577881
Batch 11/64 loss: -0.08445268869400024
Batch 12/64 loss: -0.06801366806030273
Batch 13/64 loss: -0.07362854480743408
Batch 14/64 loss: -0.07623076438903809
Batch 15/64 loss: -0.09012758731842041
Batch 16/64 loss: -0.06252932548522949
Batch 17/64 loss: -0.04844146966934204
Batch 18/64 loss: -0.057821810245513916
Batch 19/64 loss: -0.06303119659423828
Batch 20/64 loss: -0.06453543901443481
Batch 21/64 loss: -0.06524759531021118
Batch 22/64 loss: -0.061957597732543945
Batch 23/64 loss: -0.08468413352966309
Batch 24/64 loss: -0.0836825966835022
Batch 25/64 loss: -0.08386719226837158
Batch 26/64 loss: -0.05711561441421509
Batch 27/64 loss: -0.0716925859451294
Batch 28/64 loss: -0.06294834613800049
Batch 29/64 loss: -0.04070347547531128
Batch 30/64 loss: -0.08341825008392334
Batch 31/64 loss: -0.08565068244934082
Batch 32/64 loss: -0.07259047031402588
Batch 33/64 loss: -0.08448797464370728
Batch 34/64 loss: -0.06735587120056152
Batch 35/64 loss: -0.06780409812927246
Batch 36/64 loss: -0.05930829048156738
Batch 37/64 loss: -0.0874146819114685
Batch 38/64 loss: -0.054499924182891846
Batch 39/64 loss: -0.07951754331588745
Batch 40/64 loss: -0.10314488410949707
Batch 41/64 loss: -0.06823396682739258
Batch 42/64 loss: -0.09156012535095215
Batch 43/64 loss: -0.07038259506225586
Batch 44/64 loss: -0.0598752498626709
Batch 45/64 loss: -0.07339394092559814
Batch 46/64 loss: -0.0861392617225647
Batch 47/64 loss: -0.06896406412124634
Batch 48/64 loss: -0.08774161338806152
Batch 49/64 loss: -0.08388960361480713
Batch 50/64 loss: -0.08380347490310669
Batch 51/64 loss: -0.08316755294799805
Batch 52/64 loss: -0.058440327644348145
Batch 53/64 loss: -0.06947863101959229
Batch 54/64 loss: -0.05712461471557617
Batch 55/64 loss: -0.053365230560302734
Batch 56/64 loss: -0.05518460273742676
Batch 57/64 loss: -0.08618587255477905
Batch 58/64 loss: -0.06199944019317627
Batch 59/64 loss: -0.07804322242736816
Batch 60/64 loss: -0.07300716638565063
Batch 61/64 loss: -0.0668637752532959
Batch 62/64 loss: -0.0714065432548523
Batch 63/64 loss: -0.06517601013183594
Batch 64/64 loss: -0.09054136276245117
Epoch 240  Train loss: -0.07283437298793419  Val loss: 0.04477468962521897
Epoch 241
-------------------------------
Batch 1/64 loss: -0.09224557876586914
Batch 2/64 loss: -0.098338782787323
Batch 3/64 loss: -0.09410858154296875
Batch 4/64 loss: -0.06383925676345825
Batch 5/64 loss: -0.08473598957061768
Batch 6/64 loss: -0.06868445873260498
Batch 7/64 loss: -0.08731979131698608
Batch 8/64 loss: -0.0923466682434082
Batch 9/64 loss: -0.07230222225189209
Batch 10/64 loss: -0.030498385429382324
Batch 11/64 loss: -0.07658421993255615
Batch 12/64 loss: -0.0704265832901001
Batch 13/64 loss: -0.08678120374679565
Batch 14/64 loss: -0.08249902725219727
Batch 15/64 loss: -0.0693930983543396
Batch 16/64 loss: -0.06671786308288574
Batch 17/64 loss: -0.07329124212265015
Batch 18/64 loss: -0.07214200496673584
Batch 19/64 loss: -0.04629981517791748
Batch 20/64 loss: -0.06628233194351196
Batch 21/64 loss: -0.06450808048248291
Batch 22/64 loss: -0.09540224075317383
Batch 23/64 loss: -0.0778348445892334
Batch 24/64 loss: -0.07881385087966919
Batch 25/64 loss: -0.08317488431930542
Batch 26/64 loss: -0.06095576286315918
Batch 27/64 loss: -0.0789453387260437
Batch 28/64 loss: -0.0840522050857544
Batch 29/64 loss: -0.054191529750823975
Batch 30/64 loss: -0.09146124124526978
Batch 31/64 loss: -0.0825338363647461
Batch 32/64 loss: -0.04416024684906006
Batch 33/64 loss: -0.0776401162147522
Batch 34/64 loss: -0.048031628131866455
Batch 35/64 loss: -0.09373807907104492
Batch 36/64 loss: -0.08029377460479736
Batch 37/64 loss: -0.07720452547073364
Batch 38/64 loss: -0.08308351039886475
Batch 39/64 loss: -0.08029919862747192
Batch 40/64 loss: -0.06713998317718506
Batch 41/64 loss: -0.063850998878479
Batch 42/64 loss: -0.06586778163909912
Batch 43/64 loss: -0.06523972749710083
Batch 44/64 loss: -0.0741114616394043
Batch 45/64 loss: -0.03760284185409546
Batch 46/64 loss: -0.05606484413146973
Batch 47/64 loss: -0.07653629779815674
Batch 48/64 loss: -0.07501459121704102
Batch 49/64 loss: -0.06640732288360596
Batch 50/64 loss: -0.04942810535430908
Batch 51/64 loss: -0.05022364854812622
Batch 52/64 loss: -0.07043987512588501
Batch 53/64 loss: -0.08669781684875488
Batch 54/64 loss: -0.05929875373840332
Batch 55/64 loss: -0.06593149900436401
Batch 56/64 loss: -0.04502004384994507
Batch 57/64 loss: -0.08990985155105591
Batch 58/64 loss: -0.07935315370559692
Batch 59/64 loss: -0.09381473064422607
Batch 60/64 loss: -0.09228134155273438
Batch 61/64 loss: -0.0905112624168396
Batch 62/64 loss: -0.08263027667999268
Batch 63/64 loss: -0.08903563022613525
Batch 64/64 loss: -0.07331186532974243
Epoch 241  Train loss: -0.07338914099861593  Val loss: 0.04776450715114161
Epoch 242
-------------------------------
Batch 1/64 loss: -0.04302394390106201
Batch 2/64 loss: -0.08716130256652832
Batch 3/64 loss: -0.08443880081176758
Batch 4/64 loss: -0.08437371253967285
Batch 5/64 loss: -0.07004547119140625
Batch 6/64 loss: -0.09617745876312256
Batch 7/64 loss: -0.0853453278541565
Batch 8/64 loss: -0.0899856686592102
Batch 9/64 loss: -0.10881829261779785
Batch 10/64 loss: -0.06395304203033447
Batch 11/64 loss: -0.08305162191390991
Batch 12/64 loss: -0.04632890224456787
Batch 13/64 loss: -0.08297896385192871
Batch 14/64 loss: -0.08073484897613525
Batch 15/64 loss: -0.08892834186553955
Batch 16/64 loss: -0.0892798900604248
Batch 17/64 loss: -0.07771420478820801
Batch 18/64 loss: -0.056143105030059814
Batch 19/64 loss: -0.07081782817840576
Batch 20/64 loss: -0.0942302942276001
Batch 21/64 loss: -0.06936663389205933
Batch 22/64 loss: -0.04691731929779053
Batch 23/64 loss: -0.0684351921081543
Batch 24/64 loss: -0.07599341869354248
Batch 25/64 loss: -0.09728807210922241
Batch 26/64 loss: -0.06726205348968506
Batch 27/64 loss: -0.0655210018157959
Batch 28/64 loss: -0.07608413696289062
Batch 29/64 loss: -0.09198731184005737
Batch 30/64 loss: -0.05733269453048706
Batch 31/64 loss: -0.05233484506607056
Batch 32/64 loss: -0.08741801977157593
Batch 33/64 loss: -0.060727834701538086
Batch 34/64 loss: -0.065623939037323
Batch 35/64 loss: -0.08322775363922119
Batch 36/64 loss: -0.08123773336410522
Batch 37/64 loss: -0.08010375499725342
Batch 38/64 loss: -0.06843268871307373
Batch 39/64 loss: -0.08101731538772583
Batch 40/64 loss: -0.055378079414367676
Batch 41/64 loss: -0.08768987655639648
Batch 42/64 loss: -0.04052567481994629
Batch 43/64 loss: -0.10580116510391235
Batch 44/64 loss: -0.03012073040008545
Batch 45/64 loss: -0.059244751930236816
Batch 46/64 loss: -0.07964926958084106
Batch 47/64 loss: -0.04308056831359863
Batch 48/64 loss: -0.07307332754135132
Batch 49/64 loss: -0.08246946334838867
Batch 50/64 loss: -0.0757594108581543
Batch 51/64 loss: -0.08241355419158936
Batch 52/64 loss: -0.04498636722564697
Batch 53/64 loss: -0.07436347007751465
Batch 54/64 loss: -0.07593435049057007
Batch 55/64 loss: -0.05814504623413086
Batch 56/64 loss: -0.07031458616256714
Batch 57/64 loss: -0.056192994117736816
Batch 58/64 loss: -0.017090141773223877
Batch 59/64 loss: -0.06562840938568115
Batch 60/64 loss: -0.06860435009002686
Batch 61/64 loss: -0.07230371236801147
Batch 62/64 loss: -0.08132690191268921
Batch 63/64 loss: -0.06302487850189209
Batch 64/64 loss: -0.07686024904251099
Epoch 242  Train loss: -0.07185269033207613  Val loss: 0.04906350599531455
Epoch 243
-------------------------------
Batch 1/64 loss: -0.07246148586273193
Batch 2/64 loss: -0.0837741494178772
Batch 3/64 loss: -0.037577807903289795
Batch 4/64 loss: -0.04961073398590088
Batch 5/64 loss: -0.07949972152709961
Batch 6/64 loss: -0.08714497089385986
Batch 7/64 loss: -0.0564265251159668
Batch 8/64 loss: -0.06855309009552002
Batch 9/64 loss: -0.06688714027404785
Batch 10/64 loss: -0.09280872344970703
Batch 11/64 loss: -0.0925443172454834
Batch 12/64 loss: -0.06097972393035889
Batch 13/64 loss: -0.06376731395721436
Batch 14/64 loss: -0.07907629013061523
Batch 15/64 loss: -0.09163856506347656
Batch 16/64 loss: -0.07014614343643188
Batch 17/64 loss: -0.07649976015090942
Batch 18/64 loss: -0.07826483249664307
Batch 19/64 loss: -0.09651225805282593
Batch 20/64 loss: -0.08675163984298706
Batch 21/64 loss: -0.08602786064147949
Batch 22/64 loss: -0.056077003479003906
Batch 23/64 loss: -0.06844431161880493
Batch 24/64 loss: -0.08098602294921875
Batch 25/64 loss: -0.0549464225769043
Batch 26/64 loss: -0.07741755247116089
Batch 27/64 loss: -0.07788276672363281
Batch 28/64 loss: -0.07144314050674438
Batch 29/64 loss: -0.08186483383178711
Batch 30/64 loss: -0.078460693359375
Batch 31/64 loss: -0.06558471918106079
Batch 32/64 loss: -0.08465772867202759
Batch 33/64 loss: -0.07546108961105347
Batch 34/64 loss: -0.05377495288848877
Batch 35/64 loss: -0.05224573612213135
Batch 36/64 loss: -0.06380081176757812
Batch 37/64 loss: -0.08033502101898193
Batch 38/64 loss: -0.08385694026947021
Batch 39/64 loss: -0.08733713626861572
Batch 40/64 loss: -0.07741552591323853
Batch 41/64 loss: -0.07206612825393677
Batch 42/64 loss: -0.07024109363555908
Batch 43/64 loss: -0.06657397747039795
Batch 44/64 loss: -0.07423138618469238
Batch 45/64 loss: -0.0805010199546814
Batch 46/64 loss: -0.08059108257293701
Batch 47/64 loss: -0.06310474872589111
Batch 48/64 loss: -0.042928874492645264
Batch 49/64 loss: -0.055961668491363525
Batch 50/64 loss: -0.07535809278488159
Batch 51/64 loss: -0.09487128257751465
Batch 52/64 loss: -0.06979268789291382
Batch 53/64 loss: -0.07965594530105591
Batch 54/64 loss: -0.08394074440002441
Batch 55/64 loss: -0.023484110832214355
Batch 56/64 loss: -0.09579110145568848
Batch 57/64 loss: -0.06730157136917114
Batch 58/64 loss: -0.06553995609283447
Batch 59/64 loss: -0.047362685203552246
Batch 60/64 loss: -0.050356268882751465
Batch 61/64 loss: -0.06564432382583618
Batch 62/64 loss: -0.06797671318054199
Batch 63/64 loss: -0.0861825942993164
Batch 64/64 loss: -0.03240966796875
Epoch 243  Train loss: -0.07141507092644186  Val loss: 0.0465634276776789
Epoch 244
-------------------------------
Batch 1/64 loss: -0.06566333770751953
Batch 2/64 loss: -0.0898810625076294
Batch 3/64 loss: -0.044754862785339355
Batch 4/64 loss: -0.1050868034362793
Batch 5/64 loss: -0.0793423056602478
Batch 6/64 loss: -0.09256994724273682
Batch 7/64 loss: -0.06654953956604004
Batch 8/64 loss: -0.0519639253616333
Batch 9/64 loss: -0.09633892774581909
Batch 10/64 loss: -0.07926249504089355
Batch 11/64 loss: -0.08792585134506226
Batch 12/64 loss: -0.09599512815475464
Batch 13/64 loss: -0.07465130090713501
Batch 14/64 loss: -0.08369874954223633
Batch 15/64 loss: -0.09416866302490234
Batch 16/64 loss: -0.07922583818435669
Batch 17/64 loss: -0.07220107316970825
Batch 18/64 loss: -0.05628782510757446
Batch 19/64 loss: -0.07399582862854004
Batch 20/64 loss: -0.03128403425216675
Batch 21/64 loss: -0.07755148410797119
Batch 22/64 loss: -0.06725406646728516
Batch 23/64 loss: -0.0966607928276062
Batch 24/64 loss: -0.06109738349914551
Batch 25/64 loss: -0.07997268438339233
Batch 26/64 loss: -0.07463198900222778
Batch 27/64 loss: -0.07595634460449219
Batch 28/64 loss: -0.05214035511016846
Batch 29/64 loss: -0.07497966289520264
Batch 30/64 loss: -0.09220021963119507
Batch 31/64 loss: -0.07627558708190918
Batch 32/64 loss: -0.05893474817276001
Batch 33/64 loss: -0.06270766258239746
Batch 34/64 loss: -0.06897222995758057
Batch 35/64 loss: -0.0711061954498291
Batch 36/64 loss: -0.05064666271209717
Batch 37/64 loss: -0.06441640853881836
Batch 38/64 loss: -0.0688396692276001
Batch 39/64 loss: -0.037348806858062744
Batch 40/64 loss: -0.09706246852874756
Batch 41/64 loss: -0.06766963005065918
Batch 42/64 loss: -0.07624047994613647
Batch 43/64 loss: -0.08609986305236816
Batch 44/64 loss: -0.07321417331695557
Batch 45/64 loss: -0.08694010972976685
Batch 46/64 loss: -0.06079643964767456
Batch 47/64 loss: -0.07409703731536865
Batch 48/64 loss: -0.058318376541137695
Batch 49/64 loss: -0.07848525047302246
Batch 50/64 loss: -0.0821923017501831
Batch 51/64 loss: -0.08266407251358032
Batch 52/64 loss: -0.08136719465255737
Batch 53/64 loss: -0.07875275611877441
Batch 54/64 loss: -0.08106273412704468
Batch 55/64 loss: -0.052947402000427246
Batch 56/64 loss: -0.07998579740524292
Batch 57/64 loss: -0.06775325536727905
Batch 58/64 loss: -0.08211308717727661
Batch 59/64 loss: -0.072765052318573
Batch 60/64 loss: -0.06924712657928467
Batch 61/64 loss: -0.09251976013183594
Batch 62/64 loss: -0.04491668939590454
Batch 63/64 loss: -0.058143675327301025
Batch 64/64 loss: -0.053462862968444824
Epoch 244  Train loss: -0.07306654640272552  Val loss: 0.043659856638957543
Epoch 245
-------------------------------
Batch 1/64 loss: -0.09058552980422974
Batch 2/64 loss: -0.055223286151885986
Batch 3/64 loss: -0.06875056028366089
Batch 4/64 loss: -0.06480777263641357
Batch 5/64 loss: -0.07332468032836914
Batch 6/64 loss: -0.08204537630081177
Batch 7/64 loss: -0.07438963651657104
Batch 8/64 loss: -0.09654265642166138
Batch 9/64 loss: -0.09920895099639893
Batch 10/64 loss: -0.0601269006729126
Batch 11/64 loss: -0.07865273952484131
Batch 12/64 loss: -0.05069702863693237
Batch 13/64 loss: -0.05428171157836914
Batch 14/64 loss: -0.08390951156616211
Batch 15/64 loss: -0.0804445743560791
Batch 16/64 loss: -0.060813069343566895
Batch 17/64 loss: -0.07783085107803345
Batch 18/64 loss: -0.10143852233886719
Batch 19/64 loss: -0.0724492073059082
Batch 20/64 loss: -0.05873870849609375
Batch 21/64 loss: -0.05496513843536377
Batch 22/64 loss: -0.07612895965576172
Batch 23/64 loss: -0.0753169059753418
Batch 24/64 loss: -0.07554972171783447
Batch 25/64 loss: -0.0924258828163147
Batch 26/64 loss: -0.10684400796890259
Batch 27/64 loss: -0.060862183570861816
Batch 28/64 loss: -0.08139872550964355
Batch 29/64 loss: -0.06392627954483032
Batch 30/64 loss: -0.03755640983581543
Batch 31/64 loss: -0.0882035493850708
Batch 32/64 loss: -0.0879170298576355
Batch 33/64 loss: -0.07490122318267822
Batch 34/64 loss: -0.07384103536605835
Batch 35/64 loss: -0.061464667320251465
Batch 36/64 loss: -0.06517225503921509
Batch 37/64 loss: -0.09735620021820068
Batch 38/64 loss: -0.09228956699371338
Batch 39/64 loss: -0.05343693494796753
Batch 40/64 loss: -0.06963258981704712
Batch 41/64 loss: -0.0568583607673645
Batch 42/64 loss: -0.06691265106201172
Batch 43/64 loss: -0.08027338981628418
Batch 44/64 loss: -0.07623797655105591
Batch 45/64 loss: -0.09712731838226318
Batch 46/64 loss: -0.06920856237411499
Batch 47/64 loss: -0.056104302406311035
Batch 48/64 loss: -0.05735743045806885
Batch 49/64 loss: -0.07453328371047974
Batch 50/64 loss: -0.08091491460800171
Batch 51/64 loss: -0.06950438022613525
Batch 52/64 loss: -0.07392942905426025
Batch 53/64 loss: -0.08526372909545898
Batch 54/64 loss: -0.06657111644744873
Batch 55/64 loss: -0.08537173271179199
Batch 56/64 loss: -0.06086719036102295
Batch 57/64 loss: -0.04615330696105957
Batch 58/64 loss: -0.08705651760101318
Batch 59/64 loss: -0.07757127285003662
Batch 60/64 loss: -0.07168078422546387
Batch 61/64 loss: -0.09227859973907471
Batch 62/64 loss: -0.06520146131515503
Batch 63/64 loss: -0.08202242851257324
Batch 64/64 loss: -0.05982691049575806
Epoch 245  Train loss: -0.07368346452713012  Val loss: 0.04731138588226948
Epoch 246
-------------------------------
Batch 1/64 loss: -0.06451177597045898
Batch 2/64 loss: -0.05939304828643799
Batch 3/64 loss: -0.09936577081680298
Batch 4/64 loss: -0.09586864709854126
Batch 5/64 loss: -0.07673478126525879
Batch 6/64 loss: -0.050900936126708984
Batch 7/64 loss: -0.06396543979644775
Batch 8/64 loss: -0.09190374612808228
Batch 9/64 loss: -0.09135282039642334
Batch 10/64 loss: -0.09370136260986328
Batch 11/64 loss: -0.10805630683898926
Batch 12/64 loss: -0.0244256854057312
Batch 13/64 loss: -0.07982814311981201
Batch 14/64 loss: -0.07351887226104736
Batch 15/64 loss: -0.09290444850921631
Batch 16/64 loss: -0.07655161619186401
Batch 17/64 loss: -0.08131587505340576
Batch 18/64 loss: -0.09837442636489868
Batch 19/64 loss: -0.06540924310684204
Batch 20/64 loss: -0.04347258806228638
Batch 21/64 loss: -0.0639830231666565
Batch 22/64 loss: -0.06680846214294434
Batch 23/64 loss: -0.06113547086715698
Batch 24/64 loss: -0.06594294309616089
Batch 25/64 loss: -0.0877525806427002
Batch 26/64 loss: -0.075930655002594
Batch 27/64 loss: -0.05091714859008789
Batch 28/64 loss: -0.0840865969657898
Batch 29/64 loss: -0.05687993764877319
Batch 30/64 loss: -0.0757405161857605
Batch 31/64 loss: -0.0637061595916748
Batch 32/64 loss: -0.08416241407394409
Batch 33/64 loss: -0.0531771183013916
Batch 34/64 loss: -0.07845932245254517
Batch 35/64 loss: -0.07074564695358276
Batch 36/64 loss: -0.0696800947189331
Batch 37/64 loss: -0.08511924743652344
Batch 38/64 loss: -0.06161761283874512
Batch 39/64 loss: -0.07927137613296509
Batch 40/64 loss: -0.07504081726074219
Batch 41/64 loss: -0.08859598636627197
Batch 42/64 loss: -0.080191969871521
Batch 43/64 loss: -0.09846478700637817
Batch 44/64 loss: -0.08395731449127197
Batch 45/64 loss: -0.08215957880020142
Batch 46/64 loss: -0.06889480352401733
Batch 47/64 loss: -0.06213819980621338
Batch 48/64 loss: -0.05473923683166504
Batch 49/64 loss: -0.05156069993972778
Batch 50/64 loss: -0.08765101432800293
Batch 51/64 loss: -0.07401472330093384
Batch 52/64 loss: -0.043216586112976074
Batch 53/64 loss: -0.0765337347984314
Batch 54/64 loss: -0.059797704219818115
Batch 55/64 loss: -0.0907391905784607
Batch 56/64 loss: -0.09302645921707153
Batch 57/64 loss: -0.056498944759368896
Batch 58/64 loss: -0.09365010261535645
Batch 59/64 loss: -0.05933701992034912
Batch 60/64 loss: -0.08091962337493896
Batch 61/64 loss: -0.06745833158493042
Batch 62/64 loss: -0.07679957151412964
Batch 63/64 loss: -0.0811052918434143
Batch 64/64 loss: -0.10673415660858154
Epoch 246  Train loss: -0.07424649678024592  Val loss: 0.0446544359230094
Epoch 247
-------------------------------
Batch 1/64 loss: -0.0795297622680664
Batch 2/64 loss: -0.08302432298660278
Batch 3/64 loss: -0.06449514627456665
Batch 4/64 loss: -0.0793905258178711
Batch 5/64 loss: -0.0985182523727417
Batch 6/64 loss: -0.09044289588928223
Batch 7/64 loss: -0.07486683130264282
Batch 8/64 loss: -0.09485000371932983
Batch 9/64 loss: -0.09538501501083374
Batch 10/64 loss: -0.09151655435562134
Batch 11/64 loss: -0.08673608303070068
Batch 12/64 loss: -0.08708655834197998
Batch 13/64 loss: -0.042159199714660645
Batch 14/64 loss: -0.06524139642715454
Batch 15/64 loss: -0.08673346042633057
Batch 16/64 loss: -0.07160341739654541
Batch 17/64 loss: -0.06210207939147949
Batch 18/64 loss: -0.06555765867233276
Batch 19/64 loss: -0.06782674789428711
Batch 20/64 loss: -0.08791559934616089
Batch 21/64 loss: -0.07061231136322021
Batch 22/64 loss: -0.07719659805297852
Batch 23/64 loss: -0.06870317459106445
Batch 24/64 loss: -0.08813244104385376
Batch 25/64 loss: -0.0733756422996521
Batch 26/64 loss: -0.048500120639801025
Batch 27/64 loss: -0.09000188112258911
Batch 28/64 loss: -0.09038311243057251
Batch 29/64 loss: -0.049364686012268066
Batch 30/64 loss: -0.08084142208099365
Batch 31/64 loss: -0.06962931156158447
Batch 32/64 loss: -0.07259613275527954
Batch 33/64 loss: -0.09397876262664795
Batch 34/64 loss: -0.09819430112838745
Batch 35/64 loss: -0.08367544412612915
Batch 36/64 loss: -0.07868236303329468
Batch 37/64 loss: -0.05704003572463989
Batch 38/64 loss: -0.06147724390029907
Batch 39/64 loss: -0.08680665493011475
Batch 40/64 loss: -0.0683443546295166
Batch 41/64 loss: -0.09524142742156982
Batch 42/64 loss: -0.08750104904174805
Batch 43/64 loss: -0.09602528810501099
Batch 44/64 loss: -0.10953062772750854
Batch 45/64 loss: -0.07243549823760986
Batch 46/64 loss: -0.07008254528045654
Batch 47/64 loss: -0.07211506366729736
Batch 48/64 loss: -0.08227628469467163
Batch 49/64 loss: -0.07761085033416748
Batch 50/64 loss: -0.08392274379730225
Batch 51/64 loss: -0.07010072469711304
Batch 52/64 loss: -0.07095998525619507
Batch 53/64 loss: -0.061988770961761475
Batch 54/64 loss: -0.08915436267852783
Batch 55/64 loss: -0.04463678598403931
Batch 56/64 loss: -0.07411879301071167
Batch 57/64 loss: -0.05846822261810303
Batch 58/64 loss: -0.07412242889404297
Batch 59/64 loss: -0.09439074993133545
Batch 60/64 loss: -0.04333698749542236
Batch 61/64 loss: -0.08128756284713745
Batch 62/64 loss: -0.09180599451065063
Batch 63/64 loss: -0.07745999097824097
Batch 64/64 loss: -0.05183893442153931
Epoch 247  Train loss: -0.07686226578319774  Val loss: 0.04514627665588536
Epoch 248
-------------------------------
Batch 1/64 loss: -0.07477831840515137
Batch 2/64 loss: -0.06661397218704224
Batch 3/64 loss: -0.06721001863479614
Batch 4/64 loss: -0.07364523410797119
Batch 5/64 loss: -0.08625006675720215
Batch 6/64 loss: -0.10125076770782471
Batch 7/64 loss: -0.08688312768936157
Batch 8/64 loss: -0.1107931137084961
Batch 9/64 loss: -0.09862470626831055
Batch 10/64 loss: -0.060812294483184814
Batch 11/64 loss: -0.07094603776931763
Batch 12/64 loss: -0.10357451438903809
Batch 13/64 loss: -0.08729046583175659
Batch 14/64 loss: -0.07596951723098755
Batch 15/64 loss: -0.07714444398880005
Batch 16/64 loss: -0.06499660015106201
Batch 17/64 loss: -0.08752959966659546
Batch 18/64 loss: -0.0983431339263916
Batch 19/64 loss: -0.0500638484954834
Batch 20/64 loss: -0.06049919128417969
Batch 21/64 loss: -0.07625001668930054
Batch 22/64 loss: -0.06657731533050537
Batch 23/64 loss: -0.0856637954711914
Batch 24/64 loss: -0.06409931182861328
Batch 25/64 loss: -0.07290530204772949
Batch 26/64 loss: -0.07874274253845215
Batch 27/64 loss: -0.06030893325805664
Batch 28/64 loss: -0.0669550895690918
Batch 29/64 loss: -0.07968056201934814
Batch 30/64 loss: -0.06513237953186035
Batch 31/64 loss: -0.08568871021270752
Batch 32/64 loss: -0.0803021788597107
Batch 33/64 loss: -0.09715193510055542
Batch 34/64 loss: -0.06687384843826294
Batch 35/64 loss: -0.08299070596694946
Batch 36/64 loss: -0.07884150743484497
Batch 37/64 loss: -0.09604579210281372
Batch 38/64 loss: -0.11651080846786499
Batch 39/64 loss: -0.08221244812011719
Batch 40/64 loss: -0.07778877019882202
Batch 41/64 loss: -0.07606804370880127
Batch 42/64 loss: -0.04762965440750122
Batch 43/64 loss: -0.08524656295776367
Batch 44/64 loss: -0.04905521869659424
Batch 45/64 loss: -0.05458098649978638
Batch 46/64 loss: -0.0743870735168457
Batch 47/64 loss: -0.08656907081604004
Batch 48/64 loss: -0.08716869354248047
Batch 49/64 loss: -0.08554291725158691
Batch 50/64 loss: -0.07026618719100952
Batch 51/64 loss: -0.04333162307739258
Batch 52/64 loss: -0.05157214403152466
Batch 53/64 loss: -0.08063846826553345
Batch 54/64 loss: -0.05477970838546753
Batch 55/64 loss: -0.07208108901977539
Batch 56/64 loss: -0.05427831411361694
Batch 57/64 loss: -0.07545459270477295
Batch 58/64 loss: -0.053651273250579834
Batch 59/64 loss: -0.06066030263900757
Batch 60/64 loss: -0.05008202791213989
Batch 61/64 loss: -0.060294151306152344
Batch 62/64 loss: -0.07142037153244019
Batch 63/64 loss: -0.07691574096679688
Batch 64/64 loss: -0.0827062726020813
Epoch 248  Train loss: -0.07478658895866544  Val loss: 0.044935295262287574
Epoch 249
-------------------------------
Batch 1/64 loss: -0.08130264282226562
Batch 2/64 loss: -0.06257414817810059
Batch 3/64 loss: -0.07777911424636841
Batch 4/64 loss: -0.07899415493011475
Batch 5/64 loss: -0.09180992841720581
Batch 6/64 loss: -0.06800186634063721
Batch 7/64 loss: -0.08326524496078491
Batch 8/64 loss: -0.07407313585281372
Batch 9/64 loss: -0.07273805141448975
Batch 10/64 loss: -0.08466237783432007
Batch 11/64 loss: -0.0628352165222168
Batch 12/64 loss: -0.07566976547241211
Batch 13/64 loss: -0.06340622901916504
Batch 14/64 loss: -0.07356160879135132
Batch 15/64 loss: -0.06402301788330078
Batch 16/64 loss: -0.07710033655166626
Batch 17/64 loss: -0.07326436042785645
Batch 18/64 loss: -0.09793424606323242
Batch 19/64 loss: -0.07637304067611694
Batch 20/64 loss: -0.07859361171722412
Batch 21/64 loss: -0.08595669269561768
Batch 22/64 loss: -0.0937727689743042
Batch 23/64 loss: -0.10455942153930664
Batch 24/64 loss: -0.09326040744781494
Batch 25/64 loss: -0.07934486865997314
Batch 26/64 loss: -0.08439916372299194
Batch 27/64 loss: -0.062093496322631836
Batch 28/64 loss: -0.11448371410369873
Batch 29/64 loss: -0.0892685055732727
Batch 30/64 loss: -0.08172547817230225
Batch 31/64 loss: -0.08387136459350586
Batch 32/64 loss: -0.06697183847427368
Batch 33/64 loss: -0.07182073593139648
Batch 34/64 loss: -0.08233815431594849
Batch 35/64 loss: -0.077542245388031
Batch 36/64 loss: -0.04312539100646973
Batch 37/64 loss: -0.08573365211486816
Batch 38/64 loss: -0.07107526063919067
Batch 39/64 loss: -0.06674551963806152
Batch 40/64 loss: -0.04384565353393555
Batch 41/64 loss: -0.06663662195205688
Batch 42/64 loss: -0.08320271968841553
Batch 43/64 loss: -0.05649226903915405
Batch 44/64 loss: -0.08108019828796387
Batch 45/64 loss: -0.0830308198928833
Batch 46/64 loss: -0.08162844181060791
Batch 47/64 loss: -0.0601382851600647
Batch 48/64 loss: -0.07329601049423218
Batch 49/64 loss: -0.059385061264038086
Batch 50/64 loss: -0.0872490406036377
Batch 51/64 loss: -0.08899104595184326
Batch 52/64 loss: -0.06978118419647217
Batch 53/64 loss: -0.07702124118804932
Batch 54/64 loss: -0.09483438730239868
Batch 55/64 loss: -0.07454842329025269
Batch 56/64 loss: -0.07390892505645752
Batch 57/64 loss: -0.052949488162994385
Batch 58/64 loss: -0.06015586853027344
Batch 59/64 loss: -0.05189919471740723
Batch 60/64 loss: -0.08163952827453613
Batch 61/64 loss: -0.08273160457611084
Batch 62/64 loss: -0.07420581579208374
Batch 63/64 loss: -0.05855339765548706
Batch 64/64 loss: -0.09465563297271729
Epoch 249  Train loss: -0.0759881996640972  Val loss: 0.04885581350818123
Epoch 250
-------------------------------
Batch 1/64 loss: -0.06778693199157715
Batch 2/64 loss: -0.0731160044670105
Batch 3/64 loss: -0.054559290409088135
Batch 4/64 loss: -0.06453943252563477
Batch 5/64 loss: -0.07008081674575806
Batch 6/64 loss: -0.060522258281707764
Batch 7/64 loss: -0.048542141914367676
Batch 8/64 loss: -0.09013891220092773
Batch 9/64 loss: -0.06663167476654053
Batch 10/64 loss: -0.07362008094787598
Batch 11/64 loss: -0.06354665756225586
Batch 12/64 loss: -0.0968472957611084
Batch 13/64 loss: -0.08491843938827515
Batch 14/64 loss: -0.10604798793792725
Batch 15/64 loss: -0.06943422555923462
Batch 16/64 loss: -0.07786750793457031
Batch 17/64 loss: -0.07859015464782715
Batch 18/64 loss: -0.10735702514648438
Batch 19/64 loss: -0.07676023244857788
Batch 20/64 loss: -0.0691612958908081
Batch 21/64 loss: -0.060980260372161865
Batch 22/64 loss: -0.07390040159225464
Batch 23/64 loss: -0.0841781497001648
Batch 24/64 loss: -0.05325847864151001
Batch 25/64 loss: -0.04448091983795166
Batch 26/64 loss: -0.06909602880477905
Batch 27/64 loss: -0.0912933349609375
Batch 28/64 loss: -0.09261780977249146
Batch 29/64 loss: -0.01611882448196411
Batch 30/64 loss: -0.07998710870742798
Batch 31/64 loss: -0.07590925693511963
Batch 32/64 loss: -0.06760698556900024
Batch 33/64 loss: -0.08869922161102295
Batch 34/64 loss: -0.06703424453735352
Batch 35/64 loss: -0.0824735164642334
Batch 36/64 loss: -0.08333247900009155
Batch 37/64 loss: -0.07450932264328003
Batch 38/64 loss: -0.07859760522842407
Batch 39/64 loss: -0.08787268400192261
Batch 40/64 loss: -0.09821152687072754
Batch 41/64 loss: -0.0893908143043518
Batch 42/64 loss: -0.0947921872138977
Batch 43/64 loss: -0.0707930326461792
Batch 44/64 loss: -0.08728188276290894
Batch 45/64 loss: -0.06308925151824951
Batch 46/64 loss: -0.07170063257217407
Batch 47/64 loss: -0.08819723129272461
Batch 48/64 loss: -0.06939870119094849
Batch 49/64 loss: -0.069355309009552
Batch 50/64 loss: -0.0970543622970581
Batch 51/64 loss: -0.06048101186752319
Batch 52/64 loss: -0.09276199340820312
Batch 53/64 loss: -0.07005965709686279
Batch 54/64 loss: -0.07739889621734619
Batch 55/64 loss: -0.08087921142578125
Batch 56/64 loss: -0.058223843574523926
Batch 57/64 loss: -0.09081894159317017
Batch 58/64 loss: -0.049819111824035645
Batch 59/64 loss: -0.07139366865158081
Batch 60/64 loss: -0.0887555480003357
Batch 61/64 loss: -0.08614152669906616
Batch 62/64 loss: -0.0614926815032959
Batch 63/64 loss: -0.06304168701171875
Batch 64/64 loss: -0.0692366361618042
Epoch 250  Train loss: -0.07489372842452105  Val loss: 0.0471508513201553
Epoch 251
-------------------------------
Batch 1/64 loss: -0.08893018960952759
Batch 2/64 loss: -0.09179472923278809
Batch 3/64 loss: -0.07647883892059326
Batch 4/64 loss: -0.09053272008895874
Batch 5/64 loss: -0.04947936534881592
Batch 6/64 loss: -0.06939864158630371
Batch 7/64 loss: -0.036420464515686035
Batch 8/64 loss: -0.06880611181259155
Batch 9/64 loss: -0.07786554098129272
Batch 10/64 loss: -0.0729185938835144
Batch 11/64 loss: -0.08448809385299683
Batch 12/64 loss: -0.0791783332824707
Batch 13/64 loss: -0.06827646493911743
Batch 14/64 loss: -0.04780548810958862
Batch 15/64 loss: -0.08516049385070801
Batch 16/64 loss: -0.09128385782241821
Batch 17/64 loss: -0.07902073860168457
Batch 18/64 loss: -0.07312703132629395
Batch 19/64 loss: -0.08893615007400513
Batch 20/64 loss: -0.07712042331695557
Batch 21/64 loss: -0.08992284536361694
Batch 22/64 loss: -0.08114433288574219
Batch 23/64 loss: -0.07655704021453857
Batch 24/64 loss: -0.08239281177520752
Batch 25/64 loss: -0.086925208568573
Batch 26/64 loss: -0.09495538473129272
Batch 27/64 loss: -0.09367012977600098
Batch 28/64 loss: -0.09563803672790527
Batch 29/64 loss: -0.09346187114715576
Batch 30/64 loss: -0.06799405813217163
Batch 31/64 loss: -0.08360815048217773
Batch 32/64 loss: -0.09923851490020752
Batch 33/64 loss: -0.08231472969055176
Batch 34/64 loss: -0.0454561710357666
Batch 35/64 loss: -0.05865800380706787
Batch 36/64 loss: -0.07439041137695312
Batch 37/64 loss: -0.0577625036239624
Batch 38/64 loss: -0.08107608556747437
Batch 39/64 loss: -0.08288276195526123
Batch 40/64 loss: -0.07998538017272949
Batch 41/64 loss: -0.08514261245727539
Batch 42/64 loss: -0.09711521863937378
Batch 43/64 loss: -0.08094757795333862
Batch 44/64 loss: -0.08382761478424072
Batch 45/64 loss: -0.07013779878616333
Batch 46/64 loss: -0.07454723119735718
Batch 47/64 loss: -0.11076724529266357
Batch 48/64 loss: -0.07464218139648438
Batch 49/64 loss: -0.05818819999694824
Batch 50/64 loss: -0.05436229705810547
Batch 51/64 loss: -0.07653301954269409
Batch 52/64 loss: -0.07988947629928589
Batch 53/64 loss: -0.08437037467956543
Batch 54/64 loss: -0.08040779829025269
Batch 55/64 loss: -0.07427966594696045
Batch 56/64 loss: -0.06113165616989136
Batch 57/64 loss: -0.08082330226898193
Batch 58/64 loss: -0.08415096998214722
Batch 59/64 loss: -0.06520819664001465
Batch 60/64 loss: -0.0823596715927124
Batch 61/64 loss: -0.051930367946624756
Batch 62/64 loss: -0.05611622333526611
Batch 63/64 loss: -0.04489260911941528
Batch 64/64 loss: -0.04789304733276367
Epoch 251  Train loss: -0.07612153408574122  Val loss: 0.044905721731611954
Epoch 252
-------------------------------
Batch 1/64 loss: -0.0752212405204773
Batch 2/64 loss: -0.0879671573638916
Batch 3/64 loss: -0.06230473518371582
Batch 4/64 loss: -0.05395573377609253
Batch 5/64 loss: -0.1000523567199707
Batch 6/64 loss: -0.07347208261489868
Batch 7/64 loss: -0.089988112449646
Batch 8/64 loss: -0.07555758953094482
Batch 9/64 loss: -0.06951522827148438
Batch 10/64 loss: -0.08975672721862793
Batch 11/64 loss: -0.06284654140472412
Batch 12/64 loss: -0.053162217140197754
Batch 13/64 loss: -0.07327824831008911
Batch 14/64 loss: -0.06774067878723145
Batch 15/64 loss: -0.072745680809021
Batch 16/64 loss: -0.09391903877258301
Batch 17/64 loss: -0.10820472240447998
Batch 18/64 loss: -0.09588342905044556
Batch 19/64 loss: -0.0826716423034668
Batch 20/64 loss: -0.05320918560028076
Batch 21/64 loss: -0.06130033731460571
Batch 22/64 loss: -0.08080399036407471
Batch 23/64 loss: -0.07172924280166626
Batch 24/64 loss: -0.023666679859161377
Batch 25/64 loss: -0.07804948091506958
Batch 26/64 loss: -0.05336707830429077
Batch 27/64 loss: -0.09948456287384033
Batch 28/64 loss: -0.07221519947052002
Batch 29/64 loss: -0.10556840896606445
Batch 30/64 loss: -0.08594101667404175
Batch 31/64 loss: -0.10624271631240845
Batch 32/64 loss: -0.08182573318481445
Batch 33/64 loss: -0.09134840965270996
Batch 34/64 loss: -0.09613567590713501
Batch 35/64 loss: -0.06266158819198608
Batch 36/64 loss: -0.05202072858810425
Batch 37/64 loss: -0.09915947914123535
Batch 38/64 loss: -0.06883430480957031
Batch 39/64 loss: -0.05285477638244629
Batch 40/64 loss: -0.0814482569694519
Batch 41/64 loss: -0.08375376462936401
Batch 42/64 loss: -0.08919847011566162
Batch 43/64 loss: -0.08462119102478027
Batch 44/64 loss: -0.06500351428985596
Batch 45/64 loss: -0.09625983238220215
Batch 46/64 loss: -0.0681692361831665
Batch 47/64 loss: -0.09060245752334595
Batch 48/64 loss: -0.08057689666748047
Batch 49/64 loss: -0.07735002040863037
Batch 50/64 loss: -0.06064438819885254
Batch 51/64 loss: -0.07430684566497803
Batch 52/64 loss: -0.08293074369430542
Batch 53/64 loss: -0.08247339725494385
Batch 54/64 loss: -0.06034588813781738
Batch 55/64 loss: -0.08387959003448486
Batch 56/64 loss: -0.06850683689117432
Batch 57/64 loss: -0.06941592693328857
Batch 58/64 loss: -0.05759108066558838
Batch 59/64 loss: -0.07905882596969604
Batch 60/64 loss: -0.07179731130599976
Batch 61/64 loss: -0.0763164758682251
Batch 62/64 loss: -0.07362186908721924
Batch 63/64 loss: -0.08616006374359131
Batch 64/64 loss: -0.04611468315124512
Epoch 252  Train loss: -0.07628675535613415  Val loss: 0.04212365076713955
Epoch 253
-------------------------------
Batch 1/64 loss: -0.10889595746994019
Batch 2/64 loss: -0.08313506841659546
Batch 3/64 loss: -0.07346487045288086
Batch 4/64 loss: -0.07210636138916016
Batch 5/64 loss: -0.06667059659957886
Batch 6/64 loss: -0.07887864112854004
Batch 7/64 loss: -0.07767575979232788
Batch 8/64 loss: -0.09156477451324463
Batch 9/64 loss: -0.08858126401901245
Batch 10/64 loss: -0.084988534450531
Batch 11/64 loss: -0.08018136024475098
Batch 12/64 loss: -0.05505359172821045
Batch 13/64 loss: -0.07607769966125488
Batch 14/64 loss: -0.06537681818008423
Batch 15/64 loss: -0.08947253227233887
Batch 16/64 loss: -0.09035629034042358
Batch 17/64 loss: -0.09450137615203857
Batch 18/64 loss: -0.06728971004486084
Batch 19/64 loss: -0.09036076068878174
Batch 20/64 loss: -0.0759814977645874
Batch 21/64 loss: -0.09029161930084229
Batch 22/64 loss: -0.044823288917541504
Batch 23/64 loss: -0.0757598876953125
Batch 24/64 loss: -0.08112883567810059
Batch 25/64 loss: -0.10502082109451294
Batch 26/64 loss: -0.08184385299682617
Batch 27/64 loss: -0.06015348434448242
Batch 28/64 loss: -0.06949746608734131
Batch 29/64 loss: -0.058936238288879395
Batch 30/64 loss: -0.0627967119216919
Batch 31/64 loss: -0.0837506651878357
Batch 32/64 loss: -0.08494800329208374
Batch 33/64 loss: -0.06436216831207275
Batch 34/64 loss: -0.08532083034515381
Batch 35/64 loss: -0.08567887544631958
Batch 36/64 loss: -0.08777201175689697
Batch 37/64 loss: -0.0646200180053711
Batch 38/64 loss: -0.04982185363769531
Batch 39/64 loss: -0.08860152959823608
Batch 40/64 loss: -0.08287417888641357
Batch 41/64 loss: -0.08351695537567139
Batch 42/64 loss: -0.055993080139160156
Batch 43/64 loss: -0.06263607740402222
Batch 44/64 loss: -0.07454204559326172
Batch 45/64 loss: -0.07448065280914307
Batch 46/64 loss: -0.0494769811630249
Batch 47/64 loss: -0.07574158906936646
Batch 48/64 loss: -0.08685141801834106
Batch 49/64 loss: -0.08102291822433472
Batch 50/64 loss: -0.10984516143798828
Batch 51/64 loss: -0.07062464952468872
Batch 52/64 loss: -0.036657869815826416
Batch 53/64 loss: -0.0904267430305481
Batch 54/64 loss: -0.06898128986358643
Batch 55/64 loss: -0.0329708456993103
Batch 56/64 loss: -0.07971596717834473
Batch 57/64 loss: -0.0857764482498169
Batch 58/64 loss: -0.07712811231613159
Batch 59/64 loss: -0.07197046279907227
Batch 60/64 loss: -0.07170474529266357
Batch 61/64 loss: -0.06100267171859741
Batch 62/64 loss: -0.08213222026824951
Batch 63/64 loss: -0.078144371509552
Batch 64/64 loss: -0.07155656814575195
Epoch 253  Train loss: -0.07582159229353362  Val loss: 0.04807166504286409
Epoch 254
-------------------------------
Batch 1/64 loss: -0.08165758848190308
Batch 2/64 loss: -0.06524896621704102
Batch 3/64 loss: -0.06860709190368652
Batch 4/64 loss: -0.04205429553985596
Batch 5/64 loss: -0.08269816637039185
Batch 6/64 loss: -0.06972599029541016
Batch 7/64 loss: -0.07048153877258301
Batch 8/64 loss: -0.0799785852432251
Batch 9/64 loss: -0.08789986371994019
Batch 10/64 loss: -0.07680845260620117
Batch 11/64 loss: -0.055473506450653076
Batch 12/64 loss: -0.07097911834716797
Batch 13/64 loss: -0.07607883214950562
Batch 14/64 loss: -0.0456315279006958
Batch 15/64 loss: -0.08142173290252686
Batch 16/64 loss: -0.050567805767059326
Batch 17/64 loss: -0.09618568420410156
Batch 18/64 loss: -0.07984572649002075
Batch 19/64 loss: -0.08326435089111328
Batch 20/64 loss: -0.0772448182106018
Batch 21/64 loss: -0.08612972497940063
Batch 22/64 loss: -0.095023512840271
Batch 23/64 loss: -0.08239394426345825
Batch 24/64 loss: -0.07426732778549194
Batch 25/64 loss: -0.05752843618392944
Batch 26/64 loss: -0.07791769504547119
Batch 27/64 loss: -0.08195054531097412
Batch 28/64 loss: -0.0878496766090393
Batch 29/64 loss: -0.06789422035217285
Batch 30/64 loss: -0.09380865097045898
Batch 31/64 loss: -0.06370633840560913
Batch 32/64 loss: -0.0647355318069458
Batch 33/64 loss: -0.06796956062316895
Batch 34/64 loss: -0.08809888362884521
Batch 35/64 loss: -0.04340720176696777
Batch 36/64 loss: -0.08764606714248657
Batch 37/64 loss: -0.07653099298477173
Batch 38/64 loss: -0.06569278240203857
Batch 39/64 loss: -0.07982009649276733
Batch 40/64 loss: -0.05960977077484131
Batch 41/64 loss: -0.075950026512146
Batch 42/64 loss: -0.08914405107498169
Batch 43/64 loss: -0.07685965299606323
Batch 44/64 loss: -0.06305277347564697
Batch 45/64 loss: -0.08926045894622803
Batch 46/64 loss: -0.06606221199035645
Batch 47/64 loss: -0.0731305480003357
Batch 48/64 loss: -0.08713638782501221
Batch 49/64 loss: -0.08812659978866577
Batch 50/64 loss: -0.08222711086273193
Batch 51/64 loss: -0.07898765802383423
Batch 52/64 loss: -0.0726090669631958
Batch 53/64 loss: -0.10033857822418213
Batch 54/64 loss: -0.0955163836479187
Batch 55/64 loss: -0.09069043397903442
Batch 56/64 loss: -0.08882743120193481
Batch 57/64 loss: -0.08811849355697632
Batch 58/64 loss: -0.09024351835250854
Batch 59/64 loss: -0.07632428407669067
Batch 60/64 loss: -0.08742040395736694
Batch 61/64 loss: -0.0812259316444397
Batch 62/64 loss: -0.08403921127319336
Batch 63/64 loss: -0.08795833587646484
Batch 64/64 loss: -0.05139791965484619
Epoch 254  Train loss: -0.07679423678155038  Val loss: 0.04229775081385452
Epoch 255
-------------------------------
Batch 1/64 loss: -0.10883206129074097
Batch 2/64 loss: -0.06265556812286377
Batch 3/64 loss: -0.06198740005493164
Batch 4/64 loss: -0.08496677875518799
Batch 5/64 loss: -0.0877029299736023
Batch 6/64 loss: -0.07369840145111084
Batch 7/64 loss: -0.04682350158691406
Batch 8/64 loss: -0.09410929679870605
Batch 9/64 loss: -0.09371143579483032
Batch 10/64 loss: -0.07572972774505615
Batch 11/64 loss: -0.0769503116607666
Batch 12/64 loss: -0.0990062952041626
Batch 13/64 loss: -0.08784574270248413
Batch 14/64 loss: -0.08805620670318604
Batch 15/64 loss: -0.10070466995239258
Batch 16/64 loss: -0.08100903034210205
Batch 17/64 loss: -0.07259279489517212
Batch 18/64 loss: -0.04438900947570801
Batch 19/64 loss: -0.09630948305130005
Batch 20/64 loss: -0.08981072902679443
Batch 21/64 loss: -0.08640426397323608
Batch 22/64 loss: -0.061954379081726074
Batch 23/64 loss: -0.07343745231628418
Batch 24/64 loss: -0.08232736587524414
Batch 25/64 loss: -0.07440429925918579
Batch 26/64 loss: -0.06991773843765259
Batch 27/64 loss: -0.09977561235427856
Batch 28/64 loss: -0.08663082122802734
Batch 29/64 loss: -0.09276598691940308
Batch 30/64 loss: -0.07324200868606567
Batch 31/64 loss: -0.09359586238861084
Batch 32/64 loss: -0.07910645008087158
Batch 33/64 loss: -0.09948545694351196
Batch 34/64 loss: -0.069893479347229
Batch 35/64 loss: -0.09118533134460449
Batch 36/64 loss: -0.08741986751556396
Batch 37/64 loss: -0.07430416345596313
Batch 38/64 loss: -0.11035656929016113
Batch 39/64 loss: -0.0891522765159607
Batch 40/64 loss: -0.08578753471374512
Batch 41/64 loss: -0.09180444478988647
Batch 42/64 loss: -0.08348202705383301
Batch 43/64 loss: -0.06895160675048828
Batch 44/64 loss: -0.0741499662399292
Batch 45/64 loss: -0.09886354207992554
Batch 46/64 loss: -0.08720993995666504
Batch 47/64 loss: -0.07549512386322021
Batch 48/64 loss: -0.09196370840072632
Batch 49/64 loss: -0.0845947265625
Batch 50/64 loss: -0.04991644620895386
Batch 51/64 loss: -0.09382283687591553
Batch 52/64 loss: -0.0585787296295166
Batch 53/64 loss: -0.06885683536529541
Batch 54/64 loss: -0.07264518737792969
Batch 55/64 loss: -0.06322407722473145
Batch 56/64 loss: -0.0680915117263794
Batch 57/64 loss: -0.07201218605041504
Batch 58/64 loss: -0.07010579109191895
Batch 59/64 loss: -0.05751466751098633
Batch 60/64 loss: -0.054457008838653564
Batch 61/64 loss: -0.05182462930679321
Batch 62/64 loss: -0.06939166784286499
Batch 63/64 loss: -0.08146023750305176
Batch 64/64 loss: -0.08774572610855103
Epoch 255  Train loss: -0.0794080703866248  Val loss: 0.04517547924494006
Epoch 256
-------------------------------
Batch 1/64 loss: -0.06542980670928955
Batch 2/64 loss: -0.07557880878448486
Batch 3/64 loss: -0.06606346368789673
Batch 4/64 loss: -0.07611387968063354
Batch 5/64 loss: -0.08713889122009277
Batch 6/64 loss: -0.07431304454803467
Batch 7/64 loss: -0.06444871425628662
Batch 8/64 loss: -0.060956716537475586
Batch 9/64 loss: -0.07003945112228394
Batch 10/64 loss: -0.10233914852142334
Batch 11/64 loss: -0.08481651544570923
Batch 12/64 loss: -0.1079053282737732
Batch 13/64 loss: -0.07154601812362671
Batch 14/64 loss: -0.055779099464416504
Batch 15/64 loss: -0.07881820201873779
Batch 16/64 loss: -0.09779161214828491
Batch 17/64 loss: -0.09770160913467407
Batch 18/64 loss: -0.11574125289916992
Batch 19/64 loss: -0.07243221998214722
Batch 20/64 loss: -0.08941352367401123
Batch 21/64 loss: -0.07523095607757568
Batch 22/64 loss: -0.08860450983047485
Batch 23/64 loss: -0.0703967809677124
Batch 24/64 loss: -0.07653689384460449
Batch 25/64 loss: -0.08725756406784058
Batch 26/64 loss: -0.05819904804229736
Batch 27/64 loss: -0.07331323623657227
Batch 28/64 loss: -0.07835221290588379
Batch 29/64 loss: -0.07031625509262085
Batch 30/64 loss: -0.06839722394943237
Batch 31/64 loss: -0.07337474822998047
Batch 32/64 loss: -0.07699453830718994
Batch 33/64 loss: -0.08582901954650879
Batch 34/64 loss: -0.07319945096969604
Batch 35/64 loss: -0.0721924901008606
Batch 36/64 loss: -0.07751190662384033
Batch 37/64 loss: -0.07857590913772583
Batch 38/64 loss: -0.09227955341339111
Batch 39/64 loss: -0.0870964527130127
Batch 40/64 loss: -0.06520628929138184
Batch 41/64 loss: -0.08968693017959595
Batch 42/64 loss: -0.09708917140960693
Batch 43/64 loss: -0.07001513242721558
Batch 44/64 loss: -0.09658896923065186
Batch 45/64 loss: -0.07506388425827026
Batch 46/64 loss: -0.05901080369949341
Batch 47/64 loss: -0.10265368223190308
Batch 48/64 loss: -0.07882589101791382
Batch 49/64 loss: -0.0761483907699585
Batch 50/64 loss: -0.08904755115509033
Batch 51/64 loss: -0.103130042552948
Batch 52/64 loss: -0.07083708047866821
Batch 53/64 loss: -0.09761691093444824
Batch 54/64 loss: -0.08065283298492432
Batch 55/64 loss: -0.06936079263687134
Batch 56/64 loss: -0.07569193840026855
Batch 57/64 loss: -0.0907435417175293
Batch 58/64 loss: -0.07448709011077881
Batch 59/64 loss: -0.09657067060470581
Batch 60/64 loss: -0.07988065481185913
Batch 61/64 loss: -0.0639570951461792
Batch 62/64 loss: -0.07697635889053345
Batch 63/64 loss: -0.05879002809524536
Batch 64/64 loss: -0.04841214418411255
Epoch 256  Train loss: -0.07925281407786351  Val loss: 0.04508070785974719
Epoch 257
-------------------------------
Batch 1/64 loss: -0.10428541898727417
Batch 2/64 loss: -0.09760767221450806
Batch 3/64 loss: -0.07325196266174316
Batch 4/64 loss: -0.08101367950439453
Batch 5/64 loss: -0.05509495735168457
Batch 6/64 loss: -0.10295933485031128
Batch 7/64 loss: -0.07510960102081299
Batch 8/64 loss: -0.07540065050125122
Batch 9/64 loss: -0.07366669178009033
Batch 10/64 loss: -0.07866936922073364
Batch 11/64 loss: -0.06661874055862427
Batch 12/64 loss: -0.08522313833236694
Batch 13/64 loss: -0.0734829306602478
Batch 14/64 loss: -0.08444398641586304
Batch 15/64 loss: -0.10294365882873535
Batch 16/64 loss: -0.09229439496994019
Batch 17/64 loss: -0.07604366540908813
Batch 18/64 loss: -0.08454287052154541
Batch 19/64 loss: -0.10666143894195557
Batch 20/64 loss: -0.1090543270111084
Batch 21/64 loss: -0.08521735668182373
Batch 22/64 loss: -0.08605813980102539
Batch 23/64 loss: -0.07979702949523926
Batch 24/64 loss: -0.059780120849609375
Batch 25/64 loss: -0.05217742919921875
Batch 26/64 loss: -0.07087725400924683
Batch 27/64 loss: -0.09937357902526855
Batch 28/64 loss: -0.07890868186950684
Batch 29/64 loss: -0.09261059761047363
Batch 30/64 loss: -0.07997125387191772
Batch 31/64 loss: -0.09634721279144287
Batch 32/64 loss: -0.07143902778625488
Batch 33/64 loss: -0.07691872119903564
Batch 34/64 loss: -0.09909278154373169
Batch 35/64 loss: -0.0666881799697876
Batch 36/64 loss: -0.08920705318450928
Batch 37/64 loss: -0.0741126537322998
Batch 38/64 loss: -0.07861685752868652
Batch 39/64 loss: -0.04881465435028076
Batch 40/64 loss: -0.08383786678314209
Batch 41/64 loss: -0.09027379751205444
Batch 42/64 loss: -0.06864488124847412
Batch 43/64 loss: -0.0746045708656311
Batch 44/64 loss: -0.09894925355911255
Batch 45/64 loss: -0.032333195209503174
Batch 46/64 loss: -0.08844447135925293
Batch 47/64 loss: -0.08793729543685913
Batch 48/64 loss: -0.07367616891860962
Batch 49/64 loss: -0.06374728679656982
Batch 50/64 loss: -0.08857482671737671
Batch 51/64 loss: -0.08356297016143799
Batch 52/64 loss: -0.07265138626098633
Batch 53/64 loss: -0.10646849870681763
Batch 54/64 loss: -0.06630784273147583
Batch 55/64 loss: -0.05611473321914673
Batch 56/64 loss: -0.09180957078933716
Batch 57/64 loss: -0.10384541749954224
Batch 58/64 loss: -0.092418372631073
Batch 59/64 loss: -0.07601404190063477
Batch 60/64 loss: -0.09968769550323486
Batch 61/64 loss: -0.05329173803329468
Batch 62/64 loss: -0.06476789712905884
Batch 63/64 loss: -0.07467663288116455
Batch 64/64 loss: -0.08838444948196411
Epoch 257  Train loss: -0.08067930703069649  Val loss: 0.04422756240949598
Epoch 258
-------------------------------
Batch 1/64 loss: -0.08337295055389404
Batch 2/64 loss: -0.08526891469955444
Batch 3/64 loss: -0.1002955436706543
Batch 4/64 loss: -0.07941710948944092
Batch 5/64 loss: -0.0727773904800415
Batch 6/64 loss: -0.08683443069458008
Batch 7/64 loss: -0.059540510177612305
Batch 8/64 loss: -0.08046185970306396
Batch 9/64 loss: -0.09152543544769287
Batch 10/64 loss: -0.0699567198753357
Batch 11/64 loss: -0.08238470554351807
Batch 12/64 loss: -0.07895451784133911
Batch 13/64 loss: -0.08550846576690674
Batch 14/64 loss: -0.09212994575500488
Batch 15/64 loss: -0.10884737968444824
Batch 16/64 loss: -0.10542565584182739
Batch 17/64 loss: -0.09541577100753784
Batch 18/64 loss: -0.08636254072189331
Batch 19/64 loss: -0.07203125953674316
Batch 20/64 loss: -0.0793309211730957
Batch 21/64 loss: -0.0847817063331604
Batch 22/64 loss: -0.09304177761077881
Batch 23/64 loss: -0.07390773296356201
Batch 24/64 loss: -0.05853623151779175
Batch 25/64 loss: -0.08147335052490234
Batch 26/64 loss: -0.0818285346031189
Batch 27/64 loss: -0.0946805477142334
Batch 28/64 loss: -0.07505559921264648
Batch 29/64 loss: -0.08650457859039307
Batch 30/64 loss: -0.08336859941482544
Batch 31/64 loss: -0.07232940196990967
Batch 32/64 loss: -0.09287971258163452
Batch 33/64 loss: -0.06464916467666626
Batch 34/64 loss: -0.08659273386001587
Batch 35/64 loss: -0.0658341646194458
Batch 36/64 loss: -0.09431087970733643
Batch 37/64 loss: -0.06031203269958496
Batch 38/64 loss: -0.06769102811813354
Batch 39/64 loss: -0.08103758096694946
Batch 40/64 loss: -0.08323496580123901
Batch 41/64 loss: -0.09915292263031006
Batch 42/64 loss: -0.06659263372421265
Batch 43/64 loss: -0.08320808410644531
Batch 44/64 loss: -0.04167640209197998
Batch 45/64 loss: -0.07017171382904053
Batch 46/64 loss: -0.04672574996948242
Batch 47/64 loss: -0.07028365135192871
Batch 48/64 loss: -0.09325087070465088
Batch 49/64 loss: -0.07455450296401978
Batch 50/64 loss: -0.11112141609191895
Batch 51/64 loss: -0.1112523078918457
Batch 52/64 loss: -0.09419858455657959
Batch 53/64 loss: -0.08976566791534424
Batch 54/64 loss: -0.06769514083862305
Batch 55/64 loss: -0.053579509258270264
Batch 56/64 loss: -0.061825335025787354
Batch 57/64 loss: -0.07731276750564575
Batch 58/64 loss: -0.07657980918884277
Batch 59/64 loss: -0.09708905220031738
Batch 60/64 loss: -0.08648133277893066
Batch 61/64 loss: -0.08192682266235352
Batch 62/64 loss: -0.07899391651153564
Batch 63/64 loss: -0.06366485357284546
Batch 64/64 loss: -0.0821758508682251
Epoch 258  Train loss: -0.08057454567329557  Val loss: 0.04578017512547601
Epoch 259
-------------------------------
Batch 1/64 loss: -0.0926973819732666
Batch 2/64 loss: -0.07462799549102783
Batch 3/64 loss: -0.09537577629089355
Batch 4/64 loss: -0.08321458101272583
Batch 5/64 loss: -0.07997262477874756
Batch 6/64 loss: -0.0824962854385376
Batch 7/64 loss: -0.05263316631317139
Batch 8/64 loss: -0.07533520460128784
Batch 9/64 loss: -0.057581424713134766
Batch 10/64 loss: -0.09324294328689575
Batch 11/64 loss: -0.07394444942474365
Batch 12/64 loss: -0.09100931882858276
Batch 13/64 loss: -0.0863422155380249
Batch 14/64 loss: -0.07605111598968506
Batch 15/64 loss: -0.05632650852203369
Batch 16/64 loss: -0.07961058616638184
Batch 17/64 loss: -0.08675438165664673
Batch 18/64 loss: -0.07651227712631226
Batch 19/64 loss: -0.09823364019393921
Batch 20/64 loss: -0.07279682159423828
Batch 21/64 loss: -0.07966840267181396
Batch 22/64 loss: -0.08931565284729004
Batch 23/64 loss: -0.10758233070373535
Batch 24/64 loss: -0.11304247379302979
Batch 25/64 loss: -0.07676953077316284
Batch 26/64 loss: -0.08259248733520508
Batch 27/64 loss: -0.05578821897506714
Batch 28/64 loss: -0.06991225481033325
Batch 29/64 loss: -0.103748619556427
Batch 30/64 loss: -0.08993673324584961
Batch 31/64 loss: -0.08289217948913574
Batch 32/64 loss: -0.06903308629989624
Batch 33/64 loss: -0.07255363464355469
Batch 34/64 loss: -0.09078389406204224
Batch 35/64 loss: -0.0829157829284668
Batch 36/64 loss: -0.08648788928985596
Batch 37/64 loss: -0.05256462097167969
Batch 38/64 loss: -0.09560394287109375
Batch 39/64 loss: -0.0835798978805542
Batch 40/64 loss: -0.07061374187469482
Batch 41/64 loss: -0.082530677318573
Batch 42/64 loss: -0.05835080146789551
Batch 43/64 loss: -0.08850836753845215
Batch 44/64 loss: -0.07979345321655273
Batch 45/64 loss: -0.08812117576599121
Batch 46/64 loss: -0.08382809162139893
Batch 47/64 loss: -0.06709718704223633
Batch 48/64 loss: -0.06793832778930664
Batch 49/64 loss: -0.07497638463973999
Batch 50/64 loss: -0.056749045848846436
Batch 51/64 loss: -0.0997207760810852
Batch 52/64 loss: -0.06258946657180786
Batch 53/64 loss: -0.06883209943771362
Batch 54/64 loss: -0.07596468925476074
Batch 55/64 loss: -0.08810627460479736
Batch 56/64 loss: -0.05973243713378906
Batch 57/64 loss: -0.09021168947219849
Batch 58/64 loss: -0.07670074701309204
Batch 59/64 loss: -0.09674155712127686
Batch 60/64 loss: -0.0740928053855896
Batch 61/64 loss: -0.0990227460861206
Batch 62/64 loss: -0.06833642721176147
Batch 63/64 loss: -0.10236752033233643
Batch 64/64 loss: -0.0999496579170227
Epoch 259  Train loss: -0.08039873043696086  Val loss: 0.045282268647066096
Epoch 260
-------------------------------
Batch 1/64 loss: -0.10856157541275024
Batch 2/64 loss: -0.10922026634216309
Batch 3/64 loss: -0.08315807580947876
Batch 4/64 loss: -0.0803368091583252
Batch 5/64 loss: -0.07331448793411255
Batch 6/64 loss: -0.08647334575653076
Batch 7/64 loss: -0.08093005418777466
Batch 8/64 loss: -0.07864886522293091
Batch 9/64 loss: -0.048026859760284424
Batch 10/64 loss: -0.08933645486831665
Batch 11/64 loss: -0.07322829961776733
Batch 12/64 loss: -0.0671185851097107
Batch 13/64 loss: -0.08585703372955322
Batch 14/64 loss: -0.09242570400238037
Batch 15/64 loss: -0.059989094734191895
Batch 16/64 loss: -0.0842788815498352
Batch 17/64 loss: -0.0980294942855835
Batch 18/64 loss: -0.07838624715805054
Batch 19/64 loss: -0.09055912494659424
Batch 20/64 loss: -0.08172106742858887
Batch 21/64 loss: -0.07155454158782959
Batch 22/64 loss: -0.071269690990448
Batch 23/64 loss: -0.08859413862228394
Batch 24/64 loss: -0.08165556192398071
Batch 25/64 loss: -0.0823671817779541
Batch 26/64 loss: -0.08051252365112305
Batch 27/64 loss: -0.06471478939056396
Batch 28/64 loss: -0.06362611055374146
Batch 29/64 loss: -0.06803476810455322
Batch 30/64 loss: -0.09615921974182129
Batch 31/64 loss: -0.07398569583892822
Batch 32/64 loss: -0.06960439682006836
Batch 33/64 loss: -0.06469982862472534
Batch 34/64 loss: -0.06435561180114746
Batch 35/64 loss: -0.0915757417678833
Batch 36/64 loss: -0.09821784496307373
Batch 37/64 loss: -0.06594192981719971
Batch 38/64 loss: -0.0929187536239624
Batch 39/64 loss: -0.06629288196563721
Batch 40/64 loss: -0.061447083950042725
Batch 41/64 loss: -0.08408749103546143
Batch 42/64 loss: -0.046142399311065674
Batch 43/64 loss: -0.07699358463287354
Batch 44/64 loss: -0.08266991376876831
Batch 45/64 loss: -0.0724976658821106
Batch 46/64 loss: -0.0583726167678833
Batch 47/64 loss: -0.054296016693115234
Batch 48/64 loss: -0.07103919982910156
Batch 49/64 loss: -0.08075100183486938
Batch 50/64 loss: -0.09057784080505371
Batch 51/64 loss: -0.09445559978485107
Batch 52/64 loss: -0.07877767086029053
Batch 53/64 loss: -0.09746670722961426
Batch 54/64 loss: -0.05790531635284424
Batch 55/64 loss: -0.08808732032775879
Batch 56/64 loss: -0.0737982988357544
Batch 57/64 loss: -0.07840979099273682
Batch 58/64 loss: -0.05774247646331787
Batch 59/64 loss: -0.08894962072372437
Batch 60/64 loss: -0.0957217812538147
Batch 61/64 loss: -0.08756434917449951
Batch 62/64 loss: -0.0901605486869812
Batch 63/64 loss: -0.07483381032943726
Batch 64/64 loss: -0.09337782859802246
Epoch 260  Train loss: -0.07872098847931507  Val loss: 0.04379831146948116
Epoch 261
-------------------------------
Batch 1/64 loss: -0.09499090909957886
Batch 2/64 loss: -0.08919847011566162
Batch 3/64 loss: -0.0811624526977539
Batch 4/64 loss: -0.07405030727386475
Batch 5/64 loss: -0.07535558938980103
Batch 6/64 loss: -0.055506348609924316
Batch 7/64 loss: -0.09605306386947632
Batch 8/64 loss: -0.0777055025100708
Batch 9/64 loss: -0.08367913961410522
Batch 10/64 loss: -0.08944350481033325
Batch 11/64 loss: -0.07693618535995483
Batch 12/64 loss: -0.06657862663269043
Batch 13/64 loss: -0.08127468824386597
Batch 14/64 loss: -0.06692653894424438
Batch 15/64 loss: -0.09216320514678955
Batch 16/64 loss: -0.07455658912658691
Batch 17/64 loss: -0.08596795797348022
Batch 18/64 loss: -0.07959657907485962
Batch 19/64 loss: -0.08686071634292603
Batch 20/64 loss: -0.09802830219268799
Batch 21/64 loss: -0.060719966888427734
Batch 22/64 loss: -0.08887350559234619
Batch 23/64 loss: -0.08417749404907227
Batch 24/64 loss: -0.09070354700088501
Batch 25/64 loss: -0.10342365503311157
Batch 26/64 loss: -0.07806265354156494
Batch 27/64 loss: -0.11681747436523438
Batch 28/64 loss: -0.08896464109420776
Batch 29/64 loss: -0.06633186340332031
Batch 30/64 loss: -0.09271979331970215
Batch 31/64 loss: -0.08188420534133911
Batch 32/64 loss: -0.05910968780517578
Batch 33/64 loss: -0.05871957540512085
Batch 34/64 loss: -0.09210246801376343
Batch 35/64 loss: -0.09698230028152466
Batch 36/64 loss: -0.07292568683624268
Batch 37/64 loss: -0.09338688850402832
Batch 38/64 loss: -0.10343533754348755
Batch 39/64 loss: -0.08934211730957031
Batch 40/64 loss: -0.05197101831436157
Batch 41/64 loss: -0.07959878444671631
Batch 42/64 loss: -0.08470165729522705
Batch 43/64 loss: -0.06058090925216675
Batch 44/64 loss: -0.07043516635894775
Batch 45/64 loss: -0.08074671030044556
Batch 46/64 loss: -0.10102277994155884
Batch 47/64 loss: -0.09580457210540771
Batch 48/64 loss: -0.09139108657836914
Batch 49/64 loss: -0.09834688901901245
Batch 50/64 loss: -0.07430940866470337
Batch 51/64 loss: -0.07996940612792969
Batch 52/64 loss: -0.08861327171325684
Batch 53/64 loss: -0.07047510147094727
Batch 54/64 loss: -0.09516215324401855
Batch 55/64 loss: -0.08093202114105225
Batch 56/64 loss: -0.07228600978851318
Batch 57/64 loss: -0.05940985679626465
Batch 58/64 loss: -0.047592997550964355
Batch 59/64 loss: -0.0832861065864563
Batch 60/64 loss: -0.05620616674423218
Batch 61/64 loss: -0.09092098474502563
Batch 62/64 loss: -0.0882568359375
Batch 63/64 loss: -0.07542681694030762
Batch 64/64 loss: -0.04827207326889038
Epoch 261  Train loss: -0.08091558124504837  Val loss: 0.04498204824441077
Epoch 262
-------------------------------
Batch 1/64 loss: -0.07779741287231445
Batch 2/64 loss: -0.08526396751403809
Batch 3/64 loss: -0.05446678400039673
Batch 4/64 loss: -0.07321590185165405
Batch 5/64 loss: -0.08662289381027222
Batch 6/64 loss: -0.11624789237976074
Batch 7/64 loss: -0.0917825698852539
Batch 8/64 loss: -0.09294730424880981
Batch 9/64 loss: -0.08471876382827759
Batch 10/64 loss: -0.04197120666503906
Batch 11/64 loss: -0.08460229635238647
Batch 12/64 loss: -0.09855616092681885
Batch 13/64 loss: -0.07602280378341675
Batch 14/64 loss: -0.07729542255401611
Batch 15/64 loss: -0.10488712787628174
Batch 16/64 loss: -0.06685352325439453
Batch 17/64 loss: -0.06765341758728027
Batch 18/64 loss: -0.07284170389175415
Batch 19/64 loss: -0.0644608736038208
Batch 20/64 loss: -0.0857120156288147
Batch 21/64 loss: -0.08462953567504883
Batch 22/64 loss: -0.10155391693115234
Batch 23/64 loss: -0.08254939317703247
Batch 24/64 loss: -0.1030157208442688
Batch 25/64 loss: -0.08057558536529541
Batch 26/64 loss: -0.07624292373657227
Batch 27/64 loss: -0.009090960025787354
Batch 28/64 loss: -0.0742654800415039
Batch 29/64 loss: -0.0780453085899353
Batch 30/64 loss: -0.09200698137283325
Batch 31/64 loss: -0.09477752447128296
Batch 32/64 loss: -0.08049261569976807
Batch 33/64 loss: -0.09253931045532227
Batch 34/64 loss: -0.0614093542098999
Batch 35/64 loss: -0.08601737022399902
Batch 36/64 loss: -0.06044656038284302
Batch 37/64 loss: -0.11079710721969604
Batch 38/64 loss: -0.10629147291183472
Batch 39/64 loss: -0.07786905765533447
Batch 40/64 loss: -0.06812560558319092
Batch 41/64 loss: -0.0755767822265625
Batch 42/64 loss: -0.0917922854423523
Batch 43/64 loss: -0.10958194732666016
Batch 44/64 loss: -0.08398747444152832
Batch 45/64 loss: -0.08380448818206787
Batch 46/64 loss: -0.09179556369781494
Batch 47/64 loss: -0.08442097902297974
Batch 48/64 loss: -0.10079091787338257
Batch 49/64 loss: -0.08835262060165405
Batch 50/64 loss: -0.09177184104919434
Batch 51/64 loss: -0.07226753234863281
Batch 52/64 loss: -0.08237183094024658
Batch 53/64 loss: -0.06474363803863525
Batch 54/64 loss: -0.10356926918029785
Batch 55/64 loss: -0.09615433216094971
Batch 56/64 loss: -0.08991456031799316
Batch 57/64 loss: -0.06927454471588135
Batch 58/64 loss: -0.07366865873336792
Batch 59/64 loss: -0.05782580375671387
Batch 60/64 loss: -0.08655750751495361
Batch 61/64 loss: -0.0744587779045105
Batch 62/64 loss: -0.07832002639770508
Batch 63/64 loss: -0.09944349527359009
Batch 64/64 loss: -0.07823437452316284
Epoch 262  Train loss: -0.08209854878631292  Val loss: 0.047553381764192354
Epoch 263
-------------------------------
Batch 1/64 loss: -0.09328287839889526
Batch 2/64 loss: -0.08353030681610107
Batch 3/64 loss: -0.056546926498413086
Batch 4/64 loss: -0.09438890218734741
Batch 5/64 loss: -0.08223295211791992
Batch 6/64 loss: -0.098105788230896
Batch 7/64 loss: -0.11130452156066895
Batch 8/64 loss: -0.09390372037887573
Batch 9/64 loss: -0.060486018657684326
Batch 10/64 loss: -0.05391949415206909
Batch 11/64 loss: -0.08889150619506836
Batch 12/64 loss: -0.08027780055999756
Batch 13/64 loss: -0.07554197311401367
Batch 14/64 loss: -0.0891416072845459
Batch 15/64 loss: -0.06322664022445679
Batch 16/64 loss: -0.07624697685241699
Batch 17/64 loss: -0.06741064786911011
Batch 18/64 loss: -0.09304636716842651
Batch 19/64 loss: -0.0942416787147522
Batch 20/64 loss: -0.06406629085540771
Batch 21/64 loss: -0.09364891052246094
Batch 22/64 loss: -0.08001422882080078
Batch 23/64 loss: -0.08198308944702148
Batch 24/64 loss: -0.07566940784454346
Batch 25/64 loss: -0.04437524080276489
Batch 26/64 loss: -0.062241196632385254
Batch 27/64 loss: -0.08751827478408813
Batch 28/64 loss: -0.08515137434005737
Batch 29/64 loss: -0.07967394590377808
Batch 30/64 loss: -0.08002597093582153
Batch 31/64 loss: -0.06289082765579224
Batch 32/64 loss: -0.0914316177368164
Batch 33/64 loss: -0.049607813358306885
Batch 34/64 loss: -0.07818692922592163
Batch 35/64 loss: -0.08764159679412842
Batch 36/64 loss: -0.07427787780761719
Batch 37/64 loss: -0.08259731531143188
Batch 38/64 loss: -0.07107019424438477
Batch 39/64 loss: -0.10169243812561035
Batch 40/64 loss: -0.09738850593566895
Batch 41/64 loss: -0.08751344680786133
Batch 42/64 loss: -0.08404701948165894
Batch 43/64 loss: -0.050583720207214355
Batch 44/64 loss: -0.08822417259216309
Batch 45/64 loss: -0.08415669202804565
Batch 46/64 loss: -0.05353587865829468
Batch 47/64 loss: -0.061937034130096436
Batch 48/64 loss: -0.09387403726577759
Batch 49/64 loss: -0.07590949535369873
Batch 50/64 loss: -0.0397225022315979
Batch 51/64 loss: -0.10304737091064453
Batch 52/64 loss: -0.06546896696090698
Batch 53/64 loss: -0.08562886714935303
Batch 54/64 loss: -0.1070713996887207
Batch 55/64 loss: -0.09803974628448486
Batch 56/64 loss: -0.10599547624588013
Batch 57/64 loss: -0.10082972049713135
Batch 58/64 loss: -0.040886878967285156
Batch 59/64 loss: -0.07489204406738281
Batch 60/64 loss: -0.06356227397918701
Batch 61/64 loss: -0.08685219287872314
Batch 62/64 loss: -0.10282760858535767
Batch 63/64 loss: -0.08123260736465454
Batch 64/64 loss: -0.09483504295349121
Epoch 263  Train loss: -0.07990345393910128  Val loss: 0.04397995222065457
Epoch 264
-------------------------------
Batch 1/64 loss: -0.1067209243774414
Batch 2/64 loss: -0.09440946578979492
Batch 3/64 loss: -0.09342950582504272
Batch 4/64 loss: -0.09565138816833496
Batch 5/64 loss: -0.0823829174041748
Batch 6/64 loss: -0.09483355283737183
Batch 7/64 loss: -0.08936667442321777
Batch 8/64 loss: -0.07277649641036987
Batch 9/64 loss: -0.07928496599197388
Batch 10/64 loss: -0.09190046787261963
Batch 11/64 loss: -0.07862961292266846
Batch 12/64 loss: -0.10788929462432861
Batch 13/64 loss: -0.08629763126373291
Batch 14/64 loss: -0.07543247938156128
Batch 15/64 loss: -0.06983602046966553
Batch 16/64 loss: -0.08817631006240845
Batch 17/64 loss: -0.08790141344070435
Batch 18/64 loss: -0.05072301626205444
Batch 19/64 loss: -0.07659989595413208
Batch 20/64 loss: -0.08419007062911987
Batch 21/64 loss: -0.10680234432220459
Batch 22/64 loss: -0.08710348606109619
Batch 23/64 loss: -0.08543533086776733
Batch 24/64 loss: -0.10339552164077759
Batch 25/64 loss: -0.08884918689727783
Batch 26/64 loss: -0.076427161693573
Batch 27/64 loss: -0.06521707773208618
Batch 28/64 loss: -0.09563267230987549
Batch 29/64 loss: -0.0732927918434143
Batch 30/64 loss: -0.08608263731002808
Batch 31/64 loss: -0.08448028564453125
Batch 32/64 loss: -0.08046311140060425
Batch 33/64 loss: -0.06361401081085205
Batch 34/64 loss: -0.09950757026672363
Batch 35/64 loss: -0.09942448139190674
Batch 36/64 loss: -0.06273984909057617
Batch 37/64 loss: -0.0900697112083435
Batch 38/64 loss: -0.07621276378631592
Batch 39/64 loss: -0.04346519708633423
Batch 40/64 loss: -0.07018351554870605
Batch 41/64 loss: -0.07122933864593506
Batch 42/64 loss: -0.08977001905441284
Batch 43/64 loss: -0.04608052968978882
Batch 44/64 loss: -0.07327747344970703
Batch 45/64 loss: -0.09257692098617554
Batch 46/64 loss: -0.06673359870910645
Batch 47/64 loss: -0.07282990217208862
Batch 48/64 loss: -0.10052323341369629
Batch 49/64 loss: -0.0783606767654419
Batch 50/64 loss: -0.09456634521484375
Batch 51/64 loss: -0.07565736770629883
Batch 52/64 loss: -0.06727570295333862
Batch 53/64 loss: -0.06207776069641113
Batch 54/64 loss: -0.09210795164108276
Batch 55/64 loss: -0.10247218608856201
Batch 56/64 loss: -0.06482470035552979
Batch 57/64 loss: -0.08366882801055908
Batch 58/64 loss: -0.0847618579864502
Batch 59/64 loss: -0.08071184158325195
Batch 60/64 loss: -0.09488856792449951
Batch 61/64 loss: -0.0810583233833313
Batch 62/64 loss: -0.07422953844070435
Batch 63/64 loss: -0.07457530498504639
Batch 64/64 loss: -0.09354609251022339
Epoch 264  Train loss: -0.08218428784725713  Val loss: 0.04554823583753658
Epoch 265
-------------------------------
Batch 1/64 loss: -0.10180425643920898
Batch 2/64 loss: -0.10763728618621826
Batch 3/64 loss: -0.078277587890625
Batch 4/64 loss: -0.080036461353302
Batch 5/64 loss: -0.07727229595184326
Batch 6/64 loss: -0.08064210414886475
Batch 7/64 loss: -0.06242704391479492
Batch 8/64 loss: -0.06240570545196533
Batch 9/64 loss: -0.09514635801315308
Batch 10/64 loss: -0.049900710582733154
Batch 11/64 loss: -0.10139715671539307
Batch 12/64 loss: -0.08879286050796509
Batch 13/64 loss: -0.07103413343429565
Batch 14/64 loss: -0.07531917095184326
Batch 15/64 loss: -0.08229589462280273
Batch 16/64 loss: -0.08949923515319824
Batch 17/64 loss: -0.08630824089050293
Batch 18/64 loss: -0.07940590381622314
Batch 19/64 loss: -0.0925256609916687
Batch 20/64 loss: -0.05553489923477173
Batch 21/64 loss: -0.07010650634765625
Batch 22/64 loss: -0.09599876403808594
Batch 23/64 loss: -0.08056092262268066
Batch 24/64 loss: -0.04750645160675049
Batch 25/64 loss: -0.04250156879425049
Batch 26/64 loss: -0.07739913463592529
Batch 27/64 loss: -0.08352524042129517
Batch 28/64 loss: -0.09063756465911865
Batch 29/64 loss: -0.057537972927093506
Batch 30/64 loss: -0.07922637462615967
Batch 31/64 loss: -0.0725715160369873
Batch 32/64 loss: -0.0903170108795166
Batch 33/64 loss: -0.08252882957458496
Batch 34/64 loss: -0.10024547576904297
Batch 35/64 loss: -0.059884071350097656
Batch 36/64 loss: -0.07690179347991943
Batch 37/64 loss: -0.10563457012176514
Batch 38/64 loss: -0.09254586696624756
Batch 39/64 loss: -0.10290908813476562
Batch 40/64 loss: -0.09217822551727295
Batch 41/64 loss: -0.07628756761550903
Batch 42/64 loss: -0.09071552753448486
Batch 43/64 loss: -0.07116222381591797
Batch 44/64 loss: -0.06680411100387573
Batch 45/64 loss: -0.0889444351196289
Batch 46/64 loss: -0.07170164585113525
Batch 47/64 loss: -0.07605773210525513
Batch 48/64 loss: -0.10321223735809326
Batch 49/64 loss: -0.07869702577590942
Batch 50/64 loss: -0.08559757471084595
Batch 51/64 loss: -0.08090382814407349
Batch 52/64 loss: -0.03426384925842285
Batch 53/64 loss: -0.05141103267669678
Batch 54/64 loss: -0.082069993019104
Batch 55/64 loss: -0.09219574928283691
Batch 56/64 loss: -0.08808499574661255
Batch 57/64 loss: -0.07405132055282593
Batch 58/64 loss: -0.08709979057312012
Batch 59/64 loss: -0.06205141544342041
Batch 60/64 loss: -0.07145607471466064
Batch 61/64 loss: -0.0830264687538147
Batch 62/64 loss: -0.0929865837097168
Batch 63/64 loss: -0.08129113912582397
Batch 64/64 loss: -0.07259315252304077
Epoch 265  Train loss: -0.07941799374187694  Val loss: 0.04709519922118826
Epoch 266
-------------------------------
Batch 1/64 loss: -0.1115492582321167
Batch 2/64 loss: -0.08525466918945312
Batch 3/64 loss: -0.10341894626617432
Batch 4/64 loss: -0.06940895318984985
Batch 5/64 loss: -0.07682561874389648
Batch 6/64 loss: -0.0887410044670105
Batch 7/64 loss: -0.07154792547225952
Batch 8/64 loss: -0.08100014925003052
Batch 9/64 loss: -0.09466958045959473
Batch 10/64 loss: -0.047696709632873535
Batch 11/64 loss: -0.09414505958557129
Batch 12/64 loss: -0.0945928692817688
Batch 13/64 loss: -0.08723104000091553
Batch 14/64 loss: -0.11073416471481323
Batch 15/64 loss: -0.05449199676513672
Batch 16/64 loss: -0.08244657516479492
Batch 17/64 loss: -0.07955825328826904
Batch 18/64 loss: -0.05875438451766968
Batch 19/64 loss: -0.10492151975631714
Batch 20/64 loss: -0.08115160465240479
Batch 21/64 loss: -0.08052891492843628
Batch 22/64 loss: -0.08648800849914551
Batch 23/64 loss: -0.06621479988098145
Batch 24/64 loss: -0.08059459924697876
Batch 25/64 loss: -0.07015937566757202
Batch 26/64 loss: -0.09352916479110718
Batch 27/64 loss: -0.0810403823852539
Batch 28/64 loss: -0.05854606628417969
Batch 29/64 loss: -0.08754837512969971
Batch 30/64 loss: -0.08185434341430664
Batch 31/64 loss: -0.09597492218017578
Batch 32/64 loss: -0.07936495542526245
Batch 33/64 loss: -0.09566628932952881
Batch 34/64 loss: -0.07536911964416504
Batch 35/64 loss: -0.08936458826065063
Batch 36/64 loss: -0.09065163135528564
Batch 37/64 loss: -0.07676881551742554
Batch 38/64 loss: -0.08208215236663818
Batch 39/64 loss: -0.09353017807006836
Batch 40/64 loss: -0.11937391757965088
Batch 41/64 loss: -0.09294891357421875
Batch 42/64 loss: -0.07708311080932617
Batch 43/64 loss: -0.10491997003555298
Batch 44/64 loss: -0.06964045763015747
Batch 45/64 loss: -0.07709348201751709
Batch 46/64 loss: -0.1167902946472168
Batch 47/64 loss: -0.08708471059799194
Batch 48/64 loss: -0.07497048377990723
Batch 49/64 loss: -0.07865101099014282
Batch 50/64 loss: -0.07487839460372925
Batch 51/64 loss: -0.09918594360351562
Batch 52/64 loss: -0.09559357166290283
Batch 53/64 loss: -0.07093912363052368
Batch 54/64 loss: -0.08765047788619995
Batch 55/64 loss: -0.08098888397216797
Batch 56/64 loss: -0.09181803464889526
Batch 57/64 loss: -0.095583975315094
Batch 58/64 loss: -0.07753491401672363
Batch 59/64 loss: -0.11122739315032959
Batch 60/64 loss: -0.08979606628417969
Batch 61/64 loss: -0.06471520662307739
Batch 62/64 loss: -0.059941112995147705
Batch 63/64 loss: -0.06635689735412598
Batch 64/64 loss: -0.0781744122505188
Epoch 266  Train loss: -0.08418531955457201  Val loss: 0.04648410167890726
Epoch 267
-------------------------------
Batch 1/64 loss: -0.09075707197189331
Batch 2/64 loss: -0.07516998052597046
Batch 3/64 loss: -0.08667057752609253
Batch 4/64 loss: -0.09489715099334717
Batch 5/64 loss: -0.0810728669166565
Batch 6/64 loss: -0.07493853569030762
Batch 7/64 loss: -0.08686012029647827
Batch 8/64 loss: -0.12059342861175537
Batch 9/64 loss: -0.08782398700714111
Batch 10/64 loss: -0.11350184679031372
Batch 11/64 loss: -0.10169291496276855
Batch 12/64 loss: -0.09556752443313599
Batch 13/64 loss: -0.0928087830543518
Batch 14/64 loss: -0.10682576894760132
Batch 15/64 loss: -0.07130569219589233
Batch 16/64 loss: -0.08918416500091553
Batch 17/64 loss: -0.09689480066299438
Batch 18/64 loss: -0.08762991428375244
Batch 19/64 loss: -0.09079945087432861
Batch 20/64 loss: -0.09200292825698853
Batch 21/64 loss: -0.07355767488479614
Batch 22/64 loss: -0.09767645597457886
Batch 23/64 loss: -0.04800015687942505
Batch 24/64 loss: -0.07812565565109253
Batch 25/64 loss: -0.07611554861068726
Batch 26/64 loss: -0.08146029710769653
Batch 27/64 loss: -0.08964252471923828
Batch 28/64 loss: -0.07843446731567383
Batch 29/64 loss: -0.0938417911529541
Batch 30/64 loss: -0.04236412048339844
Batch 31/64 loss: -0.08521723747253418
Batch 32/64 loss: -0.08221006393432617
Batch 33/64 loss: -0.08608496189117432
Batch 34/64 loss: -0.10552561283111572
Batch 35/64 loss: -0.10446292161941528
Batch 36/64 loss: -0.09168148040771484
Batch 37/64 loss: -0.08532834053039551
Batch 38/64 loss: -0.09938710927963257
Batch 39/64 loss: -0.08074092864990234
Batch 40/64 loss: -0.09009122848510742
Batch 41/64 loss: -0.096554696559906
Batch 42/64 loss: -0.09110188484191895
Batch 43/64 loss: -0.09942573308944702
Batch 44/64 loss: -0.07612568140029907
Batch 45/64 loss: -0.08009886741638184
Batch 46/64 loss: -0.049420833587646484
Batch 47/64 loss: -0.08818548917770386
Batch 48/64 loss: -0.07225966453552246
Batch 49/64 loss: -0.07822394371032715
Batch 50/64 loss: -0.08306944370269775
Batch 51/64 loss: -0.07254302501678467
Batch 52/64 loss: -0.07345658540725708
Batch 53/64 loss: -0.06936383247375488
Batch 54/64 loss: -0.09139609336853027
Batch 55/64 loss: -0.062428414821624756
Batch 56/64 loss: -0.0739700198173523
Batch 57/64 loss: -0.04967719316482544
Batch 58/64 loss: -0.06179499626159668
Batch 59/64 loss: -0.06845623254776001
Batch 60/64 loss: -0.07882064580917358
Batch 61/64 loss: -0.09595608711242676
Batch 62/64 loss: -0.09969568252563477
Batch 63/64 loss: -0.08885490894317627
Batch 64/64 loss: -0.07994329929351807
Epoch 267  Train loss: -0.08420052575130088  Val loss: 0.04767587442987973
Epoch 268
-------------------------------
Batch 1/64 loss: -0.09757053852081299
Batch 2/64 loss: -0.07872730493545532
Batch 3/64 loss: -0.09758859872817993
Batch 4/64 loss: -0.11118489503860474
Batch 5/64 loss: -0.07538306713104248
Batch 6/64 loss: -0.07037955522537231
Batch 7/64 loss: -0.08861637115478516
Batch 8/64 loss: -0.07736891508102417
Batch 9/64 loss: -0.08317965269088745
Batch 10/64 loss: -0.11540782451629639
Batch 11/64 loss: -0.1074400544166565
Batch 12/64 loss: -0.06961065530776978
Batch 13/64 loss: -0.08175462484359741
Batch 14/64 loss: -0.0969773530960083
Batch 15/64 loss: -0.07379138469696045
Batch 16/64 loss: -0.07631522417068481
Batch 17/64 loss: -0.09175187349319458
Batch 18/64 loss: -0.0866241455078125
Batch 19/64 loss: -0.08939099311828613
Batch 20/64 loss: -0.0875735878944397
Batch 21/64 loss: -0.1093829870223999
Batch 22/64 loss: -0.07884126901626587
Batch 23/64 loss: -0.08371883630752563
Batch 24/64 loss: -0.07289141416549683
Batch 25/64 loss: -0.06946873664855957
Batch 26/64 loss: -0.0871652364730835
Batch 27/64 loss: -0.10319769382476807
Batch 28/64 loss: -0.07450497150421143
Batch 29/64 loss: -0.06976544857025146
Batch 30/64 loss: -0.0792609453201294
Batch 31/64 loss: -0.09750288724899292
Batch 32/64 loss: -0.08112478256225586
Batch 33/64 loss: -0.06183743476867676
Batch 34/64 loss: -0.0938560962677002
Batch 35/64 loss: -0.08891355991363525
Batch 36/64 loss: -0.10052096843719482
Batch 37/64 loss: -0.09202003479003906
Batch 38/64 loss: -0.0731658935546875
Batch 39/64 loss: -0.09722769260406494
Batch 40/64 loss: -0.09811007976531982
Batch 41/64 loss: -0.09127151966094971
Batch 42/64 loss: -0.0077890753746032715
Batch 43/64 loss: -0.08323979377746582
Batch 44/64 loss: -0.0662010908126831
Batch 45/64 loss: -0.08787041902542114
Batch 46/64 loss: -0.1015770435333252
Batch 47/64 loss: -0.08522802591323853
Batch 48/64 loss: -0.09775596857070923
Batch 49/64 loss: -0.09145665168762207
Batch 50/64 loss: -0.09717726707458496
Batch 51/64 loss: -0.08858829736709595
Batch 52/64 loss: -0.08780145645141602
Batch 53/64 loss: -0.06707870960235596
Batch 54/64 loss: -0.08627533912658691
Batch 55/64 loss: -0.09843873977661133
Batch 56/64 loss: -0.07934796810150146
Batch 57/64 loss: -0.08979964256286621
Batch 58/64 loss: -0.056282222270965576
Batch 59/64 loss: -0.07165378332138062
Batch 60/64 loss: -0.08761680126190186
Batch 61/64 loss: -0.0968923568725586
Batch 62/64 loss: -0.07618927955627441
Batch 63/64 loss: -0.09101688861846924
Batch 64/64 loss: -0.06911122798919678
Epoch 268  Train loss: -0.08482345646502924  Val loss: 0.046418229124390385
Epoch 269
-------------------------------
Batch 1/64 loss: -0.07982057332992554
Batch 2/64 loss: -0.11075830459594727
Batch 3/64 loss: -0.07817840576171875
Batch 4/64 loss: -0.0830068588256836
Batch 5/64 loss: -0.10051989555358887
Batch 6/64 loss: -0.08530372381210327
Batch 7/64 loss: -0.06768053770065308
Batch 8/64 loss: -0.09181845188140869
Batch 9/64 loss: -0.08840525150299072
Batch 10/64 loss: -0.10000783205032349
Batch 11/64 loss: -0.08728164434432983
Batch 12/64 loss: -0.09611088037490845
Batch 13/64 loss: -0.07153630256652832
Batch 14/64 loss: -0.08074444532394409
Batch 15/64 loss: -0.10972058773040771
Batch 16/64 loss: -0.04827505350112915
Batch 17/64 loss: -0.09954357147216797
Batch 18/64 loss: -0.11296993494033813
Batch 19/64 loss: -0.09548193216323853
Batch 20/64 loss: -0.07790768146514893
Batch 21/64 loss: -0.0751996636390686
Batch 22/64 loss: -0.08951830863952637
Batch 23/64 loss: -0.09819382429122925
Batch 24/64 loss: -0.08924746513366699
Batch 25/64 loss: -0.07818657159805298
Batch 26/64 loss: -0.08546638488769531
Batch 27/64 loss: -0.08121418952941895
Batch 28/64 loss: -0.05558311939239502
Batch 29/64 loss: -0.1157715916633606
Batch 30/64 loss: -0.07950001955032349
Batch 31/64 loss: -0.06307858228683472
Batch 32/64 loss: -0.08594828844070435
Batch 33/64 loss: -0.09729897975921631
Batch 34/64 loss: -0.08079659938812256
Batch 35/64 loss: -0.06783401966094971
Batch 36/64 loss: -0.05048263072967529
Batch 37/64 loss: -0.086750328540802
Batch 38/64 loss: -0.06846153736114502
Batch 39/64 loss: -0.07139182090759277
Batch 40/64 loss: -0.06086963415145874
Batch 41/64 loss: -0.08472788333892822
Batch 42/64 loss: -0.10499805212020874
Batch 43/64 loss: -0.1009223461151123
Batch 44/64 loss: -0.07434642314910889
Batch 45/64 loss: -0.08149361610412598
Batch 46/64 loss: -0.07213127613067627
Batch 47/64 loss: -0.09479939937591553
Batch 48/64 loss: -0.08181095123291016
Batch 49/64 loss: -0.09499633312225342
Batch 50/64 loss: -0.09670335054397583
Batch 51/64 loss: -0.08949637413024902
Batch 52/64 loss: -0.07770520448684692
Batch 53/64 loss: -0.09310734272003174
Batch 54/64 loss: -0.07524293661117554
Batch 55/64 loss: -0.05727827548980713
Batch 56/64 loss: -0.049924254417419434
Batch 57/64 loss: -0.07141941785812378
Batch 58/64 loss: -0.0685722827911377
Batch 59/64 loss: -0.08066785335540771
Batch 60/64 loss: -0.06641674041748047
Batch 61/64 loss: -0.10239118337631226
Batch 62/64 loss: -0.0754326581954956
Batch 63/64 loss: -0.06979769468307495
Batch 64/64 loss: -0.08286887407302856
Epoch 269  Train loss: -0.08270429700028663  Val loss: 0.044451279328860775
Epoch 270
-------------------------------
Batch 1/64 loss: -0.09661358594894409
Batch 2/64 loss: -0.07880330085754395
Batch 3/64 loss: -0.0926172137260437
Batch 4/64 loss: -0.05839955806732178
Batch 5/64 loss: -0.09096348285675049
Batch 6/64 loss: -0.07614439725875854
Batch 7/64 loss: -0.08456820249557495
Batch 8/64 loss: -0.06988328695297241
Batch 9/64 loss: -0.07846194505691528
Batch 10/64 loss: -0.07886242866516113
Batch 11/64 loss: -0.0996466875076294
Batch 12/64 loss: -0.06347328424453735
Batch 13/64 loss: -0.07836097478866577
Batch 14/64 loss: -0.09079104661941528
Batch 15/64 loss: -0.09205073118209839
Batch 16/64 loss: -0.10972470045089722
Batch 17/64 loss: -0.07109439373016357
Batch 18/64 loss: -0.09661358594894409
Batch 19/64 loss: -0.06031745672225952
Batch 20/64 loss: -0.06308931112289429
Batch 21/64 loss: -0.05676764249801636
Batch 22/64 loss: -0.07310366630554199
Batch 23/64 loss: -0.08869659900665283
Batch 24/64 loss: -0.0824594497680664
Batch 25/64 loss: -0.09185910224914551
Batch 26/64 loss: -0.09045785665512085
Batch 27/64 loss: -0.08894103765487671
Batch 28/64 loss: -0.09940171241760254
Batch 29/64 loss: -0.10318124294281006
Batch 30/64 loss: -0.0814395546913147
Batch 31/64 loss: -0.10275566577911377
Batch 32/64 loss: -0.07847201824188232
Batch 33/64 loss: -0.10188353061676025
Batch 34/64 loss: -0.07101500034332275
Batch 35/64 loss: -0.07918357849121094
Batch 36/64 loss: -0.09739923477172852
Batch 37/64 loss: -0.09664136171340942
Batch 38/64 loss: -0.09987819194793701
Batch 39/64 loss: -0.06184953451156616
Batch 40/64 loss: -0.08013540506362915
Batch 41/64 loss: -0.07271355390548706
Batch 42/64 loss: -0.058696627616882324
Batch 43/64 loss: -0.08424901962280273
Batch 44/64 loss: -0.07761555910110474
Batch 45/64 loss: -0.05779826641082764
Batch 46/64 loss: -0.07122927904129028
Batch 47/64 loss: -0.0819321870803833
Batch 48/64 loss: -0.07773071527481079
Batch 49/64 loss: -0.0861588716506958
Batch 50/64 loss: -0.059867143630981445
Batch 51/64 loss: -0.07630515098571777
Batch 52/64 loss: -0.08392190933227539
Batch 53/64 loss: -0.08358556032180786
Batch 54/64 loss: -0.0968015193939209
Batch 55/64 loss: -0.11088800430297852
Batch 56/64 loss: -0.08904194831848145
Batch 57/64 loss: -0.07228124141693115
Batch 58/64 loss: -0.06651580333709717
Batch 59/64 loss: -0.07981181144714355
Batch 60/64 loss: -0.08861833810806274
Batch 61/64 loss: -0.1179381012916565
Batch 62/64 loss: -0.0936959981918335
Batch 63/64 loss: -0.09339743852615356
Batch 64/64 loss: -0.08525419235229492
Epoch 270  Train loss: -0.08314879454818426  Val loss: 0.04674580580590107
Epoch 271
-------------------------------
Batch 1/64 loss: -0.08022874593734741
Batch 2/64 loss: -0.10741961002349854
Batch 3/64 loss: -0.07793587446212769
Batch 4/64 loss: -0.1008005142211914
Batch 5/64 loss: -0.07834672927856445
Batch 6/64 loss: -0.10642135143280029
Batch 7/64 loss: -0.06526625156402588
Batch 8/64 loss: -0.0843735933303833
Batch 9/64 loss: -0.10183501243591309
Batch 10/64 loss: -0.0709913969039917
Batch 11/64 loss: -0.1030263900756836
Batch 12/64 loss: -0.07679778337478638
Batch 13/64 loss: -0.08277708292007446
Batch 14/64 loss: -0.0911397933959961
Batch 15/64 loss: -0.0893089771270752
Batch 16/64 loss: -0.09252071380615234
Batch 17/64 loss: -0.10058492422103882
Batch 18/64 loss: -0.07808089256286621
Batch 19/64 loss: -0.05679035186767578
Batch 20/64 loss: -0.07906323671340942
Batch 21/64 loss: -0.08014571666717529
Batch 22/64 loss: -0.05719059705734253
Batch 23/64 loss: -0.08758765459060669
Batch 24/64 loss: -0.06601488590240479
Batch 25/64 loss: -0.09073448181152344
Batch 26/64 loss: -0.09328454732894897
Batch 27/64 loss: -0.0866730809211731
Batch 28/64 loss: -0.07422608137130737
Batch 29/64 loss: -0.07045876979827881
Batch 30/64 loss: -0.04088979959487915
Batch 31/64 loss: -0.0937228798866272
Batch 32/64 loss: -0.09420567750930786
Batch 33/64 loss: -0.09543442726135254
Batch 34/64 loss: -0.09781819581985474
Batch 35/64 loss: -0.09279859066009521
Batch 36/64 loss: -0.09939026832580566
Batch 37/64 loss: -0.0893775224685669
Batch 38/64 loss: -0.08012616634368896
Batch 39/64 loss: -0.08258450031280518
Batch 40/64 loss: -0.08495700359344482
Batch 41/64 loss: -0.0857917070388794
Batch 42/64 loss: -0.09018582105636597
Batch 43/64 loss: -0.09343653917312622
Batch 44/64 loss: -0.10645055770874023
Batch 45/64 loss: -0.0739595890045166
Batch 46/64 loss: -0.07844996452331543
Batch 47/64 loss: -0.06863582134246826
Batch 48/64 loss: -0.06957024335861206
Batch 49/64 loss: -0.099254310131073
Batch 50/64 loss: -0.10159355401992798
Batch 51/64 loss: -0.06251800060272217
Batch 52/64 loss: -0.06800585985183716
Batch 53/64 loss: -0.09796035289764404
Batch 54/64 loss: -0.0908668041229248
Batch 55/64 loss: -0.09327912330627441
Batch 56/64 loss: -0.0809774398803711
Batch 57/64 loss: -0.06745117902755737
Batch 58/64 loss: -0.08661055564880371
Batch 59/64 loss: -0.06612569093704224
Batch 60/64 loss: -0.062071263790130615
Batch 61/64 loss: -0.08370369672775269
Batch 62/64 loss: -0.1024017333984375
Batch 63/64 loss: -0.08169043064117432
Batch 64/64 loss: -0.042388319969177246
Epoch 271  Train loss: -0.08351547531053131  Val loss: 0.04582418526980475
Epoch 272
-------------------------------
Batch 1/64 loss: -0.09703594446182251
Batch 2/64 loss: -0.054614484310150146
Batch 3/64 loss: -0.07924365997314453
Batch 4/64 loss: -0.08992362022399902
Batch 5/64 loss: -0.08563864231109619
Batch 6/64 loss: -0.08703863620758057
Batch 7/64 loss: -0.09909677505493164
Batch 8/64 loss: -0.08743977546691895
Batch 9/64 loss: -0.07086050510406494
Batch 10/64 loss: -0.08300191164016724
Batch 11/64 loss: -0.10186076164245605
Batch 12/64 loss: -0.055137455463409424
Batch 13/64 loss: -0.09737086296081543
Batch 14/64 loss: -0.04741793870925903
Batch 15/64 loss: -0.07290863990783691
Batch 16/64 loss: -0.1057845950126648
Batch 17/64 loss: -0.09998893737792969
Batch 18/64 loss: -0.048914313316345215
Batch 19/64 loss: -0.09633314609527588
Batch 20/64 loss: -0.08991563320159912
Batch 21/64 loss: -0.07442444562911987
Batch 22/64 loss: -0.09887397289276123
Batch 23/64 loss: -0.07386243343353271
Batch 24/64 loss: -0.09139806032180786
Batch 25/64 loss: -0.08933055400848389
Batch 26/64 loss: -0.0614621639251709
Batch 27/64 loss: -0.08200085163116455
Batch 28/64 loss: -0.07922303676605225
Batch 29/64 loss: -0.09798794984817505
Batch 30/64 loss: -0.0934935212135315
Batch 31/64 loss: -0.09542924165725708
Batch 32/64 loss: -0.08353149890899658
Batch 33/64 loss: -0.06878691911697388
Batch 34/64 loss: -0.08744734525680542
Batch 35/64 loss: -0.08839148283004761
Batch 36/64 loss: -0.08920955657958984
Batch 37/64 loss: -0.0975717306137085
Batch 38/64 loss: -0.07401180267333984
Batch 39/64 loss: -0.08126276731491089
Batch 40/64 loss: -0.06257826089859009
Batch 41/64 loss: -0.08640468120574951
Batch 42/64 loss: -0.0781596302986145
Batch 43/64 loss: -0.09264814853668213
Batch 44/64 loss: -0.0755685567855835
Batch 45/64 loss: -0.08018440008163452
Batch 46/64 loss: -0.08066409826278687
Batch 47/64 loss: -0.11558514833450317
Batch 48/64 loss: -0.1022266149520874
Batch 49/64 loss: -0.08710891008377075
Batch 50/64 loss: -0.08294916152954102
Batch 51/64 loss: -0.070559561252594
Batch 52/64 loss: -0.09687912464141846
Batch 53/64 loss: -0.06900960206985474
Batch 54/64 loss: -0.09121507406234741
Batch 55/64 loss: -0.07698523998260498
Batch 56/64 loss: -0.07520550489425659
Batch 57/64 loss: -0.08568090200424194
Batch 58/64 loss: -0.08794605731964111
Batch 59/64 loss: -0.09476965665817261
Batch 60/64 loss: -0.06349688768386841
Batch 61/64 loss: -0.11623835563659668
Batch 62/64 loss: -0.07241475582122803
Batch 63/64 loss: -0.08754831552505493
Batch 64/64 loss: -0.0726127028465271
Epoch 272  Train loss: -0.08382292939167396  Val loss: 0.04632015510932686
Epoch 273
-------------------------------
Batch 1/64 loss: -0.09587675333023071
Batch 2/64 loss: -0.09766852855682373
Batch 3/64 loss: -0.12028539180755615
Batch 4/64 loss: -0.10133528709411621
Batch 5/64 loss: -0.09830451011657715
Batch 6/64 loss: -0.10622638463973999
Batch 7/64 loss: -0.08961713314056396
Batch 8/64 loss: -0.09627330303192139
Batch 9/64 loss: -0.1150546669960022
Batch 10/64 loss: -0.06519514322280884
Batch 11/64 loss: -0.1254447102546692
Batch 12/64 loss: -0.06873637437820435
Batch 13/64 loss: -0.10565555095672607
Batch 14/64 loss: -0.11370176076889038
Batch 15/64 loss: -0.09317070245742798
Batch 16/64 loss: -0.07774096727371216
Batch 17/64 loss: -0.07613027095794678
Batch 18/64 loss: -0.10990774631500244
Batch 19/64 loss: -0.11313581466674805
Batch 20/64 loss: -0.07099878787994385
Batch 21/64 loss: -0.10249686241149902
Batch 22/64 loss: -0.11984038352966309
Batch 23/64 loss: -0.05917024612426758
Batch 24/64 loss: -0.07288342714309692
Batch 25/64 loss: -0.08934932947158813
Batch 26/64 loss: -0.08506155014038086
Batch 27/64 loss: -0.08136886358261108
Batch 28/64 loss: -0.08424341678619385
Batch 29/64 loss: -0.10226058959960938
Batch 30/64 loss: -0.09019052982330322
Batch 31/64 loss: -0.097994863986969
Batch 32/64 loss: -0.0734180212020874
Batch 33/64 loss: -0.07119154930114746
Batch 34/64 loss: -0.09488308429718018
Batch 35/64 loss: -0.0814024806022644
Batch 36/64 loss: -0.09961992502212524
Batch 37/64 loss: -0.08278459310531616
Batch 38/64 loss: -0.04655975103378296
Batch 39/64 loss: -0.08676135540008545
Batch 40/64 loss: -0.08821099996566772
Batch 41/64 loss: -0.08376967906951904
Batch 42/64 loss: -0.09211629629135132
Batch 43/64 loss: -0.08514833450317383
Batch 44/64 loss: -0.09833681583404541
Batch 45/64 loss: -0.08708310127258301
Batch 46/64 loss: -0.05950206518173218
Batch 47/64 loss: -0.06237924098968506
Batch 48/64 loss: -0.09575808048248291
Batch 49/64 loss: -0.0709085464477539
Batch 50/64 loss: -0.08561384677886963
Batch 51/64 loss: -0.08446645736694336
Batch 52/64 loss: -0.08338737487792969
Batch 53/64 loss: -0.06742089986801147
Batch 54/64 loss: -0.07558709383010864
Batch 55/64 loss: -0.08233392238616943
Batch 56/64 loss: -0.05638754367828369
Batch 57/64 loss: -0.0665329098701477
Batch 58/64 loss: -0.09787958860397339
Batch 59/64 loss: -0.09330999851226807
Batch 60/64 loss: -0.08837485313415527
Batch 61/64 loss: -0.081260085105896
Batch 62/64 loss: -0.09505051374435425
Batch 63/64 loss: -0.07624316215515137
Batch 64/64 loss: -0.06947928667068481
Epoch 273  Train loss: -0.08738998408411064  Val loss: 0.04719952332604792
Epoch 274
-------------------------------
Batch 1/64 loss: -0.09546661376953125
Batch 2/64 loss: -0.08476459980010986
Batch 3/64 loss: -0.08560502529144287
Batch 4/64 loss: -0.08673518896102905
Batch 5/64 loss: -0.08557868003845215
Batch 6/64 loss: -0.07162356376647949
Batch 7/64 loss: -0.10610818862915039
Batch 8/64 loss: -0.09735679626464844
Batch 9/64 loss: -0.1037188172340393
Batch 10/64 loss: -0.08159047365188599
Batch 11/64 loss: -0.1010405421257019
Batch 12/64 loss: -0.0706072449684143
Batch 13/64 loss: -0.09278404712677002
Batch 14/64 loss: -0.07699048519134521
Batch 15/64 loss: -0.1088259220123291
Batch 16/64 loss: -0.08262020349502563
Batch 17/64 loss: -0.09273272752761841
Batch 18/64 loss: -0.10880804061889648
Batch 19/64 loss: -0.08516007661819458
Batch 20/64 loss: -0.10179358720779419
Batch 21/64 loss: -0.09177970886230469
Batch 22/64 loss: -0.0827707052230835
Batch 23/64 loss: -0.09078997373580933
Batch 24/64 loss: -0.10381335020065308
Batch 25/64 loss: -0.09128695726394653
Batch 26/64 loss: -0.11005508899688721
Batch 27/64 loss: -0.1117047667503357
Batch 28/64 loss: -0.07516026496887207
Batch 29/64 loss: -0.06030416488647461
Batch 30/64 loss: -0.09986519813537598
Batch 31/64 loss: -0.07894432544708252
Batch 32/64 loss: -0.09321653842926025
Batch 33/64 loss: -0.07228314876556396
Batch 34/64 loss: -0.08637446165084839
Batch 35/64 loss: -0.11620765924453735
Batch 36/64 loss: -0.06778144836425781
Batch 37/64 loss: -0.07808458805084229
Batch 38/64 loss: -0.08672094345092773
Batch 39/64 loss: -0.1094658374786377
Batch 40/64 loss: -0.09802532196044922
Batch 41/64 loss: -0.037495434284210205
Batch 42/64 loss: -0.08859539031982422
Batch 43/64 loss: -0.09332138299942017
Batch 44/64 loss: -0.104259192943573
Batch 45/64 loss: -0.07243925333023071
Batch 46/64 loss: -0.07731539011001587
Batch 47/64 loss: -0.10102760791778564
Batch 48/64 loss: -0.08245354890823364
Batch 49/64 loss: -0.09290355443954468
Batch 50/64 loss: -0.10032844543457031
Batch 51/64 loss: -0.0726582407951355
Batch 52/64 loss: -0.08142352104187012
Batch 53/64 loss: -0.08567571640014648
Batch 54/64 loss: -0.09440571069717407
Batch 55/64 loss: -0.10383117198944092
Batch 56/64 loss: -0.07412046194076538
Batch 57/64 loss: -0.05857717990875244
Batch 58/64 loss: -0.0785720944404602
Batch 59/64 loss: -0.09947234392166138
Batch 60/64 loss: -0.06354814767837524
Batch 61/64 loss: -0.08423972129821777
Batch 62/64 loss: -0.08711743354797363
Batch 63/64 loss: -0.08678197860717773
Batch 64/64 loss: -0.08756476640701294
Epoch 274  Train loss: -0.08801226312038946  Val loss: 0.04742263190934748
Epoch 275
-------------------------------
Batch 1/64 loss: -0.10655558109283447
Batch 2/64 loss: -0.07533597946166992
Batch 3/64 loss: -0.07328599691390991
Batch 4/64 loss: -0.11061090230941772
Batch 5/64 loss: -0.09321248531341553
Batch 6/64 loss: -0.10927581787109375
Batch 7/64 loss: -0.08421188592910767
Batch 8/64 loss: -0.10843992233276367
Batch 9/64 loss: -0.08662247657775879
Batch 10/64 loss: -0.10503584146499634
Batch 11/64 loss: -0.07650583982467651
Batch 12/64 loss: -0.09892892837524414
Batch 13/64 loss: -0.10982447862625122
Batch 14/64 loss: -0.11632061004638672
Batch 15/64 loss: -0.07424676418304443
Batch 16/64 loss: -0.04452991485595703
Batch 17/64 loss: -0.09476244449615479
Batch 18/64 loss: -0.08603686094284058
Batch 19/64 loss: -0.09508764743804932
Batch 20/64 loss: -0.10659968852996826
Batch 21/64 loss: -0.0896034836769104
Batch 22/64 loss: -0.08678698539733887
Batch 23/64 loss: -0.07790881395339966
Batch 24/64 loss: -0.09062039852142334
Batch 25/64 loss: -0.07940304279327393
Batch 26/64 loss: -0.08380663394927979
Batch 27/64 loss: -0.07450228929519653
Batch 28/64 loss: -0.06882786750793457
Batch 29/64 loss: -0.07401156425476074
Batch 30/64 loss: -0.10360860824584961
Batch 31/64 loss: -0.10710692405700684
Batch 32/64 loss: -0.09576690196990967
Batch 33/64 loss: -0.05986368656158447
Batch 34/64 loss: -0.10753268003463745
Batch 35/64 loss: -0.09792017936706543
Batch 36/64 loss: -0.07100731134414673
Batch 37/64 loss: -0.0793452262878418
Batch 38/64 loss: -0.0888475775718689
Batch 39/64 loss: -0.0926637053489685
Batch 40/64 loss: -0.0799981951713562
Batch 41/64 loss: -0.08975225687026978
Batch 42/64 loss: -0.07166707515716553
Batch 43/64 loss: -0.06910312175750732
Batch 44/64 loss: -0.07987785339355469
Batch 45/64 loss: -0.09150421619415283
Batch 46/64 loss: -0.09530466794967651
Batch 47/64 loss: -0.08960533142089844
Batch 48/64 loss: -0.10058349370956421
Batch 49/64 loss: -0.10599011182785034
Batch 50/64 loss: -0.08868366479873657
Batch 51/64 loss: -0.0928342342376709
Batch 52/64 loss: -0.09562408924102783
Batch 53/64 loss: -0.08728975057601929
Batch 54/64 loss: -0.08982020616531372
Batch 55/64 loss: -0.09425938129425049
Batch 56/64 loss: -0.09601104259490967
Batch 57/64 loss: -0.07302278280258179
Batch 58/64 loss: -0.05513685941696167
Batch 59/64 loss: -0.07587039470672607
Batch 60/64 loss: -0.08326256275177002
Batch 61/64 loss: -0.06583988666534424
Batch 62/64 loss: -0.10264343023300171
Batch 63/64 loss: -0.09187138080596924
Batch 64/64 loss: -0.06719028949737549
Epoch 275  Train loss: -0.08785117887983135  Val loss: 0.04869915529624703
Epoch 276
-------------------------------
Batch 1/64 loss: -0.10933583974838257
Batch 2/64 loss: -0.10058140754699707
Batch 3/64 loss: -0.09468507766723633
Batch 4/64 loss: -0.07327777147293091
Batch 5/64 loss: -0.05921030044555664
Batch 6/64 loss: -0.10410511493682861
Batch 7/64 loss: -0.08212012052536011
Batch 8/64 loss: -0.0837518572807312
Batch 9/64 loss: -0.08225101232528687
Batch 10/64 loss: -0.10537075996398926
Batch 11/64 loss: -0.08737260103225708
Batch 12/64 loss: -0.09304100275039673
Batch 13/64 loss: -0.09160232543945312
Batch 14/64 loss: -0.06183481216430664
Batch 15/64 loss: -0.08690023422241211
Batch 16/64 loss: -0.08709371089935303
Batch 17/64 loss: -0.09274488687515259
Batch 18/64 loss: -0.10716807842254639
Batch 19/64 loss: -0.11168801784515381
Batch 20/64 loss: -0.08339107036590576
Batch 21/64 loss: -0.09533840417861938
Batch 22/64 loss: -0.09260380268096924
Batch 23/64 loss: -0.0843285322189331
Batch 24/64 loss: -0.06435376405715942
Batch 25/64 loss: -0.07746690511703491
Batch 26/64 loss: -0.10585355758666992
Batch 27/64 loss: -0.10531294345855713
Batch 28/64 loss: -0.0925438404083252
Batch 29/64 loss: -0.07554471492767334
Batch 30/64 loss: -0.11364281177520752
Batch 31/64 loss: -0.09699112176895142
Batch 32/64 loss: -0.08653557300567627
Batch 33/64 loss: -0.11017423868179321
Batch 34/64 loss: -0.11131888628005981
Batch 35/64 loss: -0.10483241081237793
Batch 36/64 loss: -0.09100937843322754
Batch 37/64 loss: -0.08065605163574219
Batch 38/64 loss: -0.07530373334884644
Batch 39/64 loss: -0.07785636186599731
Batch 40/64 loss: -0.07753235101699829
Batch 41/64 loss: -0.10255491733551025
Batch 42/64 loss: -0.0810394287109375
Batch 43/64 loss: -0.08424508571624756
Batch 44/64 loss: -0.061106860637664795
Batch 45/64 loss: -0.0977974534034729
Batch 46/64 loss: -0.09872961044311523
Batch 47/64 loss: -0.08841335773468018
Batch 48/64 loss: -0.09079653024673462
Batch 49/64 loss: -0.08537381887435913
Batch 50/64 loss: -0.0708884596824646
Batch 51/64 loss: -0.10526669025421143
Batch 52/64 loss: -0.059320926666259766
Batch 53/64 loss: -0.08375394344329834
Batch 54/64 loss: -0.07432138919830322
Batch 55/64 loss: -0.09487593173980713
Batch 56/64 loss: -0.08870184421539307
Batch 57/64 loss: -0.07861757278442383
Batch 58/64 loss: -0.11156892776489258
Batch 59/64 loss: -0.10707926750183105
Batch 60/64 loss: -0.08887052536010742
Batch 61/64 loss: -0.07556217908859253
Batch 62/64 loss: -0.06558912992477417
Batch 63/64 loss: -0.049643754959106445
Batch 64/64 loss: -0.0888603925704956
Epoch 276  Train loss: -0.08827432604397044  Val loss: 0.04585856690849226
Epoch 277
-------------------------------
Batch 1/64 loss: -0.07022488117218018
Batch 2/64 loss: -0.09726220369338989
Batch 3/64 loss: -0.09373360872268677
Batch 4/64 loss: -0.08620965480804443
Batch 5/64 loss: -0.08894461393356323
Batch 6/64 loss: -0.06673657894134521
Batch 7/64 loss: -0.09510719776153564
Batch 8/64 loss: -0.09470653533935547
Batch 9/64 loss: -0.11111307144165039
Batch 10/64 loss: -0.08460474014282227
Batch 11/64 loss: -0.10337191820144653
Batch 12/64 loss: -0.06770086288452148
Batch 13/64 loss: -0.09030252695083618
Batch 14/64 loss: -0.08579349517822266
Batch 15/64 loss: -0.08429235219955444
Batch 16/64 loss: -0.07920515537261963
Batch 17/64 loss: -0.11292284727096558
Batch 18/64 loss: -0.0915936827659607
Batch 19/64 loss: -0.06340157985687256
Batch 20/64 loss: -0.06333285570144653
Batch 21/64 loss: -0.10377275943756104
Batch 22/64 loss: -0.0913352370262146
Batch 23/64 loss: -0.082286536693573
Batch 24/64 loss: -0.09913372993469238
Batch 25/64 loss: -0.0794755220413208
Batch 26/64 loss: -0.10084950923919678
Batch 27/64 loss: -0.0826675295829773
Batch 28/64 loss: -0.07945621013641357
Batch 29/64 loss: -0.08779674768447876
Batch 30/64 loss: -0.09329241514205933
Batch 31/64 loss: -0.10867393016815186
Batch 32/64 loss: -0.06380605697631836
Batch 33/64 loss: -0.11089611053466797
Batch 34/64 loss: -0.09132474660873413
Batch 35/64 loss: -0.08660292625427246
Batch 36/64 loss: -0.0850600004196167
Batch 37/64 loss: -0.10283106565475464
Batch 38/64 loss: -0.09047532081604004
Batch 39/64 loss: -0.09633022546768188
Batch 40/64 loss: -0.07371687889099121
Batch 41/64 loss: -0.08303332328796387
Batch 42/64 loss: -0.08186239004135132
Batch 43/64 loss: -0.09990262985229492
Batch 44/64 loss: -0.08646708726882935
Batch 45/64 loss: -0.09973675012588501
Batch 46/64 loss: -0.10801196098327637
Batch 47/64 loss: -0.09065699577331543
Batch 48/64 loss: -0.057474493980407715
Batch 49/64 loss: -0.09111684560775757
Batch 50/64 loss: -0.09635567665100098
Batch 51/64 loss: -0.0944705605506897
Batch 52/64 loss: -0.10425341129302979
Batch 53/64 loss: -0.08426129817962646
Batch 54/64 loss: -0.09961462020874023
Batch 55/64 loss: -0.10378152132034302
Batch 56/64 loss: -0.10537910461425781
Batch 57/64 loss: -0.09498625993728638
Batch 58/64 loss: -0.09080469608306885
Batch 59/64 loss: -0.09718436002731323
Batch 60/64 loss: -0.09182852506637573
Batch 61/64 loss: -0.08357441425323486
Batch 62/64 loss: -0.05538815259933472
Batch 63/64 loss: -0.0855182409286499
Batch 64/64 loss: -0.09150856733322144
Epoch 277  Train loss: -0.08932766376757155  Val loss: 0.044766786581871845
Epoch 278
-------------------------------
Batch 1/64 loss: -0.06611138582229614
Batch 2/64 loss: -0.0986323356628418
Batch 3/64 loss: -0.09424066543579102
Batch 4/64 loss: -0.10435616970062256
Batch 5/64 loss: -0.12328720092773438
Batch 6/64 loss: -0.11101627349853516
Batch 7/64 loss: -0.09106004238128662
Batch 8/64 loss: -0.10333657264709473
Batch 9/64 loss: -0.07834863662719727
Batch 10/64 loss: -0.038979411125183105
Batch 11/64 loss: -0.08451414108276367
Batch 12/64 loss: -0.09625816345214844
Batch 13/64 loss: -0.09762364625930786
Batch 14/64 loss: -0.08964782953262329
Batch 15/64 loss: -0.10687118768692017
Batch 16/64 loss: -0.08272624015808105
Batch 17/64 loss: -0.08717697858810425
Batch 18/64 loss: -0.09924858808517456
Batch 19/64 loss: -0.08504772186279297
Batch 20/64 loss: -0.0960245132446289
Batch 21/64 loss: -0.09388405084609985
Batch 22/64 loss: -0.10240817070007324
Batch 23/64 loss: -0.07914316654205322
Batch 24/64 loss: -0.09031397104263306
Batch 25/64 loss: -0.07627201080322266
Batch 26/64 loss: -0.11456704139709473
Batch 27/64 loss: -0.09772735834121704
Batch 28/64 loss: -0.10454881191253662
Batch 29/64 loss: -0.08478796482086182
Batch 30/64 loss: -0.08973222970962524
Batch 31/64 loss: -0.10816729068756104
Batch 32/64 loss: -0.09187138080596924
Batch 33/64 loss: -0.08201372623443604
Batch 34/64 loss: -0.08485978841781616
Batch 35/64 loss: -0.10732144117355347
Batch 36/64 loss: -0.06935358047485352
Batch 37/64 loss: -0.09583890438079834
Batch 38/64 loss: -0.07832956314086914
Batch 39/64 loss: -0.0908765196800232
Batch 40/64 loss: -0.08014059066772461
Batch 41/64 loss: -0.08062738180160522
Batch 42/64 loss: -0.08641946315765381
Batch 43/64 loss: -0.11211293935775757
Batch 44/64 loss: -0.09705829620361328
Batch 45/64 loss: -0.09178417921066284
Batch 46/64 loss: -0.08766555786132812
Batch 47/64 loss: -0.0816144347190857
Batch 48/64 loss: -0.09028613567352295
Batch 49/64 loss: -0.09632599353790283
Batch 50/64 loss: -0.07522964477539062
Batch 51/64 loss: -0.08690249919891357
Batch 52/64 loss: -0.06569045782089233
Batch 53/64 loss: -0.0676458477973938
Batch 54/64 loss: -0.09761136770248413
Batch 55/64 loss: -0.06580877304077148
Batch 56/64 loss: -0.08333134651184082
Batch 57/64 loss: -0.07064944505691528
Batch 58/64 loss: -0.10207122564315796
Batch 59/64 loss: -0.1072700023651123
Batch 60/64 loss: -0.07714337110519409
Batch 61/64 loss: -0.06535530090332031
Batch 62/64 loss: -0.09816086292266846
Batch 63/64 loss: -0.08645045757293701
Batch 64/64 loss: -0.07408523559570312
Epoch 278  Train loss: -0.08918343805799298  Val loss: 0.045748410151176845
Epoch 279
-------------------------------
Batch 1/64 loss: -0.10849863290786743
Batch 2/64 loss: -0.10717892646789551
Batch 3/64 loss: -0.08737003803253174
Batch 4/64 loss: -0.09536600112915039
Batch 5/64 loss: -0.07911640405654907
Batch 6/64 loss: -0.11053138971328735
Batch 7/64 loss: -0.06718796491622925
Batch 8/64 loss: -0.10837548971176147
Batch 9/64 loss: -0.09622973203659058
Batch 10/64 loss: -0.07382887601852417
Batch 11/64 loss: -0.08436024188995361
Batch 12/64 loss: -0.09513366222381592
Batch 13/64 loss: -0.046042442321777344
Batch 14/64 loss: -0.08894520998001099
Batch 15/64 loss: -0.08340775966644287
Batch 16/64 loss: -0.07993710041046143
Batch 17/64 loss: -0.08753955364227295
Batch 18/64 loss: -0.0940103530883789
Batch 19/64 loss: -0.07356870174407959
Batch 20/64 loss: -0.053052544593811035
Batch 21/64 loss: -0.09160584211349487
Batch 22/64 loss: -0.07194411754608154
Batch 23/64 loss: -0.09693789482116699
Batch 24/64 loss: -0.08569866418838501
Batch 25/64 loss: -0.1037759780883789
Batch 26/64 loss: -0.09781640768051147
Batch 27/64 loss: -0.08267557621002197
Batch 28/64 loss: -0.10504782199859619
Batch 29/64 loss: -0.049057602882385254
Batch 30/64 loss: -0.07976680994033813
Batch 31/64 loss: -0.09497392177581787
Batch 32/64 loss: -0.07883161306381226
Batch 33/64 loss: -0.059929072856903076
Batch 34/64 loss: -0.10563063621520996
Batch 35/64 loss: -0.09055769443511963
Batch 36/64 loss: -0.10166627168655396
Batch 37/64 loss: -0.08895808458328247
Batch 38/64 loss: -0.08102613687515259
Batch 39/64 loss: -0.10390198230743408
Batch 40/64 loss: -0.08476483821868896
Batch 41/64 loss: -0.09522378444671631
Batch 42/64 loss: -0.07232213020324707
Batch 43/64 loss: -0.09312152862548828
Batch 44/64 loss: -0.0951467752456665
Batch 45/64 loss: -0.06913238763809204
Batch 46/64 loss: -0.09533703327178955
Batch 47/64 loss: -0.09007227420806885
Batch 48/64 loss: -0.11428362131118774
Batch 49/64 loss: -0.06423646211624146
Batch 50/64 loss: -0.09827530384063721
Batch 51/64 loss: -0.08860242366790771
Batch 52/64 loss: -0.0918194055557251
Batch 53/64 loss: -0.10807740688323975
Batch 54/64 loss: -0.09407716989517212
Batch 55/64 loss: -0.093997061252594
Batch 56/64 loss: -0.0814254879951477
Batch 57/64 loss: -0.0887371301651001
Batch 58/64 loss: -0.09416317939758301
Batch 59/64 loss: -0.09579390287399292
Batch 60/64 loss: -0.08045738935470581
Batch 61/64 loss: -0.0790514349937439
Batch 62/64 loss: -0.07903486490249634
Batch 63/64 loss: -0.10239017009735107
Batch 64/64 loss: -0.08220821619033813
Epoch 279  Train loss: -0.08785384285683726  Val loss: 0.045663128808601613
Epoch 280
-------------------------------
Batch 1/64 loss: -0.08358848094940186
Batch 2/64 loss: -0.09325975179672241
Batch 3/64 loss: -0.0830986499786377
Batch 4/64 loss: -0.07929617166519165
Batch 5/64 loss: -0.09126847982406616
Batch 6/64 loss: -0.10384601354598999
Batch 7/64 loss: -0.11110270023345947
Batch 8/64 loss: -0.08919668197631836
Batch 9/64 loss: -0.09841549396514893
Batch 10/64 loss: -0.08114099502563477
Batch 11/64 loss: -0.09765708446502686
Batch 12/64 loss: -0.0891561508178711
Batch 13/64 loss: -0.0990673303604126
Batch 14/64 loss: -0.08990466594696045
Batch 15/64 loss: -0.09597653150558472
Batch 16/64 loss: -0.10215127468109131
Batch 17/64 loss: -0.09208071231842041
Batch 18/64 loss: -0.07348108291625977
Batch 19/64 loss: -0.09440869092941284
Batch 20/64 loss: -0.0686076283454895
Batch 21/64 loss: -0.07857728004455566
Batch 22/64 loss: -0.08018738031387329
Batch 23/64 loss: -0.08088541030883789
Batch 24/64 loss: -0.09757125377655029
Batch 25/64 loss: -0.07244300842285156
Batch 26/64 loss: -0.09723585844039917
Batch 27/64 loss: -0.10440438985824585
Batch 28/64 loss: -0.07979923486709595
Batch 29/64 loss: -0.0895153284072876
Batch 30/64 loss: -0.08794152736663818
Batch 31/64 loss: -0.10055410861968994
Batch 32/64 loss: -0.10314106941223145
Batch 33/64 loss: -0.11013352870941162
Batch 34/64 loss: -0.09278631210327148
Batch 35/64 loss: -0.09469890594482422
Batch 36/64 loss: -0.1077423095703125
Batch 37/64 loss: -0.06071460247039795
Batch 38/64 loss: -0.06919395923614502
Batch 39/64 loss: -0.09262442588806152
Batch 40/64 loss: -0.07273542881011963
Batch 41/64 loss: -0.09971767663955688
Batch 42/64 loss: -0.049673378467559814
Batch 43/64 loss: -0.09703117609024048
Batch 44/64 loss: -0.08108454942703247
Batch 45/64 loss: -0.08037638664245605
Batch 46/64 loss: -0.10508650541305542
Batch 47/64 loss: -0.08196890354156494
Batch 48/64 loss: -0.07710731029510498
Batch 49/64 loss: -0.08541959524154663
Batch 50/64 loss: -0.06275343894958496
Batch 51/64 loss: -0.10194683074951172
Batch 52/64 loss: -0.10051357746124268
Batch 53/64 loss: -0.08871340751647949
Batch 54/64 loss: -0.08888775110244751
Batch 55/64 loss: -0.10606837272644043
Batch 56/64 loss: -0.09898227453231812
Batch 57/64 loss: -0.07782649993896484
Batch 58/64 loss: -0.06080418825149536
Batch 59/64 loss: -0.06830966472625732
Batch 60/64 loss: -0.08381867408752441
Batch 61/64 loss: -0.04698747396469116
Batch 62/64 loss: -0.07789170742034912
Batch 63/64 loss: -0.09758585691452026
Batch 64/64 loss: -0.07639205455780029
Epoch 280  Train loss: -0.08730100603664623  Val loss: 0.04614042600815239
Epoch 281
-------------------------------
Batch 1/64 loss: -0.12438774108886719
Batch 2/64 loss: -0.09685665369033813
Batch 3/64 loss: -0.10974729061126709
Batch 4/64 loss: -0.09939616918563843
Batch 5/64 loss: -0.08999860286712646
Batch 6/64 loss: -0.11841005086898804
Batch 7/64 loss: -0.09281933307647705
Batch 8/64 loss: -0.10530930757522583
Batch 9/64 loss: -0.10684335231781006
Batch 10/64 loss: -0.10573065280914307
Batch 11/64 loss: -0.07723277807235718
Batch 12/64 loss: -0.09888231754302979
Batch 13/64 loss: -0.10883045196533203
Batch 14/64 loss: -0.10067552328109741
Batch 15/64 loss: -0.07896089553833008
Batch 16/64 loss: -0.06949681043624878
Batch 17/64 loss: -0.08333402872085571
Batch 18/64 loss: -0.0946468710899353
Batch 19/64 loss: -0.09609830379486084
Batch 20/64 loss: -0.08619338274002075
Batch 21/64 loss: -0.07339072227478027
Batch 22/64 loss: -0.08604365587234497
Batch 23/64 loss: -0.08506840467453003
Batch 24/64 loss: -0.09986448287963867
Batch 25/64 loss: -0.07062828540802002
Batch 26/64 loss: -0.05997973680496216
Batch 27/64 loss: -0.08729243278503418
Batch 28/64 loss: -0.07798653841018677
Batch 29/64 loss: -0.09503728151321411
Batch 30/64 loss: -0.10325086116790771
Batch 31/64 loss: -0.09097456932067871
Batch 32/64 loss: -0.09055846929550171
Batch 33/64 loss: -0.07250392436981201
Batch 34/64 loss: -0.06500911712646484
Batch 35/64 loss: -0.08257967233657837
Batch 36/64 loss: -0.0924108624458313
Batch 37/64 loss: -0.051100730895996094
Batch 38/64 loss: -0.081631600856781
Batch 39/64 loss: -0.08142828941345215
Batch 40/64 loss: -0.06687963008880615
Batch 41/64 loss: -0.09803932905197144
Batch 42/64 loss: -0.11105203628540039
Batch 43/64 loss: -0.09885656833648682
Batch 44/64 loss: -0.09554940462112427
Batch 45/64 loss: -0.09445071220397949
Batch 46/64 loss: -0.06709671020507812
Batch 47/64 loss: -0.10861635208129883
Batch 48/64 loss: -0.08062130212783813
Batch 49/64 loss: -0.09938466548919678
Batch 50/64 loss: -0.0895850658416748
Batch 51/64 loss: -0.09562700986862183
Batch 52/64 loss: -0.09682637453079224
Batch 53/64 loss: -0.10890287160873413
Batch 54/64 loss: -0.08731234073638916
Batch 55/64 loss: -0.10062718391418457
Batch 56/64 loss: -0.08487462997436523
Batch 57/64 loss: -0.07766175270080566
Batch 58/64 loss: -0.09624266624450684
Batch 59/64 loss: -0.09700244665145874
Batch 60/64 loss: -0.08526301383972168
Batch 61/64 loss: -0.08577430248260498
Batch 62/64 loss: -0.1127161979675293
Batch 63/64 loss: -0.07354122400283813
Batch 64/64 loss: -0.08524841070175171
Epoch 281  Train loss: -0.0904632196706884  Val loss: 0.04496574278959294
Epoch 282
-------------------------------
Batch 1/64 loss: -0.08380448818206787
Batch 2/64 loss: -0.0738602876663208
Batch 3/64 loss: -0.10322201251983643
Batch 4/64 loss: -0.10954535007476807
Batch 5/64 loss: -0.10092878341674805
Batch 6/64 loss: -0.11111223697662354
Batch 7/64 loss: -0.09539145231246948
Batch 8/64 loss: -0.0951729416847229
Batch 9/64 loss: -0.1109810471534729
Batch 10/64 loss: -0.08123868703842163
Batch 11/64 loss: -0.08201032876968384
Batch 12/64 loss: -0.07843899726867676
Batch 13/64 loss: -0.08620750904083252
Batch 14/64 loss: -0.09143918752670288
Batch 15/64 loss: -0.09806466102600098
Batch 16/64 loss: -0.08155220746994019
Batch 17/64 loss: -0.0790751576423645
Batch 18/64 loss: -0.10867780447006226
Batch 19/64 loss: -0.10169565677642822
Batch 20/64 loss: -0.07759082317352295
Batch 21/64 loss: -0.0683739185333252
Batch 22/64 loss: -0.086539626121521
Batch 23/64 loss: -0.09037590026855469
Batch 24/64 loss: -0.0919346809387207
Batch 25/64 loss: -0.05637592077255249
Batch 26/64 loss: -0.09720075130462646
Batch 27/64 loss: -0.10201871395111084
Batch 28/64 loss: -0.10411757230758667
Batch 29/64 loss: -0.09695476293563843
Batch 30/64 loss: -0.08953666687011719
Batch 31/64 loss: -0.11094343662261963
Batch 32/64 loss: -0.10265600681304932
Batch 33/64 loss: -0.0807066559791565
Batch 34/64 loss: -0.07869130373001099
Batch 35/64 loss: -0.0724298357963562
Batch 36/64 loss: -0.07291018962860107
Batch 37/64 loss: -0.0932648777961731
Batch 38/64 loss: -0.09284961223602295
Batch 39/64 loss: -0.12096768617630005
Batch 40/64 loss: -0.07747101783752441
Batch 41/64 loss: -0.09079277515411377
Batch 42/64 loss: -0.10725516080856323
Batch 43/64 loss: -0.07231420278549194
Batch 44/64 loss: -0.05496406555175781
Batch 45/64 loss: -0.06766396760940552
Batch 46/64 loss: -0.012765765190124512
Batch 47/64 loss: -0.0951087474822998
Batch 48/64 loss: -0.08874541521072388
Batch 49/64 loss: -0.08316612243652344
Batch 50/64 loss: -0.09033620357513428
Batch 51/64 loss: -0.0959939956665039
Batch 52/64 loss: -0.10419762134552002
Batch 53/64 loss: -0.08328413963317871
Batch 54/64 loss: -0.10509335994720459
Batch 55/64 loss: -0.1039886474609375
Batch 56/64 loss: -0.10762929916381836
Batch 57/64 loss: -0.11245250701904297
Batch 58/64 loss: -0.07399904727935791
Batch 59/64 loss: -0.10332101583480835
Batch 60/64 loss: -0.11377894878387451
Batch 61/64 loss: -0.09837937355041504
Batch 62/64 loss: -0.09859323501586914
Batch 63/64 loss: -0.09904325008392334
Batch 64/64 loss: -0.09221315383911133
Epoch 282  Train loss: -0.09048400766709272  Val loss: 0.04503756822998991
Epoch 283
-------------------------------
Batch 1/64 loss: -0.10401147603988647
Batch 2/64 loss: -0.09802573919296265
Batch 3/64 loss: -0.10000848770141602
Batch 4/64 loss: -0.08196485042572021
Batch 5/64 loss: -0.06869757175445557
Batch 6/64 loss: -0.1074296236038208
Batch 7/64 loss: -0.09385830163955688
Batch 8/64 loss: -0.12017858028411865
Batch 9/64 loss: -0.04227334260940552
Batch 10/64 loss: -0.07619249820709229
Batch 11/64 loss: -0.0730929970741272
Batch 12/64 loss: -0.10069608688354492
Batch 13/64 loss: -0.0866895318031311
Batch 14/64 loss: -0.09794986248016357
Batch 15/64 loss: -0.09842652082443237
Batch 16/64 loss: -0.10249274969100952
Batch 17/64 loss: -0.09409672021865845
Batch 18/64 loss: -0.09346997737884521
Batch 19/64 loss: -0.09462863206863403
Batch 20/64 loss: -0.09991943836212158
Batch 21/64 loss: -0.09426075220108032
Batch 22/64 loss: -0.1118694543838501
Batch 23/64 loss: -0.10448390245437622
Batch 24/64 loss: -0.09448599815368652
Batch 25/64 loss: -0.0768781304359436
Batch 26/64 loss: -0.08006763458251953
Batch 27/64 loss: -0.08954071998596191
Batch 28/64 loss: -0.08205908536911011
Batch 29/64 loss: -0.043824195861816406
Batch 30/64 loss: -0.04699820280075073
Batch 31/64 loss: -0.07799315452575684
Batch 32/64 loss: -0.07483315467834473
Batch 33/64 loss: -0.08491510152816772
Batch 34/64 loss: -0.06682097911834717
Batch 35/64 loss: -0.09610152244567871
Batch 36/64 loss: -0.10087573528289795
Batch 37/64 loss: -0.11145716905593872
Batch 38/64 loss: -0.1126718521118164
Batch 39/64 loss: -0.10244953632354736
Batch 40/64 loss: -0.0710902214050293
Batch 41/64 loss: -0.07609409093856812
Batch 42/64 loss: -0.08476400375366211
Batch 43/64 loss: -0.09693741798400879
Batch 44/64 loss: -0.10628730058670044
Batch 45/64 loss: -0.09288191795349121
Batch 46/64 loss: -0.09017521142959595
Batch 47/64 loss: -0.10526597499847412
Batch 48/64 loss: -0.07352924346923828
Batch 49/64 loss: -0.10868197679519653
Batch 50/64 loss: -0.11664551496505737
Batch 51/64 loss: -0.08838659524917603
Batch 52/64 loss: -0.11202442646026611
Batch 53/64 loss: -0.06187582015991211
Batch 54/64 loss: -0.07217496633529663
Batch 55/64 loss: -0.08955347537994385
Batch 56/64 loss: -0.10232192277908325
Batch 57/64 loss: -0.10618144273757935
Batch 58/64 loss: -0.06985306739807129
Batch 59/64 loss: -0.06737744808197021
Batch 60/64 loss: -0.09103614091873169
Batch 61/64 loss: -0.10869097709655762
Batch 62/64 loss: -0.07197219133377075
Batch 63/64 loss: -0.07980293035507202
Batch 64/64 loss: -0.08314871788024902
Epoch 283  Train loss: -0.0892965503767425  Val loss: 0.044993073669905514
Epoch 284
-------------------------------
Batch 1/64 loss: -0.06284791231155396
Batch 2/64 loss: -0.09427011013031006
Batch 3/64 loss: -0.06978535652160645
Batch 4/64 loss: -0.11165392398834229
Batch 5/64 loss: -0.08997094631195068
Batch 6/64 loss: -0.09024941921234131
Batch 7/64 loss: -0.11397367715835571
Batch 8/64 loss: -0.1227186918258667
Batch 9/64 loss: -0.09764152765274048
Batch 10/64 loss: -0.12612861394882202
Batch 11/64 loss: -0.11644268035888672
Batch 12/64 loss: -0.11072295904159546
Batch 13/64 loss: -0.11612385511398315
Batch 14/64 loss: -0.11863255500793457
Batch 15/64 loss: -0.08221811056137085
Batch 16/64 loss: -0.08230352401733398
Batch 17/64 loss: -0.10225677490234375
Batch 18/64 loss: -0.10978269577026367
Batch 19/64 loss: -0.05569124221801758
Batch 20/64 loss: -0.08189350366592407
Batch 21/64 loss: -0.1059994101524353
Batch 22/64 loss: -0.10052490234375
Batch 23/64 loss: -0.10497254133224487
Batch 24/64 loss: -0.10030245780944824
Batch 25/64 loss: -0.10149943828582764
Batch 26/64 loss: -0.07762622833251953
Batch 27/64 loss: -0.09806925058364868
Batch 28/64 loss: -0.05648624897003174
Batch 29/64 loss: -0.09986013174057007
Batch 30/64 loss: -0.06798607110977173
Batch 31/64 loss: -0.08722805976867676
Batch 32/64 loss: -0.09972530603408813
Batch 33/64 loss: -0.06176871061325073
Batch 34/64 loss: -0.08773279190063477
Batch 35/64 loss: -0.07238852977752686
Batch 36/64 loss: -0.07630765438079834
Batch 37/64 loss: -0.07729494571685791
Batch 38/64 loss: -0.10153478384017944
Batch 39/64 loss: -0.08009171485900879
Batch 40/64 loss: -0.09424757957458496
Batch 41/64 loss: -0.10427820682525635
Batch 42/64 loss: -0.08931481838226318
Batch 43/64 loss: -0.07267916202545166
Batch 44/64 loss: -0.08733075857162476
Batch 45/64 loss: -0.09325742721557617
Batch 46/64 loss: -0.02941817045211792
Batch 47/64 loss: -0.09578204154968262
Batch 48/64 loss: -0.09425067901611328
Batch 49/64 loss: -0.10183680057525635
Batch 50/64 loss: -0.09523868560791016
Batch 51/64 loss: -0.09155768156051636
Batch 52/64 loss: -0.06764256954193115
Batch 53/64 loss: -0.08120626211166382
Batch 54/64 loss: -0.08392274379730225
Batch 55/64 loss: -0.09574759006500244
Batch 56/64 loss: -0.1219673752784729
Batch 57/64 loss: -0.09483188390731812
Batch 58/64 loss: -0.07821351289749146
Batch 59/64 loss: -0.08529049158096313
Batch 60/64 loss: -0.07250398397445679
Batch 61/64 loss: -0.0903768539428711
Batch 62/64 loss: -0.09472411870956421
Batch 63/64 loss: -0.10101944208145142
Batch 64/64 loss: -0.1141546368598938
Epoch 284  Train loss: -0.09121509135938158  Val loss: 0.04603088700894228
Epoch 285
-------------------------------
Batch 1/64 loss: -0.10009145736694336
Batch 2/64 loss: -0.11142230033874512
Batch 3/64 loss: -0.08971929550170898
Batch 4/64 loss: -0.07123547792434692
Batch 5/64 loss: -0.10502421855926514
Batch 6/64 loss: -0.09764647483825684
Batch 7/64 loss: -0.12351590394973755
Batch 8/64 loss: -0.0648534893989563
Batch 9/64 loss: -0.0772961974143982
Batch 10/64 loss: -0.08014523983001709
Batch 11/64 loss: -0.0915181040763855
Batch 12/64 loss: -0.10030359029769897
Batch 13/64 loss: -0.11046886444091797
Batch 14/64 loss: -0.0687023401260376
Batch 15/64 loss: -0.06103694438934326
Batch 16/64 loss: -0.10565227270126343
Batch 17/64 loss: -0.10054981708526611
Batch 18/64 loss: -0.11620330810546875
Batch 19/64 loss: -0.07540774345397949
Batch 20/64 loss: -0.09481585025787354
Batch 21/64 loss: -0.09955120086669922
Batch 22/64 loss: -0.0707964301109314
Batch 23/64 loss: -0.11242061853408813
Batch 24/64 loss: -0.1037287712097168
Batch 25/64 loss: -0.10175943374633789
Batch 26/64 loss: -0.09148412942886353
Batch 27/64 loss: -0.10219931602478027
Batch 28/64 loss: -0.08300501108169556
Batch 29/64 loss: -0.06950539350509644
Batch 30/64 loss: -0.09755462408065796
Batch 31/64 loss: -0.10711580514907837
Batch 32/64 loss: -0.08214950561523438
Batch 33/64 loss: -0.09995245933532715
Batch 34/64 loss: -0.09408801794052124
Batch 35/64 loss: -0.08592242002487183
Batch 36/64 loss: -0.09582746028900146
Batch 37/64 loss: -0.11062228679656982
Batch 38/64 loss: -0.08180457353591919
Batch 39/64 loss: -0.09066414833068848
Batch 40/64 loss: -0.09994733333587646
Batch 41/64 loss: -0.11074882745742798
Batch 42/64 loss: -0.11078846454620361
Batch 43/64 loss: -0.12745100259780884
Batch 44/64 loss: -0.06133592128753662
Batch 45/64 loss: -0.09356027841567993
Batch 46/64 loss: -0.06955307722091675
Batch 47/64 loss: -0.09758937358856201
Batch 48/64 loss: -0.08249771595001221
Batch 49/64 loss: -0.090728759765625
Batch 50/64 loss: -0.08877736330032349
Batch 51/64 loss: -0.07126528024673462
Batch 52/64 loss: -0.09728658199310303
Batch 53/64 loss: -0.07961654663085938
Batch 54/64 loss: -0.11582839488983154
Batch 55/64 loss: -0.07332348823547363
Batch 56/64 loss: -0.06740826368331909
Batch 57/64 loss: -0.08857369422912598
Batch 58/64 loss: -0.09244978427886963
Batch 59/64 loss: -0.079784095287323
Batch 60/64 loss: -0.08353352546691895
Batch 61/64 loss: -0.06372344493865967
Batch 62/64 loss: -0.09285074472427368
Batch 63/64 loss: -0.08723556995391846
Batch 64/64 loss: -0.07109230756759644
Epoch 285  Train loss: -0.09105783934686698  Val loss: 0.04881215607587414
Epoch 286
-------------------------------
Batch 1/64 loss: -0.09576666355133057
Batch 2/64 loss: -0.09485584497451782
Batch 3/64 loss: -0.09507548809051514
Batch 4/64 loss: -0.0981602668762207
Batch 5/64 loss: -0.07925420999526978
Batch 6/64 loss: -0.08865106105804443
Batch 7/64 loss: -0.10413837432861328
Batch 8/64 loss: -0.10230845212936401
Batch 9/64 loss: -0.0924336314201355
Batch 10/64 loss: -0.08029592037200928
Batch 11/64 loss: -0.09989356994628906
Batch 12/64 loss: -0.07230198383331299
Batch 13/64 loss: -0.11380374431610107
Batch 14/64 loss: -0.0789305567741394
Batch 15/64 loss: -0.10420393943786621
Batch 16/64 loss: -0.11532533168792725
Batch 17/64 loss: -0.07342833280563354
Batch 18/64 loss: -0.09522712230682373
Batch 19/64 loss: -0.11145633459091187
Batch 20/64 loss: -0.09145671129226685
Batch 21/64 loss: -0.10219746828079224
Batch 22/64 loss: -0.0991637110710144
Batch 23/64 loss: -0.08093643188476562
Batch 24/64 loss: -0.07222414016723633
Batch 25/64 loss: -0.06520426273345947
Batch 26/64 loss: -0.09583026170730591
Batch 27/64 loss: -0.08860421180725098
Batch 28/64 loss: -0.07862633466720581
Batch 29/64 loss: -0.09580612182617188
Batch 30/64 loss: -0.07578516006469727
Batch 31/64 loss: -0.10361021757125854
Batch 32/64 loss: -0.10054957866668701
Batch 33/64 loss: -0.07721763849258423
Batch 34/64 loss: -0.08065783977508545
Batch 35/64 loss: -0.07069665193557739
Batch 36/64 loss: -0.07667267322540283
Batch 37/64 loss: -0.09729748964309692
Batch 38/64 loss: -0.08821815252304077
Batch 39/64 loss: -0.09501922130584717
Batch 40/64 loss: -0.0997847318649292
Batch 41/64 loss: -0.10499680042266846
Batch 42/64 loss: -0.08327466249465942
Batch 43/64 loss: -0.0880851149559021
Batch 44/64 loss: -0.08402323722839355
Batch 45/64 loss: -0.08210581541061401
Batch 46/64 loss: -0.11419034004211426
Batch 47/64 loss: -0.10521674156188965
Batch 48/64 loss: -0.10526174306869507
Batch 49/64 loss: -0.08460783958435059
Batch 50/64 loss: -0.09026986360549927
Batch 51/64 loss: -0.10823285579681396
Batch 52/64 loss: -0.06460434198379517
Batch 53/64 loss: -0.0846671462059021
Batch 54/64 loss: -0.0851094126701355
Batch 55/64 loss: -0.1010478138923645
Batch 56/64 loss: -0.08899098634719849
Batch 57/64 loss: -0.09081685543060303
Batch 58/64 loss: -0.07575386762619019
Batch 59/64 loss: -0.08602344989776611
Batch 60/64 loss: -0.07080447673797607
Batch 61/64 loss: -0.10040795803070068
Batch 62/64 loss: -0.0865374207496643
Batch 63/64 loss: -0.07980811595916748
Batch 64/64 loss: -0.10605967044830322
Epoch 286  Train loss: -0.09059531688690185  Val loss: 0.04722905896373631
Epoch 287
-------------------------------
Batch 1/64 loss: -0.09649991989135742
Batch 2/64 loss: -0.0968698263168335
Batch 3/64 loss: -0.08754724264144897
Batch 4/64 loss: -0.09628957509994507
Batch 5/64 loss: -0.10575300455093384
Batch 6/64 loss: -0.10687345266342163
Batch 7/64 loss: -0.12013500928878784
Batch 8/64 loss: -0.09432202577590942
Batch 9/64 loss: -0.0824127197265625
Batch 10/64 loss: -0.08200865983963013
Batch 11/64 loss: -0.07521134614944458
Batch 12/64 loss: -0.1045842170715332
Batch 13/64 loss: -0.08779788017272949
Batch 14/64 loss: -0.07853233814239502
Batch 15/64 loss: -0.08865100145339966
Batch 16/64 loss: -0.08304643630981445
Batch 17/64 loss: -0.06634914875030518
Batch 18/64 loss: -0.07783794403076172
Batch 19/64 loss: -0.0991026759147644
Batch 20/64 loss: -0.07700490951538086
Batch 21/64 loss: -0.1076236367225647
Batch 22/64 loss: -0.09500432014465332
Batch 23/64 loss: -0.0988614559173584
Batch 24/64 loss: -0.13462680578231812
Batch 25/64 loss: -0.1053386926651001
Batch 26/64 loss: -0.08657211065292358
Batch 27/64 loss: -0.07527077198028564
Batch 28/64 loss: -0.07898038625717163
Batch 29/64 loss: -0.09639132022857666
Batch 30/64 loss: -0.09829503297805786
Batch 31/64 loss: -0.08294820785522461
Batch 32/64 loss: -0.08183002471923828
Batch 33/64 loss: -0.10225170850753784
Batch 34/64 loss: -0.05518460273742676
Batch 35/64 loss: -0.06716203689575195
Batch 36/64 loss: -0.09269803762435913
Batch 37/64 loss: -0.09940087795257568
Batch 38/64 loss: -0.1002148985862732
Batch 39/64 loss: -0.10317224264144897
Batch 40/64 loss: -0.10874879360198975
Batch 41/64 loss: -0.08283251523971558
Batch 42/64 loss: -0.10563486814498901
Batch 43/64 loss: -0.07577723264694214
Batch 44/64 loss: -0.07459872961044312
Batch 45/64 loss: -0.08009368181228638
Batch 46/64 loss: -0.10140025615692139
Batch 47/64 loss: -0.08609962463378906
Batch 48/64 loss: -0.11598849296569824
Batch 49/64 loss: -0.07776159048080444
Batch 50/64 loss: -0.08003437519073486
Batch 51/64 loss: -0.1205480694770813
Batch 52/64 loss: -0.08995360136032104
Batch 53/64 loss: -0.10074770450592041
Batch 54/64 loss: -0.1009032130241394
Batch 55/64 loss: -0.08365827798843384
Batch 56/64 loss: -0.10821187496185303
Batch 57/64 loss: -0.10893118381500244
Batch 58/64 loss: -0.10357868671417236
Batch 59/64 loss: -0.11051177978515625
Batch 60/64 loss: -0.07563126087188721
Batch 61/64 loss: -0.10722333192825317
Batch 62/64 loss: -0.09484684467315674
Batch 63/64 loss: -0.07694786787033081
Batch 64/64 loss: -0.11986833810806274
Epoch 287  Train loss: -0.09300739788541607  Val loss: 0.04362198536338675
Epoch 288
-------------------------------
Batch 1/64 loss: -0.08249920606613159
Batch 2/64 loss: -0.10945838689804077
Batch 3/64 loss: -0.07890576124191284
Batch 4/64 loss: -0.09891092777252197
Batch 5/64 loss: -0.09894406795501709
Batch 6/64 loss: -0.12286895513534546
Batch 7/64 loss: -0.1149817705154419
Batch 8/64 loss: -0.10492491722106934
Batch 9/64 loss: -0.0804409384727478
Batch 10/64 loss: -0.10148388147354126
Batch 11/64 loss: -0.12591803073883057
Batch 12/64 loss: -0.1016010046005249
Batch 13/64 loss: -0.08843183517456055
Batch 14/64 loss: -0.06545037031173706
Batch 15/64 loss: -0.08494126796722412
Batch 16/64 loss: -0.09152835607528687
Batch 17/64 loss: -0.07883673906326294
Batch 18/64 loss: -0.0990743637084961
Batch 19/64 loss: -0.1106749176979065
Batch 20/64 loss: -0.10905873775482178
Batch 21/64 loss: -0.09832239151000977
Batch 22/64 loss: -0.09318709373474121
Batch 23/64 loss: -0.06298184394836426
Batch 24/64 loss: -0.07231134176254272
Batch 25/64 loss: -0.10697376728057861
Batch 26/64 loss: -0.06858664751052856
Batch 27/64 loss: -0.0946660041809082
Batch 28/64 loss: -0.11317944526672363
Batch 29/64 loss: -0.11800491809844971
Batch 30/64 loss: -0.07764387130737305
Batch 31/64 loss: -0.0851966142654419
Batch 32/64 loss: -0.0830756425857544
Batch 33/64 loss: -0.08760690689086914
Batch 34/64 loss: -0.07589399814605713
Batch 35/64 loss: -0.10503983497619629
Batch 36/64 loss: -0.12728136777877808
Batch 37/64 loss: -0.06452947854995728
Batch 38/64 loss: -0.08567154407501221
Batch 39/64 loss: -0.08736288547515869
Batch 40/64 loss: -0.09532970190048218
Batch 41/64 loss: -0.1016387939453125
Batch 42/64 loss: -0.09614509344100952
Batch 43/64 loss: -0.08892881870269775
Batch 44/64 loss: -0.0971837043762207
Batch 45/64 loss: -0.07767385244369507
Batch 46/64 loss: -0.09294378757476807
Batch 47/64 loss: -0.10478204488754272
Batch 48/64 loss: -0.06786882877349854
Batch 49/64 loss: -0.11119663715362549
Batch 50/64 loss: -0.09703582525253296
Batch 51/64 loss: -0.10391789674758911
Batch 52/64 loss: -0.052551865577697754
Batch 53/64 loss: -0.11324447393417358
Batch 54/64 loss: -0.09648090600967407
Batch 55/64 loss: -0.08394920825958252
Batch 56/64 loss: -0.06280273199081421
Batch 57/64 loss: -0.09992730617523193
Batch 58/64 loss: -0.09914857149124146
Batch 59/64 loss: -0.0744398832321167
Batch 60/64 loss: -0.08914470672607422
Batch 61/64 loss: -0.09717702865600586
Batch 62/64 loss: -0.0985822081565857
Batch 63/64 loss: -0.10471272468566895
Batch 64/64 loss: -0.08691799640655518
Epoch 288  Train loss: -0.09299521773469215  Val loss: 0.0447516265194031
Epoch 289
-------------------------------
Batch 1/64 loss: -0.11380702257156372
Batch 2/64 loss: -0.10150545835494995
Batch 3/64 loss: -0.11788392066955566
Batch 4/64 loss: -0.08022582530975342
Batch 5/64 loss: -0.10770487785339355
Batch 6/64 loss: -0.09416031837463379
Batch 7/64 loss: -0.11265844106674194
Batch 8/64 loss: -0.07486557960510254
Batch 9/64 loss: -0.08382678031921387
Batch 10/64 loss: -0.07887697219848633
Batch 11/64 loss: -0.10719078779220581
Batch 12/64 loss: -0.11325901746749878
Batch 13/64 loss: -0.07693153619766235
Batch 14/64 loss: -0.08708971738815308
Batch 15/64 loss: -0.10507023334503174
Batch 16/64 loss: -0.10748136043548584
Batch 17/64 loss: -0.10447132587432861
Batch 18/64 loss: -0.0956835150718689
Batch 19/64 loss: -0.07539594173431396
Batch 20/64 loss: -0.0967942476272583
Batch 21/64 loss: -0.10478407144546509
Batch 22/64 loss: -0.10253345966339111
Batch 23/64 loss: -0.09049028158187866
Batch 24/64 loss: -0.10853946208953857
Batch 25/64 loss: -0.09919917583465576
Batch 26/64 loss: -0.08228790760040283
Batch 27/64 loss: -0.09817016124725342
Batch 28/64 loss: -0.09903228282928467
Batch 29/64 loss: -0.09066641330718994
Batch 30/64 loss: -0.0925062894821167
Batch 31/64 loss: -0.07804954051971436
Batch 32/64 loss: -0.08509373664855957
Batch 33/64 loss: -0.11636286973953247
Batch 34/64 loss: -0.0676419734954834
Batch 35/64 loss: -0.1028587818145752
Batch 36/64 loss: -0.11378592252731323
Batch 37/64 loss: -0.1190255880355835
Batch 38/64 loss: -0.055614173412323
Batch 39/64 loss: -0.085135817527771
Batch 40/64 loss: -0.10528808832168579
Batch 41/64 loss: -0.08112192153930664
Batch 42/64 loss: -0.09685099124908447
Batch 43/64 loss: -0.06387364864349365
Batch 44/64 loss: -0.08330368995666504
Batch 45/64 loss: -0.10876047611236572
Batch 46/64 loss: -0.10603082180023193
Batch 47/64 loss: -0.07035708427429199
Batch 48/64 loss: -0.11427825689315796
Batch 49/64 loss: -0.06920289993286133
Batch 50/64 loss: -0.10001510381698608
Batch 51/64 loss: -0.0828857421875
Batch 52/64 loss: -0.07326918840408325
Batch 53/64 loss: -0.11036330461502075
Batch 54/64 loss: -0.06993508338928223
Batch 55/64 loss: -0.10932177305221558
Batch 56/64 loss: -0.09465831518173218
Batch 57/64 loss: -0.11450695991516113
Batch 58/64 loss: -0.10071253776550293
Batch 59/64 loss: -0.06828641891479492
Batch 60/64 loss: -0.11366206407546997
Batch 61/64 loss: -0.09603017568588257
Batch 62/64 loss: -0.091094970703125
Batch 63/64 loss: -0.10473626852035522
Batch 64/64 loss: -0.10929960012435913
Epoch 289  Train loss: -0.09470088271533741  Val loss: 0.045629427400241604
Epoch 290
-------------------------------
Batch 1/64 loss: -0.11116218566894531
Batch 2/64 loss: -0.10917818546295166
Batch 3/64 loss: -0.09234070777893066
Batch 4/64 loss: -0.10618209838867188
Batch 5/64 loss: -0.09579086303710938
Batch 6/64 loss: -0.08304464817047119
Batch 7/64 loss: -0.09391903877258301
Batch 8/64 loss: -0.10287636518478394
Batch 9/64 loss: -0.10444045066833496
Batch 10/64 loss: -0.10343635082244873
Batch 11/64 loss: -0.11109745502471924
Batch 12/64 loss: -0.10022830963134766
Batch 13/64 loss: -0.12221115827560425
Batch 14/64 loss: -0.09655886888504028
Batch 15/64 loss: -0.0826147198677063
Batch 16/64 loss: -0.1135178804397583
Batch 17/64 loss: -0.09143805503845215
Batch 18/64 loss: -0.10184931755065918
Batch 19/64 loss: -0.09260010719299316
Batch 20/64 loss: -0.09208858013153076
Batch 21/64 loss: -0.0978119969367981
Batch 22/64 loss: -0.10047078132629395
Batch 23/64 loss: -0.08227699995040894
Batch 24/64 loss: -0.11304235458374023
Batch 25/64 loss: -0.11037302017211914
Batch 26/64 loss: -0.07495266199111938
Batch 27/64 loss: -0.09344464540481567
Batch 28/64 loss: -0.07734119892120361
Batch 29/64 loss: -0.08621358871459961
Batch 30/64 loss: -0.09729623794555664
Batch 31/64 loss: -0.10610830783843994
Batch 32/64 loss: -0.11795133352279663
Batch 33/64 loss: -0.12282544374465942
Batch 34/64 loss: -0.09732580184936523
Batch 35/64 loss: -0.10642969608306885
Batch 36/64 loss: -0.10923784971237183
Batch 37/64 loss: -0.10954040288925171
Batch 38/64 loss: -0.09462714195251465
Batch 39/64 loss: -0.09994393587112427
Batch 40/64 loss: -0.09612452983856201
Batch 41/64 loss: -0.08858639001846313
Batch 42/64 loss: -0.09394514560699463
Batch 43/64 loss: -0.07520556449890137
Batch 44/64 loss: -0.07871699333190918
Batch 45/64 loss: -0.07592922449111938
Batch 46/64 loss: -0.11130166053771973
Batch 47/64 loss: -0.07832729816436768
Batch 48/64 loss: -0.040410518646240234
Batch 49/64 loss: -0.061457157135009766
Batch 50/64 loss: -0.1029697060585022
Batch 51/64 loss: -0.08034688234329224
Batch 52/64 loss: -0.07699668407440186
Batch 53/64 loss: -0.09164786338806152
Batch 54/64 loss: -0.08094668388366699
Batch 55/64 loss: -0.10027927160263062
Batch 56/64 loss: -0.07819539308547974
Batch 57/64 loss: -0.08054167032241821
Batch 58/64 loss: -0.10225462913513184
Batch 59/64 loss: -0.07670176029205322
Batch 60/64 loss: -0.10209208726882935
Batch 61/64 loss: -0.09339290857315063
Batch 62/64 loss: -0.09719300270080566
Batch 63/64 loss: -0.09051424264907837
Batch 64/64 loss: -0.08920943737030029
Epoch 290  Train loss: -0.09450624456592635  Val loss: 0.04883159825072665
Epoch 291
-------------------------------
Batch 1/64 loss: -0.10246539115905762
Batch 2/64 loss: -0.08734709024429321
Batch 3/64 loss: -0.09075325727462769
Batch 4/64 loss: -0.09455859661102295
Batch 5/64 loss: -0.08774715662002563
Batch 6/64 loss: -0.07961058616638184
Batch 7/64 loss: -0.09675604104995728
Batch 8/64 loss: -0.0935981273651123
Batch 9/64 loss: -0.09798145294189453
Batch 10/64 loss: -0.10471421480178833
Batch 11/64 loss: -0.10593056678771973
Batch 12/64 loss: -0.10638022422790527
Batch 13/64 loss: -0.11944448947906494
Batch 14/64 loss: -0.09332704544067383
Batch 15/64 loss: -0.08271396160125732
Batch 16/64 loss: -0.08051294088363647
Batch 17/64 loss: -0.08002603054046631
Batch 18/64 loss: -0.10816031694412231
Batch 19/64 loss: -0.09502118825912476
Batch 20/64 loss: -0.09595167636871338
Batch 21/64 loss: -0.09084117412567139
Batch 22/64 loss: -0.09945350885391235
Batch 23/64 loss: -0.10701006650924683
Batch 24/64 loss: -0.12335288524627686
Batch 25/64 loss: -0.09488040208816528
Batch 26/64 loss: -0.10635638236999512
Batch 27/64 loss: -0.08817100524902344
Batch 28/64 loss: -0.1103518009185791
Batch 29/64 loss: -0.080963134765625
Batch 30/64 loss: -0.06415063142776489
Batch 31/64 loss: -0.0680801272392273
Batch 32/64 loss: -0.08654570579528809
Batch 33/64 loss: -0.08360260725021362
Batch 34/64 loss: -0.07480907440185547
Batch 35/64 loss: -0.1107950210571289
Batch 36/64 loss: -0.07531601190567017
Batch 37/64 loss: -0.10259449481964111
Batch 38/64 loss: -0.10442924499511719
Batch 39/64 loss: -0.07171458005905151
Batch 40/64 loss: -0.08024251461029053
Batch 41/64 loss: -0.10174375772476196
Batch 42/64 loss: -0.058085083961486816
Batch 43/64 loss: -0.10158771276473999
Batch 44/64 loss: -0.0922439694404602
Batch 45/64 loss: -0.07594376802444458
Batch 46/64 loss: -0.12258780002593994
Batch 47/64 loss: -0.09650331735610962
Batch 48/64 loss: -0.08023637533187866
Batch 49/64 loss: -0.07170408964157104
Batch 50/64 loss: -0.09239614009857178
Batch 51/64 loss: -0.10549646615982056
Batch 52/64 loss: -0.0943613052368164
Batch 53/64 loss: -0.0695565938949585
Batch 54/64 loss: -0.09405982494354248
Batch 55/64 loss: -0.07882481813430786
Batch 56/64 loss: -0.09892725944519043
Batch 57/64 loss: -0.10030978918075562
Batch 58/64 loss: -0.10805737972259521
Batch 59/64 loss: -0.05514240264892578
Batch 60/64 loss: -0.0799177885055542
Batch 61/64 loss: -0.10163271427154541
Batch 62/64 loss: -0.09214586019515991
Batch 63/64 loss: -0.10300248861312866
Batch 64/64 loss: -0.1123228669166565
Epoch 291  Train loss: -0.09231952288571527  Val loss: 0.04872597001262547
Epoch 292
-------------------------------
Batch 1/64 loss: -0.07073640823364258
Batch 2/64 loss: -0.06483107805252075
Batch 3/64 loss: -0.08469182252883911
Batch 4/64 loss: -0.08994871377944946
Batch 5/64 loss: -0.10946226119995117
Batch 6/64 loss: -0.08107227087020874
Batch 7/64 loss: -0.08104264736175537
Batch 8/64 loss: -0.09700161218643188
Batch 9/64 loss: -0.09405165910720825
Batch 10/64 loss: -0.10138344764709473
Batch 11/64 loss: -0.11305677890777588
Batch 12/64 loss: -0.10992330312728882
Batch 13/64 loss: -0.04769700765609741
Batch 14/64 loss: -0.10548603534698486
Batch 15/64 loss: -0.0637434720993042
Batch 16/64 loss: -0.1001138687133789
Batch 17/64 loss: -0.10431987047195435
Batch 18/64 loss: -0.09569734334945679
Batch 19/64 loss: -0.09340459108352661
Batch 20/64 loss: -0.10742884874343872
Batch 21/64 loss: -0.07335060834884644
Batch 22/64 loss: -0.09253621101379395
Batch 23/64 loss: -0.10696250200271606
Batch 24/64 loss: -0.09308505058288574
Batch 25/64 loss: -0.09472930431365967
Batch 26/64 loss: -0.12372469902038574
Batch 27/64 loss: -0.09796178340911865
Batch 28/64 loss: -0.10049647092819214
Batch 29/64 loss: -0.10232281684875488
Batch 30/64 loss: -0.08957487344741821
Batch 31/64 loss: -0.0982661247253418
Batch 32/64 loss: -0.11415725946426392
Batch 33/64 loss: -0.06785768270492554
Batch 34/64 loss: -0.12490391731262207
Batch 35/64 loss: -0.11244195699691772
Batch 36/64 loss: -0.09573876857757568
Batch 37/64 loss: -0.07704448699951172
Batch 38/64 loss: -0.07889747619628906
Batch 39/64 loss: -0.08826512098312378
Batch 40/64 loss: -0.0994563102722168
Batch 41/64 loss: -0.06895238161087036
Batch 42/64 loss: -0.1130295991897583
Batch 43/64 loss: -0.06352889537811279
Batch 44/64 loss: -0.07694673538208008
Batch 45/64 loss: -0.07516080141067505
Batch 46/64 loss: -0.10334664583206177
Batch 47/64 loss: -0.09664726257324219
Batch 48/64 loss: -0.11613744497299194
Batch 49/64 loss: -0.10140997171401978
Batch 50/64 loss: -0.07930296659469604
Batch 51/64 loss: -0.11614543199539185
Batch 52/64 loss: -0.08108949661254883
Batch 53/64 loss: -0.05768388509750366
Batch 54/64 loss: -0.09402304887771606
Batch 55/64 loss: -0.09193873405456543
Batch 56/64 loss: -0.10135114192962646
Batch 57/64 loss: -0.0886237621307373
Batch 58/64 loss: -0.07620221376419067
Batch 59/64 loss: -0.08287012577056885
Batch 60/64 loss: -0.12074494361877441
Batch 61/64 loss: -0.09930592775344849
Batch 62/64 loss: -0.10245954990386963
Batch 63/64 loss: -0.08473539352416992
Batch 64/64 loss: -0.07543671131134033
Epoch 292  Train loss: -0.09247184874964695  Val loss: 0.04459486876156732
Epoch 293
-------------------------------
Batch 1/64 loss: -0.09634923934936523
Batch 2/64 loss: -0.1174967885017395
Batch 3/64 loss: -0.10105830430984497
Batch 4/64 loss: -0.0930793285369873
Batch 5/64 loss: -0.0949866771697998
Batch 6/64 loss: -0.07065480947494507
Batch 7/64 loss: -0.0710606575012207
Batch 8/64 loss: -0.11325132846832275
Batch 9/64 loss: -0.09376394748687744
Batch 10/64 loss: -0.09276306629180908
Batch 11/64 loss: -0.09057587385177612
Batch 12/64 loss: -0.10930228233337402
Batch 13/64 loss: -0.10085523128509521
Batch 14/64 loss: -0.09229838848114014
Batch 15/64 loss: -0.09123694896697998
Batch 16/64 loss: -0.09795868396759033
Batch 17/64 loss: -0.07832109928131104
Batch 18/64 loss: -0.08417367935180664
Batch 19/64 loss: -0.09867697954177856
Batch 20/64 loss: -0.09031796455383301
Batch 21/64 loss: -0.11566495895385742
Batch 22/64 loss: -0.07634568214416504
Batch 23/64 loss: -0.09397292137145996
Batch 24/64 loss: -0.08207166194915771
Batch 25/64 loss: -0.07440322637557983
Batch 26/64 loss: -0.09115785360336304
Batch 27/64 loss: -0.12145376205444336
Batch 28/64 loss: -0.08656924962997437
Batch 29/64 loss: -0.0832471251487732
Batch 30/64 loss: -0.0796651840209961
Batch 31/64 loss: -0.08878093957901001
Batch 32/64 loss: -0.11082279682159424
Batch 33/64 loss: -0.09581059217453003
Batch 34/64 loss: -0.08790707588195801
Batch 35/64 loss: -0.09591323137283325
Batch 36/64 loss: -0.11787956953048706
Batch 37/64 loss: -0.09431993961334229
Batch 38/64 loss: -0.11580854654312134
Batch 39/64 loss: -0.08060789108276367
Batch 40/64 loss: -0.08812844753265381
Batch 41/64 loss: -0.10291087627410889
Batch 42/64 loss: -0.10034006834030151
Batch 43/64 loss: -0.06101584434509277
Batch 44/64 loss: -0.11068427562713623
Batch 45/64 loss: -0.11470824480056763
Batch 46/64 loss: -0.10679346323013306
Batch 47/64 loss: -0.0868712067604065
Batch 48/64 loss: -0.10688513517379761
Batch 49/64 loss: -0.10215169191360474
Batch 50/64 loss: -0.09652340412139893
Batch 51/64 loss: -0.07402819395065308
Batch 52/64 loss: -0.08696359395980835
Batch 53/64 loss: -0.08772486448287964
Batch 54/64 loss: -0.12424647808074951
Batch 55/64 loss: -0.0986815094947815
Batch 56/64 loss: -0.11395806074142456
Batch 57/64 loss: -0.09693187475204468
Batch 58/64 loss: -0.07285374402999878
Batch 59/64 loss: -0.08762842416763306
Batch 60/64 loss: -0.0951879620552063
Batch 61/64 loss: -0.09411537647247314
Batch 62/64 loss: -0.10506939888000488
Batch 63/64 loss: -0.08524811267852783
Batch 64/64 loss: -0.08315145969390869
Epoch 293  Train loss: -0.09462897777557373  Val loss: 0.046615372818360215
Epoch 294
-------------------------------
Batch 1/64 loss: -0.1010865569114685
Batch 2/64 loss: -0.08419370651245117
Batch 3/64 loss: -0.08524584770202637
Batch 4/64 loss: -0.09112757444381714
Batch 5/64 loss: -0.0941048264503479
Batch 6/64 loss: -0.09451901912689209
Batch 7/64 loss: -0.14169788360595703
Batch 8/64 loss: -0.11831635236740112
Batch 9/64 loss: -0.09915083646774292
Batch 10/64 loss: -0.0769052505493164
Batch 11/64 loss: -0.10011231899261475
Batch 12/64 loss: -0.07691431045532227
Batch 13/64 loss: -0.06923335790634155
Batch 14/64 loss: -0.05179882049560547
Batch 15/64 loss: -0.07994794845581055
Batch 16/64 loss: -0.0826181173324585
Batch 17/64 loss: -0.09680777788162231
Batch 18/64 loss: -0.09946811199188232
Batch 19/64 loss: -0.09044229984283447
Batch 20/64 loss: -0.09348833560943604
Batch 21/64 loss: -0.08529454469680786
Batch 22/64 loss: -0.10497665405273438
Batch 23/64 loss: -0.1206396222114563
Batch 24/64 loss: -0.0950048565864563
Batch 25/64 loss: -0.09246575832366943
Batch 26/64 loss: -0.09170335531234741
Batch 27/64 loss: -0.10758066177368164
Batch 28/64 loss: -0.10594338178634644
Batch 29/64 loss: -0.11776727437973022
Batch 30/64 loss: -0.08177375793457031
Batch 31/64 loss: -0.09558647871017456
Batch 32/64 loss: -0.07658612728118896
Batch 33/64 loss: -0.10171699523925781
Batch 34/64 loss: -0.10400581359863281
Batch 35/64 loss: -0.0941774845123291
Batch 36/64 loss: -0.07804709672927856
Batch 37/64 loss: -0.08848100900650024
Batch 38/64 loss: -0.10383802652359009
Batch 39/64 loss: -0.06848162412643433
Batch 40/64 loss: -0.10914772748947144
Batch 41/64 loss: -0.09342533349990845
Batch 42/64 loss: -0.06337600946426392
Batch 43/64 loss: -0.09302520751953125
Batch 44/64 loss: -0.09194177389144897
Batch 45/64 loss: -0.06796222925186157
Batch 46/64 loss: -0.0791369080543518
Batch 47/64 loss: -0.10277795791625977
Batch 48/64 loss: -0.10486966371536255
Batch 49/64 loss: -0.07973349094390869
Batch 50/64 loss: -0.10696142911911011
Batch 51/64 loss: -0.08455163240432739
Batch 52/64 loss: -0.07697880268096924
Batch 53/64 loss: -0.10146725177764893
Batch 54/64 loss: -0.10462218523025513
Batch 55/64 loss: -0.09698408842086792
Batch 56/64 loss: -0.10013771057128906
Batch 57/64 loss: -0.09792399406433105
Batch 58/64 loss: -0.10648894309997559
Batch 59/64 loss: -0.09918981790542603
Batch 60/64 loss: -0.10730576515197754
Batch 61/64 loss: -0.08664792776107788
Batch 62/64 loss: -0.09788447618484497
Batch 63/64 loss: -0.0839892029762268
Batch 64/64 loss: -0.086434006690979
Epoch 294  Train loss: -0.0932173303529328  Val loss: 0.04571067918207228
Epoch 295
-------------------------------
Batch 1/64 loss: -0.1123010516166687
Batch 2/64 loss: -0.10680627822875977
Batch 3/64 loss: -0.11553949117660522
Batch 4/64 loss: -0.0977330207824707
Batch 5/64 loss: -0.06806594133377075
Batch 6/64 loss: -0.0894775390625
Batch 7/64 loss: -0.08605730533599854
Batch 8/64 loss: -0.06416070461273193
Batch 9/64 loss: -0.08507251739501953
Batch 10/64 loss: -0.09263813495635986
Batch 11/64 loss: -0.12834405899047852
Batch 12/64 loss: -0.11344563961029053
Batch 13/64 loss: -0.03960263729095459
Batch 14/64 loss: -0.10071301460266113
Batch 15/64 loss: -0.07369661331176758
Batch 16/64 loss: -0.11285126209259033
Batch 17/64 loss: -0.10988926887512207
Batch 18/64 loss: -0.08107602596282959
Batch 19/64 loss: -0.06832057237625122
Batch 20/64 loss: -0.09332036972045898
Batch 21/64 loss: -0.10984134674072266
Batch 22/64 loss: -0.07514601945877075
Batch 23/64 loss: -0.08820617198944092
Batch 24/64 loss: -0.08462047576904297
Batch 25/64 loss: -0.09008961915969849
Batch 26/64 loss: -0.09201258420944214
Batch 27/64 loss: -0.080028235912323
Batch 28/64 loss: -0.0985184907913208
Batch 29/64 loss: -0.08253449201583862
Batch 30/64 loss: -0.11935532093048096
Batch 31/64 loss: -0.11922752857208252
Batch 32/64 loss: -0.11529678106307983
Batch 33/64 loss: -0.07654643058776855
Batch 34/64 loss: -0.09267491102218628
Batch 35/64 loss: -0.08353829383850098
Batch 36/64 loss: -0.08243155479431152
Batch 37/64 loss: -0.0871572494506836
Batch 38/64 loss: -0.09895068407058716
Batch 39/64 loss: -0.08273011445999146
Batch 40/64 loss: -0.09708470106124878
Batch 41/64 loss: -0.0791161060333252
Batch 42/64 loss: -0.09496116638183594
Batch 43/64 loss: -0.09815454483032227
Batch 44/64 loss: -0.07631397247314453
Batch 45/64 loss: -0.07393956184387207
Batch 46/64 loss: -0.09766793251037598
Batch 47/64 loss: -0.07026666402816772
Batch 48/64 loss: -0.0662183165550232
Batch 49/64 loss: -0.09970390796661377
Batch 50/64 loss: -0.0744284987449646
Batch 51/64 loss: -0.11971104145050049
Batch 52/64 loss: -0.09133094549179077
Batch 53/64 loss: -0.10419678688049316
Batch 54/64 loss: -0.10979300737380981
Batch 55/64 loss: -0.09267741441726685
Batch 56/64 loss: -0.09387898445129395
Batch 57/64 loss: -0.09718197584152222
Batch 58/64 loss: -0.10116767883300781
Batch 59/64 loss: -0.10398924350738525
Batch 60/64 loss: -0.0956881046295166
Batch 61/64 loss: -0.08256161212921143
Batch 62/64 loss: -0.10420846939086914
Batch 63/64 loss: -0.09210389852523804
Batch 64/64 loss: -0.07146310806274414
Epoch 295  Train loss: -0.09204642538930856  Val loss: 0.0479368815307355
Epoch 296
-------------------------------
Batch 1/64 loss: -0.07703894376754761
Batch 2/64 loss: -0.0864875316619873
Batch 3/64 loss: -0.09364748001098633
Batch 4/64 loss: -0.05079293251037598
Batch 5/64 loss: -0.09819644689559937
Batch 6/64 loss: -0.11200284957885742
Batch 7/64 loss: -0.12060809135437012
Batch 8/64 loss: -0.06305474042892456
Batch 9/64 loss: -0.11243140697479248
Batch 10/64 loss: -0.10737872123718262
Batch 11/64 loss: -0.07604420185089111
Batch 12/64 loss: -0.09498012065887451
Batch 13/64 loss: -0.09593367576599121
Batch 14/64 loss: -0.09344393014907837
Batch 15/64 loss: -0.0775681734085083
Batch 16/64 loss: -0.10153347253799438
Batch 17/64 loss: -0.10634976625442505
Batch 18/64 loss: -0.058640360832214355
Batch 19/64 loss: -0.09465938806533813
Batch 20/64 loss: -0.09432089328765869
Batch 21/64 loss: -0.10392522811889648
Batch 22/64 loss: -0.09401726722717285
Batch 23/64 loss: -0.09777176380157471
Batch 24/64 loss: -0.0913817286491394
Batch 25/64 loss: -0.07261788845062256
Batch 26/64 loss: -0.09811490774154663
Batch 27/64 loss: -0.0755385160446167
Batch 28/64 loss: -0.10767608880996704
Batch 29/64 loss: -0.10213130712509155
Batch 30/64 loss: -0.09233254194259644
Batch 31/64 loss: -0.08942770957946777
Batch 32/64 loss: -0.09381324052810669
Batch 33/64 loss: -0.08433187007904053
Batch 34/64 loss: -0.07386064529418945
Batch 35/64 loss: -0.10744363069534302
Batch 36/64 loss: -0.092415452003479
Batch 37/64 loss: -0.0889197587966919
Batch 38/64 loss: -0.09959375858306885
Batch 39/64 loss: -0.07795828580856323
Batch 40/64 loss: -0.08491599559783936
Batch 41/64 loss: -0.12001556158065796
Batch 42/64 loss: -0.1108478307723999
Batch 43/64 loss: -0.09194076061248779
Batch 44/64 loss: -0.09519213438034058
Batch 45/64 loss: -0.10508882999420166
Batch 46/64 loss: -0.10345029830932617
Batch 47/64 loss: -0.10602962970733643
Batch 48/64 loss: -0.10448199510574341
Batch 49/64 loss: -0.08399587869644165
Batch 50/64 loss: -0.09614455699920654
Batch 51/64 loss: -0.11187887191772461
Batch 52/64 loss: -0.09884452819824219
Batch 53/64 loss: -0.09258371591567993
Batch 54/64 loss: -0.07327806949615479
Batch 55/64 loss: -0.11842381954193115
Batch 56/64 loss: -0.07824945449829102
Batch 57/64 loss: -0.08051973581314087
Batch 58/64 loss: -0.11025083065032959
Batch 59/64 loss: -0.09303790330886841
Batch 60/64 loss: -0.09758996963500977
Batch 61/64 loss: -0.09840607643127441
Batch 62/64 loss: -0.09811371564865112
Batch 63/64 loss: -0.1089397668838501
Batch 64/64 loss: -0.11350619792938232
Epoch 296  Train loss: -0.09420759677886963  Val loss: 0.04590761108496755
Epoch 297
-------------------------------
Batch 1/64 loss: -0.10709279775619507
Batch 2/64 loss: -0.11078596115112305
Batch 3/64 loss: -0.0801541805267334
Batch 4/64 loss: -0.09315931797027588
Batch 5/64 loss: -0.10632741451263428
Batch 6/64 loss: -0.10290050506591797
Batch 7/64 loss: -0.10930252075195312
Batch 8/64 loss: -0.0977814793586731
Batch 9/64 loss: -0.0849270224571228
Batch 10/64 loss: -0.09971195459365845
Batch 11/64 loss: -0.08851957321166992
Batch 12/64 loss: -0.08866482973098755
Batch 13/64 loss: -0.08970433473587036
Batch 14/64 loss: -0.10392260551452637
Batch 15/64 loss: -0.09613686800003052
Batch 16/64 loss: -0.11080294847488403
Batch 17/64 loss: -0.10548019409179688
Batch 18/64 loss: -0.10138589143753052
Batch 19/64 loss: -0.0840412974357605
Batch 20/64 loss: -0.08603298664093018
Batch 21/64 loss: -0.10859715938568115
Batch 22/64 loss: -0.09776794910430908
Batch 23/64 loss: -0.0904049277305603
Batch 24/64 loss: -0.12444233894348145
Batch 25/64 loss: -0.09065288305282593
Batch 26/64 loss: -0.09727132320404053
Batch 27/64 loss: -0.07186651229858398
Batch 28/64 loss: -0.08285987377166748
Batch 29/64 loss: -0.11332321166992188
Batch 30/64 loss: -0.08263289928436279
Batch 31/64 loss: -0.09883701801300049
Batch 32/64 loss: -0.10387450456619263
Batch 33/64 loss: -0.11366629600524902
Batch 34/64 loss: -0.09812843799591064
Batch 35/64 loss: -0.09868776798248291
Batch 36/64 loss: -0.1131485104560852
Batch 37/64 loss: -0.11399275064468384
Batch 38/64 loss: -0.10258513689041138
Batch 39/64 loss: -0.08513164520263672
Batch 40/64 loss: -0.1087501049041748
Batch 41/64 loss: -0.08936834335327148
Batch 42/64 loss: -0.06143224239349365
Batch 43/64 loss: -0.1122637391090393
Batch 44/64 loss: -0.11134076118469238
Batch 45/64 loss: -0.09303957223892212
Batch 46/64 loss: -0.09320145845413208
Batch 47/64 loss: -0.10171645879745483
Batch 48/64 loss: -0.09470605850219727
Batch 49/64 loss: -0.10151255130767822
Batch 50/64 loss: -0.09596502780914307
Batch 51/64 loss: -0.0686335563659668
Batch 52/64 loss: -0.10728085041046143
Batch 53/64 loss: -0.08287966251373291
Batch 54/64 loss: -0.09439891576766968
Batch 55/64 loss: -0.09476816654205322
Batch 56/64 loss: -0.11097735166549683
Batch 57/64 loss: -0.11664152145385742
Batch 58/64 loss: -0.09813827276229858
Batch 59/64 loss: -0.10162991285324097
Batch 60/64 loss: -0.07978183031082153
Batch 61/64 loss: -0.12214988470077515
Batch 62/64 loss: -0.11394912004470825
Batch 63/64 loss: -0.1013643741607666
Batch 64/64 loss: -0.08232218027114868
Epoch 297  Train loss: -0.09807587768517288  Val loss: 0.046616002046775164
Epoch 298
-------------------------------
Batch 1/64 loss: -0.09954869747161865
Batch 2/64 loss: -0.09879511594772339
Batch 3/64 loss: -0.11858296394348145
Batch 4/64 loss: -0.09561443328857422
Batch 5/64 loss: -0.11975264549255371
Batch 6/64 loss: -0.11996787786483765
Batch 7/64 loss: -0.11615478992462158
Batch 8/64 loss: -0.10689759254455566
Batch 9/64 loss: -0.09584033489227295
Batch 10/64 loss: -0.05913889408111572
Batch 11/64 loss: -0.127363920211792
Batch 12/64 loss: -0.1300506591796875
Batch 13/64 loss: -0.11836522817611694
Batch 14/64 loss: -0.09495013952255249
Batch 15/64 loss: -0.07842910289764404
Batch 16/64 loss: -0.0785561203956604
Batch 17/64 loss: -0.10929656028747559
Batch 18/64 loss: -0.08668553829193115
Batch 19/64 loss: -0.08418130874633789
Batch 20/64 loss: -0.08752012252807617
Batch 21/64 loss: -0.05560654401779175
Batch 22/64 loss: -0.08699166774749756
Batch 23/64 loss: -0.09944391250610352
Batch 24/64 loss: -0.10662591457366943
Batch 25/64 loss: -0.08985799551010132
Batch 26/64 loss: -0.0950707197189331
Batch 27/64 loss: -0.09936493635177612
Batch 28/64 loss: -0.10163265466690063
Batch 29/64 loss: -0.060038089752197266
Batch 30/64 loss: -0.10715872049331665
Batch 31/64 loss: -0.10840994119644165
Batch 32/64 loss: -0.09102475643157959
Batch 33/64 loss: -0.11860513687133789
Batch 34/64 loss: -0.06668126583099365
Batch 35/64 loss: -0.09821921586990356
Batch 36/64 loss: -0.09913098812103271
Batch 37/64 loss: -0.08496731519699097
Batch 38/64 loss: -0.08978486061096191
Batch 39/64 loss: -0.09564054012298584
Batch 40/64 loss: -0.0823204517364502
Batch 41/64 loss: -0.088509202003479
Batch 42/64 loss: -0.06169116497039795
Batch 43/64 loss: -0.08816301822662354
Batch 44/64 loss: -0.06047999858856201
Batch 45/64 loss: -0.07900756597518921
Batch 46/64 loss: -0.061744093894958496
Batch 47/64 loss: -0.07554382085800171
Batch 48/64 loss: -0.10018247365951538
Batch 49/64 loss: -0.11700451374053955
Batch 50/64 loss: -0.11964672803878784
Batch 51/64 loss: -0.11859005689620972
Batch 52/64 loss: -0.09319263696670532
Batch 53/64 loss: -0.10943830013275146
Batch 54/64 loss: -0.07833731174468994
Batch 55/64 loss: -0.10272502899169922
Batch 56/64 loss: -0.10203737020492554
Batch 57/64 loss: -0.09889495372772217
Batch 58/64 loss: -0.09708094596862793
Batch 59/64 loss: -0.10484457015991211
Batch 60/64 loss: -0.07187145948410034
Batch 61/64 loss: -0.10371571779251099
Batch 62/64 loss: -0.11311763525009155
Batch 63/64 loss: -0.08045226335525513
Batch 64/64 loss: -0.1186785101890564
Epoch 298  Train loss: -0.09533407664766498  Val loss: 0.04750718734518359
Epoch 299
-------------------------------
Batch 1/64 loss: -0.10857897996902466
Batch 2/64 loss: -0.10536056756973267
Batch 3/64 loss: -0.08339506387710571
Batch 4/64 loss: -0.07993471622467041
Batch 5/64 loss: -0.11452168226242065
Batch 6/64 loss: -0.08038860559463501
Batch 7/64 loss: -0.09903818368911743
Batch 8/64 loss: -0.09043264389038086
Batch 9/64 loss: -0.10171329975128174
Batch 10/64 loss: -0.12257927656173706
Batch 11/64 loss: -0.10525822639465332
Batch 12/64 loss: -0.08964580297470093
Batch 13/64 loss: -0.09995114803314209
Batch 14/64 loss: -0.09059453010559082
Batch 15/64 loss: -0.08868837356567383
Batch 16/64 loss: -0.08779376745223999
Batch 17/64 loss: -0.10122448205947876
Batch 18/64 loss: -0.051152706146240234
Batch 19/64 loss: -0.06805539131164551
Batch 20/64 loss: -0.08966779708862305
Batch 21/64 loss: -0.12389791011810303
Batch 22/64 loss: -0.09742271900177002
Batch 23/64 loss: -0.0826336145401001
Batch 24/64 loss: -0.09996706247329712
Batch 25/64 loss: -0.09497153759002686
Batch 26/64 loss: -0.11633533239364624
Batch 27/64 loss: -0.08989298343658447
Batch 28/64 loss: -0.09504979848861694
Batch 29/64 loss: -0.09531545639038086
Batch 30/64 loss: -0.08807235956192017
Batch 31/64 loss: -0.10141944885253906
Batch 32/64 loss: -0.10503941774368286
Batch 33/64 loss: -0.12097793817520142
Batch 34/64 loss: -0.09541594982147217
Batch 35/64 loss: -0.08580225706100464
Batch 36/64 loss: -0.1119844913482666
Batch 37/64 loss: -0.08938831090927124
Batch 38/64 loss: -0.10891562700271606
Batch 39/64 loss: -0.10703873634338379
Batch 40/64 loss: -0.08389288187026978
Batch 41/64 loss: -0.1098175048828125
Batch 42/64 loss: -0.10614311695098877
Batch 43/64 loss: -0.10202246904373169
Batch 44/64 loss: -0.09412181377410889
Batch 45/64 loss: -0.10428047180175781
Batch 46/64 loss: -0.09203416109085083
Batch 47/64 loss: -0.11565959453582764
Batch 48/64 loss: -0.09815335273742676
Batch 49/64 loss: -0.1069035530090332
Batch 50/64 loss: -0.0855187177658081
Batch 51/64 loss: -0.08827948570251465
Batch 52/64 loss: -0.09327751398086548
Batch 53/64 loss: -0.10187572240829468
Batch 54/64 loss: -0.06937438249588013
Batch 55/64 loss: -0.12498468160629272
Batch 56/64 loss: -0.08836376667022705
Batch 57/64 loss: -0.12019860744476318
Batch 58/64 loss: -0.10578227043151855
Batch 59/64 loss: -0.110881507396698
Batch 60/64 loss: -0.09455817937850952
Batch 61/64 loss: -0.06814819574356079
Batch 62/64 loss: -0.10226321220397949
Batch 63/64 loss: -0.07789576053619385
Batch 64/64 loss: -0.06612032651901245
Epoch 299  Train loss: -0.09665156649608238  Val loss: 0.048248155829832724
Epoch 300
-------------------------------
Batch 1/64 loss: -0.09459584951400757
Batch 2/64 loss: -0.09752357006072998
Batch 3/64 loss: -0.10666275024414062
Batch 4/64 loss: -0.09794026613235474
Batch 5/64 loss: -0.06400692462921143
Batch 6/64 loss: -0.10770142078399658
Batch 7/64 loss: -0.0987284779548645
Batch 8/64 loss: -0.08625632524490356
Batch 9/64 loss: -0.09713578224182129
Batch 10/64 loss: -0.09547621011734009
Batch 11/64 loss: -0.08056914806365967
Batch 12/64 loss: -0.09209442138671875
Batch 13/64 loss: -0.08565413951873779
Batch 14/64 loss: -0.12192344665527344
Batch 15/64 loss: -0.08906716108322144
Batch 16/64 loss: -0.11509597301483154
Batch 17/64 loss: -0.10581940412521362
Batch 18/64 loss: -0.09688973426818848
Batch 19/64 loss: -0.11602002382278442
Batch 20/64 loss: -0.07500851154327393
Batch 21/64 loss: -0.09301167726516724
Batch 22/64 loss: -0.06805098056793213
Batch 23/64 loss: -0.1258556842803955
Batch 24/64 loss: -0.08196514844894409
Batch 25/64 loss: -0.09157454967498779
Batch 26/64 loss: -0.11665338277816772
Batch 27/64 loss: -0.07341641187667847
Batch 28/64 loss: -0.1127747893333435
Batch 29/64 loss: -0.10575467348098755
Batch 30/64 loss: -0.09139448404312134
Batch 31/64 loss: -0.08490365743637085
Batch 32/64 loss: -0.08702212572097778
Batch 33/64 loss: -0.08562487363815308
Batch 34/64 loss: -0.09878504276275635
Batch 35/64 loss: -0.08693110942840576
Batch 36/64 loss: -0.10506349802017212
Batch 37/64 loss: -0.10603702068328857
Batch 38/64 loss: -0.09254294633865356
Batch 39/64 loss: -0.10660946369171143
Batch 40/64 loss: -0.09205454587936401
Batch 41/64 loss: -0.0815349817276001
Batch 42/64 loss: -0.09557688236236572
Batch 43/64 loss: -0.10886603593826294
Batch 44/64 loss: -0.10629463195800781
Batch 45/64 loss: -0.1030130386352539
Batch 46/64 loss: -0.1273951530456543
Batch 47/64 loss: -0.097076416015625
Batch 48/64 loss: -0.11297452449798584
Batch 49/64 loss: -0.0859375
Batch 50/64 loss: -0.09161853790283203
Batch 51/64 loss: -0.09755229949951172
Batch 52/64 loss: -0.11624616384506226
Batch 53/64 loss: -0.10737133026123047
Batch 54/64 loss: -0.07648193836212158
Batch 55/64 loss: -0.08072096109390259
Batch 56/64 loss: -0.08752590417861938
Batch 57/64 loss: -0.09622848033905029
Batch 58/64 loss: -0.06120800971984863
Batch 59/64 loss: -0.09770941734313965
Batch 60/64 loss: -0.0880553126335144
Batch 61/64 loss: -0.10806584358215332
Batch 62/64 loss: -0.09067082405090332
Batch 63/64 loss: -0.10503971576690674
Batch 64/64 loss: -0.10229623317718506
Epoch 300  Train loss: -0.09615814405329087  Val loss: 0.049126089233713054
Epoch 301
-------------------------------
Batch 1/64 loss: -0.10160380601882935
Batch 2/64 loss: -0.11637270450592041
Batch 3/64 loss: -0.10881388187408447
Batch 4/64 loss: -0.11091500520706177
Batch 5/64 loss: -0.12038213014602661
Batch 6/64 loss: -0.10088509321212769
Batch 7/64 loss: -0.0859518051147461
Batch 8/64 loss: -0.10089778900146484
Batch 9/64 loss: -0.0776524543762207
Batch 10/64 loss: -0.05603647232055664
Batch 11/64 loss: -0.09441816806793213
Batch 12/64 loss: -0.1169438362121582
Batch 13/64 loss: -0.08904582262039185
Batch 14/64 loss: -0.11910420656204224
Batch 15/64 loss: -0.10658019781112671
Batch 16/64 loss: -0.09791231155395508
Batch 17/64 loss: -0.08767741918563843
Batch 18/64 loss: -0.08455038070678711
Batch 19/64 loss: -0.07390880584716797
Batch 20/64 loss: -0.10719525814056396
Batch 21/64 loss: -0.08437079191207886
Batch 22/64 loss: -0.11850303411483765
Batch 23/64 loss: -0.1021047830581665
Batch 24/64 loss: -0.07561510801315308
Batch 25/64 loss: -0.1209479570388794
Batch 26/64 loss: -0.09550368785858154
Batch 27/64 loss: -0.09110575914382935
Batch 28/64 loss: -0.07906472682952881
Batch 29/64 loss: -0.126076340675354
Batch 30/64 loss: -0.10567903518676758
Batch 31/64 loss: -0.09865856170654297
Batch 32/64 loss: -0.08052974939346313
Batch 33/64 loss: -0.1029667854309082
Batch 34/64 loss: -0.12626886367797852
Batch 35/64 loss: -0.0909498929977417
Batch 36/64 loss: -0.09340876340866089
Batch 37/64 loss: -0.1077767014503479
Batch 38/64 loss: -0.093436598777771
Batch 39/64 loss: -0.07473254203796387
Batch 40/64 loss: -0.11182844638824463
Batch 41/64 loss: -0.09903925657272339
Batch 42/64 loss: -0.10026341676712036
Batch 43/64 loss: -0.06309139728546143
Batch 44/64 loss: -0.11101526021957397
Batch 45/64 loss: -0.08554524183273315
Batch 46/64 loss: -0.08345276117324829
Batch 47/64 loss: -0.10822445154190063
Batch 48/64 loss: -0.08676612377166748
Batch 49/64 loss: -0.09544658660888672
Batch 50/64 loss: -0.1261441707611084
Batch 51/64 loss: -0.09330576658248901
Batch 52/64 loss: -0.08545291423797607
Batch 53/64 loss: -0.10371959209442139
Batch 54/64 loss: -0.08900868892669678
Batch 55/64 loss: -0.09175658226013184
Batch 56/64 loss: -0.08372712135314941
Batch 57/64 loss: -0.07455068826675415
Batch 58/64 loss: -0.09084272384643555
Batch 59/64 loss: -0.07521277666091919
Batch 60/64 loss: -0.09810709953308105
Batch 61/64 loss: -0.07019388675689697
Batch 62/64 loss: -0.12786900997161865
Batch 63/64 loss: -0.09511274099349976
Batch 64/64 loss: -0.09256792068481445
Epoch 301  Train loss: -0.0963709784489052  Val loss: 0.051099254093628976
Epoch 302
-------------------------------
Batch 1/64 loss: -0.10614514350891113
Batch 2/64 loss: -0.11134612560272217
Batch 3/64 loss: -0.09882718324661255
Batch 4/64 loss: -0.11540943384170532
Batch 5/64 loss: -0.07721227407455444
Batch 6/64 loss: -0.08665013313293457
Batch 7/64 loss: -0.09319019317626953
Batch 8/64 loss: -0.08870315551757812
Batch 9/64 loss: -0.11580872535705566
Batch 10/64 loss: -0.11795061826705933
Batch 11/64 loss: -0.093258798122406
Batch 12/64 loss: -0.09752225875854492
Batch 13/64 loss: -0.0959896445274353
Batch 14/64 loss: -0.11331403255462646
Batch 15/64 loss: -0.10549724102020264
Batch 16/64 loss: -0.10207986831665039
Batch 17/64 loss: -0.08258742094039917
Batch 18/64 loss: -0.1036367416381836
Batch 19/64 loss: -0.0987318754196167
Batch 20/64 loss: -0.126589834690094
Batch 21/64 loss: -0.08804553747177124
Batch 22/64 loss: -0.08431625366210938
Batch 23/64 loss: -0.11365735530853271
Batch 24/64 loss: -0.0972968339920044
Batch 25/64 loss: -0.11553150415420532
Batch 26/64 loss: -0.1188536286354065
Batch 27/64 loss: -0.10632574558258057
Batch 28/64 loss: -0.08886218070983887
Batch 29/64 loss: -0.09056943655014038
Batch 30/64 loss: -0.10839754343032837
Batch 31/64 loss: -0.0979490876197815
Batch 32/64 loss: -0.116382896900177
Batch 33/64 loss: -0.07733100652694702
Batch 34/64 loss: -0.10247623920440674
Batch 35/64 loss: -0.09766733646392822
Batch 36/64 loss: -0.07823407649993896
Batch 37/64 loss: -0.11494302749633789
Batch 38/64 loss: -0.11105024814605713
Batch 39/64 loss: -0.12306094169616699
Batch 40/64 loss: -0.05858922004699707
Batch 41/64 loss: -0.08644062280654907
Batch 42/64 loss: -0.055638670921325684
Batch 43/64 loss: -0.09006297588348389
Batch 44/64 loss: -0.11426520347595215
Batch 45/64 loss: -0.09609389305114746
Batch 46/64 loss: -0.09162598848342896
Batch 47/64 loss: -0.09508448839187622
Batch 48/64 loss: -0.09051346778869629
Batch 49/64 loss: -0.08286088705062866
Batch 50/64 loss: -0.0966368317604065
Batch 51/64 loss: -0.0965166687965393
Batch 52/64 loss: -0.08836525678634644
Batch 53/64 loss: -0.09733527898788452
Batch 54/64 loss: -0.07625895738601685
Batch 55/64 loss: -0.1130266785621643
Batch 56/64 loss: -0.07662993669509888
Batch 57/64 loss: -0.0845300555229187
Batch 58/64 loss: -0.07373315095901489
Batch 59/64 loss: -0.09809696674346924
Batch 60/64 loss: -0.09889757633209229
Batch 61/64 loss: -0.09566152095794678
Batch 62/64 loss: -0.11152631044387817
Batch 63/64 loss: -0.1049146056175232
Batch 64/64 loss: -0.0890359878540039
Epoch 302  Train loss: -0.09727817703695858  Val loss: 0.04695539376170365
Epoch 303
-------------------------------
Batch 1/64 loss: -0.10242801904678345
Batch 2/64 loss: -0.09437096118927002
Batch 3/64 loss: -0.11350667476654053
Batch 4/64 loss: -0.06562924385070801
Batch 5/64 loss: -0.0872335433959961
Batch 6/64 loss: -0.08447957038879395
Batch 7/64 loss: -0.11583554744720459
Batch 8/64 loss: -0.10691916942596436
Batch 9/64 loss: -0.11831361055374146
Batch 10/64 loss: -0.13003933429718018
Batch 11/64 loss: -0.08078312873840332
Batch 12/64 loss: -0.08162158727645874
Batch 13/64 loss: -0.08597588539123535
Batch 14/64 loss: -0.09727048873901367
Batch 15/64 loss: -0.09936290979385376
Batch 16/64 loss: -0.11289983987808228
Batch 17/64 loss: -0.07050597667694092
Batch 18/64 loss: -0.1047600507736206
Batch 19/64 loss: -0.08914071321487427
Batch 20/64 loss: -0.12157195806503296
Batch 21/64 loss: -0.1046401858329773
Batch 22/64 loss: -0.080436110496521
Batch 23/64 loss: -0.09803450107574463
Batch 24/64 loss: -0.11310285329818726
Batch 25/64 loss: -0.103543221950531
Batch 26/64 loss: -0.11837393045425415
Batch 27/64 loss: -0.10438644886016846
Batch 28/64 loss: -0.1056622862815857
Batch 29/64 loss: -0.10938382148742676
Batch 30/64 loss: -0.1012982726097107
Batch 31/64 loss: -0.08277076482772827
Batch 32/64 loss: -0.08592647314071655
Batch 33/64 loss: -0.0832403302192688
Batch 34/64 loss: -0.08423393964767456
Batch 35/64 loss: -0.10447227954864502
Batch 36/64 loss: -0.07803654670715332
Batch 37/64 loss: -0.10985606908798218
Batch 38/64 loss: -0.07509058713912964
Batch 39/64 loss: -0.11676067113876343
Batch 40/64 loss: -0.09551119804382324
Batch 41/64 loss: -0.07398808002471924
Batch 42/64 loss: -0.08929741382598877
Batch 43/64 loss: -0.10439866781234741
Batch 44/64 loss: -0.09589779376983643
Batch 45/64 loss: -0.10645973682403564
Batch 46/64 loss: -0.11010783910751343
Batch 47/64 loss: -0.09261637926101685
Batch 48/64 loss: -0.10134965181350708
Batch 49/64 loss: -0.09490841627120972
Batch 50/64 loss: -0.09530067443847656
Batch 51/64 loss: -0.11283975839614868
Batch 52/64 loss: -0.08606946468353271
Batch 53/64 loss: -0.08884686231613159
Batch 54/64 loss: -0.11009526252746582
Batch 55/64 loss: -0.11644041538238525
Batch 56/64 loss: -0.054917991161346436
Batch 57/64 loss: -0.12064337730407715
Batch 58/64 loss: -0.09289032220840454
Batch 59/64 loss: -0.10436201095581055
Batch 60/64 loss: -0.10439890623092651
Batch 61/64 loss: -0.09006726741790771
Batch 62/64 loss: -0.10262060165405273
Batch 63/64 loss: -0.10359477996826172
Batch 64/64 loss: -0.1148374080657959
Epoch 303  Train loss: -0.0981278185750924  Val loss: 0.04629105063238505
Epoch 304
-------------------------------
Batch 1/64 loss: -0.1158953309059143
Batch 2/64 loss: -0.09440183639526367
Batch 3/64 loss: -0.11459147930145264
Batch 4/64 loss: -0.09292566776275635
Batch 5/64 loss: -0.10341906547546387
Batch 6/64 loss: -0.07301479578018188
Batch 7/64 loss: -0.09734553098678589
Batch 8/64 loss: -0.09228891134262085
Batch 9/64 loss: -0.1172894835472107
Batch 10/64 loss: -0.11733841896057129
Batch 11/64 loss: -0.12633651494979858
Batch 12/64 loss: -0.12260448932647705
Batch 13/64 loss: -0.08726704120635986
Batch 14/64 loss: -0.10287594795227051
Batch 15/64 loss: -0.07731974124908447
Batch 16/64 loss: -0.10907912254333496
Batch 17/64 loss: -0.07633745670318604
Batch 18/64 loss: -0.056688785552978516
Batch 19/64 loss: -0.0771796703338623
Batch 20/64 loss: -0.10882657766342163
Batch 21/64 loss: -0.11345934867858887
Batch 22/64 loss: -0.11702555418014526
Batch 23/64 loss: -0.12738573551177979
Batch 24/64 loss: -0.10691595077514648
Batch 25/64 loss: -0.11884874105453491
Batch 26/64 loss: -0.08750158548355103
Batch 27/64 loss: -0.11238348484039307
Batch 28/64 loss: -0.09141308069229126
Batch 29/64 loss: -0.09688198566436768
Batch 30/64 loss: -0.10260844230651855
Batch 31/64 loss: -0.08305233716964722
Batch 32/64 loss: -0.08017480373382568
Batch 33/64 loss: -0.1222463846206665
Batch 34/64 loss: -0.09049844741821289
Batch 35/64 loss: -0.08397412300109863
Batch 36/64 loss: -0.10004359483718872
Batch 37/64 loss: -0.07831382751464844
Batch 38/64 loss: -0.11659079790115356
Batch 39/64 loss: -0.1022486686706543
Batch 40/64 loss: -0.0920863151550293
Batch 41/64 loss: -0.09420669078826904
Batch 42/64 loss: -0.07654273509979248
Batch 43/64 loss: -0.10334712266921997
Batch 44/64 loss: -0.09349524974822998
Batch 45/64 loss: -0.09990483522415161
Batch 46/64 loss: -0.10321468114852905
Batch 47/64 loss: -0.0934954285621643
Batch 48/64 loss: -0.08333581686019897
Batch 49/64 loss: -0.11505168676376343
Batch 50/64 loss: -0.1052541732788086
Batch 51/64 loss: -0.11180424690246582
Batch 52/64 loss: -0.09237015247344971
Batch 53/64 loss: -0.12162470817565918
Batch 54/64 loss: -0.11097943782806396
Batch 55/64 loss: -0.08199137449264526
Batch 56/64 loss: -0.08456486463546753
Batch 57/64 loss: -0.1138753890991211
Batch 58/64 loss: -0.13015079498291016
Batch 59/64 loss: -0.12765616178512573
Batch 60/64 loss: -0.0957825779914856
Batch 61/64 loss: -0.1023566722869873
Batch 62/64 loss: -0.074837327003479
Batch 63/64 loss: -0.03265690803527832
Batch 64/64 loss: -0.09307152032852173
Epoch 304  Train loss: -0.09890167456047208  Val loss: 0.0481122744452093
Epoch 305
-------------------------------
Batch 1/64 loss: -0.09246945381164551
Batch 2/64 loss: -0.08393782377243042
Batch 3/64 loss: -0.08200663328170776
Batch 4/64 loss: -0.12758207321166992
Batch 5/64 loss: -0.10665667057037354
Batch 6/64 loss: -0.09763681888580322
Batch 7/64 loss: -0.11286383867263794
Batch 8/64 loss: -0.10216677188873291
Batch 9/64 loss: -0.10572648048400879
Batch 10/64 loss: -0.09431010484695435
Batch 11/64 loss: -0.10871922969818115
Batch 12/64 loss: -0.10718178749084473
Batch 13/64 loss: -0.09601104259490967
Batch 14/64 loss: -0.07787507772445679
Batch 15/64 loss: -0.0996251106262207
Batch 16/64 loss: -0.10922253131866455
Batch 17/64 loss: -0.11612874269485474
Batch 18/64 loss: -0.04714059829711914
Batch 19/64 loss: -0.09491413831710815
Batch 20/64 loss: -0.10338586568832397
Batch 21/64 loss: -0.1089944839477539
Batch 22/64 loss: -0.09578710794448853
Batch 23/64 loss: -0.11451596021652222
Batch 24/64 loss: -0.09924584627151489
Batch 25/64 loss: -0.11001986265182495
Batch 26/64 loss: -0.09113419055938721
Batch 27/64 loss: -0.10438472032546997
Batch 28/64 loss: -0.08306699991226196
Batch 29/64 loss: -0.097831130027771
Batch 30/64 loss: -0.07146090269088745
Batch 31/64 loss: -0.10432565212249756
Batch 32/64 loss: -0.08789163827896118
Batch 33/64 loss: -0.05865055322647095
Batch 34/64 loss: -0.10807716846466064
Batch 35/64 loss: -0.09772467613220215
Batch 36/64 loss: -0.11203610897064209
Batch 37/64 loss: -0.12260150909423828
Batch 38/64 loss: -0.08300960063934326
Batch 39/64 loss: -0.09584176540374756
Batch 40/64 loss: -0.0982939600944519
Batch 41/64 loss: -0.10093849897384644
Batch 42/64 loss: -0.09831482172012329
Batch 43/64 loss: -0.11675608158111572
Batch 44/64 loss: -0.11333829164505005
Batch 45/64 loss: -0.08022546768188477
Batch 46/64 loss: -0.08876419067382812
Batch 47/64 loss: -0.07496398687362671
Batch 48/64 loss: -0.09191203117370605
Batch 49/64 loss: -0.12807905673980713
Batch 50/64 loss: -0.0900927186012268
Batch 51/64 loss: -0.09277015924453735
Batch 52/64 loss: -0.13508647680282593
Batch 53/64 loss: -0.11815351247787476
Batch 54/64 loss: -0.10346764326095581
Batch 55/64 loss: -0.10975921154022217
Batch 56/64 loss: -0.1039775013923645
Batch 57/64 loss: -0.09863930940628052
Batch 58/64 loss: -0.09563446044921875
Batch 59/64 loss: -0.09608006477355957
Batch 60/64 loss: -0.0866769552230835
Batch 61/64 loss: -0.10471475124359131
Batch 62/64 loss: -0.10235369205474854
Batch 63/64 loss: -0.1002112627029419
Batch 64/64 loss: -0.0908050537109375
Epoch 305  Train loss: -0.09897205595876657  Val loss: 0.04570030428699611
Epoch 306
-------------------------------
Batch 1/64 loss: -0.09999573230743408
Batch 2/64 loss: -0.11507898569107056
Batch 3/64 loss: -0.09739482402801514
Batch 4/64 loss: -0.1132049560546875
Batch 5/64 loss: -0.09752649068832397
Batch 6/64 loss: -0.08812189102172852
Batch 7/64 loss: -0.12113398313522339
Batch 8/64 loss: -0.10844117403030396
Batch 9/64 loss: -0.10351711511611938
Batch 10/64 loss: -0.11507612466812134
Batch 11/64 loss: -0.10637402534484863
Batch 12/64 loss: -0.10197639465332031
Batch 13/64 loss: -0.06537795066833496
Batch 14/64 loss: -0.09392362833023071
Batch 15/64 loss: -0.10776674747467041
Batch 16/64 loss: -0.09482228755950928
Batch 17/64 loss: -0.09298837184906006
Batch 18/64 loss: -0.07468664646148682
Batch 19/64 loss: -0.09747910499572754
Batch 20/64 loss: -0.10804575681686401
Batch 21/64 loss: -0.10000437498092651
Batch 22/64 loss: -0.09325295686721802
Batch 23/64 loss: -0.10752880573272705
Batch 24/64 loss: -0.08968979120254517
Batch 25/64 loss: -0.11162614822387695
Batch 26/64 loss: -0.12677550315856934
Batch 27/64 loss: -0.09808814525604248
Batch 28/64 loss: -0.10800457000732422
Batch 29/64 loss: -0.09072589874267578
Batch 30/64 loss: -0.1312645673751831
Batch 31/64 loss: -0.08294904232025146
Batch 32/64 loss: -0.12415415048599243
Batch 33/64 loss: -0.12136805057525635
Batch 34/64 loss: -0.10415589809417725
Batch 35/64 loss: -0.11021935939788818
Batch 36/64 loss: -0.11081856489181519
Batch 37/64 loss: -0.10045450925827026
Batch 38/64 loss: -0.10925960540771484
Batch 39/64 loss: -0.1007353663444519
Batch 40/64 loss: -0.11795991659164429
Batch 41/64 loss: -0.09193682670593262
Batch 42/64 loss: -0.10060930252075195
Batch 43/64 loss: -0.08184552192687988
Batch 44/64 loss: -0.08108264207839966
Batch 45/64 loss: -0.09116047620773315
Batch 46/64 loss: -0.06753724813461304
Batch 47/64 loss: -0.09869384765625
Batch 48/64 loss: -0.10457921028137207
Batch 49/64 loss: -0.10367429256439209
Batch 50/64 loss: -0.10454404354095459
Batch 51/64 loss: -0.11427450180053711
Batch 52/64 loss: -0.0947001576423645
Batch 53/64 loss: -0.10839372873306274
Batch 54/64 loss: -0.11700963973999023
Batch 55/64 loss: -0.09875577688217163
Batch 56/64 loss: -0.07668077945709229
Batch 57/64 loss: -0.1130027174949646
Batch 58/64 loss: -0.08857929706573486
Batch 59/64 loss: -0.06311357021331787
Batch 60/64 loss: -0.11001944541931152
Batch 61/64 loss: -0.12398242950439453
Batch 62/64 loss: -0.11530429124832153
Batch 63/64 loss: -0.10689055919647217
Batch 64/64 loss: -0.07674318552017212
Epoch 306  Train loss: -0.10126894291709451  Val loss: 0.046256720405264
Epoch 307
-------------------------------
Batch 1/64 loss: -0.09255373477935791
Batch 2/64 loss: -0.10399842262268066
Batch 3/64 loss: -0.09247112274169922
Batch 4/64 loss: -0.08824241161346436
Batch 5/64 loss: -0.10841459035873413
Batch 6/64 loss: -0.11070245504379272
Batch 7/64 loss: -0.1286606788635254
Batch 8/64 loss: -0.109524667263031
Batch 9/64 loss: -0.09103918075561523
Batch 10/64 loss: -0.1028904914855957
Batch 11/64 loss: -0.10245358943939209
Batch 12/64 loss: -0.10561889410018921
Batch 13/64 loss: -0.10177439451217651
Batch 14/64 loss: -0.0864264965057373
Batch 15/64 loss: -0.10791051387786865
Batch 16/64 loss: -0.11637306213378906
Batch 17/64 loss: -0.12461012601852417
Batch 18/64 loss: -0.10005617141723633
Batch 19/64 loss: -0.09930419921875
Batch 20/64 loss: -0.09755927324295044
Batch 21/64 loss: -0.11803030967712402
Batch 22/64 loss: -0.08331763744354248
Batch 23/64 loss: -0.07379806041717529
Batch 24/64 loss: -0.08931344747543335
Batch 25/64 loss: -0.07569271326065063
Batch 26/64 loss: -0.10832679271697998
Batch 27/64 loss: -0.05735546350479126
Batch 28/64 loss: -0.09364908933639526
Batch 29/64 loss: -0.059173643589019775
Batch 30/64 loss: -0.08468383550643921
Batch 31/64 loss: -0.07118487358093262
Batch 32/64 loss: -0.07704490423202515
Batch 33/64 loss: -0.12746286392211914
Batch 34/64 loss: -0.11086750030517578
Batch 35/64 loss: -0.1245003342628479
Batch 36/64 loss: -0.09569573402404785
Batch 37/64 loss: -0.10811501741409302
Batch 38/64 loss: -0.10672283172607422
Batch 39/64 loss: -0.10406148433685303
Batch 40/64 loss: -0.11122769117355347
Batch 41/64 loss: -0.04902195930480957
Batch 42/64 loss: -0.09798634052276611
Batch 43/64 loss: -0.10069125890731812
Batch 44/64 loss: -0.07053160667419434
Batch 45/64 loss: -0.12218540906906128
Batch 46/64 loss: -0.09954261779785156
Batch 47/64 loss: -0.08161544799804688
Batch 48/64 loss: -0.09484034776687622
Batch 49/64 loss: -0.10718154907226562
Batch 50/64 loss: -0.0821945071220398
Batch 51/64 loss: -0.09536385536193848
Batch 52/64 loss: -0.09599286317825317
Batch 53/64 loss: -0.08832520246505737
Batch 54/64 loss: -0.09930551052093506
Batch 55/64 loss: -0.09744763374328613
Batch 56/64 loss: -0.10804945230484009
Batch 57/64 loss: -0.07291233539581299
Batch 58/64 loss: -0.11623156070709229
Batch 59/64 loss: -0.08224934339523315
Batch 60/64 loss: -0.12167036533355713
Batch 61/64 loss: -0.08941060304641724
Batch 62/64 loss: -0.12444651126861572
Batch 63/64 loss: -0.11499011516571045
Batch 64/64 loss: -0.10048508644104004
Epoch 307  Train loss: -0.0978566104290532  Val loss: 0.04596733657764815
Epoch 308
-------------------------------
Batch 1/64 loss: -0.11603277921676636
Batch 2/64 loss: -0.11684674024581909
Batch 3/64 loss: -0.10873687267303467
Batch 4/64 loss: -0.10474652051925659
Batch 5/64 loss: -0.11469084024429321
Batch 6/64 loss: -0.0675046443939209
Batch 7/64 loss: -0.1033775806427002
Batch 8/64 loss: -0.11506623029708862
Batch 9/64 loss: -0.10665631294250488
Batch 10/64 loss: -0.07183903455734253
Batch 11/64 loss: -0.0706680417060852
Batch 12/64 loss: -0.10329043865203857
Batch 13/64 loss: -0.10874676704406738
Batch 14/64 loss: -0.06207132339477539
Batch 15/64 loss: -0.12867385149002075
Batch 16/64 loss: -0.10882413387298584
Batch 17/64 loss: -0.10350322723388672
Batch 18/64 loss: -0.10417002439498901
Batch 19/64 loss: -0.09353137016296387
Batch 20/64 loss: -0.11708956956863403
Batch 21/64 loss: -0.11129510402679443
Batch 22/64 loss: -0.10733938217163086
Batch 23/64 loss: -0.08823096752166748
Batch 24/64 loss: -0.12169450521469116
Batch 25/64 loss: -0.09068459272384644
Batch 26/64 loss: -0.10160768032073975
Batch 27/64 loss: -0.11651182174682617
Batch 28/64 loss: -0.1099136471748352
Batch 29/64 loss: -0.08985376358032227
Batch 30/64 loss: -0.10252201557159424
Batch 31/64 loss: -0.11156195402145386
Batch 32/64 loss: -0.08566516637802124
Batch 33/64 loss: -0.10939383506774902
Batch 34/64 loss: -0.10500431060791016
Batch 35/64 loss: -0.09287476539611816
Batch 36/64 loss: -0.10425424575805664
Batch 37/64 loss: -0.12108045816421509
Batch 38/64 loss: -0.11348497867584229
Batch 39/64 loss: -0.1038215160369873
Batch 40/64 loss: -0.10601907968521118
Batch 41/64 loss: -0.07235562801361084
Batch 42/64 loss: -0.09350961446762085
Batch 43/64 loss: -0.11652511358261108
Batch 44/64 loss: -0.10693669319152832
Batch 45/64 loss: -0.08829927444458008
Batch 46/64 loss: -0.07894355058670044
Batch 47/64 loss: -0.1051170825958252
Batch 48/64 loss: -0.09278404712677002
Batch 49/64 loss: -0.09147554636001587
Batch 50/64 loss: -0.08243614435195923
Batch 51/64 loss: -0.08282262086868286
Batch 52/64 loss: -0.07056868076324463
Batch 53/64 loss: -0.1128462553024292
Batch 54/64 loss: -0.0921250581741333
Batch 55/64 loss: -0.08886778354644775
Batch 56/64 loss: -0.09982478618621826
Batch 57/64 loss: -0.11916816234588623
Batch 58/64 loss: -0.09525895118713379
Batch 59/64 loss: -0.08583790063858032
Batch 60/64 loss: -0.09392255544662476
Batch 61/64 loss: -0.10079324245452881
Batch 62/64 loss: -0.10336238145828247
Batch 63/64 loss: -0.09824424982070923
Batch 64/64 loss: -0.12879765033721924
Epoch 308  Train loss: -0.10019613574532901  Val loss: 0.047355741364849394
Epoch 309
-------------------------------
Batch 1/64 loss: -0.08689075708389282
Batch 2/64 loss: -0.12707781791687012
Batch 3/64 loss: -0.09815812110900879
Batch 4/64 loss: -0.10833513736724854
Batch 5/64 loss: -0.09265601634979248
Batch 6/64 loss: -0.07544130086898804
Batch 7/64 loss: -0.12350553274154663
Batch 8/64 loss: -0.10582011938095093
Batch 9/64 loss: -0.09499400854110718
Batch 10/64 loss: -0.0728868842124939
Batch 11/64 loss: -0.10360348224639893
Batch 12/64 loss: -0.11341673135757446
Batch 13/64 loss: -0.060867488384246826
Batch 14/64 loss: -0.11288928985595703
Batch 15/64 loss: -0.10234981775283813
Batch 16/64 loss: -0.130706787109375
Batch 17/64 loss: -0.13404786586761475
Batch 18/64 loss: -0.11830896139144897
Batch 19/64 loss: -0.11486595869064331
Batch 20/64 loss: -0.08779299259185791
Batch 21/64 loss: -0.08829480409622192
Batch 22/64 loss: -0.09858298301696777
Batch 23/64 loss: -0.13683438301086426
Batch 24/64 loss: -0.0970757007598877
Batch 25/64 loss: -0.09402990341186523
Batch 26/64 loss: -0.10699522495269775
Batch 27/64 loss: -0.08667021989822388
Batch 28/64 loss: -0.08998227119445801
Batch 29/64 loss: -0.10587942600250244
Batch 30/64 loss: -0.09061706066131592
Batch 31/64 loss: -0.09792435169219971
Batch 32/64 loss: -0.12348461151123047
Batch 33/64 loss: -0.10529160499572754
Batch 34/64 loss: -0.11718851327896118
Batch 35/64 loss: -0.10289329290390015
Batch 36/64 loss: -0.10092473030090332
Batch 37/64 loss: -0.10546755790710449
Batch 38/64 loss: -0.09633398056030273
Batch 39/64 loss: -0.10431993007659912
Batch 40/64 loss: -0.10764479637145996
Batch 41/64 loss: -0.09364122152328491
Batch 42/64 loss: -0.11691755056381226
Batch 43/64 loss: -0.11882650852203369
Batch 44/64 loss: -0.1190367341041565
Batch 45/64 loss: -0.09737920761108398
Batch 46/64 loss: -0.10768359899520874
Batch 47/64 loss: -0.08754843473434448
Batch 48/64 loss: -0.11214464902877808
Batch 49/64 loss: -0.09546273946762085
Batch 50/64 loss: -0.1203920841217041
Batch 51/64 loss: -0.12438344955444336
Batch 52/64 loss: -0.08524399995803833
Batch 53/64 loss: -0.10613489151000977
Batch 54/64 loss: -0.08573812246322632
Batch 55/64 loss: -0.08139491081237793
Batch 56/64 loss: -0.10712814331054688
Batch 57/64 loss: -0.10296475887298584
Batch 58/64 loss: -0.08284246921539307
Batch 59/64 loss: -0.10097765922546387
Batch 60/64 loss: -0.11158192157745361
Batch 61/64 loss: -0.08817160129547119
Batch 62/64 loss: -0.07602131366729736
Batch 63/64 loss: -0.08723872900009155
Batch 64/64 loss: -0.10263681411743164
Epoch 309  Train loss: -0.10206874866111605  Val loss: 0.04976957606286118
Epoch 310
-------------------------------
Batch 1/64 loss: -0.07777094841003418
Batch 2/64 loss: -0.12127649784088135
Batch 3/64 loss: -0.09537702798843384
Batch 4/64 loss: -0.10098892450332642
Batch 5/64 loss: -0.09577685594558716
Batch 6/64 loss: -0.10373640060424805
Batch 7/64 loss: -0.10600990056991577
Batch 8/64 loss: -0.1086663007736206
Batch 9/64 loss: -0.12069833278656006
Batch 10/64 loss: -0.10388922691345215
Batch 11/64 loss: -0.11432826519012451
Batch 12/64 loss: -0.09814757108688354
Batch 13/64 loss: -0.10469472408294678
Batch 14/64 loss: -0.11236649751663208
Batch 15/64 loss: -0.0820494294166565
Batch 16/64 loss: -0.10645914077758789
Batch 17/64 loss: -0.10950350761413574
Batch 18/64 loss: -0.09426122903823853
Batch 19/64 loss: -0.08618944883346558
Batch 20/64 loss: -0.09433561563491821
Batch 21/64 loss: -0.09935116767883301
Batch 22/64 loss: -0.1012682318687439
Batch 23/64 loss: -0.10324633121490479
Batch 24/64 loss: -0.09447836875915527
Batch 25/64 loss: -0.09348756074905396
Batch 26/64 loss: -0.05523782968521118
Batch 27/64 loss: -0.1246490478515625
Batch 28/64 loss: -0.10534334182739258
Batch 29/64 loss: -0.10907995700836182
Batch 30/64 loss: -0.11537271738052368
Batch 31/64 loss: -0.12670177221298218
Batch 32/64 loss: -0.10069304704666138
Batch 33/64 loss: -0.10697877407073975
Batch 34/64 loss: -0.12286746501922607
Batch 35/64 loss: -0.13318735361099243
Batch 36/64 loss: -0.11405342817306519
Batch 37/64 loss: -0.10431748628616333
Batch 38/64 loss: -0.10530000925064087
Batch 39/64 loss: -0.11450308561325073
Batch 40/64 loss: -0.09342014789581299
Batch 41/64 loss: -0.07915341854095459
Batch 42/64 loss: -0.09703230857849121
Batch 43/64 loss: -0.06207460165023804
Batch 44/64 loss: -0.09907180070877075
Batch 45/64 loss: -0.10184955596923828
Batch 46/64 loss: -0.11380326747894287
Batch 47/64 loss: -0.11115777492523193
Batch 48/64 loss: -0.10430240631103516
Batch 49/64 loss: -0.09453713893890381
Batch 50/64 loss: -0.10653877258300781
Batch 51/64 loss: -0.05660969018936157
Batch 52/64 loss: -0.10217553377151489
Batch 53/64 loss: -0.09549933671951294
Batch 54/64 loss: -0.0866844654083252
Batch 55/64 loss: -0.10580945014953613
Batch 56/64 loss: -0.09469872713088989
Batch 57/64 loss: -0.11922061443328857
Batch 58/64 loss: -0.11121225357055664
Batch 59/64 loss: -0.10223233699798584
Batch 60/64 loss: -0.10417777299880981
Batch 61/64 loss: -0.0940169095993042
Batch 62/64 loss: -0.08638012409210205
Batch 63/64 loss: -0.0973958969116211
Batch 64/64 loss: -0.08739888668060303
Epoch 310  Train loss: -0.10119602025723924  Val loss: 0.04683805208435583
Epoch 311
-------------------------------
Batch 1/64 loss: -0.10588932037353516
Batch 2/64 loss: -0.10651063919067383
Batch 3/64 loss: -0.11657494306564331
Batch 4/64 loss: -0.12581419944763184
Batch 5/64 loss: -0.08269369602203369
Batch 6/64 loss: -0.07845509052276611
Batch 7/64 loss: -0.104850172996521
Batch 8/64 loss: -0.09143102169036865
Batch 9/64 loss: -0.11153900623321533
Batch 10/64 loss: -0.07522213459014893
Batch 11/64 loss: -0.08172988891601562
Batch 12/64 loss: -0.11203014850616455
Batch 13/64 loss: -0.11100596189498901
Batch 14/64 loss: -0.10669082403182983
Batch 15/64 loss: -0.07076317071914673
Batch 16/64 loss: -0.10414302349090576
Batch 17/64 loss: -0.07744550704956055
Batch 18/64 loss: -0.10867476463317871
Batch 19/64 loss: -0.07499080896377563
Batch 20/64 loss: -0.1041073203086853
Batch 21/64 loss: -0.08836781978607178
Batch 22/64 loss: -0.07900238037109375
Batch 23/64 loss: -0.10275304317474365
Batch 24/64 loss: -0.11049044132232666
Batch 25/64 loss: -0.08862859010696411
Batch 26/64 loss: -0.09830623865127563
Batch 27/64 loss: -0.10756486654281616
Batch 28/64 loss: -0.09719878435134888
Batch 29/64 loss: -0.1009758710861206
Batch 30/64 loss: -0.11051684617996216
Batch 31/64 loss: -0.1022258996963501
Batch 32/64 loss: -0.10815930366516113
Batch 33/64 loss: -0.1139366626739502
Batch 34/64 loss: -0.1064615249633789
Batch 35/64 loss: -0.07767879962921143
Batch 36/64 loss: -0.11074715852737427
Batch 37/64 loss: -0.08983474969863892
Batch 38/64 loss: -0.07285261154174805
Batch 39/64 loss: -0.1217648983001709
Batch 40/64 loss: -0.08109384775161743
Batch 41/64 loss: -0.1077185869216919
Batch 42/64 loss: -0.11746054887771606
Batch 43/64 loss: -0.1190759539604187
Batch 44/64 loss: -0.07874482870101929
Batch 45/64 loss: -0.10958611965179443
Batch 46/64 loss: -0.11470603942871094
Batch 47/64 loss: -0.11906582117080688
Batch 48/64 loss: -0.09911435842514038
Batch 49/64 loss: -0.08749997615814209
Batch 50/64 loss: -0.09961128234863281
Batch 51/64 loss: -0.11111968755722046
Batch 52/64 loss: -0.08649343252182007
Batch 53/64 loss: -0.0915183424949646
Batch 54/64 loss: -0.09968483448028564
Batch 55/64 loss: -0.08547401428222656
Batch 56/64 loss: -0.10404157638549805
Batch 57/64 loss: -0.11428654193878174
Batch 58/64 loss: -0.12598848342895508
Batch 59/64 loss: -0.11774188280105591
Batch 60/64 loss: -0.10713481903076172
Batch 61/64 loss: -0.11681437492370605
Batch 62/64 loss: -0.10165083408355713
Batch 63/64 loss: -0.09402155876159668
Batch 64/64 loss: -0.08676880598068237
Epoch 311  Train loss: -0.10027847032920988  Val loss: 0.0483446727503616
Epoch 312
-------------------------------
Batch 1/64 loss: -0.06620210409164429
Batch 2/64 loss: -0.11847865581512451
Batch 3/64 loss: -0.11979395151138306
Batch 4/64 loss: -0.07984161376953125
Batch 5/64 loss: -0.08997225761413574
Batch 6/64 loss: -0.10688614845275879
Batch 7/64 loss: -0.10984557867050171
Batch 8/64 loss: -0.1306057572364807
Batch 9/64 loss: -0.07665431499481201
Batch 10/64 loss: -0.11595159769058228
Batch 11/64 loss: -0.1167764663696289
Batch 12/64 loss: -0.12446117401123047
Batch 13/64 loss: -0.10598421096801758
Batch 14/64 loss: -0.10509264469146729
Batch 15/64 loss: -0.10318434238433838
Batch 16/64 loss: -0.09710776805877686
Batch 17/64 loss: -0.11423695087432861
Batch 18/64 loss: -0.0948522686958313
Batch 19/64 loss: -0.07530665397644043
Batch 20/64 loss: -0.10728096961975098
Batch 21/64 loss: -0.11471796035766602
Batch 22/64 loss: -0.12447386980056763
Batch 23/64 loss: -0.11397582292556763
Batch 24/64 loss: -0.1104971170425415
Batch 25/64 loss: -0.09463512897491455
Batch 26/64 loss: -0.0979272723197937
Batch 27/64 loss: -0.09396809339523315
Batch 28/64 loss: -0.1031152606010437
Batch 29/64 loss: -0.08135098218917847
Batch 30/64 loss: -0.10569721460342407
Batch 31/64 loss: -0.12978595495224
Batch 32/64 loss: -0.11755234003067017
Batch 33/64 loss: -0.10201752185821533
Batch 34/64 loss: -0.10662829875946045
Batch 35/64 loss: -0.10342782735824585
Batch 36/64 loss: -0.10988283157348633
Batch 37/64 loss: -0.10710716247558594
Batch 38/64 loss: -0.0861121416091919
Batch 39/64 loss: -0.0980268120765686
Batch 40/64 loss: -0.11006104946136475
Batch 41/64 loss: -0.08306336402893066
Batch 42/64 loss: -0.0720834732055664
Batch 43/64 loss: -0.07113885879516602
Batch 44/64 loss: -0.09530901908874512
Batch 45/64 loss: -0.10520321130752563
Batch 46/64 loss: -0.11217248439788818
Batch 47/64 loss: -0.10242283344268799
Batch 48/64 loss: -0.100131094455719
Batch 49/64 loss: -0.09821021556854248
Batch 50/64 loss: -0.11794215440750122
Batch 51/64 loss: -0.12260842323303223
Batch 52/64 loss: -0.09962964057922363
Batch 53/64 loss: -0.07989394664764404
Batch 54/64 loss: -0.10116910934448242
Batch 55/64 loss: -0.12561166286468506
Batch 56/64 loss: -0.09017592668533325
Batch 57/64 loss: -0.1235230565071106
Batch 58/64 loss: -0.09422963857650757
Batch 59/64 loss: -0.10141187906265259
Batch 60/64 loss: -0.13218814134597778
Batch 61/64 loss: -0.07256865501403809
Batch 62/64 loss: -0.10821914672851562
Batch 63/64 loss: -0.0978078842163086
Batch 64/64 loss: -0.10533696413040161
Epoch 312  Train loss: -0.10282655112883624  Val loss: 0.04613512897819178
Epoch 313
-------------------------------
Batch 1/64 loss: -0.05547976493835449
Batch 2/64 loss: -0.09471046924591064
Batch 3/64 loss: -0.08550423383712769
Batch 4/64 loss: -0.10845291614532471
Batch 5/64 loss: -0.1317462921142578
Batch 6/64 loss: -0.12193727493286133
Batch 7/64 loss: -0.1112634539604187
Batch 8/64 loss: -0.0750887393951416
Batch 9/64 loss: -0.10045981407165527
Batch 10/64 loss: -0.11951541900634766
Batch 11/64 loss: -0.0693090558052063
Batch 12/64 loss: -0.11706161499023438
Batch 13/64 loss: -0.09773409366607666
Batch 14/64 loss: -0.10349971055984497
Batch 15/64 loss: -0.11056500673294067
Batch 16/64 loss: -0.09463322162628174
Batch 17/64 loss: -0.1143462061882019
Batch 18/64 loss: -0.09077978134155273
Batch 19/64 loss: -0.10451006889343262
Batch 20/64 loss: -0.11540460586547852
Batch 21/64 loss: -0.06620180606842041
Batch 22/64 loss: -0.12803518772125244
Batch 23/64 loss: -0.11022031307220459
Batch 24/64 loss: -0.09394538402557373
Batch 25/64 loss: -0.1407797932624817
Batch 26/64 loss: -0.08540678024291992
Batch 27/64 loss: -0.12622594833374023
Batch 28/64 loss: -0.08857423067092896
Batch 29/64 loss: -0.1068657636642456
Batch 30/64 loss: -0.06439507007598877
Batch 31/64 loss: -0.08411198854446411
Batch 32/64 loss: -0.10302692651748657
Batch 33/64 loss: -0.1072043776512146
Batch 34/64 loss: -0.10736268758773804
Batch 35/64 loss: -0.11718213558197021
Batch 36/64 loss: -0.07189929485321045
Batch 37/64 loss: -0.1187666654586792
Batch 38/64 loss: -0.09317368268966675
Batch 39/64 loss: -0.11114883422851562
Batch 40/64 loss: -0.10430008172988892
Batch 41/64 loss: -0.11810600757598877
Batch 42/64 loss: -0.10082268714904785
Batch 43/64 loss: -0.08535361289978027
Batch 44/64 loss: -0.12323975563049316
Batch 45/64 loss: -0.06491148471832275
Batch 46/64 loss: -0.11951416730880737
Batch 47/64 loss: -0.07850110530853271
Batch 48/64 loss: -0.10453474521636963
Batch 49/64 loss: -0.09284067153930664
Batch 50/64 loss: -0.10582363605499268
Batch 51/64 loss: -0.10076946020126343
Batch 52/64 loss: -0.10210877656936646
Batch 53/64 loss: -0.09034323692321777
Batch 54/64 loss: -0.11642366647720337
Batch 55/64 loss: -0.11793780326843262
Batch 56/64 loss: -0.09534162282943726
Batch 57/64 loss: -0.12066811323165894
Batch 58/64 loss: -0.10275125503540039
Batch 59/64 loss: -0.08685970306396484
Batch 60/64 loss: -0.09564602375030518
Batch 61/64 loss: -0.10110336542129517
Batch 62/64 loss: -0.07852709293365479
Batch 63/64 loss: -0.08822184801101685
Batch 64/64 loss: -0.06069129705429077
Epoch 313  Train loss: -0.10018348249734617  Val loss: 0.04714036090267483
Epoch 314
-------------------------------
Batch 1/64 loss: -0.13195288181304932
Batch 2/64 loss: -0.11224555969238281
Batch 3/64 loss: -0.0978582501411438
Batch 4/64 loss: -0.11356139183044434
Batch 5/64 loss: -0.10784280300140381
Batch 6/64 loss: -0.10835200548171997
Batch 7/64 loss: -0.14071393013000488
Batch 8/64 loss: -0.0800366997718811
Batch 9/64 loss: -0.0999179482460022
Batch 10/64 loss: -0.10695403814315796
Batch 11/64 loss: -0.0886068344116211
Batch 12/64 loss: -0.10596656799316406
Batch 13/64 loss: -0.13179242610931396
Batch 14/64 loss: -0.0788722038269043
Batch 15/64 loss: -0.11754202842712402
Batch 16/64 loss: -0.10766005516052246
Batch 17/64 loss: -0.0802145004272461
Batch 18/64 loss: -0.10887354612350464
Batch 19/64 loss: -0.11364752054214478
Batch 20/64 loss: -0.07103204727172852
Batch 21/64 loss: -0.08532834053039551
Batch 22/64 loss: -0.10434108972549438
Batch 23/64 loss: -0.11644601821899414
Batch 24/64 loss: -0.08123505115509033
Batch 25/64 loss: -0.09559035301208496
Batch 26/64 loss: -0.099129319190979
Batch 27/64 loss: -0.11731696128845215
Batch 28/64 loss: -0.11243569850921631
Batch 29/64 loss: -0.10038566589355469
Batch 30/64 loss: -0.10686510801315308
Batch 31/64 loss: -0.10275137424468994
Batch 32/64 loss: -0.09538161754608154
Batch 33/64 loss: -0.10396385192871094
Batch 34/64 loss: -0.08722734451293945
Batch 35/64 loss: -0.11546802520751953
Batch 36/64 loss: -0.1066964864730835
Batch 37/64 loss: -0.12161725759506226
Batch 38/64 loss: -0.06851387023925781
Batch 39/64 loss: -0.10290384292602539
Batch 40/64 loss: -0.10967177152633667
Batch 41/64 loss: -0.09962451457977295
Batch 42/64 loss: -0.11999225616455078
Batch 43/64 loss: -0.10034394264221191
Batch 44/64 loss: -0.08400338888168335
Batch 45/64 loss: -0.09922623634338379
Batch 46/64 loss: -0.08417457342147827
Batch 47/64 loss: -0.1190720796585083
Batch 48/64 loss: -0.08724725246429443
Batch 49/64 loss: -0.11098319292068481
Batch 50/64 loss: -0.10380178689956665
Batch 51/64 loss: -0.09985631704330444
Batch 52/64 loss: -0.0919681191444397
Batch 53/64 loss: -0.09336018562316895
Batch 54/64 loss: -0.10729843378067017
Batch 55/64 loss: -0.09612917900085449
Batch 56/64 loss: -0.09041750431060791
Batch 57/64 loss: -0.09741640090942383
Batch 58/64 loss: -0.11401349306106567
Batch 59/64 loss: -0.10812193155288696
Batch 60/64 loss: -0.10830962657928467
Batch 61/64 loss: -0.10531371831893921
Batch 62/64 loss: -0.10227954387664795
Batch 63/64 loss: -0.12043863534927368
Batch 64/64 loss: -0.1294274926185608
Epoch 314  Train loss: -0.10317451322779936  Val loss: 0.04931100475829082
Epoch 315
-------------------------------
Batch 1/64 loss: -0.13027697801589966
Batch 2/64 loss: -0.11865401268005371
Batch 3/64 loss: -0.0940900444984436
Batch 4/64 loss: -0.10622423887252808
Batch 5/64 loss: -0.11386191844940186
Batch 6/64 loss: -0.10472255945205688
Batch 7/64 loss: -0.10344427824020386
Batch 8/64 loss: -0.0979202389717102
Batch 9/64 loss: -0.09411805868148804
Batch 10/64 loss: -0.07514476776123047
Batch 11/64 loss: -0.0697258710861206
Batch 12/64 loss: -0.10315942764282227
Batch 13/64 loss: -0.09975075721740723
Batch 14/64 loss: -0.10120588541030884
Batch 15/64 loss: -0.11642777919769287
Batch 16/64 loss: -0.10725075006484985
Batch 17/64 loss: -0.0919337272644043
Batch 18/64 loss: -0.10081905126571655
Batch 19/64 loss: -0.1174013614654541
Batch 20/64 loss: -0.06656736135482788
Batch 21/64 loss: -0.10416311025619507
Batch 22/64 loss: -0.10855972766876221
Batch 23/64 loss: -0.11147409677505493
Batch 24/64 loss: -0.09103238582611084
Batch 25/64 loss: -0.10547685623168945
Batch 26/64 loss: -0.08965718746185303
Batch 27/64 loss: -0.08191549777984619
Batch 28/64 loss: -0.0880511999130249
Batch 29/64 loss: -0.07816267013549805
Batch 30/64 loss: -0.10576242208480835
Batch 31/64 loss: -0.1059882640838623
Batch 32/64 loss: -0.07023340463638306
Batch 33/64 loss: -0.10815829038619995
Batch 34/64 loss: -0.11090075969696045
Batch 35/64 loss: -0.06968492269515991
Batch 36/64 loss: -0.13569283485412598
Batch 37/64 loss: -0.10436224937438965
Batch 38/64 loss: -0.12169677019119263
Batch 39/64 loss: -0.09414136409759521
Batch 40/64 loss: -0.08364689350128174
Batch 41/64 loss: -0.11405271291732788
Batch 42/64 loss: -0.07471030950546265
Batch 43/64 loss: -0.09316766262054443
Batch 44/64 loss: -0.11768150329589844
Batch 45/64 loss: -0.0923270583152771
Batch 46/64 loss: -0.1071404218673706
Batch 47/64 loss: -0.10656970739364624
Batch 48/64 loss: -0.13087522983551025
Batch 49/64 loss: -0.11767995357513428
Batch 50/64 loss: -0.10216540098190308
Batch 51/64 loss: -0.10980474948883057
Batch 52/64 loss: -0.10286635160446167
Batch 53/64 loss: -0.12633568048477173
Batch 54/64 loss: -0.08817172050476074
Batch 55/64 loss: -0.09813052415847778
Batch 56/64 loss: -0.12029391527175903
Batch 57/64 loss: -0.08804631233215332
Batch 58/64 loss: -0.10072976350784302
Batch 59/64 loss: -0.11611670255661011
Batch 60/64 loss: -0.10631781816482544
Batch 61/64 loss: -0.10841023921966553
Batch 62/64 loss: -0.1024090051651001
Batch 63/64 loss: -0.07661449909210205
Batch 64/64 loss: -0.08565831184387207
Epoch 315  Train loss: -0.10111876001545027  Val loss: 0.046471644922630076
Epoch 316
-------------------------------
Batch 1/64 loss: -0.10799199342727661
Batch 2/64 loss: -0.1391858458518982
Batch 3/64 loss: -0.09431636333465576
Batch 4/64 loss: -0.10737133026123047
Batch 5/64 loss: -0.10674071311950684
Batch 6/64 loss: -0.1110566258430481
Batch 7/64 loss: -0.11849963665008545
Batch 8/64 loss: -0.11368489265441895
Batch 9/64 loss: -0.11095720529556274
Batch 10/64 loss: -0.09738051891326904
Batch 11/64 loss: -0.09411710500717163
Batch 12/64 loss: -0.10264486074447632
Batch 13/64 loss: -0.08679229021072388
Batch 14/64 loss: -0.09693491458892822
Batch 15/64 loss: -0.09605789184570312
Batch 16/64 loss: -0.0940600037574768
Batch 17/64 loss: -0.12090307474136353
Batch 18/64 loss: -0.12040764093399048
Batch 19/64 loss: -0.09115368127822876
Batch 20/64 loss: -0.10751152038574219
Batch 21/64 loss: -0.10171067714691162
Batch 22/64 loss: -0.10948175191879272
Batch 23/64 loss: -0.12408643960952759
Batch 24/64 loss: -0.08311599493026733
Batch 25/64 loss: -0.11365288496017456
Batch 26/64 loss: -0.09869199991226196
Batch 27/64 loss: -0.12133055925369263
Batch 28/64 loss: -0.06333673000335693
Batch 29/64 loss: -0.10532623529434204
Batch 30/64 loss: -0.08043307065963745
Batch 31/64 loss: -0.1306954026222229
Batch 32/64 loss: -0.08894181251525879
Batch 33/64 loss: -0.08251732587814331
Batch 34/64 loss: -0.10487127304077148
Batch 35/64 loss: -0.10227054357528687
Batch 36/64 loss: -0.10397297143936157
Batch 37/64 loss: -0.1154320240020752
Batch 38/64 loss: -0.09517538547515869
Batch 39/64 loss: -0.10481244325637817
Batch 40/64 loss: -0.12234437465667725
Batch 41/64 loss: -0.10874038934707642
Batch 42/64 loss: -0.09185957908630371
Batch 43/64 loss: -0.10174167156219482
Batch 44/64 loss: -0.11082929372787476
Batch 45/64 loss: -0.07539504766464233
Batch 46/64 loss: -0.1093406081199646
Batch 47/64 loss: -0.10811841487884521
Batch 48/64 loss: -0.09981900453567505
Batch 49/64 loss: -0.09458625316619873
Batch 50/64 loss: -0.08783072233200073
Batch 51/64 loss: -0.1179971694946289
Batch 52/64 loss: -0.11685889959335327
Batch 53/64 loss: -0.12869524955749512
Batch 54/64 loss: -0.11468344926834106
Batch 55/64 loss: -0.09620034694671631
Batch 56/64 loss: -0.0956760048866272
Batch 57/64 loss: -0.10051131248474121
Batch 58/64 loss: -0.11451596021652222
Batch 59/64 loss: -0.10319817066192627
Batch 60/64 loss: -0.09616208076477051
Batch 61/64 loss: -0.09599769115447998
Batch 62/64 loss: -0.06407302618026733
Batch 63/64 loss: -0.07924425601959229
Batch 64/64 loss: -0.09273391962051392
Epoch 316  Train loss: -0.10277008706448125  Val loss: 0.04517880502025696
Epoch 317
-------------------------------
Batch 1/64 loss: -0.1222425103187561
Batch 2/64 loss: -0.10681605339050293
Batch 3/64 loss: -0.09902948141098022
Batch 4/64 loss: -0.06512254476547241
Batch 5/64 loss: -0.11595618724822998
Batch 6/64 loss: -0.09199035167694092
Batch 7/64 loss: -0.1094052791595459
Batch 8/64 loss: -0.10976868867874146
Batch 9/64 loss: -0.09180563688278198
Batch 10/64 loss: -0.08511823415756226
Batch 11/64 loss: -0.10969281196594238
Batch 12/64 loss: -0.08945071697235107
Batch 13/64 loss: -0.11882466077804565
Batch 14/64 loss: -0.09390592575073242
Batch 15/64 loss: -0.11904674768447876
Batch 16/64 loss: -0.1355210542678833
Batch 17/64 loss: -0.08246296644210815
Batch 18/64 loss: -0.1026606559753418
Batch 19/64 loss: -0.11071914434432983
Batch 20/64 loss: -0.09513235092163086
Batch 21/64 loss: -0.07188314199447632
Batch 22/64 loss: -0.1022568941116333
Batch 23/64 loss: -0.10615593194961548
Batch 24/64 loss: -0.13993602991104126
Batch 25/64 loss: -0.10167282819747925
Batch 26/64 loss: -0.11892944574356079
Batch 27/64 loss: -0.11163073778152466
Batch 28/64 loss: -0.06934744119644165
Batch 29/64 loss: -0.09645909070968628
Batch 30/64 loss: -0.14190667867660522
Batch 31/64 loss: -0.0982515811920166
Batch 32/64 loss: -0.11767840385437012
Batch 33/64 loss: -0.12259399890899658
Batch 34/64 loss: -0.09169411659240723
Batch 35/64 loss: -0.1283523440361023
Batch 36/64 loss: -0.09217101335525513
Batch 37/64 loss: -0.0881536602973938
Batch 38/64 loss: -0.0835038423538208
Batch 39/64 loss: -0.102439284324646
Batch 40/64 loss: -0.10680407285690308
Batch 41/64 loss: -0.1176912784576416
Batch 42/64 loss: -0.11046969890594482
Batch 43/64 loss: -0.11795234680175781
Batch 44/64 loss: -0.11109423637390137
Batch 45/64 loss: -0.12625110149383545
Batch 46/64 loss: -0.0979698896408081
Batch 47/64 loss: -0.10211926698684692
Batch 48/64 loss: -0.06144702434539795
Batch 49/64 loss: -0.11781513690948486
Batch 50/64 loss: -0.127996027469635
Batch 51/64 loss: -0.05402374267578125
Batch 52/64 loss: -0.11167669296264648
Batch 53/64 loss: -0.10472387075424194
Batch 54/64 loss: -0.09293484687805176
Batch 55/64 loss: -0.12306839227676392
Batch 56/64 loss: -0.11471420526504517
Batch 57/64 loss: -0.07757341861724854
Batch 58/64 loss: -0.10044354200363159
Batch 59/64 loss: -0.09834611415863037
Batch 60/64 loss: -0.11184030771255493
Batch 61/64 loss: -0.11123073101043701
Batch 62/64 loss: -0.10241222381591797
Batch 63/64 loss: -0.08977842330932617
Batch 64/64 loss: -0.10235738754272461
Epoch 317  Train loss: -0.10363659765206132  Val loss: 0.048952719599930285
Epoch 318
-------------------------------
Batch 1/64 loss: -0.12055045366287231
Batch 2/64 loss: -0.10559964179992676
Batch 3/64 loss: -0.10996419191360474
Batch 4/64 loss: -0.11283516883850098
Batch 5/64 loss: -0.08303064107894897
Batch 6/64 loss: -0.1044650673866272
Batch 7/64 loss: -0.091758131980896
Batch 8/64 loss: -0.11714190244674683
Batch 9/64 loss: -0.1004101037979126
Batch 10/64 loss: -0.09971129894256592
Batch 11/64 loss: -0.11173224449157715
Batch 12/64 loss: -0.09586167335510254
Batch 13/64 loss: -0.10912299156188965
Batch 14/64 loss: -0.11249947547912598
Batch 15/64 loss: -0.1117667555809021
Batch 16/64 loss: -0.13598382472991943
Batch 17/64 loss: -0.107421875
Batch 18/64 loss: -0.0993911623954773
Batch 19/64 loss: -0.10377436876296997
Batch 20/64 loss: -0.10014563798904419
Batch 21/64 loss: -0.10137218236923218
Batch 22/64 loss: -0.11995196342468262
Batch 23/64 loss: -0.095725417137146
Batch 24/64 loss: -0.08757609128952026
Batch 25/64 loss: -0.10100513696670532
Batch 26/64 loss: -0.1260349154472351
Batch 27/64 loss: -0.09608018398284912
Batch 28/64 loss: -0.0913817286491394
Batch 29/64 loss: -0.11279898881912231
Batch 30/64 loss: -0.13430511951446533
Batch 31/64 loss: -0.10073059797286987
Batch 32/64 loss: -0.07723343372344971
Batch 33/64 loss: -0.11277878284454346
Batch 34/64 loss: -0.09602844715118408
Batch 35/64 loss: -0.11871707439422607
Batch 36/64 loss: -0.08807104825973511
Batch 37/64 loss: -0.08262938261032104
Batch 38/64 loss: -0.09235268831253052
Batch 39/64 loss: -0.12323915958404541
Batch 40/64 loss: -0.07670718431472778
Batch 41/64 loss: -0.10818177461624146
Batch 42/64 loss: -0.09266632795333862
Batch 43/64 loss: -0.10692977905273438
Batch 44/64 loss: -0.08494406938552856
Batch 45/64 loss: -0.11876636743545532
Batch 46/64 loss: -0.10249406099319458
Batch 47/64 loss: -0.13253062963485718
Batch 48/64 loss: -0.10662537813186646
Batch 49/64 loss: -0.11987811326980591
Batch 50/64 loss: -0.05443531274795532
Batch 51/64 loss: -0.10203045606613159
Batch 52/64 loss: -0.10596686601638794
Batch 53/64 loss: -0.09604841470718384
Batch 54/64 loss: -0.10139834880828857
Batch 55/64 loss: -0.12406939268112183
Batch 56/64 loss: -0.11070245504379272
Batch 57/64 loss: -0.11224544048309326
Batch 58/64 loss: -0.07495242357254028
Batch 59/64 loss: -0.08208906650543213
Batch 60/64 loss: -0.13574862480163574
Batch 61/64 loss: -0.0806117057800293
Batch 62/64 loss: -0.07609069347381592
Batch 63/64 loss: -0.09405142068862915
Batch 64/64 loss: -0.11618781089782715
Epoch 318  Train loss: -0.10319190773309446  Val loss: 0.046124009537123326
Epoch 319
-------------------------------
Batch 1/64 loss: -0.12038767337799072
Batch 2/64 loss: -0.09372711181640625
Batch 3/64 loss: -0.12608349323272705
Batch 4/64 loss: -0.13210749626159668
Batch 5/64 loss: -0.14177531003952026
Batch 6/64 loss: -0.10150742530822754
Batch 7/64 loss: -0.1210860013961792
Batch 8/64 loss: -0.08782172203063965
Batch 9/64 loss: -0.09456455707550049
Batch 10/64 loss: -0.06799232959747314
Batch 11/64 loss: -0.12847554683685303
Batch 12/64 loss: -0.10652846097946167
Batch 13/64 loss: -0.10530519485473633
Batch 14/64 loss: -0.12115061283111572
Batch 15/64 loss: -0.1022043228149414
Batch 16/64 loss: -0.1153678297996521
Batch 17/64 loss: -0.11350297927856445
Batch 18/64 loss: -0.09367358684539795
Batch 19/64 loss: -0.1193884015083313
Batch 20/64 loss: -0.08804959058761597
Batch 21/64 loss: -0.12456274032592773
Batch 22/64 loss: -0.09850895404815674
Batch 23/64 loss: -0.11374151706695557
Batch 24/64 loss: -0.06936866044998169
Batch 25/64 loss: -0.12527960538864136
Batch 26/64 loss: -0.09949302673339844
Batch 27/64 loss: -0.08336865901947021
Batch 28/64 loss: -0.10696732997894287
Batch 29/64 loss: -0.08499914407730103
Batch 30/64 loss: -0.0934901237487793
Batch 31/64 loss: -0.10308456420898438
Batch 32/64 loss: -0.12356197834014893
Batch 33/64 loss: -0.10675919055938721
Batch 34/64 loss: -0.10580301284790039
Batch 35/64 loss: -0.10656106472015381
Batch 36/64 loss: -0.10175293684005737
Batch 37/64 loss: -0.14199912548065186
Batch 38/64 loss: -0.06991851329803467
Batch 39/64 loss: -0.09542334079742432
Batch 40/64 loss: -0.08423715829849243
Batch 41/64 loss: -0.108883798122406
Batch 42/64 loss: -0.10837692022323608
Batch 43/64 loss: -0.1113044023513794
Batch 44/64 loss: -0.10103744268417358
Batch 45/64 loss: -0.0990445613861084
Batch 46/64 loss: -0.09137630462646484
Batch 47/64 loss: -0.11676573753356934
Batch 48/64 loss: -0.1210368275642395
Batch 49/64 loss: -0.11489677429199219
Batch 50/64 loss: -0.1129869818687439
Batch 51/64 loss: -0.09125125408172607
Batch 52/64 loss: -0.0895652174949646
Batch 53/64 loss: -0.08318620920181274
Batch 54/64 loss: -0.12726938724517822
Batch 55/64 loss: -0.07666170597076416
Batch 56/64 loss: -0.08952617645263672
Batch 57/64 loss: -0.11072772741317749
Batch 58/64 loss: -0.0904536247253418
Batch 59/64 loss: -0.12096256017684937
Batch 60/64 loss: -0.09879881143569946
Batch 61/64 loss: -0.10766535997390747
Batch 62/64 loss: -0.12989741563796997
Batch 63/64 loss: -0.08594709634780884
Batch 64/64 loss: -0.1201331615447998
Epoch 319  Train loss: -0.1050557248732623  Val loss: 0.050089735755396055
Epoch 320
-------------------------------
Batch 1/64 loss: -0.09178602695465088
Batch 2/64 loss: -0.07768702507019043
Batch 3/64 loss: -0.11051374673843384
Batch 4/64 loss: -0.1374979019165039
Batch 5/64 loss: -0.089718759059906
Batch 6/64 loss: -0.08514928817749023
Batch 7/64 loss: -0.09155523777008057
Batch 8/64 loss: -0.10052800178527832
Batch 9/64 loss: -0.11362338066101074
Batch 10/64 loss: -0.1107105016708374
Batch 11/64 loss: -0.11122852563858032
Batch 12/64 loss: -0.11566543579101562
Batch 13/64 loss: -0.07267749309539795
Batch 14/64 loss: -0.11696809530258179
Batch 15/64 loss: -0.08953303098678589
Batch 16/64 loss: -0.12384074926376343
Batch 17/64 loss: -0.1177932620048523
Batch 18/64 loss: -0.10071384906768799
Batch 19/64 loss: -0.08778554201126099
Batch 20/64 loss: -0.11763566732406616
Batch 21/64 loss: -0.1029314398765564
Batch 22/64 loss: -0.07607084512710571
Batch 23/64 loss: -0.11196357011795044
Batch 24/64 loss: -0.11475014686584473
Batch 25/64 loss: -0.09105813503265381
Batch 26/64 loss: -0.133905291557312
Batch 27/64 loss: -0.10104268789291382
Batch 28/64 loss: -0.1321602463722229
Batch 29/64 loss: -0.1100759506225586
Batch 30/64 loss: -0.09151577949523926
Batch 31/64 loss: -0.08520007133483887
Batch 32/64 loss: -0.12172472476959229
Batch 33/64 loss: -0.09936273097991943
Batch 34/64 loss: -0.09934848546981812
Batch 35/64 loss: -0.12318545579910278
Batch 36/64 loss: -0.09312456846237183
Batch 37/64 loss: -0.13218235969543457
Batch 38/64 loss: -0.09996610879898071
Batch 39/64 loss: -0.11953991651535034
Batch 40/64 loss: -0.12547457218170166
Batch 41/64 loss: -0.10464853048324585
Batch 42/64 loss: -0.09119313955307007
Batch 43/64 loss: -0.08659917116165161
Batch 44/64 loss: -0.08321398496627808
Batch 45/64 loss: -0.08983135223388672
Batch 46/64 loss: -0.09031808376312256
Batch 47/64 loss: -0.12107729911804199
Batch 48/64 loss: -0.1138683557510376
Batch 49/64 loss: -0.08755552768707275
Batch 50/64 loss: -0.06923210620880127
Batch 51/64 loss: -0.10372358560562134
Batch 52/64 loss: -0.12540709972381592
Batch 53/64 loss: -0.12717294692993164
Batch 54/64 loss: -0.09756159782409668
Batch 55/64 loss: -0.10522937774658203
Batch 56/64 loss: -0.09548503160476685
Batch 57/64 loss: -0.1293337345123291
Batch 58/64 loss: -0.09340953826904297
Batch 59/64 loss: -0.10070085525512695
Batch 60/64 loss: -0.12242549657821655
Batch 61/64 loss: -0.09116768836975098
Batch 62/64 loss: -0.12264490127563477
Batch 63/64 loss: -0.10559433698654175
Batch 64/64 loss: -0.08241617679595947
Epoch 320  Train loss: -0.1042572624543134  Val loss: 0.0454719808093461
Epoch 321
-------------------------------
Batch 1/64 loss: -0.11660784482955933
Batch 2/64 loss: -0.1172330379486084
Batch 3/64 loss: -0.11201530694961548
Batch 4/64 loss: -0.11011999845504761
Batch 5/64 loss: -0.10813426971435547
Batch 6/64 loss: -0.08209717273712158
Batch 7/64 loss: -0.10828089714050293
Batch 8/64 loss: -0.10033398866653442
Batch 9/64 loss: -0.1097712516784668
Batch 10/64 loss: -0.09659558534622192
Batch 11/64 loss: -0.12868010997772217
Batch 12/64 loss: -0.08481621742248535
Batch 13/64 loss: -0.1184205412864685
Batch 14/64 loss: -0.0948062539100647
Batch 15/64 loss: -0.08226662874221802
Batch 16/64 loss: -0.09905242919921875
Batch 17/64 loss: -0.10002970695495605
Batch 18/64 loss: -0.10331499576568604
Batch 19/64 loss: -0.06543207168579102
Batch 20/64 loss: -0.11579763889312744
Batch 21/64 loss: -0.08838295936584473
Batch 22/64 loss: -0.112124502658844
Batch 23/64 loss: -0.0900810956954956
Batch 24/64 loss: -0.11673194169998169
Batch 25/64 loss: -0.094576895236969
Batch 26/64 loss: -0.12308359146118164
Batch 27/64 loss: -0.1139257550239563
Batch 28/64 loss: -0.08745265007019043
Batch 29/64 loss: -0.08951425552368164
Batch 30/64 loss: -0.10148406028747559
Batch 31/64 loss: -0.08737730979919434
Batch 32/64 loss: -0.08280956745147705
Batch 33/64 loss: -0.10830551385879517
Batch 34/64 loss: -0.1236652135848999
Batch 35/64 loss: -0.10886591672897339
Batch 36/64 loss: -0.08094501495361328
Batch 37/64 loss: -0.10928058624267578
Batch 38/64 loss: -0.11544454097747803
Batch 39/64 loss: -0.10570257902145386
Batch 40/64 loss: -0.1221233606338501
Batch 41/64 loss: -0.09165698289871216
Batch 42/64 loss: -0.12629860639572144
Batch 43/64 loss: -0.10557758808135986
Batch 44/64 loss: -0.08646619319915771
Batch 45/64 loss: -0.11335426568984985
Batch 46/64 loss: -0.09207558631896973
Batch 47/64 loss: -0.09144026041030884
Batch 48/64 loss: -0.09364926815032959
Batch 49/64 loss: -0.13887077569961548
Batch 50/64 loss: -0.10787487030029297
Batch 51/64 loss: -0.10045880079269409
Batch 52/64 loss: -0.10878127813339233
Batch 53/64 loss: -0.11938035488128662
Batch 54/64 loss: -0.09567594528198242
Batch 55/64 loss: -0.12127518653869629
Batch 56/64 loss: -0.10009187459945679
Batch 57/64 loss: -0.09158653020858765
Batch 58/64 loss: -0.11435818672180176
Batch 59/64 loss: -0.11622309684753418
Batch 60/64 loss: -0.1331413984298706
Batch 61/64 loss: -0.10936516523361206
Batch 62/64 loss: -0.11212247610092163
Batch 63/64 loss: -0.0930246114730835
Batch 64/64 loss: -0.08794897794723511
Epoch 321  Train loss: -0.10422579283807792  Val loss: 0.04905554539559223
Epoch 322
-------------------------------
Batch 1/64 loss: -0.10032135248184204
Batch 2/64 loss: -0.07243150472640991
Batch 3/64 loss: -0.11721915006637573
Batch 4/64 loss: -0.12208765745162964
Batch 5/64 loss: -0.13247424364089966
Batch 6/64 loss: -0.11635065078735352
Batch 7/64 loss: -0.12044006586074829
Batch 8/64 loss: -0.11076110601425171
Batch 9/64 loss: -0.09248101711273193
Batch 10/64 loss: -0.04972982406616211
Batch 11/64 loss: -0.09745252132415771
Batch 12/64 loss: -0.10426706075668335
Batch 13/64 loss: -0.10830473899841309
Batch 14/64 loss: -0.11582696437835693
Batch 15/64 loss: -0.09818339347839355
Batch 16/64 loss: -0.10059553384780884
Batch 17/64 loss: -0.09021615982055664
Batch 18/64 loss: -0.11954164505004883
Batch 19/64 loss: -0.10551315546035767
Batch 20/64 loss: -0.10510814189910889
Batch 21/64 loss: -0.09354305267333984
Batch 22/64 loss: -0.10934299230575562
Batch 23/64 loss: -0.08534383773803711
Batch 24/64 loss: -0.1180042028427124
Batch 25/64 loss: -0.08770912885665894
Batch 26/64 loss: -0.12876969575881958
Batch 27/64 loss: -0.08259057998657227
Batch 28/64 loss: -0.09608572721481323
Batch 29/64 loss: -0.10421919822692871
Batch 30/64 loss: -0.08143413066864014
Batch 31/64 loss: -0.0859784483909607
Batch 32/64 loss: -0.09328979253768921
Batch 33/64 loss: -0.11356371641159058
Batch 34/64 loss: -0.08532154560089111
Batch 35/64 loss: -0.12407153844833374
Batch 36/64 loss: -0.0914188027381897
Batch 37/64 loss: -0.10100442171096802
Batch 38/64 loss: -0.09390246868133545
Batch 39/64 loss: -0.10418826341629028
Batch 40/64 loss: -0.09573447704315186
Batch 41/64 loss: -0.10682928562164307
Batch 42/64 loss: -0.12294352054595947
Batch 43/64 loss: -0.10769718885421753
Batch 44/64 loss: -0.08675253391265869
Batch 45/64 loss: -0.10693603754043579
Batch 46/64 loss: -0.10447418689727783
Batch 47/64 loss: -0.10191327333450317
Batch 48/64 loss: -0.09148114919662476
Batch 49/64 loss: -0.10227775573730469
Batch 50/64 loss: -0.08146852254867554
Batch 51/64 loss: -0.09935963153839111
Batch 52/64 loss: -0.11945122480392456
Batch 53/64 loss: -0.1146584153175354
Batch 54/64 loss: -0.10193365812301636
Batch 55/64 loss: -0.12146985530853271
Batch 56/64 loss: -0.11117756366729736
Batch 57/64 loss: -0.08928316831588745
Batch 58/64 loss: -0.12302273511886597
Batch 59/64 loss: -0.1083444356918335
Batch 60/64 loss: -0.1267702579498291
Batch 61/64 loss: -0.0979452133178711
Batch 62/64 loss: -0.10108715295791626
Batch 63/64 loss: -0.11459934711456299
Batch 64/64 loss: -0.11875873804092407
Epoch 322  Train loss: -0.10330615020265767  Val loss: 0.05080279677184587
Epoch 323
-------------------------------
Batch 1/64 loss: -0.11575436592102051
Batch 2/64 loss: -0.11284369230270386
Batch 3/64 loss: -0.1351844072341919
Batch 4/64 loss: -0.10146385431289673
Batch 5/64 loss: -0.10653853416442871
Batch 6/64 loss: -0.10993403196334839
Batch 7/64 loss: -0.14143788814544678
Batch 8/64 loss: -0.10251176357269287
Batch 9/64 loss: -0.1008496880531311
Batch 10/64 loss: -0.10205602645874023
Batch 11/64 loss: -0.11581003665924072
Batch 12/64 loss: -0.11937344074249268
Batch 13/64 loss: -0.11514079570770264
Batch 14/64 loss: -0.12501001358032227
Batch 15/64 loss: -0.11077553033828735
Batch 16/64 loss: -0.0915759801864624
Batch 17/64 loss: -0.0940430760383606
Batch 18/64 loss: -0.11609888076782227
Batch 19/64 loss: -0.11293286085128784
Batch 20/64 loss: -0.0883941650390625
Batch 21/64 loss: -0.10876613855361938
Batch 22/64 loss: -0.0873945951461792
Batch 23/64 loss: -0.06994462013244629
Batch 24/64 loss: -0.10885238647460938
Batch 25/64 loss: -0.09361279010772705
Batch 26/64 loss: -0.10918664932250977
Batch 27/64 loss: -0.10363602638244629
Batch 28/64 loss: -0.10578399896621704
Batch 29/64 loss: -0.11811530590057373
Batch 30/64 loss: -0.11713826656341553
Batch 31/64 loss: -0.10016059875488281
Batch 32/64 loss: -0.09825444221496582
Batch 33/64 loss: -0.07778799533843994
Batch 34/64 loss: -0.09280943870544434
Batch 35/64 loss: -0.10549867153167725
Batch 36/64 loss: -0.10776937007904053
Batch 37/64 loss: -0.09311211109161377
Batch 38/64 loss: -0.10293442010879517
Batch 39/64 loss: -0.08361947536468506
Batch 40/64 loss: -0.08388692140579224
Batch 41/64 loss: -0.07967233657836914
Batch 42/64 loss: -0.10461288690567017
Batch 43/64 loss: -0.09638345241546631
Batch 44/64 loss: -0.09969222545623779
Batch 45/64 loss: -0.09411656856536865
Batch 46/64 loss: -0.10863697528839111
Batch 47/64 loss: -0.10335618257522583
Batch 48/64 loss: -0.09963399171829224
Batch 49/64 loss: -0.08491146564483643
Batch 50/64 loss: -0.11282777786254883
Batch 51/64 loss: -0.12293022871017456
Batch 52/64 loss: -0.0908617377281189
Batch 53/64 loss: -0.10720181465148926
Batch 54/64 loss: -0.11239928007125854
Batch 55/64 loss: -0.10730981826782227
Batch 56/64 loss: -0.12437844276428223
Batch 57/64 loss: -0.13487869501113892
Batch 58/64 loss: -0.06140100955963135
Batch 59/64 loss: -0.12238121032714844
Batch 60/64 loss: -0.1055402159690857
Batch 61/64 loss: -0.09561306238174438
Batch 62/64 loss: -0.08267861604690552
Batch 63/64 loss: -0.13582056760787964
Batch 64/64 loss: -0.13377714157104492
Epoch 323  Train loss: -0.10468336741129557  Val loss: 0.04677738111043714
Epoch 324
-------------------------------
Batch 1/64 loss: -0.10872727632522583
Batch 2/64 loss: -0.10572636127471924
Batch 3/64 loss: -0.10983103513717651
Batch 4/64 loss: -0.11738091707229614
Batch 5/64 loss: -0.11556363105773926
Batch 6/64 loss: -0.10152775049209595
Batch 7/64 loss: -0.12563151121139526
Batch 8/64 loss: -0.10959237813949585
Batch 9/64 loss: -0.10754168033599854
Batch 10/64 loss: -0.1194995641708374
Batch 11/64 loss: -0.10911107063293457
Batch 12/64 loss: -0.12116748094558716
Batch 13/64 loss: -0.1154555082321167
Batch 14/64 loss: -0.12166309356689453
Batch 15/64 loss: -0.10932838916778564
Batch 16/64 loss: -0.12883508205413818
Batch 17/64 loss: -0.10105288028717041
Batch 18/64 loss: -0.1074647307395935
Batch 19/64 loss: -0.08127892017364502
Batch 20/64 loss: -0.09759390354156494
Batch 21/64 loss: -0.12434864044189453
Batch 22/64 loss: -0.1183779239654541
Batch 23/64 loss: -0.11624282598495483
Batch 24/64 loss: -0.10761666297912598
Batch 25/64 loss: -0.08423739671707153
Batch 26/64 loss: -0.11539852619171143
Batch 27/64 loss: -0.11107337474822998
Batch 28/64 loss: -0.14152032136917114
Batch 29/64 loss: -0.09520065784454346
Batch 30/64 loss: -0.07498294115066528
Batch 31/64 loss: -0.09197407960891724
Batch 32/64 loss: -0.07436567544937134
Batch 33/64 loss: -0.09426748752593994
Batch 34/64 loss: -0.12245118618011475
Batch 35/64 loss: -0.09646004438400269
Batch 36/64 loss: -0.13122498989105225
Batch 37/64 loss: -0.11223387718200684
Batch 38/64 loss: -0.09990692138671875
Batch 39/64 loss: -0.07981705665588379
Batch 40/64 loss: -0.09013205766677856
Batch 41/64 loss: -0.1032257080078125
Batch 42/64 loss: -0.08305531740188599
Batch 43/64 loss: -0.09012734889984131
Batch 44/64 loss: -0.09342193603515625
Batch 45/64 loss: -0.11088359355926514
Batch 46/64 loss: -0.09127503633499146
Batch 47/64 loss: -0.10618740320205688
Batch 48/64 loss: -0.1115572452545166
Batch 49/64 loss: -0.11918830871582031
Batch 50/64 loss: -0.09647560119628906
Batch 51/64 loss: -0.13143235445022583
Batch 52/64 loss: -0.09041690826416016
Batch 53/64 loss: -0.10649001598358154
Batch 54/64 loss: -0.1317242980003357
Batch 55/64 loss: -0.13005328178405762
Batch 56/64 loss: -0.08581924438476562
Batch 57/64 loss: -0.12460297346115112
Batch 58/64 loss: -0.12383365631103516
Batch 59/64 loss: -0.1053471565246582
Batch 60/64 loss: -0.0810776948928833
Batch 61/64 loss: -0.10372287034988403
Batch 62/64 loss: -0.12162148952484131
Batch 63/64 loss: -0.07277917861938477
Batch 64/64 loss: -0.08716201782226562
Epoch 324  Train loss: -0.10628228935540891  Val loss: 0.04814496462287772
Epoch 325
-------------------------------
Batch 1/64 loss: -0.11359864473342896
Batch 2/64 loss: -0.10965621471405029
Batch 3/64 loss: -0.11944836378097534
Batch 4/64 loss: -0.09727680683135986
Batch 5/64 loss: -0.11323666572570801
Batch 6/64 loss: -0.10269391536712646
Batch 7/64 loss: -0.08458101749420166
Batch 8/64 loss: -0.1407373547554016
Batch 9/64 loss: -0.12249225378036499
Batch 10/64 loss: -0.10298025608062744
Batch 11/64 loss: -0.0707520842552185
Batch 12/64 loss: -0.12125921249389648
Batch 13/64 loss: -0.11855071783065796
Batch 14/64 loss: -0.09784984588623047
Batch 15/64 loss: -0.08852005004882812
Batch 16/64 loss: -0.11164069175720215
Batch 17/64 loss: -0.11381703615188599
Batch 18/64 loss: -0.11196905374526978
Batch 19/64 loss: -0.0970609188079834
Batch 20/64 loss: -0.13816148042678833
Batch 21/64 loss: -0.11475235223770142
Batch 22/64 loss: -0.11911606788635254
Batch 23/64 loss: -0.11705070734024048
Batch 24/64 loss: -0.10451090335845947
Batch 25/64 loss: -0.11062252521514893
Batch 26/64 loss: -0.10250413417816162
Batch 27/64 loss: -0.09665161371231079
Batch 28/64 loss: -0.10030001401901245
Batch 29/64 loss: -0.07428032159805298
Batch 30/64 loss: -0.1126333475112915
Batch 31/64 loss: -0.10676562786102295
Batch 32/64 loss: -0.1282721757888794
Batch 33/64 loss: -0.10774683952331543
Batch 34/64 loss: -0.12745189666748047
Batch 35/64 loss: -0.10956770181655884
Batch 36/64 loss: -0.09983289241790771
Batch 37/64 loss: -0.11116570234298706
Batch 38/64 loss: -0.12373912334442139
Batch 39/64 loss: -0.10011112689971924
Batch 40/64 loss: -0.08727824687957764
Batch 41/64 loss: -0.09063720703125
Batch 42/64 loss: -0.10863596200942993
Batch 43/64 loss: -0.107391357421875
Batch 44/64 loss: -0.11966049671173096
Batch 45/64 loss: -0.11814594268798828
Batch 46/64 loss: -0.12451136112213135
Batch 47/64 loss: -0.11233782768249512
Batch 48/64 loss: -0.1031273603439331
Batch 49/64 loss: -0.09944885969161987
Batch 50/64 loss: -0.133847177028656
Batch 51/64 loss: -0.08002215623855591
Batch 52/64 loss: -0.09492415189743042
Batch 53/64 loss: -0.10700565576553345
Batch 54/64 loss: -0.10057061910629272
Batch 55/64 loss: -0.09481275081634521
Batch 56/64 loss: -0.09773480892181396
Batch 57/64 loss: -0.11651229858398438
Batch 58/64 loss: -0.10837304592132568
Batch 59/64 loss: -0.07765549421310425
Batch 60/64 loss: -0.0660659670829773
Batch 61/64 loss: -0.12318873405456543
Batch 62/64 loss: -0.11896443367004395
Batch 63/64 loss: -0.11166650056838989
Batch 64/64 loss: -0.0887218713760376
Epoch 325  Train loss: -0.10686145062540092  Val loss: 0.05175685595810618
Epoch 326
-------------------------------
Batch 1/64 loss: -0.12247014045715332
Batch 2/64 loss: -0.11730355024337769
Batch 3/64 loss: -0.09379851818084717
Batch 4/64 loss: -0.1192278265953064
Batch 5/64 loss: -0.11514842510223389
Batch 6/64 loss: -0.08299148082733154
Batch 7/64 loss: -0.12879526615142822
Batch 8/64 loss: -0.0879899263381958
Batch 9/64 loss: -0.11393195390701294
Batch 10/64 loss: -0.10979694128036499
Batch 11/64 loss: -0.0942721962928772
Batch 12/64 loss: -0.11786651611328125
Batch 13/64 loss: -0.12727141380310059
Batch 14/64 loss: -0.09692412614822388
Batch 15/64 loss: -0.09941887855529785
Batch 16/64 loss: -0.11128246784210205
Batch 17/64 loss: -0.11701834201812744
Batch 18/64 loss: -0.10924017429351807
Batch 19/64 loss: -0.11675846576690674
Batch 20/64 loss: -0.11595791578292847
Batch 21/64 loss: -0.11666750907897949
Batch 22/64 loss: -0.10674679279327393
Batch 23/64 loss: -0.12706834077835083
Batch 24/64 loss: -0.10021448135375977
Batch 25/64 loss: -0.11383450031280518
Batch 26/64 loss: -0.11005806922912598
Batch 27/64 loss: -0.10911691188812256
Batch 28/64 loss: -0.10740488767623901
Batch 29/64 loss: -0.10128927230834961
Batch 30/64 loss: -0.1140751838684082
Batch 31/64 loss: -0.11224228143692017
Batch 32/64 loss: -0.07770323753356934
Batch 33/64 loss: -0.10622739791870117
Batch 34/64 loss: -0.10021352767944336
Batch 35/64 loss: -0.10901278257369995
Batch 36/64 loss: -0.1269431710243225
Batch 37/64 loss: -0.10530602931976318
Batch 38/64 loss: -0.09808409214019775
Batch 39/64 loss: -0.12229692935943604
Batch 40/64 loss: -0.07113367319107056
Batch 41/64 loss: -0.12275809049606323
Batch 42/64 loss: -0.11197185516357422
Batch 43/64 loss: -0.09848541021347046
Batch 44/64 loss: -0.11038553714752197
Batch 45/64 loss: -0.1145399808883667
Batch 46/64 loss: -0.10484904050827026
Batch 47/64 loss: -0.1321532130241394
Batch 48/64 loss: -0.11982935667037964
Batch 49/64 loss: -0.10157650709152222
Batch 50/64 loss: -0.11534678936004639
Batch 51/64 loss: -0.10837751626968384
Batch 52/64 loss: -0.1052049994468689
Batch 53/64 loss: -0.09246277809143066
Batch 54/64 loss: -0.11041003465652466
Batch 55/64 loss: -0.11615312099456787
Batch 56/64 loss: -0.11570584774017334
Batch 57/64 loss: -0.1082230806350708
Batch 58/64 loss: -0.07717418670654297
Batch 59/64 loss: -0.10357832908630371
Batch 60/64 loss: -0.10260283946990967
Batch 61/64 loss: -0.11109977960586548
Batch 62/64 loss: -0.11313915252685547
Batch 63/64 loss: -0.11540567874908447
Batch 64/64 loss: -0.12556225061416626
Epoch 326  Train loss: -0.1088424848575218  Val loss: 0.04568475259538369
Epoch 327
-------------------------------
Batch 1/64 loss: -0.14220255613327026
Batch 2/64 loss: -0.12112271785736084
Batch 3/64 loss: -0.1410003900527954
Batch 4/64 loss: -0.10121816396713257
Batch 5/64 loss: -0.12305116653442383
Batch 6/64 loss: -0.059417009353637695
Batch 7/64 loss: -0.11921244859695435
Batch 8/64 loss: -0.11660093069076538
Batch 9/64 loss: -0.13480043411254883
Batch 10/64 loss: -0.10579043626785278
Batch 11/64 loss: -0.14579486846923828
Batch 12/64 loss: -0.1119680404663086
Batch 13/64 loss: -0.09457981586456299
Batch 14/64 loss: -0.11926031112670898
Batch 15/64 loss: -0.12842291593551636
Batch 16/64 loss: -0.10416442155838013
Batch 17/64 loss: -0.13064420223236084
Batch 18/64 loss: -0.10306662321090698
Batch 19/64 loss: -0.12630939483642578
Batch 20/64 loss: -0.11859416961669922
Batch 21/64 loss: -0.07015323638916016
Batch 22/64 loss: -0.1330125331878662
Batch 23/64 loss: -0.09696334600448608
Batch 24/64 loss: -0.12230914831161499
Batch 25/64 loss: -0.10107201337814331
Batch 26/64 loss: -0.10538458824157715
Batch 27/64 loss: -0.10911548137664795
Batch 28/64 loss: -0.11103284358978271
Batch 29/64 loss: -0.10057330131530762
Batch 30/64 loss: -0.13994234800338745
Batch 31/64 loss: -0.12508147954940796
Batch 32/64 loss: -0.09023451805114746
Batch 33/64 loss: -0.10541427135467529
Batch 34/64 loss: -0.08920484781265259
Batch 35/64 loss: -0.14166516065597534
Batch 36/64 loss: -0.07791060209274292
Batch 37/64 loss: -0.10217124223709106
Batch 38/64 loss: -0.1103752851486206
Batch 39/64 loss: -0.10876208543777466
Batch 40/64 loss: -0.11509436368942261
Batch 41/64 loss: -0.09816157817840576
Batch 42/64 loss: -0.0969463586807251
Batch 43/64 loss: -0.10090744495391846
Batch 44/64 loss: -0.08090990781784058
Batch 45/64 loss: -0.08720439672470093
Batch 46/64 loss: -0.06907474994659424
Batch 47/64 loss: -0.11151885986328125
Batch 48/64 loss: -0.09186995029449463
Batch 49/64 loss: -0.11841237545013428
Batch 50/64 loss: -0.10974621772766113
Batch 51/64 loss: -0.10763424634933472
Batch 52/64 loss: -0.12651658058166504
Batch 53/64 loss: -0.1361369490623474
Batch 54/64 loss: -0.09965825080871582
Batch 55/64 loss: -0.10785728693008423
Batch 56/64 loss: -0.10184597969055176
Batch 57/64 loss: -0.07029235363006592
Batch 58/64 loss: -0.08731585741043091
Batch 59/64 loss: -0.106498122215271
Batch 60/64 loss: -0.10612982511520386
Batch 61/64 loss: -0.07923418283462524
Batch 62/64 loss: -0.12904709577560425
Batch 63/64 loss: -0.11652755737304688
Batch 64/64 loss: -0.11777836084365845
Epoch 327  Train loss: -0.10871331154131422  Val loss: 0.04689281502949823
Epoch 328
-------------------------------
Batch 1/64 loss: -0.07754969596862793
Batch 2/64 loss: -0.08804744482040405
Batch 3/64 loss: -0.09486335515975952
Batch 4/64 loss: -0.13453739881515503
Batch 5/64 loss: -0.09974890947341919
Batch 6/64 loss: -0.12728595733642578
Batch 7/64 loss: -0.10878539085388184
Batch 8/64 loss: -0.09014678001403809
Batch 9/64 loss: -0.08405560255050659
Batch 10/64 loss: -0.13238024711608887
Batch 11/64 loss: -0.11700624227523804
Batch 12/64 loss: -0.09769248962402344
Batch 13/64 loss: -0.10278558731079102
Batch 14/64 loss: -0.08840453624725342
Batch 15/64 loss: -0.12188535928726196
Batch 16/64 loss: -0.11358368396759033
Batch 17/64 loss: -0.10327452421188354
Batch 18/64 loss: -0.10673993825912476
Batch 19/64 loss: -0.11576128005981445
Batch 20/64 loss: -0.09885430335998535
Batch 21/64 loss: -0.0921773910522461
Batch 22/64 loss: -0.12499773502349854
Batch 23/64 loss: -0.1302759051322937
Batch 24/64 loss: -0.09111320972442627
Batch 25/64 loss: -0.11660796403884888
Batch 26/64 loss: -0.112143874168396
Batch 27/64 loss: -0.09292429685592651
Batch 28/64 loss: -0.13665056228637695
Batch 29/64 loss: -0.11754953861236572
Batch 30/64 loss: -0.11812567710876465
Batch 31/64 loss: -0.10935425758361816
Batch 32/64 loss: -0.1215772032737732
Batch 33/64 loss: -0.11727237701416016
Batch 34/64 loss: -0.10545408725738525
Batch 35/64 loss: -0.11777818202972412
Batch 36/64 loss: -0.1175273060798645
Batch 37/64 loss: -0.13350141048431396
Batch 38/64 loss: -0.11950194835662842
Batch 39/64 loss: -0.10394871234893799
Batch 40/64 loss: -0.11432600021362305
Batch 41/64 loss: -0.12314057350158691
Batch 42/64 loss: -0.10051476955413818
Batch 43/64 loss: -0.09168428182601929
Batch 44/64 loss: -0.11227357387542725
Batch 45/64 loss: -0.08180701732635498
Batch 46/64 loss: -0.07592463493347168
Batch 47/64 loss: -0.10159933567047119
Batch 48/64 loss: -0.09955745935440063
Batch 49/64 loss: -0.10663735866546631
Batch 50/64 loss: -0.09718513488769531
Batch 51/64 loss: -0.09923291206359863
Batch 52/64 loss: -0.10413533449172974
Batch 53/64 loss: -0.0743410587310791
Batch 54/64 loss: -0.11869668960571289
Batch 55/64 loss: -0.10578042268753052
Batch 56/64 loss: -0.09639567136764526
Batch 57/64 loss: -0.11087340116500854
Batch 58/64 loss: -0.1326085329055786
Batch 59/64 loss: -0.11111581325531006
Batch 60/64 loss: -0.1285465955734253
Batch 61/64 loss: -0.10398566722869873
Batch 62/64 loss: -0.10758280754089355
Batch 63/64 loss: -0.1280314326286316
Batch 64/64 loss: -0.0723409652709961
Epoch 328  Train loss: -0.10764082459842457  Val loss: 0.047316837761410324
Epoch 329
-------------------------------
Batch 1/64 loss: -0.11531984806060791
Batch 2/64 loss: -0.10595345497131348
Batch 3/64 loss: -0.0761750340461731
Batch 4/64 loss: -0.12062698602676392
Batch 5/64 loss: -0.1091088056564331
Batch 6/64 loss: -0.14610624313354492
Batch 7/64 loss: -0.10968923568725586
Batch 8/64 loss: -0.10912621021270752
Batch 9/64 loss: -0.10834181308746338
Batch 10/64 loss: -0.08571809530258179
Batch 11/64 loss: -0.08729475736618042
Batch 12/64 loss: -0.11073637008666992
Batch 13/64 loss: -0.12262231111526489
Batch 14/64 loss: -0.11167675256729126
Batch 15/64 loss: -0.08172667026519775
Batch 16/64 loss: -0.11303073167800903
Batch 17/64 loss: -0.11785173416137695
Batch 18/64 loss: -0.12441694736480713
Batch 19/64 loss: -0.13362133502960205
Batch 20/64 loss: -0.09197378158569336
Batch 21/64 loss: -0.09995055198669434
Batch 22/64 loss: -0.09764403104782104
Batch 23/64 loss: -0.1279306411743164
Batch 24/64 loss: -0.09649479389190674
Batch 25/64 loss: -0.13184738159179688
Batch 26/64 loss: -0.11139672994613647
Batch 27/64 loss: -0.09676128625869751
Batch 28/64 loss: -0.1176115870475769
Batch 29/64 loss: -0.08107709884643555
Batch 30/64 loss: -0.09733402729034424
Batch 31/64 loss: -0.10645437240600586
Batch 32/64 loss: -0.12375754117965698
Batch 33/64 loss: -0.10333883762359619
Batch 34/64 loss: -0.10547763109207153
Batch 35/64 loss: -0.11547058820724487
Batch 36/64 loss: -0.11887317895889282
Batch 37/64 loss: -0.11975425481796265
Batch 38/64 loss: -0.12073683738708496
Batch 39/64 loss: -0.09790724515914917
Batch 40/64 loss: -0.1259903907775879
Batch 41/64 loss: -0.1024324893951416
Batch 42/64 loss: -0.11428231000900269
Batch 43/64 loss: -0.12061697244644165
Batch 44/64 loss: -0.10452079772949219
Batch 45/64 loss: -0.11677753925323486
Batch 46/64 loss: -0.08210790157318115
Batch 47/64 loss: -0.12041586637496948
Batch 48/64 loss: -0.11122184991836548
Batch 49/64 loss: -0.10391569137573242
Batch 50/64 loss: -0.10627973079681396
Batch 51/64 loss: -0.10951119661331177
Batch 52/64 loss: -0.1206367015838623
Batch 53/64 loss: -0.12620341777801514
Batch 54/64 loss: -0.10690295696258545
Batch 55/64 loss: -0.10362285375595093
Batch 56/64 loss: -0.12128913402557373
Batch 57/64 loss: -0.11392712593078613
Batch 58/64 loss: -0.07598066329956055
Batch 59/64 loss: -0.10563439130783081
Batch 60/64 loss: -0.10453563928604126
Batch 61/64 loss: -0.08596569299697876
Batch 62/64 loss: -0.12825149297714233
Batch 63/64 loss: -0.10190969705581665
Batch 64/64 loss: -0.10149890184402466
Epoch 329  Train loss: -0.10886262608509438  Val loss: 0.04648114664038432
Epoch 330
-------------------------------
Batch 1/64 loss: -0.11739993095397949
Batch 2/64 loss: -0.10226762294769287
Batch 3/64 loss: -0.12142395973205566
Batch 4/64 loss: -0.13233381509780884
Batch 5/64 loss: -0.1278313398361206
Batch 6/64 loss: -0.1273205280303955
Batch 7/64 loss: -0.112155020236969
Batch 8/64 loss: -0.1213991641998291
Batch 9/64 loss: -0.11802554130554199
Batch 10/64 loss: -0.10581827163696289
Batch 11/64 loss: -0.11261296272277832
Batch 12/64 loss: -0.08880102634429932
Batch 13/64 loss: -0.12582850456237793
Batch 14/64 loss: -0.12250769138336182
Batch 15/64 loss: -0.11076545715332031
Batch 16/64 loss: -0.11521267890930176
Batch 17/64 loss: -0.11089092493057251
Batch 18/64 loss: -0.11281263828277588
Batch 19/64 loss: -0.13076424598693848
Batch 20/64 loss: -0.07624047994613647
Batch 21/64 loss: -0.10825276374816895
Batch 22/64 loss: -0.09885299205780029
Batch 23/64 loss: -0.0976020097732544
Batch 24/64 loss: -0.10452771186828613
Batch 25/64 loss: -0.09276515245437622
Batch 26/64 loss: -0.10701876878738403
Batch 27/64 loss: -0.08698654174804688
Batch 28/64 loss: -0.13127565383911133
Batch 29/64 loss: -0.12097084522247314
Batch 30/64 loss: -0.07187318801879883
Batch 31/64 loss: -0.12641125917434692
Batch 32/64 loss: -0.10714912414550781
Batch 33/64 loss: -0.11136925220489502
Batch 34/64 loss: -0.12628912925720215
Batch 35/64 loss: -0.1165359616279602
Batch 36/64 loss: -0.13887596130371094
Batch 37/64 loss: -0.08216029405593872
Batch 38/64 loss: -0.11295020580291748
Batch 39/64 loss: -0.10806399583816528
Batch 40/64 loss: -0.09453356266021729
Batch 41/64 loss: -0.10430788993835449
Batch 42/64 loss: -0.11627376079559326
Batch 43/64 loss: -0.09824526309967041
Batch 44/64 loss: -0.10813099145889282
Batch 45/64 loss: -0.10900664329528809
Batch 46/64 loss: -0.11032015085220337
Batch 47/64 loss: -0.09788352251052856
Batch 48/64 loss: -0.06790280342102051
Batch 49/64 loss: -0.08419013023376465
Batch 50/64 loss: -0.09441256523132324
Batch 51/64 loss: -0.09860366582870483
Batch 52/64 loss: -0.09395259618759155
Batch 53/64 loss: -0.11109894514083862
Batch 54/64 loss: -0.10230237245559692
Batch 55/64 loss: -0.07833993434906006
Batch 56/64 loss: -0.10636049509048462
Batch 57/64 loss: -0.1021851897239685
Batch 58/64 loss: -0.11429870128631592
Batch 59/64 loss: -0.11849546432495117
Batch 60/64 loss: -0.08913850784301758
Batch 61/64 loss: -0.11302274465560913
Batch 62/64 loss: -0.11209249496459961
Batch 63/64 loss: -0.10819852352142334
Batch 64/64 loss: -0.1093108057975769
Epoch 330  Train loss: -0.1075705511897218  Val loss: 0.04764847882424843
Epoch 331
-------------------------------
Batch 1/64 loss: -0.12349492311477661
Batch 2/64 loss: -0.1397002935409546
Batch 3/64 loss: -0.09638720750808716
Batch 4/64 loss: -0.11677581071853638
Batch 5/64 loss: -0.08950483798980713
Batch 6/64 loss: -0.10742604732513428
Batch 7/64 loss: -0.08473145961761475
Batch 8/64 loss: -0.1407221555709839
Batch 9/64 loss: -0.09236973524093628
Batch 10/64 loss: -0.08955299854278564
Batch 11/64 loss: -0.12737315893173218
Batch 12/64 loss: -0.13407361507415771
Batch 13/64 loss: -0.11282235383987427
Batch 14/64 loss: -0.13450109958648682
Batch 15/64 loss: -0.12379062175750732
Batch 16/64 loss: -0.1293836236000061
Batch 17/64 loss: -0.09061145782470703
Batch 18/64 loss: -0.09080660343170166
Batch 19/64 loss: -0.12172877788543701
Batch 20/64 loss: -0.10962522029876709
Batch 21/64 loss: -0.1303756833076477
Batch 22/64 loss: -0.13120156526565552
Batch 23/64 loss: -0.10783571004867554
Batch 24/64 loss: -0.11218339204788208
Batch 25/64 loss: -0.09474533796310425
Batch 26/64 loss: -0.0675497055053711
Batch 27/64 loss: -0.09058970212936401
Batch 28/64 loss: -0.12612247467041016
Batch 29/64 loss: -0.10510730743408203
Batch 30/64 loss: -0.11054706573486328
Batch 31/64 loss: -0.09258776903152466
Batch 32/64 loss: -0.1158742904663086
Batch 33/64 loss: -0.1272118091583252
Batch 34/64 loss: -0.10723954439163208
Batch 35/64 loss: -0.12720322608947754
Batch 36/64 loss: -0.10532575845718384
Batch 37/64 loss: -0.10809481143951416
Batch 38/64 loss: -0.1169060468673706
Batch 39/64 loss: -0.07869654893875122
Batch 40/64 loss: -0.11606574058532715
Batch 41/64 loss: -0.07589417695999146
Batch 42/64 loss: -0.11784923076629639
Batch 43/64 loss: -0.08971071243286133
Batch 44/64 loss: -0.1087140440940857
Batch 45/64 loss: -0.1184842586517334
Batch 46/64 loss: -0.1155121922492981
Batch 47/64 loss: -0.11574822664260864
Batch 48/64 loss: -0.08782857656478882
Batch 49/64 loss: -0.12053263187408447
Batch 50/64 loss: -0.0657300353050232
Batch 51/64 loss: -0.11047226190567017
Batch 52/64 loss: -0.08590191602706909
Batch 53/64 loss: -0.12067782878875732
Batch 54/64 loss: -0.11438262462615967
Batch 55/64 loss: -0.1071246862411499
Batch 56/64 loss: -0.117409348487854
Batch 57/64 loss: -0.08764106035232544
Batch 58/64 loss: -0.10673773288726807
Batch 59/64 loss: -0.10001140832901001
Batch 60/64 loss: -0.1048574447631836
Batch 61/64 loss: -0.10578393936157227
Batch 62/64 loss: -0.11110103130340576
Batch 63/64 loss: -0.08309221267700195
Batch 64/64 loss: -0.10802721977233887
Epoch 331  Train loss: -0.10790687822828106  Val loss: 0.048628582577525135
Epoch 332
-------------------------------
Batch 1/64 loss: -0.10459393262863159
Batch 2/64 loss: -0.11800998449325562
Batch 3/64 loss: -0.08371490240097046
Batch 4/64 loss: -0.12643790245056152
Batch 5/64 loss: -0.12468487024307251
Batch 6/64 loss: -0.08587515354156494
Batch 7/64 loss: -0.12112122774124146
Batch 8/64 loss: -0.13787120580673218
Batch 9/64 loss: -0.12605154514312744
Batch 10/64 loss: -0.12040472030639648
Batch 11/64 loss: -0.09421807527542114
Batch 12/64 loss: -0.11758577823638916
Batch 13/64 loss: -0.09718203544616699
Batch 14/64 loss: -0.11842644214630127
Batch 15/64 loss: -0.13178545236587524
Batch 16/64 loss: -0.11203813552856445
Batch 17/64 loss: -0.11680185794830322
Batch 18/64 loss: -0.10916882753372192
Batch 19/64 loss: -0.11426222324371338
Batch 20/64 loss: -0.11678469181060791
Batch 21/64 loss: -0.10158395767211914
Batch 22/64 loss: -0.1106676459312439
Batch 23/64 loss: -0.0981016755104065
Batch 24/64 loss: -0.1254866123199463
Batch 25/64 loss: -0.09830260276794434
Batch 26/64 loss: -0.07043612003326416
Batch 27/64 loss: -0.10203796625137329
Batch 28/64 loss: -0.09446382522583008
Batch 29/64 loss: -0.09316903352737427
Batch 30/64 loss: -0.11227428913116455
Batch 31/64 loss: -0.1133463978767395
Batch 32/64 loss: -0.12325918674468994
Batch 33/64 loss: -0.09305715560913086
Batch 34/64 loss: -0.09356772899627686
Batch 35/64 loss: -0.1093021035194397
Batch 36/64 loss: -0.13291674852371216
Batch 37/64 loss: -0.07444900274276733
Batch 38/64 loss: -0.11639636754989624
Batch 39/64 loss: -0.12286245822906494
Batch 40/64 loss: -0.10662192106246948
Batch 41/64 loss: -0.11840659379959106
Batch 42/64 loss: -0.11887180805206299
Batch 43/64 loss: -0.1082831621170044
Batch 44/64 loss: -0.13231432437896729
Batch 45/64 loss: -0.11481916904449463
Batch 46/64 loss: -0.11979663372039795
Batch 47/64 loss: -0.11536121368408203
Batch 48/64 loss: -0.11563509702682495
Batch 49/64 loss: -0.09214341640472412
Batch 50/64 loss: -0.09288132190704346
Batch 51/64 loss: -0.1053054928779602
Batch 52/64 loss: -0.10281985998153687
Batch 53/64 loss: -0.10353946685791016
Batch 54/64 loss: -0.13055098056793213
Batch 55/64 loss: -0.10574668645858765
Batch 56/64 loss: -0.0963369607925415
Batch 57/64 loss: -0.10174024105072021
Batch 58/64 loss: -0.11481982469558716
Batch 59/64 loss: -0.11362135410308838
Batch 60/64 loss: -0.12977170944213867
Batch 61/64 loss: -0.08899760246276855
Batch 62/64 loss: -0.09998631477355957
Batch 63/64 loss: -0.12410032749176025
Batch 64/64 loss: -0.14440035820007324
Epoch 332  Train loss: -0.11017210343304802  Val loss: 0.04917883135608791
Epoch 333
-------------------------------
Batch 1/64 loss: -0.10122853517532349
Batch 2/64 loss: -0.08122020959854126
Batch 3/64 loss: -0.1283394694328308
Batch 4/64 loss: -0.10876679420471191
Batch 5/64 loss: -0.1290000081062317
Batch 6/64 loss: -0.11385113000869751
Batch 7/64 loss: -0.1006932258605957
Batch 8/64 loss: -0.13363611698150635
Batch 9/64 loss: -0.09077370166778564
Batch 10/64 loss: -0.1291225552558899
Batch 11/64 loss: -0.10092705488204956
Batch 12/64 loss: -0.10270190238952637
Batch 13/64 loss: -0.1239004135131836
Batch 14/64 loss: -0.10954463481903076
Batch 15/64 loss: -0.11722159385681152
Batch 16/64 loss: -0.1195104718208313
Batch 17/64 loss: -0.10509079694747925
Batch 18/64 loss: -0.11133438348770142
Batch 19/64 loss: -0.11501681804656982
Batch 20/64 loss: -0.10681933164596558
Batch 21/64 loss: -0.08468252420425415
Batch 22/64 loss: -0.09149229526519775
Batch 23/64 loss: -0.07915693521499634
Batch 24/64 loss: -0.08499777317047119
Batch 25/64 loss: -0.09548622369766235
Batch 26/64 loss: -0.11566400527954102
Batch 27/64 loss: -0.09241759777069092
Batch 28/64 loss: -0.11958754062652588
Batch 29/64 loss: -0.11704760789871216
Batch 30/64 loss: -0.12128973007202148
Batch 31/64 loss: -0.07857972383499146
Batch 32/64 loss: -0.09350854158401489
Batch 33/64 loss: -0.13143271207809448
Batch 34/64 loss: -0.12854504585266113
Batch 35/64 loss: -0.11875128746032715
Batch 36/64 loss: -0.11123466491699219
Batch 37/64 loss: -0.12814784049987793
Batch 38/64 loss: -0.10988223552703857
Batch 39/64 loss: -0.14167040586471558
Batch 40/64 loss: -0.11990392208099365
Batch 41/64 loss: -0.11008858680725098
Batch 42/64 loss: -0.09330427646636963
Batch 43/64 loss: -0.10512560606002808
Batch 44/64 loss: -0.11282628774642944
Batch 45/64 loss: -0.09845048189163208
Batch 46/64 loss: -0.11067426204681396
Batch 47/64 loss: -0.10461604595184326
Batch 48/64 loss: -0.10503339767456055
Batch 49/64 loss: -0.10449779033660889
Batch 50/64 loss: -0.12898117303848267
Batch 51/64 loss: -0.10569608211517334
Batch 52/64 loss: -0.10169863700866699
Batch 53/64 loss: -0.1371861696243286
Batch 54/64 loss: -0.09768575429916382
Batch 55/64 loss: -0.09769314527511597
Batch 56/64 loss: -0.10954010486602783
Batch 57/64 loss: -0.1305241584777832
Batch 58/64 loss: -0.11977177858352661
Batch 59/64 loss: -0.08746707439422607
Batch 60/64 loss: -0.0986628532409668
Batch 61/64 loss: -0.09224432706832886
Batch 62/64 loss: -0.09155422449111938
Batch 63/64 loss: -0.1080898642539978
Batch 64/64 loss: -0.11428308486938477
Epoch 333  Train loss: -0.10869493577994552  Val loss: 0.04863381549664789
Epoch 334
-------------------------------
Batch 1/64 loss: -0.12128114700317383
Batch 2/64 loss: -0.11717003583908081
Batch 3/64 loss: -0.1392357349395752
Batch 4/64 loss: -0.09359520673751831
Batch 5/64 loss: -0.10525977611541748
Batch 6/64 loss: -0.0817367434501648
Batch 7/64 loss: -0.1138162612915039
Batch 8/64 loss: -0.11577165126800537
Batch 9/64 loss: -0.09456181526184082
Batch 10/64 loss: -0.08650475740432739
Batch 11/64 loss: -0.11919009685516357
Batch 12/64 loss: -0.12897443771362305
Batch 13/64 loss: -0.10649603605270386
Batch 14/64 loss: -0.1395360231399536
Batch 15/64 loss: -0.11098188161849976
Batch 16/64 loss: -0.12632495164871216
Batch 17/64 loss: -0.12353861331939697
Batch 18/64 loss: -0.09682399034500122
Batch 19/64 loss: -0.12221676111221313
Batch 20/64 loss: -0.09803706407546997
Batch 21/64 loss: -0.08763861656188965
Batch 22/64 loss: -0.10964423418045044
Batch 23/64 loss: -0.10373711585998535
Batch 24/64 loss: -0.11931866407394409
Batch 25/64 loss: -0.08917021751403809
Batch 26/64 loss: -0.10148698091506958
Batch 27/64 loss: -0.0996350646018982
Batch 28/64 loss: -0.10823798179626465
Batch 29/64 loss: -0.09577757120132446
Batch 30/64 loss: -0.11065083742141724
Batch 31/64 loss: -0.11418735980987549
Batch 32/64 loss: -0.12695777416229248
Batch 33/64 loss: -0.11790883541107178
Batch 34/64 loss: -0.11522752046585083
Batch 35/64 loss: -0.09107989072799683
Batch 36/64 loss: -0.11896449327468872
Batch 37/64 loss: -0.09278535842895508
Batch 38/64 loss: -0.13238608837127686
Batch 39/64 loss: -0.09451460838317871
Batch 40/64 loss: -0.12513655424118042
Batch 41/64 loss: -0.10072588920593262
Batch 42/64 loss: -0.08993113040924072
Batch 43/64 loss: -0.1334788203239441
Batch 44/64 loss: -0.11446082592010498
Batch 45/64 loss: -0.11066687107086182
Batch 46/64 loss: -0.11249029636383057
Batch 47/64 loss: -0.11763203144073486
Batch 48/64 loss: -0.143474280834198
Batch 49/64 loss: -0.13364297151565552
Batch 50/64 loss: -0.08195292949676514
Batch 51/64 loss: -0.1135023832321167
Batch 52/64 loss: -0.10311514139175415
Batch 53/64 loss: -0.11918497085571289
Batch 54/64 loss: -0.09901809692382812
Batch 55/64 loss: -0.08205962181091309
Batch 56/64 loss: -0.11288779973983765
Batch 57/64 loss: -0.1130821704864502
Batch 58/64 loss: -0.08813583850860596
Batch 59/64 loss: -0.1025688648223877
Batch 60/64 loss: -0.10221540927886963
Batch 61/64 loss: -0.09214723110198975
Batch 62/64 loss: -0.10951536893844604
Batch 63/64 loss: -0.11092996597290039
Batch 64/64 loss: -0.11658906936645508
Epoch 334  Train loss: -0.10932956022374771  Val loss: 0.05004930291388862
Epoch 335
-------------------------------
Batch 1/64 loss: -0.10252118110656738
Batch 2/64 loss: -0.14463716745376587
Batch 3/64 loss: -0.13761848211288452
Batch 4/64 loss: -0.13449203968048096
Batch 5/64 loss: -0.09591978788375854
Batch 6/64 loss: -0.1180676817893982
Batch 7/64 loss: -0.10786455869674683
Batch 8/64 loss: -0.09440755844116211
Batch 9/64 loss: -0.11874198913574219
Batch 10/64 loss: -0.11093997955322266
Batch 11/64 loss: -0.12003171443939209
Batch 12/64 loss: -0.10911232233047485
Batch 13/64 loss: -0.12058055400848389
Batch 14/64 loss: -0.10800182819366455
Batch 15/64 loss: -0.11824589967727661
Batch 16/64 loss: -0.10582977533340454
Batch 17/64 loss: -0.09419840574264526
Batch 18/64 loss: -0.11719775199890137
Batch 19/64 loss: -0.09881913661956787
Batch 20/64 loss: -0.06964302062988281
Batch 21/64 loss: -0.12441086769104004
Batch 22/64 loss: -0.11171412467956543
Batch 23/64 loss: -0.10289359092712402
Batch 24/64 loss: -0.07460999488830566
Batch 25/64 loss: -0.1448308825492859
Batch 26/64 loss: -0.11471229791641235
Batch 27/64 loss: -0.11306458711624146
Batch 28/64 loss: -0.10397440195083618
Batch 29/64 loss: -0.11587131023406982
Batch 30/64 loss: -0.1044851541519165
Batch 31/64 loss: -0.13718968629837036
Batch 32/64 loss: -0.11783170700073242
Batch 33/64 loss: -0.09541863203048706
Batch 34/64 loss: -0.12407875061035156
Batch 35/64 loss: -0.07221716642379761
Batch 36/64 loss: -0.09397530555725098
Batch 37/64 loss: -0.11115378141403198
Batch 38/64 loss: -0.10162538290023804
Batch 39/64 loss: -0.10687541961669922
Batch 40/64 loss: -0.13919776678085327
Batch 41/64 loss: -0.11623191833496094
Batch 42/64 loss: -0.12377184629440308
Batch 43/64 loss: -0.09121716022491455
Batch 44/64 loss: -0.10894036293029785
Batch 45/64 loss: -0.11324137449264526
Batch 46/64 loss: -0.08836662769317627
Batch 47/64 loss: -0.1172947883605957
Batch 48/64 loss: -0.13131862878799438
Batch 49/64 loss: -0.10267692804336548
Batch 50/64 loss: -0.08851969242095947
Batch 51/64 loss: -0.10291898250579834
Batch 52/64 loss: -0.10939610004425049
Batch 53/64 loss: -0.10596972703933716
Batch 54/64 loss: -0.11875766515731812
Batch 55/64 loss: -0.09345203638076782
Batch 56/64 loss: -0.11226940155029297
Batch 57/64 loss: -0.12874746322631836
Batch 58/64 loss: -0.11227905750274658
Batch 59/64 loss: -0.10872805118560791
Batch 60/64 loss: -0.09408712387084961
Batch 61/64 loss: -0.11960041522979736
Batch 62/64 loss: -0.1332300305366516
Batch 63/64 loss: -0.11601066589355469
Batch 64/64 loss: -0.09496951103210449
Epoch 335  Train loss: -0.1105138012007171  Val loss: 0.04965248591301777
Epoch 336
-------------------------------
Batch 1/64 loss: -0.10688930749893188
Batch 2/64 loss: -0.13304901123046875
Batch 3/64 loss: -0.1196897029876709
Batch 4/64 loss: -0.13127392530441284
Batch 5/64 loss: -0.1103067398071289
Batch 6/64 loss: -0.1229926347732544
Batch 7/64 loss: -0.10637474060058594
Batch 8/64 loss: -0.12669312953948975
Batch 9/64 loss: -0.11104243993759155
Batch 10/64 loss: -0.12531012296676636
Batch 11/64 loss: -0.13409942388534546
Batch 12/64 loss: -0.11986517906188965
Batch 13/64 loss: -0.1007264256477356
Batch 14/64 loss: -0.11158919334411621
Batch 15/64 loss: -0.11069995164871216
Batch 16/64 loss: -0.10487979650497437
Batch 17/64 loss: -0.1032785177230835
Batch 18/64 loss: -0.10145920515060425
Batch 19/64 loss: -0.11009663343429565
Batch 20/64 loss: -0.12497687339782715
Batch 21/64 loss: -0.11102008819580078
Batch 22/64 loss: -0.0619695782661438
Batch 23/64 loss: -0.12020760774612427
Batch 24/64 loss: -0.09773939847946167
Batch 25/64 loss: -0.11418181657791138
Batch 26/64 loss: -0.11501270532608032
Batch 27/64 loss: -0.09958523511886597
Batch 28/64 loss: -0.08014941215515137
Batch 29/64 loss: -0.09409976005554199
Batch 30/64 loss: -0.12989693880081177
Batch 31/64 loss: -0.09092795848846436
Batch 32/64 loss: -0.11593830585479736
Batch 33/64 loss: -0.11763083934783936
Batch 34/64 loss: -0.12418591976165771
Batch 35/64 loss: -0.11978089809417725
Batch 36/64 loss: -0.10993814468383789
Batch 37/64 loss: -0.13443922996520996
Batch 38/64 loss: -0.10790276527404785
Batch 39/64 loss: -0.10816764831542969
Batch 40/64 loss: -0.11722356081008911
Batch 41/64 loss: -0.12222069501876831
Batch 42/64 loss: -0.12914502620697021
Batch 43/64 loss: -0.1151171326637268
Batch 44/64 loss: -0.12127840518951416
Batch 45/64 loss: -0.09455502033233643
Batch 46/64 loss: -0.12051308155059814
Batch 47/64 loss: -0.09048426151275635
Batch 48/64 loss: -0.09437030553817749
Batch 49/64 loss: -0.08619248867034912
Batch 50/64 loss: -0.11414062976837158
Batch 51/64 loss: -0.12144964933395386
Batch 52/64 loss: -0.11287093162536621
Batch 53/64 loss: -0.08690965175628662
Batch 54/64 loss: -0.09511446952819824
Batch 55/64 loss: -0.10559743642807007
Batch 56/64 loss: -0.109347403049469
Batch 57/64 loss: -0.12419140338897705
Batch 58/64 loss: -0.09208923578262329
Batch 59/64 loss: -0.11463004350662231
Batch 60/64 loss: -0.11261880397796631
Batch 61/64 loss: -0.12096208333969116
Batch 62/64 loss: -0.08980560302734375
Batch 63/64 loss: -0.09739285707473755
Batch 64/64 loss: -0.10690635442733765
Epoch 336  Train loss: -0.11037595529182284  Val loss: 0.0505571473914733
Epoch 337
-------------------------------
Batch 1/64 loss: -0.10442668199539185
Batch 2/64 loss: -0.11794430017471313
Batch 3/64 loss: -0.12293106317520142
Batch 4/64 loss: -0.11385232210159302
Batch 5/64 loss: -0.1105087399482727
Batch 6/64 loss: -0.08076179027557373
Batch 7/64 loss: -0.13175153732299805
Batch 8/64 loss: -0.09445232152938843
Batch 9/64 loss: -0.10708993673324585
Batch 10/64 loss: -0.11924964189529419
Batch 11/64 loss: -0.13074153661727905
Batch 12/64 loss: -0.12497073411941528
Batch 13/64 loss: -0.10128724575042725
Batch 14/64 loss: -0.11138468980789185
Batch 15/64 loss: -0.11837577819824219
Batch 16/64 loss: -0.10882657766342163
Batch 17/64 loss: -0.06433773040771484
Batch 18/64 loss: -0.11175024509429932
Batch 19/64 loss: -0.124703049659729
Batch 20/64 loss: -0.10896450281143188
Batch 21/64 loss: -0.11690783500671387
Batch 22/64 loss: -0.0718618631362915
Batch 23/64 loss: -0.09575897455215454
Batch 24/64 loss: -0.09051758050918579
Batch 25/64 loss: -0.10958665609359741
Batch 26/64 loss: -0.1043897271156311
Batch 27/64 loss: -0.11088836193084717
Batch 28/64 loss: -0.10941815376281738
Batch 29/64 loss: -0.13866055011749268
Batch 30/64 loss: -0.06280499696731567
Batch 31/64 loss: -0.11812639236450195
Batch 32/64 loss: -0.0853954553604126
Batch 33/64 loss: -0.09081488847732544
Batch 34/64 loss: -0.12572836875915527
Batch 35/64 loss: -0.11852681636810303
Batch 36/64 loss: -0.11660581827163696
Batch 37/64 loss: -0.11059331893920898
Batch 38/64 loss: -0.0978958010673523
Batch 39/64 loss: -0.14097696542739868
Batch 40/64 loss: -0.10777777433395386
Batch 41/64 loss: -0.11524581909179688
Batch 42/64 loss: -0.09506267309188843
Batch 43/64 loss: -0.1103743314743042
Batch 44/64 loss: -0.11421018838882446
Batch 45/64 loss: -0.08988118171691895
Batch 46/64 loss: -0.10934072732925415
Batch 47/64 loss: -0.09132611751556396
Batch 48/64 loss: -0.13287848234176636
Batch 49/64 loss: -0.0945214033126831
Batch 50/64 loss: -0.1117180585861206
Batch 51/64 loss: -0.08990561962127686
Batch 52/64 loss: -0.10099345445632935
Batch 53/64 loss: -0.1168677806854248
Batch 54/64 loss: -0.11851763725280762
Batch 55/64 loss: -0.1266559362411499
Batch 56/64 loss: -0.10917091369628906
Batch 57/64 loss: -0.11043250560760498
Batch 58/64 loss: -0.10388565063476562
Batch 59/64 loss: -0.0994994044303894
Batch 60/64 loss: -0.10029363632202148
Batch 61/64 loss: -0.10353779792785645
Batch 62/64 loss: -0.10336488485336304
Batch 63/64 loss: -0.13328087329864502
Batch 64/64 loss: -0.10956239700317383
Epoch 337  Train loss: -0.10815189959956151  Val loss: 0.05092048645019531
Epoch 338
-------------------------------
Batch 1/64 loss: -0.11162614822387695
Batch 2/64 loss: -0.1343299150466919
Batch 3/64 loss: -0.1465817093849182
Batch 4/64 loss: -0.13379991054534912
Batch 5/64 loss: -0.13880491256713867
Batch 6/64 loss: -0.11467862129211426
Batch 7/64 loss: -0.13707351684570312
Batch 8/64 loss: -0.12360954284667969
Batch 9/64 loss: -0.11262267827987671
Batch 10/64 loss: -0.12494319677352905
Batch 11/64 loss: -0.12433505058288574
Batch 12/64 loss: -0.12147045135498047
Batch 13/64 loss: -0.14274924993515015
Batch 14/64 loss: -0.08379936218261719
Batch 15/64 loss: -0.1125417947769165
Batch 16/64 loss: -0.12158936262130737
Batch 17/64 loss: -0.11673855781555176
Batch 18/64 loss: -0.10618740320205688
Batch 19/64 loss: -0.12293988466262817
Batch 20/64 loss: -0.12064677476882935
Batch 21/64 loss: -0.11034530401229858
Batch 22/64 loss: -0.12227380275726318
Batch 23/64 loss: -0.0973360538482666
Batch 24/64 loss: -0.09120321273803711
Batch 25/64 loss: -0.1397320032119751
Batch 26/64 loss: -0.11902350187301636
Batch 27/64 loss: -0.10472732782363892
Batch 28/64 loss: -0.10435187816619873
Batch 29/64 loss: -0.13304096460342407
Batch 30/64 loss: -0.11276042461395264
Batch 31/64 loss: -0.10785168409347534
Batch 32/64 loss: -0.11213529109954834
Batch 33/64 loss: -0.11476558446884155
Batch 34/64 loss: -0.12646406888961792
Batch 35/64 loss: -0.09225219488143921
Batch 36/64 loss: -0.098846435546875
Batch 37/64 loss: -0.08206194639205933
Batch 38/64 loss: -0.10192245244979858
Batch 39/64 loss: -0.09524929523468018
Batch 40/64 loss: -0.11416411399841309
Batch 41/64 loss: -0.12724429368972778
Batch 42/64 loss: -0.07809853553771973
Batch 43/64 loss: -0.10523796081542969
Batch 44/64 loss: -0.13463187217712402
Batch 45/64 loss: -0.08906328678131104
Batch 46/64 loss: -0.09339243173599243
Batch 47/64 loss: -0.09285080432891846
Batch 48/64 loss: -0.08669990301132202
Batch 49/64 loss: -0.10647130012512207
Batch 50/64 loss: -0.08149230480194092
Batch 51/64 loss: -0.09361326694488525
Batch 52/64 loss: -0.10864806175231934
Batch 53/64 loss: -0.09911566972732544
Batch 54/64 loss: -0.08034282922744751
Batch 55/64 loss: -0.11059439182281494
Batch 56/64 loss: -0.11833447217941284
Batch 57/64 loss: -0.11101078987121582
Batch 58/64 loss: -0.11801183223724365
Batch 59/64 loss: -0.1156567931175232
Batch 60/64 loss: -0.1260852813720703
Batch 61/64 loss: -0.11010968685150146
Batch 62/64 loss: -0.0998152494430542
Batch 63/64 loss: -0.12639033794403076
Batch 64/64 loss: -0.1311187744140625
Epoch 338  Train loss: -0.112044329736747  Val loss: 0.049177749869749716
Epoch 339
-------------------------------
Batch 1/64 loss: -0.11515963077545166
Batch 2/64 loss: -0.11948764324188232
Batch 3/64 loss: -0.11385113000869751
Batch 4/64 loss: -0.11786758899688721
Batch 5/64 loss: -0.15549415349960327
Batch 6/64 loss: -0.1225743293762207
Batch 7/64 loss: -0.1244438886642456
Batch 8/64 loss: -0.08152663707733154
Batch 9/64 loss: -0.11706185340881348
Batch 10/64 loss: -0.1158064603805542
Batch 11/64 loss: -0.14096909761428833
Batch 12/64 loss: -0.13211357593536377
Batch 13/64 loss: -0.11652940511703491
Batch 14/64 loss: -0.13002270460128784
Batch 15/64 loss: -0.0707242488861084
Batch 16/64 loss: -0.11789774894714355
Batch 17/64 loss: -0.0942223072052002
Batch 18/64 loss: -0.12063866853713989
Batch 19/64 loss: -0.13794249296188354
Batch 20/64 loss: -0.10982853174209595
Batch 21/64 loss: -0.10129088163375854
Batch 22/64 loss: -0.11145085096359253
Batch 23/64 loss: -0.13404595851898193
Batch 24/64 loss: -0.08201050758361816
Batch 25/64 loss: -0.09561091661453247
Batch 26/64 loss: -0.0959787368774414
Batch 27/64 loss: -0.08993041515350342
Batch 28/64 loss: -0.11771893501281738
Batch 29/64 loss: -0.08330667018890381
Batch 30/64 loss: -0.09703695774078369
Batch 31/64 loss: -0.12365579605102539
Batch 32/64 loss: -0.11574721336364746
Batch 33/64 loss: -0.11286032199859619
Batch 34/64 loss: -0.1004912257194519
Batch 35/64 loss: -0.11607295274734497
Batch 36/64 loss: -0.10704100131988525
Batch 37/64 loss: -0.0795716643333435
Batch 38/64 loss: -0.0990821123123169
Batch 39/64 loss: -0.08447957038879395
Batch 40/64 loss: -0.09367197751998901
Batch 41/64 loss: -0.09403979778289795
Batch 42/64 loss: -0.09363752603530884
Batch 43/64 loss: -0.09372937679290771
Batch 44/64 loss: -0.12109071016311646
Batch 45/64 loss: -0.11948668956756592
Batch 46/64 loss: -0.12424015998840332
Batch 47/64 loss: -0.09209060668945312
Batch 48/64 loss: -0.133794903755188
Batch 49/64 loss: -0.10859823226928711
Batch 50/64 loss: -0.10848629474639893
Batch 51/64 loss: -0.11344635486602783
Batch 52/64 loss: -0.08348143100738525
Batch 53/64 loss: -0.11218559741973877
Batch 54/64 loss: -0.11619776487350464
Batch 55/64 loss: -0.10787701606750488
Batch 56/64 loss: -0.12225037813186646
Batch 57/64 loss: -0.11469274759292603
Batch 58/64 loss: -0.1032412052154541
Batch 59/64 loss: -0.11969423294067383
Batch 60/64 loss: -0.10943108797073364
Batch 61/64 loss: -0.11097627878189087
Batch 62/64 loss: -0.10561227798461914
Batch 63/64 loss: -0.11780542135238647
Batch 64/64 loss: -0.10274076461791992
Epoch 339  Train loss: -0.10977817142710966  Val loss: 0.04890473274021214
Epoch 340
-------------------------------
Batch 1/64 loss: -0.12482577562332153
Batch 2/64 loss: -0.12461614608764648
Batch 3/64 loss: -0.133070707321167
Batch 4/64 loss: -0.08145368099212646
Batch 5/64 loss: -0.11010175943374634
Batch 6/64 loss: -0.13067865371704102
Batch 7/64 loss: -0.11277669668197632
Batch 8/64 loss: -0.13080036640167236
Batch 9/64 loss: -0.11368322372436523
Batch 10/64 loss: -0.09106278419494629
Batch 11/64 loss: -0.12306928634643555
Batch 12/64 loss: -0.10502368211746216
Batch 13/64 loss: -0.10996794700622559
Batch 14/64 loss: -0.10231882333755493
Batch 15/64 loss: -0.10547661781311035
Batch 16/64 loss: -0.09498107433319092
Batch 17/64 loss: -0.11561363935470581
Batch 18/64 loss: -0.10147625207901001
Batch 19/64 loss: -0.1303013563156128
Batch 20/64 loss: -0.10250020027160645
Batch 21/64 loss: -0.11127972602844238
Batch 22/64 loss: -0.12897872924804688
Batch 23/64 loss: -0.11711156368255615
Batch 24/64 loss: -0.10517823696136475
Batch 25/64 loss: -0.054669201374053955
Batch 26/64 loss: -0.1122056245803833
Batch 27/64 loss: -0.1340312957763672
Batch 28/64 loss: -0.11224520206451416
Batch 29/64 loss: -0.12333559989929199
Batch 30/64 loss: -0.11885738372802734
Batch 31/64 loss: -0.12253451347351074
Batch 32/64 loss: -0.11514735221862793
Batch 33/64 loss: -0.1178208589553833
Batch 34/64 loss: -0.13377737998962402
Batch 35/64 loss: -0.13782745599746704
Batch 36/64 loss: -0.12222492694854736
Batch 37/64 loss: -0.11324459314346313
Batch 38/64 loss: -0.11675375699996948
Batch 39/64 loss: -0.1230173110961914
Batch 40/64 loss: -0.12722289562225342
Batch 41/64 loss: -0.12381058931350708
Batch 42/64 loss: -0.08951199054718018
Batch 43/64 loss: -0.10716354846954346
Batch 44/64 loss: -0.11240142583847046
Batch 45/64 loss: -0.13210630416870117
Batch 46/64 loss: -0.10729986429214478
Batch 47/64 loss: -0.10905492305755615
Batch 48/64 loss: -0.09515076875686646
Batch 49/64 loss: -0.1239672303199768
Batch 50/64 loss: -0.1200520396232605
Batch 51/64 loss: -0.08175766468048096
Batch 52/64 loss: -0.1114773154258728
Batch 53/64 loss: -0.08078271150588989
Batch 54/64 loss: -0.11512786149978638
Batch 55/64 loss: -0.10722970962524414
Batch 56/64 loss: -0.12858474254608154
Batch 57/64 loss: -0.10899525880813599
Batch 58/64 loss: -0.09827959537506104
Batch 59/64 loss: -0.1242983341217041
Batch 60/64 loss: -0.12162894010543823
Batch 61/64 loss: -0.09267902374267578
Batch 62/64 loss: -0.07867145538330078
Batch 63/64 loss: -0.09211468696594238
Batch 64/64 loss: -0.11507165431976318
Epoch 340  Train loss: -0.11199548244476318  Val loss: 0.04955244064331055
Epoch 341
-------------------------------
Batch 1/64 loss: -0.13309478759765625
Batch 2/64 loss: -0.10341238975524902
Batch 3/64 loss: -0.13232147693634033
Batch 4/64 loss: -0.1050826907157898
Batch 5/64 loss: -0.10584324598312378
Batch 6/64 loss: -0.12711679935455322
Batch 7/64 loss: -0.11983102560043335
Batch 8/64 loss: -0.10815536975860596
Batch 9/64 loss: -0.10859805345535278
Batch 10/64 loss: -0.14045512676239014
Batch 11/64 loss: -0.10998457670211792
Batch 12/64 loss: -0.11487257480621338
Batch 13/64 loss: -0.10511362552642822
Batch 14/64 loss: -0.09769892692565918
Batch 15/64 loss: -0.11140918731689453
Batch 16/64 loss: -0.10599607229232788
Batch 17/64 loss: -0.12004059553146362
Batch 18/64 loss: -0.11408454179763794
Batch 19/64 loss: -0.09929412603378296
Batch 20/64 loss: -0.13505709171295166
Batch 21/64 loss: -0.1122201681137085
Batch 22/64 loss: -0.1172446608543396
Batch 23/64 loss: -0.1381809115409851
Batch 24/64 loss: -0.08954477310180664
Batch 25/64 loss: -0.08919364213943481
Batch 26/64 loss: -0.10048335790634155
Batch 27/64 loss: -0.10197079181671143
Batch 28/64 loss: -0.11970841884613037
Batch 29/64 loss: -0.08802711963653564
Batch 30/64 loss: -0.11962324380874634
Batch 31/64 loss: -0.09486985206604004
Batch 32/64 loss: -0.09588128328323364
Batch 33/64 loss: -0.1295914649963379
Batch 34/64 loss: -0.13007521629333496
Batch 35/64 loss: -0.12691855430603027
Batch 36/64 loss: -0.10624420642852783
Batch 37/64 loss: -0.11099547147750854
Batch 38/64 loss: -0.14169460535049438
Batch 39/64 loss: -0.1196669340133667
Batch 40/64 loss: -0.09160161018371582
Batch 41/64 loss: -0.11585408449172974
Batch 42/64 loss: -0.12811291217803955
Batch 43/64 loss: -0.10420578718185425
Batch 44/64 loss: -0.11165255308151245
Batch 45/64 loss: -0.12142503261566162
Batch 46/64 loss: -0.10252135992050171
Batch 47/64 loss: -0.10657495260238647
Batch 48/64 loss: -0.12326782941818237
Batch 49/64 loss: -0.10026109218597412
Batch 50/64 loss: -0.11996698379516602
Batch 51/64 loss: -0.08768045902252197
Batch 52/64 loss: -0.11028778553009033
Batch 53/64 loss: -0.1167914867401123
Batch 54/64 loss: -0.12607204914093018
Batch 55/64 loss: -0.1209905743598938
Batch 56/64 loss: -0.12733209133148193
Batch 57/64 loss: -0.0698697566986084
Batch 58/64 loss: -0.09110462665557861
Batch 59/64 loss: -0.09478545188903809
Batch 60/64 loss: -0.10089516639709473
Batch 61/64 loss: -0.11987960338592529
Batch 62/64 loss: -0.11237961053848267
Batch 63/64 loss: -0.10144555568695068
Batch 64/64 loss: -0.09558135271072388
Epoch 341  Train loss: -0.11147045317818137  Val loss: 0.05058229112952845
Epoch 342
-------------------------------
Batch 1/64 loss: -0.11661213636398315
Batch 2/64 loss: -0.1429358720779419
Batch 3/64 loss: -0.13605248928070068
Batch 4/64 loss: -0.09515339136123657
Batch 5/64 loss: -0.11833512783050537
Batch 6/64 loss: -0.07123023271560669
Batch 7/64 loss: -0.11546331644058228
Batch 8/64 loss: -0.08135300874710083
Batch 9/64 loss: -0.12096786499023438
Batch 10/64 loss: -0.09000998735427856
Batch 11/64 loss: -0.11215931177139282
Batch 12/64 loss: -0.104519784450531
Batch 13/64 loss: -0.10022670030593872
Batch 14/64 loss: -0.11001551151275635
Batch 15/64 loss: -0.13196945190429688
Batch 16/64 loss: -0.11930513381958008
Batch 17/64 loss: -0.12714874744415283
Batch 18/64 loss: -0.12351584434509277
Batch 19/64 loss: -0.11581093072891235
Batch 20/64 loss: -0.11196243762969971
Batch 21/64 loss: -0.10894691944122314
Batch 22/64 loss: -0.1308574080467224
Batch 23/64 loss: -0.09293663501739502
Batch 24/64 loss: -0.10595965385437012
Batch 25/64 loss: -0.11674100160598755
Batch 26/64 loss: -0.08089935779571533
Batch 27/64 loss: -0.11237770318984985
Batch 28/64 loss: -0.11079013347625732
Batch 29/64 loss: -0.11533957719802856
Batch 30/64 loss: -0.12453770637512207
Batch 31/64 loss: -0.10916733741760254
Batch 32/64 loss: -0.13046866655349731
Batch 33/64 loss: -0.13260823488235474
Batch 34/64 loss: -0.09445744752883911
Batch 35/64 loss: -0.11947900056838989
Batch 36/64 loss: -0.1136351227760315
Batch 37/64 loss: -0.11964458227157593
Batch 38/64 loss: -0.13581734895706177
Batch 39/64 loss: -0.11556988954544067
Batch 40/64 loss: -0.11055374145507812
Batch 41/64 loss: -0.1003846526145935
Batch 42/64 loss: -0.11536657810211182
Batch 43/64 loss: -0.12877684831619263
Batch 44/64 loss: -0.12145805358886719
Batch 45/64 loss: -0.1163482666015625
Batch 46/64 loss: -0.117148756980896
Batch 47/64 loss: -0.08746445178985596
Batch 48/64 loss: -0.0973924994468689
Batch 49/64 loss: -0.11371678113937378
Batch 50/64 loss: -0.12073194980621338
Batch 51/64 loss: -0.12132704257965088
Batch 52/64 loss: -0.11315047740936279
Batch 53/64 loss: -0.11806446313858032
Batch 54/64 loss: -0.1071130633354187
Batch 55/64 loss: -0.11973994970321655
Batch 56/64 loss: -0.10840922594070435
Batch 57/64 loss: -0.1097099781036377
Batch 58/64 loss: -0.10187101364135742
Batch 59/64 loss: -0.12133169174194336
Batch 60/64 loss: -0.10546094179153442
Batch 61/64 loss: -0.09267860651016235
Batch 62/64 loss: -0.10945099592208862
Batch 63/64 loss: -0.09939444065093994
Batch 64/64 loss: -0.12057006359100342
Epoch 342  Train loss: -0.11235220432281494  Val loss: 0.04840303778238723
Epoch 343
-------------------------------
Batch 1/64 loss: -0.10821127891540527
Batch 2/64 loss: -0.11930155754089355
Batch 3/64 loss: -0.1459723711013794
Batch 4/64 loss: -0.08924579620361328
Batch 5/64 loss: -0.12018853425979614
Batch 6/64 loss: -0.1215202808380127
Batch 7/64 loss: -0.13699215650558472
Batch 8/64 loss: -0.1203116774559021
Batch 9/64 loss: -0.100300133228302
Batch 10/64 loss: -0.12030988931655884
Batch 11/64 loss: -0.07735294103622437
Batch 12/64 loss: -0.11897391080856323
Batch 13/64 loss: -0.13965117931365967
Batch 14/64 loss: -0.1300615668296814
Batch 15/64 loss: -0.07423412799835205
Batch 16/64 loss: -0.11860859394073486
Batch 17/64 loss: -0.1195838451385498
Batch 18/64 loss: -0.10896754264831543
Batch 19/64 loss: -0.1244538426399231
Batch 20/64 loss: -0.10418838262557983
Batch 21/64 loss: -0.12468057870864868
Batch 22/64 loss: -0.12006711959838867
Batch 23/64 loss: -0.11203628778457642
Batch 24/64 loss: -0.13962459564208984
Batch 25/64 loss: -0.1266438364982605
Batch 26/64 loss: -0.11261743307113647
Batch 27/64 loss: -0.13869315385818481
Batch 28/64 loss: -0.10623037815093994
Batch 29/64 loss: -0.12567490339279175
Batch 30/64 loss: -0.11979097127914429
Batch 31/64 loss: -0.10611200332641602
Batch 32/64 loss: -0.12200486660003662
Batch 33/64 loss: -0.11207765340805054
Batch 34/64 loss: -0.11345034837722778
Batch 35/64 loss: -0.10998421907424927
Batch 36/64 loss: -0.11612474918365479
Batch 37/64 loss: -0.10861128568649292
Batch 38/64 loss: -0.10622894763946533
Batch 39/64 loss: -0.0849236249923706
Batch 40/64 loss: -0.11109709739685059
Batch 41/64 loss: -0.10663348436355591
Batch 42/64 loss: -0.11589431762695312
Batch 43/64 loss: -0.08488816022872925
Batch 44/64 loss: -0.11005949974060059
Batch 45/64 loss: -0.12082540988922119
Batch 46/64 loss: -0.10509705543518066
Batch 47/64 loss: -0.11202758550643921
Batch 48/64 loss: -0.10887938737869263
Batch 49/64 loss: -0.11122411489486694
Batch 50/64 loss: -0.1165773868560791
Batch 51/64 loss: -0.08170706033706665
Batch 52/64 loss: -0.13211166858673096
Batch 53/64 loss: -0.1118084192276001
Batch 54/64 loss: -0.13948333263397217
Batch 55/64 loss: -0.14235317707061768
Batch 56/64 loss: -0.12429553270339966
Batch 57/64 loss: -0.09309971332550049
Batch 58/64 loss: -0.11666178703308105
Batch 59/64 loss: -0.09934937953948975
Batch 60/64 loss: -0.09377044439315796
Batch 61/64 loss: -0.11220544576644897
Batch 62/64 loss: -0.12634766101837158
Batch 63/64 loss: -0.13296663761138916
Batch 64/64 loss: -0.11270445585250854
Epoch 343  Train loss: -0.11447684180502798  Val loss: 0.04689973346965829
Epoch 344
-------------------------------
Batch 1/64 loss: -0.13149648904800415
Batch 2/64 loss: -0.1289198398590088
Batch 3/64 loss: -0.12311756610870361
Batch 4/64 loss: -0.10798966884613037
Batch 5/64 loss: -0.13761824369430542
Batch 6/64 loss: -0.1150699257850647
Batch 7/64 loss: -0.12810879945755005
Batch 8/64 loss: -0.1290341019630432
Batch 9/64 loss: -0.10512816905975342
Batch 10/64 loss: -0.11441892385482788
Batch 11/64 loss: -0.06882047653198242
Batch 12/64 loss: -0.11533588171005249
Batch 13/64 loss: -0.09686100482940674
Batch 14/64 loss: -0.12710988521575928
Batch 15/64 loss: -0.1093263030052185
Batch 16/64 loss: -0.10968172550201416
Batch 17/64 loss: -0.1179344654083252
Batch 18/64 loss: -0.12665963172912598
Batch 19/64 loss: -0.11641687154769897
Batch 20/64 loss: -0.12604117393493652
Batch 21/64 loss: -0.12646329402923584
Batch 22/64 loss: -0.12124365568161011
Batch 23/64 loss: -0.12370812892913818
Batch 24/64 loss: -0.09075307846069336
Batch 25/64 loss: -0.10079139471054077
Batch 26/64 loss: -0.10854697227478027
Batch 27/64 loss: -0.11539798974990845
Batch 28/64 loss: -0.11289310455322266
Batch 29/64 loss: -0.12287294864654541
Batch 30/64 loss: -0.1481717824935913
Batch 31/64 loss: -0.12309825420379639
Batch 32/64 loss: -0.10806632041931152
Batch 33/64 loss: -0.16004300117492676
Batch 34/64 loss: -0.13238906860351562
Batch 35/64 loss: -0.11564505100250244
Batch 36/64 loss: -0.11978268623352051
Batch 37/64 loss: -0.12214130163192749
Batch 38/64 loss: -0.1053435206413269
Batch 39/64 loss: -0.13318359851837158
Batch 40/64 loss: -0.14046448469161987
Batch 41/64 loss: -0.11687058210372925
Batch 42/64 loss: -0.1008104681968689
Batch 43/64 loss: -0.12538766860961914
Batch 44/64 loss: -0.1054689884185791
Batch 45/64 loss: -0.1427972912788391
Batch 46/64 loss: -0.09290051460266113
Batch 47/64 loss: -0.12873148918151855
Batch 48/64 loss: -0.10606652498245239
Batch 49/64 loss: -0.10234218835830688
Batch 50/64 loss: -0.13811171054840088
Batch 51/64 loss: -0.08990037441253662
Batch 52/64 loss: -0.0819903016090393
Batch 53/64 loss: -0.0917360782623291
Batch 54/64 loss: -0.10335612297058105
Batch 55/64 loss: -0.09288322925567627
Batch 56/64 loss: -0.10620397329330444
Batch 57/64 loss: -0.08305555582046509
Batch 58/64 loss: -0.1216321587562561
Batch 59/64 loss: -0.12265336513519287
Batch 60/64 loss: -0.07119977474212646
Batch 61/64 loss: -0.08533978462219238
Batch 62/64 loss: -0.10873156785964966
Batch 63/64 loss: -0.07177627086639404
Batch 64/64 loss: -0.1370946764945984
Epoch 344  Train loss: -0.11386440431370455  Val loss: 0.05276732981409814
Epoch 345
-------------------------------
Batch 1/64 loss: -0.11247658729553223
Batch 2/64 loss: -0.129602313041687
Batch 3/64 loss: -0.12107241153717041
Batch 4/64 loss: -0.12190032005310059
Batch 5/64 loss: -0.11580932140350342
Batch 6/64 loss: -0.1309577226638794
Batch 7/64 loss: -0.10453498363494873
Batch 8/64 loss: -0.12324273586273193
Batch 9/64 loss: -0.09004205465316772
Batch 10/64 loss: -0.1293940544128418
Batch 11/64 loss: -0.13341057300567627
Batch 12/64 loss: -0.12624025344848633
Batch 13/64 loss: -0.11867427825927734
Batch 14/64 loss: -0.11349177360534668
Batch 15/64 loss: -0.11515688896179199
Batch 16/64 loss: -0.14043331146240234
Batch 17/64 loss: -0.11701595783233643
Batch 18/64 loss: -0.10386961698532104
Batch 19/64 loss: -0.12921977043151855
Batch 20/64 loss: -0.09927928447723389
Batch 21/64 loss: -0.11728471517562866
Batch 22/64 loss: -0.09471738338470459
Batch 23/64 loss: -0.10517436265945435
Batch 24/64 loss: -0.10881978273391724
Batch 25/64 loss: -0.11903774738311768
Batch 26/64 loss: -0.1207432746887207
Batch 27/64 loss: -0.10392671823501587
Batch 28/64 loss: -0.12745928764343262
Batch 29/64 loss: -0.11501443386077881
Batch 30/64 loss: -0.10627132654190063
Batch 31/64 loss: -0.10214120149612427
Batch 32/64 loss: -0.11397838592529297
Batch 33/64 loss: -0.12738335132598877
Batch 34/64 loss: -0.10343962907791138
Batch 35/64 loss: -0.12733089923858643
Batch 36/64 loss: -0.10401630401611328
Batch 37/64 loss: -0.12825334072113037
Batch 38/64 loss: -0.1304914355278015
Batch 39/64 loss: -0.07894909381866455
Batch 40/64 loss: -0.11330211162567139
Batch 41/64 loss: -0.14102375507354736
Batch 42/64 loss: -0.08319944143295288
Batch 43/64 loss: -0.11090528964996338
Batch 44/64 loss: -0.10926806926727295
Batch 45/64 loss: -0.08843624591827393
Batch 46/64 loss: -0.10744297504425049
Batch 47/64 loss: -0.10646545886993408
Batch 48/64 loss: -0.12729895114898682
Batch 49/64 loss: -0.13340026140213013
Batch 50/64 loss: -0.09482765197753906
Batch 51/64 loss: -0.09663951396942139
Batch 52/64 loss: -0.10852116346359253
Batch 53/64 loss: -0.1105685830116272
Batch 54/64 loss: -0.110043466091156
Batch 55/64 loss: -0.13981413841247559
Batch 56/64 loss: -0.13246136903762817
Batch 57/64 loss: -0.12927883863449097
Batch 58/64 loss: -0.07641887664794922
Batch 59/64 loss: -0.0892135500907898
Batch 60/64 loss: -0.10733038187026978
Batch 61/64 loss: -0.1252543330192566
Batch 62/64 loss: -0.08038759231567383
Batch 63/64 loss: -0.1054428219795227
Batch 64/64 loss: -0.09564512968063354
Epoch 345  Train loss: -0.11308134233250337  Val loss: 0.046222176543625766
Epoch 346
-------------------------------
Batch 1/64 loss: -0.10886639356613159
Batch 2/64 loss: -0.11842495203018188
Batch 3/64 loss: -0.1216166615486145
Batch 4/64 loss: -0.1177712082862854
Batch 5/64 loss: -0.07612520456314087
Batch 6/64 loss: -0.10514092445373535
Batch 7/64 loss: -0.08472716808319092
Batch 8/64 loss: -0.11949306726455688
Batch 9/64 loss: -0.09854060411453247
Batch 10/64 loss: -0.09958958625793457
Batch 11/64 loss: -0.10889631509780884
Batch 12/64 loss: -0.11415064334869385
Batch 13/64 loss: -0.09855294227600098
Batch 14/64 loss: -0.13125312328338623
Batch 15/64 loss: -0.1094546914100647
Batch 16/64 loss: -0.12285727262496948
Batch 17/64 loss: -0.12785303592681885
Batch 18/64 loss: -0.15441083908081055
Batch 19/64 loss: -0.0788503885269165
Batch 20/64 loss: -0.13458269834518433
Batch 21/64 loss: -0.10341429710388184
Batch 22/64 loss: -0.12420028448104858
Batch 23/64 loss: -0.13031309843063354
Batch 24/64 loss: -0.11145460605621338
Batch 25/64 loss: -0.13781553506851196
Batch 26/64 loss: -0.10107511281967163
Batch 27/64 loss: -0.1329125165939331
Batch 28/64 loss: -0.07804417610168457
Batch 29/64 loss: -0.09269064664840698
Batch 30/64 loss: -0.09804272651672363
Batch 31/64 loss: -0.12565553188323975
Batch 32/64 loss: -0.09850561618804932
Batch 33/64 loss: -0.10136681795120239
Batch 34/64 loss: -0.10932177305221558
Batch 35/64 loss: -0.12197154760360718
Batch 36/64 loss: -0.10925573110580444
Batch 37/64 loss: -0.09246277809143066
Batch 38/64 loss: -0.12411946058273315
Batch 39/64 loss: -0.10128706693649292
Batch 40/64 loss: -0.11675024032592773
Batch 41/64 loss: -0.13915598392486572
Batch 42/64 loss: -0.09852755069732666
Batch 43/64 loss: -0.10437202453613281
Batch 44/64 loss: -0.1208983063697815
Batch 45/64 loss: -0.12189394235610962
Batch 46/64 loss: -0.12229442596435547
Batch 47/64 loss: -0.124961256980896
Batch 48/64 loss: -0.09809756278991699
Batch 49/64 loss: -0.12364429235458374
Batch 50/64 loss: -0.11963558197021484
Batch 51/64 loss: -0.1227673888206482
Batch 52/64 loss: -0.1409851312637329
Batch 53/64 loss: -0.11698931455612183
Batch 54/64 loss: -0.08678549528121948
Batch 55/64 loss: -0.09615570306777954
Batch 56/64 loss: -0.10440939664840698
Batch 57/64 loss: -0.11905932426452637
Batch 58/64 loss: -0.11981749534606934
Batch 59/64 loss: -0.13786619901657104
Batch 60/64 loss: -0.12982577085494995
Batch 61/64 loss: -0.13671565055847168
Batch 62/64 loss: -0.09945225715637207
Batch 63/64 loss: -0.13023293018341064
Batch 64/64 loss: -0.10251986980438232
Epoch 346  Train loss: -0.113462747779547  Val loss: 0.047905678191955146
Epoch 347
-------------------------------
Batch 1/64 loss: -0.11348432302474976
Batch 2/64 loss: -0.12762564420700073
Batch 3/64 loss: -0.1235855221748352
Batch 4/64 loss: -0.09784209728240967
Batch 5/64 loss: -0.10267996788024902
Batch 6/64 loss: -0.13295239210128784
Batch 7/64 loss: -0.11417979001998901
Batch 8/64 loss: -0.09512144327163696
Batch 9/64 loss: -0.12053263187408447
Batch 10/64 loss: -0.09703028202056885
Batch 11/64 loss: -0.11504244804382324
Batch 12/64 loss: -0.13660651445388794
Batch 13/64 loss: -0.1338089108467102
Batch 14/64 loss: -0.1148717999458313
Batch 15/64 loss: -0.11840015649795532
Batch 16/64 loss: -0.1116132140159607
Batch 17/64 loss: -0.1281225085258484
Batch 18/64 loss: -0.11476022005081177
Batch 19/64 loss: -0.09782320261001587
Batch 20/64 loss: -0.11169326305389404
Batch 21/64 loss: -0.1300995945930481
Batch 22/64 loss: -0.13978445529937744
Batch 23/64 loss: -0.11928713321685791
Batch 24/64 loss: -0.13639241456985474
Batch 25/64 loss: -0.11387163400650024
Batch 26/64 loss: -0.1604522466659546
Batch 27/64 loss: -0.1284152865409851
Batch 28/64 loss: -0.10069054365158081
Batch 29/64 loss: -0.12473148107528687
Batch 30/64 loss: -0.11731463670730591
Batch 31/64 loss: -0.1327441930770874
Batch 32/64 loss: -0.1275659203529358
Batch 33/64 loss: -0.1022026538848877
Batch 34/64 loss: -0.12273526191711426
Batch 35/64 loss: -0.1245468258857727
Batch 36/64 loss: -0.11948412656784058
Batch 37/64 loss: -0.1217947006225586
Batch 38/64 loss: -0.1072378158569336
Batch 39/64 loss: -0.09386181831359863
Batch 40/64 loss: -0.14696156978607178
Batch 41/64 loss: -0.1149139404296875
Batch 42/64 loss: -0.11629778146743774
Batch 43/64 loss: -0.129175066947937
Batch 44/64 loss: -0.10959088802337646
Batch 45/64 loss: -0.10810261964797974
Batch 46/64 loss: -0.14461266994476318
Batch 47/64 loss: -0.12354665994644165
Batch 48/64 loss: -0.08503210544586182
Batch 49/64 loss: -0.08313721418380737
Batch 50/64 loss: -0.11812293529510498
Batch 51/64 loss: -0.11237996816635132
Batch 52/64 loss: -0.0971531867980957
Batch 53/64 loss: -0.10479259490966797
Batch 54/64 loss: -0.13337796926498413
Batch 55/64 loss: -0.10338795185089111
Batch 56/64 loss: -0.12429767847061157
Batch 57/64 loss: -0.12878793478012085
Batch 58/64 loss: -0.09811562299728394
Batch 59/64 loss: -0.09293031692504883
Batch 60/64 loss: -0.14266568422317505
Batch 61/64 loss: -0.09905260801315308
Batch 62/64 loss: -0.09649401903152466
Batch 63/64 loss: -0.1233251690864563
Batch 64/64 loss: -0.11163520812988281
Epoch 347  Train loss: -0.11687795508141612  Val loss: 0.04960006291104346
Epoch 348
-------------------------------
Batch 1/64 loss: -0.12532919645309448
Batch 2/64 loss: -0.12572944164276123
Batch 3/64 loss: -0.11075729131698608
Batch 4/64 loss: -0.12318462133407593
Batch 5/64 loss: -0.11201250553131104
Batch 6/64 loss: -0.10988765954971313
Batch 7/64 loss: -0.11095333099365234
Batch 8/64 loss: -0.14155787229537964
Batch 9/64 loss: -0.1305810809135437
Batch 10/64 loss: -0.064655601978302
Batch 11/64 loss: -0.12109923362731934
Batch 12/64 loss: -0.132097065448761
Batch 13/64 loss: -0.13592034578323364
Batch 14/64 loss: -0.12832558155059814
Batch 15/64 loss: -0.14552736282348633
Batch 16/64 loss: -0.14540284872055054
Batch 17/64 loss: -0.13700950145721436
Batch 18/64 loss: -0.12988954782485962
Batch 19/64 loss: -0.09066927433013916
Batch 20/64 loss: -0.13790589570999146
Batch 21/64 loss: -0.14483916759490967
Batch 22/64 loss: -0.11265838146209717
Batch 23/64 loss: -0.10389870405197144
Batch 24/64 loss: -0.12726867198944092
Batch 25/64 loss: -0.11515462398529053
Batch 26/64 loss: -0.07889294624328613
Batch 27/64 loss: -0.13284623622894287
Batch 28/64 loss: -0.15371334552764893
Batch 29/64 loss: -0.12686139345169067
Batch 30/64 loss: -0.13010138273239136
Batch 31/64 loss: -0.12657809257507324
Batch 32/64 loss: -0.11491066217422485
Batch 33/64 loss: -0.12787705659866333
Batch 34/64 loss: -0.11938869953155518
Batch 35/64 loss: -0.10263568162918091
Batch 36/64 loss: -0.144481360912323
Batch 37/64 loss: -0.12681466341018677
Batch 38/64 loss: -0.11207687854766846
Batch 39/64 loss: -0.11502915620803833
Batch 40/64 loss: -0.09988701343536377
Batch 41/64 loss: -0.1170799732208252
Batch 42/64 loss: -0.12332582473754883
Batch 43/64 loss: -0.07708162069320679
Batch 44/64 loss: -0.11941713094711304
Batch 45/64 loss: -0.1056627631187439
Batch 46/64 loss: -0.12519782781600952
Batch 47/64 loss: -0.1251145601272583
Batch 48/64 loss: -0.13272422552108765
Batch 49/64 loss: -0.12943494319915771
Batch 50/64 loss: -0.11425834894180298
Batch 51/64 loss: -0.13263779878616333
Batch 52/64 loss: -0.10461211204528809
Batch 53/64 loss: -0.11101001501083374
Batch 54/64 loss: -0.08661383390426636
Batch 55/64 loss: -0.08236223459243774
Batch 56/64 loss: -0.07239842414855957
Batch 57/64 loss: -0.10303550958633423
Batch 58/64 loss: -0.11575806140899658
Batch 59/64 loss: -0.12725871801376343
Batch 60/64 loss: -0.10288733243942261
Batch 61/64 loss: -0.13104689121246338
Batch 62/64 loss: -0.08960139751434326
Batch 63/64 loss: -0.08301311731338501
Batch 64/64 loss: -0.0930180549621582
Epoch 348  Train loss: -0.11695224444071452  Val loss: 0.049170518658824804
Epoch 349
-------------------------------
Batch 1/64 loss: -0.10337603092193604
Batch 2/64 loss: -0.12205815315246582
Batch 3/64 loss: -0.11822187900543213
Batch 4/64 loss: -0.14359354972839355
Batch 5/64 loss: -0.13535374402999878
Batch 6/64 loss: -0.11848235130310059
Batch 7/64 loss: -0.11719799041748047
Batch 8/64 loss: -0.1029130220413208
Batch 9/64 loss: -0.11364239454269409
Batch 10/64 loss: -0.1338108777999878
Batch 11/64 loss: -0.11720848083496094
Batch 12/64 loss: -0.09967255592346191
Batch 13/64 loss: -0.12849289178848267
Batch 14/64 loss: -0.12764465808868408
Batch 15/64 loss: -0.13518452644348145
Batch 16/64 loss: -0.11615347862243652
Batch 17/64 loss: -0.1083071231842041
Batch 18/64 loss: -0.11163216829299927
Batch 19/64 loss: -0.10778719186782837
Batch 20/64 loss: -0.10838675498962402
Batch 21/64 loss: -0.11535191535949707
Batch 22/64 loss: -0.09404420852661133
Batch 23/64 loss: -0.09298509359359741
Batch 24/64 loss: -0.12213331460952759
Batch 25/64 loss: -0.09193629026412964
Batch 26/64 loss: -0.11304181814193726
Batch 27/64 loss: -0.10669517517089844
Batch 28/64 loss: -0.11787080764770508
Batch 29/64 loss: -0.12927502393722534
Batch 30/64 loss: -0.1346493363380432
Batch 31/64 loss: -0.12850254774093628
Batch 32/64 loss: -0.10570263862609863
Batch 33/64 loss: -0.06785589456558228
Batch 34/64 loss: -0.09384608268737793
Batch 35/64 loss: -0.09539979696273804
Batch 36/64 loss: -0.11751830577850342
Batch 37/64 loss: -0.12368017435073853
Batch 38/64 loss: -0.1163400411605835
Batch 39/64 loss: -0.11004501581192017
Batch 40/64 loss: -0.10353982448577881
Batch 41/64 loss: -0.12087047100067139
Batch 42/64 loss: -0.10510492324829102
Batch 43/64 loss: -0.12669789791107178
Batch 44/64 loss: -0.12159144878387451
Batch 45/64 loss: -0.10822057723999023
Batch 46/64 loss: -0.12607234716415405
Batch 47/64 loss: -0.1254919171333313
Batch 48/64 loss: -0.11232709884643555
Batch 49/64 loss: -0.13351887464523315
Batch 50/64 loss: -0.09613436460494995
Batch 51/64 loss: -0.1283036470413208
Batch 52/64 loss: -0.09327518939971924
Batch 53/64 loss: -0.13012760877609253
Batch 54/64 loss: -0.1275010108947754
Batch 55/64 loss: -0.09599870443344116
Batch 56/64 loss: -0.11391431093215942
Batch 57/64 loss: -0.0946662425994873
Batch 58/64 loss: -0.08317196369171143
Batch 59/64 loss: -0.07920759916305542
Batch 60/64 loss: -0.1101866364479065
Batch 61/64 loss: -0.11025053262710571
Batch 62/64 loss: -0.10051745176315308
Batch 63/64 loss: -0.08530533313751221
Batch 64/64 loss: -0.10020482540130615
Epoch 349  Train loss: -0.11220616312587962  Val loss: 0.05231860638484103
Epoch 350
-------------------------------
Batch 1/64 loss: -0.12511920928955078
Batch 2/64 loss: -0.11553359031677246
Batch 3/64 loss: -0.11986541748046875
Batch 4/64 loss: -0.10064446926116943
Batch 5/64 loss: -0.11263632774353027
Batch 6/64 loss: -0.11393207311630249
Batch 7/64 loss: -0.13140499591827393
Batch 8/64 loss: -0.13063764572143555
Batch 9/64 loss: -0.11381590366363525
Batch 10/64 loss: -0.11810302734375
Batch 11/64 loss: -0.12075275182723999
Batch 12/64 loss: -0.10964179039001465
Batch 13/64 loss: -0.12789708375930786
Batch 14/64 loss: -0.12274718284606934
Batch 15/64 loss: -0.10823953151702881
Batch 16/64 loss: -0.10101664066314697
Batch 17/64 loss: -0.12469500303268433
Batch 18/64 loss: -0.13434600830078125
Batch 19/64 loss: -0.10132241249084473
Batch 20/64 loss: -0.09882992506027222
Batch 21/64 loss: -0.1037909984588623
Batch 22/64 loss: -0.1325538158416748
Batch 23/64 loss: -0.13114047050476074
Batch 24/64 loss: -0.12206697463989258
Batch 25/64 loss: -0.10175377130508423
Batch 26/64 loss: -0.09987610578536987
Batch 27/64 loss: -0.1157066822052002
Batch 28/64 loss: -0.08978140354156494
Batch 29/64 loss: -0.1233857274055481
Batch 30/64 loss: -0.12444305419921875
Batch 31/64 loss: -0.12574410438537598
Batch 32/64 loss: -0.12355536222457886
Batch 33/64 loss: -0.13659226894378662
Batch 34/64 loss: -0.11556613445281982
Batch 35/64 loss: -0.10746538639068604
Batch 36/64 loss: -0.12352395057678223
Batch 37/64 loss: -0.12683779001235962
Batch 38/64 loss: -0.13790273666381836
Batch 39/64 loss: -0.12883752584457397
Batch 40/64 loss: -0.11362385749816895
Batch 41/64 loss: -0.10098958015441895
Batch 42/64 loss: -0.12815093994140625
Batch 43/64 loss: -0.054271578788757324
Batch 44/64 loss: -0.1181986927986145
Batch 45/64 loss: -0.10250139236450195
Batch 46/64 loss: -0.1384803056716919
Batch 47/64 loss: -0.13044798374176025
Batch 48/64 loss: -0.10049968957901001
Batch 49/64 loss: -0.12233841419219971
Batch 50/64 loss: -0.13398104906082153
Batch 51/64 loss: -0.12078356742858887
Batch 52/64 loss: -0.07840210199356079
Batch 53/64 loss: -0.11446869373321533
Batch 54/64 loss: -0.1161547303199768
Batch 55/64 loss: -0.11364555358886719
Batch 56/64 loss: -0.08447265625
Batch 57/64 loss: -0.0779411792755127
Batch 58/64 loss: -0.12551426887512207
Batch 59/64 loss: -0.10176229476928711
Batch 60/64 loss: -0.11057418584823608
Batch 61/64 loss: -0.1018211841583252
Batch 62/64 loss: -0.1012459397315979
Batch 63/64 loss: -0.10047632455825806
Batch 64/64 loss: -0.11685836315155029
Epoch 350  Train loss: -0.11419757160485959  Val loss: 0.049650665001361234
Epoch 351
-------------------------------
Batch 1/64 loss: -0.11987727880477905
Batch 2/64 loss: -0.09089720249176025
Batch 3/64 loss: -0.09138685464859009
Batch 4/64 loss: -0.11820089817047119
Batch 5/64 loss: -0.1199268102645874
Batch 6/64 loss: -0.10350382328033447
Batch 7/64 loss: -0.09302467107772827
Batch 8/64 loss: -0.08472853899002075
Batch 9/64 loss: -0.13058596849441528
Batch 10/64 loss: -0.10765361785888672
Batch 11/64 loss: -0.11391317844390869
Batch 12/64 loss: -0.13478058576583862
Batch 13/64 loss: -0.13327324390411377
Batch 14/64 loss: -0.11665338277816772
Batch 15/64 loss: -0.09667009115219116
Batch 16/64 loss: -0.12174475193023682
Batch 17/64 loss: -0.12078732252120972
Batch 18/64 loss: -0.10717010498046875
Batch 19/64 loss: -0.12714451551437378
Batch 20/64 loss: -0.12512153387069702
Batch 21/64 loss: -0.10771310329437256
Batch 22/64 loss: -0.11037439107894897
Batch 23/64 loss: -0.09953677654266357
Batch 24/64 loss: -0.13223296403884888
Batch 25/64 loss: -0.09866893291473389
Batch 26/64 loss: -0.12204468250274658
Batch 27/64 loss: -0.13853847980499268
Batch 28/64 loss: -0.1346532106399536
Batch 29/64 loss: -0.1461719274520874
Batch 30/64 loss: -0.1286832094192505
Batch 31/64 loss: -0.09909474849700928
Batch 32/64 loss: -0.09849876165390015
Batch 33/64 loss: -0.10658961534500122
Batch 34/64 loss: -0.09399741888046265
Batch 35/64 loss: -0.09278064966201782
Batch 36/64 loss: -0.13283991813659668
Batch 37/64 loss: -0.12846672534942627
Batch 38/64 loss: -0.09135967493057251
Batch 39/64 loss: -0.1325143575668335
Batch 40/64 loss: -0.11434996128082275
Batch 41/64 loss: -0.07624441385269165
Batch 42/64 loss: -0.1337488293647766
Batch 43/64 loss: -0.09790420532226562
Batch 44/64 loss: -0.12637627124786377
Batch 45/64 loss: -0.091380774974823
Batch 46/64 loss: -0.11614012718200684
Batch 47/64 loss: -0.10783767700195312
Batch 48/64 loss: -0.11998158693313599
Batch 49/64 loss: -0.11789393424987793
Batch 50/64 loss: -0.14431864023208618
Batch 51/64 loss: -0.1340392827987671
Batch 52/64 loss: -0.10200858116149902
Batch 53/64 loss: -0.10673898458480835
Batch 54/64 loss: -0.11083483695983887
Batch 55/64 loss: -0.10267704725265503
Batch 56/64 loss: -0.12619894742965698
Batch 57/64 loss: -0.1286659836769104
Batch 58/64 loss: -0.13234221935272217
Batch 59/64 loss: -0.11651235818862915
Batch 60/64 loss: -0.12854665517807007
Batch 61/64 loss: -0.11672711372375488
Batch 62/64 loss: -0.11156612634658813
Batch 63/64 loss: -0.11149293184280396
Batch 64/64 loss: -0.11261564493179321
Epoch 351  Train loss: -0.11467910818025177  Val loss: 0.04726141752655973
Epoch 352
-------------------------------
Batch 1/64 loss: -0.09625524282455444
Batch 2/64 loss: -0.09218496084213257
Batch 3/64 loss: -0.1301167607307434
Batch 4/64 loss: -0.119121253490448
Batch 5/64 loss: -0.13581204414367676
Batch 6/64 loss: -0.11655902862548828
Batch 7/64 loss: -0.11368757486343384
Batch 8/64 loss: -0.13450628519058228
Batch 9/64 loss: -0.12070536613464355
Batch 10/64 loss: -0.1330852508544922
Batch 11/64 loss: -0.11303448677062988
Batch 12/64 loss: -0.15194165706634521
Batch 13/64 loss: -0.11241066455841064
Batch 14/64 loss: -0.11484909057617188
Batch 15/64 loss: -0.09402352571487427
Batch 16/64 loss: -0.12203347682952881
Batch 17/64 loss: -0.07781678438186646
Batch 18/64 loss: -0.1153862476348877
Batch 19/64 loss: -0.12094622850418091
Batch 20/64 loss: -0.11755406856536865
Batch 21/64 loss: -0.1310255527496338
Batch 22/64 loss: -0.1385135054588318
Batch 23/64 loss: -0.11924189329147339
Batch 24/64 loss: -0.1052241325378418
Batch 25/64 loss: -0.10172492265701294
Batch 26/64 loss: -0.11619627475738525
Batch 27/64 loss: -0.13539153337478638
Batch 28/64 loss: -0.1264512538909912
Batch 29/64 loss: -0.10797983407974243
Batch 30/64 loss: -0.13870805501937866
Batch 31/64 loss: -0.13855278491973877
Batch 32/64 loss: -0.14128518104553223
Batch 33/64 loss: -0.109161376953125
Batch 34/64 loss: -0.1453099250793457
Batch 35/64 loss: -0.11539745330810547
Batch 36/64 loss: -0.1271018385887146
Batch 37/64 loss: -0.13460111618041992
Batch 38/64 loss: -0.08517789840698242
Batch 39/64 loss: -0.12496072053909302
Batch 40/64 loss: -0.10811114311218262
Batch 41/64 loss: -0.11395120620727539
Batch 42/64 loss: -0.11937761306762695
Batch 43/64 loss: -0.13642507791519165
Batch 44/64 loss: -0.11754012107849121
Batch 45/64 loss: -0.13539958000183105
Batch 46/64 loss: -0.08625495433807373
Batch 47/64 loss: -0.13496488332748413
Batch 48/64 loss: -0.08933770656585693
Batch 49/64 loss: -0.11342918872833252
Batch 50/64 loss: -0.10999429225921631
Batch 51/64 loss: -0.10835796594619751
Batch 52/64 loss: -0.09197431802749634
Batch 53/64 loss: -0.08085882663726807
Batch 54/64 loss: -0.07145470380783081
Batch 55/64 loss: -0.11366468667984009
Batch 56/64 loss: -0.10162150859832764
Batch 57/64 loss: -0.08537918329238892
Batch 58/64 loss: -0.1285446286201477
Batch 59/64 loss: -0.12892264127731323
Batch 60/64 loss: -0.0967254638671875
Batch 61/64 loss: -0.11301040649414062
Batch 62/64 loss: -0.10632038116455078
Batch 63/64 loss: -0.11950886249542236
Batch 64/64 loss: -0.10520321130752563
Epoch 352  Train loss: -0.11551477651970059  Val loss: 0.04885369703122431
Epoch 353
-------------------------------
Batch 1/64 loss: -0.11337220668792725
Batch 2/64 loss: -0.12637794017791748
Batch 3/64 loss: -0.11983108520507812
Batch 4/64 loss: -0.1054723858833313
Batch 5/64 loss: -0.11089068651199341
Batch 6/64 loss: -0.1161036491394043
Batch 7/64 loss: -0.13657653331756592
Batch 8/64 loss: -0.1415398120880127
Batch 9/64 loss: -0.11724281311035156
Batch 10/64 loss: -0.13374757766723633
Batch 11/64 loss: -0.12108516693115234
Batch 12/64 loss: -0.14767193794250488
Batch 13/64 loss: -0.12076723575592041
Batch 14/64 loss: -0.13449734449386597
Batch 15/64 loss: -0.1309521198272705
Batch 16/64 loss: -0.11134463548660278
Batch 17/64 loss: -0.13781100511550903
Batch 18/64 loss: -0.12432140111923218
Batch 19/64 loss: -0.1161501407623291
Batch 20/64 loss: -0.13203108310699463
Batch 21/64 loss: -0.09676653146743774
Batch 22/64 loss: -0.08411228656768799
Batch 23/64 loss: -0.1397683024406433
Batch 24/64 loss: -0.10380923748016357
Batch 25/64 loss: -0.1318126916885376
Batch 26/64 loss: -0.12658238410949707
Batch 27/64 loss: -0.10501974821090698
Batch 28/64 loss: -0.13754552602767944
Batch 29/64 loss: -0.11123108863830566
Batch 30/64 loss: -0.0704646110534668
Batch 31/64 loss: -0.10875576734542847
Batch 32/64 loss: -0.11351275444030762
Batch 33/64 loss: -0.1273881196975708
Batch 34/64 loss: -0.116996169090271
Batch 35/64 loss: -0.0978131890296936
Batch 36/64 loss: -0.1147887110710144
Batch 37/64 loss: -0.08769881725311279
Batch 38/64 loss: -0.09730684757232666
Batch 39/64 loss: -0.10935091972351074
Batch 40/64 loss: -0.08310341835021973
Batch 41/64 loss: -0.11325764656066895
Batch 42/64 loss: -0.09849441051483154
Batch 43/64 loss: -0.08987903594970703
Batch 44/64 loss: -0.1205291748046875
Batch 45/64 loss: -0.101279616355896
Batch 46/64 loss: -0.14108407497406006
Batch 47/64 loss: -0.11656266450881958
Batch 48/64 loss: -0.10092133283615112
Batch 49/64 loss: -0.07286810874938965
Batch 50/64 loss: -0.12509006261825562
Batch 51/64 loss: -0.11502480506896973
Batch 52/64 loss: -0.11984694004058838
Batch 53/64 loss: -0.12423551082611084
Batch 54/64 loss: -0.12602204084396362
Batch 55/64 loss: -0.12082356214523315
Batch 56/64 loss: -0.1309216022491455
Batch 57/64 loss: -0.12385469675064087
Batch 58/64 loss: -0.12473589181900024
Batch 59/64 loss: -0.1407843828201294
Batch 60/64 loss: -0.12182784080505371
Batch 61/64 loss: -0.1304801106452942
Batch 62/64 loss: -0.071380615234375
Batch 63/64 loss: -0.1029353141784668
Batch 64/64 loss: -0.08228123188018799
Epoch 353  Train loss: -0.11539081171447156  Val loss: 0.05176732363979431
Epoch 354
-------------------------------
Batch 1/64 loss: -0.11507540941238403
Batch 2/64 loss: -0.14110255241394043
Batch 3/64 loss: -0.11903119087219238
Batch 4/64 loss: -0.11354893445968628
Batch 5/64 loss: -0.11189574003219604
Batch 6/64 loss: -0.11684012413024902
Batch 7/64 loss: -0.12403887510299683
Batch 8/64 loss: -0.12903988361358643
Batch 9/64 loss: -0.10750168561935425
Batch 10/64 loss: -0.11655545234680176
Batch 11/64 loss: -0.13713335990905762
Batch 12/64 loss: -0.13894015550613403
Batch 13/64 loss: -0.13913267850875854
Batch 14/64 loss: -0.11221837997436523
Batch 15/64 loss: -0.1351802945137024
Batch 16/64 loss: -0.11632519960403442
Batch 17/64 loss: -0.13051414489746094
Batch 18/64 loss: -0.10981333255767822
Batch 19/64 loss: -0.12953650951385498
Batch 20/64 loss: -0.09795761108398438
Batch 21/64 loss: -0.08179926872253418
Batch 22/64 loss: -0.10577994585037231
Batch 23/64 loss: -0.1497456431388855
Batch 24/64 loss: -0.047837018966674805
Batch 25/64 loss: -0.11735796928405762
Batch 26/64 loss: -0.08711850643157959
Batch 27/64 loss: -0.11352455615997314
Batch 28/64 loss: -0.10751664638519287
Batch 29/64 loss: -0.11327958106994629
Batch 30/64 loss: -0.11563581228256226
Batch 31/64 loss: -0.13693511486053467
Batch 32/64 loss: -0.11949694156646729
Batch 33/64 loss: -0.08933722972869873
Batch 34/64 loss: -0.11960339546203613
Batch 35/64 loss: -0.09892910718917847
Batch 36/64 loss: -0.10823613405227661
Batch 37/64 loss: -0.13173842430114746
Batch 38/64 loss: -0.11758863925933838
Batch 39/64 loss: -0.06331819295883179
Batch 40/64 loss: -0.10847091674804688
Batch 41/64 loss: -0.13347798585891724
Batch 42/64 loss: -0.11072170734405518
Batch 43/64 loss: -0.12229174375534058
Batch 44/64 loss: -0.13557463884353638
Batch 45/64 loss: -0.09713804721832275
Batch 46/64 loss: -0.11442410945892334
Batch 47/64 loss: -0.0922389030456543
Batch 48/64 loss: -0.13316071033477783
Batch 49/64 loss: -0.1319294571876526
Batch 50/64 loss: -0.13217628002166748
Batch 51/64 loss: -0.1323661208152771
Batch 52/64 loss: -0.10533511638641357
Batch 53/64 loss: -0.1314268708229065
Batch 54/64 loss: -0.14224004745483398
Batch 55/64 loss: -0.11718618869781494
Batch 56/64 loss: -0.09680712223052979
Batch 57/64 loss: -0.12327992916107178
Batch 58/64 loss: -0.11604732275009155
Batch 59/64 loss: -0.12344449758529663
Batch 60/64 loss: -0.10508787631988525
Batch 61/64 loss: -0.11659544706344604
Batch 62/64 loss: -0.10491824150085449
Batch 63/64 loss: -0.11302751302719116
Batch 64/64 loss: -0.10163122415542603
Epoch 354  Train loss: -0.11579215456457699  Val loss: 0.04958651258363757
Epoch 355
-------------------------------
Batch 1/64 loss: -0.13330918550491333
Batch 2/64 loss: -0.15063363313674927
Batch 3/64 loss: -0.14142179489135742
Batch 4/64 loss: -0.13662010431289673
Batch 5/64 loss: -0.0900641679763794
Batch 6/64 loss: -0.1325954794883728
Batch 7/64 loss: -0.11775833368301392
Batch 8/64 loss: -0.12677150964736938
Batch 9/64 loss: -0.09391683340072632
Batch 10/64 loss: -0.1260095238685608
Batch 11/64 loss: -0.13384854793548584
Batch 12/64 loss: -0.11361974477767944
Batch 13/64 loss: -0.11470448970794678
Batch 14/64 loss: -0.12086820602416992
Batch 15/64 loss: -0.12233126163482666
Batch 16/64 loss: -0.1457567811012268
Batch 17/64 loss: -0.09995377063751221
Batch 18/64 loss: -0.13895636796951294
Batch 19/64 loss: -0.09524112939834595
Batch 20/64 loss: -0.10783421993255615
Batch 21/64 loss: -0.12257051467895508
Batch 22/64 loss: -0.11414778232574463
Batch 23/64 loss: -0.1375812292098999
Batch 24/64 loss: -0.11868101358413696
Batch 25/64 loss: -0.14111149311065674
Batch 26/64 loss: -0.1191943883895874
Batch 27/64 loss: -0.13195115327835083
Batch 28/64 loss: -0.10613507032394409
Batch 29/64 loss: -0.12826400995254517
Batch 30/64 loss: -0.12308251857757568
Batch 31/64 loss: -0.13105857372283936
Batch 32/64 loss: -0.11274367570877075
Batch 33/64 loss: -0.1453704833984375
Batch 34/64 loss: -0.10972869396209717
Batch 35/64 loss: -0.12180352210998535
Batch 36/64 loss: -0.1062387228012085
Batch 37/64 loss: -0.09546619653701782
Batch 38/64 loss: -0.09846341609954834
Batch 39/64 loss: -0.09625554084777832
Batch 40/64 loss: -0.1001017689704895
Batch 41/64 loss: -0.11625593900680542
Batch 42/64 loss: -0.12062358856201172
Batch 43/64 loss: -0.10541051626205444
Batch 44/64 loss: -0.10340666770935059
Batch 45/64 loss: -0.1194601058959961
Batch 46/64 loss: -0.1110541820526123
Batch 47/64 loss: -0.11631208658218384
Batch 48/64 loss: -0.08856087923049927
Batch 49/64 loss: -0.11876392364501953
Batch 50/64 loss: -0.10165899991989136
Batch 51/64 loss: -0.09273052215576172
Batch 52/64 loss: -0.06998980045318604
Batch 53/64 loss: -0.11116552352905273
Batch 54/64 loss: -0.11394143104553223
Batch 55/64 loss: -0.11424440145492554
Batch 56/64 loss: -0.07701253890991211
Batch 57/64 loss: -0.1328040361404419
Batch 58/64 loss: -0.12795674800872803
Batch 59/64 loss: -0.11364632844924927
Batch 60/64 loss: -0.1110568642616272
Batch 61/64 loss: -0.13246691226959229
Batch 62/64 loss: -0.05347400903701782
Batch 63/64 loss: -0.11173862218856812
Batch 64/64 loss: -0.12888503074645996
Epoch 355  Train loss: -0.11549118827371037  Val loss: 0.04889257482646667
Epoch 356
-------------------------------
Batch 1/64 loss: -0.0861961841583252
Batch 2/64 loss: -0.10185003280639648
Batch 3/64 loss: -0.10508638620376587
Batch 4/64 loss: -0.14859747886657715
Batch 5/64 loss: -0.13693612813949585
Batch 6/64 loss: -0.09997785091400146
Batch 7/64 loss: -0.11024284362792969
Batch 8/64 loss: -0.13259124755859375
Batch 9/64 loss: -0.13229328393936157
Batch 10/64 loss: -0.12274312973022461
Batch 11/64 loss: -0.11636847257614136
Batch 12/64 loss: -0.12939739227294922
Batch 13/64 loss: -0.13248205184936523
Batch 14/64 loss: -0.11690998077392578
Batch 15/64 loss: -0.11484456062316895
Batch 16/64 loss: -0.12417024374008179
Batch 17/64 loss: -0.12144535779953003
Batch 18/64 loss: -0.13212621212005615
Batch 19/64 loss: -0.08865386247634888
Batch 20/64 loss: -0.11394113302230835
Batch 21/64 loss: -0.11431431770324707
Batch 22/64 loss: -0.1286780834197998
Batch 23/64 loss: -0.12552493810653687
Batch 24/64 loss: -0.11287331581115723
Batch 25/64 loss: -0.1399340033531189
Batch 26/64 loss: -0.1167188286781311
Batch 27/64 loss: -0.12841951847076416
Batch 28/64 loss: -0.14169669151306152
Batch 29/64 loss: -0.1105610728263855
Batch 30/64 loss: -0.12072229385375977
Batch 31/64 loss: -0.1269109845161438
Batch 32/64 loss: -0.0994752049446106
Batch 33/64 loss: -0.11438196897506714
Batch 34/64 loss: -0.088584303855896
Batch 35/64 loss: -0.11799150705337524
Batch 36/64 loss: -0.11980879306793213
Batch 37/64 loss: -0.14296895265579224
Batch 38/64 loss: -0.1301526427268982
Batch 39/64 loss: -0.08891516923904419
Batch 40/64 loss: -0.08493161201477051
Batch 41/64 loss: -0.11008429527282715
Batch 42/64 loss: -0.12088066339492798
Batch 43/64 loss: -0.09502565860748291
Batch 44/64 loss: -0.11505341529846191
Batch 45/64 loss: -0.08425188064575195
Batch 46/64 loss: -0.11602342128753662
Batch 47/64 loss: -0.10630285739898682
Batch 48/64 loss: -0.11590415239334106
Batch 49/64 loss: -0.12302368879318237
Batch 50/64 loss: -0.12248194217681885
Batch 51/64 loss: -0.11104822158813477
Batch 52/64 loss: -0.1287943720817566
Batch 53/64 loss: -0.12706971168518066
Batch 54/64 loss: -0.08545655012130737
Batch 55/64 loss: -0.12642735242843628
Batch 56/64 loss: -0.08787167072296143
Batch 57/64 loss: -0.11655402183532715
Batch 58/64 loss: -0.09420961141586304
Batch 59/64 loss: -0.12540078163146973
Batch 60/64 loss: -0.11625391244888306
Batch 61/64 loss: -0.11399024724960327
Batch 62/64 loss: -0.10065937042236328
Batch 63/64 loss: -0.1261712908744812
Batch 64/64 loss: -0.09563678503036499
Epoch 356  Train loss: -0.115467995522069  Val loss: 0.050960598123032615
Epoch 357
-------------------------------
Batch 1/64 loss: -0.12253880500793457
Batch 2/64 loss: -0.1127970814704895
Batch 3/64 loss: -0.1359471082687378
Batch 4/64 loss: -0.10554975271224976
Batch 5/64 loss: -0.10540270805358887
Batch 6/64 loss: -0.13851916790008545
Batch 7/64 loss: -0.11982846260070801
Batch 8/64 loss: -0.1318240761756897
Batch 9/64 loss: -0.12207233905792236
Batch 10/64 loss: -0.11831235885620117
Batch 11/64 loss: -0.12062704563140869
Batch 12/64 loss: -0.14921307563781738
Batch 13/64 loss: -0.15278947353363037
Batch 14/64 loss: -0.13441842794418335
Batch 15/64 loss: -0.10360592603683472
Batch 16/64 loss: -0.10832011699676514
Batch 17/64 loss: -0.12062513828277588
Batch 18/64 loss: -0.11808770895004272
Batch 19/64 loss: -0.09341436624526978
Batch 20/64 loss: -0.11903667449951172
Batch 21/64 loss: -0.13098865747451782
Batch 22/64 loss: -0.13004350662231445
Batch 23/64 loss: -0.14412862062454224
Batch 24/64 loss: -0.12357699871063232
Batch 25/64 loss: -0.10759443044662476
Batch 26/64 loss: -0.12211757898330688
Batch 27/64 loss: -0.11926984786987305
Batch 28/64 loss: -0.13013696670532227
Batch 29/64 loss: -0.09467494487762451
Batch 30/64 loss: -0.11272001266479492
Batch 31/64 loss: -0.10419058799743652
Batch 32/64 loss: -0.1263437271118164
Batch 33/64 loss: -0.13762938976287842
Batch 34/64 loss: -0.11149144172668457
Batch 35/64 loss: -0.12190085649490356
Batch 36/64 loss: -0.13360512256622314
Batch 37/64 loss: -0.11601990461349487
Batch 38/64 loss: -0.1280193328857422
Batch 39/64 loss: -0.12665367126464844
Batch 40/64 loss: -0.09131842851638794
Batch 41/64 loss: -0.12617647647857666
Batch 42/64 loss: -0.10779416561126709
Batch 43/64 loss: -0.11917412281036377
Batch 44/64 loss: -0.11751514673233032
Batch 45/64 loss: -0.1193656325340271
Batch 46/64 loss: -0.13916736841201782
Batch 47/64 loss: -0.11348330974578857
Batch 48/64 loss: -0.11647003889083862
Batch 49/64 loss: -0.11386394500732422
Batch 50/64 loss: -0.09931755065917969
Batch 51/64 loss: -0.12124866247177124
Batch 52/64 loss: -0.08812665939331055
Batch 53/64 loss: -0.14181435108184814
Batch 54/64 loss: -0.1153709888458252
Batch 55/64 loss: -0.12046557664871216
Batch 56/64 loss: -0.09744608402252197
Batch 57/64 loss: -0.12163126468658447
Batch 58/64 loss: -0.12338137626647949
Batch 59/64 loss: -0.1313326358795166
Batch 60/64 loss: -0.10776084661483765
Batch 61/64 loss: -0.09691929817199707
Batch 62/64 loss: -0.11953151226043701
Batch 63/64 loss: -0.1171717643737793
Batch 64/64 loss: -0.11106902360916138
Epoch 357  Train loss: -0.11926563744451485  Val loss: 0.04987784226735433
Epoch 358
-------------------------------
Batch 1/64 loss: -0.13271504640579224
Batch 2/64 loss: -0.08886033296585083
Batch 3/64 loss: -0.10197335481643677
Batch 4/64 loss: -0.12673884630203247
Batch 5/64 loss: -0.1355074644088745
Batch 6/64 loss: -0.13827389478683472
Batch 7/64 loss: -0.11763155460357666
Batch 8/64 loss: -0.1098029613494873
Batch 9/64 loss: -0.1381257176399231
Batch 10/64 loss: -0.12241631746292114
Batch 11/64 loss: -0.1223839521408081
Batch 12/64 loss: -0.11857908964157104
Batch 13/64 loss: -0.1471865177154541
Batch 14/64 loss: -0.1073107123374939
Batch 15/64 loss: -0.13781660795211792
Batch 16/64 loss: -0.13366657495498657
Batch 17/64 loss: -0.09427016973495483
Batch 18/64 loss: -0.12699657678604126
Batch 19/64 loss: -0.11085975170135498
Batch 20/64 loss: -0.13003146648406982
Batch 21/64 loss: -0.12534421682357788
Batch 22/64 loss: -0.10075873136520386
Batch 23/64 loss: -0.11480271816253662
Batch 24/64 loss: -0.11276483535766602
Batch 25/64 loss: -0.11944067478179932
Batch 26/64 loss: -0.10490423440933228
Batch 27/64 loss: -0.11856436729431152
Batch 28/64 loss: -0.09726691246032715
Batch 29/64 loss: -0.11705034971237183
Batch 30/64 loss: -0.11421561241149902
Batch 31/64 loss: -0.1076500415802002
Batch 32/64 loss: -0.10660445690155029
Batch 33/64 loss: -0.09238219261169434
Batch 34/64 loss: -0.11603069305419922
Batch 35/64 loss: -0.11217701435089111
Batch 36/64 loss: -0.09596079587936401
Batch 37/64 loss: -0.11382806301116943
Batch 38/64 loss: -0.1105988621711731
Batch 39/64 loss: -0.12439846992492676
Batch 40/64 loss: -0.0981760025024414
Batch 41/64 loss: -0.14379942417144775
Batch 42/64 loss: -0.10448157787322998
Batch 43/64 loss: -0.12449377775192261
Batch 44/64 loss: -0.13125914335250854
Batch 45/64 loss: -0.1275038719177246
Batch 46/64 loss: -0.11244004964828491
Batch 47/64 loss: -0.1268630027770996
Batch 48/64 loss: -0.13331109285354614
Batch 49/64 loss: -0.13622742891311646
Batch 50/64 loss: -0.13138830661773682
Batch 51/64 loss: -0.13506287336349487
Batch 52/64 loss: -0.1318543553352356
Batch 53/64 loss: -0.13222968578338623
Batch 54/64 loss: -0.09747314453125
Batch 55/64 loss: -0.09272587299346924
Batch 56/64 loss: -0.10409051179885864
Batch 57/64 loss: -0.11132049560546875
Batch 58/64 loss: -0.11934208869934082
Batch 59/64 loss: -0.09427082538604736
Batch 60/64 loss: -0.10675632953643799
Batch 61/64 loss: -0.09914267063140869
Batch 62/64 loss: -0.07407641410827637
Batch 63/64 loss: -0.11337381601333618
Batch 64/64 loss: -0.11741167306900024
Epoch 358  Train loss: -0.1163233203046462  Val loss: 0.052163662574545216
Epoch 359
-------------------------------
Batch 1/64 loss: -0.10565000772476196
Batch 2/64 loss: -0.11158013343811035
Batch 3/64 loss: -0.13694721460342407
Batch 4/64 loss: -0.12911564111709595
Batch 5/64 loss: -0.11344093084335327
Batch 6/64 loss: -0.1067085862159729
Batch 7/64 loss: -0.1016964316368103
Batch 8/64 loss: -0.1165887713432312
Batch 9/64 loss: -0.1227993369102478
Batch 10/64 loss: -0.09906560182571411
Batch 11/64 loss: -0.1160130500793457
Batch 12/64 loss: -0.10789352655410767
Batch 13/64 loss: -0.12137889862060547
Batch 14/64 loss: -0.13149422407150269
Batch 15/64 loss: -0.13520509004592896
Batch 16/64 loss: -0.13336336612701416
Batch 17/64 loss: -0.09811115264892578
Batch 18/64 loss: -0.11842405796051025
Batch 19/64 loss: -0.10289871692657471
Batch 20/64 loss: -0.10575687885284424
Batch 21/64 loss: -0.12797188758850098
Batch 22/64 loss: -0.13874679803848267
Batch 23/64 loss: -0.12023210525512695
Batch 24/64 loss: -0.13179337978363037
Batch 25/64 loss: -0.1300068497657776
Batch 26/64 loss: -0.12401169538497925
Batch 27/64 loss: -0.11409163475036621
Batch 28/64 loss: -0.1140066385269165
Batch 29/64 loss: -0.10013031959533691
Batch 30/64 loss: -0.1101025938987732
Batch 31/64 loss: -0.12572741508483887
Batch 32/64 loss: -0.12268924713134766
Batch 33/64 loss: -0.08354437351226807
Batch 34/64 loss: -0.13204741477966309
Batch 35/64 loss: -0.104483962059021
Batch 36/64 loss: -0.10771524906158447
Batch 37/64 loss: -0.12055826187133789
Batch 38/64 loss: -0.10283708572387695
Batch 39/64 loss: -0.11980164051055908
Batch 40/64 loss: -0.08583962917327881
Batch 41/64 loss: -0.08694452047348022
Batch 42/64 loss: -0.10574066638946533
Batch 43/64 loss: -0.12181168794631958
Batch 44/64 loss: -0.1151587963104248
Batch 45/64 loss: -0.10208666324615479
Batch 46/64 loss: -0.10632073879241943
Batch 47/64 loss: -0.10301154851913452
Batch 48/64 loss: -0.12621909379959106
Batch 49/64 loss: -0.12203699350357056
Batch 50/64 loss: -0.1321810483932495
Batch 51/64 loss: -0.1082143783569336
Batch 52/64 loss: -0.10906589031219482
Batch 53/64 loss: -0.11409646272659302
Batch 54/64 loss: -0.1194879412651062
Batch 55/64 loss: -0.13442003726959229
Batch 56/64 loss: -0.10734927654266357
Batch 57/64 loss: -0.1439165472984314
Batch 58/64 loss: -0.1091909408569336
Batch 59/64 loss: -0.11487752199172974
Batch 60/64 loss: -0.13948482275009155
Batch 61/64 loss: -0.12533581256866455
Batch 62/64 loss: -0.1307026743888855
Batch 63/64 loss: -0.11424553394317627
Batch 64/64 loss: -0.10358619689941406
Epoch 359  Train loss: -0.11607935755860571  Val loss: 0.052271710023847236
Epoch 360
-------------------------------
Batch 1/64 loss: -0.12398397922515869
Batch 2/64 loss: -0.08805191516876221
Batch 3/64 loss: -0.11681437492370605
Batch 4/64 loss: -0.0912889838218689
Batch 5/64 loss: -0.14364182949066162
Batch 6/64 loss: -0.1095113754272461
Batch 7/64 loss: -0.11860871315002441
Batch 8/64 loss: -0.11631608009338379
Batch 9/64 loss: -0.10367333889007568
Batch 10/64 loss: -0.13795924186706543
Batch 11/64 loss: -0.1500067114830017
Batch 12/64 loss: -0.1491374969482422
Batch 13/64 loss: -0.13509070873260498
Batch 14/64 loss: -0.1295163631439209
Batch 15/64 loss: -0.12536120414733887
Batch 16/64 loss: -0.1189647912979126
Batch 17/64 loss: -0.13333499431610107
Batch 18/64 loss: -0.1101042628288269
Batch 19/64 loss: -0.11680871248245239
Batch 20/64 loss: -0.12165403366088867
Batch 21/64 loss: -0.12661200761795044
Batch 22/64 loss: -0.11415880918502808
Batch 23/64 loss: -0.11180198192596436
Batch 24/64 loss: -0.12231487035751343
Batch 25/64 loss: -0.13657289743423462
Batch 26/64 loss: -0.1400076150894165
Batch 27/64 loss: -0.1459355354309082
Batch 28/64 loss: -0.11965674161911011
Batch 29/64 loss: -0.09027504920959473
Batch 30/64 loss: -0.13452577590942383
Batch 31/64 loss: -0.09460484981536865
Batch 32/64 loss: -0.15097934007644653
Batch 33/64 loss: -0.12413746118545532
Batch 34/64 loss: -0.12783485651016235
Batch 35/64 loss: -0.11406528949737549
Batch 36/64 loss: -0.07170486450195312
Batch 37/64 loss: -0.09022665023803711
Batch 38/64 loss: -0.10014551877975464
Batch 39/64 loss: -0.10831290483474731
Batch 40/64 loss: -0.11832863092422485
Batch 41/64 loss: -0.14980125427246094
Batch 42/64 loss: -0.12794619798660278
Batch 43/64 loss: -0.0982210636138916
Batch 44/64 loss: -0.11476105451583862
Batch 45/64 loss: -0.13762438297271729
Batch 46/64 loss: -0.06657165288925171
Batch 47/64 loss: -0.11761218309402466
Batch 48/64 loss: -0.1381966471672058
Batch 49/64 loss: -0.11255741119384766
Batch 50/64 loss: -0.12384945154190063
Batch 51/64 loss: -0.10907244682312012
Batch 52/64 loss: -0.12159538269042969
Batch 53/64 loss: -0.09484446048736572
Batch 54/64 loss: -0.10762625932693481
Batch 55/64 loss: -0.091727614402771
Batch 56/64 loss: -0.1251552700996399
Batch 57/64 loss: -0.10804039239883423
Batch 58/64 loss: -0.11484217643737793
Batch 59/64 loss: -0.1152958869934082
Batch 60/64 loss: -0.10928869247436523
Batch 61/64 loss: -0.09371918439865112
Batch 62/64 loss: -0.09997189044952393
Batch 63/64 loss: -0.12466269731521606
Batch 64/64 loss: -0.1269189715385437
Epoch 360  Train loss: -0.11733652774025412  Val loss: 0.04696538030486746
Epoch 361
-------------------------------
Batch 1/64 loss: -0.14400380849838257
Batch 2/64 loss: -0.13406270742416382
Batch 3/64 loss: -0.10229146480560303
Batch 4/64 loss: -0.13452351093292236
Batch 5/64 loss: -0.085898756980896
Batch 6/64 loss: -0.10318887233734131
Batch 7/64 loss: -0.1126028299331665
Batch 8/64 loss: -0.09673947095870972
Batch 9/64 loss: -0.13817685842514038
Batch 10/64 loss: -0.13807231187820435
Batch 11/64 loss: -0.1290191411972046
Batch 12/64 loss: -0.13313448429107666
Batch 13/64 loss: -0.13147354125976562
Batch 14/64 loss: -0.14869511127471924
Batch 15/64 loss: -0.09943878650665283
Batch 16/64 loss: -0.14077824354171753
Batch 17/64 loss: -0.12293559312820435
Batch 18/64 loss: -0.14741265773773193
Batch 19/64 loss: -0.13269305229187012
Batch 20/64 loss: -0.08940380811691284
Batch 21/64 loss: -0.1411495804786682
Batch 22/64 loss: -0.10924148559570312
Batch 23/64 loss: -0.11494016647338867
Batch 24/64 loss: -0.13507163524627686
Batch 25/64 loss: -0.1080324649810791
Batch 26/64 loss: -0.12823688983917236
Batch 27/64 loss: -0.11568963527679443
Batch 28/64 loss: -0.1365697979927063
Batch 29/64 loss: -0.09059262275695801
Batch 30/64 loss: -0.1307573914527893
Batch 31/64 loss: -0.12621670961380005
Batch 32/64 loss: -0.10331857204437256
Batch 33/64 loss: -0.08435297012329102
Batch 34/64 loss: -0.14747822284698486
Batch 35/64 loss: -0.1394352912902832
Batch 36/64 loss: -0.11641508340835571
Batch 37/64 loss: -0.1204865574836731
Batch 38/64 loss: -0.12572991847991943
Batch 39/64 loss: -0.09963202476501465
Batch 40/64 loss: -0.12236791849136353
Batch 41/64 loss: -0.13387268781661987
Batch 42/64 loss: -0.121543288230896
Batch 43/64 loss: -0.11897861957550049
Batch 44/64 loss: -0.11624109745025635
Batch 45/64 loss: -0.12343651056289673
Batch 46/64 loss: -0.13266056776046753
Batch 47/64 loss: -0.11471688747406006
Batch 48/64 loss: -0.11394697427749634
Batch 49/64 loss: -0.09702378511428833
Batch 50/64 loss: -0.11065036058425903
Batch 51/64 loss: -0.11138159036636353
Batch 52/64 loss: -0.10416752099990845
Batch 53/64 loss: -0.12390404939651489
Batch 54/64 loss: -0.11691224575042725
Batch 55/64 loss: -0.1145550012588501
Batch 56/64 loss: -0.08908277750015259
Batch 57/64 loss: -0.11293566226959229
Batch 58/64 loss: -0.12341159582138062
Batch 59/64 loss: -0.1428694725036621
Batch 60/64 loss: -0.14010494947433472
Batch 61/64 loss: -0.10122734308242798
Batch 62/64 loss: -0.11037194728851318
Batch 63/64 loss: -0.11473095417022705
Batch 64/64 loss: -0.12397587299346924
Epoch 361  Train loss: -0.11987400382172828  Val loss: 0.04897533700228557
Epoch 362
-------------------------------
Batch 1/64 loss: -0.100674569606781
Batch 2/64 loss: -0.13587242364883423
Batch 3/64 loss: -0.10954570770263672
Batch 4/64 loss: -0.13603925704956055
Batch 5/64 loss: -0.13946610689163208
Batch 6/64 loss: -0.11387747526168823
Batch 7/64 loss: -0.12480282783508301
Batch 8/64 loss: -0.15142875909805298
Batch 9/64 loss: -0.13692855834960938
Batch 10/64 loss: -0.1108466386795044
Batch 11/64 loss: -0.12156420946121216
Batch 12/64 loss: -0.12157082557678223
Batch 13/64 loss: -0.13968074321746826
Batch 14/64 loss: -0.1483398675918579
Batch 15/64 loss: -0.13388872146606445
Batch 16/64 loss: -0.12566572427749634
Batch 17/64 loss: -0.11077111959457397
Batch 18/64 loss: -0.135628342628479
Batch 19/64 loss: -0.1411784291267395
Batch 20/64 loss: -0.12877649068832397
Batch 21/64 loss: -0.1311589479446411
Batch 22/64 loss: -0.09276652336120605
Batch 23/64 loss: -0.12352627515792847
Batch 24/64 loss: -0.11136758327484131
Batch 25/64 loss: -0.09514075517654419
Batch 26/64 loss: -0.12528491020202637
Batch 27/64 loss: -0.11435192823410034
Batch 28/64 loss: -0.12642133235931396
Batch 29/64 loss: -0.1378358006477356
Batch 30/64 loss: -0.11686134338378906
Batch 31/64 loss: -0.14377576112747192
Batch 32/64 loss: -0.13818442821502686
Batch 33/64 loss: -0.11413240432739258
Batch 34/64 loss: -0.14210975170135498
Batch 35/64 loss: -0.12234961986541748
Batch 36/64 loss: -0.10813838243484497
Batch 37/64 loss: -0.1079636812210083
Batch 38/64 loss: -0.1276477575302124
Batch 39/64 loss: -0.12792730331420898
Batch 40/64 loss: -0.13248813152313232
Batch 41/64 loss: -0.13916248083114624
Batch 42/64 loss: -0.12822777032852173
Batch 43/64 loss: -0.11272174119949341
Batch 44/64 loss: -0.1279665231704712
Batch 45/64 loss: -0.12796735763549805
Batch 46/64 loss: -0.09336185455322266
Batch 47/64 loss: -0.12446439266204834
Batch 48/64 loss: -0.13925909996032715
Batch 49/64 loss: -0.1292155385017395
Batch 50/64 loss: -0.1068880558013916
Batch 51/64 loss: -0.12797534465789795
Batch 52/64 loss: -0.11918902397155762
Batch 53/64 loss: -0.07671189308166504
Batch 54/64 loss: -0.10919040441513062
Batch 55/64 loss: -0.11440670490264893
Batch 56/64 loss: -0.13033360242843628
Batch 57/64 loss: -0.10374939441680908
Batch 58/64 loss: -0.07052719593048096
Batch 59/64 loss: -0.10709047317504883
Batch 60/64 loss: -0.1464083194732666
Batch 61/64 loss: -0.1161278486251831
Batch 62/64 loss: -0.11119413375854492
Batch 63/64 loss: -0.1122475266456604
Batch 64/64 loss: -0.11191785335540771
Epoch 362  Train loss: -0.12179301159054626  Val loss: 0.04891968961433856
Epoch 363
-------------------------------
Batch 1/64 loss: -0.12882936000823975
Batch 2/64 loss: -0.1308671236038208
Batch 3/64 loss: -0.10341179370880127
Batch 4/64 loss: -0.12469786405563354
Batch 5/64 loss: -0.135492742061615
Batch 6/64 loss: -0.1297324299812317
Batch 7/64 loss: -0.15463030338287354
Batch 8/64 loss: -0.12957239151000977
Batch 9/64 loss: -0.15338373184204102
Batch 10/64 loss: -0.11248987913131714
Batch 11/64 loss: -0.13102591037750244
Batch 12/64 loss: -0.14689505100250244
Batch 13/64 loss: -0.14099079370498657
Batch 14/64 loss: -0.15076100826263428
Batch 15/64 loss: -0.1500924825668335
Batch 16/64 loss: -0.12023508548736572
Batch 17/64 loss: -0.08389955759048462
Batch 18/64 loss: -0.11827486753463745
Batch 19/64 loss: -0.14466148614883423
Batch 20/64 loss: -0.13255983591079712
Batch 21/64 loss: -0.11518740653991699
Batch 22/64 loss: -0.1347506046295166
Batch 23/64 loss: -0.10886234045028687
Batch 24/64 loss: -0.11522722244262695
Batch 25/64 loss: -0.10366195440292358
Batch 26/64 loss: -0.1371670365333557
Batch 27/64 loss: -0.11683475971221924
Batch 28/64 loss: -0.12325429916381836
Batch 29/64 loss: -0.11965674161911011
Batch 30/64 loss: -0.10904091596603394
Batch 31/64 loss: -0.12321233749389648
Batch 32/64 loss: -0.10413575172424316
Batch 33/64 loss: -0.11641311645507812
Batch 34/64 loss: -0.11110305786132812
Batch 35/64 loss: -0.1349564790725708
Batch 36/64 loss: -0.10892856121063232
Batch 37/64 loss: -0.12511885166168213
Batch 38/64 loss: -0.11407941579818726
Batch 39/64 loss: -0.10673731565475464
Batch 40/64 loss: -0.08756810426712036
Batch 41/64 loss: -0.1329062581062317
Batch 42/64 loss: -0.11990773677825928
Batch 43/64 loss: -0.08608353137969971
Batch 44/64 loss: -0.13869833946228027
Batch 45/64 loss: -0.13192862272262573
Batch 46/64 loss: -0.11610984802246094
Batch 47/64 loss: -0.13235849142074585
Batch 48/64 loss: -0.08203452825546265
Batch 49/64 loss: -0.1432752013206482
Batch 50/64 loss: -0.11487782001495361
Batch 51/64 loss: -0.079231858253479
Batch 52/64 loss: -0.11980932950973511
Batch 53/64 loss: -0.11537438631057739
Batch 54/64 loss: -0.10480797290802002
Batch 55/64 loss: -0.09670770168304443
Batch 56/64 loss: -0.12893784046173096
Batch 57/64 loss: -0.10319608449935913
Batch 58/64 loss: -0.11031931638717651
Batch 59/64 loss: -0.10564613342285156
Batch 60/64 loss: -0.11109071969985962
Batch 61/64 loss: -0.1305813193321228
Batch 62/64 loss: -0.15310323238372803
Batch 63/64 loss: -0.095234215259552
Batch 64/64 loss: -0.1285933256149292
Epoch 363  Train loss: -0.1205814188601924  Val loss: 0.049983356212012955
Epoch 364
-------------------------------
Batch 1/64 loss: -0.10788071155548096
Batch 2/64 loss: -0.09486377239227295
Batch 3/64 loss: -0.1648540496826172
Batch 4/64 loss: -0.11576735973358154
Batch 5/64 loss: -0.12700748443603516
Batch 6/64 loss: -0.11374390125274658
Batch 7/64 loss: -0.13940143585205078
Batch 8/64 loss: -0.13063520193099976
Batch 9/64 loss: -0.10212182998657227
Batch 10/64 loss: -0.10191893577575684
Batch 11/64 loss: -0.12847900390625
Batch 12/64 loss: -0.15700680017471313
Batch 13/64 loss: -0.1251886487007141
Batch 14/64 loss: -0.10113167762756348
Batch 15/64 loss: -0.09616303443908691
Batch 16/64 loss: -0.1485951542854309
Batch 17/64 loss: -0.10853147506713867
Batch 18/64 loss: -0.12391942739486694
Batch 19/64 loss: -0.13854765892028809
Batch 20/64 loss: -0.10131031274795532
Batch 21/64 loss: -0.12618249654769897
Batch 22/64 loss: -0.09995567798614502
Batch 23/64 loss: -0.12972867488861084
Batch 24/64 loss: -0.09757852554321289
Batch 25/64 loss: -0.14424383640289307
Batch 26/64 loss: -0.13969218730926514
Batch 27/64 loss: -0.12695133686065674
Batch 28/64 loss: -0.1176874041557312
Batch 29/64 loss: -0.12217831611633301
Batch 30/64 loss: -0.10096877813339233
Batch 31/64 loss: -0.08808845281600952
Batch 32/64 loss: -0.14643311500549316
Batch 33/64 loss: -0.12989389896392822
Batch 34/64 loss: -0.14209192991256714
Batch 35/64 loss: -0.12291252613067627
Batch 36/64 loss: -0.12564212083816528
Batch 37/64 loss: -0.1353210210800171
Batch 38/64 loss: -0.13133639097213745
Batch 39/64 loss: -0.11980539560317993
Batch 40/64 loss: -0.10906362533569336
Batch 41/64 loss: -0.1299077272415161
Batch 42/64 loss: -0.1195724606513977
Batch 43/64 loss: -0.11169344186782837
Batch 44/64 loss: -0.12878912687301636
Batch 45/64 loss: -0.11800062656402588
Batch 46/64 loss: -0.09516936540603638
Batch 47/64 loss: -0.1205826997756958
Batch 48/64 loss: -0.10247832536697388
Batch 49/64 loss: -0.11939948797225952
Batch 50/64 loss: -0.11275631189346313
Batch 51/64 loss: -0.10687965154647827
Batch 52/64 loss: -0.11366778612136841
Batch 53/64 loss: -0.11922973394393921
Batch 54/64 loss: -0.09873616695404053
Batch 55/64 loss: -0.10986202955245972
Batch 56/64 loss: -0.10346132516860962
Batch 57/64 loss: -0.09657108783721924
Batch 58/64 loss: -0.11460036039352417
Batch 59/64 loss: -0.1269177794456482
Batch 60/64 loss: -0.11076384782791138
Batch 61/64 loss: -0.12368476390838623
Batch 62/64 loss: -0.12818175554275513
Batch 63/64 loss: -0.11942565441131592
Batch 64/64 loss: -0.15082693099975586
Epoch 364  Train loss: -0.11962784785850375  Val loss: 0.04966162221947896
Epoch 365
-------------------------------
Batch 1/64 loss: -0.12203150987625122
Batch 2/64 loss: -0.10922610759735107
Batch 3/64 loss: -0.1247519850730896
Batch 4/64 loss: -0.08538401126861572
Batch 5/64 loss: -0.09368735551834106
Batch 6/64 loss: -0.10346466302871704
Batch 7/64 loss: -0.11668682098388672
Batch 8/64 loss: -0.1146121621131897
Batch 9/64 loss: -0.11016654968261719
Batch 10/64 loss: -0.11226809024810791
Batch 11/64 loss: -0.13186615705490112
Batch 12/64 loss: -0.140089750289917
Batch 13/64 loss: -0.14748108386993408
Batch 14/64 loss: -0.08194202184677124
Batch 15/64 loss: -0.0835498571395874
Batch 16/64 loss: -0.09615147113800049
Batch 17/64 loss: -0.11593466997146606
Batch 18/64 loss: -0.1257396936416626
Batch 19/64 loss: -0.11585861444473267
Batch 20/64 loss: -0.09844046831130981
Batch 21/64 loss: -0.10921531915664673
Batch 22/64 loss: -0.13156986236572266
Batch 23/64 loss: -0.1249089241027832
Batch 24/64 loss: -0.12890714406967163
Batch 25/64 loss: -0.13299942016601562
Batch 26/64 loss: -0.1250239610671997
Batch 27/64 loss: -0.12157732248306274
Batch 28/64 loss: -0.10618412494659424
Batch 29/64 loss: -0.11007142066955566
Batch 30/64 loss: -0.11344790458679199
Batch 31/64 loss: -0.10885804891586304
Batch 32/64 loss: -0.16373056173324585
Batch 33/64 loss: -0.09948652982711792
Batch 34/64 loss: -0.12268275022506714
Batch 35/64 loss: -0.10022956132888794
Batch 36/64 loss: -0.14907914400100708
Batch 37/64 loss: -0.14643001556396484
Batch 38/64 loss: -0.12414848804473877
Batch 39/64 loss: -0.11652624607086182
Batch 40/64 loss: -0.1363964080810547
Batch 41/64 loss: -0.1100456714630127
Batch 42/64 loss: -0.1610894799232483
Batch 43/64 loss: -0.11747819185256958
Batch 44/64 loss: -0.15304136276245117
Batch 45/64 loss: -0.09966570138931274
Batch 46/64 loss: -0.1172437071800232
Batch 47/64 loss: -0.13183218240737915
Batch 48/64 loss: -0.12326627969741821
Batch 49/64 loss: -0.10643470287322998
Batch 50/64 loss: -0.1345464587211609
Batch 51/64 loss: -0.1309412717819214
Batch 52/64 loss: -0.08349251747131348
Batch 53/64 loss: -0.11165237426757812
Batch 54/64 loss: -0.1363379955291748
Batch 55/64 loss: -0.12836313247680664
Batch 56/64 loss: -0.1371251940727234
Batch 57/64 loss: -0.1346656084060669
Batch 58/64 loss: -0.12922918796539307
Batch 59/64 loss: -0.09323930740356445
Batch 60/64 loss: -0.13096344470977783
Batch 61/64 loss: -0.08280611038208008
Batch 62/64 loss: -0.09935164451599121
Batch 63/64 loss: -0.1287122368812561
Batch 64/64 loss: -0.12240397930145264
Epoch 365  Train loss: -0.11880992861355053  Val loss: 0.05091696322169091
Epoch 366
-------------------------------
Batch 1/64 loss: -0.13387739658355713
Batch 2/64 loss: -0.12618541717529297
Batch 3/64 loss: -0.11576741933822632
Batch 4/64 loss: -0.12347841262817383
Batch 5/64 loss: -0.1384875774383545
Batch 6/64 loss: -0.1512598991394043
Batch 7/64 loss: -0.13055062294006348
Batch 8/64 loss: -0.15523242950439453
Batch 9/64 loss: -0.12804675102233887
Batch 10/64 loss: -0.11917513608932495
Batch 11/64 loss: -0.12679129838943481
Batch 12/64 loss: -0.08340251445770264
Batch 13/64 loss: -0.1407104730606079
Batch 14/64 loss: -0.13097333908081055
Batch 15/64 loss: -0.13121473789215088
Batch 16/64 loss: -0.13404500484466553
Batch 17/64 loss: -0.12273609638214111
Batch 18/64 loss: -0.12909722328186035
Batch 19/64 loss: -0.13492882251739502
Batch 20/64 loss: -0.12105858325958252
Batch 21/64 loss: -0.15907371044158936
Batch 22/64 loss: -0.1034209132194519
Batch 23/64 loss: -0.12082016468048096
Batch 24/64 loss: -0.14716476202011108
Batch 25/64 loss: -0.12434351444244385
Batch 26/64 loss: -0.0807870626449585
Batch 27/64 loss: -0.12347930669784546
Batch 28/64 loss: -0.13298803567886353
Batch 29/64 loss: -0.12736499309539795
Batch 30/64 loss: -0.0925789475440979
Batch 31/64 loss: -0.11064553260803223
Batch 32/64 loss: -0.10820353031158447
Batch 33/64 loss: -0.11544150114059448
Batch 34/64 loss: -0.12365961074829102
Batch 35/64 loss: -0.11924272775650024
Batch 36/64 loss: -0.13473492860794067
Batch 37/64 loss: -0.1352900266647339
Batch 38/64 loss: -0.08687394857406616
Batch 39/64 loss: -0.12028360366821289
Batch 40/64 loss: -0.12780368328094482
Batch 41/64 loss: -0.12478351593017578
Batch 42/64 loss: -0.11891216039657593
Batch 43/64 loss: -0.09751272201538086
Batch 44/64 loss: -0.11299479007720947
Batch 45/64 loss: -0.12508684396743774
Batch 46/64 loss: -0.12064826488494873
Batch 47/64 loss: -0.13101506233215332
Batch 48/64 loss: -0.1471291184425354
Batch 49/64 loss: -0.11354595422744751
Batch 50/64 loss: -0.1319507360458374
Batch 51/64 loss: -0.09116756916046143
Batch 52/64 loss: -0.0990670919418335
Batch 53/64 loss: -0.09310013055801392
Batch 54/64 loss: -0.12700575590133667
Batch 55/64 loss: -0.12782108783721924
Batch 56/64 loss: -0.09705162048339844
Batch 57/64 loss: -0.11843377351760864
Batch 58/64 loss: -0.10825389623641968
Batch 59/64 loss: -0.09006071090698242
Batch 60/64 loss: -0.13478273153305054
Batch 61/64 loss: -0.12054455280303955
Batch 62/64 loss: -0.11268901824951172
Batch 63/64 loss: -0.13384854793548584
Batch 64/64 loss: -0.11889833211898804
Epoch 366  Train loss: -0.12137724020901848  Val loss: 0.04761286535623557
Epoch 367
-------------------------------
Batch 1/64 loss: -0.12918919324874878
Batch 2/64 loss: -0.10755586624145508
Batch 3/64 loss: -0.1106460690498352
Batch 4/64 loss: -0.13283419609069824
Batch 5/64 loss: -0.14997637271881104
Batch 6/64 loss: -0.13237279653549194
Batch 7/64 loss: -0.12030541896820068
Batch 8/64 loss: -0.14756101369857788
Batch 9/64 loss: -0.15492701530456543
Batch 10/64 loss: -0.1294052004814148
Batch 11/64 loss: -0.1329425573348999
Batch 12/64 loss: -0.127932608127594
Batch 13/64 loss: -0.08014726638793945
Batch 14/64 loss: -0.09945273399353027
Batch 15/64 loss: -0.10118895769119263
Batch 16/64 loss: -0.10843384265899658
Batch 17/64 loss: -0.1285584568977356
Batch 18/64 loss: -0.11754435300827026
Batch 19/64 loss: -0.14039063453674316
Batch 20/64 loss: -0.11452382802963257
Batch 21/64 loss: -0.12502795457839966
Batch 22/64 loss: -0.07696413993835449
Batch 23/64 loss: -0.12467265129089355
Batch 24/64 loss: -0.11395871639251709
Batch 25/64 loss: -0.11233019828796387
Batch 26/64 loss: -0.13388961553573608
Batch 27/64 loss: -0.13266503810882568
Batch 28/64 loss: -0.1311531662940979
Batch 29/64 loss: -0.11949354410171509
Batch 30/64 loss: -0.1250673532485962
Batch 31/64 loss: -0.14227187633514404
Batch 32/64 loss: -0.12278777360916138
Batch 33/64 loss: -0.13417279720306396
Batch 34/64 loss: -0.1327495574951172
Batch 35/64 loss: -0.11978548765182495
Batch 36/64 loss: -0.1268671154975891
Batch 37/64 loss: -0.10006058216094971
Batch 38/64 loss: -0.14706885814666748
Batch 39/64 loss: -0.13975191116333008
Batch 40/64 loss: -0.12843775749206543
Batch 41/64 loss: -0.1190328598022461
Batch 42/64 loss: -0.09826040267944336
Batch 43/64 loss: -0.10698169469833374
Batch 44/64 loss: -0.12525951862335205
Batch 45/64 loss: -0.11160814762115479
Batch 46/64 loss: -0.1360366940498352
Batch 47/64 loss: -0.10891413688659668
Batch 48/64 loss: -0.12736248970031738
Batch 49/64 loss: -0.08781087398529053
Batch 50/64 loss: -0.12396979331970215
Batch 51/64 loss: -0.11824154853820801
Batch 52/64 loss: -0.12930500507354736
Batch 53/64 loss: -0.1157369613647461
Batch 54/64 loss: -0.12862426042556763
Batch 55/64 loss: -0.12210428714752197
Batch 56/64 loss: -0.1396384835243225
Batch 57/64 loss: -0.09122258424758911
Batch 58/64 loss: -0.11769717931747437
Batch 59/64 loss: -0.13533204793930054
Batch 60/64 loss: -0.11932647228240967
Batch 61/64 loss: -0.13744813203811646
Batch 62/64 loss: -0.11887311935424805
Batch 63/64 loss: -0.10901021957397461
Batch 64/64 loss: -0.0915597677230835
Epoch 367  Train loss: -0.12159264648661894  Val loss: 0.051568691468320764
Epoch 368
-------------------------------
Batch 1/64 loss: -0.09552383422851562
Batch 2/64 loss: -0.10794645547866821
Batch 3/64 loss: -0.11635541915893555
Batch 4/64 loss: -0.10046672821044922
Batch 5/64 loss: -0.1026463508605957
Batch 6/64 loss: -0.1082618236541748
Batch 7/64 loss: -0.11017447710037231
Batch 8/64 loss: -0.11091578006744385
Batch 9/64 loss: -0.10627567768096924
Batch 10/64 loss: -0.11654698848724365
Batch 11/64 loss: -0.1334245204925537
Batch 12/64 loss: -0.1119389533996582
Batch 13/64 loss: -0.125499427318573
Batch 14/64 loss: -0.09636002779006958
Batch 15/64 loss: -0.11290502548217773
Batch 16/64 loss: -0.11162984371185303
Batch 17/64 loss: -0.09213632345199585
Batch 18/64 loss: -0.09236699342727661
Batch 19/64 loss: -0.12389791011810303
Batch 20/64 loss: -0.1466430425643921
Batch 21/64 loss: -0.125091552734375
Batch 22/64 loss: -0.10487687587738037
Batch 23/64 loss: -0.13347190618515015
Batch 24/64 loss: -0.08476191759109497
Batch 25/64 loss: -0.14340639114379883
Batch 26/64 loss: -0.10251253843307495
Batch 27/64 loss: -0.14182430505752563
Batch 28/64 loss: -0.14476442337036133
Batch 29/64 loss: -0.12913745641708374
Batch 30/64 loss: -0.1335080862045288
Batch 31/64 loss: -0.11720937490463257
Batch 32/64 loss: -0.12688660621643066
Batch 33/64 loss: -0.12039905786514282
Batch 34/64 loss: -0.09759891033172607
Batch 35/64 loss: -0.10947901010513306
Batch 36/64 loss: -0.12984752655029297
Batch 37/64 loss: -0.13862347602844238
Batch 38/64 loss: -0.12077301740646362
Batch 39/64 loss: -0.1529713273048401
Batch 40/64 loss: -0.10581868886947632
Batch 41/64 loss: -0.11399167776107788
Batch 42/64 loss: -0.10531747341156006
Batch 43/64 loss: -0.12789636850357056
Batch 44/64 loss: -0.13216030597686768
Batch 45/64 loss: -0.12747043371200562
Batch 46/64 loss: -0.08794933557510376
Batch 47/64 loss: -0.1327776312828064
Batch 48/64 loss: -0.10067015886306763
Batch 49/64 loss: -0.12219434976577759
Batch 50/64 loss: -0.1178702712059021
Batch 51/64 loss: -0.09653478860855103
Batch 52/64 loss: -0.14217710494995117
Batch 53/64 loss: -0.10673624277114868
Batch 54/64 loss: -0.10673809051513672
Batch 55/64 loss: -0.135542094707489
Batch 56/64 loss: -0.15015387535095215
Batch 57/64 loss: -0.14625781774520874
Batch 58/64 loss: -0.138508141040802
Batch 59/64 loss: -0.1350616216659546
Batch 60/64 loss: -0.0881730318069458
Batch 61/64 loss: -0.08325690031051636
Batch 62/64 loss: -0.15147477388381958
Batch 63/64 loss: -0.14088737964630127
Batch 64/64 loss: -0.10107249021530151
Epoch 368  Train loss: -0.11843893785102695  Val loss: 0.048563099399055405
Epoch 369
-------------------------------
Batch 1/64 loss: -0.15232473611831665
Batch 2/64 loss: -0.13357847929000854
Batch 3/64 loss: -0.13054364919662476
Batch 4/64 loss: -0.1162375807762146
Batch 5/64 loss: -0.12993019819259644
Batch 6/64 loss: -0.11137795448303223
Batch 7/64 loss: -0.1327657699584961
Batch 8/64 loss: -0.1311318278312683
Batch 9/64 loss: -0.14210456609725952
Batch 10/64 loss: -0.1260421872138977
Batch 11/64 loss: -0.1227414608001709
Batch 12/64 loss: -0.09865033626556396
Batch 13/64 loss: -0.12104558944702148
Batch 14/64 loss: -0.1392834186553955
Batch 15/64 loss: -0.07680380344390869
Batch 16/64 loss: -0.09625846147537231
Batch 17/64 loss: -0.12963223457336426
Batch 18/64 loss: -0.10114085674285889
Batch 19/64 loss: -0.11725854873657227
Batch 20/64 loss: -0.11754500865936279
Batch 21/64 loss: -0.1095658540725708
Batch 22/64 loss: -0.10340654850006104
Batch 23/64 loss: -0.11696910858154297
Batch 24/64 loss: -0.10553145408630371
Batch 25/64 loss: -0.12282651662826538
Batch 26/64 loss: -0.15148472785949707
Batch 27/64 loss: -0.12724155187606812
Batch 28/64 loss: -0.1544402837753296
Batch 29/64 loss: -0.12102377414703369
Batch 30/64 loss: -0.12692290544509888
Batch 31/64 loss: -0.10498213768005371
Batch 32/64 loss: -0.12699568271636963
Batch 33/64 loss: -0.10893207788467407
Batch 34/64 loss: -0.1623876690864563
Batch 35/64 loss: -0.13076329231262207
Batch 36/64 loss: -0.12406176328659058
Batch 37/64 loss: -0.11874556541442871
Batch 38/64 loss: -0.08166223764419556
Batch 39/64 loss: -0.0877685546875
Batch 40/64 loss: -0.1242133378982544
Batch 41/64 loss: -0.1009858250617981
Batch 42/64 loss: -0.1259251832962036
Batch 43/64 loss: -0.13344991207122803
Batch 44/64 loss: -0.16086912155151367
Batch 45/64 loss: -0.11980926990509033
Batch 46/64 loss: -0.13997101783752441
Batch 47/64 loss: -0.11810600757598877
Batch 48/64 loss: -0.102958083152771
Batch 49/64 loss: -0.13136959075927734
Batch 50/64 loss: -0.09707993268966675
Batch 51/64 loss: -0.11934512853622437
Batch 52/64 loss: -0.11907798051834106
Batch 53/64 loss: -0.1450127363204956
Batch 54/64 loss: -0.0895075798034668
Batch 55/64 loss: -0.0872085690498352
Batch 56/64 loss: -0.13498282432556152
Batch 57/64 loss: -0.13908714056015015
Batch 58/64 loss: -0.12404507398605347
Batch 59/64 loss: -0.13015156984329224
Batch 60/64 loss: -0.09715521335601807
Batch 61/64 loss: -0.11837482452392578
Batch 62/64 loss: -0.14703208208084106
Batch 63/64 loss: -0.1295616626739502
Batch 64/64 loss: -0.10806053876876831
Epoch 369  Train loss: -0.12123032855052573  Val loss: 0.04826991787481144
Epoch 370
-------------------------------
Batch 1/64 loss: -0.14228767156600952
Batch 2/64 loss: -0.12623202800750732
Batch 3/64 loss: -0.15480220317840576
Batch 4/64 loss: -0.12350702285766602
Batch 5/64 loss: -0.12128043174743652
Batch 6/64 loss: -0.14758223295211792
Batch 7/64 loss: -0.1302623152732849
Batch 8/64 loss: -0.09471189975738525
Batch 9/64 loss: -0.12610512971878052
Batch 10/64 loss: -0.13568508625030518
Batch 11/64 loss: -0.10768020153045654
Batch 12/64 loss: -0.12007570266723633
Batch 13/64 loss: -0.08831560611724854
Batch 14/64 loss: -0.13663560152053833
Batch 15/64 loss: -0.10049176216125488
Batch 16/64 loss: -0.15097439289093018
Batch 17/64 loss: -0.14388233423233032
Batch 18/64 loss: -0.11389654874801636
Batch 19/64 loss: -0.09734398126602173
Batch 20/64 loss: -0.10958319902420044
Batch 21/64 loss: -0.1037517786026001
Batch 22/64 loss: -0.12806612253189087
Batch 23/64 loss: -0.12448793649673462
Batch 24/64 loss: -0.11294019222259521
Batch 25/64 loss: -0.1065017580986023
Batch 26/64 loss: -0.1269022822380066
Batch 27/64 loss: -0.13580530881881714
Batch 28/64 loss: -0.12808126211166382
Batch 29/64 loss: -0.10700547695159912
Batch 30/64 loss: -0.1067458987236023
Batch 31/64 loss: -0.1253107190132141
Batch 32/64 loss: -0.13816481828689575
Batch 33/64 loss: -0.12700670957565308
Batch 34/64 loss: -0.0897982120513916
Batch 35/64 loss: -0.12682849168777466
Batch 36/64 loss: -0.10535752773284912
Batch 37/64 loss: -0.11954635381698608
Batch 38/64 loss: -0.14577656984329224
Batch 39/64 loss: -0.1155131459236145
Batch 40/64 loss: -0.12166446447372437
Batch 41/64 loss: -0.11891728639602661
Batch 42/64 loss: -0.12219321727752686
Batch 43/64 loss: -0.1114206314086914
Batch 44/64 loss: -0.11150044202804565
Batch 45/64 loss: -0.10407048463821411
Batch 46/64 loss: -0.11593902111053467
Batch 47/64 loss: -0.108833909034729
Batch 48/64 loss: -0.10924607515335083
Batch 49/64 loss: -0.14331984519958496
Batch 50/64 loss: -0.10361653566360474
Batch 51/64 loss: -0.11678802967071533
Batch 52/64 loss: -0.1542789340019226
Batch 53/64 loss: -0.11377227306365967
Batch 54/64 loss: -0.10010921955108643
Batch 55/64 loss: -0.12348759174346924
Batch 56/64 loss: -0.130499005317688
Batch 57/64 loss: -0.1306217908859253
Batch 58/64 loss: -0.1366039514541626
Batch 59/64 loss: -0.1235131025314331
Batch 60/64 loss: -0.12185239791870117
Batch 61/64 loss: -0.11834919452667236
Batch 62/64 loss: -0.1408478021621704
Batch 63/64 loss: -0.12492775917053223
Batch 64/64 loss: -0.1344878077507019
Epoch 370  Train loss: -0.12160258410023708  Val loss: 0.047219003598714615
Epoch 371
-------------------------------
Batch 1/64 loss: -0.10912072658538818
Batch 2/64 loss: -0.14259785413742065
Batch 3/64 loss: -0.11680185794830322
Batch 4/64 loss: -0.09683763980865479
Batch 5/64 loss: -0.14274954795837402
Batch 6/64 loss: -0.12197154760360718
Batch 7/64 loss: -0.11428916454315186
Batch 8/64 loss: -0.15041089057922363
Batch 9/64 loss: -0.11645090579986572
Batch 10/64 loss: -0.13984203338623047
Batch 11/64 loss: -0.10249489545822144
Batch 12/64 loss: -0.11229288578033447
Batch 13/64 loss: -0.0908326506614685
Batch 14/64 loss: -0.08724600076675415
Batch 15/64 loss: -0.1338118314743042
Batch 16/64 loss: -0.11690253019332886
Batch 17/64 loss: -0.13879412412643433
Batch 18/64 loss: -0.1306307315826416
Batch 19/64 loss: -0.12643396854400635
Batch 20/64 loss: -0.14588779211044312
Batch 21/64 loss: -0.14839863777160645
Batch 22/64 loss: -0.12062078714370728
Batch 23/64 loss: -0.08437716960906982
Batch 24/64 loss: -0.10739105939865112
Batch 25/64 loss: -0.10669779777526855
Batch 26/64 loss: -0.11634266376495361
Batch 27/64 loss: -0.1050226092338562
Batch 28/64 loss: -0.11042046546936035
Batch 29/64 loss: -0.11705678701400757
Batch 30/64 loss: -0.10186272859573364
Batch 31/64 loss: -0.12721902132034302
Batch 32/64 loss: -0.11790251731872559
Batch 33/64 loss: -0.1375129222869873
Batch 34/64 loss: -0.12335056066513062
Batch 35/64 loss: -0.11501413583755493
Batch 36/64 loss: -0.1267145872116089
Batch 37/64 loss: -0.11531257629394531
Batch 38/64 loss: -0.10097569227218628
Batch 39/64 loss: -0.11407727003097534
Batch 40/64 loss: -0.11550343036651611
Batch 41/64 loss: -0.12007880210876465
Batch 42/64 loss: -0.13889217376708984
Batch 43/64 loss: -0.11812746524810791
Batch 44/64 loss: -0.11217403411865234
Batch 45/64 loss: -0.12254571914672852
Batch 46/64 loss: -0.14004117250442505
Batch 47/64 loss: -0.1220407485961914
Batch 48/64 loss: -0.1058652400970459
Batch 49/64 loss: -0.11950957775115967
Batch 50/64 loss: -0.1216583251953125
Batch 51/64 loss: -0.1515575647354126
Batch 52/64 loss: -0.09998148679733276
Batch 53/64 loss: -0.11741936206817627
Batch 54/64 loss: -0.14239132404327393
Batch 55/64 loss: -0.1359073519706726
Batch 56/64 loss: -0.10828256607055664
Batch 57/64 loss: -0.11136335134506226
Batch 58/64 loss: -0.09163767099380493
Batch 59/64 loss: -0.1497902274131775
Batch 60/64 loss: -0.15500211715698242
Batch 61/64 loss: -0.13291329145431519
Batch 62/64 loss: -0.10527396202087402
Batch 63/64 loss: -0.12440574169158936
Batch 64/64 loss: -0.12261563539505005
Epoch 371  Train loss: -0.12058029805912691  Val loss: 0.05151187431361667
Epoch 372
-------------------------------
Batch 1/64 loss: -0.1300649642944336
Batch 2/64 loss: -0.10841339826583862
Batch 3/64 loss: -0.1290406584739685
Batch 4/64 loss: -0.14469456672668457
Batch 5/64 loss: -0.11229968070983887
Batch 6/64 loss: -0.12804484367370605
Batch 7/64 loss: -0.12797725200653076
Batch 8/64 loss: -0.1380138397216797
Batch 9/64 loss: -0.09336459636688232
Batch 10/64 loss: -0.13442182540893555
Batch 11/64 loss: -0.15071016550064087
Batch 12/64 loss: -0.12705695629119873
Batch 13/64 loss: -0.10856485366821289
Batch 14/64 loss: -0.11750906705856323
Batch 15/64 loss: -0.1222795844078064
Batch 16/64 loss: -0.11458677053451538
Batch 17/64 loss: -0.13088780641555786
Batch 18/64 loss: -0.14549946784973145
Batch 19/64 loss: -0.12477177381515503
Batch 20/64 loss: -0.1383724808692932
Batch 21/64 loss: -0.11593401432037354
Batch 22/64 loss: -0.10645151138305664
Batch 23/64 loss: -0.14831578731536865
Batch 24/64 loss: -0.1268177032470703
Batch 25/64 loss: -0.12487339973449707
Batch 26/64 loss: -0.10888040065765381
Batch 27/64 loss: -0.12553542852401733
Batch 28/64 loss: -0.11480987071990967
Batch 29/64 loss: -0.11821764707565308
Batch 30/64 loss: -0.10313665866851807
Batch 31/64 loss: -0.14154082536697388
Batch 32/64 loss: -0.0982937216758728
Batch 33/64 loss: -0.11751651763916016
Batch 34/64 loss: -0.13329875469207764
Batch 35/64 loss: -0.14576923847198486
Batch 36/64 loss: -0.12196582555770874
Batch 37/64 loss: -0.14382505416870117
Batch 38/64 loss: -0.13378220796585083
Batch 39/64 loss: -0.1137535572052002
Batch 40/64 loss: -0.12072718143463135
Batch 41/64 loss: -0.12910974025726318
Batch 42/64 loss: -0.10918396711349487
Batch 43/64 loss: -0.12427413463592529
Batch 44/64 loss: -0.13344240188598633
Batch 45/64 loss: -0.08994323015213013
Batch 46/64 loss: -0.11513006687164307
Batch 47/64 loss: -0.11854171752929688
Batch 48/64 loss: -0.09168899059295654
Batch 49/64 loss: -0.11917901039123535
Batch 50/64 loss: -0.147427499294281
Batch 51/64 loss: -0.12263691425323486
Batch 52/64 loss: -0.13714349269866943
Batch 53/64 loss: -0.1416383981704712
Batch 54/64 loss: -0.1200074553489685
Batch 55/64 loss: -0.11318373680114746
Batch 56/64 loss: -0.09557950496673584
Batch 57/64 loss: -0.1283888816833496
Batch 58/64 loss: -0.1284075379371643
Batch 59/64 loss: -0.13347309827804565
Batch 60/64 loss: -0.1334742307662964
Batch 61/64 loss: -0.1212502121925354
Batch 62/64 loss: -0.08033514022827148
Batch 63/64 loss: -0.10992473363876343
Batch 64/64 loss: -0.11188209056854248
Epoch 372  Train loss: -0.12262424347447413  Val loss: 0.04845158279556589
Epoch 373
-------------------------------
Batch 1/64 loss: -0.13553190231323242
Batch 2/64 loss: -0.14986926317214966
Batch 3/64 loss: -0.16123491525650024
Batch 4/64 loss: -0.13922935724258423
Batch 5/64 loss: -0.11381316184997559
Batch 6/64 loss: -0.12206381559371948
Batch 7/64 loss: -0.147186279296875
Batch 8/64 loss: -0.1600533127784729
Batch 9/64 loss: -0.10776019096374512
Batch 10/64 loss: -0.12683367729187012
Batch 11/64 loss: -0.07739019393920898
Batch 12/64 loss: -0.1237296462059021
Batch 13/64 loss: -0.1472792625427246
Batch 14/64 loss: -0.1397266983985901
Batch 15/64 loss: -0.11644256114959717
Batch 16/64 loss: -0.1348400115966797
Batch 17/64 loss: -0.13344717025756836
Batch 18/64 loss: -0.13232260942459106
Batch 19/64 loss: -0.1498003602027893
Batch 20/64 loss: -0.14470601081848145
Batch 21/64 loss: -0.11957544088363647
Batch 22/64 loss: -0.12472802400588989
Batch 23/64 loss: -0.11838889122009277
Batch 24/64 loss: -0.1244698166847229
Batch 25/64 loss: -0.12576258182525635
Batch 26/64 loss: -0.11140942573547363
Batch 27/64 loss: -0.10800886154174805
Batch 28/64 loss: -0.11699843406677246
Batch 29/64 loss: -0.11727261543273926
Batch 30/64 loss: -0.1373305320739746
Batch 31/64 loss: -0.11088180541992188
Batch 32/64 loss: -0.06045937538146973
Batch 33/64 loss: -0.12047022581100464
Batch 34/64 loss: -0.15333348512649536
Batch 35/64 loss: -0.12489157915115356
Batch 36/64 loss: -0.13104581832885742
Batch 37/64 loss: -0.13973993062973022
Batch 38/64 loss: -0.1580182909965515
Batch 39/64 loss: -0.11187970638275146
Batch 40/64 loss: -0.13479995727539062
Batch 41/64 loss: -0.11818760633468628
Batch 42/64 loss: -0.0953325629234314
Batch 43/64 loss: -0.09593909978866577
Batch 44/64 loss: -0.10538429021835327
Batch 45/64 loss: -0.13326239585876465
Batch 46/64 loss: -0.11276400089263916
Batch 47/64 loss: -0.1235121488571167
Batch 48/64 loss: -0.1212383508682251
Batch 49/64 loss: -0.1295962929725647
Batch 50/64 loss: -0.130998432636261
Batch 51/64 loss: -0.1245008111000061
Batch 52/64 loss: -0.12035655975341797
Batch 53/64 loss: -0.12882882356643677
Batch 54/64 loss: -0.10614275932312012
Batch 55/64 loss: -0.13337767124176025
Batch 56/64 loss: -0.1076275110244751
Batch 57/64 loss: -0.126051127910614
Batch 58/64 loss: -0.12385034561157227
Batch 59/64 loss: -0.12520134449005127
Batch 60/64 loss: -0.11263447999954224
Batch 61/64 loss: -0.1153382658958435
Batch 62/64 loss: -0.10663145780563354
Batch 63/64 loss: -0.13064169883728027
Batch 64/64 loss: -0.12138104438781738
Epoch 373  Train loss: -0.1244103375603171  Val loss: 0.05072390254830167
Epoch 374
-------------------------------
Batch 1/64 loss: -0.1353929042816162
Batch 2/64 loss: -0.12543821334838867
Batch 3/64 loss: -0.13510727882385254
Batch 4/64 loss: -0.1489519476890564
Batch 5/64 loss: -0.13762950897216797
Batch 6/64 loss: -0.1338779330253601
Batch 7/64 loss: -0.14405035972595215
Batch 8/64 loss: -0.08960902690887451
Batch 9/64 loss: -0.08992856740951538
Batch 10/64 loss: -0.13309800624847412
Batch 11/64 loss: -0.14321857690811157
Batch 12/64 loss: -0.14452487230300903
Batch 13/64 loss: -0.12187027931213379
Batch 14/64 loss: -0.12050461769104004
Batch 15/64 loss: -0.11794060468673706
Batch 16/64 loss: -0.13401442766189575
Batch 17/64 loss: -0.1252921223640442
Batch 18/64 loss: -0.1297445297241211
Batch 19/64 loss: -0.13978654146194458
Batch 20/64 loss: -0.13287264108657837
Batch 21/64 loss: -0.10895699262619019
Batch 22/64 loss: -0.10467827320098877
Batch 23/64 loss: -0.1261020302772522
Batch 24/64 loss: -0.13506585359573364
Batch 25/64 loss: -0.12907660007476807
Batch 26/64 loss: -0.11544710397720337
Batch 27/64 loss: -0.10285139083862305
Batch 28/64 loss: -0.11255526542663574
Batch 29/64 loss: -0.10604923963546753
Batch 30/64 loss: -0.12420457601547241
Batch 31/64 loss: -0.10529565811157227
Batch 32/64 loss: -0.12363040447235107
Batch 33/64 loss: -0.10997432470321655
Batch 34/64 loss: -0.10855990648269653
Batch 35/64 loss: -0.14012789726257324
Batch 36/64 loss: -0.12563520669937134
Batch 37/64 loss: -0.13284432888031006
Batch 38/64 loss: -0.13306576013565063
Batch 39/64 loss: -0.14393115043640137
Batch 40/64 loss: -0.12036216259002686
Batch 41/64 loss: -0.1083306074142456
Batch 42/64 loss: -0.09249937534332275
Batch 43/64 loss: -0.11090308427810669
Batch 44/64 loss: -0.11777633428573608
Batch 45/64 loss: -0.1408492922782898
Batch 46/64 loss: -0.13131916522979736
Batch 47/64 loss: -0.09785395860671997
Batch 48/64 loss: -0.08797228336334229
Batch 49/64 loss: -0.13470417261123657
Batch 50/64 loss: -0.12688100337982178
Batch 51/64 loss: -0.11164110898971558
Batch 52/64 loss: -0.1311071515083313
Batch 53/64 loss: -0.10905373096466064
Batch 54/64 loss: -0.10474824905395508
Batch 55/64 loss: -0.11639583110809326
Batch 56/64 loss: -0.13025104999542236
Batch 57/64 loss: -0.0915101170539856
Batch 58/64 loss: -0.12505042552947998
Batch 59/64 loss: -0.11461293697357178
Batch 60/64 loss: -0.11771655082702637
Batch 61/64 loss: -0.11366438865661621
Batch 62/64 loss: -0.1278364658355713
Batch 63/64 loss: -0.14022505283355713
Batch 64/64 loss: -0.12180840969085693
Epoch 374  Train loss: -0.12181216828963336  Val loss: 0.04804358121865394
Epoch 375
-------------------------------
Batch 1/64 loss: -0.10520195960998535
Batch 2/64 loss: -0.13820946216583252
Batch 3/64 loss: -0.13791030645370483
Batch 4/64 loss: -0.1255074143409729
Batch 5/64 loss: -0.1385939121246338
Batch 6/64 loss: -0.13440585136413574
Batch 7/64 loss: -0.15212100744247437
Batch 8/64 loss: -0.10423451662063599
Batch 9/64 loss: -0.1427266001701355
Batch 10/64 loss: -0.10925376415252686
Batch 11/64 loss: -0.11975687742233276
Batch 12/64 loss: -0.1132349967956543
Batch 13/64 loss: -0.16278374195098877
Batch 14/64 loss: -0.1338443160057068
Batch 15/64 loss: -0.12395906448364258
Batch 16/64 loss: -0.08451211452484131
Batch 17/64 loss: -0.1195075511932373
Batch 18/64 loss: -0.12863361835479736
Batch 19/64 loss: -0.09420168399810791
Batch 20/64 loss: -0.1270090937614441
Batch 21/64 loss: -0.14117002487182617
Batch 22/64 loss: -0.126056969165802
Batch 23/64 loss: -0.13915151357650757
Batch 24/64 loss: -0.1270543336868286
Batch 25/64 loss: -0.10272657871246338
Batch 26/64 loss: -0.13078105449676514
Batch 27/64 loss: -0.14903557300567627
Batch 28/64 loss: -0.12749260663986206
Batch 29/64 loss: -0.11396104097366333
Batch 30/64 loss: -0.12813067436218262
Batch 31/64 loss: -0.11691731214523315
Batch 32/64 loss: -0.11167389154434204
Batch 33/64 loss: -0.11676973104476929
Batch 34/64 loss: -0.13770437240600586
Batch 35/64 loss: -0.12147259712219238
Batch 36/64 loss: -0.11973392963409424
Batch 37/64 loss: -0.1250147819519043
Batch 38/64 loss: -0.1144610047340393
Batch 39/64 loss: -0.12549680471420288
Batch 40/64 loss: -0.11837804317474365
Batch 41/64 loss: -0.12489902973175049
Batch 42/64 loss: -0.11239928007125854
Batch 43/64 loss: -0.13941723108291626
Batch 44/64 loss: -0.1354641318321228
Batch 45/64 loss: -0.08219045400619507
Batch 46/64 loss: -0.11943358182907104
Batch 47/64 loss: -0.1467902660369873
Batch 48/64 loss: -0.1267576813697815
Batch 49/64 loss: -0.134965717792511
Batch 50/64 loss: -0.13534748554229736
Batch 51/64 loss: -0.12477701902389526
Batch 52/64 loss: -0.13009554147720337
Batch 53/64 loss: -0.11206632852554321
Batch 54/64 loss: -0.11374467611312866
Batch 55/64 loss: -0.13665097951889038
Batch 56/64 loss: -0.11915671825408936
Batch 57/64 loss: -0.12955421209335327
Batch 58/64 loss: -0.13649117946624756
Batch 59/64 loss: -0.13154077529907227
Batch 60/64 loss: -0.09473633766174316
Batch 61/64 loss: -0.12249696254730225
Batch 62/64 loss: -0.11137926578521729
Batch 63/64 loss: -0.11037206649780273
Batch 64/64 loss: -0.1399288773536682
Epoch 375  Train loss: -0.12430532188976512  Val loss: 0.04916406660964809
Epoch 376
-------------------------------
Batch 1/64 loss: -0.13523077964782715
Batch 2/64 loss: -0.11271464824676514
Batch 3/64 loss: -0.12257599830627441
Batch 4/64 loss: -0.1618213653564453
Batch 5/64 loss: -0.13797414302825928
Batch 6/64 loss: -0.16148591041564941
Batch 7/64 loss: -0.14635980129241943
Batch 8/64 loss: -0.12794876098632812
Batch 9/64 loss: -0.11643445491790771
Batch 10/64 loss: -0.13234877586364746
Batch 11/64 loss: -0.16173648834228516
Batch 12/64 loss: -0.12147223949432373
Batch 13/64 loss: -0.13277822732925415
Batch 14/64 loss: -0.13442867994308472
Batch 15/64 loss: -0.1527819037437439
Batch 16/64 loss: -0.11197590827941895
Batch 17/64 loss: -0.13901078701019287
Batch 18/64 loss: -0.13235431909561157
Batch 19/64 loss: -0.11517220735549927
Batch 20/64 loss: -0.11994290351867676
Batch 21/64 loss: -0.11492151021957397
Batch 22/64 loss: -0.14443212747573853
Batch 23/64 loss: -0.09317255020141602
Batch 24/64 loss: -0.13355767726898193
Batch 25/64 loss: -0.11609476804733276
Batch 26/64 loss: -0.14549285173416138
Batch 27/64 loss: -0.12632393836975098
Batch 28/64 loss: -0.09849041700363159
Batch 29/64 loss: -0.12074798345565796
Batch 30/64 loss: -0.12398797273635864
Batch 31/64 loss: -0.13230657577514648
Batch 32/64 loss: -0.13388961553573608
Batch 33/64 loss: -0.12461298704147339
Batch 34/64 loss: -0.08494603633880615
Batch 35/64 loss: -0.12859320640563965
Batch 36/64 loss: -0.1172056794166565
Batch 37/64 loss: -0.13568216562271118
Batch 38/64 loss: -0.1229749321937561
Batch 39/64 loss: -0.14052528142929077
Batch 40/64 loss: -0.1252138614654541
Batch 41/64 loss: -0.12301671504974365
Batch 42/64 loss: -0.13449543714523315
Batch 43/64 loss: -0.11481016874313354
Batch 44/64 loss: -0.1320914626121521
Batch 45/64 loss: -0.14041805267333984
Batch 46/64 loss: -0.10566765069961548
Batch 47/64 loss: -0.11709070205688477
Batch 48/64 loss: -0.11067306995391846
Batch 49/64 loss: -0.12512540817260742
Batch 50/64 loss: -0.10654342174530029
Batch 51/64 loss: -0.11041676998138428
Batch 52/64 loss: -0.12562888860702515
Batch 53/64 loss: -0.1063995361328125
Batch 54/64 loss: -0.1228264570236206
Batch 55/64 loss: -0.1467549204826355
Batch 56/64 loss: -0.10240375995635986
Batch 57/64 loss: -0.1265297532081604
Batch 58/64 loss: -0.13868218660354614
Batch 59/64 loss: -0.10813981294631958
Batch 60/64 loss: -0.12758207321166992
Batch 61/64 loss: -0.12059640884399414
Batch 62/64 loss: -0.09053599834442139
Batch 63/64 loss: -0.12034660577774048
Batch 64/64 loss: -0.12868624925613403
Epoch 376  Train loss: -0.12538055508744483  Val loss: 0.04797446133754507
Epoch 377
-------------------------------
Batch 1/64 loss: -0.0747489333152771
Batch 2/64 loss: -0.1222527027130127
Batch 3/64 loss: -0.15673679113388062
Batch 4/64 loss: -0.12407535314559937
Batch 5/64 loss: -0.14446306228637695
Batch 6/64 loss: -0.14007568359375
Batch 7/64 loss: -0.10283946990966797
Batch 8/64 loss: -0.13195884227752686
Batch 9/64 loss: -0.0965232253074646
Batch 10/64 loss: -0.13026976585388184
Batch 11/64 loss: -0.1284453272819519
Batch 12/64 loss: -0.1322115659713745
Batch 13/64 loss: -0.1327720284461975
Batch 14/64 loss: -0.09709417819976807
Batch 15/64 loss: -0.10577082633972168
Batch 16/64 loss: -0.15157294273376465
Batch 17/64 loss: -0.12290263175964355
Batch 18/64 loss: -0.13539904356002808
Batch 19/64 loss: -0.14898115396499634
Batch 20/64 loss: -0.10575008392333984
Batch 21/64 loss: -0.13494712114334106
Batch 22/64 loss: -0.12040621042251587
Batch 23/64 loss: -0.13164103031158447
Batch 24/64 loss: -0.14654850959777832
Batch 25/64 loss: -0.13186317682266235
Batch 26/64 loss: -0.10417026281356812
Batch 27/64 loss: -0.14236187934875488
Batch 28/64 loss: -0.1415078043937683
Batch 29/64 loss: -0.13144594430923462
Batch 30/64 loss: -0.09764879941940308
Batch 31/64 loss: -0.12308412790298462
Batch 32/64 loss: -0.1454404592514038
Batch 33/64 loss: -0.1661895513534546
Batch 34/64 loss: -0.13123387098312378
Batch 35/64 loss: -0.06957852840423584
Batch 36/64 loss: -0.13275814056396484
Batch 37/64 loss: -0.08936995267868042
Batch 38/64 loss: -0.12867951393127441
Batch 39/64 loss: -0.13879799842834473
Batch 40/64 loss: -0.11057960987091064
Batch 41/64 loss: -0.1174662709236145
Batch 42/64 loss: -0.14088916778564453
Batch 43/64 loss: -0.122214674949646
Batch 44/64 loss: -0.15468311309814453
Batch 45/64 loss: -0.09320181608200073
Batch 46/64 loss: -0.11820334196090698
Batch 47/64 loss: -0.10952025651931763
Batch 48/64 loss: -0.13865256309509277
Batch 49/64 loss: -0.11508899927139282
Batch 50/64 loss: -0.15118688344955444
Batch 51/64 loss: -0.1171339750289917
Batch 52/64 loss: -0.1143541932106018
Batch 53/64 loss: -0.13991379737854004
Batch 54/64 loss: -0.15068191289901733
Batch 55/64 loss: -0.14374125003814697
Batch 56/64 loss: -0.13144594430923462
Batch 57/64 loss: -0.08416181802749634
Batch 58/64 loss: -0.10819977521896362
Batch 59/64 loss: -0.11393409967422485
Batch 60/64 loss: -0.13326966762542725
Batch 61/64 loss: -0.14263325929641724
Batch 62/64 loss: -0.11735838651657104
Batch 63/64 loss: -0.12128031253814697
Batch 64/64 loss: -0.13179230690002441
Epoch 377  Train loss: -0.12519460098416196  Val loss: 0.049103673995565304
Epoch 378
-------------------------------
Batch 1/64 loss: -0.11894911527633667
Batch 2/64 loss: -0.12884521484375
Batch 3/64 loss: -0.10634660720825195
Batch 4/64 loss: -0.11815547943115234
Batch 5/64 loss: -0.12732398509979248
Batch 6/64 loss: -0.15147840976715088
Batch 7/64 loss: -0.10588300228118896
Batch 8/64 loss: -0.11698806285858154
Batch 9/64 loss: -0.1449185609817505
Batch 10/64 loss: -0.1476472020149231
Batch 11/64 loss: -0.1329197883605957
Batch 12/64 loss: -0.1495354175567627
Batch 13/64 loss: -0.10009872913360596
Batch 14/64 loss: -0.11448615789413452
Batch 15/64 loss: -0.13731110095977783
Batch 16/64 loss: -0.12595593929290771
Batch 17/64 loss: -0.13197487592697144
Batch 18/64 loss: -0.11843276023864746
Batch 19/64 loss: -0.10182887315750122
Batch 20/64 loss: -0.125224769115448
Batch 21/64 loss: -0.11769664287567139
Batch 22/64 loss: -0.13617926836013794
Batch 23/64 loss: -0.12932831048965454
Batch 24/64 loss: -0.10947787761688232
Batch 25/64 loss: -0.1129683256149292
Batch 26/64 loss: -0.13933950662612915
Batch 27/64 loss: -0.11517709493637085
Batch 28/64 loss: -0.09161990880966187
Batch 29/64 loss: -0.11386418342590332
Batch 30/64 loss: -0.12841761112213135
Batch 31/64 loss: -0.13542330265045166
Batch 32/64 loss: -0.12656301259994507
Batch 33/64 loss: -0.12006270885467529
Batch 34/64 loss: -0.1344747543334961
Batch 35/64 loss: -0.11669474840164185
Batch 36/64 loss: -0.13758224248886108
Batch 37/64 loss: -0.10755288600921631
Batch 38/64 loss: -0.1291346549987793
Batch 39/64 loss: -0.11894875764846802
Batch 40/64 loss: -0.08876019716262817
Batch 41/64 loss: -0.11248475313186646
Batch 42/64 loss: -0.13553428649902344
Batch 43/64 loss: -0.12069088220596313
Batch 44/64 loss: -0.13735783100128174
Batch 45/64 loss: -0.12382209300994873
Batch 46/64 loss: -0.09395754337310791
Batch 47/64 loss: -0.13042986392974854
Batch 48/64 loss: -0.144054114818573
Batch 49/64 loss: -0.1370382308959961
Batch 50/64 loss: -0.1395546793937683
Batch 51/64 loss: -0.1492142677307129
Batch 52/64 loss: -0.09907746315002441
Batch 53/64 loss: -0.1231313943862915
Batch 54/64 loss: -0.12771987915039062
Batch 55/64 loss: -0.1380068063735962
Batch 56/64 loss: -0.12793415784835815
Batch 57/64 loss: -0.1408696174621582
Batch 58/64 loss: -0.12651461362838745
Batch 59/64 loss: -0.12822997570037842
Batch 60/64 loss: -0.11528283357620239
Batch 61/64 loss: -0.09228283166885376
Batch 62/64 loss: -0.11325687170028687
Batch 63/64 loss: -0.10908669233322144
Batch 64/64 loss: -0.13980859518051147
Epoch 378  Train loss: -0.12366993216907277  Val loss: 0.05027345201813478
Epoch 379
-------------------------------
Batch 1/64 loss: -0.139984130859375
Batch 2/64 loss: -0.09606313705444336
Batch 3/64 loss: -0.14187747240066528
Batch 4/64 loss: -0.11879074573516846
Batch 5/64 loss: -0.10267788171768188
Batch 6/64 loss: -0.11729556322097778
Batch 7/64 loss: -0.124045729637146
Batch 8/64 loss: -0.14190930128097534
Batch 9/64 loss: -0.10946190357208252
Batch 10/64 loss: -0.10866111516952515
Batch 11/64 loss: -0.10573405027389526
Batch 12/64 loss: -0.149830162525177
Batch 13/64 loss: -0.12173306941986084
Batch 14/64 loss: -0.12256109714508057
Batch 15/64 loss: -0.13406699895858765
Batch 16/64 loss: -0.13321197032928467
Batch 17/64 loss: -0.14523518085479736
Batch 18/64 loss: -0.11933636665344238
Batch 19/64 loss: -0.14472538232803345
Batch 20/64 loss: -0.13519704341888428
Batch 21/64 loss: -0.11471092700958252
Batch 22/64 loss: -0.12216860055923462
Batch 23/64 loss: -0.13681119680404663
Batch 24/64 loss: -0.13902884721755981
Batch 25/64 loss: -0.15312886238098145
Batch 26/64 loss: -0.11611884832382202
Batch 27/64 loss: -0.13195353746414185
Batch 28/64 loss: -0.09428507089614868
Batch 29/64 loss: -0.1319776177406311
Batch 30/64 loss: -0.12425589561462402
Batch 31/64 loss: -0.13793623447418213
Batch 32/64 loss: -0.13948416709899902
Batch 33/64 loss: -0.152604877948761
Batch 34/64 loss: -0.1355333924293518
Batch 35/64 loss: -0.14368927478790283
Batch 36/64 loss: -0.11462259292602539
Batch 37/64 loss: -0.14041036367416382
Batch 38/64 loss: -0.1365985870361328
Batch 39/64 loss: -0.1362437605857849
Batch 40/64 loss: -0.11359220743179321
Batch 41/64 loss: -0.11539334058761597
Batch 42/64 loss: -0.10733824968338013
Batch 43/64 loss: -0.13536721467971802
Batch 44/64 loss: -0.13360625505447388
Batch 45/64 loss: -0.1355583667755127
Batch 46/64 loss: -0.09424763917922974
Batch 47/64 loss: -0.12480294704437256
Batch 48/64 loss: -0.13002890348434448
Batch 49/64 loss: -0.14789146184921265
Batch 50/64 loss: -0.11984974145889282
Batch 51/64 loss: -0.1460820436477661
Batch 52/64 loss: -0.12211418151855469
Batch 53/64 loss: -0.09474211931228638
Batch 54/64 loss: -0.13171708583831787
Batch 55/64 loss: -0.1609656810760498
Batch 56/64 loss: -0.13406628370285034
Batch 57/64 loss: -0.1258484125137329
Batch 58/64 loss: -0.11428099870681763
Batch 59/64 loss: -0.12774771451950073
Batch 60/64 loss: -0.12856507301330566
Batch 61/64 loss: -0.13156557083129883
Batch 62/64 loss: -0.12304723262786865
Batch 63/64 loss: -0.09582853317260742
Batch 64/64 loss: -0.15002703666687012
Epoch 379  Train loss: -0.12738397635665594  Val loss: 0.0511967693407511
Epoch 380
-------------------------------
Batch 1/64 loss: -0.12238240242004395
Batch 2/64 loss: -0.13263916969299316
Batch 3/64 loss: -0.12896990776062012
Batch 4/64 loss: -0.12570881843566895
Batch 5/64 loss: -0.1477615237236023
Batch 6/64 loss: -0.11335504055023193
Batch 7/64 loss: -0.1191561222076416
Batch 8/64 loss: -0.13038891553878784
Batch 9/64 loss: -0.1070629358291626
Batch 10/64 loss: -0.1384221911430359
Batch 11/64 loss: -0.1429165005683899
Batch 12/64 loss: -0.12379604578018188
Batch 13/64 loss: -0.13461947441101074
Batch 14/64 loss: -0.14767694473266602
Batch 15/64 loss: -0.10782015323638916
Batch 16/64 loss: -0.16867977380752563
Batch 17/64 loss: -0.12256759405136108
Batch 18/64 loss: -0.1314792037010193
Batch 19/64 loss: -0.13360518217086792
Batch 20/64 loss: -0.13339662551879883
Batch 21/64 loss: -0.1280253529548645
Batch 22/64 loss: -0.11253190040588379
Batch 23/64 loss: -0.1465625762939453
Batch 24/64 loss: -0.1425301432609558
Batch 25/64 loss: -0.14344751834869385
Batch 26/64 loss: -0.11968892812728882
Batch 27/64 loss: -0.12587320804595947
Batch 28/64 loss: -0.11455684900283813
Batch 29/64 loss: -0.07191556692123413
Batch 30/64 loss: -0.1168246865272522
Batch 31/64 loss: -0.13785290718078613
Batch 32/64 loss: -0.13737887144088745
Batch 33/64 loss: -0.1297934651374817
Batch 34/64 loss: -0.08759576082229614
Batch 35/64 loss: -0.12595772743225098
Batch 36/64 loss: -0.14361023902893066
Batch 37/64 loss: -0.11890727281570435
Batch 38/64 loss: -0.1336277723312378
Batch 39/64 loss: -0.10781615972518921
Batch 40/64 loss: -0.1078423261642456
Batch 41/64 loss: -0.1295710802078247
Batch 42/64 loss: -0.1363641619682312
Batch 43/64 loss: -0.11091452836990356
Batch 44/64 loss: -0.12167418003082275
Batch 45/64 loss: -0.11960512399673462
Batch 46/64 loss: -0.1351991891860962
Batch 47/64 loss: -0.13825780153274536
Batch 48/64 loss: -0.1219637393951416
Batch 49/64 loss: -0.12069624662399292
Batch 50/64 loss: -0.11145949363708496
Batch 51/64 loss: -0.13126832246780396
Batch 52/64 loss: -0.11236953735351562
Batch 53/64 loss: -0.10749620199203491
Batch 54/64 loss: -0.12168651819229126
Batch 55/64 loss: -0.1365412473678589
Batch 56/64 loss: -0.13791203498840332
Batch 57/64 loss: -0.11784911155700684
Batch 58/64 loss: -0.10952669382095337
Batch 59/64 loss: -0.12516897916793823
Batch 60/64 loss: -0.13511013984680176
Batch 61/64 loss: -0.12266790866851807
Batch 62/64 loss: -0.1240738034248352
Batch 63/64 loss: -0.13815218210220337
Batch 64/64 loss: -0.12647730112075806
Epoch 380  Train loss: -0.12588439151352526  Val loss: 0.05143422687176576
Epoch 381
-------------------------------
Batch 1/64 loss: -0.14485454559326172
Batch 2/64 loss: -0.11601543426513672
Batch 3/64 loss: -0.13126206398010254
Batch 4/64 loss: -0.16152238845825195
Batch 5/64 loss: -0.1394290328025818
Batch 6/64 loss: -0.15392571687698364
Batch 7/64 loss: -0.11760944128036499
Batch 8/64 loss: -0.1054876446723938
Batch 9/64 loss: -0.131524920463562
Batch 10/64 loss: -0.12603849172592163
Batch 11/64 loss: -0.15147411823272705
Batch 12/64 loss: -0.1255473494529724
Batch 13/64 loss: -0.1213194727897644
Batch 14/64 loss: -0.15735524892807007
Batch 15/64 loss: -0.15630340576171875
Batch 16/64 loss: -0.10593873262405396
Batch 17/64 loss: -0.1143837571144104
Batch 18/64 loss: -0.1585271954536438
Batch 19/64 loss: -0.1540611982345581
Batch 20/64 loss: -0.0798693299293518
Batch 21/64 loss: -0.09870857000350952
Batch 22/64 loss: -0.12766897678375244
Batch 23/64 loss: -0.1467503309249878
Batch 24/64 loss: -0.12387508153915405
Batch 25/64 loss: -0.12557971477508545
Batch 26/64 loss: -0.11663150787353516
Batch 27/64 loss: -0.09919023513793945
Batch 28/64 loss: -0.13182491064071655
Batch 29/64 loss: -0.11017906665802002
Batch 30/64 loss: -0.13174057006835938
Batch 31/64 loss: -0.13753128051757812
Batch 32/64 loss: -0.13401275873184204
Batch 33/64 loss: -0.1385679841041565
Batch 34/64 loss: -0.15584158897399902
Batch 35/64 loss: -0.14289188385009766
Batch 36/64 loss: -0.1005510687828064
Batch 37/64 loss: -0.13584470748901367
Batch 38/64 loss: -0.14424574375152588
Batch 39/64 loss: -0.13336175680160522
Batch 40/64 loss: -0.1296931505203247
Batch 41/64 loss: -0.12257891893386841
Batch 42/64 loss: -0.10099160671234131
Batch 43/64 loss: -0.12997102737426758
Batch 44/64 loss: -0.10369992256164551
Batch 45/64 loss: -0.1317729949951172
Batch 46/64 loss: -0.11960381269454956
Batch 47/64 loss: -0.11079144477844238
Batch 48/64 loss: -0.1173979640007019
Batch 49/64 loss: -0.1323767900466919
Batch 50/64 loss: -0.1191643476486206
Batch 51/64 loss: -0.1384183168411255
Batch 52/64 loss: -0.14717072248458862
Batch 53/64 loss: -0.13955557346343994
Batch 54/64 loss: -0.10611152648925781
Batch 55/64 loss: -0.10990577936172485
Batch 56/64 loss: -0.13746768236160278
Batch 57/64 loss: -0.11897909641265869
Batch 58/64 loss: -0.12051326036453247
Batch 59/64 loss: -0.12658298015594482
Batch 60/64 loss: -0.11632168292999268
Batch 61/64 loss: -0.12833213806152344
Batch 62/64 loss: -0.12500518560409546
Batch 63/64 loss: -0.1456339955329895
Batch 64/64 loss: -0.09715896844863892
Epoch 381  Train loss: -0.12766049212100458  Val loss: 0.054122849018713046
Epoch 382
-------------------------------
Batch 1/64 loss: -0.1004863977432251
Batch 2/64 loss: -0.12161040306091309
Batch 3/64 loss: -0.13303142786026
Batch 4/64 loss: -0.12393242120742798
Batch 5/64 loss: -0.10359442234039307
Batch 6/64 loss: -0.12428832054138184
Batch 7/64 loss: -0.10957121849060059
Batch 8/64 loss: -0.13531911373138428
Batch 9/64 loss: -0.12325543165206909
Batch 10/64 loss: -0.1418706178665161
Batch 11/64 loss: -0.13230496644973755
Batch 12/64 loss: -0.13812017440795898
Batch 13/64 loss: -0.11425495147705078
Batch 14/64 loss: -0.12406808137893677
Batch 15/64 loss: -0.1406751275062561
Batch 16/64 loss: -0.12815076112747192
Batch 17/64 loss: -0.12581193447113037
Batch 18/64 loss: -0.13833820819854736
Batch 19/64 loss: -0.15221118927001953
Batch 20/64 loss: -0.09988421201705933
Batch 21/64 loss: -0.0968751311302185
Batch 22/64 loss: -0.1348978877067566
Batch 23/64 loss: -0.12949126958847046
Batch 24/64 loss: -0.12182372808456421
Batch 25/64 loss: -0.08815532922744751
Batch 26/64 loss: -0.14172285795211792
Batch 27/64 loss: -0.13123369216918945
Batch 28/64 loss: -0.1063382625579834
Batch 29/64 loss: -0.1274067759513855
Batch 30/64 loss: -0.13115590810775757
Batch 31/64 loss: -0.14106780290603638
Batch 32/64 loss: -0.15504580736160278
Batch 33/64 loss: -0.11940091848373413
Batch 34/64 loss: -0.1112593412399292
Batch 35/64 loss: -0.10102057456970215
Batch 36/64 loss: -0.09778672456741333
Batch 37/64 loss: -0.14088672399520874
Batch 38/64 loss: -0.15081393718719482
Batch 39/64 loss: -0.16291570663452148
Batch 40/64 loss: -0.12499183416366577
Batch 41/64 loss: -0.1228717565536499
Batch 42/64 loss: -0.12418365478515625
Batch 43/64 loss: -0.14593273401260376
Batch 44/64 loss: -0.12920892238616943
Batch 45/64 loss: -0.10272681713104248
Batch 46/64 loss: -0.10445064306259155
Batch 47/64 loss: -0.13613879680633545
Batch 48/64 loss: -0.12263727188110352
Batch 49/64 loss: -0.135719895362854
Batch 50/64 loss: -0.158860981464386
Batch 51/64 loss: -0.07599085569381714
Batch 52/64 loss: -0.13833588361740112
Batch 53/64 loss: -0.12887418270111084
Batch 54/64 loss: -0.13967329263687134
Batch 55/64 loss: -0.12342900037765503
Batch 56/64 loss: -0.13497227430343628
Batch 57/64 loss: -0.1290019154548645
Batch 58/64 loss: -0.1350039839744568
Batch 59/64 loss: -0.12172812223434448
Batch 60/64 loss: -0.09882622957229614
Batch 61/64 loss: -0.11595559120178223
Batch 62/64 loss: -0.1064329743385315
Batch 63/64 loss: -0.14367008209228516
Batch 64/64 loss: -0.1398128867149353
Epoch 382  Train loss: -0.1255616489578696  Val loss: 0.05278952875497825
Epoch 383
-------------------------------
Batch 1/64 loss: -0.12232023477554321
Batch 2/64 loss: -0.14259785413742065
Batch 3/64 loss: -0.12506383657455444
Batch 4/64 loss: -0.1512439250946045
Batch 5/64 loss: -0.12084656953811646
Batch 6/64 loss: -0.11066627502441406
Batch 7/64 loss: -0.12375622987747192
Batch 8/64 loss: -0.13633567094802856
Batch 9/64 loss: -0.11816239356994629
Batch 10/64 loss: -0.12604576349258423
Batch 11/64 loss: -0.09850680828094482
Batch 12/64 loss: -0.11534380912780762
Batch 13/64 loss: -0.13015782833099365
Batch 14/64 loss: -0.10779458284378052
Batch 15/64 loss: -0.0802234411239624
Batch 16/64 loss: -0.1383032202720642
Batch 17/64 loss: -0.11517065763473511
Batch 18/64 loss: -0.1304207444190979
Batch 19/64 loss: -0.10774081945419312
Batch 20/64 loss: -0.08396869897842407
Batch 21/64 loss: -0.12971413135528564
Batch 22/64 loss: -0.15470540523529053
Batch 23/64 loss: -0.144073486328125
Batch 24/64 loss: -0.11563897132873535
Batch 25/64 loss: -0.10348498821258545
Batch 26/64 loss: -0.13574981689453125
Batch 27/64 loss: -0.12783223390579224
Batch 28/64 loss: -0.14129531383514404
Batch 29/64 loss: -0.1400964856147766
Batch 30/64 loss: -0.1242939829826355
Batch 31/64 loss: -0.1273927092552185
Batch 32/64 loss: -0.1370844841003418
Batch 33/64 loss: -0.12155818939208984
Batch 34/64 loss: -0.1338598132133484
Batch 35/64 loss: -0.14301544427871704
Batch 36/64 loss: -0.1329919695854187
Batch 37/64 loss: -0.09946060180664062
Batch 38/64 loss: -0.13485097885131836
Batch 39/64 loss: -0.1327713131904602
Batch 40/64 loss: -0.11438822746276855
Batch 41/64 loss: -0.1125299334526062
Batch 42/64 loss: -0.09579789638519287
Batch 43/64 loss: -0.12067985534667969
Batch 44/64 loss: -0.1216198205947876
Batch 45/64 loss: -0.13335585594177246
Batch 46/64 loss: -0.14420735836029053
Batch 47/64 loss: -0.12286943197250366
Batch 48/64 loss: -0.12098181247711182
Batch 49/64 loss: -0.12470775842666626
Batch 50/64 loss: -0.1146230697631836
Batch 51/64 loss: -0.11793720722198486
Batch 52/64 loss: -0.12358683347702026
Batch 53/64 loss: -0.13924843072891235
Batch 54/64 loss: -0.13906264305114746
Batch 55/64 loss: -0.12676900625228882
Batch 56/64 loss: -0.12849360704421997
Batch 57/64 loss: -0.1245155930519104
Batch 58/64 loss: -0.10043621063232422
Batch 59/64 loss: -0.12966585159301758
Batch 60/64 loss: -0.1331382393836975
Batch 61/64 loss: -0.11102050542831421
Batch 62/64 loss: -0.10730183124542236
Batch 63/64 loss: -0.12235772609710693
Batch 64/64 loss: -0.11145305633544922
Epoch 383  Train loss: -0.12356743812561036  Val loss: 0.0523139291612553
Epoch 384
-------------------------------
Batch 1/64 loss: -0.11966753005981445
Batch 2/64 loss: -0.12447059154510498
Batch 3/64 loss: -0.10293769836425781
Batch 4/64 loss: -0.13812053203582764
Batch 5/64 loss: -0.170599102973938
Batch 6/64 loss: -0.13658231496810913
Batch 7/64 loss: -0.15468287467956543
Batch 8/64 loss: -0.11433166265487671
Batch 9/64 loss: -0.12721216678619385
Batch 10/64 loss: -0.13420265913009644
Batch 11/64 loss: -0.15503865480422974
Batch 12/64 loss: -0.11521458625793457
Batch 13/64 loss: -0.12736153602600098
Batch 14/64 loss: -0.13040590286254883
Batch 15/64 loss: -0.1257053017616272
Batch 16/64 loss: -0.13025355339050293
Batch 17/64 loss: -0.13020503520965576
Batch 18/64 loss: -0.11088567972183228
Batch 19/64 loss: -0.15131056308746338
Batch 20/64 loss: -0.11035430431365967
Batch 21/64 loss: -0.11738330125808716
Batch 22/64 loss: -0.11485010385513306
Batch 23/64 loss: -0.15104073286056519
Batch 24/64 loss: -0.1246524453163147
Batch 25/64 loss: -0.11891335248947144
Batch 26/64 loss: -0.15108221769332886
Batch 27/64 loss: -0.13053077459335327
Batch 28/64 loss: -0.1461092233657837
Batch 29/64 loss: -0.11138099431991577
Batch 30/64 loss: -0.10225480794906616
Batch 31/64 loss: -0.12184900045394897
Batch 32/64 loss: -0.13631969690322876
Batch 33/64 loss: -0.13262182474136353
Batch 34/64 loss: -0.12272441387176514
Batch 35/64 loss: -0.13526004552841187
Batch 36/64 loss: -0.13066601753234863
Batch 37/64 loss: -0.08221173286437988
Batch 38/64 loss: -0.11248266696929932
Batch 39/64 loss: -0.12746518850326538
Batch 40/64 loss: -0.12017875909805298
Batch 41/64 loss: -0.11350929737091064
Batch 42/64 loss: -0.14832651615142822
Batch 43/64 loss: -0.1294904351234436
Batch 44/64 loss: -0.12007641792297363
Batch 45/64 loss: -0.11910271644592285
Batch 46/64 loss: -0.1366332769393921
Batch 47/64 loss: -0.1470206379890442
Batch 48/64 loss: -0.15098446607589722
Batch 49/64 loss: -0.11531299352645874
Batch 50/64 loss: -0.12652701139450073
Batch 51/64 loss: -0.132695734500885
Batch 52/64 loss: -0.13737720251083374
Batch 53/64 loss: -0.12233853340148926
Batch 54/64 loss: -0.1124536395072937
Batch 55/64 loss: -0.1150597333908081
Batch 56/64 loss: -0.13100194931030273
Batch 57/64 loss: -0.10165917873382568
Batch 58/64 loss: -0.10012304782867432
Batch 59/64 loss: -0.12832599878311157
Batch 60/64 loss: -0.13373970985412598
Batch 61/64 loss: -0.1373356580734253
Batch 62/64 loss: -0.13660037517547607
Batch 63/64 loss: -0.1377848982810974
Batch 64/64 loss: -0.14746016263961792
Epoch 384  Train loss: -0.12774265256582523  Val loss: 0.051665586089760164
Epoch 385
-------------------------------
Batch 1/64 loss: -0.13672912120819092
Batch 2/64 loss: -0.10392683744430542
Batch 3/64 loss: -0.1534002423286438
Batch 4/64 loss: -0.1421855092048645
Batch 5/64 loss: -0.11858946084976196
Batch 6/64 loss: -0.13274627923965454
Batch 7/64 loss: -0.15248245000839233
Batch 8/64 loss: -0.12478172779083252
Batch 9/64 loss: -0.14037001132965088
Batch 10/64 loss: -0.12219202518463135
Batch 11/64 loss: -0.14259618520736694
Batch 12/64 loss: -0.1222490668296814
Batch 13/64 loss: -0.10978764295578003
Batch 14/64 loss: -0.14049094915390015
Batch 15/64 loss: -0.13557535409927368
Batch 16/64 loss: -0.1490657925605774
Batch 17/64 loss: -0.13566607236862183
Batch 18/64 loss: -0.13145190477371216
Batch 19/64 loss: -0.1312497854232788
Batch 20/64 loss: -0.15807586908340454
Batch 21/64 loss: -0.15143990516662598
Batch 22/64 loss: -0.1515648365020752
Batch 23/64 loss: -0.11419343948364258
Batch 24/64 loss: -0.12329328060150146
Batch 25/64 loss: -0.13223916292190552
Batch 26/64 loss: -0.12509572505950928
Batch 27/64 loss: -0.11817920207977295
Batch 28/64 loss: -0.1275147795677185
Batch 29/64 loss: -0.12497460842132568
Batch 30/64 loss: -0.14805948734283447
Batch 31/64 loss: -0.13245266675949097
Batch 32/64 loss: -0.14252901077270508
Batch 33/64 loss: -0.15571922063827515
Batch 34/64 loss: -0.10935258865356445
Batch 35/64 loss: -0.12292349338531494
Batch 36/64 loss: -0.1314820647239685
Batch 37/64 loss: -0.1443932056427002
Batch 38/64 loss: -0.13738667964935303
Batch 39/64 loss: -0.10846322774887085
Batch 40/64 loss: -0.10571026802062988
Batch 41/64 loss: -0.08625727891921997
Batch 42/64 loss: -0.11307018995285034
Batch 43/64 loss: -0.12182354927062988
Batch 44/64 loss: -0.11453980207443237
Batch 45/64 loss: -0.12798762321472168
Batch 46/64 loss: -0.12837105989456177
Batch 47/64 loss: -0.15087437629699707
Batch 48/64 loss: -0.0808877944946289
Batch 49/64 loss: -0.12097334861755371
Batch 50/64 loss: -0.14041811227798462
Batch 51/64 loss: -0.09478402137756348
Batch 52/64 loss: -0.10903280973434448
Batch 53/64 loss: -0.13881832361221313
Batch 54/64 loss: -0.11150753498077393
Batch 55/64 loss: -0.09053218364715576
Batch 56/64 loss: -0.1303965449333191
Batch 57/64 loss: -0.12206923961639404
Batch 58/64 loss: -0.10696321725845337
Batch 59/64 loss: -0.13882040977478027
Batch 60/64 loss: -0.14810442924499512
Batch 61/64 loss: -0.1235547661781311
Batch 62/64 loss: -0.1265462040901184
Batch 63/64 loss: -0.09914201498031616
Batch 64/64 loss: -0.1291142702102661
Epoch 385  Train loss: -0.12726101454566507  Val loss: 0.050145290356731084
Epoch 386
-------------------------------
Batch 1/64 loss: -0.12178701162338257
Batch 2/64 loss: -0.16125768423080444
Batch 3/64 loss: -0.11525559425354004
Batch 4/64 loss: -0.14892232418060303
Batch 5/64 loss: -0.11481046676635742
Batch 6/64 loss: -0.10981780290603638
Batch 7/64 loss: -0.11602205038070679
Batch 8/64 loss: -0.14456766843795776
Batch 9/64 loss: -0.13984012603759766
Batch 10/64 loss: -0.11721855401992798
Batch 11/64 loss: -0.1401578187942505
Batch 12/64 loss: -0.15687376260757446
Batch 13/64 loss: -0.11200731992721558
Batch 14/64 loss: -0.10851830244064331
Batch 15/64 loss: -0.11046719551086426
Batch 16/64 loss: -0.12318557500839233
Batch 17/64 loss: -0.099545419216156
Batch 18/64 loss: -0.13380032777786255
Batch 19/64 loss: -0.13377362489700317
Batch 20/64 loss: -0.14385592937469482
Batch 21/64 loss: -0.1454351544380188
Batch 22/64 loss: -0.11964172124862671
Batch 23/64 loss: -0.11461126804351807
Batch 24/64 loss: -0.14774996042251587
Batch 25/64 loss: -0.1336117386817932
Batch 26/64 loss: -0.130950927734375
Batch 27/64 loss: -0.13630527257919312
Batch 28/64 loss: -0.13877606391906738
Batch 29/64 loss: -0.1132056713104248
Batch 30/64 loss: -0.13326764106750488
Batch 31/64 loss: -0.1271073818206787
Batch 32/64 loss: -0.11393964290618896
Batch 33/64 loss: -0.09908467531204224
Batch 34/64 loss: -0.13399267196655273
Batch 35/64 loss: -0.13099408149719238
Batch 36/64 loss: -0.120749831199646
Batch 37/64 loss: -0.11786484718322754
Batch 38/64 loss: -0.11997753381729126
Batch 39/64 loss: -0.1428508162498474
Batch 40/64 loss: -0.12499183416366577
Batch 41/64 loss: -0.13523703813552856
Batch 42/64 loss: -0.11679333448410034
Batch 43/64 loss: -0.12136733531951904
Batch 44/64 loss: -0.1428707242012024
Batch 45/64 loss: -0.11959445476531982
Batch 46/64 loss: -0.13185018301010132
Batch 47/64 loss: -0.1348298192024231
Batch 48/64 loss: -0.11460602283477783
Batch 49/64 loss: -0.11418986320495605
Batch 50/64 loss: -0.12622541189193726
Batch 51/64 loss: -0.10200828313827515
Batch 52/64 loss: -0.11004900932312012
Batch 53/64 loss: -0.08035397529602051
Batch 54/64 loss: -0.1553034782409668
Batch 55/64 loss: -0.14228487014770508
Batch 56/64 loss: -0.14947831630706787
Batch 57/64 loss: -0.11106473207473755
Batch 58/64 loss: -0.14218288660049438
Batch 59/64 loss: -0.11868506669998169
Batch 60/64 loss: -0.11654990911483765
Batch 61/64 loss: -0.12251007556915283
Batch 62/64 loss: -0.15776348114013672
Batch 63/64 loss: -0.13436633348464966
Batch 64/64 loss: -0.11205929517745972
Epoch 386  Train loss: -0.12676082148271448  Val loss: 0.049619016983255076
Epoch 387
-------------------------------
Batch 1/64 loss: -0.15089654922485352
Batch 2/64 loss: -0.14541339874267578
Batch 3/64 loss: -0.15295040607452393
Batch 4/64 loss: -0.12414032220840454
Batch 5/64 loss: -0.1317318081855774
Batch 6/64 loss: -0.15864479541778564
Batch 7/64 loss: -0.148054301738739
Batch 8/64 loss: -0.09970295429229736
Batch 9/64 loss: -0.15906846523284912
Batch 10/64 loss: -0.15075361728668213
Batch 11/64 loss: -0.16457366943359375
Batch 12/64 loss: -0.13708102703094482
Batch 13/64 loss: -0.1402907371520996
Batch 14/64 loss: -0.13340729475021362
Batch 15/64 loss: -0.1263566017150879
Batch 16/64 loss: -0.11032688617706299
Batch 17/64 loss: -0.09125584363937378
Batch 18/64 loss: -0.1272873878479004
Batch 19/64 loss: -0.12699830532073975
Batch 20/64 loss: -0.1182786226272583
Batch 21/64 loss: -0.11503744125366211
Batch 22/64 loss: -0.14239072799682617
Batch 23/64 loss: -0.1159050464630127
Batch 24/64 loss: -0.09735548496246338
Batch 25/64 loss: -0.12785542011260986
Batch 26/64 loss: -0.12222987413406372
Batch 27/64 loss: -0.11532413959503174
Batch 28/64 loss: -0.13796967267990112
Batch 29/64 loss: -0.15174806118011475
Batch 30/64 loss: -0.11266165971755981
Batch 31/64 loss: -0.11871123313903809
Batch 32/64 loss: -0.13831228017807007
Batch 33/64 loss: -0.11931443214416504
Batch 34/64 loss: -0.11900371313095093
Batch 35/64 loss: -0.13099515438079834
Batch 36/64 loss: -0.11897951364517212
Batch 37/64 loss: -0.13997262716293335
Batch 38/64 loss: -0.10780781507492065
Batch 39/64 loss: -0.0979851484298706
Batch 40/64 loss: -0.1291932463645935
Batch 41/64 loss: -0.1152607798576355
Batch 42/64 loss: -0.13249707221984863
Batch 43/64 loss: -0.12445199489593506
Batch 44/64 loss: -0.1376780867576599
Batch 45/64 loss: -0.1251589059829712
Batch 46/64 loss: -0.10376256704330444
Batch 47/64 loss: -0.10940629243850708
Batch 48/64 loss: -0.11109530925750732
Batch 49/64 loss: -0.115134596824646
Batch 50/64 loss: -0.14190280437469482
Batch 51/64 loss: -0.10853523015975952
Batch 52/64 loss: -0.11911267042160034
Batch 53/64 loss: -0.11745858192443848
Batch 54/64 loss: -0.10767519474029541
Batch 55/64 loss: -0.13112813234329224
Batch 56/64 loss: -0.13904523849487305
Batch 57/64 loss: -0.09907335042953491
Batch 58/64 loss: -0.12840652465820312
Batch 59/64 loss: -0.13927561044692993
Batch 60/64 loss: -0.13098597526550293
Batch 61/64 loss: -0.12404054403305054
Batch 62/64 loss: -0.13668042421340942
Batch 63/64 loss: -0.12766963243484497
Batch 64/64 loss: -0.10670900344848633
Epoch 387  Train loss: -0.12645385031606637  Val loss: 0.05157021517606126
Epoch 388
-------------------------------
Batch 1/64 loss: -0.11539047956466675
Batch 2/64 loss: -0.1319904923439026
Batch 3/64 loss: -0.14723658561706543
Batch 4/64 loss: -0.1392146348953247
Batch 5/64 loss: -0.13704431056976318
Batch 6/64 loss: -0.11917704343795776
Batch 7/64 loss: -0.11175864934921265
Batch 8/64 loss: -0.12874382734298706
Batch 9/64 loss: -0.14249330759048462
Batch 10/64 loss: -0.1571778655052185
Batch 11/64 loss: -0.1414068341255188
Batch 12/64 loss: -0.15490353107452393
Batch 13/64 loss: -0.15656471252441406
Batch 14/64 loss: -0.1064751148223877
Batch 15/64 loss: -0.13032889366149902
Batch 16/64 loss: -0.1633741855621338
Batch 17/64 loss: -0.11694777011871338
Batch 18/64 loss: -0.0957794189453125
Batch 19/64 loss: -0.10513722896575928
Batch 20/64 loss: -0.1061553955078125
Batch 21/64 loss: -0.07594633102416992
Batch 22/64 loss: -0.11736345291137695
Batch 23/64 loss: -0.15204590559005737
Batch 24/64 loss: -0.11193448305130005
Batch 25/64 loss: -0.08465975522994995
Batch 26/64 loss: -0.14598292112350464
Batch 27/64 loss: -0.12274187803268433
Batch 28/64 loss: -0.1456877589225769
Batch 29/64 loss: -0.13085836172103882
Batch 30/64 loss: -0.1342858076095581
Batch 31/64 loss: -0.11303776502609253
Batch 32/64 loss: -0.12847203016281128
Batch 33/64 loss: -0.13322168588638306
Batch 34/64 loss: -0.13613706827163696
Batch 35/64 loss: -0.13732820749282837
Batch 36/64 loss: -0.11939013004302979
Batch 37/64 loss: -0.143166184425354
Batch 38/64 loss: -0.1325172781944275
Batch 39/64 loss: -0.1103401780128479
Batch 40/64 loss: -0.14408564567565918
Batch 41/64 loss: -0.13875502347946167
Batch 42/64 loss: -0.1492939591407776
Batch 43/64 loss: -0.10491597652435303
Batch 44/64 loss: -0.11568987369537354
Batch 45/64 loss: -0.14229243993759155
Batch 46/64 loss: -0.10768413543701172
Batch 47/64 loss: -0.12791627645492554
Batch 48/64 loss: -0.1414651870727539
Batch 49/64 loss: -0.11564171314239502
Batch 50/64 loss: -0.13681131601333618
Batch 51/64 loss: -0.12424296140670776
Batch 52/64 loss: -0.10264623165130615
Batch 53/64 loss: -0.13616907596588135
Batch 54/64 loss: -0.12215077877044678
Batch 55/64 loss: -0.15880393981933594
Batch 56/64 loss: -0.1489574909210205
Batch 57/64 loss: -0.1279727816581726
Batch 58/64 loss: -0.1148231029510498
Batch 59/64 loss: -0.12150323390960693
Batch 60/64 loss: -0.14382660388946533
Batch 61/64 loss: -0.11779135465621948
Batch 62/64 loss: -0.13445812463760376
Batch 63/64 loss: -0.10552024841308594
Batch 64/64 loss: -0.08998966217041016
Epoch 388  Train loss: -0.12758160291933546  Val loss: 0.050509977381663636
Epoch 389
-------------------------------
Batch 1/64 loss: -0.1563006043434143
Batch 2/64 loss: -0.13915151357650757
Batch 3/64 loss: -0.1349075436592102
Batch 4/64 loss: -0.14805006980895996
Batch 5/64 loss: -0.12536990642547607
Batch 6/64 loss: -0.1343582272529602
Batch 7/64 loss: -0.1319582462310791
Batch 8/64 loss: -0.10711383819580078
Batch 9/64 loss: -0.11620831489562988
Batch 10/64 loss: -0.1287669539451599
Batch 11/64 loss: -0.11412626504898071
Batch 12/64 loss: -0.1253330111503601
Batch 13/64 loss: -0.12756699323654175
Batch 14/64 loss: -0.1330500841140747
Batch 15/64 loss: -0.10787665843963623
Batch 16/64 loss: -0.14339077472686768
Batch 17/64 loss: -0.13522982597351074
Batch 18/64 loss: -0.13452178239822388
Batch 19/64 loss: -0.11419850587844849
Batch 20/64 loss: -0.15711498260498047
Batch 21/64 loss: -0.1415705680847168
Batch 22/64 loss: -0.12844187021255493
Batch 23/64 loss: -0.13119721412658691
Batch 24/64 loss: -0.11174511909484863
Batch 25/64 loss: -0.14838087558746338
Batch 26/64 loss: -0.10500282049179077
Batch 27/64 loss: -0.12796056270599365
Batch 28/64 loss: -0.11114621162414551
Batch 29/64 loss: -0.1149446964263916
Batch 30/64 loss: -0.11446499824523926
Batch 31/64 loss: -0.1266040802001953
Batch 32/64 loss: -0.11402904987335205
Batch 33/64 loss: -0.13498997688293457
Batch 34/64 loss: -0.1394302248954773
Batch 35/64 loss: -0.1194257140159607
Batch 36/64 loss: -0.1366710662841797
Batch 37/64 loss: -0.14807337522506714
Batch 38/64 loss: -0.12977975606918335
Batch 39/64 loss: -0.1308138370513916
Batch 40/64 loss: -0.13882428407669067
Batch 41/64 loss: -0.12988436222076416
Batch 42/64 loss: -0.134940505027771
Batch 43/64 loss: -0.15197044610977173
Batch 44/64 loss: -0.15577590465545654
Batch 45/64 loss: -0.11605870723724365
Batch 46/64 loss: -0.11418527364730835
Batch 47/64 loss: -0.07960635423660278
Batch 48/64 loss: -0.10551398992538452
Batch 49/64 loss: -0.11726129055023193
Batch 50/64 loss: -0.12626826763153076
Batch 51/64 loss: -0.13147562742233276
Batch 52/64 loss: -0.10962069034576416
Batch 53/64 loss: -0.14429807662963867
Batch 54/64 loss: -0.1393296718597412
Batch 55/64 loss: -0.14069950580596924
Batch 56/64 loss: -0.11801046133041382
Batch 57/64 loss: -0.13421857357025146
Batch 58/64 loss: -0.10890907049179077
Batch 59/64 loss: -0.13970071077346802
Batch 60/64 loss: -0.11489027738571167
Batch 61/64 loss: -0.13420367240905762
Batch 62/64 loss: -0.15249347686767578
Batch 63/64 loss: -0.15510785579681396
Batch 64/64 loss: -0.10991078615188599
Epoch 389  Train loss: -0.1287050394450917  Val loss: 0.04951131302876161
Epoch 390
-------------------------------
Batch 1/64 loss: -0.13286781311035156
Batch 2/64 loss: -0.09816092252731323
Batch 3/64 loss: -0.11943793296813965
Batch 4/64 loss: -0.13272154331207275
Batch 5/64 loss: -0.09622383117675781
Batch 6/64 loss: -0.13508367538452148
Batch 7/64 loss: -0.1286015510559082
Batch 8/64 loss: -0.13066041469573975
Batch 9/64 loss: -0.146165668964386
Batch 10/64 loss: -0.12323111295700073
Batch 11/64 loss: -0.09849494695663452
Batch 12/64 loss: -0.09256041049957275
Batch 13/64 loss: -0.1554051637649536
Batch 14/64 loss: -0.12402838468551636
Batch 15/64 loss: -0.1491783857345581
Batch 16/64 loss: -0.12687325477600098
Batch 17/64 loss: -0.1222008466720581
Batch 18/64 loss: -0.14123523235321045
Batch 19/64 loss: -0.1395803689956665
Batch 20/64 loss: -0.1294858455657959
Batch 21/64 loss: -0.1257535219192505
Batch 22/64 loss: -0.10649776458740234
Batch 23/64 loss: -0.13082075119018555
Batch 24/64 loss: -0.1184963583946228
Batch 25/64 loss: -0.1398468017578125
Batch 26/64 loss: -0.14685487747192383
Batch 27/64 loss: -0.14290887117385864
Batch 28/64 loss: -0.14363890886306763
Batch 29/64 loss: -0.13826656341552734
Batch 30/64 loss: -0.1248658299446106
Batch 31/64 loss: -0.14782816171646118
Batch 32/64 loss: -0.12619870901107788
Batch 33/64 loss: -0.12299096584320068
Batch 34/64 loss: -0.14213329553604126
Batch 35/64 loss: -0.13031721115112305
Batch 36/64 loss: -0.1407153606414795
Batch 37/64 loss: -0.11923420429229736
Batch 38/64 loss: -0.12085753679275513
Batch 39/64 loss: -0.10617458820343018
Batch 40/64 loss: -0.13150495290756226
Batch 41/64 loss: -0.1421952247619629
Batch 42/64 loss: -0.14392054080963135
Batch 43/64 loss: -0.13232040405273438
Batch 44/64 loss: -0.16291990876197815
Batch 45/64 loss: -0.12054860591888428
Batch 46/64 loss: -0.07889777421951294
Batch 47/64 loss: -0.11395454406738281
Batch 48/64 loss: -0.13927078247070312
Batch 49/64 loss: -0.14586371183395386
Batch 50/64 loss: -0.1307048201560974
Batch 51/64 loss: -0.10363423824310303
Batch 52/64 loss: -0.12012434005737305
Batch 53/64 loss: -0.1475534439086914
Batch 54/64 loss: -0.11861920356750488
Batch 55/64 loss: -0.1534385085105896
Batch 56/64 loss: -0.11128479242324829
Batch 57/64 loss: -0.11825895309448242
Batch 58/64 loss: -0.09406471252441406
Batch 59/64 loss: -0.14367234706878662
Batch 60/64 loss: -0.11746817827224731
Batch 61/64 loss: -0.12941622734069824
Batch 62/64 loss: -0.15715205669403076
Batch 63/64 loss: -0.13470065593719482
Batch 64/64 loss: -0.13874363899230957
Epoch 390  Train loss: -0.12850422344955745  Val loss: 0.05035582707100308
Epoch 391
-------------------------------
Batch 1/64 loss: -0.13153088092803955
Batch 2/64 loss: -0.12447589635848999
Batch 3/64 loss: -0.14825934171676636
Batch 4/64 loss: -0.12433266639709473
Batch 5/64 loss: -0.16278892755508423
Batch 6/64 loss: -0.17373931407928467
Batch 7/64 loss: -0.1499059796333313
Batch 8/64 loss: -0.13321685791015625
Batch 9/64 loss: -0.13800549507141113
Batch 10/64 loss: -0.12913382053375244
Batch 11/64 loss: -0.13205337524414062
Batch 12/64 loss: -0.13409364223480225
Batch 13/64 loss: -0.14865297079086304
Batch 14/64 loss: -0.13266444206237793
Batch 15/64 loss: -0.14199161529541016
Batch 16/64 loss: -0.1458163857460022
Batch 17/64 loss: -0.1199195384979248
Batch 18/64 loss: -0.12324535846710205
Batch 19/64 loss: -0.13002455234527588
Batch 20/64 loss: -0.1164281964302063
Batch 21/64 loss: -0.14532685279846191
Batch 22/64 loss: -0.15290093421936035
Batch 23/64 loss: -0.14266353845596313
Batch 24/64 loss: -0.09119844436645508
Batch 25/64 loss: -0.1230851411819458
Batch 26/64 loss: -0.12621796131134033
Batch 27/64 loss: -0.1350530982017517
Batch 28/64 loss: -0.11419516801834106
Batch 29/64 loss: -0.1384153962135315
Batch 30/64 loss: -0.13146883249282837
Batch 31/64 loss: -0.12189042568206787
Batch 32/64 loss: -0.1425083875656128
Batch 33/64 loss: -0.12761259078979492
Batch 34/64 loss: -0.1372358798980713
Batch 35/64 loss: -0.10624176263809204
Batch 36/64 loss: -0.13760530948638916
Batch 37/64 loss: -0.11988627910614014
Batch 38/64 loss: -0.1263498067855835
Batch 39/64 loss: -0.13117706775665283
Batch 40/64 loss: -0.10367447137832642
Batch 41/64 loss: -0.15153896808624268
Batch 42/64 loss: -0.1251533031463623
Batch 43/64 loss: -0.12499135732650757
Batch 44/64 loss: -0.08333522081375122
Batch 45/64 loss: -0.11283588409423828
Batch 46/64 loss: -0.1765872836112976
Batch 47/64 loss: -0.11178708076477051
Batch 48/64 loss: -0.12682604789733887
Batch 49/64 loss: -0.13911956548690796
Batch 50/64 loss: -0.14589428901672363
Batch 51/64 loss: -0.12204605340957642
Batch 52/64 loss: -0.133222758769989
Batch 53/64 loss: -0.14074116945266724
Batch 54/64 loss: -0.12613344192504883
Batch 55/64 loss: -0.13216614723205566
Batch 56/64 loss: -0.11474525928497314
Batch 57/64 loss: -0.11622434854507446
Batch 58/64 loss: -0.12449014186859131
Batch 59/64 loss: -0.1254226565361023
Batch 60/64 loss: -0.1341271996498108
Batch 61/64 loss: -0.10933035612106323
Batch 62/64 loss: -0.12775897979736328
Batch 63/64 loss: -0.12929630279541016
Batch 64/64 loss: -0.10167914628982544
Epoch 391  Train loss: -0.1302748200940151  Val loss: 0.051384560226165145
Epoch 392
-------------------------------
Batch 1/64 loss: -0.14725583791732788
Batch 2/64 loss: -0.14961737394332886
Batch 3/64 loss: -0.11694884300231934
Batch 4/64 loss: -0.10282635688781738
Batch 5/64 loss: -0.1480860710144043
Batch 6/64 loss: -0.11792302131652832
Batch 7/64 loss: -0.1380932331085205
Batch 8/64 loss: -0.12604904174804688
Batch 9/64 loss: -0.1273077130317688
Batch 10/64 loss: -0.12458759546279907
Batch 11/64 loss: -0.14152109622955322
Batch 12/64 loss: -0.12003922462463379
Batch 13/64 loss: -0.11774283647537231
Batch 14/64 loss: -0.15511715412139893
Batch 15/64 loss: -0.11977475881576538
Batch 16/64 loss: -0.12399357557296753
Batch 17/64 loss: -0.10242033004760742
Batch 18/64 loss: -0.13480138778686523
Batch 19/64 loss: -0.11040019989013672
Batch 20/64 loss: -0.11420565843582153
Batch 21/64 loss: -0.13388746976852417
Batch 22/64 loss: -0.1100044846534729
Batch 23/64 loss: -0.12623083591461182
Batch 24/64 loss: -0.10010838508605957
Batch 25/64 loss: -0.13237178325653076
Batch 26/64 loss: -0.11967915296554565
Batch 27/64 loss: -0.1597180962562561
Batch 28/64 loss: -0.13843309879302979
Batch 29/64 loss: -0.13844364881515503
Batch 30/64 loss: -0.12482744455337524
Batch 31/64 loss: -0.11363506317138672
Batch 32/64 loss: -0.12696689367294312
Batch 33/64 loss: -0.14306104183197021
Batch 34/64 loss: -0.14903372526168823
Batch 35/64 loss: -0.13927602767944336
Batch 36/64 loss: -0.15924614667892456
Batch 37/64 loss: -0.11294686794281006
Batch 38/64 loss: -0.15390557050704956
Batch 39/64 loss: -0.1457749605178833
Batch 40/64 loss: -0.14740562438964844
Batch 41/64 loss: -0.11997473239898682
Batch 42/64 loss: -0.14154374599456787
Batch 43/64 loss: -0.11366003751754761
Batch 44/64 loss: -0.11108559370040894
Batch 45/64 loss: -0.13680285215377808
Batch 46/64 loss: -0.11566829681396484
Batch 47/64 loss: -0.14246046543121338
Batch 48/64 loss: -0.13391900062561035
Batch 49/64 loss: -0.1592387557029724
Batch 50/64 loss: -0.1246877908706665
Batch 51/64 loss: -0.14671975374221802
Batch 52/64 loss: -0.1181633472442627
Batch 53/64 loss: -0.10987973213195801
Batch 54/64 loss: -0.1338145136833191
Batch 55/64 loss: -0.13409191370010376
Batch 56/64 loss: -0.11911308765411377
Batch 57/64 loss: -0.12310951948165894
Batch 58/64 loss: -0.12392044067382812
Batch 59/64 loss: -0.12002730369567871
Batch 60/64 loss: -0.1354236602783203
Batch 61/64 loss: -0.1182013750076294
Batch 62/64 loss: -0.1205018162727356
Batch 63/64 loss: -0.1497272253036499
Batch 64/64 loss: -0.12517255544662476
Epoch 392  Train loss: -0.12955736482844632  Val loss: 0.050370669242033025
Epoch 393
-------------------------------
Batch 1/64 loss: -0.13368558883666992
Batch 2/64 loss: -0.12216806411743164
Batch 3/64 loss: -0.14343315362930298
Batch 4/64 loss: -0.14230746030807495
Batch 5/64 loss: -0.13798224925994873
Batch 6/64 loss: -0.15980291366577148
Batch 7/64 loss: -0.13709908723831177
Batch 8/64 loss: -0.10970836877822876
Batch 9/64 loss: -0.12548643350601196
Batch 10/64 loss: -0.13558077812194824
Batch 11/64 loss: -0.11613291501998901
Batch 12/64 loss: -0.0999416708946228
Batch 13/64 loss: -0.14897501468658447
Batch 14/64 loss: -0.1265212893486023
Batch 15/64 loss: -0.13214296102523804
Batch 16/64 loss: -0.1338157057762146
Batch 17/64 loss: -0.13538426160812378
Batch 18/64 loss: -0.11697214841842651
Batch 19/64 loss: -0.1199263334274292
Batch 20/64 loss: -0.13369649648666382
Batch 21/64 loss: -0.12842214107513428
Batch 22/64 loss: -0.13371634483337402
Batch 23/64 loss: -0.13556861877441406
Batch 24/64 loss: -0.1351737380027771
Batch 25/64 loss: -0.14206892251968384
Batch 26/64 loss: -0.14039403200149536
Batch 27/64 loss: -0.14021360874176025
Batch 28/64 loss: -0.13324028253555298
Batch 29/64 loss: -0.15996289253234863
Batch 30/64 loss: -0.10155045986175537
Batch 31/64 loss: -0.11369562149047852
Batch 32/64 loss: -0.13526839017868042
Batch 33/64 loss: -0.11566585302352905
Batch 34/64 loss: -0.11635327339172363
Batch 35/64 loss: -0.14407002925872803
Batch 36/64 loss: -0.1400250792503357
Batch 37/64 loss: -0.12429523468017578
Batch 38/64 loss: -0.11838412284851074
Batch 39/64 loss: -0.12290561199188232
Batch 40/64 loss: -0.09073793888092041
Batch 41/64 loss: -0.14224350452423096
Batch 42/64 loss: -0.14457684755325317
Batch 43/64 loss: -0.15182650089263916
Batch 44/64 loss: -0.11788910627365112
Batch 45/64 loss: -0.1363552212715149
Batch 46/64 loss: -0.13090407848358154
Batch 47/64 loss: -0.10510730743408203
Batch 48/64 loss: -0.11040067672729492
Batch 49/64 loss: -0.12076354026794434
Batch 50/64 loss: -0.13007038831710815
Batch 51/64 loss: -0.14313411712646484
Batch 52/64 loss: -0.12532705068588257
Batch 53/64 loss: -0.1380653977394104
Batch 54/64 loss: -0.1405935287475586
Batch 55/64 loss: -0.13032454252243042
Batch 56/64 loss: -0.1358504295349121
Batch 57/64 loss: -0.12784123420715332
Batch 58/64 loss: -0.10880106687545776
Batch 59/64 loss: -0.14845526218414307
Batch 60/64 loss: -0.1219167709350586
Batch 61/64 loss: -0.11300450563430786
Batch 62/64 loss: -0.13094139099121094
Batch 63/64 loss: -0.14707690477371216
Batch 64/64 loss: -0.10734760761260986
Epoch 393  Train loss: -0.1296385124617932  Val loss: 0.05017676611536557
Epoch 394
-------------------------------
Batch 1/64 loss: -0.140339195728302
Batch 2/64 loss: -0.1341988444328308
Batch 3/64 loss: -0.15541112422943115
Batch 4/64 loss: -0.15541458129882812
Batch 5/64 loss: -0.13378894329071045
Batch 6/64 loss: -0.1291239857673645
Batch 7/64 loss: -0.122494637966156
Batch 8/64 loss: -0.13614177703857422
Batch 9/64 loss: -0.16000080108642578
Batch 10/64 loss: -0.12358927726745605
Batch 11/64 loss: -0.10275882482528687
Batch 12/64 loss: -0.14680689573287964
Batch 13/64 loss: -0.16217255592346191
Batch 14/64 loss: -0.11120599508285522
Batch 15/64 loss: -0.12155795097351074
Batch 16/64 loss: -0.1530599594116211
Batch 17/64 loss: -0.09065991640090942
Batch 18/64 loss: -0.1534731388092041
Batch 19/64 loss: -0.15373927354812622
Batch 20/64 loss: -0.13478487730026245
Batch 21/64 loss: -0.11618036031723022
Batch 22/64 loss: -0.12906616926193237
Batch 23/64 loss: -0.12050676345825195
Batch 24/64 loss: -0.1480557918548584
Batch 25/64 loss: -0.15238326787948608
Batch 26/64 loss: -0.12040901184082031
Batch 27/64 loss: -0.1538679003715515
Batch 28/64 loss: -0.14674854278564453
Batch 29/64 loss: -0.1334364414215088
Batch 30/64 loss: -0.13497871160507202
Batch 31/64 loss: -0.07173258066177368
Batch 32/64 loss: -0.13005930185317993
Batch 33/64 loss: -0.12532389163970947
Batch 34/64 loss: -0.15335893630981445
Batch 35/64 loss: -0.1200031042098999
Batch 36/64 loss: -0.1173127293586731
Batch 37/64 loss: -0.0741226077079773
Batch 38/64 loss: -0.1356503963470459
Batch 39/64 loss: -0.11968857049942017
Batch 40/64 loss: -0.13087475299835205
Batch 41/64 loss: -0.14894592761993408
Batch 42/64 loss: -0.13772475719451904
Batch 43/64 loss: -0.14831286668777466
Batch 44/64 loss: -0.1261095404624939
Batch 45/64 loss: -0.1547732949256897
Batch 46/64 loss: -0.12720263004302979
Batch 47/64 loss: -0.10828149318695068
Batch 48/64 loss: -0.11710965633392334
Batch 49/64 loss: -0.12456047534942627
Batch 50/64 loss: -0.15890830755233765
Batch 51/64 loss: -0.10613197088241577
Batch 52/64 loss: -0.136671781539917
Batch 53/64 loss: -0.12216085195541382
Batch 54/64 loss: -0.09988820552825928
Batch 55/64 loss: -0.14071261882781982
Batch 56/64 loss: -0.12457704544067383
Batch 57/64 loss: -0.14409101009368896
Batch 58/64 loss: -0.14365828037261963
Batch 59/64 loss: -0.11812096834182739
Batch 60/64 loss: -0.10678517818450928
Batch 61/64 loss: -0.1227712631225586
Batch 62/64 loss: -0.13236987590789795
Batch 63/64 loss: -0.1380549669265747
Batch 64/64 loss: -0.084835946559906
Epoch 394  Train loss: -0.13029070297876993  Val loss: 0.052978884313524384
Epoch 395
-------------------------------
Batch 1/64 loss: -0.11302053928375244
Batch 2/64 loss: -0.11036241054534912
Batch 3/64 loss: -0.15618634223937988
Batch 4/64 loss: -0.12864941358566284
Batch 5/64 loss: -0.13019108772277832
Batch 6/64 loss: -0.1339319944381714
Batch 7/64 loss: -0.13210010528564453
Batch 8/64 loss: -0.11201131343841553
Batch 9/64 loss: -0.11515766382217407
Batch 10/64 loss: -0.14771783351898193
Batch 11/64 loss: -0.13375931978225708
Batch 12/64 loss: -0.1258448362350464
Batch 13/64 loss: -0.13090789318084717
Batch 14/64 loss: -0.12766659259796143
Batch 15/64 loss: -0.11749440431594849
Batch 16/64 loss: -0.14935141801834106
Batch 17/64 loss: -0.12217652797698975
Batch 18/64 loss: -0.13232874870300293
Batch 19/64 loss: -0.10284179449081421
Batch 20/64 loss: -0.1423349380493164
Batch 21/64 loss: -0.1483250856399536
Batch 22/64 loss: -0.135922372341156
Batch 23/64 loss: -0.11770570278167725
Batch 24/64 loss: -0.15417957305908203
Batch 25/64 loss: -0.14532655477523804
Batch 26/64 loss: -0.13697147369384766
Batch 27/64 loss: -0.11552470922470093
Batch 28/64 loss: -0.14212775230407715
Batch 29/64 loss: -0.12474632263183594
Batch 30/64 loss: -0.13610589504241943
Batch 31/64 loss: -0.1115032434463501
Batch 32/64 loss: -0.1333777904510498
Batch 33/64 loss: -0.13391029834747314
Batch 34/64 loss: -0.12571024894714355
Batch 35/64 loss: -0.15249508619308472
Batch 36/64 loss: -0.1331915259361267
Batch 37/64 loss: -0.1321268081665039
Batch 38/64 loss: -0.12978029251098633
Batch 39/64 loss: -0.11374956369400024
Batch 40/64 loss: -0.0715179443359375
Batch 41/64 loss: -0.13163548707962036
Batch 42/64 loss: -0.1460973024368286
Batch 43/64 loss: -0.11703789234161377
Batch 44/64 loss: -0.11171567440032959
Batch 45/64 loss: -0.11384689807891846
Batch 46/64 loss: -0.14404791593551636
Batch 47/64 loss: -0.1411188840866089
Batch 48/64 loss: -0.14734399318695068
Batch 49/64 loss: -0.1332363486289978
Batch 50/64 loss: -0.1411920189857483
Batch 51/64 loss: -0.15442025661468506
Batch 52/64 loss: -0.1456843614578247
Batch 53/64 loss: -0.11252552270889282
Batch 54/64 loss: -0.15211641788482666
Batch 55/64 loss: -0.1356068253517151
Batch 56/64 loss: -0.1175072193145752
Batch 57/64 loss: -0.1516798734664917
Batch 58/64 loss: -0.15462112426757812
Batch 59/64 loss: -0.1343715786933899
Batch 60/64 loss: -0.10960233211517334
Batch 61/64 loss: -0.13833218812942505
Batch 62/64 loss: -0.10615372657775879
Batch 63/64 loss: -0.14121025800704956
Batch 64/64 loss: -0.11681151390075684
Epoch 395  Train loss: -0.13062033185771868  Val loss: 0.05038519331679721
Epoch 396
-------------------------------
Batch 1/64 loss: -0.12264907360076904
Batch 2/64 loss: -0.1449020504951477
Batch 3/64 loss: -0.14636880159378052
Batch 4/64 loss: -0.1544051170349121
Batch 5/64 loss: -0.13602638244628906
Batch 6/64 loss: -0.1314089298248291
Batch 7/64 loss: -0.13947010040283203
Batch 8/64 loss: -0.12275475263595581
Batch 9/64 loss: -0.129747211933136
Batch 10/64 loss: -0.15866166353225708
Batch 11/64 loss: -0.13622093200683594
Batch 12/64 loss: -0.16518521308898926
Batch 13/64 loss: -0.1565837860107422
Batch 14/64 loss: -0.11830496788024902
Batch 15/64 loss: -0.13191920518875122
Batch 16/64 loss: -0.14320218563079834
Batch 17/64 loss: -0.12732553482055664
Batch 18/64 loss: -0.10157591104507446
Batch 19/64 loss: -0.1111675500869751
Batch 20/64 loss: -0.13728106021881104
Batch 21/64 loss: -0.1373920440673828
Batch 22/64 loss: -0.14842534065246582
Batch 23/64 loss: -0.13674211502075195
Batch 24/64 loss: -0.14969253540039062
Batch 25/64 loss: -0.16038846969604492
Batch 26/64 loss: -0.12084877490997314
Batch 27/64 loss: -0.12772703170776367
Batch 28/64 loss: -0.1342155933380127
Batch 29/64 loss: -0.1552242636680603
Batch 30/64 loss: -0.06511759757995605
Batch 31/64 loss: -0.14549320936203003
Batch 32/64 loss: -0.16033011674880981
Batch 33/64 loss: -0.09875476360321045
Batch 34/64 loss: -0.12940174341201782
Batch 35/64 loss: -0.14352917671203613
Batch 36/64 loss: -0.14888662099838257
Batch 37/64 loss: -0.11236941814422607
Batch 38/64 loss: -0.10717135667800903
Batch 39/64 loss: -0.13111937046051025
Batch 40/64 loss: -0.1288830041885376
Batch 41/64 loss: -0.11662006378173828
Batch 42/64 loss: -0.14200949668884277
Batch 43/64 loss: -0.11788654327392578
Batch 44/64 loss: -0.1393536925315857
Batch 45/64 loss: -0.13871324062347412
Batch 46/64 loss: -0.13310909271240234
Batch 47/64 loss: -0.12752437591552734
Batch 48/64 loss: -0.11852312088012695
Batch 49/64 loss: -0.1192554235458374
Batch 50/64 loss: -0.13876032829284668
Batch 51/64 loss: -0.08563041687011719
Batch 52/64 loss: -0.13378244638442993
Batch 53/64 loss: -0.14279639720916748
Batch 54/64 loss: -0.11331665515899658
Batch 55/64 loss: -0.13659632205963135
Batch 56/64 loss: -0.14761000871658325
Batch 57/64 loss: -0.1215096116065979
Batch 58/64 loss: -0.12024128437042236
Batch 59/64 loss: -0.13560092449188232
Batch 60/64 loss: -0.12194275856018066
Batch 61/64 loss: -0.15311986207962036
Batch 62/64 loss: -0.13295352458953857
Batch 63/64 loss: -0.11854928731918335
Batch 64/64 loss: -0.10521358251571655
Epoch 396  Train loss: -0.131626479064717  Val loss: 0.05098007184123665
Epoch 397
-------------------------------
Batch 1/64 loss: -0.132249116897583
Batch 2/64 loss: -0.1475735902786255
Batch 3/64 loss: -0.1460554599761963
Batch 4/64 loss: -0.12654387950897217
Batch 5/64 loss: -0.14320063591003418
Batch 6/64 loss: -0.12440288066864014
Batch 7/64 loss: -0.1142810583114624
Batch 8/64 loss: -0.13231194019317627
Batch 9/64 loss: -0.13841480016708374
Batch 10/64 loss: -0.12458628416061401
Batch 11/64 loss: -0.12618577480316162
Batch 12/64 loss: -0.149988055229187
Batch 13/64 loss: -0.10335999727249146
Batch 14/64 loss: -0.13079416751861572
Batch 15/64 loss: -0.1444525122642517
Batch 16/64 loss: -0.13676172494888306
Batch 17/64 loss: -0.14026886224746704
Batch 18/64 loss: -0.13773226737976074
Batch 19/64 loss: -0.11673378944396973
Batch 20/64 loss: -0.12199318408966064
Batch 21/64 loss: -0.07110989093780518
Batch 22/64 loss: -0.13687902688980103
Batch 23/64 loss: -0.11526882648468018
Batch 24/64 loss: -0.1254563331604004
Batch 25/64 loss: -0.1452270746231079
Batch 26/64 loss: -0.122234046459198
Batch 27/64 loss: -0.14757055044174194
Batch 28/64 loss: -0.15174788236618042
Batch 29/64 loss: -0.1398705244064331
Batch 30/64 loss: -0.1357964277267456
Batch 31/64 loss: -0.15636855363845825
Batch 32/64 loss: -0.12865710258483887
Batch 33/64 loss: -0.10835003852844238
Batch 34/64 loss: -0.12157058715820312
Batch 35/64 loss: -0.12166351079940796
Batch 36/64 loss: -0.1322411298751831
Batch 37/64 loss: -0.14230048656463623
Batch 38/64 loss: -0.1165316104888916
Batch 39/64 loss: -0.12309974431991577
Batch 40/64 loss: -0.152732253074646
Batch 41/64 loss: -0.14405560493469238
Batch 42/64 loss: -0.14474916458129883
Batch 43/64 loss: -0.09858095645904541
Batch 44/64 loss: -0.12125980854034424
Batch 45/64 loss: -0.1274263858795166
Batch 46/64 loss: -0.1251574158668518
Batch 47/64 loss: -0.10347473621368408
Batch 48/64 loss: -0.10560894012451172
Batch 49/64 loss: -0.11023163795471191
Batch 50/64 loss: -0.11599904298782349
Batch 51/64 loss: -0.1266213059425354
Batch 52/64 loss: -0.16240733861923218
Batch 53/64 loss: -0.13495492935180664
Batch 54/64 loss: -0.13461726903915405
Batch 55/64 loss: -0.15333092212677002
Batch 56/64 loss: -0.14898204803466797
Batch 57/64 loss: -0.14753878116607666
Batch 58/64 loss: -0.1508561372756958
Batch 59/64 loss: -0.15958821773529053
Batch 60/64 loss: -0.13289672136306763
Batch 61/64 loss: -0.12768369913101196
Batch 62/64 loss: -0.12699073553085327
Batch 63/64 loss: -0.1334822177886963
Batch 64/64 loss: -0.1456487774848938
Epoch 397  Train loss: -0.1314242538283853  Val loss: 0.0522131475386341
Epoch 398
-------------------------------
Batch 1/64 loss: -0.1583651304244995
Batch 2/64 loss: -0.10652405023574829
Batch 3/64 loss: -0.14054584503173828
Batch 4/64 loss: -0.13397037982940674
Batch 5/64 loss: -0.11870050430297852
Batch 6/64 loss: -0.11288279294967651
Batch 7/64 loss: -0.14824599027633667
Batch 8/64 loss: -0.1325608491897583
Batch 9/64 loss: -0.15100985765457153
Batch 10/64 loss: -0.13060224056243896
Batch 11/64 loss: -0.14007210731506348
Batch 12/64 loss: -0.13799887895584106
Batch 13/64 loss: -0.12780296802520752
Batch 14/64 loss: -0.10581725835800171
Batch 15/64 loss: -0.15263110399246216
Batch 16/64 loss: -0.14332270622253418
Batch 17/64 loss: -0.13946419954299927
Batch 18/64 loss: -0.1375277042388916
Batch 19/64 loss: -0.15778350830078125
Batch 20/64 loss: -0.11264902353286743
Batch 21/64 loss: -0.11328125
Batch 22/64 loss: -0.1346626877784729
Batch 23/64 loss: -0.12181246280670166
Batch 24/64 loss: -0.13499361276626587
Batch 25/64 loss: -0.11644589900970459
Batch 26/64 loss: -0.13839030265808105
Batch 27/64 loss: -0.14906907081604004
Batch 28/64 loss: -0.15060508251190186
Batch 29/64 loss: -0.14501476287841797
Batch 30/64 loss: -0.09218472242355347
Batch 31/64 loss: -0.13450950384140015
Batch 32/64 loss: -0.13580656051635742
Batch 33/64 loss: -0.12750065326690674
Batch 34/64 loss: -0.11993110179901123
Batch 35/64 loss: -0.12239408493041992
Batch 36/64 loss: -0.12847882509231567
Batch 37/64 loss: -0.09750968217849731
Batch 38/64 loss: -0.1261311173439026
Batch 39/64 loss: -0.14642733335494995
Batch 40/64 loss: -0.12693220376968384
Batch 41/64 loss: -0.13751435279846191
Batch 42/64 loss: -0.11997127532958984
Batch 43/64 loss: -0.12273788452148438
Batch 44/64 loss: -0.13706672191619873
Batch 45/64 loss: -0.1444401741027832
Batch 46/64 loss: -0.11688852310180664
Batch 47/64 loss: -0.15044695138931274
Batch 48/64 loss: -0.10913717746734619
Batch 49/64 loss: -0.1459473967552185
Batch 50/64 loss: -0.1285730004310608
Batch 51/64 loss: -0.13158965110778809
Batch 52/64 loss: -0.1322135329246521
Batch 53/64 loss: -0.14350497722625732
Batch 54/64 loss: -0.12690472602844238
Batch 55/64 loss: -0.14141547679901123
Batch 56/64 loss: -0.1275845766067505
Batch 57/64 loss: -0.14768856763839722
Batch 58/64 loss: -0.12346512079238892
Batch 59/64 loss: -0.12381446361541748
Batch 60/64 loss: -0.1440943479537964
Batch 61/64 loss: -0.1373380422592163
Batch 62/64 loss: -0.1026078462600708
Batch 63/64 loss: -0.1249045729637146
Batch 64/64 loss: -0.09678220748901367
Epoch 398  Train loss: -0.13087049465553433  Val loss: 0.05328646906462731
Epoch 399
-------------------------------
Batch 1/64 loss: -0.15056997537612915
Batch 2/64 loss: -0.16524195671081543
Batch 3/64 loss: -0.13609802722930908
Batch 4/64 loss: -0.16714614629745483
Batch 5/64 loss: -0.1449047327041626
Batch 6/64 loss: -0.1187887191772461
Batch 7/64 loss: -0.1392146348953247
Batch 8/64 loss: -0.1521187424659729
Batch 9/64 loss: -0.1398562788963318
Batch 10/64 loss: -0.14424651861190796
Batch 11/64 loss: -0.15113097429275513
Batch 12/64 loss: -0.11741244792938232
Batch 13/64 loss: -0.12031334638595581
Batch 14/64 loss: -0.13587534427642822
Batch 15/64 loss: -0.14077693223953247
Batch 16/64 loss: -0.13341563940048218
Batch 17/64 loss: -0.11958956718444824
Batch 18/64 loss: -0.13548779487609863
Batch 19/64 loss: -0.12381494045257568
Batch 20/64 loss: -0.12892162799835205
Batch 21/64 loss: -0.1698693037033081
Batch 22/64 loss: -0.1132737398147583
Batch 23/64 loss: -0.14209431409835815
Batch 24/64 loss: -0.13335734605789185
Batch 25/64 loss: -0.15657031536102295
Batch 26/64 loss: -0.12536883354187012
Batch 27/64 loss: -0.09258061647415161
Batch 28/64 loss: -0.11546790599822998
Batch 29/64 loss: -0.13357782363891602
Batch 30/64 loss: -0.12398380041122437
Batch 31/64 loss: -0.09377080202102661
Batch 32/64 loss: -0.12384730577468872
Batch 33/64 loss: -0.13906949758529663
Batch 34/64 loss: -0.10827499628067017
Batch 35/64 loss: -0.1426517367362976
Batch 36/64 loss: -0.14398932456970215
Batch 37/64 loss: -0.12773644924163818
Batch 38/64 loss: -0.13114869594573975
Batch 39/64 loss: -0.11834508180618286
Batch 40/64 loss: -0.11637258529663086
Batch 41/64 loss: -0.12221962213516235
Batch 42/64 loss: -0.10550892353057861
Batch 43/64 loss: -0.1305416226387024
Batch 44/64 loss: -0.12395882606506348
Batch 45/64 loss: -0.16151413321495056
Batch 46/64 loss: -0.10935300588607788
Batch 47/64 loss: -0.12465035915374756
Batch 48/64 loss: -0.12248766422271729
Batch 49/64 loss: -0.12914329767227173
Batch 50/64 loss: -0.13870877027511597
Batch 51/64 loss: -0.14114123582839966
Batch 52/64 loss: -0.15346020460128784
Batch 53/64 loss: -0.11209356784820557
Batch 54/64 loss: -0.11948221921920776
Batch 55/64 loss: -0.15075194835662842
Batch 56/64 loss: -0.12986493110656738
Batch 57/64 loss: -0.1318143606185913
Batch 58/64 loss: -0.1304861307144165
Batch 59/64 loss: -0.11370599269866943
Batch 60/64 loss: -0.12258279323577881
Batch 61/64 loss: -0.1197807788848877
Batch 62/64 loss: -0.12961506843566895
Batch 63/64 loss: -0.09568065404891968
Batch 64/64 loss: -0.13165777921676636
Epoch 399  Train loss: -0.13072257673039156  Val loss: 0.05536268011401199
Epoch 400
-------------------------------
Batch 1/64 loss: -0.1258847713470459
Batch 2/64 loss: -0.12999755144119263
Batch 3/64 loss: -0.11958616971969604
Batch 4/64 loss: -0.12134242057800293
Batch 5/64 loss: -0.13133734464645386
Batch 6/64 loss: -0.10901099443435669
Batch 7/64 loss: -0.15135568380355835
Batch 8/64 loss: -0.13744419813156128
Batch 9/64 loss: -0.1339471936225891
Batch 10/64 loss: -0.13180673122406006
Batch 11/64 loss: -0.13946688175201416
Batch 12/64 loss: -0.14940214157104492
Batch 13/64 loss: -0.1476975679397583
Batch 14/64 loss: -0.14974534511566162
Batch 15/64 loss: -0.10516762733459473
Batch 16/64 loss: -0.14333045482635498
Batch 17/64 loss: -0.16027528047561646
Batch 18/64 loss: -0.14696598052978516
Batch 19/64 loss: -0.13609731197357178
Batch 20/64 loss: -0.15489059686660767
Batch 21/64 loss: -0.14401519298553467
Batch 22/64 loss: -0.11466586589813232
Batch 23/64 loss: -0.12031996250152588
Batch 24/64 loss: -0.12054705619812012
Batch 25/64 loss: -0.12559616565704346
Batch 26/64 loss: -0.15563833713531494
Batch 27/64 loss: -0.12452483177185059
Batch 28/64 loss: -0.13240927457809448
Batch 29/64 loss: -0.09537369012832642
Batch 30/64 loss: -0.09739303588867188
Batch 31/64 loss: -0.12828463315963745
Batch 32/64 loss: -0.1270207166671753
Batch 33/64 loss: -0.1361503005027771
Batch 34/64 loss: -0.11411011219024658
Batch 35/64 loss: -0.12255591154098511
Batch 36/64 loss: -0.14091777801513672
Batch 37/64 loss: -0.14942693710327148
Batch 38/64 loss: -0.1510326862335205
Batch 39/64 loss: -0.14718854427337646
Batch 40/64 loss: -0.15694379806518555
Batch 41/64 loss: -0.11796212196350098
Batch 42/64 loss: -0.10403996706008911
Batch 43/64 loss: -0.12571418285369873
Batch 44/64 loss: -0.1289575695991516
Batch 45/64 loss: -0.12363028526306152
Batch 46/64 loss: -0.12808871269226074
Batch 47/64 loss: -0.13579535484313965
Batch 48/64 loss: -0.15676718950271606
Batch 49/64 loss: -0.1360301971435547
Batch 50/64 loss: -0.13465696573257446
Batch 51/64 loss: -0.12933409214019775
Batch 52/64 loss: -0.07949686050415039
Batch 53/64 loss: -0.1368710994720459
Batch 54/64 loss: -0.13125771284103394
Batch 55/64 loss: -0.13212549686431885
Batch 56/64 loss: -0.14823520183563232
Batch 57/64 loss: -0.13102465867996216
Batch 58/64 loss: -0.1211441159248352
Batch 59/64 loss: -0.12966221570968628
Batch 60/64 loss: -0.1485307812690735
Batch 61/64 loss: -0.1314803957939148
Batch 62/64 loss: -0.10797387361526489
Batch 63/64 loss: -0.14978396892547607
Batch 64/64 loss: -0.08999186754226685
Epoch 400  Train loss: -0.13121449409746655  Val loss: 0.0518784269024826
Epoch 401
-------------------------------
Batch 1/64 loss: -0.09793514013290405
Batch 2/64 loss: -0.12370169162750244
Batch 3/64 loss: -0.1327669620513916
Batch 4/64 loss: -0.14027100801467896
Batch 5/64 loss: -0.11639648675918579
Batch 6/64 loss: -0.1382492184638977
Batch 7/64 loss: -0.14663761854171753
Batch 8/64 loss: -0.14139056205749512
Batch 9/64 loss: -0.13880997896194458
Batch 10/64 loss: -0.15443319082260132
Batch 11/64 loss: -0.10904139280319214
Batch 12/64 loss: -0.11222094297409058
Batch 13/64 loss: -0.1371709704399109
Batch 14/64 loss: -0.15265315771102905
Batch 15/64 loss: -0.14270198345184326
Batch 16/64 loss: -0.14160364866256714
Batch 17/64 loss: -0.1194804310798645
Batch 18/64 loss: -0.15139031410217285
Batch 19/64 loss: -0.0916903018951416
Batch 20/64 loss: -0.13879776000976562
Batch 21/64 loss: -0.13464248180389404
Batch 22/64 loss: -0.12158417701721191
Batch 23/64 loss: -0.1560092568397522
Batch 24/64 loss: -0.13347244262695312
Batch 25/64 loss: -0.11065679788589478
Batch 26/64 loss: -0.07442206144332886
Batch 27/64 loss: -0.13007980585098267
Batch 28/64 loss: -0.1392177939414978
Batch 29/64 loss: -0.14504969120025635
Batch 30/64 loss: -0.13078701496124268
Batch 31/64 loss: -0.13589859008789062
Batch 32/64 loss: -0.13032174110412598
Batch 33/64 loss: -0.12252384424209595
Batch 34/64 loss: -0.13607293367385864
Batch 35/64 loss: -0.13509303331375122
Batch 36/64 loss: -0.13883274793624878
Batch 37/64 loss: -0.15583401918411255
Batch 38/64 loss: -0.1482536792755127
Batch 39/64 loss: -0.10302895307540894
Batch 40/64 loss: -0.14269810914993286
Batch 41/64 loss: -0.15910273790359497
Batch 42/64 loss: -0.11997538805007935
Batch 43/64 loss: -0.1480574607849121
Batch 44/64 loss: -0.13769328594207764
Batch 45/64 loss: -0.11092334985733032
Batch 46/64 loss: -0.14516597986221313
Batch 47/64 loss: -0.11529767513275146
Batch 48/64 loss: -0.1455312967300415
Batch 49/64 loss: -0.1265714168548584
Batch 50/64 loss: -0.10737162828445435
Batch 51/64 loss: -0.12683331966400146
Batch 52/64 loss: -0.14239364862442017
Batch 53/64 loss: -0.14080870151519775
Batch 54/64 loss: -0.10046893358230591
Batch 55/64 loss: -0.14014416933059692
Batch 56/64 loss: -0.14366257190704346
Batch 57/64 loss: -0.12717008590698242
Batch 58/64 loss: -0.14240700006484985
Batch 59/64 loss: -0.1276072859764099
Batch 60/64 loss: -0.1513928771018982
Batch 61/64 loss: -0.15733230113983154
Batch 62/64 loss: -0.15090668201446533
Batch 63/64 loss: -0.13412821292877197
Batch 64/64 loss: -0.1436278223991394
Epoch 401  Train loss: -0.1327135499785928  Val loss: 0.048874961346695105
Epoch 402
-------------------------------
Batch 1/64 loss: -0.11974042654037476
Batch 2/64 loss: -0.14821529388427734
Batch 3/64 loss: -0.15015041828155518
Batch 4/64 loss: -0.14115023612976074
Batch 5/64 loss: -0.15743553638458252
Batch 6/64 loss: -0.12123721837997437
Batch 7/64 loss: -0.13271820545196533
Batch 8/64 loss: -0.1517311930656433
Batch 9/64 loss: -0.12976503372192383
Batch 10/64 loss: -0.14609146118164062
Batch 11/64 loss: -0.14034581184387207
Batch 12/64 loss: -0.15055787563323975
Batch 13/64 loss: -0.14086413383483887
Batch 14/64 loss: -0.15714037418365479
Batch 15/64 loss: -0.14282965660095215
Batch 16/64 loss: -0.14798271656036377
Batch 17/64 loss: -0.1463000774383545
Batch 18/64 loss: -0.11856317520141602
Batch 19/64 loss: -0.1458674669265747
Batch 20/64 loss: -0.13275235891342163
Batch 21/64 loss: -0.1380443572998047
Batch 22/64 loss: -0.14037340879440308
Batch 23/64 loss: -0.1416553258895874
Batch 24/64 loss: -0.1299954056739807
Batch 25/64 loss: -0.14215624332427979
Batch 26/64 loss: -0.1447640061378479
Batch 27/64 loss: -0.11064285039901733
Batch 28/64 loss: -0.12122035026550293
Batch 29/64 loss: -0.13813185691833496
Batch 30/64 loss: -0.13632452487945557
Batch 31/64 loss: -0.14879894256591797
Batch 32/64 loss: -0.13044685125350952
Batch 33/64 loss: -0.12739616632461548
Batch 34/64 loss: -0.12509214878082275
Batch 35/64 loss: -0.10666555166244507
Batch 36/64 loss: -0.14492899179458618
Batch 37/64 loss: -0.1274127960205078
Batch 38/64 loss: -0.13892602920532227
Batch 39/64 loss: -0.0930059552192688
Batch 40/64 loss: -0.13142722845077515
Batch 41/64 loss: -0.1224203109741211
Batch 42/64 loss: -0.14072656631469727
Batch 43/64 loss: -0.10706549882888794
Batch 44/64 loss: -0.1373283863067627
Batch 45/64 loss: -0.12071502208709717
Batch 46/64 loss: -0.13900864124298096
Batch 47/64 loss: -0.11332511901855469
Batch 48/64 loss: -0.1216733455657959
Batch 49/64 loss: -0.12341082096099854
Batch 50/64 loss: -0.13855451345443726
Batch 51/64 loss: -0.12893223762512207
Batch 52/64 loss: -0.1482803225517273
Batch 53/64 loss: -0.14751654863357544
Batch 54/64 loss: -0.129754900932312
Batch 55/64 loss: -0.1595199704170227
Batch 56/64 loss: -0.14850986003875732
Batch 57/64 loss: -0.1499841809272766
Batch 58/64 loss: -0.12006151676177979
Batch 59/64 loss: -0.13721168041229248
Batch 60/64 loss: -0.14912980794906616
Batch 61/64 loss: -0.12075787782669067
Batch 62/64 loss: -0.1289299726486206
Batch 63/64 loss: -0.11949282884597778
Batch 64/64 loss: -0.14349520206451416
Epoch 402  Train loss: -0.13488280773162842  Val loss: 0.05197754082401184
Epoch 403
-------------------------------
Batch 1/64 loss: -0.1403791308403015
Batch 2/64 loss: -0.15341508388519287
Batch 3/64 loss: -0.12584346532821655
Batch 4/64 loss: -0.15097832679748535
Batch 5/64 loss: -0.13424944877624512
Batch 6/64 loss: -0.11400496959686279
Batch 7/64 loss: -0.13420969247817993
Batch 8/64 loss: -0.13789689540863037
Batch 9/64 loss: -0.1368616819381714
Batch 10/64 loss: -0.15011382102966309
Batch 11/64 loss: -0.13374042510986328
Batch 12/64 loss: -0.133855402469635
Batch 13/64 loss: -0.12883663177490234
Batch 14/64 loss: -0.1127123236656189
Batch 15/64 loss: -0.15130698680877686
Batch 16/64 loss: -0.13167345523834229
Batch 17/64 loss: -0.12019109725952148
Batch 18/64 loss: -0.12475156784057617
Batch 19/64 loss: -0.1460505723953247
Batch 20/64 loss: -0.13893747329711914
Batch 21/64 loss: -0.1456892490386963
Batch 22/64 loss: -0.16650265455245972
Batch 23/64 loss: -0.13308709859848022
Batch 24/64 loss: -0.1449342966079712
Batch 25/64 loss: -0.16114431619644165
Batch 26/64 loss: -0.12031441926956177
Batch 27/64 loss: -0.1453801393508911
Batch 28/64 loss: -0.11601722240447998
Batch 29/64 loss: -0.14124655723571777
Batch 30/64 loss: -0.12808048725128174
Batch 31/64 loss: -0.12134867906570435
Batch 32/64 loss: -0.17155838012695312
Batch 33/64 loss: -0.13774406909942627
Batch 34/64 loss: -0.14339321851730347
Batch 35/64 loss: -0.1146395206451416
Batch 36/64 loss: -0.12323379516601562
Batch 37/64 loss: -0.13230258226394653
Batch 38/64 loss: -0.15290969610214233
Batch 39/64 loss: -0.11926066875457764
Batch 40/64 loss: -0.12708455324172974
Batch 41/64 loss: -0.11585700511932373
Batch 42/64 loss: -0.11444681882858276
Batch 43/64 loss: -0.13816630840301514
Batch 44/64 loss: -0.12921565771102905
Batch 45/64 loss: -0.11202573776245117
Batch 46/64 loss: -0.14350956678390503
Batch 47/64 loss: -0.1263331174850464
Batch 48/64 loss: -0.14439547061920166
Batch 49/64 loss: -0.12789779901504517
Batch 50/64 loss: -0.1260494589805603
Batch 51/64 loss: -0.12406843900680542
Batch 52/64 loss: -0.13621747493743896
Batch 53/64 loss: -0.1093219518661499
Batch 54/64 loss: -0.14446425437927246
Batch 55/64 loss: -0.1465134620666504
Batch 56/64 loss: -0.14028912782669067
Batch 57/64 loss: -0.11421054601669312
Batch 58/64 loss: -0.1435476541519165
Batch 59/64 loss: -0.11310982704162598
Batch 60/64 loss: -0.11373645067214966
Batch 61/64 loss: -0.139359712600708
Batch 62/64 loss: -0.11729121208190918
Batch 63/64 loss: -0.13520067930221558
Batch 64/64 loss: -0.10371518135070801
Epoch 403  Train loss: -0.13300226155449363  Val loss: 0.051669454246861826
Epoch 404
-------------------------------
Batch 1/64 loss: -0.15212780237197876
Batch 2/64 loss: -0.12917256355285645
Batch 3/64 loss: -0.15992164611816406
Batch 4/64 loss: -0.1304410696029663
Batch 5/64 loss: -0.14827603101730347
Batch 6/64 loss: -0.12284356355667114
Batch 7/64 loss: -0.15496158599853516
Batch 8/64 loss: -0.1449766755104065
Batch 9/64 loss: -0.10337787866592407
Batch 10/64 loss: -0.10254478454589844
Batch 11/64 loss: -0.14285016059875488
Batch 12/64 loss: -0.16133588552474976
Batch 13/64 loss: -0.1324608325958252
Batch 14/64 loss: -0.1229962706565857
Batch 15/64 loss: -0.12983018159866333
Batch 16/64 loss: -0.1489238739013672
Batch 17/64 loss: -0.1559767723083496
Batch 18/64 loss: -0.13548994064331055
Batch 19/64 loss: -0.16918331384658813
Batch 20/64 loss: -0.1310199499130249
Batch 21/64 loss: -0.13362807035446167
Batch 22/64 loss: -0.11209946870803833
Batch 23/64 loss: -0.16150587797164917
Batch 24/64 loss: -0.1353369951248169
Batch 25/64 loss: -0.14578670263290405
Batch 26/64 loss: -0.09790486097335815
Batch 27/64 loss: -0.14327317476272583
Batch 28/64 loss: -0.14390164613723755
Batch 29/64 loss: -0.12860256433486938
Batch 30/64 loss: -0.1394355297088623
Batch 31/64 loss: -0.12194359302520752
Batch 32/64 loss: -0.11683756113052368
Batch 33/64 loss: -0.13888239860534668
Batch 34/64 loss: -0.09448099136352539
Batch 35/64 loss: -0.1352400779724121
Batch 36/64 loss: -0.10322904586791992
Batch 37/64 loss: -0.15395301580429077
Batch 38/64 loss: -0.1512824296951294
Batch 39/64 loss: -0.1360751986503601
Batch 40/64 loss: -0.14505457878112793
Batch 41/64 loss: -0.13122957944869995
Batch 42/64 loss: -0.13243746757507324
Batch 43/64 loss: -0.13209021091461182
Batch 44/64 loss: -0.1066889762878418
Batch 45/64 loss: -0.12969642877578735
Batch 46/64 loss: -0.11807447671890259
Batch 47/64 loss: -0.08610206842422485
Batch 48/64 loss: -0.1308496594429016
Batch 49/64 loss: -0.14178121089935303
Batch 50/64 loss: -0.12892115116119385
Batch 51/64 loss: -0.13498973846435547
Batch 52/64 loss: -0.12575316429138184
Batch 53/64 loss: -0.13149261474609375
Batch 54/64 loss: -0.12476390600204468
Batch 55/64 loss: -0.14018386602401733
Batch 56/64 loss: -0.13742375373840332
Batch 57/64 loss: -0.14055007696151733
Batch 58/64 loss: -0.12447702884674072
Batch 59/64 loss: -0.15743613243103027
Batch 60/64 loss: -0.15209674835205078
Batch 61/64 loss: -0.12193381786346436
Batch 62/64 loss: -0.12161970138549805
Batch 63/64 loss: -0.09918880462646484
Batch 64/64 loss: -0.13713759183883667
Epoch 404  Train loss: -0.13285958182577992  Val loss: 0.05219639863345221
Epoch 405
-------------------------------
Batch 1/64 loss: -0.12100929021835327
Batch 2/64 loss: -0.1452593207359314
Batch 3/64 loss: -0.13850224018096924
Batch 4/64 loss: -0.14070463180541992
Batch 5/64 loss: -0.13921064138412476
Batch 6/64 loss: -0.1546993851661682
Batch 7/64 loss: -0.16509729623794556
Batch 8/64 loss: -0.14694315195083618
Batch 9/64 loss: -0.13470959663391113
Batch 10/64 loss: -0.10290789604187012
Batch 11/64 loss: -0.1476467251777649
Batch 12/64 loss: -0.1465725302696228
Batch 13/64 loss: -0.1366032361984253
Batch 14/64 loss: -0.1441415548324585
Batch 15/64 loss: -0.12856918573379517
Batch 16/64 loss: -0.14884501695632935
Batch 17/64 loss: -0.14089560508728027
Batch 18/64 loss: -0.1605399250984192
Batch 19/64 loss: -0.14109599590301514
Batch 20/64 loss: -0.12668001651763916
Batch 21/64 loss: -0.11478263139724731
Batch 22/64 loss: -0.1221732497215271
Batch 23/64 loss: -0.12504762411117554
Batch 24/64 loss: -0.1331508755683899
Batch 25/64 loss: -0.13560378551483154
Batch 26/64 loss: -0.12527722120285034
Batch 27/64 loss: -0.17168965935707092
Batch 28/64 loss: -0.13252472877502441
Batch 29/64 loss: -0.14944839477539062
Batch 30/64 loss: -0.12001150846481323
Batch 31/64 loss: -0.10368204116821289
Batch 32/64 loss: -0.13792705535888672
Batch 33/64 loss: -0.11873674392700195
Batch 34/64 loss: -0.13422340154647827
Batch 35/64 loss: -0.143693745136261
Batch 36/64 loss: -0.14720499515533447
Batch 37/64 loss: -0.13571470975875854
Batch 38/64 loss: -0.12179166078567505
Batch 39/64 loss: -0.16133415699005127
Batch 40/64 loss: -0.11015892028808594
Batch 41/64 loss: -0.12268984317779541
Batch 42/64 loss: -0.13627487421035767
Batch 43/64 loss: -0.1290948987007141
Batch 44/64 loss: -0.10604715347290039
Batch 45/64 loss: -0.13123852014541626
Batch 46/64 loss: -0.132770836353302
Batch 47/64 loss: -0.12667608261108398
Batch 48/64 loss: -0.14398521184921265
Batch 49/64 loss: -0.13467693328857422
Batch 50/64 loss: -0.145524799823761
Batch 51/64 loss: -0.12838613986968994
Batch 52/64 loss: -0.11485093832015991
Batch 53/64 loss: -0.13957232236862183
Batch 54/64 loss: -0.11362028121948242
Batch 55/64 loss: -0.1312187910079956
Batch 56/64 loss: -0.14224791526794434
Batch 57/64 loss: -0.11746025085449219
Batch 58/64 loss: -0.15236693620681763
Batch 59/64 loss: -0.09640628099441528
Batch 60/64 loss: -0.11172795295715332
Batch 61/64 loss: -0.13938969373703003
Batch 62/64 loss: -0.1237075924873352
Batch 63/64 loss: -0.13934361934661865
Batch 64/64 loss: -0.13415372371673584
Epoch 405  Train loss: -0.13356397666183173  Val loss: 0.05274740035591256
Epoch 406
-------------------------------
Batch 1/64 loss: -0.14262545108795166
Batch 2/64 loss: -0.15366250276565552
Batch 3/64 loss: -0.10799664258956909
Batch 4/64 loss: -0.15040647983551025
Batch 5/64 loss: -0.1270829439163208
Batch 6/64 loss: -0.13266366720199585
Batch 7/64 loss: -0.13930439949035645
Batch 8/64 loss: -0.14225894212722778
Batch 9/64 loss: -0.11696147918701172
Batch 10/64 loss: -0.15025454759597778
Batch 11/64 loss: -0.1488094925880432
Batch 12/64 loss: -0.1103430986404419
Batch 13/64 loss: -0.10604166984558105
Batch 14/64 loss: -0.13198721408843994
Batch 15/64 loss: -0.159915030002594
Batch 16/64 loss: -0.08943158388137817
Batch 17/64 loss: -0.1335504651069641
Batch 18/64 loss: -0.14465445280075073
Batch 19/64 loss: -0.10333669185638428
Batch 20/64 loss: -0.1038515567779541
Batch 21/64 loss: -0.13055360317230225
Batch 22/64 loss: -0.14895397424697876
Batch 23/64 loss: -0.13555705547332764
Batch 24/64 loss: -0.14514124393463135
Batch 25/64 loss: -0.13131260871887207
Batch 26/64 loss: -0.1323036551475525
Batch 27/64 loss: -0.10738217830657959
Batch 28/64 loss: -0.12723857164382935
Batch 29/64 loss: -0.10529357194900513
Batch 30/64 loss: -0.1402488350868225
Batch 31/64 loss: -0.1360083818435669
Batch 32/64 loss: -0.13374269008636475
Batch 33/64 loss: -0.13511055707931519
Batch 34/64 loss: -0.15592211484909058
Batch 35/64 loss: -0.13696223497390747
Batch 36/64 loss: -0.143438458442688
Batch 37/64 loss: -0.13395369052886963
Batch 38/64 loss: -0.13512003421783447
Batch 39/64 loss: -0.14310425519943237
Batch 40/64 loss: -0.10911530256271362
Batch 41/64 loss: -0.12793272733688354
Batch 42/64 loss: -0.13021856546401978
Batch 43/64 loss: -0.14605951309204102
Batch 44/64 loss: -0.13343262672424316
Batch 45/64 loss: -0.13488870859146118
Batch 46/64 loss: -0.13842976093292236
Batch 47/64 loss: -0.13983798027038574
Batch 48/64 loss: -0.15843379497528076
Batch 49/64 loss: -0.12537384033203125
Batch 50/64 loss: -0.15354657173156738
Batch 51/64 loss: -0.11532622575759888
Batch 52/64 loss: -0.12003147602081299
Batch 53/64 loss: -0.11879003047943115
Batch 54/64 loss: -0.10022330284118652
Batch 55/64 loss: -0.13431143760681152
Batch 56/64 loss: -0.1377297043800354
Batch 57/64 loss: -0.14611399173736572
Batch 58/64 loss: -0.13914811611175537
Batch 59/64 loss: -0.1414961814880371
Batch 60/64 loss: -0.14445751905441284
Batch 61/64 loss: -0.1322265863418579
Batch 62/64 loss: -0.10200285911560059
Batch 63/64 loss: -0.1250891089439392
Batch 64/64 loss: -0.1083419919013977
Epoch 406  Train loss: -0.1315758188565572  Val loss: 0.05397709579402229
Epoch 407
-------------------------------
Batch 1/64 loss: -0.10280561447143555
Batch 2/64 loss: -0.127227783203125
Batch 3/64 loss: -0.14103013277053833
Batch 4/64 loss: -0.15598779916763306
Batch 5/64 loss: -0.12734782695770264
Batch 6/64 loss: -0.15380340814590454
Batch 7/64 loss: -0.1511629819869995
Batch 8/64 loss: -0.136677086353302
Batch 9/64 loss: -0.1211845874786377
Batch 10/64 loss: -0.14483642578125
Batch 11/64 loss: -0.16175198554992676
Batch 12/64 loss: -0.14598149061203003
Batch 13/64 loss: -0.15403145551681519
Batch 14/64 loss: -0.13559556007385254
Batch 15/64 loss: -0.17164039611816406
Batch 16/64 loss: -0.14256083965301514
Batch 17/64 loss: -0.15491408109664917
Batch 18/64 loss: -0.14254963397979736
Batch 19/64 loss: -0.1327270269393921
Batch 20/64 loss: -0.16932329535484314
Batch 21/64 loss: -0.13735008239746094
Batch 22/64 loss: -0.10393774509429932
Batch 23/64 loss: -0.14632296562194824
Batch 24/64 loss: -0.10282254219055176
Batch 25/64 loss: -0.1386139988899231
Batch 26/64 loss: -0.15048658847808838
Batch 27/64 loss: -0.1545504331588745
Batch 28/64 loss: -0.1506149172782898
Batch 29/64 loss: -0.10981398820877075
Batch 30/64 loss: -0.11340051889419556
Batch 31/64 loss: -0.11908948421478271
Batch 32/64 loss: -0.13294923305511475
Batch 33/64 loss: -0.14116215705871582
Batch 34/64 loss: -0.12030762434005737
Batch 35/64 loss: -0.12954652309417725
Batch 36/64 loss: -0.10998547077178955
Batch 37/64 loss: -0.11386716365814209
Batch 38/64 loss: -0.12747186422348022
Batch 39/64 loss: -0.13360029458999634
Batch 40/64 loss: -0.13261562585830688
Batch 41/64 loss: -0.14307475090026855
Batch 42/64 loss: -0.10951972007751465
Batch 43/64 loss: -0.11567920446395874
Batch 44/64 loss: -0.13898426294326782
Batch 45/64 loss: -0.12012749910354614
Batch 46/64 loss: -0.1103372573852539
Batch 47/64 loss: -0.10511630773544312
Batch 48/64 loss: -0.1546725630760193
Batch 49/64 loss: -0.1416742205619812
Batch 50/64 loss: -0.12698447704315186
Batch 51/64 loss: -0.1321128010749817
Batch 52/64 loss: -0.1073949933052063
Batch 53/64 loss: -0.1434289813041687
Batch 54/64 loss: -0.1055641770362854
Batch 55/64 loss: -0.1438022255897522
Batch 56/64 loss: -0.14020264148712158
Batch 57/64 loss: -0.09297657012939453
Batch 58/64 loss: -0.14035522937774658
Batch 59/64 loss: -0.12218153476715088
Batch 60/64 loss: -0.08946800231933594
Batch 61/64 loss: -0.13018131256103516
Batch 62/64 loss: -0.13403677940368652
Batch 63/64 loss: -0.1331566572189331
Batch 64/64 loss: -0.1387183666229248
Epoch 407  Train loss: -0.13218383648816276  Val loss: 0.05176460599571569
Epoch 408
-------------------------------
Batch 1/64 loss: -0.14452850818634033
Batch 2/64 loss: -0.13933587074279785
Batch 3/64 loss: -0.13730406761169434
Batch 4/64 loss: -0.1565183401107788
Batch 5/64 loss: -0.15181362628936768
Batch 6/64 loss: -0.1315290927886963
Batch 7/64 loss: -0.13975852727890015
Batch 8/64 loss: -0.11920833587646484
Batch 9/64 loss: -0.17913195490837097
Batch 10/64 loss: -0.14586567878723145
Batch 11/64 loss: -0.1427398920059204
Batch 12/64 loss: -0.1327490210533142
Batch 13/64 loss: -0.14486908912658691
Batch 14/64 loss: -0.14116281270980835
Batch 15/64 loss: -0.14528435468673706
Batch 16/64 loss: -0.13219225406646729
Batch 17/64 loss: -0.11099356412887573
Batch 18/64 loss: -0.08265084028244019
Batch 19/64 loss: -0.12611663341522217
Batch 20/64 loss: -0.1408960223197937
Batch 21/64 loss: -0.11919701099395752
Batch 22/64 loss: -0.12322461605072021
Batch 23/64 loss: -0.17064109444618225
Batch 24/64 loss: -0.11052674055099487
Batch 25/64 loss: -0.14260971546173096
Batch 26/64 loss: -0.12604695558547974
Batch 27/64 loss: -0.13593006134033203
Batch 28/64 loss: -0.11857485771179199
Batch 29/64 loss: -0.12831157445907593
Batch 30/64 loss: -0.13276469707489014
Batch 31/64 loss: -0.1235162615776062
Batch 32/64 loss: -0.13264334201812744
Batch 33/64 loss: -0.13288819789886475
Batch 34/64 loss: -0.11626303195953369
Batch 35/64 loss: -0.11472809314727783
Batch 36/64 loss: -0.14543437957763672
Batch 37/64 loss: -0.11118215322494507
Batch 38/64 loss: -0.12378877401351929
Batch 39/64 loss: -0.14606189727783203
Batch 40/64 loss: -0.15662777423858643
Batch 41/64 loss: -0.11297845840454102
Batch 42/64 loss: -0.11797624826431274
Batch 43/64 loss: -0.1376802921295166
Batch 44/64 loss: -0.1179589033126831
Batch 45/64 loss: -0.06620323657989502
Batch 46/64 loss: -0.1449156403541565
Batch 47/64 loss: -0.1279439926147461
Batch 48/64 loss: -0.12668383121490479
Batch 49/64 loss: -0.1308642029762268
Batch 50/64 loss: -0.12005841732025146
Batch 51/64 loss: -0.1455777883529663
Batch 52/64 loss: -0.1555057168006897
Batch 53/64 loss: -0.1469717025756836
Batch 54/64 loss: -0.12959277629852295
Batch 55/64 loss: -0.12250840663909912
Batch 56/64 loss: -0.13274610042572021
Batch 57/64 loss: -0.12298649549484253
Batch 58/64 loss: -0.12963652610778809
Batch 59/64 loss: -0.15613943338394165
Batch 60/64 loss: -0.15850991010665894
Batch 61/64 loss: -0.12338435649871826
Batch 62/64 loss: -0.1369611620903015
Batch 63/64 loss: -0.1423203945159912
Batch 64/64 loss: -0.12501919269561768
Epoch 408  Train loss: -0.1326349506191179  Val loss: 0.05405063870846201
Epoch 409
-------------------------------
Batch 1/64 loss: -0.13141405582427979
Batch 2/64 loss: -0.1126517653465271
Batch 3/64 loss: -0.13133621215820312
Batch 4/64 loss: -0.12808823585510254
Batch 5/64 loss: -0.14067065715789795
Batch 6/64 loss: -0.12843936681747437
Batch 7/64 loss: -0.11746233701705933
Batch 8/64 loss: -0.11630016565322876
Batch 9/64 loss: -0.11951225996017456
Batch 10/64 loss: -0.13318294286727905
Batch 11/64 loss: -0.14938795566558838
Batch 12/64 loss: -0.13635319471359253
Batch 13/64 loss: -0.13426262140274048
Batch 14/64 loss: -0.13945651054382324
Batch 15/64 loss: -0.14258277416229248
Batch 16/64 loss: -0.15671706199645996
Batch 17/64 loss: -0.1086195707321167
Batch 18/64 loss: -0.11985886096954346
Batch 19/64 loss: -0.13572126626968384
Batch 20/64 loss: -0.13755571842193604
Batch 21/64 loss: -0.14628303050994873
Batch 22/64 loss: -0.15269523859024048
Batch 23/64 loss: -0.1298808455467224
Batch 24/64 loss: -0.12613767385482788
Batch 25/64 loss: -0.13529443740844727
Batch 26/64 loss: -0.15457981824874878
Batch 27/64 loss: -0.16776451468467712
Batch 28/64 loss: -0.13812100887298584
Batch 29/64 loss: -0.1699693202972412
Batch 30/64 loss: -0.14508295059204102
Batch 31/64 loss: -0.13772350549697876
Batch 32/64 loss: -0.09909141063690186
Batch 33/64 loss: -0.14709758758544922
Batch 34/64 loss: -0.12959730625152588
Batch 35/64 loss: -0.16278839111328125
Batch 36/64 loss: -0.13713860511779785
Batch 37/64 loss: -0.13528841733932495
Batch 38/64 loss: -0.12641537189483643
Batch 39/64 loss: -0.16869351267814636
Batch 40/64 loss: -0.15290701389312744
Batch 41/64 loss: -0.11913937330245972
Batch 42/64 loss: -0.13504940271377563
Batch 43/64 loss: -0.12638837099075317
Batch 44/64 loss: -0.1291649341583252
Batch 45/64 loss: -0.1352783441543579
Batch 46/64 loss: -0.10888040065765381
Batch 47/64 loss: -0.14612531661987305
Batch 48/64 loss: -0.15779155492782593
Batch 49/64 loss: -0.09897774457931519
Batch 50/64 loss: -0.12385517358779907
Batch 51/64 loss: -0.13411015272140503
Batch 52/64 loss: -0.15235894918441772
Batch 53/64 loss: -0.1349196434020996
Batch 54/64 loss: -0.148914635181427
Batch 55/64 loss: -0.14496946334838867
Batch 56/64 loss: -0.13764947652816772
Batch 57/64 loss: -0.11104899644851685
Batch 58/64 loss: -0.13894027471542358
Batch 59/64 loss: -0.11106657981872559
Batch 60/64 loss: -0.11368530988693237
Batch 61/64 loss: -0.10790526866912842
Batch 62/64 loss: -0.14061737060546875
Batch 63/64 loss: -0.14961034059524536
Batch 64/64 loss: -0.11800205707550049
Epoch 409  Train loss: -0.13454230766670378  Val loss: 0.0539308634820263
Epoch 410
-------------------------------
Batch 1/64 loss: -0.14526033401489258
Batch 2/64 loss: -0.12311869859695435
Batch 3/64 loss: -0.1371619701385498
Batch 4/64 loss: -0.13899528980255127
Batch 5/64 loss: -0.1191103458404541
Batch 6/64 loss: -0.15151119232177734
Batch 7/64 loss: -0.11872851848602295
Batch 8/64 loss: -0.12039470672607422
Batch 9/64 loss: -0.141862154006958
Batch 10/64 loss: -0.13165253400802612
Batch 11/64 loss: -0.1507556438446045
Batch 12/64 loss: -0.12936240434646606
Batch 13/64 loss: -0.13597047328948975
Batch 14/64 loss: -0.14532268047332764
Batch 15/64 loss: -0.12515604496002197
Batch 16/64 loss: -0.14679789543151855
Batch 17/64 loss: -0.14364683628082275
Batch 18/64 loss: -0.15105926990509033
Batch 19/64 loss: -0.12887424230575562
Batch 20/64 loss: -0.14987891912460327
Batch 21/64 loss: -0.1409028172492981
Batch 22/64 loss: -0.10649150609970093
Batch 23/64 loss: -0.15594828128814697
Batch 24/64 loss: -0.14572477340698242
Batch 25/64 loss: -0.15302234888076782
Batch 26/64 loss: -0.131428062915802
Batch 27/64 loss: -0.15853869915008545
Batch 28/64 loss: -0.13985556364059448
Batch 29/64 loss: -0.1436261534690857
Batch 30/64 loss: -0.11533457040786743
Batch 31/64 loss: -0.1480812430381775
Batch 32/64 loss: -0.14546442031860352
Batch 33/64 loss: -0.11345380544662476
Batch 34/64 loss: -0.1498703956604004
Batch 35/64 loss: -0.1353548765182495
Batch 36/64 loss: -0.1233675479888916
Batch 37/64 loss: -0.12088996171951294
Batch 38/64 loss: -0.10550463199615479
Batch 39/64 loss: -0.13892775774002075
Batch 40/64 loss: -0.1637336015701294
Batch 41/64 loss: -0.1386662721633911
Batch 42/64 loss: -0.11515986919403076
Batch 43/64 loss: -0.14475077390670776
Batch 44/64 loss: -0.13076812028884888
Batch 45/64 loss: -0.15226417779922485
Batch 46/64 loss: -0.14116591215133667
Batch 47/64 loss: -0.12965607643127441
Batch 48/64 loss: -0.11966943740844727
Batch 49/64 loss: -0.13551491498947144
Batch 50/64 loss: -0.13370877504348755
Batch 51/64 loss: -0.1342785358428955
Batch 52/64 loss: -0.1381266713142395
Batch 53/64 loss: -0.15344524383544922
Batch 54/64 loss: -0.11391037702560425
Batch 55/64 loss: -0.13400715589523315
Batch 56/64 loss: -0.13510960340499878
Batch 57/64 loss: -0.12243187427520752
Batch 58/64 loss: -0.15610408782958984
Batch 59/64 loss: -0.1286676526069641
Batch 60/64 loss: -0.14691269397735596
Batch 61/64 loss: -0.11218881607055664
Batch 62/64 loss: -0.12351775169372559
Batch 63/64 loss: -0.12185001373291016
Batch 64/64 loss: -0.11678367853164673
Epoch 410  Train loss: -0.1352094704029607  Val loss: 0.0531450653403895
Epoch 411
-------------------------------
Batch 1/64 loss: -0.12640202045440674
Batch 2/64 loss: -0.16464123129844666
Batch 3/64 loss: -0.15163958072662354
Batch 4/64 loss: -0.14402824640274048
Batch 5/64 loss: -0.12798607349395752
Batch 6/64 loss: -0.143266499042511
Batch 7/64 loss: -0.11802530288696289
Batch 8/64 loss: -0.12072837352752686
Batch 9/64 loss: -0.11704212427139282
Batch 10/64 loss: -0.15460026264190674
Batch 11/64 loss: -0.09366023540496826
Batch 12/64 loss: -0.13146984577178955
Batch 13/64 loss: -0.11462759971618652
Batch 14/64 loss: -0.13270163536071777
Batch 15/64 loss: -0.15390050411224365
Batch 16/64 loss: -0.14120584726333618
Batch 17/64 loss: -0.10715460777282715
Batch 18/64 loss: -0.14225232601165771
Batch 19/64 loss: -0.1447618007659912
Batch 20/64 loss: -0.1256769895553589
Batch 21/64 loss: -0.11159825325012207
Batch 22/64 loss: -0.10726320743560791
Batch 23/64 loss: -0.1284400224685669
Batch 24/64 loss: -0.1389331817626953
Batch 25/64 loss: -0.14109814167022705
Batch 26/64 loss: -0.10878163576126099
Batch 27/64 loss: -0.14759117364883423
Batch 28/64 loss: -0.1102454662322998
Batch 29/64 loss: -0.1313934326171875
Batch 30/64 loss: -0.14524877071380615
Batch 31/64 loss: -0.13206171989440918
Batch 32/64 loss: -0.1417602300643921
Batch 33/64 loss: -0.1113882064819336
Batch 34/64 loss: -0.14172542095184326
Batch 35/64 loss: -0.13621622323989868
Batch 36/64 loss: -0.1345018744468689
Batch 37/64 loss: -0.10412079095840454
Batch 38/64 loss: -0.13263249397277832
Batch 39/64 loss: -0.14269280433654785
Batch 40/64 loss: -0.11839693784713745
Batch 41/64 loss: -0.14289844036102295
Batch 42/64 loss: -0.12744677066802979
Batch 43/64 loss: -0.12265640497207642
Batch 44/64 loss: -0.1368335485458374
Batch 45/64 loss: -0.1426529884338379
Batch 46/64 loss: -0.14281153678894043
Batch 47/64 loss: -0.16932201385498047
Batch 48/64 loss: -0.12484300136566162
Batch 49/64 loss: -0.11301195621490479
Batch 50/64 loss: -0.1131938099861145
Batch 51/64 loss: -0.13070660829544067
Batch 52/64 loss: -0.16607636213302612
Batch 53/64 loss: -0.13591742515563965
Batch 54/64 loss: -0.11530739068984985
Batch 55/64 loss: -0.1393280029296875
Batch 56/64 loss: -0.1373494267463684
Batch 57/64 loss: -0.14793044328689575
Batch 58/64 loss: -0.13270968198776245
Batch 59/64 loss: -0.12626266479492188
Batch 60/64 loss: -0.14540982246398926
Batch 61/64 loss: -0.14849263429641724
Batch 62/64 loss: -0.1490992307662964
Batch 63/64 loss: -0.1289825439453125
Batch 64/64 loss: -0.12628191709518433
Epoch 411  Train loss: -0.13264023907044353  Val loss: 0.053348463015867674
Epoch 412
-------------------------------
Batch 1/64 loss: -0.14279955625534058
Batch 2/64 loss: -0.15060639381408691
Batch 3/64 loss: -0.1302868127822876
Batch 4/64 loss: -0.1282908320426941
Batch 5/64 loss: -0.13421308994293213
Batch 6/64 loss: -0.1294018030166626
Batch 7/64 loss: -0.14583361148834229
Batch 8/64 loss: -0.10607832670211792
Batch 9/64 loss: -0.14197081327438354
Batch 10/64 loss: -0.1422278881072998
Batch 11/64 loss: -0.15094655752182007
Batch 12/64 loss: -0.1260804533958435
Batch 13/64 loss: -0.1589357852935791
Batch 14/64 loss: -0.1211887001991272
Batch 15/64 loss: -0.15067428350448608
Batch 16/64 loss: -0.12322986125946045
Batch 17/64 loss: -0.15214842557907104
Batch 18/64 loss: -0.11123549938201904
Batch 19/64 loss: -0.13607418537139893
Batch 20/64 loss: -0.12201535701751709
Batch 21/64 loss: -0.16969066858291626
Batch 22/64 loss: -0.14264237880706787
Batch 23/64 loss: -0.16478747129440308
Batch 24/64 loss: -0.13347548246383667
Batch 25/64 loss: -0.137193500995636
Batch 26/64 loss: -0.11754930019378662
Batch 27/64 loss: -0.13826626539230347
Batch 28/64 loss: -0.16024667024612427
Batch 29/64 loss: -0.08795946836471558
Batch 30/64 loss: -0.13335466384887695
Batch 31/64 loss: -0.13323760032653809
Batch 32/64 loss: -0.1303480863571167
Batch 33/64 loss: -0.1522364616394043
Batch 34/64 loss: -0.15859925746917725
Batch 35/64 loss: -0.10253638029098511
Batch 36/64 loss: -0.1308135986328125
Batch 37/64 loss: -0.16073447465896606
Batch 38/64 loss: -0.10952210426330566
Batch 39/64 loss: -0.10620224475860596
Batch 40/64 loss: -0.13990408182144165
Batch 41/64 loss: -0.10752886533737183
Batch 42/64 loss: -0.1411534547805786
Batch 43/64 loss: -0.1297397017478943
Batch 44/64 loss: -0.1393066644668579
Batch 45/64 loss: -0.13098108768463135
Batch 46/64 loss: -0.12909555435180664
Batch 47/64 loss: -0.14284539222717285
Batch 48/64 loss: -0.12246918678283691
Batch 49/64 loss: -0.11406588554382324
Batch 50/64 loss: -0.13837116956710815
Batch 51/64 loss: -0.12020516395568848
Batch 52/64 loss: -0.13221365213394165
Batch 53/64 loss: -0.15423578023910522
Batch 54/64 loss: -0.1153377890586853
Batch 55/64 loss: -0.11924296617507935
Batch 56/64 loss: -0.14621615409851074
Batch 57/64 loss: -0.10412335395812988
Batch 58/64 loss: -0.10766053199768066
Batch 59/64 loss: -0.15529406070709229
Batch 60/64 loss: -0.12036412954330444
Batch 61/64 loss: -0.14900809526443481
Batch 62/64 loss: -0.13752073049545288
Batch 63/64 loss: -0.1291137933731079
Batch 64/64 loss: -0.13095623254776
Epoch 412  Train loss: -0.13329958798838595  Val loss: 0.05852716546697715
Epoch 413
-------------------------------
Batch 1/64 loss: -0.13262611627578735
Batch 2/64 loss: -0.12802064418792725
Batch 3/64 loss: -0.14529269933700562
Batch 4/64 loss: -0.1525256633758545
Batch 5/64 loss: -0.13843989372253418
Batch 6/64 loss: -0.13072001934051514
Batch 7/64 loss: -0.13355618715286255
Batch 8/64 loss: -0.1273348331451416
Batch 9/64 loss: -0.16280755400657654
Batch 10/64 loss: -0.13707327842712402
Batch 11/64 loss: -0.12603551149368286
Batch 12/64 loss: -0.12283903360366821
Batch 13/64 loss: -0.14526408910751343
Batch 14/64 loss: -0.14803874492645264
Batch 15/64 loss: -0.12235492467880249
Batch 16/64 loss: -0.13810813426971436
Batch 17/64 loss: -0.1522367000579834
Batch 18/64 loss: -0.15542227029800415
Batch 19/64 loss: -0.12644118070602417
Batch 20/64 loss: -0.16738659143447876
Batch 21/64 loss: -0.12550663948059082
Batch 22/64 loss: -0.16196131706237793
Batch 23/64 loss: -0.13250523805618286
Batch 24/64 loss: -0.13790059089660645
Batch 25/64 loss: -0.14490091800689697
Batch 26/64 loss: -0.14076775312423706
Batch 27/64 loss: -0.14737308025360107
Batch 28/64 loss: -0.14326465129852295
Batch 29/64 loss: -0.14616626501083374
Batch 30/64 loss: -0.14683961868286133
Batch 31/64 loss: -0.13773977756500244
Batch 32/64 loss: -0.13617265224456787
Batch 33/64 loss: -0.13520139455795288
Batch 34/64 loss: -0.13703417778015137
Batch 35/64 loss: -0.12411588430404663
Batch 36/64 loss: -0.1151847243309021
Batch 37/64 loss: -0.14323484897613525
Batch 38/64 loss: -0.14826548099517822
Batch 39/64 loss: -0.1272679567337036
Batch 40/64 loss: -0.12929487228393555
Batch 41/64 loss: -0.10982131958007812
Batch 42/64 loss: -0.159065842628479
Batch 43/64 loss: -0.15367132425308228
Batch 44/64 loss: -0.12461996078491211
Batch 45/64 loss: -0.10034066438674927
Batch 46/64 loss: -0.13524127006530762
Batch 47/64 loss: -0.1136317253112793
Batch 48/64 loss: -0.12644290924072266
Batch 49/64 loss: -0.1257835030555725
Batch 50/64 loss: -0.1366492509841919
Batch 51/64 loss: -0.1354084610939026
Batch 52/64 loss: -0.14659059047698975
Batch 53/64 loss: -0.12361228466033936
Batch 54/64 loss: -0.1209789514541626
Batch 55/64 loss: -0.13458967208862305
Batch 56/64 loss: -0.17180460691452026
Batch 57/64 loss: -0.10859096050262451
Batch 58/64 loss: -0.14217472076416016
Batch 59/64 loss: -0.1421065330505371
Batch 60/64 loss: -0.1338120698928833
Batch 61/64 loss: -0.11474835872650146
Batch 62/64 loss: -0.14419865608215332
Batch 63/64 loss: -0.12647992372512817
Batch 64/64 loss: -0.1049509048461914
Epoch 413  Train loss: -0.135879194502737  Val loss: 0.05393844699531896
Epoch 414
-------------------------------
Batch 1/64 loss: -0.1620239019393921
Batch 2/64 loss: -0.12817084789276123
Batch 3/64 loss: -0.1526566743850708
Batch 4/64 loss: -0.12391579151153564
Batch 5/64 loss: -0.12953191995620728
Batch 6/64 loss: -0.13172924518585205
Batch 7/64 loss: -0.1283169984817505
Batch 8/64 loss: -0.11163538694381714
Batch 9/64 loss: -0.17058229446411133
Batch 10/64 loss: -0.09251230955123901
Batch 11/64 loss: -0.14588183164596558
Batch 12/64 loss: -0.12898391485214233
Batch 13/64 loss: -0.14385128021240234
Batch 14/64 loss: -0.15983259677886963
Batch 15/64 loss: -0.0999419093132019
Batch 16/64 loss: -0.14138013124465942
Batch 17/64 loss: -0.12930142879486084
Batch 18/64 loss: -0.14211517572402954
Batch 19/64 loss: -0.12259376049041748
Batch 20/64 loss: -0.1275879144668579
Batch 21/64 loss: -0.13532280921936035
Batch 22/64 loss: -0.09907132387161255
Batch 23/64 loss: -0.12340891361236572
Batch 24/64 loss: -0.13851892948150635
Batch 25/64 loss: -0.14307022094726562
Batch 26/64 loss: -0.11509090662002563
Batch 27/64 loss: -0.14765483140945435
Batch 28/64 loss: -0.1407368779182434
Batch 29/64 loss: -0.13165730237960815
Batch 30/64 loss: -0.1619960069656372
Batch 31/64 loss: -0.1465463638305664
Batch 32/64 loss: -0.13456255197525024
Batch 33/64 loss: -0.13393086194992065
Batch 34/64 loss: -0.1280638575553894
Batch 35/64 loss: -0.1590721607208252
Batch 36/64 loss: -0.1434897780418396
Batch 37/64 loss: -0.13199776411056519
Batch 38/64 loss: -0.15994727611541748
Batch 39/64 loss: -0.14390259981155396
Batch 40/64 loss: -0.15368229150772095
Batch 41/64 loss: -0.11200356483459473
Batch 42/64 loss: -0.1523836851119995
Batch 43/64 loss: -0.12948906421661377
Batch 44/64 loss: -0.13765311241149902
Batch 45/64 loss: -0.12854856252670288
Batch 46/64 loss: -0.13532406091690063
Batch 47/64 loss: -0.13572734594345093
Batch 48/64 loss: -0.13679969310760498
Batch 49/64 loss: -0.15408116579055786
Batch 50/64 loss: -0.1344199776649475
Batch 51/64 loss: -0.1462283730506897
Batch 52/64 loss: -0.16404303908348083
Batch 53/64 loss: -0.10731768608093262
Batch 54/64 loss: -0.14050662517547607
Batch 55/64 loss: -0.14573049545288086
Batch 56/64 loss: -0.11912250518798828
Batch 57/64 loss: -0.14137309789657593
Batch 58/64 loss: -0.11277991533279419
Batch 59/64 loss: -0.15812182426452637
Batch 60/64 loss: -0.12724632024765015
Batch 61/64 loss: -0.15545636415481567
Batch 62/64 loss: -0.14077824354171753
Batch 63/64 loss: -0.16478678584098816
Batch 64/64 loss: -0.13977926969528198
Epoch 414  Train loss: -0.13692588081546858  Val loss: 0.05376385843630919
Epoch 415
-------------------------------
Batch 1/64 loss: -0.11807715892791748
Batch 2/64 loss: -0.1372939944267273
Batch 3/64 loss: -0.11183685064315796
Batch 4/64 loss: -0.12937331199645996
Batch 5/64 loss: -0.1633141040802002
Batch 6/64 loss: -0.13253265619277954
Batch 7/64 loss: -0.15761399269104004
Batch 8/64 loss: -0.1368006467819214
Batch 9/64 loss: -0.1482555866241455
Batch 10/64 loss: -0.14399725198745728
Batch 11/64 loss: -0.1435927152633667
Batch 12/64 loss: -0.1429128646850586
Batch 13/64 loss: -0.13065195083618164
Batch 14/64 loss: -0.12693673372268677
Batch 15/64 loss: -0.13151609897613525
Batch 16/64 loss: -0.13577502965927124
Batch 17/64 loss: -0.14635270833969116
Batch 18/64 loss: -0.179365336894989
Batch 19/64 loss: -0.15416747331619263
Batch 20/64 loss: -0.1257125735282898
Batch 21/64 loss: -0.14976513385772705
Batch 22/64 loss: -0.14849483966827393
Batch 23/64 loss: -0.11497604846954346
Batch 24/64 loss: -0.13758617639541626
Batch 25/64 loss: -0.14715570211410522
Batch 26/64 loss: -0.13844060897827148
Batch 27/64 loss: -0.13522183895111084
Batch 28/64 loss: -0.12399488687515259
Batch 29/64 loss: -0.133331298828125
Batch 30/64 loss: -0.10674035549163818
Batch 31/64 loss: -0.12575364112854004
Batch 32/64 loss: -0.15127503871917725
Batch 33/64 loss: -0.15707659721374512
Batch 34/64 loss: -0.16545099020004272
Batch 35/64 loss: -0.1385732889175415
Batch 36/64 loss: -0.11609458923339844
Batch 37/64 loss: -0.11908507347106934
Batch 38/64 loss: -0.16145068407058716
Batch 39/64 loss: -0.15429973602294922
Batch 40/64 loss: -0.14072126150131226
Batch 41/64 loss: -0.13115590810775757
Batch 42/64 loss: -0.11795598268508911
Batch 43/64 loss: -0.1403520107269287
Batch 44/64 loss: -0.12294948101043701
Batch 45/64 loss: -0.15159392356872559
Batch 46/64 loss: -0.1280565857887268
Batch 47/64 loss: -0.1386662721633911
Batch 48/64 loss: -0.11601191759109497
Batch 49/64 loss: -0.13522601127624512
Batch 50/64 loss: -0.16239818930625916
Batch 51/64 loss: -0.1385362148284912
Batch 52/64 loss: -0.13769930601119995
Batch 53/64 loss: -0.11154758930206299
Batch 54/64 loss: -0.14632785320281982
Batch 55/64 loss: -0.12294155359268188
Batch 56/64 loss: -0.13910430669784546
Batch 57/64 loss: -0.14441323280334473
Batch 58/64 loss: -0.14967727661132812
Batch 59/64 loss: -0.13082748651504517
Batch 60/64 loss: -0.12826335430145264
Batch 61/64 loss: -0.13635516166687012
Batch 62/64 loss: -0.13480961322784424
Batch 63/64 loss: -0.1294267177581787
Batch 64/64 loss: -0.15578866004943848
Epoch 415  Train loss: -0.13761104742685953  Val loss: 0.049683047733765695
Epoch 416
-------------------------------
Batch 1/64 loss: -0.11964106559753418
Batch 2/64 loss: -0.14289283752441406
Batch 3/64 loss: -0.1341370940208435
Batch 4/64 loss: -0.1251644492149353
Batch 5/64 loss: -0.14788568019866943
Batch 6/64 loss: -0.14354127645492554
Batch 7/64 loss: -0.12251514196395874
Batch 8/64 loss: -0.14394903182983398
Batch 9/64 loss: -0.15740704536437988
Batch 10/64 loss: -0.14806699752807617
Batch 11/64 loss: -0.1348336935043335
Batch 12/64 loss: -0.1432713270187378
Batch 13/64 loss: -0.143915057182312
Batch 14/64 loss: -0.11931079626083374
Batch 15/64 loss: -0.13480037450790405
Batch 16/64 loss: -0.13952785730361938
Batch 17/64 loss: -0.16290315985679626
Batch 18/64 loss: -0.10238766670227051
Batch 19/64 loss: -0.12235939502716064
Batch 20/64 loss: -0.15669745206832886
Batch 21/64 loss: -0.12221664190292358
Batch 22/64 loss: -0.12155759334564209
Batch 23/64 loss: -0.14619475603103638
Batch 24/64 loss: -0.13874191045761108
Batch 25/64 loss: -0.12597644329071045
Batch 26/64 loss: -0.16636446118354797
Batch 27/64 loss: -0.1349591612815857
Batch 28/64 loss: -0.13351857662200928
Batch 29/64 loss: -0.09962069988250732
Batch 30/64 loss: -0.13568294048309326
Batch 31/64 loss: -0.1579301953315735
Batch 32/64 loss: -0.13037002086639404
Batch 33/64 loss: -0.12457680702209473
Batch 34/64 loss: -0.1282244324684143
Batch 35/64 loss: -0.14747297763824463
Batch 36/64 loss: -0.13211196660995483
Batch 37/64 loss: -0.11002087593078613
Batch 38/64 loss: -0.12690401077270508
Batch 39/64 loss: -0.13140344619750977
Batch 40/64 loss: -0.1409168243408203
Batch 41/64 loss: -0.15160298347473145
Batch 42/64 loss: -0.12554407119750977
Batch 43/64 loss: -0.13831180334091187
Batch 44/64 loss: -0.12584364414215088
Batch 45/64 loss: -0.11766380071640015
Batch 46/64 loss: -0.1414647102355957
Batch 47/64 loss: -0.15276175737380981
Batch 48/64 loss: -0.15511554479599
Batch 49/64 loss: -0.13645464181900024
Batch 50/64 loss: -0.13472098112106323
Batch 51/64 loss: -0.13522124290466309
Batch 52/64 loss: -0.12557852268218994
Batch 53/64 loss: -0.1465282440185547
Batch 54/64 loss: -0.12167155742645264
Batch 55/64 loss: -0.14752709865570068
Batch 56/64 loss: -0.11190295219421387
Batch 57/64 loss: -0.13754785060882568
Batch 58/64 loss: -0.1310843825340271
Batch 59/64 loss: -0.1283586621284485
Batch 60/64 loss: -0.13589805364608765
Batch 61/64 loss: -0.13770407438278198
Batch 62/64 loss: -0.12359529733657837
Batch 63/64 loss: -0.13896971940994263
Batch 64/64 loss: -0.13773995637893677
Epoch 416  Train loss: -0.13497017575245276  Val loss: 0.0546558136792527
Epoch 417
-------------------------------
Batch 1/64 loss: -0.11766529083251953
Batch 2/64 loss: -0.11131125688552856
Batch 3/64 loss: -0.16017234325408936
Batch 4/64 loss: -0.1373281478881836
Batch 5/64 loss: -0.15751564502716064
Batch 6/64 loss: -0.1416444182395935
Batch 7/64 loss: -0.135797917842865
Batch 8/64 loss: -0.14787888526916504
Batch 9/64 loss: -0.13631778955459595
Batch 10/64 loss: -0.1395743489265442
Batch 11/64 loss: -0.13078218698501587
Batch 12/64 loss: -0.13356536626815796
Batch 13/64 loss: -0.12600862979888916
Batch 14/64 loss: -0.1347755789756775
Batch 15/64 loss: -0.13198262453079224
Batch 16/64 loss: -0.14930683374404907
Batch 17/64 loss: -0.12598580121994019
Batch 18/64 loss: -0.15031570196151733
Batch 19/64 loss: -0.16048288345336914
Batch 20/64 loss: -0.1470491886138916
Batch 21/64 loss: -0.13943016529083252
Batch 22/64 loss: -0.12706518173217773
Batch 23/64 loss: -0.1323915719985962
Batch 24/64 loss: -0.13226628303527832
Batch 25/64 loss: -0.13853847980499268
Batch 26/64 loss: -0.151627779006958
Batch 27/64 loss: -0.13989967107772827
Batch 28/64 loss: -0.1414947509765625
Batch 29/64 loss: -0.1615470051765442
Batch 30/64 loss: -0.16043215990066528
Batch 31/64 loss: -0.10418152809143066
Batch 32/64 loss: -0.11411982774734497
Batch 33/64 loss: -0.0959165096282959
Batch 34/64 loss: -0.1428983211517334
Batch 35/64 loss: -0.13432621955871582
Batch 36/64 loss: -0.150013267993927
Batch 37/64 loss: -0.15222841501235962
Batch 38/64 loss: -0.16367971897125244
Batch 39/64 loss: -0.13771426677703857
Batch 40/64 loss: -0.1512119174003601
Batch 41/64 loss: -0.1401536464691162
Batch 42/64 loss: -0.12801456451416016
Batch 43/64 loss: -0.14556729793548584
Batch 44/64 loss: -0.13308781385421753
Batch 45/64 loss: -0.1259593963623047
Batch 46/64 loss: -0.14601284265518188
Batch 47/64 loss: -0.11871570348739624
Batch 48/64 loss: -0.13261187076568604
Batch 49/64 loss: -0.12069666385650635
Batch 50/64 loss: -0.12226766347885132
Batch 51/64 loss: -0.1447979211807251
Batch 52/64 loss: -0.13649505376815796
Batch 53/64 loss: -0.12512999773025513
Batch 54/64 loss: -0.14163249731063843
Batch 55/64 loss: -0.1356602907180786
Batch 56/64 loss: -0.10957324504852295
Batch 57/64 loss: -0.1485351324081421
Batch 58/64 loss: -0.13494598865509033
Batch 59/64 loss: -0.1488078236579895
Batch 60/64 loss: -0.15759581327438354
Batch 61/64 loss: -0.12843471765518188
Batch 62/64 loss: -0.11593466997146606
Batch 63/64 loss: -0.13517719507217407
Batch 64/64 loss: -0.0936894416809082
Epoch 417  Train loss: -0.1363532356187409  Val loss: 0.052078168416760634
Epoch 418
-------------------------------
Batch 1/64 loss: -0.13225245475769043
Batch 2/64 loss: -0.14868730306625366
Batch 3/64 loss: -0.1532435417175293
Batch 4/64 loss: -0.17208576202392578
Batch 5/64 loss: -0.13176608085632324
Batch 6/64 loss: -0.133009672164917
Batch 7/64 loss: -0.14229953289031982
Batch 8/64 loss: -0.14668786525726318
Batch 9/64 loss: -0.09526145458221436
Batch 10/64 loss: -0.14897996187210083
Batch 11/64 loss: -0.10726839303970337
Batch 12/64 loss: -0.11197429895401001
Batch 13/64 loss: -0.1433832049369812
Batch 14/64 loss: -0.16667985916137695
Batch 15/64 loss: -0.13683539628982544
Batch 16/64 loss: -0.13675570487976074
Batch 17/64 loss: -0.12819039821624756
Batch 18/64 loss: -0.14872485399246216
Batch 19/64 loss: -0.14589399099349976
Batch 20/64 loss: -0.11737072467803955
Batch 21/64 loss: -0.15898489952087402
Batch 22/64 loss: -0.14562183618545532
Batch 23/64 loss: -0.10324746370315552
Batch 24/64 loss: -0.14977306127548218
Batch 25/64 loss: -0.17247146368026733
Batch 26/64 loss: -0.1459307074546814
Batch 27/64 loss: -0.14609980583190918
Batch 28/64 loss: -0.14681297540664673
Batch 29/64 loss: -0.1284564733505249
Batch 30/64 loss: -0.16602826118469238
Batch 31/64 loss: -0.14561909437179565
Batch 32/64 loss: -0.12743890285491943
Batch 33/64 loss: -0.10807287693023682
Batch 34/64 loss: -0.14088302850723267
Batch 35/64 loss: -0.15629172325134277
Batch 36/64 loss: -0.12255430221557617
Batch 37/64 loss: -0.14805728197097778
Batch 38/64 loss: -0.15877974033355713
Batch 39/64 loss: -0.1193622350692749
Batch 40/64 loss: -0.1274356245994568
Batch 41/64 loss: -0.1482965350151062
Batch 42/64 loss: -0.13334912061691284
Batch 43/64 loss: -0.1435486078262329
Batch 44/64 loss: -0.16404438018798828
Batch 45/64 loss: -0.11718416213989258
Batch 46/64 loss: -0.15170913934707642
Batch 47/64 loss: -0.13426589965820312
Batch 48/64 loss: -0.11264443397521973
Batch 49/64 loss: -0.15724575519561768
Batch 50/64 loss: -0.10774767398834229
Batch 51/64 loss: -0.14384061098098755
Batch 52/64 loss: -0.16415220499038696
Batch 53/64 loss: -0.0967211127281189
Batch 54/64 loss: -0.1433754563331604
Batch 55/64 loss: -0.1294528841972351
Batch 56/64 loss: -0.11694753170013428
Batch 57/64 loss: -0.12788814306259155
Batch 58/64 loss: -0.08192145824432373
Batch 59/64 loss: -0.13336747884750366
Batch 60/64 loss: -0.13335764408111572
Batch 61/64 loss: -0.1260925531387329
Batch 62/64 loss: -0.15488559007644653
Batch 63/64 loss: -0.13688671588897705
Batch 64/64 loss: -0.10933929681777954
Epoch 418  Train loss: -0.13656787100960227  Val loss: 0.05241686863587894
Epoch 419
-------------------------------
Batch 1/64 loss: -0.15142607688903809
Batch 2/64 loss: -0.1459273099899292
Batch 3/64 loss: -0.12263762950897217
Batch 4/64 loss: -0.14482498168945312
Batch 5/64 loss: -0.12024796009063721
Batch 6/64 loss: -0.13862168788909912
Batch 7/64 loss: -0.1527022123336792
Batch 8/64 loss: -0.143537700176239
Batch 9/64 loss: -0.11528706550598145
Batch 10/64 loss: -0.15887528657913208
Batch 11/64 loss: -0.12402147054672241
Batch 12/64 loss: -0.15759634971618652
Batch 13/64 loss: -0.14093220233917236
Batch 14/64 loss: -0.14203226566314697
Batch 15/64 loss: -0.1578834056854248
Batch 16/64 loss: -0.14362120628356934
Batch 17/64 loss: -0.15125620365142822
Batch 18/64 loss: -0.12942880392074585
Batch 19/64 loss: -0.10976338386535645
Batch 20/64 loss: -0.11633908748626709
Batch 21/64 loss: -0.14704865217208862
Batch 22/64 loss: -0.16141802072525024
Batch 23/64 loss: -0.15452146530151367
Batch 24/64 loss: -0.12574505805969238
Batch 25/64 loss: -0.1286662220954895
Batch 26/64 loss: -0.11280173063278198
Batch 27/64 loss: -0.1068229079246521
Batch 28/64 loss: -0.14773958921432495
Batch 29/64 loss: -0.13037371635437012
Batch 30/64 loss: -0.1465613842010498
Batch 31/64 loss: -0.11836016178131104
Batch 32/64 loss: -0.13751745223999023
Batch 33/64 loss: -0.13536489009857178
Batch 34/64 loss: -0.1599903106689453
Batch 35/64 loss: -0.13167846202850342
Batch 36/64 loss: -0.13267898559570312
Batch 37/64 loss: -0.15459990501403809
Batch 38/64 loss: -0.1229138970375061
Batch 39/64 loss: -0.14084702730178833
Batch 40/64 loss: -0.10038614273071289
Batch 41/64 loss: -0.12327295541763306
Batch 42/64 loss: -0.1513078212738037
Batch 43/64 loss: -0.13407409191131592
Batch 44/64 loss: -0.14858442544937134
Batch 45/64 loss: -0.12265926599502563
Batch 46/64 loss: -0.16550323367118835
Batch 47/64 loss: -0.1407657265663147
Batch 48/64 loss: -0.13591670989990234
Batch 49/64 loss: -0.15038049221038818
Batch 50/64 loss: -0.15060681104660034
Batch 51/64 loss: -0.13867264986038208
Batch 52/64 loss: -0.13346225023269653
Batch 53/64 loss: -0.13763481378555298
Batch 54/64 loss: -0.1540454626083374
Batch 55/64 loss: -0.16133785247802734
Batch 56/64 loss: -0.1397373080253601
Batch 57/64 loss: -0.109222412109375
Batch 58/64 loss: -0.10461592674255371
Batch 59/64 loss: -0.12743908166885376
Batch 60/64 loss: -0.13062268495559692
Batch 61/64 loss: -0.12973535060882568
Batch 62/64 loss: -0.14786356687545776
Batch 63/64 loss: -0.12918508052825928
Batch 64/64 loss: -0.12988567352294922
Epoch 419  Train loss: -0.13689503529492547  Val loss: 0.05351244522533875
Epoch 420
-------------------------------
Batch 1/64 loss: -0.14820051193237305
Batch 2/64 loss: -0.14129137992858887
Batch 3/64 loss: -0.12499791383743286
Batch 4/64 loss: -0.15273374319076538
Batch 5/64 loss: -0.1330212950706482
Batch 6/64 loss: -0.14648884534835815
Batch 7/64 loss: -0.1635819375514984
Batch 8/64 loss: -0.13846147060394287
Batch 9/64 loss: -0.13900762796401978
Batch 10/64 loss: -0.15615689754486084
Batch 11/64 loss: -0.14501887559890747
Batch 12/64 loss: -0.14749610424041748
Batch 13/64 loss: -0.17421063780784607
Batch 14/64 loss: -0.12271171808242798
Batch 15/64 loss: -0.15941214561462402
Batch 16/64 loss: -0.13927364349365234
Batch 17/64 loss: -0.1537758708000183
Batch 18/64 loss: -0.13656675815582275
Batch 19/64 loss: -0.14585328102111816
Batch 20/64 loss: -0.13735687732696533
Batch 21/64 loss: -0.11122864484786987
Batch 22/64 loss: -0.13214588165283203
Batch 23/64 loss: -0.1393446922302246
Batch 24/64 loss: -0.12437641620635986
Batch 25/64 loss: -0.12018096446990967
Batch 26/64 loss: -0.15154743194580078
Batch 27/64 loss: -0.12177401781082153
Batch 28/64 loss: -0.16163337230682373
Batch 29/64 loss: -0.126508891582489
Batch 30/64 loss: -0.12320911884307861
Batch 31/64 loss: -0.12328892946243286
Batch 32/64 loss: -0.12673044204711914
Batch 33/64 loss: -0.15342730283737183
Batch 34/64 loss: -0.07005220651626587
Batch 35/64 loss: -0.1545688509941101
Batch 36/64 loss: -0.1418071985244751
Batch 37/64 loss: -0.17317312955856323
Batch 38/64 loss: -0.12230700254440308
Batch 39/64 loss: -0.13234621286392212
Batch 40/64 loss: -0.14401215314865112
Batch 41/64 loss: -0.129330575466156
Batch 42/64 loss: -0.15085184574127197
Batch 43/64 loss: -0.09725189208984375
Batch 44/64 loss: -0.15402251482009888
Batch 45/64 loss: -0.12278562784194946
Batch 46/64 loss: -0.1266067624092102
Batch 47/64 loss: -0.16212844848632812
Batch 48/64 loss: -0.14021086692810059
Batch 49/64 loss: -0.13508981466293335
Batch 50/64 loss: -0.12884807586669922
Batch 51/64 loss: -0.12949728965759277
Batch 52/64 loss: -0.12304997444152832
Batch 53/64 loss: -0.1433701515197754
Batch 54/64 loss: -0.1355329155921936
Batch 55/64 loss: -0.15985774993896484
Batch 56/64 loss: -0.0970221757888794
Batch 57/64 loss: -0.13795554637908936
Batch 58/64 loss: -0.1491861343383789
Batch 59/64 loss: -0.13351929187774658
Batch 60/64 loss: -0.14087611436843872
Batch 61/64 loss: -0.12429213523864746
Batch 62/64 loss: -0.15163511037826538
Batch 63/64 loss: -0.16820698976516724
Batch 64/64 loss: -0.10006982088088989
Epoch 420  Train loss: -0.13765428650612924  Val loss: 0.05261903018066563
Epoch 421
-------------------------------
Batch 1/64 loss: -0.16329139471054077
Batch 2/64 loss: -0.16901692748069763
Batch 3/64 loss: -0.1250605583190918
Batch 4/64 loss: -0.1472504734992981
Batch 5/64 loss: -0.15540945529937744
Batch 6/64 loss: -0.0949099063873291
Batch 7/64 loss: -0.14796185493469238
Batch 8/64 loss: -0.12383800745010376
Batch 9/64 loss: -0.14895546436309814
Batch 10/64 loss: -0.11866962909698486
Batch 11/64 loss: -0.13171237707138062
Batch 12/64 loss: -0.1470767855644226
Batch 13/64 loss: -0.0964195728302002
Batch 14/64 loss: -0.16290318965911865
Batch 15/64 loss: -0.1303616166114807
Batch 16/64 loss: -0.15954113006591797
Batch 17/64 loss: -0.09948223829269409
Batch 18/64 loss: -0.1208917498588562
Batch 19/64 loss: -0.12806326150894165
Batch 20/64 loss: -0.1129295825958252
Batch 21/64 loss: -0.13680708408355713
Batch 22/64 loss: -0.14469242095947266
Batch 23/64 loss: -0.13413774967193604
Batch 24/64 loss: -0.1278918981552124
Batch 25/64 loss: -0.1495128870010376
Batch 26/64 loss: -0.12727570533752441
Batch 27/64 loss: -0.11475056409835815
Batch 28/64 loss: -0.1287408471107483
Batch 29/64 loss: -0.1416609287261963
Batch 30/64 loss: -0.1221199631690979
Batch 31/64 loss: -0.14574581384658813
Batch 32/64 loss: -0.1328343152999878
Batch 33/64 loss: -0.1481652855873108
Batch 34/64 loss: -0.14119601249694824
Batch 35/64 loss: -0.15503281354904175
Batch 36/64 loss: -0.1260068416595459
Batch 37/64 loss: -0.13746559619903564
Batch 38/64 loss: -0.17015138268470764
Batch 39/64 loss: -0.16254934668540955
Batch 40/64 loss: -0.14511942863464355
Batch 41/64 loss: -0.15754938125610352
Batch 42/64 loss: -0.1602737307548523
Batch 43/64 loss: -0.10178673267364502
Batch 44/64 loss: -0.14067602157592773
Batch 45/64 loss: -0.10569632053375244
Batch 46/64 loss: -0.1488940715789795
Batch 47/64 loss: -0.1302165985107422
Batch 48/64 loss: -0.13050127029418945
Batch 49/64 loss: -0.1354910135269165
Batch 50/64 loss: -0.10996896028518677
Batch 51/64 loss: -0.11470264196395874
Batch 52/64 loss: -0.14172720909118652
Batch 53/64 loss: -0.14579397439956665
Batch 54/64 loss: -0.13470304012298584
Batch 55/64 loss: -0.14328724145889282
Batch 56/64 loss: -0.14260649681091309
Batch 57/64 loss: -0.14129668474197388
Batch 58/64 loss: -0.13207966089248657
Batch 59/64 loss: -0.11933600902557373
Batch 60/64 loss: -0.1295773983001709
Batch 61/64 loss: -0.12616294622421265
Batch 62/64 loss: -0.14867985248565674
Batch 63/64 loss: -0.137334406375885
Batch 64/64 loss: -0.10601156949996948
Epoch 421  Train loss: -0.13542670432259055  Val loss: 0.05211500838859794
Epoch 422
-------------------------------
Batch 1/64 loss: -0.16516971588134766
Batch 2/64 loss: -0.15102970600128174
Batch 3/64 loss: -0.1522359848022461
Batch 4/64 loss: -0.12340867519378662
Batch 5/64 loss: -0.1489708423614502
Batch 6/64 loss: -0.13997548818588257
Batch 7/64 loss: -0.15493547916412354
Batch 8/64 loss: -0.1522483229637146
Batch 9/64 loss: -0.14568579196929932
Batch 10/64 loss: -0.13309162855148315
Batch 11/64 loss: -0.13128167390823364
Batch 12/64 loss: -0.16326475143432617
Batch 13/64 loss: -0.15352165699005127
Batch 14/64 loss: -0.14930886030197144
Batch 15/64 loss: -0.16328084468841553
Batch 16/64 loss: -0.1342419981956482
Batch 17/64 loss: -0.15109598636627197
Batch 18/64 loss: -0.09773576259613037
Batch 19/64 loss: -0.126947820186615
Batch 20/64 loss: -0.15596264600753784
Batch 21/64 loss: -0.15520691871643066
Batch 22/64 loss: -0.1295710802078247
Batch 23/64 loss: -0.14348053932189941
Batch 24/64 loss: -0.1717940866947174
Batch 25/64 loss: -0.13011693954467773
Batch 26/64 loss: -0.1652878224849701
Batch 27/64 loss: -0.15255039930343628
Batch 28/64 loss: -0.14219868183135986
Batch 29/64 loss: -0.14203274250030518
Batch 30/64 loss: -0.12149322032928467
Batch 31/64 loss: -0.14808601140975952
Batch 32/64 loss: -0.12077593803405762
Batch 33/64 loss: -0.16251569986343384
Batch 34/64 loss: -0.13170599937438965
Batch 35/64 loss: -0.14438271522521973
Batch 36/64 loss: -0.15262234210968018
Batch 37/64 loss: -0.12246429920196533
Batch 38/64 loss: -0.13618016242980957
Batch 39/64 loss: -0.14978158473968506
Batch 40/64 loss: -0.15472853183746338
Batch 41/64 loss: -0.14429879188537598
Batch 42/64 loss: -0.14308017492294312
Batch 43/64 loss: -0.11428582668304443
Batch 44/64 loss: -0.1484794020652771
Batch 45/64 loss: -0.13272398710250854
Batch 46/64 loss: -0.14843136072158813
Batch 47/64 loss: -0.135700523853302
Batch 48/64 loss: -0.12163883447647095
Batch 49/64 loss: -0.1155596375465393
Batch 50/64 loss: -0.14648103713989258
Batch 51/64 loss: -0.14342373609542847
Batch 52/64 loss: -0.16948318481445312
Batch 53/64 loss: -0.1243547797203064
Batch 54/64 loss: -0.15311092138290405
Batch 55/64 loss: -0.1224290132522583
Batch 56/64 loss: -0.15660393238067627
Batch 57/64 loss: -0.12014365196228027
Batch 58/64 loss: -0.14017796516418457
Batch 59/64 loss: -0.16090255975723267
Batch 60/64 loss: -0.11147230863571167
Batch 61/64 loss: -0.13943731784820557
Batch 62/64 loss: -0.12617814540863037
Batch 63/64 loss: -0.13348925113677979
Batch 64/64 loss: -0.13165652751922607
Epoch 422  Train loss: -0.14150585240008784  Val loss: 0.05077065594007879
Epoch 423
-------------------------------
Batch 1/64 loss: -0.1332361102104187
Batch 2/64 loss: -0.09799438714981079
Batch 3/64 loss: -0.15005207061767578
Batch 4/64 loss: -0.14122998714447021
Batch 5/64 loss: -0.13786566257476807
Batch 6/64 loss: -0.15109729766845703
Batch 7/64 loss: -0.13265126943588257
Batch 8/64 loss: -0.1402105689048767
Batch 9/64 loss: -0.1317812204360962
Batch 10/64 loss: -0.1335100531578064
Batch 11/64 loss: -0.14620918035507202
Batch 12/64 loss: -0.10079342126846313
Batch 13/64 loss: -0.15345877408981323
Batch 14/64 loss: -0.12307989597320557
Batch 15/64 loss: -0.12453126907348633
Batch 16/64 loss: -0.13387537002563477
Batch 17/64 loss: -0.14448809623718262
Batch 18/64 loss: -0.12094146013259888
Batch 19/64 loss: -0.12298774719238281
Batch 20/64 loss: -0.12689614295959473
Batch 21/64 loss: -0.13954228162765503
Batch 22/64 loss: -0.11684465408325195
Batch 23/64 loss: -0.16381725668907166
Batch 24/64 loss: -0.1171875
Batch 25/64 loss: -0.15034133195877075
Batch 26/64 loss: -0.144586980342865
Batch 27/64 loss: -0.14957153797149658
Batch 28/64 loss: -0.15192240476608276
Batch 29/64 loss: -0.12772643566131592
Batch 30/64 loss: -0.14020591974258423
Batch 31/64 loss: -0.13388150930404663
Batch 32/64 loss: -0.1649119257926941
Batch 33/64 loss: -0.11709070205688477
Batch 34/64 loss: -0.13698750734329224
Batch 35/64 loss: -0.1265428066253662
Batch 36/64 loss: -0.12797927856445312
Batch 37/64 loss: -0.13259005546569824
Batch 38/64 loss: -0.1440613865852356
Batch 39/64 loss: -0.13526374101638794
Batch 40/64 loss: -0.1435566544532776
Batch 41/64 loss: -0.1397574543952942
Batch 42/64 loss: -0.1514032483100891
Batch 43/64 loss: -0.16560477018356323
Batch 44/64 loss: -0.09423846006393433
Batch 45/64 loss: -0.15023702383041382
Batch 46/64 loss: -0.11234360933303833
Batch 47/64 loss: -0.16151994466781616
Batch 48/64 loss: -0.1429479718208313
Batch 49/64 loss: -0.141432523727417
Batch 50/64 loss: -0.13975298404693604
Batch 51/64 loss: -0.1090846061706543
Batch 52/64 loss: -0.14003199338912964
Batch 53/64 loss: -0.11951148509979248
Batch 54/64 loss: -0.1579253077507019
Batch 55/64 loss: -0.14765632152557373
Batch 56/64 loss: -0.12478446960449219
Batch 57/64 loss: -0.14231526851654053
Batch 58/64 loss: -0.13571059703826904
Batch 59/64 loss: -0.14910483360290527
Batch 60/64 loss: -0.14564824104309082
Batch 61/64 loss: -0.15751850605010986
Batch 62/64 loss: -0.1301506757736206
Batch 63/64 loss: -0.11359179019927979
Batch 64/64 loss: -0.15290510654449463
Epoch 423  Train loss: -0.13644631797192144  Val loss: 0.052804902656791136
Epoch 424
-------------------------------
Batch 1/64 loss: -0.15868806838989258
Batch 2/64 loss: -0.14266377687454224
Batch 3/64 loss: -0.17036160826683044
Batch 4/64 loss: -0.15330809354782104
Batch 5/64 loss: -0.15042740106582642
Batch 6/64 loss: -0.12488329410552979
Batch 7/64 loss: -0.16892126202583313
Batch 8/64 loss: -0.1425870656967163
Batch 9/64 loss: -0.1371924877166748
Batch 10/64 loss: -0.11696732044219971
Batch 11/64 loss: -0.15441477298736572
Batch 12/64 loss: -0.14444100856781006
Batch 13/64 loss: -0.16844937205314636
Batch 14/64 loss: -0.1349840760231018
Batch 15/64 loss: -0.13873571157455444
Batch 16/64 loss: -0.14275527000427246
Batch 17/64 loss: -0.13455897569656372
Batch 18/64 loss: -0.13327980041503906
Batch 19/64 loss: -0.15054219961166382
Batch 20/64 loss: -0.17140328884124756
Batch 21/64 loss: -0.12274456024169922
Batch 22/64 loss: -0.10074371099472046
Batch 23/64 loss: -0.10432475805282593
Batch 24/64 loss: -0.15921702980995178
Batch 25/64 loss: -0.13328415155410767
Batch 26/64 loss: -0.14559286832809448
Batch 27/64 loss: -0.14183050394058228
Batch 28/64 loss: -0.13418346643447876
Batch 29/64 loss: -0.12248897552490234
Batch 30/64 loss: -0.13891065120697021
Batch 31/64 loss: -0.12126976251602173
Batch 32/64 loss: -0.15275049209594727
Batch 33/64 loss: -0.1665704846382141
Batch 34/64 loss: -0.11985570192337036
Batch 35/64 loss: -0.14077800512313843
Batch 36/64 loss: -0.14510905742645264
Batch 37/64 loss: -0.19390609860420227
Batch 38/64 loss: -0.1491432785987854
Batch 39/64 loss: -0.1469135284423828
Batch 40/64 loss: -0.12058866024017334
Batch 41/64 loss: -0.11034125089645386
Batch 42/64 loss: -0.1682766079902649
Batch 43/64 loss: -0.15948563814163208
Batch 44/64 loss: -0.1135321855545044
Batch 45/64 loss: -0.16507333517074585
Batch 46/64 loss: -0.16070175170898438
Batch 47/64 loss: -0.10348129272460938
Batch 48/64 loss: -0.13534826040267944
Batch 49/64 loss: -0.13139301538467407
Batch 50/64 loss: -0.12003558874130249
Batch 51/64 loss: -0.1487969160079956
Batch 52/64 loss: -0.13901829719543457
Batch 53/64 loss: -0.11716443300247192
Batch 54/64 loss: -0.13131338357925415
Batch 55/64 loss: -0.11174309253692627
Batch 56/64 loss: -0.13337242603302002
Batch 57/64 loss: -0.12107998132705688
Batch 58/64 loss: -0.156807541847229
Batch 59/64 loss: -0.13460832834243774
Batch 60/64 loss: -0.12799781560897827
Batch 61/64 loss: -0.1491987705230713
Batch 62/64 loss: -0.12317568063735962
Batch 63/64 loss: -0.15317082405090332
Batch 64/64 loss: -0.11703681945800781
Epoch 424  Train loss: -0.13964964128008076  Val loss: 0.05166414246936025
Epoch 425
-------------------------------
Batch 1/64 loss: -0.14443719387054443
Batch 2/64 loss: -0.17263448238372803
Batch 3/64 loss: -0.14128053188323975
Batch 4/64 loss: -0.14123088121414185
Batch 5/64 loss: -0.12872779369354248
Batch 6/64 loss: -0.1458801031112671
Batch 7/64 loss: -0.13854652643203735
Batch 8/64 loss: -0.1611645221710205
Batch 9/64 loss: -0.1454210877418518
Batch 10/64 loss: -0.14482367038726807
Batch 11/64 loss: -0.16082167625427246
Batch 12/64 loss: -0.13781148195266724
Batch 13/64 loss: -0.13625949621200562
Batch 14/64 loss: -0.16159483790397644
Batch 15/64 loss: -0.13749325275421143
Batch 16/64 loss: -0.15957596898078918
Batch 17/64 loss: -0.1572176218032837
Batch 18/64 loss: -0.12362611293792725
Batch 19/64 loss: -0.16115665435791016
Batch 20/64 loss: -0.15930843353271484
Batch 21/64 loss: -0.14712649583816528
Batch 22/64 loss: -0.11687421798706055
Batch 23/64 loss: -0.12359637022018433
Batch 24/64 loss: -0.12068605422973633
Batch 25/64 loss: -0.1664709746837616
Batch 26/64 loss: -0.12853974103927612
Batch 27/64 loss: -0.1540042757987976
Batch 28/64 loss: -0.15573382377624512
Batch 29/64 loss: -0.12282490730285645
Batch 30/64 loss: -0.13682258129119873
Batch 31/64 loss: -0.12168639898300171
Batch 32/64 loss: -0.11941999197006226
Batch 33/64 loss: -0.14492762088775635
Batch 34/64 loss: -0.13344955444335938
Batch 35/64 loss: -0.11498355865478516
Batch 36/64 loss: -0.13678842782974243
Batch 37/64 loss: -0.1381075382232666
Batch 38/64 loss: -0.1502264142036438
Batch 39/64 loss: -0.12811040878295898
Batch 40/64 loss: -0.13496559858322144
Batch 41/64 loss: -0.1402248740196228
Batch 42/64 loss: -0.17928546667099
Batch 43/64 loss: -0.13031327724456787
Batch 44/64 loss: -0.1388433575630188
Batch 45/64 loss: -0.13997900485992432
Batch 46/64 loss: -0.11841320991516113
Batch 47/64 loss: -0.13783401250839233
Batch 48/64 loss: -0.11933290958404541
Batch 49/64 loss: -0.1403992772102356
Batch 50/64 loss: -0.12819421291351318
Batch 51/64 loss: -0.1337890625
Batch 52/64 loss: -0.15269237756729126
Batch 53/64 loss: -0.08459514379501343
Batch 54/64 loss: -0.1295396089553833
Batch 55/64 loss: -0.11841833591461182
Batch 56/64 loss: -0.1260508894920349
Batch 57/64 loss: -0.13809716701507568
Batch 58/64 loss: -0.12066382169723511
Batch 59/64 loss: -0.10756462812423706
Batch 60/64 loss: -0.10638755559921265
Batch 61/64 loss: -0.14871561527252197
Batch 62/64 loss: -0.1366417407989502
Batch 63/64 loss: -0.11335587501525879
Batch 64/64 loss: -0.13392746448516846
Epoch 425  Train loss: -0.13716289108874752  Val loss: 0.055880811820734816
Epoch 426
-------------------------------
Batch 1/64 loss: -0.15018188953399658
Batch 2/64 loss: -0.13197338581085205
Batch 3/64 loss: -0.12814778089523315
Batch 4/64 loss: -0.13529139757156372
Batch 5/64 loss: -0.14083456993103027
Batch 6/64 loss: -0.1363566517829895
Batch 7/64 loss: -0.12693649530410767
Batch 8/64 loss: -0.12743616104125977
Batch 9/64 loss: -0.131397545337677
Batch 10/64 loss: -0.14670264720916748
Batch 11/64 loss: -0.14115655422210693
Batch 12/64 loss: -0.1035759449005127
Batch 13/64 loss: -0.1572057604789734
Batch 14/64 loss: -0.12861156463623047
Batch 15/64 loss: -0.11661070585250854
Batch 16/64 loss: -0.1488436460494995
Batch 17/64 loss: -0.12472909688949585
Batch 18/64 loss: -0.1407686471939087
Batch 19/64 loss: -0.13906043767929077
Batch 20/64 loss: -0.12913936376571655
Batch 21/64 loss: -0.15334093570709229
Batch 22/64 loss: -0.14711421728134155
Batch 23/64 loss: -0.14739561080932617
Batch 24/64 loss: -0.12641698122024536
Batch 25/64 loss: -0.13603615760803223
Batch 26/64 loss: -0.1374291181564331
Batch 27/64 loss: -0.1347590684890747
Batch 28/64 loss: -0.14755821228027344
Batch 29/64 loss: -0.16071176528930664
Batch 30/64 loss: -0.11158806085586548
Batch 31/64 loss: -0.1315441131591797
Batch 32/64 loss: -0.12359821796417236
Batch 33/64 loss: -0.15341901779174805
Batch 34/64 loss: -0.13967084884643555
Batch 35/64 loss: -0.09331619739532471
Batch 36/64 loss: -0.1545931100845337
Batch 37/64 loss: -0.09171587228775024
Batch 38/64 loss: -0.13517498970031738
Batch 39/64 loss: -0.13909262418746948
Batch 40/64 loss: -0.12369072437286377
Batch 41/64 loss: -0.1443115472793579
Batch 42/64 loss: -0.13284051418304443
Batch 43/64 loss: -0.12084263563156128
Batch 44/64 loss: -0.15421360731124878
Batch 45/64 loss: -0.13655346632003784
Batch 46/64 loss: -0.11719810962677002
Batch 47/64 loss: -0.13976091146469116
Batch 48/64 loss: -0.15493780374526978
Batch 49/64 loss: -0.1452542543411255
Batch 50/64 loss: -0.13892877101898193
Batch 51/64 loss: -0.15439295768737793
Batch 52/64 loss: -0.11623996496200562
Batch 53/64 loss: -0.13470697402954102
Batch 54/64 loss: -0.14562326669692993
Batch 55/64 loss: -0.1523367166519165
Batch 56/64 loss: -0.15361803770065308
Batch 57/64 loss: -0.15009021759033203
Batch 58/64 loss: -0.1450842022895813
Batch 59/64 loss: -0.11004638671875
Batch 60/64 loss: -0.14591431617736816
Batch 61/64 loss: -0.0958244800567627
Batch 62/64 loss: -0.159211665391922
Batch 63/64 loss: -0.14545601606369019
Batch 64/64 loss: -0.14642256498336792
Epoch 426  Train loss: -0.13609929154900943  Val loss: 0.05214049295871118
Epoch 427
-------------------------------
Batch 1/64 loss: -0.161271333694458
Batch 2/64 loss: -0.1438482403755188
Batch 3/64 loss: -0.13009095191955566
Batch 4/64 loss: -0.15339505672454834
Batch 5/64 loss: -0.14624345302581787
Batch 6/64 loss: -0.14343172311782837
Batch 7/64 loss: -0.15059059858322144
Batch 8/64 loss: -0.12890398502349854
Batch 9/64 loss: -0.15386688709259033
Batch 10/64 loss: -0.15046906471252441
Batch 11/64 loss: -0.11865365505218506
Batch 12/64 loss: -0.13805019855499268
Batch 13/64 loss: -0.15405625104904175
Batch 14/64 loss: -0.12906694412231445
Batch 15/64 loss: -0.13006311655044556
Batch 16/64 loss: -0.15543276071548462
Batch 17/64 loss: -0.1692940890789032
Batch 18/64 loss: -0.10968577861785889
Batch 19/64 loss: -0.159346342086792
Batch 20/64 loss: -0.11088693141937256
Batch 21/64 loss: -0.12480956315994263
Batch 22/64 loss: -0.1534692645072937
Batch 23/64 loss: -0.1323777437210083
Batch 24/64 loss: -0.1356189250946045
Batch 25/64 loss: -0.14946770668029785
Batch 26/64 loss: -0.11522150039672852
Batch 27/64 loss: -0.13896018266677856
Batch 28/64 loss: -0.11763542890548706
Batch 29/64 loss: -0.13584506511688232
Batch 30/64 loss: -0.13107281923294067
Batch 31/64 loss: -0.15031057596206665
Batch 32/64 loss: -0.10926926136016846
Batch 33/64 loss: -0.15659165382385254
Batch 34/64 loss: -0.13306260108947754
Batch 35/64 loss: -0.16121113300323486
Batch 36/64 loss: -0.11694502830505371
Batch 37/64 loss: -0.14714598655700684
Batch 38/64 loss: -0.13581252098083496
Batch 39/64 loss: -0.09860295057296753
Batch 40/64 loss: -0.12649834156036377
Batch 41/64 loss: -0.1335325837135315
Batch 42/64 loss: -0.1402451992034912
Batch 43/64 loss: -0.1489892601966858
Batch 44/64 loss: -0.15647786855697632
Batch 45/64 loss: -0.15850096940994263
Batch 46/64 loss: -0.13194501399993896
Batch 47/64 loss: -0.1665886640548706
Batch 48/64 loss: -0.15541410446166992
Batch 49/64 loss: -0.13217604160308838
Batch 50/64 loss: -0.14404374361038208
Batch 51/64 loss: -0.13700515031814575
Batch 52/64 loss: -0.10753017663955688
Batch 53/64 loss: -0.14411914348602295
Batch 54/64 loss: -0.14333969354629517
Batch 55/64 loss: -0.15887528657913208
Batch 56/64 loss: -0.11701774597167969
Batch 57/64 loss: -0.1464862823486328
Batch 58/64 loss: -0.1433582901954651
Batch 59/64 loss: -0.12958848476409912
Batch 60/64 loss: -0.14485681056976318
Batch 61/64 loss: -0.14493399858474731
Batch 62/64 loss: -0.11480450630187988
Batch 63/64 loss: -0.15505552291870117
Batch 64/64 loss: -0.168319970369339
Epoch 427  Train loss: -0.13941490404746112  Val loss: 0.05243372015936678
Epoch 428
-------------------------------
Batch 1/64 loss: -0.1452099084854126
Batch 2/64 loss: -0.1510518193244934
Batch 3/64 loss: -0.15225565433502197
Batch 4/64 loss: -0.13316023349761963
Batch 5/64 loss: -0.15333473682403564
Batch 6/64 loss: -0.16128188371658325
Batch 7/64 loss: -0.1453474760055542
Batch 8/64 loss: -0.17090529203414917
Batch 9/64 loss: -0.14224910736083984
Batch 10/64 loss: -0.1609007716178894
Batch 11/64 loss: -0.16023021936416626
Batch 12/64 loss: -0.13316947221755981
Batch 13/64 loss: -0.14280825853347778
Batch 14/64 loss: -0.12143003940582275
Batch 15/64 loss: -0.14842724800109863
Batch 16/64 loss: -0.15912210941314697
Batch 17/64 loss: -0.12378036975860596
Batch 18/64 loss: -0.1422863006591797
Batch 19/64 loss: -0.10696280002593994
Batch 20/64 loss: -0.15518224239349365
Batch 21/64 loss: -0.18135428428649902
Batch 22/64 loss: -0.15488684177398682
Batch 23/64 loss: -0.15069127082824707
Batch 24/64 loss: -0.1532788872718811
Batch 25/64 loss: -0.15544939041137695
Batch 26/64 loss: -0.1472112536430359
Batch 27/64 loss: -0.14093458652496338
Batch 28/64 loss: -0.15034377574920654
Batch 29/64 loss: -0.19140425324440002
Batch 30/64 loss: -0.14496004581451416
Batch 31/64 loss: -0.14099997282028198
Batch 32/64 loss: -0.14944756031036377
Batch 33/64 loss: -0.14044231176376343
Batch 34/64 loss: -0.14156830310821533
Batch 35/64 loss: -0.12959033250808716
Batch 36/64 loss: -0.12054437398910522
Batch 37/64 loss: -0.12921643257141113
Batch 38/64 loss: -0.11460256576538086
Batch 39/64 loss: -0.16280978918075562
Batch 40/64 loss: -0.13191437721252441
Batch 41/64 loss: -0.13399755954742432
Batch 42/64 loss: -0.12148982286453247
Batch 43/64 loss: -0.11729580163955688
Batch 44/64 loss: -0.09688502550125122
Batch 45/64 loss: -0.10252916812896729
Batch 46/64 loss: -0.14802175760269165
Batch 47/64 loss: -0.10320830345153809
Batch 48/64 loss: -0.13896256685256958
Batch 49/64 loss: -0.1444808840751648
Batch 50/64 loss: -0.12814325094223022
Batch 51/64 loss: -0.13761502504348755
Batch 52/64 loss: -0.12472975254058838
Batch 53/64 loss: -0.16991791129112244
Batch 54/64 loss: -0.1313183307647705
Batch 55/64 loss: -0.14616644382476807
Batch 56/64 loss: -0.1387065052986145
Batch 57/64 loss: -0.1454479694366455
Batch 58/64 loss: -0.13404172658920288
Batch 59/64 loss: -0.1288301944732666
Batch 60/64 loss: -0.15380358695983887
Batch 61/64 loss: -0.14167314767837524
Batch 62/64 loss: -0.1571497917175293
Batch 63/64 loss: -0.16255375742912292
Batch 64/64 loss: -0.13920718431472778
Epoch 428  Train loss: -0.1419940427237866  Val loss: 0.05282903864621297
Epoch 429
-------------------------------
Batch 1/64 loss: -0.1544089913368225
Batch 2/64 loss: -0.16185757517814636
Batch 3/64 loss: -0.13404393196105957
Batch 4/64 loss: -0.14275825023651123
Batch 5/64 loss: -0.15383505821228027
Batch 6/64 loss: -0.16329002380371094
Batch 7/64 loss: -0.16670864820480347
Batch 8/64 loss: -0.152296781539917
Batch 9/64 loss: -0.15786421298980713
Batch 10/64 loss: -0.1572067141532898
Batch 11/64 loss: -0.1368166208267212
Batch 12/64 loss: -0.14382803440093994
Batch 13/64 loss: -0.14742320775985718
Batch 14/64 loss: -0.1402895450592041
Batch 15/64 loss: -0.13322770595550537
Batch 16/64 loss: -0.11441701650619507
Batch 17/64 loss: -0.15117228031158447
Batch 18/64 loss: -0.14444786310195923
Batch 19/64 loss: -0.15009790658950806
Batch 20/64 loss: -0.12621188163757324
Batch 21/64 loss: -0.16162514686584473
Batch 22/64 loss: -0.1436588168144226
Batch 23/64 loss: -0.14013469219207764
Batch 24/64 loss: -0.10907185077667236
Batch 25/64 loss: -0.16473430395126343
Batch 26/64 loss: -0.14693576097488403
Batch 27/64 loss: -0.15058231353759766
Batch 28/64 loss: -0.15259569883346558
Batch 29/64 loss: -0.13001090288162231
Batch 30/64 loss: -0.14670222997665405
Batch 31/64 loss: -0.14407378435134888
Batch 32/64 loss: -0.13238829374313354
Batch 33/64 loss: -0.10751056671142578
Batch 34/64 loss: -0.1267375349998474
Batch 35/64 loss: -0.15183806419372559
Batch 36/64 loss: -0.14803528785705566
Batch 37/64 loss: -0.16164618730545044
Batch 38/64 loss: -0.10160434246063232
Batch 39/64 loss: -0.14256250858306885
Batch 40/64 loss: -0.13435477018356323
Batch 41/64 loss: -0.1391381025314331
Batch 42/64 loss: -0.11156392097473145
Batch 43/64 loss: -0.14067339897155762
Batch 44/64 loss: -0.15421128273010254
Batch 45/64 loss: -0.13019871711730957
Batch 46/64 loss: -0.16036847233772278
Batch 47/64 loss: -0.127280592918396
Batch 48/64 loss: -0.12388157844543457
Batch 49/64 loss: -0.1449723243713379
Batch 50/64 loss: -0.13952642679214478
Batch 51/64 loss: -0.13186150789260864
Batch 52/64 loss: -0.11873286962509155
Batch 53/64 loss: -0.13849276304244995
Batch 54/64 loss: -0.1373419165611267
Batch 55/64 loss: -0.12538617849349976
Batch 56/64 loss: -0.13868999481201172
Batch 57/64 loss: -0.1516534686088562
Batch 58/64 loss: -0.1533900499343872
Batch 59/64 loss: -0.13709086179733276
Batch 60/64 loss: -0.1410353183746338
Batch 61/64 loss: -0.1572779417037964
Batch 62/64 loss: -0.1420300006866455
Batch 63/64 loss: -0.13089489936828613
Batch 64/64 loss: -0.16509220004081726
Epoch 429  Train loss: -0.14162379678557901  Val loss: 0.05191338676767251
Epoch 430
-------------------------------
Batch 1/64 loss: -0.14479410648345947
Batch 2/64 loss: -0.17053309082984924
Batch 3/64 loss: -0.13060706853866577
Batch 4/64 loss: -0.15369820594787598
Batch 5/64 loss: -0.14663302898406982
Batch 6/64 loss: -0.11751866340637207
Batch 7/64 loss: -0.1463947892189026
Batch 8/64 loss: -0.11617112159729004
Batch 9/64 loss: -0.15277224779129028
Batch 10/64 loss: -0.16458511352539062
Batch 11/64 loss: -0.155206561088562
Batch 12/64 loss: -0.15667742490768433
Batch 13/64 loss: -0.147327721118927
Batch 14/64 loss: -0.1165345311164856
Batch 15/64 loss: -0.16870900988578796
Batch 16/64 loss: -0.15606367588043213
Batch 17/64 loss: -0.11419790983200073
Batch 18/64 loss: -0.13741719722747803
Batch 19/64 loss: -0.1397419571876526
Batch 20/64 loss: -0.16322427988052368
Batch 21/64 loss: -0.1476290225982666
Batch 22/64 loss: -0.13974547386169434
Batch 23/64 loss: -0.1573878526687622
Batch 24/64 loss: -0.12314122915267944
Batch 25/64 loss: -0.14627337455749512
Batch 26/64 loss: -0.12351495027542114
Batch 27/64 loss: -0.15622222423553467
Batch 28/64 loss: -0.16200560331344604
Batch 29/64 loss: -0.1369938850402832
Batch 30/64 loss: -0.12891918420791626
Batch 31/64 loss: -0.14679229259490967
Batch 32/64 loss: -0.12661099433898926
Batch 33/64 loss: -0.13483941555023193
Batch 34/64 loss: -0.10763776302337646
Batch 35/64 loss: -0.15417057275772095
Batch 36/64 loss: -0.15649640560150146
Batch 37/64 loss: -0.15447115898132324
Batch 38/64 loss: -0.13235092163085938
Batch 39/64 loss: -0.1643959879875183
Batch 40/64 loss: -0.14545273780822754
Batch 41/64 loss: -0.1382312774658203
Batch 42/64 loss: -0.14905357360839844
Batch 43/64 loss: -0.1588793396949768
Batch 44/64 loss: -0.1261998414993286
Batch 45/64 loss: -0.1566663384437561
Batch 46/64 loss: -0.1307775378227234
Batch 47/64 loss: -0.12267720699310303
Batch 48/64 loss: -0.12542665004730225
Batch 49/64 loss: -0.1385844349861145
Batch 50/64 loss: -0.13729751110076904
Batch 51/64 loss: -0.1167905330657959
Batch 52/64 loss: -0.13843542337417603
Batch 53/64 loss: -0.13369029760360718
Batch 54/64 loss: -0.17304867506027222
Batch 55/64 loss: -0.14237141609191895
Batch 56/64 loss: -0.11302495002746582
Batch 57/64 loss: -0.1195443868637085
Batch 58/64 loss: -0.1763594150543213
Batch 59/64 loss: -0.11364567279815674
Batch 60/64 loss: -0.1411636471748352
Batch 61/64 loss: -0.13389164209365845
Batch 62/64 loss: -0.11472731828689575
Batch 63/64 loss: -0.13678765296936035
Batch 64/64 loss: -0.14191514253616333
Epoch 430  Train loss: -0.14098148790060305  Val loss: 0.05491377684668577
Epoch 431
-------------------------------
Batch 1/64 loss: -0.16209253668785095
Batch 2/64 loss: -0.16046571731567383
Batch 3/64 loss: -0.16379261016845703
Batch 4/64 loss: -0.15377050638198853
Batch 5/64 loss: -0.14435476064682007
Batch 6/64 loss: -0.1508697271347046
Batch 7/64 loss: -0.1372813582420349
Batch 8/64 loss: -0.1471015214920044
Batch 9/64 loss: -0.16930878162384033
Batch 10/64 loss: -0.17028886079788208
Batch 11/64 loss: -0.13021332025527954
Batch 12/64 loss: -0.13566327095031738
Batch 13/64 loss: -0.15379714965820312
Batch 14/64 loss: -0.14801383018493652
Batch 15/64 loss: -0.14196628332138062
Batch 16/64 loss: -0.16069716215133667
Batch 17/64 loss: -0.14656007289886475
Batch 18/64 loss: -0.11699163913726807
Batch 19/64 loss: -0.16250714659690857
Batch 20/64 loss: -0.12422490119934082
Batch 21/64 loss: -0.15653204917907715
Batch 22/64 loss: -0.142691969871521
Batch 23/64 loss: -0.14875710010528564
Batch 24/64 loss: -0.1352686882019043
Batch 25/64 loss: -0.1728883981704712
Batch 26/64 loss: -0.14955878257751465
Batch 27/64 loss: -0.1732434332370758
Batch 28/64 loss: -0.16305473446846008
Batch 29/64 loss: -0.1637948751449585
Batch 30/64 loss: -0.15896058082580566
Batch 31/64 loss: -0.12737882137298584
Batch 32/64 loss: -0.13671648502349854
Batch 33/64 loss: -0.14014822244644165
Batch 34/64 loss: -0.1388271450996399
Batch 35/64 loss: -0.12901926040649414
Batch 36/64 loss: -0.12459015846252441
Batch 37/64 loss: -0.16029471158981323
Batch 38/64 loss: -0.17065173387527466
Batch 39/64 loss: -0.12364506721496582
Batch 40/64 loss: -0.1442670226097107
Batch 41/64 loss: -0.13237249851226807
Batch 42/64 loss: -0.16619586944580078
Batch 43/64 loss: -0.14362114667892456
Batch 44/64 loss: -0.1365976333618164
Batch 45/64 loss: -0.1334543228149414
Batch 46/64 loss: -0.1449446678161621
Batch 47/64 loss: -0.13147962093353271
Batch 48/64 loss: -0.131206214427948
Batch 49/64 loss: -0.12002557516098022
Batch 50/64 loss: -0.1094050407409668
Batch 51/64 loss: -0.09874892234802246
Batch 52/64 loss: -0.11360466480255127
Batch 53/64 loss: -0.10313618183135986
Batch 54/64 loss: -0.15063905715942383
Batch 55/64 loss: -0.1599476933479309
Batch 56/64 loss: -0.13437044620513916
Batch 57/64 loss: -0.1446930170059204
Batch 58/64 loss: -0.12302696704864502
Batch 59/64 loss: -0.1208985447883606
Batch 60/64 loss: -0.1468278169631958
Batch 61/64 loss: -0.12989991903305054
Batch 62/64 loss: -0.09871256351470947
Batch 63/64 loss: -0.14989465475082397
Batch 64/64 loss: -0.15234261751174927
Epoch 431  Train loss: -0.1424033003694871  Val loss: 0.05308535807730816
Epoch 432
-------------------------------
Batch 1/64 loss: -0.1624680459499359
Batch 2/64 loss: -0.10982018709182739
Batch 3/64 loss: -0.16348251700401306
Batch 4/64 loss: -0.15879392623901367
Batch 5/64 loss: -0.17251884937286377
Batch 6/64 loss: -0.13913649320602417
Batch 7/64 loss: -0.15328055620193481
Batch 8/64 loss: -0.14572465419769287
Batch 9/64 loss: -0.1697414219379425
Batch 10/64 loss: -0.1554211974143982
Batch 11/64 loss: -0.14534294605255127
Batch 12/64 loss: -0.16623812913894653
Batch 13/64 loss: -0.09648388624191284
Batch 14/64 loss: -0.15283775329589844
Batch 15/64 loss: -0.15639454126358032
Batch 16/64 loss: -0.13081997632980347
Batch 17/64 loss: -0.1197248101234436
Batch 18/64 loss: -0.1393243670463562
Batch 19/64 loss: -0.16080433130264282
Batch 20/64 loss: -0.13908416032791138
Batch 21/64 loss: -0.11615109443664551
Batch 22/64 loss: -0.1256987452507019
Batch 23/64 loss: -0.14226281642913818
Batch 24/64 loss: -0.13424211740493774
Batch 25/64 loss: -0.13280707597732544
Batch 26/64 loss: -0.13749098777770996
Batch 27/64 loss: -0.1537538766860962
Batch 28/64 loss: -0.13970953226089478
Batch 29/64 loss: -0.16155004501342773
Batch 30/64 loss: -0.14621269702911377
Batch 31/64 loss: -0.13361817598342896
Batch 32/64 loss: -0.13602668046951294
Batch 33/64 loss: -0.1437811255455017
Batch 34/64 loss: -0.14661449193954468
Batch 35/64 loss: -0.11715942621231079
Batch 36/64 loss: -0.14503377676010132
Batch 37/64 loss: -0.14851629734039307
Batch 38/64 loss: -0.11051899194717407
Batch 39/64 loss: -0.1418430209159851
Batch 40/64 loss: -0.12826859951019287
Batch 41/64 loss: -0.15489041805267334
Batch 42/64 loss: -0.13047993183135986
Batch 43/64 loss: -0.1252061128616333
Batch 44/64 loss: -0.1023019552230835
Batch 45/64 loss: -0.12588882446289062
Batch 46/64 loss: -0.1366194486618042
Batch 47/64 loss: -0.1374298334121704
Batch 48/64 loss: -0.14654648303985596
Batch 49/64 loss: -0.1270083785057068
Batch 50/64 loss: -0.12757110595703125
Batch 51/64 loss: -0.0948219895362854
Batch 52/64 loss: -0.1542344093322754
Batch 53/64 loss: -0.15705496072769165
Batch 54/64 loss: -0.15226060152053833
Batch 55/64 loss: -0.15286791324615479
Batch 56/64 loss: -0.16261062026023865
Batch 57/64 loss: -0.11336112022399902
Batch 58/64 loss: -0.12247639894485474
Batch 59/64 loss: -0.1262834668159485
Batch 60/64 loss: -0.16177216172218323
Batch 61/64 loss: -0.12072932720184326
Batch 62/64 loss: -0.13768279552459717
Batch 63/64 loss: -0.1290743350982666
Batch 64/64 loss: -0.09990638494491577
Epoch 432  Train loss: -0.13886752479216632  Val loss: 0.05585347418113263
Epoch 433
-------------------------------
Batch 1/64 loss: -0.12656450271606445
Batch 2/64 loss: -0.1801692247390747
Batch 3/64 loss: -0.1284564733505249
Batch 4/64 loss: -0.171024888753891
Batch 5/64 loss: -0.14209866523742676
Batch 6/64 loss: -0.13952547311782837
Batch 7/64 loss: -0.11416584253311157
Batch 8/64 loss: -0.14883995056152344
Batch 9/64 loss: -0.14196234941482544
Batch 10/64 loss: -0.1075901985168457
Batch 11/64 loss: -0.1366053819656372
Batch 12/64 loss: -0.14855313301086426
Batch 13/64 loss: -0.11243891716003418
Batch 14/64 loss: -0.1504489779472351
Batch 15/64 loss: -0.13630008697509766
Batch 16/64 loss: -0.14695841073989868
Batch 17/64 loss: -0.13980793952941895
Batch 18/64 loss: -0.13109898567199707
Batch 19/64 loss: -0.17906036972999573
Batch 20/64 loss: -0.1417701244354248
Batch 21/64 loss: -0.15175873041152954
Batch 22/64 loss: -0.13873696327209473
Batch 23/64 loss: -0.12014526128768921
Batch 24/64 loss: -0.14940738677978516
Batch 25/64 loss: -0.14198064804077148
Batch 26/64 loss: -0.14793312549591064
Batch 27/64 loss: -0.11159229278564453
Batch 28/64 loss: -0.13006621599197388
Batch 29/64 loss: -0.13052940368652344
Batch 30/64 loss: -0.1281963586807251
Batch 31/64 loss: -0.1307660937309265
Batch 32/64 loss: -0.1410852074623108
Batch 33/64 loss: -0.13371896743774414
Batch 34/64 loss: -0.14643347263336182
Batch 35/64 loss: -0.1587824821472168
Batch 36/64 loss: -0.12538594007492065
Batch 37/64 loss: -0.11997711658477783
Batch 38/64 loss: -0.13789165019989014
Batch 39/64 loss: -0.11625450849533081
Batch 40/64 loss: -0.12162089347839355
Batch 41/64 loss: -0.1332254409790039
Batch 42/64 loss: -0.1433037519454956
Batch 43/64 loss: -0.14500093460083008
Batch 44/64 loss: -0.15524792671203613
Batch 45/64 loss: -0.13380253314971924
Batch 46/64 loss: -0.15912997722625732
Batch 47/64 loss: -0.13885784149169922
Batch 48/64 loss: -0.16043448448181152
Batch 49/64 loss: -0.1368926763534546
Batch 50/64 loss: -0.1595740020275116
Batch 51/64 loss: -0.15849271416664124
Batch 52/64 loss: -0.12672871351242065
Batch 53/64 loss: -0.12619149684906006
Batch 54/64 loss: -0.14231038093566895
Batch 55/64 loss: -0.14629530906677246
Batch 56/64 loss: -0.10213351249694824
Batch 57/64 loss: -0.1572962999343872
Batch 58/64 loss: -0.15580791234970093
Batch 59/64 loss: -0.13809263706207275
Batch 60/64 loss: -0.12624049186706543
Batch 61/64 loss: -0.15537601709365845
Batch 62/64 loss: -0.12954092025756836
Batch 63/64 loss: -0.09670644998550415
Batch 64/64 loss: -0.15268272161483765
Epoch 433  Train loss: -0.13877485616534363  Val loss: 0.05219467603873551
Epoch 434
-------------------------------
Batch 1/64 loss: -0.14886170625686646
Batch 2/64 loss: -0.14102381467819214
Batch 3/64 loss: -0.1342005729675293
Batch 4/64 loss: -0.11741399765014648
Batch 5/64 loss: -0.16768473386764526
Batch 6/64 loss: -0.13230061531066895
Batch 7/64 loss: -0.12144672870635986
Batch 8/64 loss: -0.16251859068870544
Batch 9/64 loss: -0.14444702863693237
Batch 10/64 loss: -0.1435115933418274
Batch 11/64 loss: -0.13864928483963013
Batch 12/64 loss: -0.11250919103622437
Batch 13/64 loss: -0.14707833528518677
Batch 14/64 loss: -0.1279696226119995
Batch 15/64 loss: -0.14679408073425293
Batch 16/64 loss: -0.14500486850738525
Batch 17/64 loss: -0.1428924798965454
Batch 18/64 loss: -0.14473354816436768
Batch 19/64 loss: -0.14198529720306396
Batch 20/64 loss: -0.1543407440185547
Batch 21/64 loss: -0.12990844249725342
Batch 22/64 loss: -0.12044382095336914
Batch 23/64 loss: -0.13499462604522705
Batch 24/64 loss: -0.14633440971374512
Batch 25/64 loss: -0.13381356000900269
Batch 26/64 loss: -0.11951112747192383
Batch 27/64 loss: -0.1420518159866333
Batch 28/64 loss: -0.13755762577056885
Batch 29/64 loss: -0.1304384469985962
Batch 30/64 loss: -0.12346500158309937
Batch 31/64 loss: -0.1334201693534851
Batch 32/64 loss: -0.15423297882080078
Batch 33/64 loss: -0.1537429690361023
Batch 34/64 loss: -0.15675508975982666
Batch 35/64 loss: -0.12164825201034546
Batch 36/64 loss: -0.13434600830078125
Batch 37/64 loss: -0.15278667211532593
Batch 38/64 loss: -0.15172487497329712
Batch 39/64 loss: -0.14918702840805054
Batch 40/64 loss: -0.12100672721862793
Batch 41/64 loss: -0.14117634296417236
Batch 42/64 loss: -0.13910257816314697
Batch 43/64 loss: -0.1320275068283081
Batch 44/64 loss: -0.12499666213989258
Batch 45/64 loss: -0.13034415245056152
Batch 46/64 loss: -0.15473884344100952
Batch 47/64 loss: -0.11077219247817993
Batch 48/64 loss: -0.11690032482147217
Batch 49/64 loss: -0.1408117413520813
Batch 50/64 loss: -0.15389323234558105
Batch 51/64 loss: -0.15943312644958496
Batch 52/64 loss: -0.12295413017272949
Batch 53/64 loss: -0.1401820182800293
Batch 54/64 loss: -0.1131851077079773
Batch 55/64 loss: -0.15653103590011597
Batch 56/64 loss: -0.14245563745498657
Batch 57/64 loss: -0.13697302341461182
Batch 58/64 loss: -0.1342606544494629
Batch 59/64 loss: -0.14696872234344482
Batch 60/64 loss: -0.16124635934829712
Batch 61/64 loss: -0.14477336406707764
Batch 62/64 loss: -0.1467263102531433
Batch 63/64 loss: -0.1509106159210205
Batch 64/64 loss: -0.14477074146270752
Epoch 434  Train loss: -0.1391792662003461  Val loss: 0.05115431649578396
Epoch 435
-------------------------------
Batch 1/64 loss: -0.1510595679283142
Batch 2/64 loss: -0.1515994668006897
Batch 3/64 loss: -0.1492944359779358
Batch 4/64 loss: -0.11953568458557129
Batch 5/64 loss: -0.13897007703781128
Batch 6/64 loss: -0.1659797728061676
Batch 7/64 loss: -0.14771050214767456
Batch 8/64 loss: -0.1564549207687378
Batch 9/64 loss: -0.11068105697631836
Batch 10/64 loss: -0.139723539352417
Batch 11/64 loss: -0.14075583219528198
Batch 12/64 loss: -0.1461198925971985
Batch 13/64 loss: -0.13691836595535278
Batch 14/64 loss: -0.12697440385818481
Batch 15/64 loss: -0.1310560703277588
Batch 16/64 loss: -0.13428449630737305
Batch 17/64 loss: -0.14097756147384644
Batch 18/64 loss: -0.1538146734237671
Batch 19/64 loss: -0.1703292429447174
Batch 20/64 loss: -0.11392849683761597
Batch 21/64 loss: -0.115567147731781
Batch 22/64 loss: -0.17006510496139526
Batch 23/64 loss: -0.15242892503738403
Batch 24/64 loss: -0.1606423258781433
Batch 25/64 loss: -0.13011842966079712
Batch 26/64 loss: -0.1395493745803833
Batch 27/64 loss: -0.13297325372695923
Batch 28/64 loss: -0.14117443561553955
Batch 29/64 loss: -0.17283087968826294
Batch 30/64 loss: -0.13455086946487427
Batch 31/64 loss: -0.13834553956985474
Batch 32/64 loss: -0.1608622670173645
Batch 33/64 loss: -0.129225492477417
Batch 34/64 loss: -0.15202564001083374
Batch 35/64 loss: -0.14100581407546997
Batch 36/64 loss: -0.14566314220428467
Batch 37/64 loss: -0.13768315315246582
Batch 38/64 loss: -0.12351363897323608
Batch 39/64 loss: -0.1479250192642212
Batch 40/64 loss: -0.16090095043182373
Batch 41/64 loss: -0.13697874546051025
Batch 42/64 loss: -0.13519513607025146
Batch 43/64 loss: -0.13849133253097534
Batch 44/64 loss: -0.13487380743026733
Batch 45/64 loss: -0.15568971633911133
Batch 46/64 loss: -0.16425323486328125
Batch 47/64 loss: -0.16397827863693237
Batch 48/64 loss: -0.13228929042816162
Batch 49/64 loss: -0.1457412838935852
Batch 50/64 loss: -0.12505346536636353
Batch 51/64 loss: -0.13526010513305664
Batch 52/64 loss: -0.15651947259902954
Batch 53/64 loss: -0.16086804866790771
Batch 54/64 loss: -0.13614630699157715
Batch 55/64 loss: -0.14182764291763306
Batch 56/64 loss: -0.1353546380996704
Batch 57/64 loss: -0.1258624792098999
Batch 58/64 loss: -0.11703729629516602
Batch 59/64 loss: -0.12753677368164062
Batch 60/64 loss: -0.14120888710021973
Batch 61/64 loss: -0.13218146562576294
Batch 62/64 loss: -0.11683976650238037
Batch 63/64 loss: -0.1346379518508911
Batch 64/64 loss: -0.1523110270500183
Epoch 435  Train loss: -0.14151024093814923  Val loss: 0.05526409485086133
Epoch 436
-------------------------------
Batch 1/64 loss: -0.1147952675819397
Batch 2/64 loss: -0.1385442018508911
Batch 3/64 loss: -0.13383948802947998
Batch 4/64 loss: -0.16747358441352844
Batch 5/64 loss: -0.1556926965713501
Batch 6/64 loss: -0.1252443790435791
Batch 7/64 loss: -0.1639990210533142
Batch 8/64 loss: -0.1428554654121399
Batch 9/64 loss: -0.12083321809768677
Batch 10/64 loss: -0.15490972995758057
Batch 11/64 loss: -0.15692222118377686
Batch 12/64 loss: -0.1486600637435913
Batch 13/64 loss: -0.12985557317733765
Batch 14/64 loss: -0.1377231478691101
Batch 15/64 loss: -0.13627421855926514
Batch 16/64 loss: -0.1394331455230713
Batch 17/64 loss: -0.12953126430511475
Batch 18/64 loss: -0.11764222383499146
Batch 19/64 loss: -0.14854389429092407
Batch 20/64 loss: -0.1364898681640625
Batch 21/64 loss: -0.1492863893508911
Batch 22/64 loss: -0.14007198810577393
Batch 23/64 loss: -0.13042032718658447
Batch 24/64 loss: -0.1518019437789917
Batch 25/64 loss: -0.13560134172439575
Batch 26/64 loss: -0.16700181365013123
Batch 27/64 loss: -0.169720858335495
Batch 28/64 loss: -0.14827817678451538
Batch 29/64 loss: -0.14146536588668823
Batch 30/64 loss: -0.14573705196380615
Batch 31/64 loss: -0.09516137838363647
Batch 32/64 loss: -0.13650411367416382
Batch 33/64 loss: -0.1432490348815918
Batch 34/64 loss: -0.1168438196182251
Batch 35/64 loss: -0.11935156583786011
Batch 36/64 loss: -0.0873258113861084
Batch 37/64 loss: -0.14084666967391968
Batch 38/64 loss: -0.13748902082443237
Batch 39/64 loss: -0.15877661108970642
Batch 40/64 loss: -0.13858598470687866
Batch 41/64 loss: -0.1335882544517517
Batch 42/64 loss: -0.12946897745132446
Batch 43/64 loss: -0.12903112173080444
Batch 44/64 loss: -0.1311025619506836
Batch 45/64 loss: -0.15255409479141235
Batch 46/64 loss: -0.11824607849121094
Batch 47/64 loss: -0.15590202808380127
Batch 48/64 loss: -0.13945454359054565
Batch 49/64 loss: -0.14828932285308838
Batch 50/64 loss: -0.12821996212005615
Batch 51/64 loss: -0.13439536094665527
Batch 52/64 loss: -0.15222007036209106
Batch 53/64 loss: -0.13571959733963013
Batch 54/64 loss: -0.16056042909622192
Batch 55/64 loss: -0.14007925987243652
Batch 56/64 loss: -0.1257440447807312
Batch 57/64 loss: -0.1329403519630432
Batch 58/64 loss: -0.14683008193969727
Batch 59/64 loss: -0.1352391242980957
Batch 60/64 loss: -0.16507774591445923
Batch 61/64 loss: -0.14375078678131104
Batch 62/64 loss: -0.15562593936920166
Batch 63/64 loss: -0.13632816076278687
Batch 64/64 loss: -0.1316840648651123
Epoch 436  Train loss: -0.13932412371915928  Val loss: 0.05321835099216999
Epoch 437
-------------------------------
Batch 1/64 loss: -0.14057785272598267
Batch 2/64 loss: -0.14564001560211182
Batch 3/64 loss: -0.1607876420021057
Batch 4/64 loss: -0.16916441917419434
Batch 5/64 loss: -0.14085692167282104
Batch 6/64 loss: -0.15992885828018188
Batch 7/64 loss: -0.15571826696395874
Batch 8/64 loss: -0.15694975852966309
Batch 9/64 loss: -0.14573240280151367
Batch 10/64 loss: -0.09814286231994629
Batch 11/64 loss: -0.1566474437713623
Batch 12/64 loss: -0.1443607211112976
Batch 13/64 loss: -0.14237189292907715
Batch 14/64 loss: -0.14375686645507812
Batch 15/64 loss: -0.16579008102416992
Batch 16/64 loss: -0.1512569785118103
Batch 17/64 loss: -0.14852774143218994
Batch 18/64 loss: -0.1453075408935547
Batch 19/64 loss: -0.1448259949684143
Batch 20/64 loss: -0.1607072651386261
Batch 21/64 loss: -0.1575021743774414
Batch 22/64 loss: -0.1191214919090271
Batch 23/64 loss: -0.16235888004302979
Batch 24/64 loss: -0.14449679851531982
Batch 25/64 loss: -0.11940872669219971
Batch 26/64 loss: -0.14607661962509155
Batch 27/64 loss: -0.15109944343566895
Batch 28/64 loss: -0.13805431127548218
Batch 29/64 loss: -0.1268099546432495
Batch 30/64 loss: -0.1467081904411316
Batch 31/64 loss: -0.1455506682395935
Batch 32/64 loss: -0.1542491316795349
Batch 33/64 loss: -0.10716676712036133
Batch 34/64 loss: -0.16327208280563354
Batch 35/64 loss: -0.12612229585647583
Batch 36/64 loss: -0.13031864166259766
Batch 37/64 loss: -0.15847742557525635
Batch 38/64 loss: -0.1368662714958191
Batch 39/64 loss: -0.15341240167617798
Batch 40/64 loss: -0.13142740726470947
Batch 41/64 loss: -0.1291925311088562
Batch 42/64 loss: -0.12793898582458496
Batch 43/64 loss: -0.1393418312072754
Batch 44/64 loss: -0.13626188039779663
Batch 45/64 loss: -0.11365342140197754
Batch 46/64 loss: -0.13236349821090698
Batch 47/64 loss: -0.15276747941970825
Batch 48/64 loss: -0.15930232405662537
Batch 49/64 loss: -0.145094633102417
Batch 50/64 loss: -0.13785994052886963
Batch 51/64 loss: -0.10359972715377808
Batch 52/64 loss: -0.1287122368812561
Batch 53/64 loss: -0.15046417713165283
Batch 54/64 loss: -0.1357516050338745
Batch 55/64 loss: -0.16515934467315674
Batch 56/64 loss: -0.17453378438949585
Batch 57/64 loss: -0.14744937419891357
Batch 58/64 loss: -0.14353811740875244
Batch 59/64 loss: -0.13879162073135376
Batch 60/64 loss: -0.11845552921295166
Batch 61/64 loss: -0.15549004077911377
Batch 62/64 loss: -0.12988853454589844
Batch 63/64 loss: -0.11901414394378662
Batch 64/64 loss: -0.15371233224868774
Epoch 437  Train loss: -0.14267388582229615  Val loss: 0.05056218941187121
Epoch 438
-------------------------------
Batch 1/64 loss: -0.15846771001815796
Batch 2/64 loss: -0.1485757827758789
Batch 3/64 loss: -0.14015710353851318
Batch 4/64 loss: -0.11705178022384644
Batch 5/64 loss: -0.1587960124015808
Batch 6/64 loss: -0.14495527744293213
Batch 7/64 loss: -0.1413528323173523
Batch 8/64 loss: -0.14322364330291748
Batch 9/64 loss: -0.14172601699829102
Batch 10/64 loss: -0.12973922491073608
Batch 11/64 loss: -0.15784358978271484
Batch 12/64 loss: -0.13480472564697266
Batch 13/64 loss: -0.139958918094635
Batch 14/64 loss: -0.14044582843780518
Batch 15/64 loss: -0.156993567943573
Batch 16/64 loss: -0.12205350399017334
Batch 17/64 loss: -0.11747044324874878
Batch 18/64 loss: -0.1367582082748413
Batch 19/64 loss: -0.1619834005832672
Batch 20/64 loss: -0.1297944188117981
Batch 21/64 loss: -0.12682533264160156
Batch 22/64 loss: -0.13476860523223877
Batch 23/64 loss: -0.15150964260101318
Batch 24/64 loss: -0.13886874914169312
Batch 25/64 loss: -0.13430291414260864
Batch 26/64 loss: -0.15956205129623413
Batch 27/64 loss: -0.13092416524887085
Batch 28/64 loss: -0.14251244068145752
Batch 29/64 loss: -0.14679080247879028
Batch 30/64 loss: -0.15948426723480225
Batch 31/64 loss: -0.09479331970214844
Batch 32/64 loss: -0.16459399461746216
Batch 33/64 loss: -0.1395092010498047
Batch 34/64 loss: -0.13703596591949463
Batch 35/64 loss: -0.1367126703262329
Batch 36/64 loss: -0.13478296995162964
Batch 37/64 loss: -0.1485716700553894
Batch 38/64 loss: -0.17037999629974365
Batch 39/64 loss: -0.14291179180145264
Batch 40/64 loss: -0.14626318216323853
Batch 41/64 loss: -0.13667380809783936
Batch 42/64 loss: -0.1241152286529541
Batch 43/64 loss: -0.16203850507736206
Batch 44/64 loss: -0.11650228500366211
Batch 45/64 loss: -0.15580624341964722
Batch 46/64 loss: -0.14464789628982544
Batch 47/64 loss: -0.13706672191619873
Batch 48/64 loss: -0.1285516619682312
Batch 49/64 loss: -0.1331782341003418
Batch 50/64 loss: -0.15015673637390137
Batch 51/64 loss: -0.16028356552124023
Batch 52/64 loss: -0.145058274269104
Batch 53/64 loss: -0.14707940816879272
Batch 54/64 loss: -0.14402663707733154
Batch 55/64 loss: -0.14843666553497314
Batch 56/64 loss: -0.13659489154815674
Batch 57/64 loss: -0.18961825966835022
Batch 58/64 loss: -0.13998937606811523
Batch 59/64 loss: -0.1018781065940857
Batch 60/64 loss: -0.1426907181739807
Batch 61/64 loss: -0.11487889289855957
Batch 62/64 loss: -0.1325053572654724
Batch 63/64 loss: -0.15048938989639282
Batch 64/64 loss: -0.10980534553527832
Epoch 438  Train loss: -0.1409862995147705  Val loss: 0.05298872133300886
Epoch 439
-------------------------------
Batch 1/64 loss: -0.14047563076019287
Batch 2/64 loss: -0.1292095184326172
Batch 3/64 loss: -0.13610708713531494
Batch 4/64 loss: -0.15132355690002441
Batch 5/64 loss: -0.11213123798370361
Batch 6/64 loss: -0.14418023824691772
Batch 7/64 loss: -0.1547023057937622
Batch 8/64 loss: -0.16556206345558167
Batch 9/64 loss: -0.14313024282455444
Batch 10/64 loss: -0.14922261238098145
Batch 11/64 loss: -0.14789438247680664
Batch 12/64 loss: -0.12990540266036987
Batch 13/64 loss: -0.14670366048812866
Batch 14/64 loss: -0.16722369194030762
Batch 15/64 loss: -0.1740603744983673
Batch 16/64 loss: -0.11523163318634033
Batch 17/64 loss: -0.14728856086730957
Batch 18/64 loss: -0.14463990926742554
Batch 19/64 loss: -0.1461573839187622
Batch 20/64 loss: -0.14222025871276855
Batch 21/64 loss: -0.16998785734176636
Batch 22/64 loss: -0.13312989473342896
Batch 23/64 loss: -0.13056623935699463
Batch 24/64 loss: -0.13488543033599854
Batch 25/64 loss: -0.14507746696472168
Batch 26/64 loss: -0.13451409339904785
Batch 27/64 loss: -0.14270853996276855
Batch 28/64 loss: -0.12475079298019409
Batch 29/64 loss: -0.14858180284500122
Batch 30/64 loss: -0.12778574228286743
Batch 31/64 loss: -0.14483952522277832
Batch 32/64 loss: -0.15249139070510864
Batch 33/64 loss: -0.14517349004745483
Batch 34/64 loss: -0.18180102109909058
Batch 35/64 loss: -0.14406824111938477
Batch 36/64 loss: -0.15078425407409668
Batch 37/64 loss: -0.17176693677902222
Batch 38/64 loss: -0.13571345806121826
Batch 39/64 loss: -0.13357043266296387
Batch 40/64 loss: -0.13757801055908203
Batch 41/64 loss: -0.1347370147705078
Batch 42/64 loss: -0.13902133703231812
Batch 43/64 loss: -0.1359398365020752
Batch 44/64 loss: -0.14193904399871826
Batch 45/64 loss: -0.1433112621307373
Batch 46/64 loss: -0.1575559377670288
Batch 47/64 loss: -0.16179180145263672
Batch 48/64 loss: -0.1441362500190735
Batch 49/64 loss: -0.15100473165512085
Batch 50/64 loss: -0.15884089469909668
Batch 51/64 loss: -0.13921058177947998
Batch 52/64 loss: -0.14052510261535645
Batch 53/64 loss: -0.126512348651886
Batch 54/64 loss: -0.14904624223709106
Batch 55/64 loss: -0.12325942516326904
Batch 56/64 loss: -0.15569937229156494
Batch 57/64 loss: -0.16796648502349854
Batch 58/64 loss: -0.1542760729789734
Batch 59/64 loss: -0.149455726146698
Batch 60/64 loss: -0.16109463572502136
Batch 61/64 loss: -0.1352839469909668
Batch 62/64 loss: -0.12470579147338867
Batch 63/64 loss: -0.15461552143096924
Batch 64/64 loss: -0.17276903986930847
Epoch 439  Train loss: -0.14520236101804995  Val loss: 0.05246601297273669
Epoch 440
-------------------------------
Batch 1/64 loss: -0.14871084690093994
Batch 2/64 loss: -0.17621397972106934
Batch 3/64 loss: -0.14702671766281128
Batch 4/64 loss: -0.132502019405365
Batch 5/64 loss: -0.15340185165405273
Batch 6/64 loss: -0.14842748641967773
Batch 7/64 loss: -0.14613783359527588
Batch 8/64 loss: -0.13877272605895996
Batch 9/64 loss: -0.14360284805297852
Batch 10/64 loss: -0.15340161323547363
Batch 11/64 loss: -0.1589476466178894
Batch 12/64 loss: -0.13674145936965942
Batch 13/64 loss: -0.1546803116798401
Batch 14/64 loss: -0.1518040895462036
Batch 15/64 loss: -0.16462084650993347
Batch 16/64 loss: -0.11427783966064453
Batch 17/64 loss: -0.16829895973205566
Batch 18/64 loss: -0.15907621383666992
Batch 19/64 loss: -0.163848876953125
Batch 20/64 loss: -0.14140743017196655
Batch 21/64 loss: -0.12302893400192261
Batch 22/64 loss: -0.15739014744758606
Batch 23/64 loss: -0.1715712547302246
Batch 24/64 loss: -0.1621202826499939
Batch 25/64 loss: -0.17257171869277954
Batch 26/64 loss: -0.15635335445404053
Batch 27/64 loss: -0.17148548364639282
Batch 28/64 loss: -0.1408156156539917
Batch 29/64 loss: -0.1565535068511963
Batch 30/64 loss: -0.16315114498138428
Batch 31/64 loss: -0.15634453296661377
Batch 32/64 loss: -0.15575003623962402
Batch 33/64 loss: -0.09314662218093872
Batch 34/64 loss: -0.13886278867721558
Batch 35/64 loss: -0.12366300821304321
Batch 36/64 loss: -0.13979947566986084
Batch 37/64 loss: -0.16430464386940002
Batch 38/64 loss: -0.11625325679779053
Batch 39/64 loss: -0.11798107624053955
Batch 40/64 loss: -0.15954163670539856
Batch 41/64 loss: -0.10355973243713379
Batch 42/64 loss: -0.14629220962524414
Batch 43/64 loss: -0.12104415893554688
Batch 44/64 loss: -0.1570112705230713
Batch 45/64 loss: -0.14019161462783813
Batch 46/64 loss: -0.1489390730857849
Batch 47/64 loss: -0.1477990746498108
Batch 48/64 loss: -0.1115153431892395
Batch 49/64 loss: -0.1412522792816162
Batch 50/64 loss: -0.14354932308197021
Batch 51/64 loss: -0.13246911764144897
Batch 52/64 loss: -0.15653544664382935
Batch 53/64 loss: -0.12590819597244263
Batch 54/64 loss: -0.16056936979293823
Batch 55/64 loss: -0.13175427913665771
Batch 56/64 loss: -0.15265017747879028
Batch 57/64 loss: -0.14996588230133057
Batch 58/64 loss: -0.1670323610305786
Batch 59/64 loss: -0.13697242736816406
Batch 60/64 loss: -0.14502829313278198
Batch 61/64 loss: -0.1697474718093872
Batch 62/64 loss: -0.1395803689956665
Batch 63/64 loss: -0.1327795386314392
Batch 64/64 loss: -0.11309045553207397
Epoch 440  Train loss: -0.1457185093094321  Val loss: 0.053625492091031415
Epoch 441
-------------------------------
Batch 1/64 loss: -0.1693558692932129
Batch 2/64 loss: -0.14727813005447388
Batch 3/64 loss: -0.165696918964386
Batch 4/64 loss: -0.12748003005981445
Batch 5/64 loss: -0.15496081113815308
Batch 6/64 loss: -0.12468403577804565
Batch 7/64 loss: -0.12546813488006592
Batch 8/64 loss: -0.14846789836883545
Batch 9/64 loss: -0.14558196067810059
Batch 10/64 loss: -0.16503006219863892
Batch 11/64 loss: -0.14721715450286865
Batch 12/64 loss: -0.16595661640167236
Batch 13/64 loss: -0.11934107542037964
Batch 14/64 loss: -0.1424342393875122
Batch 15/64 loss: -0.11181604862213135
Batch 16/64 loss: -0.14820808172225952
Batch 17/64 loss: -0.13729500770568848
Batch 18/64 loss: -0.15314370393753052
Batch 19/64 loss: -0.14037084579467773
Batch 20/64 loss: -0.11701911687850952
Batch 21/64 loss: -0.1425139307975769
Batch 22/64 loss: -0.10185754299163818
Batch 23/64 loss: -0.12222176790237427
Batch 24/64 loss: -0.137481689453125
Batch 25/64 loss: -0.13194608688354492
Batch 26/64 loss: -0.12186884880065918
Batch 27/64 loss: -0.13674414157867432
Batch 28/64 loss: -0.12389320135116577
Batch 29/64 loss: -0.14684611558914185
Batch 30/64 loss: -0.147510826587677
Batch 31/64 loss: -0.11555951833724976
Batch 32/64 loss: -0.16298675537109375
Batch 33/64 loss: -0.1695692539215088
Batch 34/64 loss: -0.1586686372756958
Batch 35/64 loss: -0.16246634721755981
Batch 36/64 loss: -0.151655912399292
Batch 37/64 loss: -0.1477000117301941
Batch 38/64 loss: -0.15787357091903687
Batch 39/64 loss: -0.15190935134887695
Batch 40/64 loss: -0.0991508960723877
Batch 41/64 loss: -0.12251389026641846
Batch 42/64 loss: -0.1368502974510193
Batch 43/64 loss: -0.15136092901229858
Batch 44/64 loss: -0.15218698978424072
Batch 45/64 loss: -0.16725894808769226
Batch 46/64 loss: -0.13110530376434326
Batch 47/64 loss: -0.13561296463012695
Batch 48/64 loss: -0.14185500144958496
Batch 49/64 loss: -0.13859599828720093
Batch 50/64 loss: -0.11891686916351318
Batch 51/64 loss: -0.14424443244934082
Batch 52/64 loss: -0.15535950660705566
Batch 53/64 loss: -0.15249520540237427
Batch 54/64 loss: -0.1334383487701416
Batch 55/64 loss: -0.13339084386825562
Batch 56/64 loss: -0.14496755599975586
Batch 57/64 loss: -0.1609637439250946
Batch 58/64 loss: -0.15993177890777588
Batch 59/64 loss: -0.15385150909423828
Batch 60/64 loss: -0.1461448073387146
Batch 61/64 loss: -0.14015769958496094
Batch 62/64 loss: -0.11947178840637207
Batch 63/64 loss: -0.144616961479187
Batch 64/64 loss: -0.17127493023872375
Epoch 441  Train loss: -0.14213298383881065  Val loss: 0.05481182771040402
Epoch 442
-------------------------------
Batch 1/64 loss: -0.16324734687805176
Batch 2/64 loss: -0.16941505670547485
Batch 3/64 loss: -0.15612655878067017
Batch 4/64 loss: -0.15108615159988403
Batch 5/64 loss: -0.1466282606124878
Batch 6/64 loss: -0.1435450315475464
Batch 7/64 loss: -0.12669885158538818
Batch 8/64 loss: -0.14870506525039673
Batch 9/64 loss: -0.15701627731323242
Batch 10/64 loss: -0.17303529381752014
Batch 11/64 loss: -0.1488419771194458
Batch 12/64 loss: -0.1638089418411255
Batch 13/64 loss: -0.14062488079071045
Batch 14/64 loss: -0.15255016088485718
Batch 15/64 loss: -0.1305510401725769
Batch 16/64 loss: -0.1391584277153015
Batch 17/64 loss: -0.13899695873260498
Batch 18/64 loss: -0.14310777187347412
Batch 19/64 loss: -0.1325453519821167
Batch 20/64 loss: -0.13739854097366333
Batch 21/64 loss: -0.13322216272354126
Batch 22/64 loss: -0.14427196979522705
Batch 23/64 loss: -0.1581840217113495
Batch 24/64 loss: -0.12679243087768555
Batch 25/64 loss: -0.14670449495315552
Batch 26/64 loss: -0.1532517671585083
Batch 27/64 loss: -0.13583779335021973
Batch 28/64 loss: -0.15801191329956055
Batch 29/64 loss: -0.1451188325881958
Batch 30/64 loss: -0.14546865224838257
Batch 31/64 loss: -0.16660010814666748
Batch 32/64 loss: -0.1517326831817627
Batch 33/64 loss: -0.15151506662368774
Batch 34/64 loss: -0.14875227212905884
Batch 35/64 loss: -0.15764981508255005
Batch 36/64 loss: -0.1449759602546692
Batch 37/64 loss: -0.17166322469711304
Batch 38/64 loss: -0.13484054803848267
Batch 39/64 loss: -0.15093040466308594
Batch 40/64 loss: -0.1525377631187439
Batch 41/64 loss: -0.15281730890274048
Batch 42/64 loss: -0.12422585487365723
Batch 43/64 loss: -0.11942505836486816
Batch 44/64 loss: -0.1645280122756958
Batch 45/64 loss: -0.15213704109191895
Batch 46/64 loss: -0.12534987926483154
Batch 47/64 loss: -0.14761972427368164
Batch 48/64 loss: -0.1309511661529541
Batch 49/64 loss: -0.1368083357810974
Batch 50/64 loss: -0.13717561960220337
Batch 51/64 loss: -0.12823539972305298
Batch 52/64 loss: -0.15491139888763428
Batch 53/64 loss: -0.1236768364906311
Batch 54/64 loss: -0.11947113275527954
Batch 55/64 loss: -0.12698566913604736
Batch 56/64 loss: -0.1418013572692871
Batch 57/64 loss: -0.14202266931533813
Batch 58/64 loss: -0.12195479869842529
Batch 59/64 loss: -0.1490011215209961
Batch 60/64 loss: -0.16994309425354004
Batch 61/64 loss: -0.1349889636039734
Batch 62/64 loss: -0.15177065134048462
Batch 63/64 loss: -0.1528071165084839
Batch 64/64 loss: -0.11625683307647705
Epoch 442  Train loss: -0.1448933437758801  Val loss: 0.05317064003436426
Epoch 443
-------------------------------
Batch 1/64 loss: -0.16071075201034546
Batch 2/64 loss: -0.1577741503715515
Batch 3/64 loss: -0.12005424499511719
Batch 4/64 loss: -0.16180413961410522
Batch 5/64 loss: -0.1101912260055542
Batch 6/64 loss: -0.13550174236297607
Batch 7/64 loss: -0.15678879618644714
Batch 8/64 loss: -0.10800278186798096
Batch 9/64 loss: -0.1300748586654663
Batch 10/64 loss: -0.14632856845855713
Batch 11/64 loss: -0.1526498794555664
Batch 12/64 loss: -0.1139557957649231
Batch 13/64 loss: -0.14644771814346313
Batch 14/64 loss: -0.15497714281082153
Batch 15/64 loss: -0.14982086420059204
Batch 16/64 loss: -0.1419476866722107
Batch 17/64 loss: -0.1320122480392456
Batch 18/64 loss: -0.18013131618499756
Batch 19/64 loss: -0.18026506900787354
Batch 20/64 loss: -0.16105437278747559
Batch 21/64 loss: -0.1394250988960266
Batch 22/64 loss: -0.16044583916664124
Batch 23/64 loss: -0.15388637781143188
Batch 24/64 loss: -0.16364547610282898
Batch 25/64 loss: -0.1663227677345276
Batch 26/64 loss: -0.17328712344169617
Batch 27/64 loss: -0.15704333782196045
Batch 28/64 loss: -0.17638909816741943
Batch 29/64 loss: -0.13814830780029297
Batch 30/64 loss: -0.1372535228729248
Batch 31/64 loss: -0.15515542030334473
Batch 32/64 loss: -0.1250566840171814
Batch 33/64 loss: -0.14265257120132446
Batch 34/64 loss: -0.15088367462158203
Batch 35/64 loss: -0.16461658477783203
Batch 36/64 loss: -0.154624342918396
Batch 37/64 loss: -0.1438698172569275
Batch 38/64 loss: -0.10298359394073486
Batch 39/64 loss: -0.16889899969100952
Batch 40/64 loss: -0.12884026765823364
Batch 41/64 loss: -0.12609511613845825
Batch 42/64 loss: -0.16084721684455872
Batch 43/64 loss: -0.15268617868423462
Batch 44/64 loss: -0.14198267459869385
Batch 45/64 loss: -0.15993332862854004
Batch 46/64 loss: -0.17022418975830078
Batch 47/64 loss: -0.14575481414794922
Batch 48/64 loss: -0.13206416368484497
Batch 49/64 loss: -0.10321438312530518
Batch 50/64 loss: -0.1581287980079651
Batch 51/64 loss: -0.12254095077514648
Batch 52/64 loss: -0.13254165649414062
Batch 53/64 loss: -0.1304178237915039
Batch 54/64 loss: -0.1251506209373474
Batch 55/64 loss: -0.14724987745285034
Batch 56/64 loss: -0.155565083026886
Batch 57/64 loss: -0.15672850608825684
Batch 58/64 loss: -0.10284423828125
Batch 59/64 loss: -0.17169010639190674
Batch 60/64 loss: -0.16990244388580322
Batch 61/64 loss: -0.15245002508163452
Batch 62/64 loss: -0.13993310928344727
Batch 63/64 loss: -0.1402829885482788
Batch 64/64 loss: -0.12490791082382202
Epoch 443  Train loss: -0.14581696449541579  Val loss: 0.05454712819397654
Epoch 444
-------------------------------
Batch 1/64 loss: -0.16897135972976685
Batch 2/64 loss: -0.16860532760620117
Batch 3/64 loss: -0.11700981855392456
Batch 4/64 loss: -0.11430519819259644
Batch 5/64 loss: -0.15374910831451416
Batch 6/64 loss: -0.1398501992225647
Batch 7/64 loss: -0.15625429153442383
Batch 8/64 loss: -0.1693347692489624
Batch 9/64 loss: -0.13099193572998047
Batch 10/64 loss: -0.1539161205291748
Batch 11/64 loss: -0.16212183237075806
Batch 12/64 loss: -0.16904392838478088
Batch 13/64 loss: -0.1270965337753296
Batch 14/64 loss: -0.15454328060150146
Batch 15/64 loss: -0.15887761116027832
Batch 16/64 loss: -0.12897658348083496
Batch 17/64 loss: -0.14780449867248535
Batch 18/64 loss: -0.15016520023345947
Batch 19/64 loss: -0.17487066984176636
Batch 20/64 loss: -0.11521166563034058
Batch 21/64 loss: -0.14665722846984863
Batch 22/64 loss: -0.16502758860588074
Batch 23/64 loss: -0.149441659450531
Batch 24/64 loss: -0.17133837938308716
Batch 25/64 loss: -0.15662509202957153
Batch 26/64 loss: -0.14668679237365723
Batch 27/64 loss: -0.15015089511871338
Batch 28/64 loss: -0.12456512451171875
Batch 29/64 loss: -0.12956315279006958
Batch 30/64 loss: -0.1288624405860901
Batch 31/64 loss: -0.14428871870040894
Batch 32/64 loss: -0.14724016189575195
Batch 33/64 loss: -0.1277812123298645
Batch 34/64 loss: -0.1548733115196228
Batch 35/64 loss: -0.13792675733566284
Batch 36/64 loss: -0.1497514247894287
Batch 37/64 loss: -0.15274977684020996
Batch 38/64 loss: -0.16911816596984863
Batch 39/64 loss: -0.1632968783378601
Batch 40/64 loss: -0.1417832374572754
Batch 41/64 loss: -0.12027722597122192
Batch 42/64 loss: -0.15253615379333496
Batch 43/64 loss: -0.1585397720336914
Batch 44/64 loss: -0.15699076652526855
Batch 45/64 loss: -0.13626950979232788
Batch 46/64 loss: -0.10964685678482056
Batch 47/64 loss: -0.1350730061531067
Batch 48/64 loss: -0.13652420043945312
Batch 49/64 loss: -0.14176368713378906
Batch 50/64 loss: -0.15240442752838135
Batch 51/64 loss: -0.13290554285049438
Batch 52/64 loss: -0.1581059694290161
Batch 53/64 loss: -0.13869214057922363
Batch 54/64 loss: -0.15608152747154236
Batch 55/64 loss: -0.14538425207138062
Batch 56/64 loss: -0.16174530982971191
Batch 57/64 loss: -0.1613122820854187
Batch 58/64 loss: -0.1314706802368164
Batch 59/64 loss: -0.1188231110572815
Batch 60/64 loss: -0.13578128814697266
Batch 61/64 loss: -0.16016244888305664
Batch 62/64 loss: -0.12321984767913818
Batch 63/64 loss: -0.1256198287010193
Batch 64/64 loss: -0.124664306640625
Epoch 444  Train loss: -0.14529029014063816  Val loss: 0.054794846736278734
Epoch 445
-------------------------------
Batch 1/64 loss: -0.1345233917236328
Batch 2/64 loss: -0.13657885789871216
Batch 3/64 loss: -0.15604817867279053
Batch 4/64 loss: -0.1157066822052002
Batch 5/64 loss: -0.17432746291160583
Batch 6/64 loss: -0.12391769886016846
Batch 7/64 loss: -0.14011090993881226
Batch 8/64 loss: -0.1365797519683838
Batch 9/64 loss: -0.16308724880218506
Batch 10/64 loss: -0.11773258447647095
Batch 11/64 loss: -0.16234159469604492
Batch 12/64 loss: -0.13644641637802124
Batch 13/64 loss: -0.15663212537765503
Batch 14/64 loss: -0.18031233549118042
Batch 15/64 loss: -0.15585660934448242
Batch 16/64 loss: -0.16037291288375854
Batch 17/64 loss: -0.16717231273651123
Batch 18/64 loss: -0.14190363883972168
Batch 19/64 loss: -0.10557031631469727
Batch 20/64 loss: -0.15837252140045166
Batch 21/64 loss: -0.17726635932922363
Batch 22/64 loss: -0.10869169235229492
Batch 23/64 loss: -0.14533162117004395
Batch 24/64 loss: -0.1478751301765442
Batch 25/64 loss: -0.12809664011001587
Batch 26/64 loss: -0.1440832018852234
Batch 27/64 loss: -0.12397456169128418
Batch 28/64 loss: -0.14760178327560425
Batch 29/64 loss: -0.1687290072441101
Batch 30/64 loss: -0.12929439544677734
Batch 31/64 loss: -0.14550328254699707
Batch 32/64 loss: -0.13896995782852173
Batch 33/64 loss: -0.1815916895866394
Batch 34/64 loss: -0.14146137237548828
Batch 35/64 loss: -0.13671910762786865
Batch 36/64 loss: -0.12216722965240479
Batch 37/64 loss: -0.13035255670547485
Batch 38/64 loss: -0.1687132716178894
Batch 39/64 loss: -0.14369386434555054
Batch 40/64 loss: -0.13420617580413818
Batch 41/64 loss: -0.1511368751525879
Batch 42/64 loss: -0.13651633262634277
Batch 43/64 loss: -0.14056086540222168
Batch 44/64 loss: -0.12141835689544678
Batch 45/64 loss: -0.1623132824897766
Batch 46/64 loss: -0.1256101131439209
Batch 47/64 loss: -0.14381492137908936
Batch 48/64 loss: -0.17338979244232178
Batch 49/64 loss: -0.16173821687698364
Batch 50/64 loss: -0.11006760597229004
Batch 51/64 loss: -0.15870893001556396
Batch 52/64 loss: -0.13994711637496948
Batch 53/64 loss: -0.13674622774124146
Batch 54/64 loss: -0.15185940265655518
Batch 55/64 loss: -0.15206807851791382
Batch 56/64 loss: -0.1532609462738037
Batch 57/64 loss: -0.11353248357772827
Batch 58/64 loss: -0.15401345491409302
Batch 59/64 loss: -0.17656004428863525
Batch 60/64 loss: -0.12433850765228271
Batch 61/64 loss: -0.16335254907608032
Batch 62/64 loss: -0.10833799839019775
Batch 63/64 loss: -0.16445454955101013
Batch 64/64 loss: -0.1738593578338623
Epoch 445  Train loss: -0.14497345288594563  Val loss: 0.05427353877791834
Epoch 446
-------------------------------
Batch 1/64 loss: -0.1709335446357727
Batch 2/64 loss: -0.1429080367088318
Batch 3/64 loss: -0.168545663356781
Batch 4/64 loss: -0.17868566513061523
Batch 5/64 loss: -0.14970695972442627
Batch 6/64 loss: -0.16518014669418335
Batch 7/64 loss: -0.16012892127037048
Batch 8/64 loss: -0.1343705654144287
Batch 9/64 loss: -0.18015968799591064
Batch 10/64 loss: -0.13558387756347656
Batch 11/64 loss: -0.14736783504486084
Batch 12/64 loss: -0.14229226112365723
Batch 13/64 loss: -0.1662519872188568
Batch 14/64 loss: -0.1200132966041565
Batch 15/64 loss: -0.1578616499900818
Batch 16/64 loss: -0.14737457036972046
Batch 17/64 loss: -0.1459975242614746
Batch 18/64 loss: -0.14435654878616333
Batch 19/64 loss: -0.133650004863739
Batch 20/64 loss: -0.12741196155548096
Batch 21/64 loss: -0.16848081350326538
Batch 22/64 loss: -0.1527853012084961
Batch 23/64 loss: -0.1254022717475891
Batch 24/64 loss: -0.13858342170715332
Batch 25/64 loss: -0.14791470766067505
Batch 26/64 loss: -0.13458251953125
Batch 27/64 loss: -0.1382056474685669
Batch 28/64 loss: -0.15813475847244263
Batch 29/64 loss: -0.16959550976753235
Batch 30/64 loss: -0.13738447427749634
Batch 31/64 loss: -0.13261395692825317
Batch 32/64 loss: -0.17634278535842896
Batch 33/64 loss: -0.1353839635848999
Batch 34/64 loss: -0.1381932497024536
Batch 35/64 loss: -0.14307552576065063
Batch 36/64 loss: -0.14243930578231812
Batch 37/64 loss: -0.1026540994644165
Batch 38/64 loss: -0.1395542025566101
Batch 39/64 loss: -0.14503949880599976
Batch 40/64 loss: -0.15438556671142578
Batch 41/64 loss: -0.11552309989929199
Batch 42/64 loss: -0.12303268909454346
Batch 43/64 loss: -0.14458608627319336
Batch 44/64 loss: -0.15590685606002808
Batch 45/64 loss: -0.14072048664093018
Batch 46/64 loss: -0.14217674732208252
Batch 47/64 loss: -0.1466715931892395
Batch 48/64 loss: -0.14181262254714966
Batch 49/64 loss: -0.1323598027229309
Batch 50/64 loss: -0.10794752836227417
Batch 51/64 loss: -0.15338456630706787
Batch 52/64 loss: -0.15169578790664673
Batch 53/64 loss: -0.13914388418197632
Batch 54/64 loss: -0.12695956230163574
Batch 55/64 loss: -0.14028620719909668
Batch 56/64 loss: -0.1605832576751709
Batch 57/64 loss: -0.1620500087738037
Batch 58/64 loss: -0.15495610237121582
Batch 59/64 loss: -0.13818854093551636
Batch 60/64 loss: -0.09334695339202881
Batch 61/64 loss: -0.09129106998443604
Batch 62/64 loss: -0.1393800973892212
Batch 63/64 loss: -0.1412191390991211
Batch 64/64 loss: -0.15679174661636353
Epoch 446  Train loss: -0.1436917456926084  Val loss: 0.05498207229928872
Epoch 447
-------------------------------
Batch 1/64 loss: -0.16839957237243652
Batch 2/64 loss: -0.1316775679588318
Batch 3/64 loss: -0.15925481915473938
Batch 4/64 loss: -0.1246986985206604
Batch 5/64 loss: -0.12997007369995117
Batch 6/64 loss: -0.15131914615631104
Batch 7/64 loss: -0.1315211057662964
Batch 8/64 loss: -0.1164240837097168
Batch 9/64 loss: -0.15669602155685425
Batch 10/64 loss: -0.11699903011322021
Batch 11/64 loss: -0.14252781867980957
Batch 12/64 loss: -0.1374528408050537
Batch 13/64 loss: -0.1228187084197998
Batch 14/64 loss: -0.16386473178863525
Batch 15/64 loss: -0.13822513818740845
Batch 16/64 loss: -0.1279117465019226
Batch 17/64 loss: -0.1478310227394104
Batch 18/64 loss: -0.12999534606933594
Batch 19/64 loss: -0.1375407576560974
Batch 20/64 loss: -0.1354275941848755
Batch 21/64 loss: -0.138452410697937
Batch 22/64 loss: -0.13022738695144653
Batch 23/64 loss: -0.15644711256027222
Batch 24/64 loss: -0.1393020749092102
Batch 25/64 loss: -0.13544505834579468
Batch 26/64 loss: -0.15967783331871033
Batch 27/64 loss: -0.1429750919342041
Batch 28/64 loss: -0.14120447635650635
Batch 29/64 loss: -0.1651630401611328
Batch 30/64 loss: -0.14494985342025757
Batch 31/64 loss: -0.1775684952735901
Batch 32/64 loss: -0.1628328263759613
Batch 33/64 loss: -0.17216497659683228
Batch 34/64 loss: -0.1395835280418396
Batch 35/64 loss: -0.15436047315597534
Batch 36/64 loss: -0.14903652667999268
Batch 37/64 loss: -0.16173261404037476
Batch 38/64 loss: -0.1467139720916748
Batch 39/64 loss: -0.15327036380767822
Batch 40/64 loss: -0.11570900678634644
Batch 41/64 loss: -0.1287139654159546
Batch 42/64 loss: -0.16413408517837524
Batch 43/64 loss: -0.14475101232528687
Batch 44/64 loss: -0.15486490726470947
Batch 45/64 loss: -0.1397244930267334
Batch 46/64 loss: -0.16532403230667114
Batch 47/64 loss: -0.16851234436035156
Batch 48/64 loss: -0.16965609788894653
Batch 49/64 loss: -0.12221658229827881
Batch 50/64 loss: -0.10921370983123779
Batch 51/64 loss: -0.14047950506210327
Batch 52/64 loss: -0.16131117939949036
Batch 53/64 loss: -0.15238255262374878
Batch 54/64 loss: -0.1671697497367859
Batch 55/64 loss: -0.17496365308761597
Batch 56/64 loss: -0.14914196729660034
Batch 57/64 loss: -0.15360784530639648
Batch 58/64 loss: -0.1605740189552307
Batch 59/64 loss: -0.13618755340576172
Batch 60/64 loss: -0.14790838956832886
Batch 61/64 loss: -0.12098169326782227
Batch 62/64 loss: -0.17147964239120483
Batch 63/64 loss: -0.16465523838996887
Batch 64/64 loss: -0.1451941728591919
Epoch 447  Train loss: -0.1464191810757506  Val loss: 0.054766790973361824
Epoch 448
-------------------------------
Batch 1/64 loss: -0.12716257572174072
Batch 2/64 loss: -0.13919669389724731
Batch 3/64 loss: -0.1510685682296753
Batch 4/64 loss: -0.14507007598876953
Batch 5/64 loss: -0.14344733953475952
Batch 6/64 loss: -0.16617673635482788
Batch 7/64 loss: -0.1338251829147339
Batch 8/64 loss: -0.1406199336051941
Batch 9/64 loss: -0.16273874044418335
Batch 10/64 loss: -0.13979190587997437
Batch 11/64 loss: -0.14957791566848755
Batch 12/64 loss: -0.1357111930847168
Batch 13/64 loss: -0.18336829543113708
Batch 14/64 loss: -0.16428756713867188
Batch 15/64 loss: -0.1615889072418213
Batch 16/64 loss: -0.16364356875419617
Batch 17/64 loss: -0.16025161743164062
Batch 18/64 loss: -0.16223537921905518
Batch 19/64 loss: -0.13792353868484497
Batch 20/64 loss: -0.15812528133392334
Batch 21/64 loss: -0.13578420877456665
Batch 22/64 loss: -0.1195523738861084
Batch 23/64 loss: -0.1654241979122162
Batch 24/64 loss: -0.12210452556610107
Batch 25/64 loss: -0.12390094995498657
Batch 26/64 loss: -0.17245018482208252
Batch 27/64 loss: -0.14909052848815918
Batch 28/64 loss: -0.1638832688331604
Batch 29/64 loss: -0.1779612898826599
Batch 30/64 loss: -0.15700852870941162
Batch 31/64 loss: -0.1288852095603943
Batch 32/64 loss: -0.1486789584159851
Batch 33/64 loss: -0.1365075707435608
Batch 34/64 loss: -0.1505981683731079
Batch 35/64 loss: -0.14954859018325806
Batch 36/64 loss: -0.14986580610275269
Batch 37/64 loss: -0.16084745526313782
Batch 38/64 loss: -0.15737825632095337
Batch 39/64 loss: -0.1837843954563141
Batch 40/64 loss: -0.12470364570617676
Batch 41/64 loss: -0.12525588274002075
Batch 42/64 loss: -0.14973455667495728
Batch 43/64 loss: -0.18413639068603516
Batch 44/64 loss: -0.1524367332458496
Batch 45/64 loss: -0.15105688571929932
Batch 46/64 loss: -0.13902872800827026
Batch 47/64 loss: -0.08639758825302124
Batch 48/64 loss: -0.13374167680740356
Batch 49/64 loss: -0.15375161170959473
Batch 50/64 loss: -0.15195846557617188
Batch 51/64 loss: -0.14498299360275269
Batch 52/64 loss: -0.14992761611938477
Batch 53/64 loss: -0.0979660153388977
Batch 54/64 loss: -0.13442867994308472
Batch 55/64 loss: -0.13152503967285156
Batch 56/64 loss: -0.14736390113830566
Batch 57/64 loss: -0.1349635124206543
Batch 58/64 loss: -0.1446170210838318
Batch 59/64 loss: -0.17380517721176147
Batch 60/64 loss: -0.11594665050506592
Batch 61/64 loss: -0.1288164258003235
Batch 62/64 loss: -0.15358120203018188
Batch 63/64 loss: -0.14358890056610107
Batch 64/64 loss: -0.1492612361907959
Epoch 448  Train loss: -0.14658394841586841  Val loss: 0.05624248563628836
Epoch 449
-------------------------------
Batch 1/64 loss: -0.09360283613204956
Batch 2/64 loss: -0.14903903007507324
Batch 3/64 loss: -0.15055298805236816
Batch 4/64 loss: -0.15872341394424438
Batch 5/64 loss: -0.14557135105133057
Batch 6/64 loss: -0.14483654499053955
Batch 7/64 loss: -0.17084577679634094
Batch 8/64 loss: -0.18404388427734375
Batch 9/64 loss: -0.15808075666427612
Batch 10/64 loss: -0.1388145089149475
Batch 11/64 loss: -0.15059226751327515
Batch 12/64 loss: -0.16032129526138306
Batch 13/64 loss: -0.1653541922569275
Batch 14/64 loss: -0.12309920787811279
Batch 15/64 loss: -0.17137008905410767
Batch 16/64 loss: -0.16791677474975586
Batch 17/64 loss: -0.1653294861316681
Batch 18/64 loss: -0.14274924993515015
Batch 19/64 loss: -0.15282201766967773
Batch 20/64 loss: -0.11154448986053467
Batch 21/64 loss: -0.17244848608970642
Batch 22/64 loss: -0.15226435661315918
Batch 23/64 loss: -0.13162410259246826
Batch 24/64 loss: -0.13501763343811035
Batch 25/64 loss: -0.13476520776748657
Batch 26/64 loss: -0.15101724863052368
Batch 27/64 loss: -0.14522546529769897
Batch 28/64 loss: -0.13634294271469116
Batch 29/64 loss: -0.15410858392715454
Batch 30/64 loss: -0.13282394409179688
Batch 31/64 loss: -0.1329978108406067
Batch 32/64 loss: -0.128434419631958
Batch 33/64 loss: -0.13542330265045166
Batch 34/64 loss: -0.1503276228904724
Batch 35/64 loss: -0.15015721321105957
Batch 36/64 loss: -0.15305852890014648
Batch 37/64 loss: -0.15569031238555908
Batch 38/64 loss: -0.15118378400802612
Batch 39/64 loss: -0.1293061375617981
Batch 40/64 loss: -0.11895871162414551
Batch 41/64 loss: -0.1520335078239441
Batch 42/64 loss: -0.15317624807357788
Batch 43/64 loss: -0.1792753040790558
Batch 44/64 loss: -0.15175288915634155
Batch 45/64 loss: -0.16838222742080688
Batch 46/64 loss: -0.13761138916015625
Batch 47/64 loss: -0.11780738830566406
Batch 48/64 loss: -0.1549416184425354
Batch 49/64 loss: -0.16887453198432922
Batch 50/64 loss: -0.1759585738182068
Batch 51/64 loss: -0.15509039163589478
Batch 52/64 loss: -0.12165796756744385
Batch 53/64 loss: -0.14091068506240845
Batch 54/64 loss: -0.14903825521469116
Batch 55/64 loss: -0.15225517749786377
Batch 56/64 loss: -0.10573780536651611
Batch 57/64 loss: -0.1427960991859436
Batch 58/64 loss: -0.17629772424697876
Batch 59/64 loss: -0.15962406992912292
Batch 60/64 loss: -0.12230569124221802
Batch 61/64 loss: -0.1426842212677002
Batch 62/64 loss: -0.14787578582763672
Batch 63/64 loss: -0.11404252052307129
Batch 64/64 loss: -0.0983508825302124
Epoch 449  Train loss: -0.14620045820871988  Val loss: 0.05534799627422057
Epoch 450
-------------------------------
Batch 1/64 loss: -0.13923406600952148
Batch 2/64 loss: -0.14541202783584595
Batch 3/64 loss: -0.15825152397155762
Batch 4/64 loss: -0.12811166048049927
Batch 5/64 loss: -0.15959271788597107
Batch 6/64 loss: -0.12559455633163452
Batch 7/64 loss: -0.15287232398986816
Batch 8/64 loss: -0.15593701601028442
Batch 9/64 loss: -0.13913989067077637
Batch 10/64 loss: -0.17298370599746704
Batch 11/64 loss: -0.12376415729522705
Batch 12/64 loss: -0.17227613925933838
Batch 13/64 loss: -0.17156875133514404
Batch 14/64 loss: -0.13007879257202148
Batch 15/64 loss: -0.17909765243530273
Batch 16/64 loss: -0.1454164981842041
Batch 17/64 loss: -0.162031888961792
Batch 18/64 loss: -0.11667758226394653
Batch 19/64 loss: -0.15602219104766846
Batch 20/64 loss: -0.12243056297302246
Batch 21/64 loss: -0.14259541034698486
Batch 22/64 loss: -0.16045498847961426
Batch 23/64 loss: -0.15766239166259766
Batch 24/64 loss: -0.14762932062149048
Batch 25/64 loss: -0.171858012676239
Batch 26/64 loss: -0.1418980360031128
Batch 27/64 loss: -0.14189481735229492
Batch 28/64 loss: -0.14593446254730225
Batch 29/64 loss: -0.14652931690216064
Batch 30/64 loss: -0.12874138355255127
Batch 31/64 loss: -0.14421045780181885
Batch 32/64 loss: -0.15194815397262573
Batch 33/64 loss: -0.14033746719360352
Batch 34/64 loss: -0.13017159700393677
Batch 35/64 loss: -0.1797465682029724
Batch 36/64 loss: -0.1573629379272461
Batch 37/64 loss: -0.16025257110595703
Batch 38/64 loss: -0.1614866554737091
Batch 39/64 loss: -0.14401549100875854
Batch 40/64 loss: -0.17462685704231262
Batch 41/64 loss: -0.15428388118743896
Batch 42/64 loss: -0.16685497760772705
Batch 43/64 loss: -0.12803155183792114
Batch 44/64 loss: -0.12498295307159424
Batch 45/64 loss: -0.12399148941040039
Batch 46/64 loss: -0.16064375638961792
Batch 47/64 loss: -0.14185881614685059
Batch 48/64 loss: -0.14015644788742065
Batch 49/64 loss: -0.15817123651504517
Batch 50/64 loss: -0.15463298559188843
Batch 51/64 loss: -0.1671346127986908
Batch 52/64 loss: -0.14637267589569092
Batch 53/64 loss: -0.15774458646774292
Batch 54/64 loss: -0.1399528980255127
Batch 55/64 loss: -0.090129554271698
Batch 56/64 loss: -0.13035517930984497
Batch 57/64 loss: -0.14328861236572266
Batch 58/64 loss: -0.10846781730651855
Batch 59/64 loss: -0.13515478372573853
Batch 60/64 loss: -0.14603447914123535
Batch 61/64 loss: -0.1426263451576233
Batch 62/64 loss: -0.1509702205657959
Batch 63/64 loss: -0.11974990367889404
Batch 64/64 loss: -0.09793001413345337
Epoch 450  Train loss: -0.14573941768384446  Val loss: 0.05617777854716245
Epoch 451
-------------------------------
Batch 1/64 loss: -0.16179347038269043
Batch 2/64 loss: -0.13626056909561157
Batch 3/64 loss: -0.13542914390563965
Batch 4/64 loss: -0.1457088589668274
Batch 5/64 loss: -0.14871788024902344
Batch 6/64 loss: -0.1648826003074646
Batch 7/64 loss: -0.1507396101951599
Batch 8/64 loss: -0.1587391197681427
Batch 9/64 loss: -0.1618884801864624
Batch 10/64 loss: -0.12348568439483643
Batch 11/64 loss: -0.14832323789596558
Batch 12/64 loss: -0.15540891885757446
Batch 13/64 loss: -0.16169145703315735
Batch 14/64 loss: -0.1008000373840332
Batch 15/64 loss: -0.09899640083312988
Batch 16/64 loss: -0.15086930990219116
Batch 17/64 loss: -0.10688364505767822
Batch 18/64 loss: -0.09481000900268555
Batch 19/64 loss: -0.14432209730148315
Batch 20/64 loss: -0.13746845722198486
Batch 21/64 loss: -0.15750432014465332
Batch 22/64 loss: -0.15009886026382446
Batch 23/64 loss: -0.15438127517700195
Batch 24/64 loss: -0.1399693489074707
Batch 25/64 loss: -0.13661056756973267
Batch 26/64 loss: -0.14737474918365479
Batch 27/64 loss: -0.14935564994812012
Batch 28/64 loss: -0.13530224561691284
Batch 29/64 loss: -0.1727205514907837
Batch 30/64 loss: -0.1487177014350891
Batch 31/64 loss: -0.14944618940353394
Batch 32/64 loss: -0.16401463747024536
Batch 33/64 loss: -0.14208030700683594
Batch 34/64 loss: -0.14681249856948853
Batch 35/64 loss: -0.1485348343849182
Batch 36/64 loss: -0.16507920622825623
Batch 37/64 loss: -0.1576722264289856
Batch 38/64 loss: -0.1248708963394165
Batch 39/64 loss: -0.17473649978637695
Batch 40/64 loss: -0.15137165784835815
Batch 41/64 loss: -0.16536593437194824
Batch 42/64 loss: -0.15965795516967773
Batch 43/64 loss: -0.1427215337753296
Batch 44/64 loss: -0.16430959105491638
Batch 45/64 loss: -0.14479756355285645
Batch 46/64 loss: -0.10125505924224854
Batch 47/64 loss: -0.11149084568023682
Batch 48/64 loss: -0.1547924280166626
Batch 49/64 loss: -0.1576419472694397
Batch 50/64 loss: -0.16464737057685852
Batch 51/64 loss: -0.15427064895629883
Batch 52/64 loss: -0.1297140121459961
Batch 53/64 loss: -0.13578379154205322
Batch 54/64 loss: -0.14924460649490356
Batch 55/64 loss: -0.14084786176681519
Batch 56/64 loss: -0.1417229175567627
Batch 57/64 loss: -0.1488468050956726
Batch 58/64 loss: -0.14329904317855835
Batch 59/64 loss: -0.12435972690582275
Batch 60/64 loss: -0.15712666511535645
Batch 61/64 loss: -0.1433091163635254
Batch 62/64 loss: -0.14270812273025513
Batch 63/64 loss: -0.11780178546905518
Batch 64/64 loss: -0.15950781106948853
Epoch 451  Train loss: -0.14461520629770616  Val loss: 0.05405880106273795
Epoch 452
-------------------------------
Batch 1/64 loss: -0.14235180616378784
Batch 2/64 loss: -0.15876930952072144
Batch 3/64 loss: -0.17000442743301392
Batch 4/64 loss: -0.15987122058868408
Batch 5/64 loss: -0.1446460485458374
Batch 6/64 loss: -0.16868185997009277
Batch 7/64 loss: -0.13078486919403076
Batch 8/64 loss: -0.13734596967697144
Batch 9/64 loss: -0.1797005832195282
Batch 10/64 loss: -0.15524983406066895
Batch 11/64 loss: -0.1438341736793518
Batch 12/64 loss: -0.15189921855926514
Batch 13/64 loss: -0.1443934440612793
Batch 14/64 loss: -0.12594759464263916
Batch 15/64 loss: -0.1584162712097168
Batch 16/64 loss: -0.15782475471496582
Batch 17/64 loss: -0.1671169400215149
Batch 18/64 loss: -0.15455341339111328
Batch 19/64 loss: -0.17275899648666382
Batch 20/64 loss: -0.14163738489151
Batch 21/64 loss: -0.14842301607131958
Batch 22/64 loss: -0.1280001401901245
Batch 23/64 loss: -0.12660861015319824
Batch 24/64 loss: -0.1534215807914734
Batch 25/64 loss: -0.16454368829727173
Batch 26/64 loss: -0.13376951217651367
Batch 27/64 loss: -0.14917904138565063
Batch 28/64 loss: -0.1646544337272644
Batch 29/64 loss: -0.15363627672195435
Batch 30/64 loss: -0.15411776304244995
Batch 31/64 loss: -0.15903359651565552
Batch 32/64 loss: -0.14960336685180664
Batch 33/64 loss: -0.13094866275787354
Batch 34/64 loss: -0.14876443147659302
Batch 35/64 loss: -0.14059072732925415
Batch 36/64 loss: -0.1437128186225891
Batch 37/64 loss: -0.11844712495803833
Batch 38/64 loss: -0.15376228094100952
Batch 39/64 loss: -0.1396147608757019
Batch 40/64 loss: -0.13343948125839233
Batch 41/64 loss: -0.1377394199371338
Batch 42/64 loss: -0.15352630615234375
Batch 43/64 loss: -0.12574011087417603
Batch 44/64 loss: -0.10650694370269775
Batch 45/64 loss: -0.14691942930221558
Batch 46/64 loss: -0.13746744394302368
Batch 47/64 loss: -0.09041786193847656
Batch 48/64 loss: -0.15954908728599548
Batch 49/64 loss: -0.17126363515853882
Batch 50/64 loss: -0.15960299968719482
Batch 51/64 loss: -0.1497424840927124
Batch 52/64 loss: -0.1632181704044342
Batch 53/64 loss: -0.15079212188720703
Batch 54/64 loss: -0.1392221450805664
Batch 55/64 loss: -0.12567484378814697
Batch 56/64 loss: -0.10205912590026855
Batch 57/64 loss: -0.14252698421478271
Batch 58/64 loss: -0.1742783486843109
Batch 59/64 loss: -0.15332341194152832
Batch 60/64 loss: -0.15266644954681396
Batch 61/64 loss: -0.14447253942489624
Batch 62/64 loss: -0.132787823677063
Batch 63/64 loss: -0.15121376514434814
Batch 64/64 loss: -0.1157185435295105
Epoch 452  Train loss: -0.14615780068378822  Val loss: 0.05420099091284054
Epoch 453
-------------------------------
Batch 1/64 loss: -0.13622307777404785
Batch 2/64 loss: -0.15222173929214478
Batch 3/64 loss: -0.14678078889846802
Batch 4/64 loss: -0.15530574321746826
Batch 5/64 loss: -0.1558314561843872
Batch 6/64 loss: -0.15914809703826904
Batch 7/64 loss: -0.13832640647888184
Batch 8/64 loss: -0.14415216445922852
Batch 9/64 loss: -0.13216650485992432
Batch 10/64 loss: -0.14906316995620728
Batch 11/64 loss: -0.12683314085006714
Batch 12/64 loss: -0.16585975885391235
Batch 13/64 loss: -0.15247893333435059
Batch 14/64 loss: -0.15265285968780518
Batch 15/64 loss: -0.1398773193359375
Batch 16/64 loss: -0.1643485724925995
Batch 17/64 loss: -0.15282219648361206
Batch 18/64 loss: -0.17718082666397095
Batch 19/64 loss: -0.1502392292022705
Batch 20/64 loss: -0.12644946575164795
Batch 21/64 loss: -0.158108651638031
Batch 22/64 loss: -0.17792177200317383
Batch 23/64 loss: -0.15240728855133057
Batch 24/64 loss: -0.15674322843551636
Batch 25/64 loss: -0.13185054063796997
Batch 26/64 loss: -0.1517735719680786
Batch 27/64 loss: -0.15796881914138794
Batch 28/64 loss: -0.14343363046646118
Batch 29/64 loss: -0.145796537399292
Batch 30/64 loss: -0.1582167148590088
Batch 31/64 loss: -0.162503182888031
Batch 32/64 loss: -0.15461355447769165
Batch 33/64 loss: -0.1515880823135376
Batch 34/64 loss: -0.13199979066848755
Batch 35/64 loss: -0.1482551097869873
Batch 36/64 loss: -0.14565670490264893
Batch 37/64 loss: -0.13848567008972168
Batch 38/64 loss: -0.14486908912658691
Batch 39/64 loss: -0.14753425121307373
Batch 40/64 loss: -0.15970221161842346
Batch 41/64 loss: -0.12164473533630371
Batch 42/64 loss: -0.1356145739555359
Batch 43/64 loss: -0.16013383865356445
Batch 44/64 loss: -0.1354711651802063
Batch 45/64 loss: -0.16071856021881104
Batch 46/64 loss: -0.1515231728553772
Batch 47/64 loss: -0.12723970413208008
Batch 48/64 loss: -0.1623641848564148
Batch 49/64 loss: -0.14411884546279907
Batch 50/64 loss: -0.14915233850479126
Batch 51/64 loss: -0.13364827632904053
Batch 52/64 loss: -0.12885499000549316
Batch 53/64 loss: -0.12326514720916748
Batch 54/64 loss: -0.15152502059936523
Batch 55/64 loss: -0.15056073665618896
Batch 56/64 loss: -0.12867307662963867
Batch 57/64 loss: -0.15377944707870483
Batch 58/64 loss: -0.1356843113899231
Batch 59/64 loss: -0.1217505931854248
Batch 60/64 loss: -0.14907598495483398
Batch 61/64 loss: -0.12481653690338135
Batch 62/64 loss: -0.1072225570678711
Batch 63/64 loss: -0.17459210753440857
Batch 64/64 loss: -0.11838865280151367
Epoch 453  Train loss: -0.1461586078008016  Val loss: 0.05493612633538
Epoch 454
-------------------------------
Batch 1/64 loss: -0.13214391469955444
Batch 2/64 loss: -0.14011073112487793
Batch 3/64 loss: -0.1592562198638916
Batch 4/64 loss: -0.18985185027122498
Batch 5/64 loss: -0.1798006296157837
Batch 6/64 loss: -0.13913452625274658
Batch 7/64 loss: -0.16178089380264282
Batch 8/64 loss: -0.13146430253982544
Batch 9/64 loss: -0.1362265944480896
Batch 10/64 loss: -0.17137333750724792
Batch 11/64 loss: -0.14868009090423584
Batch 12/64 loss: -0.16865038871765137
Batch 13/64 loss: -0.12826687097549438
Batch 14/64 loss: -0.16124534606933594
Batch 15/64 loss: -0.14850759506225586
Batch 16/64 loss: -0.14631056785583496
Batch 17/64 loss: -0.1324779987335205
Batch 18/64 loss: -0.1580578088760376
Batch 19/64 loss: -0.1553938388824463
Batch 20/64 loss: -0.13881850242614746
Batch 21/64 loss: -0.15564310550689697
Batch 22/64 loss: -0.14667075872421265
Batch 23/64 loss: -0.16030168533325195
Batch 24/64 loss: -0.1514054536819458
Batch 25/64 loss: -0.15718603134155273
Batch 26/64 loss: -0.15475773811340332
Batch 27/64 loss: -0.13651448488235474
Batch 28/64 loss: -0.16103386878967285
Batch 29/64 loss: -0.150631844997406
Batch 30/64 loss: -0.1589640974998474
Batch 31/64 loss: -0.153140127658844
Batch 32/64 loss: -0.1569840908050537
Batch 33/64 loss: -0.14642304182052612
Batch 34/64 loss: -0.09613388776779175
Batch 35/64 loss: -0.1371077299118042
Batch 36/64 loss: -0.13736283779144287
Batch 37/64 loss: -0.1141672134399414
Batch 38/64 loss: -0.14158576726913452
Batch 39/64 loss: -0.17166465520858765
Batch 40/64 loss: -0.16113421320915222
Batch 41/64 loss: -0.15901196002960205
Batch 42/64 loss: -0.15999943017959595
Batch 43/64 loss: -0.1524006724357605
Batch 44/64 loss: -0.1290750503540039
Batch 45/64 loss: -0.16272646188735962
Batch 46/64 loss: -0.14572477340698242
Batch 47/64 loss: -0.16095638275146484
Batch 48/64 loss: -0.11373084783554077
Batch 49/64 loss: -0.16890424489974976
Batch 50/64 loss: -0.13622891902923584
Batch 51/64 loss: -0.13890331983566284
Batch 52/64 loss: -0.10204136371612549
Batch 53/64 loss: -0.15296053886413574
Batch 54/64 loss: -0.1336500644683838
Batch 55/64 loss: -0.13327348232269287
Batch 56/64 loss: -0.15665465593338013
Batch 57/64 loss: -0.143124520778656
Batch 58/64 loss: -0.14650678634643555
Batch 59/64 loss: -0.14314329624176025
Batch 60/64 loss: -0.13410907983779907
Batch 61/64 loss: -0.15685436129570007
Batch 62/64 loss: -0.1650201678276062
Batch 63/64 loss: -0.14173758029937744
Batch 64/64 loss: -0.1552029848098755
Epoch 454  Train loss: -0.14791380143633076  Val loss: 0.052914703629680516
Epoch 455
-------------------------------
Batch 1/64 loss: -0.13362592458724976
Batch 2/64 loss: -0.12279725074768066
Batch 3/64 loss: -0.15218162536621094
Batch 4/64 loss: -0.1804009974002838
Batch 5/64 loss: -0.13678407669067383
Batch 6/64 loss: -0.1298213005065918
Batch 7/64 loss: -0.1756480634212494
Batch 8/64 loss: -0.13647717237472534
Batch 9/64 loss: -0.15063953399658203
Batch 10/64 loss: -0.13576918840408325
Batch 11/64 loss: -0.137739896774292
Batch 12/64 loss: -0.1317724585533142
Batch 13/64 loss: -0.12692314386367798
Batch 14/64 loss: -0.13125991821289062
Batch 15/64 loss: -0.1416892409324646
Batch 16/64 loss: -0.1638229489326477
Batch 17/64 loss: -0.12246370315551758
Batch 18/64 loss: -0.13483744859695435
Batch 19/64 loss: -0.1443328857421875
Batch 20/64 loss: -0.13704609870910645
Batch 21/64 loss: -0.1499425172805786
Batch 22/64 loss: -0.1548740267753601
Batch 23/64 loss: -0.11872202157974243
Batch 24/64 loss: -0.13179755210876465
Batch 25/64 loss: -0.15834447741508484
Batch 26/64 loss: -0.15153396129608154
Batch 27/64 loss: -0.15841144323349
Batch 28/64 loss: -0.14245456457138062
Batch 29/64 loss: -0.15100497007369995
Batch 30/64 loss: -0.13051843643188477
Batch 31/64 loss: -0.16877657175064087
Batch 32/64 loss: -0.1528078317642212
Batch 33/64 loss: -0.15113306045532227
Batch 34/64 loss: -0.16209927201271057
Batch 35/64 loss: -0.15052855014801025
Batch 36/64 loss: -0.15231984853744507
Batch 37/64 loss: -0.14652913808822632
Batch 38/64 loss: -0.12730562686920166
Batch 39/64 loss: -0.1693420708179474
Batch 40/64 loss: -0.10980993509292603
Batch 41/64 loss: -0.15352731943130493
Batch 42/64 loss: -0.15834838151931763
Batch 43/64 loss: -0.16035917401313782
Batch 44/64 loss: -0.151228129863739
Batch 45/64 loss: -0.1351718306541443
Batch 46/64 loss: -0.1326735019683838
Batch 47/64 loss: -0.14946073293685913
Batch 48/64 loss: -0.16614192724227905
Batch 49/64 loss: -0.15645521879196167
Batch 50/64 loss: -0.11677408218383789
Batch 51/64 loss: -0.14008700847625732
Batch 52/64 loss: -0.14778488874435425
Batch 53/64 loss: -0.1651071012020111
Batch 54/64 loss: -0.1581900715827942
Batch 55/64 loss: -0.15125805139541626
Batch 56/64 loss: -0.15417486429214478
Batch 57/64 loss: -0.15477287769317627
Batch 58/64 loss: -0.15213119983673096
Batch 59/64 loss: -0.14716672897338867
Batch 60/64 loss: -0.15243715047836304
Batch 61/64 loss: -0.15858161449432373
Batch 62/64 loss: -0.13469833135604858
Batch 63/64 loss: -0.1408197283744812
Batch 64/64 loss: -0.12658172845840454
Epoch 455  Train loss: -0.14582862690383314  Val loss: 0.05550058191174904
Epoch 456
-------------------------------
Batch 1/64 loss: -0.13107317686080933
Batch 2/64 loss: -0.15368318557739258
Batch 3/64 loss: -0.17637813091278076
Batch 4/64 loss: -0.16040503978729248
Batch 5/64 loss: -0.1370517611503601
Batch 6/64 loss: -0.13968807458877563
Batch 7/64 loss: -0.1585751175880432
Batch 8/64 loss: -0.1338416337966919
Batch 9/64 loss: -0.16659122705459595
Batch 10/64 loss: -0.16114965081214905
Batch 11/64 loss: -0.16265243291854858
Batch 12/64 loss: -0.14743077754974365
Batch 13/64 loss: -0.1665075719356537
Batch 14/64 loss: -0.13807642459869385
Batch 15/64 loss: -0.17731884121894836
Batch 16/64 loss: -0.16417473554611206
Batch 17/64 loss: -0.1351836919784546
Batch 18/64 loss: -0.15294289588928223
Batch 19/64 loss: -0.16069573163986206
Batch 20/64 loss: -0.14933937788009644
Batch 21/64 loss: -0.15142220258712769
Batch 22/64 loss: -0.15587836503982544
Batch 23/64 loss: -0.17386668920516968
Batch 24/64 loss: -0.157225102186203
Batch 25/64 loss: -0.15370512008666992
Batch 26/64 loss: -0.11502385139465332
Batch 27/64 loss: -0.16221612691879272
Batch 28/64 loss: -0.16854670643806458
Batch 29/64 loss: -0.11708188056945801
Batch 30/64 loss: -0.15591001510620117
Batch 31/64 loss: -0.11129558086395264
Batch 32/64 loss: -0.1581965684890747
Batch 33/64 loss: -0.17013350129127502
Batch 34/64 loss: -0.12093281745910645
Batch 35/64 loss: -0.10891467332839966
Batch 36/64 loss: -0.14990657567977905
Batch 37/64 loss: -0.16488829255104065
Batch 38/64 loss: -0.1428859829902649
Batch 39/64 loss: -0.12429177761077881
Batch 40/64 loss: -0.14037179946899414
Batch 41/64 loss: -0.13595330715179443
Batch 42/64 loss: -0.14055955410003662
Batch 43/64 loss: -0.18522042036056519
Batch 44/64 loss: -0.1272318959236145
Batch 45/64 loss: -0.13137459754943848
Batch 46/64 loss: -0.1517837643623352
Batch 47/64 loss: -0.13577675819396973
Batch 48/64 loss: -0.16141951084136963
Batch 49/64 loss: -0.15676209330558777
Batch 50/64 loss: -0.14280062913894653
Batch 51/64 loss: -0.12333118915557861
Batch 52/64 loss: -0.14536380767822266
Batch 53/64 loss: -0.12498921155929565
Batch 54/64 loss: -0.15781497955322266
Batch 55/64 loss: -0.1419888138771057
Batch 56/64 loss: -0.1706663966178894
Batch 57/64 loss: -0.17482390999794006
Batch 58/64 loss: -0.15398931503295898
Batch 59/64 loss: -0.1450812816619873
Batch 60/64 loss: -0.14710193872451782
Batch 61/64 loss: -0.15590894222259521
Batch 62/64 loss: -0.16049546003341675
Batch 63/64 loss: -0.1256030797958374
Batch 64/64 loss: -0.14919865131378174
Epoch 456  Train loss: -0.14875910515878715  Val loss: 0.05557167591507902
Epoch 457
-------------------------------
Batch 1/64 loss: -0.15956145524978638
Batch 2/64 loss: -0.10603982210159302
Batch 3/64 loss: -0.13121455907821655
Batch 4/64 loss: -0.14005666971206665
Batch 5/64 loss: -0.15649688243865967
Batch 6/64 loss: -0.17058849334716797
Batch 7/64 loss: -0.16445225477218628
Batch 8/64 loss: -0.16455993056297302
Batch 9/64 loss: -0.16229048371315002
Batch 10/64 loss: -0.14911407232284546
Batch 11/64 loss: -0.1734658181667328
Batch 12/64 loss: -0.181712806224823
Batch 13/64 loss: -0.13769519329071045
Batch 14/64 loss: -0.12811678647994995
Batch 15/64 loss: -0.15234315395355225
Batch 16/64 loss: -0.12073630094528198
Batch 17/64 loss: -0.18286442756652832
Batch 18/64 loss: -0.14995110034942627
Batch 19/64 loss: -0.1715870499610901
Batch 20/64 loss: -0.16551601886749268
Batch 21/64 loss: -0.13461995124816895
Batch 22/64 loss: -0.16666027903556824
Batch 23/64 loss: -0.16099795699119568
Batch 24/64 loss: -0.17905110120773315
Batch 25/64 loss: -0.16476470232009888
Batch 26/64 loss: -0.16997653245925903
Batch 27/64 loss: -0.1300482153892517
Batch 28/64 loss: -0.14149349927902222
Batch 29/64 loss: -0.1309356689453125
Batch 30/64 loss: -0.15754961967468262
Batch 31/64 loss: -0.16994205117225647
Batch 32/64 loss: -0.13151991367340088
Batch 33/64 loss: -0.11704516410827637
Batch 34/64 loss: -0.14075201749801636
Batch 35/64 loss: -0.1592012643814087
Batch 36/64 loss: -0.1530964970588684
Batch 37/64 loss: -0.1507411003112793
Batch 38/64 loss: -0.17727404832839966
Batch 39/64 loss: -0.1399822235107422
Batch 40/64 loss: -0.095195472240448
Batch 41/64 loss: -0.15139144659042358
Batch 42/64 loss: -0.1457538604736328
Batch 43/64 loss: -0.14071017503738403
Batch 44/64 loss: -0.1599223017692566
Batch 45/64 loss: -0.13794726133346558
Batch 46/64 loss: -0.13979244232177734
Batch 47/64 loss: -0.1557610034942627
Batch 48/64 loss: -0.1536446213722229
Batch 49/64 loss: -0.1368948221206665
Batch 50/64 loss: -0.14390695095062256
Batch 51/64 loss: -0.15084415674209595
Batch 52/64 loss: -0.1432366967201233
Batch 53/64 loss: -0.17503339052200317
Batch 54/64 loss: -0.16285789012908936
Batch 55/64 loss: -0.16386336088180542
Batch 56/64 loss: -0.1209149956703186
Batch 57/64 loss: -0.16868335008621216
Batch 58/64 loss: -0.1343441605567932
Batch 59/64 loss: -0.16522002220153809
Batch 60/64 loss: -0.14839565753936768
Batch 61/64 loss: -0.1286691427230835
Batch 62/64 loss: -0.13132596015930176
Batch 63/64 loss: -0.16141831874847412
Batch 64/64 loss: -0.12807512283325195
Epoch 457  Train loss: -0.14989488452088598  Val loss: 0.056499876722027755
Epoch 458
-------------------------------
Batch 1/64 loss: -0.15340209007263184
Batch 2/64 loss: -0.13563776016235352
Batch 3/64 loss: -0.17058134078979492
Batch 4/64 loss: -0.18720218539237976
Batch 5/64 loss: -0.14900892972946167
Batch 6/64 loss: -0.15410441160202026
Batch 7/64 loss: -0.1608247458934784
Batch 8/64 loss: -0.16420847177505493
Batch 9/64 loss: -0.1637309491634369
Batch 10/64 loss: -0.16182705760002136
Batch 11/64 loss: -0.16221731901168823
Batch 12/64 loss: -0.17214888334274292
Batch 13/64 loss: -0.1298898458480835
Batch 14/64 loss: -0.13288599252700806
Batch 15/64 loss: -0.18307596445083618
Batch 16/64 loss: -0.17761150002479553
Batch 17/64 loss: -0.14135044813156128
Batch 18/64 loss: -0.1725103259086609
Batch 19/64 loss: -0.14091432094573975
Batch 20/64 loss: -0.15719199180603027
Batch 21/64 loss: -0.145693838596344
Batch 22/64 loss: -0.13849103450775146
Batch 23/64 loss: -0.15878042578697205
Batch 24/64 loss: -0.15218442678451538
Batch 25/64 loss: -0.14582455158233643
Batch 26/64 loss: -0.12742513418197632
Batch 27/64 loss: -0.12210661172866821
Batch 28/64 loss: -0.12424039840698242
Batch 29/64 loss: -0.15564113855361938
Batch 30/64 loss: -0.15506744384765625
Batch 31/64 loss: -0.15241283178329468
Batch 32/64 loss: -0.13602620363235474
Batch 33/64 loss: -0.1526595950126648
Batch 34/64 loss: -0.1617743968963623
Batch 35/64 loss: -0.1281338930130005
Batch 36/64 loss: -0.11957573890686035
Batch 37/64 loss: -0.16098666191101074
Batch 38/64 loss: -0.15071451663970947
Batch 39/64 loss: -0.16242721676826477
Batch 40/64 loss: -0.1507812738418579
Batch 41/64 loss: -0.15476500988006592
Batch 42/64 loss: -0.16103368997573853
Batch 43/64 loss: -0.15682506561279297
Batch 44/64 loss: -0.14077907800674438
Batch 45/64 loss: -0.1406925916671753
Batch 46/64 loss: -0.16198956966400146
Batch 47/64 loss: -0.1455797553062439
Batch 48/64 loss: -0.14786696434020996
Batch 49/64 loss: -0.1556868553161621
Batch 50/64 loss: -0.16345885396003723
Batch 51/64 loss: -0.16456669569015503
Batch 52/64 loss: -0.1194448471069336
Batch 53/64 loss: -0.12372004985809326
Batch 54/64 loss: -0.14769864082336426
Batch 55/64 loss: -0.15152102708816528
Batch 56/64 loss: -0.16003310680389404
Batch 57/64 loss: -0.12961941957473755
Batch 58/64 loss: -0.160541832447052
Batch 59/64 loss: -0.14062917232513428
Batch 60/64 loss: -0.13128608465194702
Batch 61/64 loss: -0.1505335569381714
Batch 62/64 loss: -0.13807719945907593
Batch 63/64 loss: -0.15090686082839966
Batch 64/64 loss: -0.15699315071105957
Epoch 458  Train loss: -0.15037290442223641  Val loss: 0.05676513992224362
Epoch 459
-------------------------------
Batch 1/64 loss: -0.1542101502418518
Batch 2/64 loss: -0.16971057653427124
Batch 3/64 loss: -0.1593417525291443
Batch 4/64 loss: -0.16266560554504395
Batch 5/64 loss: -0.15752428770065308
Batch 6/64 loss: -0.15272963047027588
Batch 7/64 loss: -0.18040964007377625
Batch 8/64 loss: -0.16964727640151978
Batch 9/64 loss: -0.14273405075073242
Batch 10/64 loss: -0.17656809091567993
Batch 11/64 loss: -0.15036576986312866
Batch 12/64 loss: -0.14244061708450317
Batch 13/64 loss: -0.1519315242767334
Batch 14/64 loss: -0.1674647331237793
Batch 15/64 loss: -0.10928922891616821
Batch 16/64 loss: -0.179365873336792
Batch 17/64 loss: -0.14264017343521118
Batch 18/64 loss: -0.130845308303833
Batch 19/64 loss: -0.13536059856414795
Batch 20/64 loss: -0.17211511731147766
Batch 21/64 loss: -0.15929123759269714
Batch 22/64 loss: -0.162695974111557
Batch 23/64 loss: -0.15829896926879883
Batch 24/64 loss: -0.15628176927566528
Batch 25/64 loss: -0.13095611333847046
Batch 26/64 loss: -0.15151000022888184
Batch 27/64 loss: -0.13504594564437866
Batch 28/64 loss: -0.11551588773727417
Batch 29/64 loss: -0.12438756227493286
Batch 30/64 loss: -0.12565934658050537
Batch 31/64 loss: -0.15099233388900757
Batch 32/64 loss: -0.16648870706558228
Batch 33/64 loss: -0.13623172044754028
Batch 34/64 loss: -0.15214967727661133
Batch 35/64 loss: -0.15075933933258057
Batch 36/64 loss: -0.15469098091125488
Batch 37/64 loss: -0.17124444246292114
Batch 38/64 loss: -0.17112547159194946
Batch 39/64 loss: -0.14519047737121582
Batch 40/64 loss: -0.13659340143203735
Batch 41/64 loss: -0.14315783977508545
Batch 42/64 loss: -0.1318334937095642
Batch 43/64 loss: -0.12690961360931396
Batch 44/64 loss: -0.16177687048912048
Batch 45/64 loss: -0.14907222986221313
Batch 46/64 loss: -0.11298376321792603
Batch 47/64 loss: -0.1327141523361206
Batch 48/64 loss: -0.13616657257080078
Batch 49/64 loss: -0.15034019947052002
Batch 50/64 loss: -0.14214015007019043
Batch 51/64 loss: -0.11250364780426025
Batch 52/64 loss: -0.17929816246032715
Batch 53/64 loss: -0.14763885736465454
Batch 54/64 loss: -0.1620737910270691
Batch 55/64 loss: -0.1578896939754486
Batch 56/64 loss: -0.13998907804489136
Batch 57/64 loss: -0.16037186980247498
Batch 58/64 loss: -0.14640730619430542
Batch 59/64 loss: -0.16013818979263306
Batch 60/64 loss: -0.14050191640853882
Batch 61/64 loss: -0.1457359790802002
Batch 62/64 loss: -0.16369333863258362
Batch 63/64 loss: -0.15929877758026123
Batch 64/64 loss: -0.08557730913162231
Epoch 459  Train loss: -0.14885157393474205  Val loss: 0.05505539031372857
Epoch 460
-------------------------------
Batch 1/64 loss: -0.16107121109962463
Batch 2/64 loss: -0.16431689262390137
Batch 3/64 loss: -0.17207950353622437
Batch 4/64 loss: -0.14026492834091187
Batch 5/64 loss: -0.14903581142425537
Batch 6/64 loss: -0.16616421937942505
Batch 7/64 loss: -0.11615151166915894
Batch 8/64 loss: -0.150814950466156
Batch 9/64 loss: -0.15881001949310303
Batch 10/64 loss: -0.18046534061431885
Batch 11/64 loss: -0.15196168422698975
Batch 12/64 loss: -0.13784456253051758
Batch 13/64 loss: -0.12720882892608643
Batch 14/64 loss: -0.13396906852722168
Batch 15/64 loss: -0.15950658917427063
Batch 16/64 loss: -0.15669310092926025
Batch 17/64 loss: -0.14542245864868164
Batch 18/64 loss: -0.14981627464294434
Batch 19/64 loss: -0.1418701410293579
Batch 20/64 loss: -0.15367227792739868
Batch 21/64 loss: -0.15939825773239136
Batch 22/64 loss: -0.17477959394454956
Batch 23/64 loss: -0.14550328254699707
Batch 24/64 loss: -0.15237385034561157
Batch 25/64 loss: -0.13147151470184326
Batch 26/64 loss: -0.14561694860458374
Batch 27/64 loss: -0.15067654848098755
Batch 28/64 loss: -0.12606489658355713
Batch 29/64 loss: -0.1221092939376831
Batch 30/64 loss: -0.16028434038162231
Batch 31/64 loss: -0.14267849922180176
Batch 32/64 loss: -0.15916889905929565
Batch 33/64 loss: -0.15579617023468018
Batch 34/64 loss: -0.13641923666000366
Batch 35/64 loss: -0.1686001420021057
Batch 36/64 loss: -0.14932852983474731
Batch 37/64 loss: -0.12982863187789917
Batch 38/64 loss: -0.1331576108932495
Batch 39/64 loss: -0.13994789123535156
Batch 40/64 loss: -0.15334463119506836
Batch 41/64 loss: -0.14622944593429565
Batch 42/64 loss: -0.16193148493766785
Batch 43/64 loss: -0.14022648334503174
Batch 44/64 loss: -0.1637117564678192
Batch 45/64 loss: -0.13827413320541382
Batch 46/64 loss: -0.13390934467315674
Batch 47/64 loss: -0.13589537143707275
Batch 48/64 loss: -0.15680652856826782
Batch 49/64 loss: -0.16166284680366516
Batch 50/64 loss: -0.14348608255386353
Batch 51/64 loss: -0.152978777885437
Batch 52/64 loss: -0.12643706798553467
Batch 53/64 loss: -0.15032851696014404
Batch 54/64 loss: -0.1441327929496765
Batch 55/64 loss: -0.11859303712844849
Batch 56/64 loss: -0.156019389629364
Batch 57/64 loss: -0.13629555702209473
Batch 58/64 loss: -0.16278967261314392
Batch 59/64 loss: -0.16428858041763306
Batch 60/64 loss: -0.1364736557006836
Batch 61/64 loss: -0.15956056118011475
Batch 62/64 loss: -0.15499281883239746
Batch 63/64 loss: -0.17396777868270874
Batch 64/64 loss: -0.18631362915039062
Epoch 460  Train loss: -0.14921435374839634  Val loss: 0.055323165716584195
Epoch 461
-------------------------------
Batch 1/64 loss: -0.16055792570114136
Batch 2/64 loss: -0.17091739177703857
Batch 3/64 loss: -0.1571175456047058
Batch 4/64 loss: -0.13922369480133057
Batch 5/64 loss: -0.14488625526428223
Batch 6/64 loss: -0.1590113639831543
Batch 7/64 loss: -0.14206063747406006
Batch 8/64 loss: -0.13048481941223145
Batch 9/64 loss: -0.14883768558502197
Batch 10/64 loss: -0.17056941986083984
Batch 11/64 loss: -0.11276501417160034
Batch 12/64 loss: -0.14887166023254395
Batch 13/64 loss: -0.1576385796070099
Batch 14/64 loss: -0.16423392295837402
Batch 15/64 loss: -0.13052773475646973
Batch 16/64 loss: -0.1437070369720459
Batch 17/64 loss: -0.1373346447944641
Batch 18/64 loss: -0.15141475200653076
Batch 19/64 loss: -0.14444929361343384
Batch 20/64 loss: -0.15814673900604248
Batch 21/64 loss: -0.17335230112075806
Batch 22/64 loss: -0.17828965187072754
Batch 23/64 loss: -0.14973324537277222
Batch 24/64 loss: -0.1611027717590332
Batch 25/64 loss: -0.1642029583454132
Batch 26/64 loss: -0.12981301546096802
Batch 27/64 loss: -0.17584803700447083
Batch 28/64 loss: -0.1725907027721405
Batch 29/64 loss: -0.1365155577659607
Batch 30/64 loss: -0.16116994619369507
Batch 31/64 loss: -0.16911455988883972
Batch 32/64 loss: -0.13917195796966553
Batch 33/64 loss: -0.13727933168411255
Batch 34/64 loss: -0.17391574382781982
Batch 35/64 loss: -0.15319013595581055
Batch 36/64 loss: -0.1445940136909485
Batch 37/64 loss: -0.15542733669281006
Batch 38/64 loss: -0.18014216423034668
Batch 39/64 loss: -0.13588488101959229
Batch 40/64 loss: -0.14790654182434082
Batch 41/64 loss: -0.16771292686462402
Batch 42/64 loss: -0.14446419477462769
Batch 43/64 loss: -0.16728070378303528
Batch 44/64 loss: -0.1669352650642395
Batch 45/64 loss: -0.12307798862457275
Batch 46/64 loss: -0.1415235996246338
Batch 47/64 loss: -0.14520835876464844
Batch 48/64 loss: -0.140261709690094
Batch 49/64 loss: -0.17135435342788696
Batch 50/64 loss: -0.132307767868042
Batch 51/64 loss: -0.14071905612945557
Batch 52/64 loss: -0.15520066022872925
Batch 53/64 loss: -0.14805012941360474
Batch 54/64 loss: -0.16696280241012573
Batch 55/64 loss: -0.1106259822845459
Batch 56/64 loss: -0.16646656394004822
Batch 57/64 loss: -0.17302697896957397
Batch 58/64 loss: -0.09261685609817505
Batch 59/64 loss: -0.16088378429412842
Batch 60/64 loss: -0.1378239393234253
Batch 61/64 loss: -0.14262735843658447
Batch 62/64 loss: -0.15904653072357178
Batch 63/64 loss: -0.1390036940574646
Batch 64/64 loss: -0.11791771650314331
Epoch 461  Train loss: -0.1504881641444038  Val loss: 0.05338184223142276
Epoch 462
-------------------------------
Batch 1/64 loss: -0.16764888167381287
Batch 2/64 loss: -0.14935684204101562
Batch 3/64 loss: -0.1511020064353943
Batch 4/64 loss: -0.158424973487854
Batch 5/64 loss: -0.11318033933639526
Batch 6/64 loss: -0.149370014667511
Batch 7/64 loss: -0.12924045324325562
Batch 8/64 loss: -0.13807696104049683
Batch 9/64 loss: -0.14341944456100464
Batch 10/64 loss: -0.17089951038360596
Batch 11/64 loss: -0.14569896459579468
Batch 12/64 loss: -0.1537814736366272
Batch 13/64 loss: -0.1457153558731079
Batch 14/64 loss: -0.14292407035827637
Batch 15/64 loss: -0.1482904553413391
Batch 16/64 loss: -0.16022950410842896
Batch 17/64 loss: -0.1629621684551239
Batch 18/64 loss: -0.17488950490951538
Batch 19/64 loss: -0.16441741585731506
Batch 20/64 loss: -0.1738094687461853
Batch 21/64 loss: -0.1370297074317932
Batch 22/64 loss: -0.12607920169830322
Batch 23/64 loss: -0.15446394681930542
Batch 24/64 loss: -0.11767035722732544
Batch 25/64 loss: -0.17066481709480286
Batch 26/64 loss: -0.17929673194885254
Batch 27/64 loss: -0.14417803287506104
Batch 28/64 loss: -0.15453171730041504
Batch 29/64 loss: -0.17736583948135376
Batch 30/64 loss: -0.1370047926902771
Batch 31/64 loss: -0.15433895587921143
Batch 32/64 loss: -0.13530349731445312
Batch 33/64 loss: -0.14402145147323608
Batch 34/64 loss: -0.13304495811462402
Batch 35/64 loss: -0.14936411380767822
Batch 36/64 loss: -0.14796268939971924
Batch 37/64 loss: -0.1394709348678589
Batch 38/64 loss: -0.18992489576339722
Batch 39/64 loss: -0.17310839891433716
Batch 40/64 loss: -0.11400395631790161
Batch 41/64 loss: -0.1535819172859192
Batch 42/64 loss: -0.16492819786071777
Batch 43/64 loss: -0.15537846088409424
Batch 44/64 loss: -0.1692749261856079
Batch 45/64 loss: -0.17281979322433472
Batch 46/64 loss: -0.15821301937103271
Batch 47/64 loss: -0.16132602095603943
Batch 48/64 loss: -0.13288092613220215
Batch 49/64 loss: -0.14470189809799194
Batch 50/64 loss: -0.16633349657058716
Batch 51/64 loss: -0.14326822757720947
Batch 52/64 loss: -0.14826065301895142
Batch 53/64 loss: -0.15386658906936646
Batch 54/64 loss: -0.1504960060119629
Batch 55/64 loss: -0.16013744473457336
Batch 56/64 loss: -0.14224135875701904
Batch 57/64 loss: -0.1575447916984558
Batch 58/64 loss: -0.1261836290359497
Batch 59/64 loss: -0.11813026666641235
Batch 60/64 loss: -0.14347147941589355
Batch 61/64 loss: -0.1514434814453125
Batch 62/64 loss: -0.12573844194412231
Batch 63/64 loss: -0.13951390981674194
Batch 64/64 loss: -0.11153006553649902
Epoch 462  Train loss: -0.1497356751385857  Val loss: 0.05585586615034805
Epoch 463
-------------------------------
Batch 1/64 loss: -0.16822102665901184
Batch 2/64 loss: -0.12508797645568848
Batch 3/64 loss: -0.1653069257736206
Batch 4/64 loss: -0.1579807996749878
Batch 5/64 loss: -0.1678670048713684
Batch 6/64 loss: -0.15034431219100952
Batch 7/64 loss: -0.16261249780654907
Batch 8/64 loss: -0.15873074531555176
Batch 9/64 loss: -0.14940249919891357
Batch 10/64 loss: -0.14938491582870483
Batch 11/64 loss: -0.14449286460876465
Batch 12/64 loss: -0.14983177185058594
Batch 13/64 loss: -0.15576642751693726
Batch 14/64 loss: -0.15499258041381836
Batch 15/64 loss: -0.1583367884159088
Batch 16/64 loss: -0.1304311752319336
Batch 17/64 loss: -0.14141839742660522
Batch 18/64 loss: -0.14519786834716797
Batch 19/64 loss: -0.12826186418533325
Batch 20/64 loss: -0.1669456660747528
Batch 21/64 loss: -0.1695062518119812
Batch 22/64 loss: -0.13690412044525146
Batch 23/64 loss: -0.15411710739135742
Batch 24/64 loss: -0.1591464877128601
Batch 25/64 loss: -0.14078229665756226
Batch 26/64 loss: -0.17026901245117188
Batch 27/64 loss: -0.14339947700500488
Batch 28/64 loss: -0.1410912275314331
Batch 29/64 loss: -0.14546972513198853
Batch 30/64 loss: -0.12439906597137451
Batch 31/64 loss: -0.16264456510543823
Batch 32/64 loss: -0.15941262245178223
Batch 33/64 loss: -0.14610964059829712
Batch 34/64 loss: -0.13331174850463867
Batch 35/64 loss: -0.12052726745605469
Batch 36/64 loss: -0.13800591230392456
Batch 37/64 loss: -0.14865553379058838
Batch 38/64 loss: -0.1419999599456787
Batch 39/64 loss: -0.167003333568573
Batch 40/64 loss: -0.14634084701538086
Batch 41/64 loss: -0.16656914353370667
Batch 42/64 loss: -0.09490841627120972
Batch 43/64 loss: -0.16107818484306335
Batch 44/64 loss: -0.1529560089111328
Batch 45/64 loss: -0.12526357173919678
Batch 46/64 loss: -0.14806616306304932
Batch 47/64 loss: -0.15892404317855835
Batch 48/64 loss: -0.14387238025665283
Batch 49/64 loss: -0.15582358837127686
Batch 50/64 loss: -0.13096332550048828
Batch 51/64 loss: -0.1564585566520691
Batch 52/64 loss: -0.14019978046417236
Batch 53/64 loss: -0.14407211542129517
Batch 54/64 loss: -0.1445583701133728
Batch 55/64 loss: -0.14404666423797607
Batch 56/64 loss: -0.14976012706756592
Batch 57/64 loss: -0.1647861897945404
Batch 58/64 loss: -0.12860321998596191
Batch 59/64 loss: -0.1484394669532776
Batch 60/64 loss: -0.12747794389724731
Batch 61/64 loss: -0.1444762945175171
Batch 62/64 loss: -0.12953627109527588
Batch 63/64 loss: -0.14867740869522095
Batch 64/64 loss: -0.14944136142730713
Epoch 463  Train loss: -0.14747150692285277  Val loss: 0.0577215947646046
Epoch 464
-------------------------------
Batch 1/64 loss: -0.14562857151031494
Batch 2/64 loss: -0.16635897755622864
Batch 3/64 loss: -0.13630962371826172
Batch 4/64 loss: -0.14994597434997559
Batch 5/64 loss: -0.13028627634048462
Batch 6/64 loss: -0.13680607080459595
Batch 7/64 loss: -0.14084208011627197
Batch 8/64 loss: -0.14159315824508667
Batch 9/64 loss: -0.15764454007148743
Batch 10/64 loss: -0.15367621183395386
Batch 11/64 loss: -0.14448070526123047
Batch 12/64 loss: -0.1799524426460266
Batch 13/64 loss: -0.1004171371459961
Batch 14/64 loss: -0.14877468347549438
Batch 15/64 loss: -0.14129877090454102
Batch 16/64 loss: -0.12990128993988037
Batch 17/64 loss: -0.1535390019416809
Batch 18/64 loss: -0.17263031005859375
Batch 19/64 loss: -0.16562020778656006
Batch 20/64 loss: -0.15240395069122314
Batch 21/64 loss: -0.1550883650779724
Batch 22/64 loss: -0.14389348030090332
Batch 23/64 loss: -0.17743203043937683
Batch 24/64 loss: -0.1670677661895752
Batch 25/64 loss: -0.1418655514717102
Batch 26/64 loss: -0.14334166049957275
Batch 27/64 loss: -0.13641667366027832
Batch 28/64 loss: -0.14105522632598877
Batch 29/64 loss: -0.1673215627670288
Batch 30/64 loss: -0.12826913595199585
Batch 31/64 loss: -0.12239587306976318
Batch 32/64 loss: -0.13386327028274536
Batch 33/64 loss: -0.13702315092086792
Batch 34/64 loss: -0.16911423206329346
Batch 35/64 loss: -0.11548739671707153
Batch 36/64 loss: -0.15850728750228882
Batch 37/64 loss: -0.16327756643295288
Batch 38/64 loss: -0.17218869924545288
Batch 39/64 loss: -0.1562621295452118
Batch 40/64 loss: -0.13076263666152954
Batch 41/64 loss: -0.18027812242507935
Batch 42/64 loss: -0.14131683111190796
Batch 43/64 loss: -0.13337361812591553
Batch 44/64 loss: -0.17320406436920166
Batch 45/64 loss: -0.1614099144935608
Batch 46/64 loss: -0.16363811492919922
Batch 47/64 loss: -0.14831221103668213
Batch 48/64 loss: -0.11251091957092285
Batch 49/64 loss: -0.15927934646606445
Batch 50/64 loss: -0.15309852361679077
Batch 51/64 loss: -0.16249412298202515
Batch 52/64 loss: -0.1433863639831543
Batch 53/64 loss: -0.15892791748046875
Batch 54/64 loss: -0.15590345859527588
Batch 55/64 loss: -0.12189614772796631
Batch 56/64 loss: -0.1772361397743225
Batch 57/64 loss: -0.14366304874420166
Batch 58/64 loss: -0.12232339382171631
Batch 59/64 loss: -0.13973462581634521
Batch 60/64 loss: -0.1607799530029297
Batch 61/64 loss: -0.16381728649139404
Batch 62/64 loss: -0.17694956064224243
Batch 63/64 loss: -0.12386882305145264
Batch 64/64 loss: -0.14499378204345703
Epoch 464  Train loss: -0.1489394748912138  Val loss: 0.05616769778359797
Epoch 465
-------------------------------
Batch 1/64 loss: -0.19728755950927734
Batch 2/64 loss: -0.1416546106338501
Batch 3/64 loss: -0.16702452301979065
Batch 4/64 loss: -0.1644538938999176
Batch 5/64 loss: -0.14901185035705566
Batch 6/64 loss: -0.18325701355934143
Batch 7/64 loss: -0.13979500532150269
Batch 8/64 loss: -0.16156333684921265
Batch 9/64 loss: -0.1575489044189453
Batch 10/64 loss: -0.17177224159240723
Batch 11/64 loss: -0.1826305389404297
Batch 12/64 loss: -0.17006796598434448
Batch 13/64 loss: -0.16164904832839966
Batch 14/64 loss: -0.13339018821716309
Batch 15/64 loss: -0.1651061773300171
Batch 16/64 loss: -0.16700586676597595
Batch 17/64 loss: -0.14458179473876953
Batch 18/64 loss: -0.12835389375686646
Batch 19/64 loss: -0.13643085956573486
Batch 20/64 loss: -0.12961238622665405
Batch 21/64 loss: -0.14465147256851196
Batch 22/64 loss: -0.15117061138153076
Batch 23/64 loss: -0.13112026453018188
Batch 24/64 loss: -0.16794481873512268
Batch 25/64 loss: -0.11705631017684937
Batch 26/64 loss: -0.13064968585968018
Batch 27/64 loss: -0.17207598686218262
Batch 28/64 loss: -0.17851942777633667
Batch 29/64 loss: -0.12895232439041138
Batch 30/64 loss: -0.14485418796539307
Batch 31/64 loss: -0.1156817078590393
Batch 32/64 loss: -0.1557549238204956
Batch 33/64 loss: -0.15700334310531616
Batch 34/64 loss: -0.14631325006484985
Batch 35/64 loss: -0.17544019222259521
Batch 36/64 loss: -0.1382538080215454
Batch 37/64 loss: -0.14491873979568481
Batch 38/64 loss: -0.15417033433914185
Batch 39/64 loss: -0.1293969750404358
Batch 40/64 loss: -0.1351841688156128
Batch 41/64 loss: -0.13493555784225464
Batch 42/64 loss: -0.1548582911491394
Batch 43/64 loss: -0.18981796503067017
Batch 44/64 loss: -0.14647924900054932
Batch 45/64 loss: -0.17083534598350525
Batch 46/64 loss: -0.1290651559829712
Batch 47/64 loss: -0.12567919492721558
Batch 48/64 loss: -0.15250420570373535
Batch 49/64 loss: -0.1531296968460083
Batch 50/64 loss: -0.12161386013031006
Batch 51/64 loss: -0.12397253513336182
Batch 52/64 loss: -0.19000419974327087
Batch 53/64 loss: -0.16674327850341797
Batch 54/64 loss: -0.1683032512664795
Batch 55/64 loss: -0.15373021364212036
Batch 56/64 loss: -0.1478450894355774
Batch 57/64 loss: -0.17712551355361938
Batch 58/64 loss: -0.16831588745117188
Batch 59/64 loss: -0.09489458799362183
Batch 60/64 loss: -0.14734309911727905
Batch 61/64 loss: -0.15853098034858704
Batch 62/64 loss: -0.18092697858810425
Batch 63/64 loss: -0.1540086269378662
Batch 64/64 loss: -0.17401209473609924
Epoch 465  Train loss: -0.15235265925818797  Val loss: 0.05438475797266485
Epoch 466
-------------------------------
Batch 1/64 loss: -0.16602033376693726
Batch 2/64 loss: -0.16632509231567383
Batch 3/64 loss: -0.12191188335418701
Batch 4/64 loss: -0.17139491438865662
Batch 5/64 loss: -0.14524215459823608
Batch 6/64 loss: -0.14115118980407715
Batch 7/64 loss: -0.12991023063659668
Batch 8/64 loss: -0.146612286567688
Batch 9/64 loss: -0.18494540452957153
Batch 10/64 loss: -0.16613301634788513
Batch 11/64 loss: -0.1565219759941101
Batch 12/64 loss: -0.15475225448608398
Batch 13/64 loss: -0.1721366047859192
Batch 14/64 loss: -0.14330142736434937
Batch 15/64 loss: -0.15501779317855835
Batch 16/64 loss: -0.15119415521621704
Batch 17/64 loss: -0.16507264971733093
Batch 18/64 loss: -0.1343662142753601
Batch 19/64 loss: -0.1541045904159546
Batch 20/64 loss: -0.1757867932319641
Batch 21/64 loss: -0.15098071098327637
Batch 22/64 loss: -0.13903117179870605
Batch 23/64 loss: -0.13579756021499634
Batch 24/64 loss: -0.16444027423858643
Batch 25/64 loss: -0.10578250885009766
Batch 26/64 loss: -0.15802031755447388
Batch 27/64 loss: -0.1374143362045288
Batch 28/64 loss: -0.13435113430023193
Batch 29/64 loss: -0.12097054719924927
Batch 30/64 loss: -0.1560516357421875
Batch 31/64 loss: -0.15443837642669678
Batch 32/64 loss: -0.1664133369922638
Batch 33/64 loss: -0.15559858083724976
Batch 34/64 loss: -0.15391623973846436
Batch 35/64 loss: -0.16127628087997437
Batch 36/64 loss: -0.14734208583831787
Batch 37/64 loss: -0.1340315341949463
Batch 38/64 loss: -0.14750128984451294
Batch 39/64 loss: -0.15382957458496094
Batch 40/64 loss: -0.18158257007598877
Batch 41/64 loss: -0.17559099197387695
Batch 42/64 loss: -0.14133769273757935
Batch 43/64 loss: -0.16437023878097534
Batch 44/64 loss: -0.10114675760269165
Batch 45/64 loss: -0.16873222589492798
Batch 46/64 loss: -0.12277871370315552
Batch 47/64 loss: -0.17209991812705994
Batch 48/64 loss: -0.1674688756465912
Batch 49/64 loss: -0.15632885694503784
Batch 50/64 loss: -0.16844207048416138
Batch 51/64 loss: -0.13551324605941772
Batch 52/64 loss: -0.17893999814987183
Batch 53/64 loss: -0.14165186882019043
Batch 54/64 loss: -0.12828081846237183
Batch 55/64 loss: -0.14787423610687256
Batch 56/64 loss: -0.10347312688827515
Batch 57/64 loss: -0.1499173641204834
Batch 58/64 loss: -0.15517902374267578
Batch 59/64 loss: -0.171519935131073
Batch 60/64 loss: -0.1472078561782837
Batch 61/64 loss: -0.14277362823486328
Batch 62/64 loss: -0.14372169971466064
Batch 63/64 loss: -0.1751251220703125
Batch 64/64 loss: -0.15557613968849182
Epoch 466  Train loss: -0.15116592002849952  Val loss: 0.05686062246663464
Epoch 467
-------------------------------
Batch 1/64 loss: -0.17892134189605713
Batch 2/64 loss: -0.16518592834472656
Batch 3/64 loss: -0.18001699447631836
Batch 4/64 loss: -0.1500382423400879
Batch 5/64 loss: -0.1630367636680603
Batch 6/64 loss: -0.17066198587417603
Batch 7/64 loss: -0.17757615447044373
Batch 8/64 loss: -0.15452253818511963
Batch 9/64 loss: -0.152515709400177
Batch 10/64 loss: -0.1697644591331482
Batch 11/64 loss: -0.14180082082748413
Batch 12/64 loss: -0.14030146598815918
Batch 13/64 loss: -0.14265024662017822
Batch 14/64 loss: -0.1252008080482483
Batch 15/64 loss: -0.1627441644668579
Batch 16/64 loss: -0.13558220863342285
Batch 17/64 loss: -0.154585599899292
Batch 18/64 loss: -0.16835254430770874
Batch 19/64 loss: -0.17698603868484497
Batch 20/64 loss: -0.16471019387245178
Batch 21/64 loss: -0.1496703028678894
Batch 22/64 loss: -0.15376198291778564
Batch 23/64 loss: -0.14613032341003418
Batch 24/64 loss: -0.12575560808181763
Batch 25/64 loss: -0.13666033744812012
Batch 26/64 loss: -0.15482425689697266
Batch 27/64 loss: -0.14091473817825317
Batch 28/64 loss: -0.1485036015510559
Batch 29/64 loss: -0.14513516426086426
Batch 30/64 loss: -0.1561574935913086
Batch 31/64 loss: -0.14217287302017212
Batch 32/64 loss: -0.16774120926856995
Batch 33/64 loss: -0.13110119104385376
Batch 34/64 loss: -0.16536802053451538
Batch 35/64 loss: -0.16206032037734985
Batch 36/64 loss: -0.1811211109161377
Batch 37/64 loss: -0.14570868015289307
Batch 38/64 loss: -0.14474916458129883
Batch 39/64 loss: -0.14359194040298462
Batch 40/64 loss: -0.16547656059265137
Batch 41/64 loss: -0.15776902437210083
Batch 42/64 loss: -0.13131463527679443
Batch 43/64 loss: -0.15266335010528564
Batch 44/64 loss: -0.17129164934158325
Batch 45/64 loss: -0.10221993923187256
Batch 46/64 loss: -0.16499745845794678
Batch 47/64 loss: -0.18314552307128906
Batch 48/64 loss: -0.17268848419189453
Batch 49/64 loss: -0.15816187858581543
Batch 50/64 loss: -0.11906403303146362
Batch 51/64 loss: -0.13482749462127686
Batch 52/64 loss: -0.17362314462661743
Batch 53/64 loss: -0.14566320180892944
Batch 54/64 loss: -0.1485583782196045
Batch 55/64 loss: -0.13554948568344116
Batch 56/64 loss: -0.1629745066165924
Batch 57/64 loss: -0.1355486512184143
Batch 58/64 loss: -0.15665504336357117
Batch 59/64 loss: -0.15627890825271606
Batch 60/64 loss: -0.14157676696777344
Batch 61/64 loss: -0.1381780505180359
Batch 62/64 loss: -0.1347724199295044
Batch 63/64 loss: -0.1616952121257782
Batch 64/64 loss: -0.14354068040847778
Epoch 467  Train loss: -0.15260598963382196  Val loss: 0.05957147571229443
Epoch 468
-------------------------------
Batch 1/64 loss: -0.15093082189559937
Batch 2/64 loss: -0.15206396579742432
Batch 3/64 loss: -0.149422287940979
Batch 4/64 loss: -0.14073282480239868
Batch 5/64 loss: -0.12442547082901001
Batch 6/64 loss: -0.18564432859420776
Batch 7/64 loss: -0.15176421403884888
Batch 8/64 loss: -0.15937206149101257
Batch 9/64 loss: -0.1688779592514038
Batch 10/64 loss: -0.126001238822937
Batch 11/64 loss: -0.16449692845344543
Batch 12/64 loss: -0.13809430599212646
Batch 13/64 loss: -0.14877039194107056
Batch 14/64 loss: -0.15336930751800537
Batch 15/64 loss: -0.16959941387176514
Batch 16/64 loss: -0.16140025854110718
Batch 17/64 loss: -0.17510396242141724
Batch 18/64 loss: -0.13147950172424316
Batch 19/64 loss: -0.11732035875320435
Batch 20/64 loss: -0.13430416584014893
Batch 21/64 loss: -0.14650678634643555
Batch 22/64 loss: -0.14504843950271606
Batch 23/64 loss: -0.1676584780216217
Batch 24/64 loss: -0.1475151777267456
Batch 25/64 loss: -0.14761531352996826
Batch 26/64 loss: -0.14876139163970947
Batch 27/64 loss: -0.184116393327713
Batch 28/64 loss: -0.12620031833648682
Batch 29/64 loss: -0.1320279836654663
Batch 30/64 loss: -0.14029806852340698
Batch 31/64 loss: -0.17181316018104553
Batch 32/64 loss: -0.13203275203704834
Batch 33/64 loss: -0.145033597946167
Batch 34/64 loss: -0.13714677095413208
Batch 35/64 loss: -0.14426887035369873
Batch 36/64 loss: -0.15286248922348022
Batch 37/64 loss: -0.1411372423171997
Batch 38/64 loss: -0.16883444786071777
Batch 39/64 loss: -0.1457849144935608
Batch 40/64 loss: -0.111533522605896
Batch 41/64 loss: -0.174233078956604
Batch 42/64 loss: -0.1570746898651123
Batch 43/64 loss: -0.17169919610023499
Batch 44/64 loss: -0.14715039730072021
Batch 45/64 loss: -0.14962726831436157
Batch 46/64 loss: -0.15392357110977173
Batch 47/64 loss: -0.14022845029830933
Batch 48/64 loss: -0.14389663934707642
Batch 49/64 loss: -0.15099048614501953
Batch 50/64 loss: -0.15136390924453735
Batch 51/64 loss: -0.15209650993347168
Batch 52/64 loss: -0.15478521585464478
Batch 53/64 loss: -0.16351598501205444
Batch 54/64 loss: -0.14939218759536743
Batch 55/64 loss: -0.14659982919692993
Batch 56/64 loss: -0.18506652116775513
Batch 57/64 loss: -0.1606438159942627
Batch 58/64 loss: -0.1768818497657776
Batch 59/64 loss: -0.1703810691833496
Batch 60/64 loss: -0.1164141297340393
Batch 61/64 loss: -0.15357285737991333
Batch 62/64 loss: -0.14356553554534912
Batch 63/64 loss: -0.14350885152816772
Batch 64/64 loss: -0.1749095618724823
Epoch 468  Train loss: -0.15101433887201196  Val loss: 0.05373458026610699
Epoch 469
-------------------------------
Batch 1/64 loss: -0.16762644052505493
Batch 2/64 loss: -0.1507253646850586
Batch 3/64 loss: -0.17051050066947937
Batch 4/64 loss: -0.19109010696411133
Batch 5/64 loss: -0.1617182493209839
Batch 6/64 loss: -0.16621661186218262
Batch 7/64 loss: -0.1891055703163147
Batch 8/64 loss: -0.1335599422454834
Batch 9/64 loss: -0.1439875364303589
Batch 10/64 loss: -0.1657637357711792
Batch 11/64 loss: -0.1606547236442566
Batch 12/64 loss: -0.12416601181030273
Batch 13/64 loss: -0.1117103099822998
Batch 14/64 loss: -0.14996600151062012
Batch 15/64 loss: -0.1548776626586914
Batch 16/64 loss: -0.13431507349014282
Batch 17/64 loss: -0.12967216968536377
Batch 18/64 loss: -0.16537410020828247
Batch 19/64 loss: -0.1720578372478485
Batch 20/64 loss: -0.14186906814575195
Batch 21/64 loss: -0.12195849418640137
Batch 22/64 loss: -0.14124947786331177
Batch 23/64 loss: -0.1727432906627655
Batch 24/64 loss: -0.16575384140014648
Batch 25/64 loss: -0.17143124341964722
Batch 26/64 loss: -0.1402515172958374
Batch 27/64 loss: -0.1487206220626831
Batch 28/64 loss: -0.15335708856582642
Batch 29/64 loss: -0.17776399850845337
Batch 30/64 loss: -0.1226968765258789
Batch 31/64 loss: -0.11855518817901611
Batch 32/64 loss: -0.15473335981369019
Batch 33/64 loss: -0.14966237545013428
Batch 34/64 loss: -0.15347468852996826
Batch 35/64 loss: -0.15042704343795776
Batch 36/64 loss: -0.16453459858894348
Batch 37/64 loss: -0.16186121106147766
Batch 38/64 loss: -0.16234609484672546
Batch 39/64 loss: -0.15817493200302124
Batch 40/64 loss: -0.1605042815208435
Batch 41/64 loss: -0.16005754470825195
Batch 42/64 loss: -0.1646578311920166
Batch 43/64 loss: -0.16670677065849304
Batch 44/64 loss: -0.1364912986755371
Batch 45/64 loss: -0.14422976970672607
Batch 46/64 loss: -0.15241330862045288
Batch 47/64 loss: -0.145185649394989
Batch 48/64 loss: -0.1552487015724182
Batch 49/64 loss: -0.10603487491607666
Batch 50/64 loss: -0.14345955848693848
Batch 51/64 loss: -0.12933415174484253
Batch 52/64 loss: -0.13679879903793335
Batch 53/64 loss: -0.1606227159500122
Batch 54/64 loss: -0.1699155867099762
Batch 55/64 loss: -0.161188542842865
Batch 56/64 loss: -0.13575422763824463
Batch 57/64 loss: -0.15642988681793213
Batch 58/64 loss: -0.13709330558776855
Batch 59/64 loss: -0.15844875574111938
Batch 60/64 loss: -0.12709510326385498
Batch 61/64 loss: -0.13358330726623535
Batch 62/64 loss: -0.12176579236984253
Batch 63/64 loss: -0.16641348600387573
Batch 64/64 loss: -0.1486644744873047
Epoch 469  Train loss: -0.15083285593519025  Val loss: 0.056989092187783155
Epoch 470
-------------------------------
Batch 1/64 loss: -0.13545644283294678
Batch 2/64 loss: -0.1256694197654724
Batch 3/64 loss: -0.14889782667160034
Batch 4/64 loss: -0.1770499348640442
Batch 5/64 loss: -0.13270944356918335
Batch 6/64 loss: -0.15196073055267334
Batch 7/64 loss: -0.18092691898345947
Batch 8/64 loss: -0.14614880084991455
Batch 9/64 loss: -0.12838083505630493
Batch 10/64 loss: -0.1502472162246704
Batch 11/64 loss: -0.11121261119842529
Batch 12/64 loss: -0.16454747319221497
Batch 13/64 loss: -0.17004114389419556
Batch 14/64 loss: -0.15138059854507446
Batch 15/64 loss: -0.1553206443786621
Batch 16/64 loss: -0.13236212730407715
Batch 17/64 loss: -0.17558184266090393
Batch 18/64 loss: -0.14466404914855957
Batch 19/64 loss: -0.14726626873016357
Batch 20/64 loss: -0.16258785128593445
Batch 21/64 loss: -0.16402623057365417
Batch 22/64 loss: -0.12820053100585938
Batch 23/64 loss: -0.1586381196975708
Batch 24/64 loss: -0.13958168029785156
Batch 25/64 loss: -0.15041106939315796
Batch 26/64 loss: -0.1676531732082367
Batch 27/64 loss: -0.16926264762878418
Batch 28/64 loss: -0.1634744107723236
Batch 29/64 loss: -0.1801193356513977
Batch 30/64 loss: -0.16322696208953857
Batch 31/64 loss: -0.16312965750694275
Batch 32/64 loss: -0.16832932829856873
Batch 33/64 loss: -0.1714639663696289
Batch 34/64 loss: -0.11824733018875122
Batch 35/64 loss: -0.16758155822753906
Batch 36/64 loss: -0.1291303038597107
Batch 37/64 loss: -0.1472463607788086
Batch 38/64 loss: -0.14029395580291748
Batch 39/64 loss: -0.1715230941772461
Batch 40/64 loss: -0.16172656416893005
Batch 41/64 loss: -0.1423126459121704
Batch 42/64 loss: -0.14764642715454102
Batch 43/64 loss: -0.15884006023406982
Batch 44/64 loss: -0.18380576372146606
Batch 45/64 loss: -0.17242127656936646
Batch 46/64 loss: -0.17083650827407837
Batch 47/64 loss: -0.1582370102405548
Batch 48/64 loss: -0.13451159000396729
Batch 49/64 loss: -0.13582324981689453
Batch 50/64 loss: -0.17661964893341064
Batch 51/64 loss: -0.14798569679260254
Batch 52/64 loss: -0.12439054250717163
Batch 53/64 loss: -0.15231812000274658
Batch 54/64 loss: -0.1355004906654358
Batch 55/64 loss: -0.13556748628616333
Batch 56/64 loss: -0.1675085425376892
Batch 57/64 loss: -0.14558839797973633
Batch 58/64 loss: -0.15345853567123413
Batch 59/64 loss: -0.1568315029144287
Batch 60/64 loss: -0.15984606742858887
Batch 61/64 loss: -0.10972368717193604
Batch 62/64 loss: -0.11826008558273315
Batch 63/64 loss: -0.15323138236999512
Batch 64/64 loss: -0.1638902723789215
Epoch 470  Train loss: -0.1518409550189972  Val loss: 0.05672421078501698
Epoch 471
-------------------------------
Batch 1/64 loss: -0.19001895189285278
Batch 2/64 loss: -0.1607246994972229
Batch 3/64 loss: -0.11462122201919556
Batch 4/64 loss: -0.14037179946899414
Batch 5/64 loss: -0.15168756246566772
Batch 6/64 loss: -0.16854643821716309
Batch 7/64 loss: -0.1783846914768219
Batch 8/64 loss: -0.16697004437446594
Batch 9/64 loss: -0.14622282981872559
Batch 10/64 loss: -0.1578150987625122
Batch 11/64 loss: -0.18221968412399292
Batch 12/64 loss: -0.188859224319458
Batch 13/64 loss: -0.13304924964904785
Batch 14/64 loss: -0.16218945384025574
Batch 15/64 loss: -0.14025580883026123
Batch 16/64 loss: -0.17888140678405762
Batch 17/64 loss: -0.14063012599945068
Batch 18/64 loss: -0.14557838439941406
Batch 19/64 loss: -0.14611858129501343
Batch 20/64 loss: -0.14157575368881226
Batch 21/64 loss: -0.1716342568397522
Batch 22/64 loss: -0.16695821285247803
Batch 23/64 loss: -0.13954603672027588
Batch 24/64 loss: -0.15213543176651
Batch 25/64 loss: -0.15632325410842896
Batch 26/64 loss: -0.13652241230010986
Batch 27/64 loss: -0.1346239447593689
Batch 28/64 loss: -0.10892462730407715
Batch 29/64 loss: -0.14862513542175293
Batch 30/64 loss: -0.15927764773368835
Batch 31/64 loss: -0.18329399824142456
Batch 32/64 loss: -0.152579665184021
Batch 33/64 loss: -0.1378791332244873
Batch 34/64 loss: -0.17171505093574524
Batch 35/64 loss: -0.15137821435928345
Batch 36/64 loss: -0.15036576986312866
Batch 37/64 loss: -0.17252784967422485
Batch 38/64 loss: -0.12330472469329834
Batch 39/64 loss: -0.15764355659484863
Batch 40/64 loss: -0.16752231121063232
Batch 41/64 loss: -0.15074104070663452
Batch 42/64 loss: -0.14510977268218994
Batch 43/64 loss: -0.15939825773239136
Batch 44/64 loss: -0.1481897234916687
Batch 45/64 loss: -0.16051232814788818
Batch 46/64 loss: -0.16663259267807007
Batch 47/64 loss: -0.1598759889602661
Batch 48/64 loss: -0.10808157920837402
Batch 49/64 loss: -0.12170791625976562
Batch 50/64 loss: -0.15198040008544922
Batch 51/64 loss: -0.1585378646850586
Batch 52/64 loss: -0.13634705543518066
Batch 53/64 loss: -0.16144278645515442
Batch 54/64 loss: -0.13956260681152344
Batch 55/64 loss: -0.15454483032226562
Batch 56/64 loss: -0.1254568099975586
Batch 57/64 loss: -0.10628604888916016
Batch 58/64 loss: -0.16239812970161438
Batch 59/64 loss: -0.17515531182289124
Batch 60/64 loss: -0.14934873580932617
Batch 61/64 loss: -0.13623535633087158
Batch 62/64 loss: -0.159162700176239
Batch 63/64 loss: -0.1459285020828247
Batch 64/64 loss: -0.08152371644973755
Epoch 471  Train loss: -0.1509220607140485  Val loss: 0.05539798019677913
Epoch 472
-------------------------------
Batch 1/64 loss: -0.1558157205581665
Batch 2/64 loss: -0.14813590049743652
Batch 3/64 loss: -0.13858908414840698
Batch 4/64 loss: -0.14902770519256592
Batch 5/64 loss: -0.14610052108764648
Batch 6/64 loss: -0.16682714223861694
Batch 7/64 loss: -0.11761271953582764
Batch 8/64 loss: -0.16754072904586792
Batch 9/64 loss: -0.1569528877735138
Batch 10/64 loss: -0.1389361023902893
Batch 11/64 loss: -0.17971491813659668
Batch 12/64 loss: -0.1614701747894287
Batch 13/64 loss: -0.15356463193893433
Batch 14/64 loss: -0.1470128297805786
Batch 15/64 loss: -0.14724105596542358
Batch 16/64 loss: -0.1612793505191803
Batch 17/64 loss: -0.13483476638793945
Batch 18/64 loss: -0.18234091997146606
Batch 19/64 loss: -0.14382821321487427
Batch 20/64 loss: -0.1433086395263672
Batch 21/64 loss: -0.15466046333312988
Batch 22/64 loss: -0.16262465715408325
Batch 23/64 loss: -0.14182895421981812
Batch 24/64 loss: -0.1549621820449829
Batch 25/64 loss: -0.15826022624969482
Batch 26/64 loss: -0.15828680992126465
Batch 27/64 loss: -0.16295427083969116
Batch 28/64 loss: -0.15584677457809448
Batch 29/64 loss: -0.1643136739730835
Batch 30/64 loss: -0.15727418661117554
Batch 31/64 loss: -0.1646905243396759
Batch 32/64 loss: -0.16750818490982056
Batch 33/64 loss: -0.16831710934638977
Batch 34/64 loss: -0.18054699897766113
Batch 35/64 loss: -0.1278490424156189
Batch 36/64 loss: -0.1408863067626953
Batch 37/64 loss: -0.15209120512008667
Batch 38/64 loss: -0.11714398860931396
Batch 39/64 loss: -0.15712958574295044
Batch 40/64 loss: -0.12001693248748779
Batch 41/64 loss: -0.16392892599105835
Batch 42/64 loss: -0.1579398512840271
Batch 43/64 loss: -0.1624402403831482
Batch 44/64 loss: -0.17432323098182678
Batch 45/64 loss: -0.15033096075057983
Batch 46/64 loss: -0.130201518535614
Batch 47/64 loss: -0.16854214668273926
Batch 48/64 loss: -0.16093257069587708
Batch 49/64 loss: -0.16691190004348755
Batch 50/64 loss: -0.16774499416351318
Batch 51/64 loss: -0.14449340105056763
Batch 52/64 loss: -0.13057589530944824
Batch 53/64 loss: -0.163270503282547
Batch 54/64 loss: -0.15439099073410034
Batch 55/64 loss: -0.13942182064056396
Batch 56/64 loss: -0.16945666074752808
Batch 57/64 loss: -0.1577778160572052
Batch 58/64 loss: -0.1642599105834961
Batch 59/64 loss: -0.16642355918884277
Batch 60/64 loss: -0.15529704093933105
Batch 61/64 loss: -0.13977497816085815
Batch 62/64 loss: -0.1639043092727661
Batch 63/64 loss: -0.1639363169670105
Batch 64/64 loss: -0.1717088520526886
Epoch 472  Train loss: -0.15454725132269018  Val loss: 0.05452949279772047
Epoch 473
-------------------------------
Batch 1/64 loss: -0.14642071723937988
Batch 2/64 loss: -0.15776538848876953
Batch 3/64 loss: -0.14647126197814941
Batch 4/64 loss: -0.18326592445373535
Batch 5/64 loss: -0.1558435559272766
Batch 6/64 loss: -0.1639903485774994
Batch 7/64 loss: -0.18757417798042297
Batch 8/64 loss: -0.16829311847686768
Batch 9/64 loss: -0.18693339824676514
Batch 10/64 loss: -0.14296191930770874
Batch 11/64 loss: -0.15648537874221802
Batch 12/64 loss: -0.15058684349060059
Batch 13/64 loss: -0.14957940578460693
Batch 14/64 loss: -0.1756611466407776
Batch 15/64 loss: -0.16928324103355408
Batch 16/64 loss: -0.1372828483581543
Batch 17/64 loss: -0.16400045156478882
Batch 18/64 loss: -0.14003980159759521
Batch 19/64 loss: -0.15853756666183472
Batch 20/64 loss: -0.16904205083847046
Batch 21/64 loss: -0.1387588381767273
Batch 22/64 loss: -0.13506537675857544
Batch 23/64 loss: -0.12802988290786743
Batch 24/64 loss: -0.16291654109954834
Batch 25/64 loss: -0.13551217317581177
Batch 26/64 loss: -0.16549965739250183
Batch 27/64 loss: -0.14345461130142212
Batch 28/64 loss: -0.15814208984375
Batch 29/64 loss: -0.14585286378860474
Batch 30/64 loss: -0.167880117893219
Batch 31/64 loss: -0.1340789794921875
Batch 32/64 loss: -0.13534963130950928
Batch 33/64 loss: -0.1353137493133545
Batch 34/64 loss: -0.17530089616775513
Batch 35/64 loss: -0.14651578664779663
Batch 36/64 loss: -0.167136549949646
Batch 37/64 loss: -0.16222485899925232
Batch 38/64 loss: -0.18046626448631287
Batch 39/64 loss: -0.13699984550476074
Batch 40/64 loss: -0.16931986808776855
Batch 41/64 loss: -0.12643444538116455
Batch 42/64 loss: -0.1578606367111206
Batch 43/64 loss: -0.14654803276062012
Batch 44/64 loss: -0.14400428533554077
Batch 45/64 loss: -0.13725465536117554
Batch 46/64 loss: -0.16032400727272034
Batch 47/64 loss: -0.14234507083892822
Batch 48/64 loss: -0.14583170413970947
Batch 49/64 loss: -0.17036113142967224
Batch 50/64 loss: -0.1563592553138733
Batch 51/64 loss: -0.17058467864990234
Batch 52/64 loss: -0.15790033340454102
Batch 53/64 loss: -0.1570999026298523
Batch 54/64 loss: -0.13286429643630981
Batch 55/64 loss: -0.15104365348815918
Batch 56/64 loss: -0.14059704542160034
Batch 57/64 loss: -0.14176028966903687
Batch 58/64 loss: -0.15600323677062988
Batch 59/64 loss: -0.1406090259552002
Batch 60/64 loss: -0.16669824719429016
Batch 61/64 loss: -0.1344611644744873
Batch 62/64 loss: -0.11809927225112915
Batch 63/64 loss: -0.15829205513000488
Batch 64/64 loss: -0.1682509183883667
Epoch 473  Train loss: -0.15330763517641555  Val loss: 0.05627503509783663
Epoch 474
-------------------------------
Batch 1/64 loss: -0.14672070741653442
Batch 2/64 loss: -0.17342126369476318
Batch 3/64 loss: -0.14997804164886475
Batch 4/64 loss: -0.16825321316719055
Batch 5/64 loss: -0.16277790069580078
Batch 6/64 loss: -0.15471011400222778
Batch 7/64 loss: -0.1789223849773407
Batch 8/64 loss: -0.17574122548103333
Batch 9/64 loss: -0.15794125199317932
Batch 10/64 loss: -0.15886712074279785
Batch 11/64 loss: -0.1455579400062561
Batch 12/64 loss: -0.17231622338294983
Batch 13/64 loss: -0.12126600742340088
Batch 14/64 loss: -0.12248003482818604
Batch 15/64 loss: -0.17552503943443298
Batch 16/64 loss: -0.15672951936721802
Batch 17/64 loss: -0.1699034571647644
Batch 18/64 loss: -0.17076101899147034
Batch 19/64 loss: -0.1773664653301239
Batch 20/64 loss: -0.1457614302635193
Batch 21/64 loss: -0.17157211899757385
Batch 22/64 loss: -0.15560591220855713
Batch 23/64 loss: -0.13601785898208618
Batch 24/64 loss: -0.16483882069587708
Batch 25/64 loss: -0.15098047256469727
Batch 26/64 loss: -0.1696290373802185
Batch 27/64 loss: -0.15638956427574158
Batch 28/64 loss: -0.17527389526367188
Batch 29/64 loss: -0.11658322811126709
Batch 30/64 loss: -0.16802936792373657
Batch 31/64 loss: -0.14215481281280518
Batch 32/64 loss: -0.14456778764724731
Batch 33/64 loss: -0.18103757500648499
Batch 34/64 loss: -0.1725398302078247
Batch 35/64 loss: -0.1751617193222046
Batch 36/64 loss: -0.1559346318244934
Batch 37/64 loss: -0.11930149793624878
Batch 38/64 loss: -0.12525367736816406
Batch 39/64 loss: -0.1620972454547882
Batch 40/64 loss: -0.13323694467544556
Batch 41/64 loss: -0.15655094385147095
Batch 42/64 loss: -0.16584396362304688
Batch 43/64 loss: -0.16880092024803162
Batch 44/64 loss: -0.16492879390716553
Batch 45/64 loss: -0.15537896752357483
Batch 46/64 loss: -0.13743430376052856
Batch 47/64 loss: -0.1352512240409851
Batch 48/64 loss: -0.1728357970714569
Batch 49/64 loss: -0.14368385076522827
Batch 50/64 loss: -0.16653972864151
Batch 51/64 loss: -0.1383371353149414
Batch 52/64 loss: -0.14717787504196167
Batch 53/64 loss: -0.1495215892791748
Batch 54/64 loss: -0.15615445375442505
Batch 55/64 loss: -0.16854941844940186
Batch 56/64 loss: -0.12484616041183472
Batch 57/64 loss: -0.15409421920776367
Batch 58/64 loss: -0.1571749448776245
Batch 59/64 loss: -0.1512264609336853
Batch 60/64 loss: -0.16004681587219238
Batch 61/64 loss: -0.13059765100479126
Batch 62/64 loss: -0.15228188037872314
Batch 63/64 loss: -0.1634223461151123
Batch 64/64 loss: -0.17651021480560303
Epoch 474  Train loss: -0.15551793668784347  Val loss: 0.05759317788881125
Epoch 475
-------------------------------
Batch 1/64 loss: -0.149469792842865
Batch 2/64 loss: -0.12536334991455078
Batch 3/64 loss: -0.17479100823402405
Batch 4/64 loss: -0.1772271990776062
Batch 5/64 loss: -0.16185146570205688
Batch 6/64 loss: -0.17588716745376587
Batch 7/64 loss: -0.15831288695335388
Batch 8/64 loss: -0.14717674255371094
Batch 9/64 loss: -0.16746288537979126
Batch 10/64 loss: -0.16646599769592285
Batch 11/64 loss: -0.1343783736228943
Batch 12/64 loss: -0.12534189224243164
Batch 13/64 loss: -0.14400321245193481
Batch 14/64 loss: -0.1615842878818512
Batch 15/64 loss: -0.16068193316459656
Batch 16/64 loss: -0.1226053237915039
Batch 17/64 loss: -0.15182769298553467
Batch 18/64 loss: -0.13830560445785522
Batch 19/64 loss: -0.16215091943740845
Batch 20/64 loss: -0.15279215574264526
Batch 21/64 loss: -0.17445024847984314
Batch 22/64 loss: -0.13360023498535156
Batch 23/64 loss: -0.16757559776306152
Batch 24/64 loss: -0.15673905611038208
Batch 25/64 loss: -0.15482598543167114
Batch 26/64 loss: -0.16357368230819702
Batch 27/64 loss: -0.12850147485733032
Batch 28/64 loss: -0.14963829517364502
Batch 29/64 loss: -0.16381609439849854
Batch 30/64 loss: -0.15834560990333557
Batch 31/64 loss: -0.1569514274597168
Batch 32/64 loss: -0.13969296216964722
Batch 33/64 loss: -0.15955644845962524
Batch 34/64 loss: -0.14462029933929443
Batch 35/64 loss: -0.17078936100006104
Batch 36/64 loss: -0.14073169231414795
Batch 37/64 loss: -0.16569432616233826
Batch 38/64 loss: -0.16140609979629517
Batch 39/64 loss: -0.18052774667739868
Batch 40/64 loss: -0.18072855472564697
Batch 41/64 loss: -0.1749303638935089
Batch 42/64 loss: -0.17242801189422607
Batch 43/64 loss: -0.14113420248031616
Batch 44/64 loss: -0.152640700340271
Batch 45/64 loss: -0.16041111946105957
Batch 46/64 loss: -0.14595794677734375
Batch 47/64 loss: -0.1568140685558319
Batch 48/64 loss: -0.1678367257118225
Batch 49/64 loss: -0.10621029138565063
Batch 50/64 loss: -0.15009480714797974
Batch 51/64 loss: -0.16014927625656128
Batch 52/64 loss: -0.16856881976127625
Batch 53/64 loss: -0.13597452640533447
Batch 54/64 loss: -0.157698392868042
Batch 55/64 loss: -0.13393235206604004
Batch 56/64 loss: -0.15416431427001953
Batch 57/64 loss: -0.13039201498031616
Batch 58/64 loss: -0.13714629411697388
Batch 59/64 loss: -0.15937170386314392
Batch 60/64 loss: -0.14178568124771118
Batch 61/64 loss: -0.14856493473052979
Batch 62/64 loss: -0.15399765968322754
Batch 63/64 loss: -0.1278700828552246
Batch 64/64 loss: -0.17708289623260498
Epoch 475  Train loss: -0.15341696552201814  Val loss: 0.056850489677022824
Epoch 476
-------------------------------
Batch 1/64 loss: -0.1294628381729126
Batch 2/64 loss: -0.17555975914001465
Batch 3/64 loss: -0.16840600967407227
Batch 4/64 loss: -0.15305566787719727
Batch 5/64 loss: -0.14411485195159912
Batch 6/64 loss: -0.1357506513595581
Batch 7/64 loss: -0.14427387714385986
Batch 8/64 loss: -0.11258858442306519
Batch 9/64 loss: -0.18467628955841064
Batch 10/64 loss: -0.1725529432296753
Batch 11/64 loss: -0.16393309831619263
Batch 12/64 loss: -0.15476131439208984
Batch 13/64 loss: -0.14735257625579834
Batch 14/64 loss: -0.17207664251327515
Batch 15/64 loss: -0.16799649596214294
Batch 16/64 loss: -0.15596705675125122
Batch 17/64 loss: -0.1475721001625061
Batch 18/64 loss: -0.15661871433258057
Batch 19/64 loss: -0.14307087659835815
Batch 20/64 loss: -0.1839953064918518
Batch 21/64 loss: -0.1148485541343689
Batch 22/64 loss: -0.18572115898132324
Batch 23/64 loss: -0.1362733244895935
Batch 24/64 loss: -0.15093696117401123
Batch 25/64 loss: -0.1242668628692627
Batch 26/64 loss: -0.15165376663208008
Batch 27/64 loss: -0.11480355262756348
Batch 28/64 loss: -0.17135381698608398
Batch 29/64 loss: -0.16253447532653809
Batch 30/64 loss: -0.15435010194778442
Batch 31/64 loss: -0.14637905359268188
Batch 32/64 loss: -0.16504526138305664
Batch 33/64 loss: -0.15451157093048096
Batch 34/64 loss: -0.1656622588634491
Batch 35/64 loss: -0.14284050464630127
Batch 36/64 loss: -0.13328522443771362
Batch 37/64 loss: -0.1737009584903717
Batch 38/64 loss: -0.13652491569519043
Batch 39/64 loss: -0.1502002477645874
Batch 40/64 loss: -0.17574083805084229
Batch 41/64 loss: -0.13290154933929443
Batch 42/64 loss: -0.1436983346939087
Batch 43/64 loss: -0.1657511591911316
Batch 44/64 loss: -0.15717291831970215
Batch 45/64 loss: -0.1769307255744934
Batch 46/64 loss: -0.15169697999954224
Batch 47/64 loss: -0.176442950963974
Batch 48/64 loss: -0.15548989176750183
Batch 49/64 loss: -0.17128372192382812
Batch 50/64 loss: -0.13272708654403687
Batch 51/64 loss: -0.17707273364067078
Batch 52/64 loss: -0.16720342636108398
Batch 53/64 loss: -0.14874988794326782
Batch 54/64 loss: -0.10006970167160034
Batch 55/64 loss: -0.15435808897018433
Batch 56/64 loss: -0.16948577761650085
Batch 57/64 loss: -0.1770588755607605
Batch 58/64 loss: -0.1536092460155487
Batch 59/64 loss: -0.146930992603302
Batch 60/64 loss: -0.1437719464302063
Batch 61/64 loss: -0.14735502004623413
Batch 62/64 loss: -0.1281021237373352
Batch 63/64 loss: -0.14736753702163696
Batch 64/64 loss: -0.12942469120025635
Epoch 476  Train loss: -0.15288969815946094  Val loss: 0.054919141469542514
Epoch 477
-------------------------------
Batch 1/64 loss: -0.14174795150756836
Batch 2/64 loss: -0.16963964700698853
Batch 3/64 loss: -0.1640656292438507
Batch 4/64 loss: -0.1378614902496338
Batch 5/64 loss: -0.15364903211593628
Batch 6/64 loss: -0.1878877878189087
Batch 7/64 loss: -0.17430353164672852
Batch 8/64 loss: -0.16605263948440552
Batch 9/64 loss: -0.10076415538787842
Batch 10/64 loss: -0.16760122776031494
Batch 11/64 loss: -0.15756511688232422
Batch 12/64 loss: -0.17815688252449036
Batch 13/64 loss: -0.1578940749168396
Batch 14/64 loss: -0.18067657947540283
Batch 15/64 loss: -0.1873869001865387
Batch 16/64 loss: -0.15534740686416626
Batch 17/64 loss: -0.1705617904663086
Batch 18/64 loss: -0.14421933889389038
Batch 19/64 loss: -0.11260533332824707
Batch 20/64 loss: -0.1467437744140625
Batch 21/64 loss: -0.1551220417022705
Batch 22/64 loss: -0.14076858758926392
Batch 23/64 loss: -0.1708117425441742
Batch 24/64 loss: -0.13166993856430054
Batch 25/64 loss: -0.1726081371307373
Batch 26/64 loss: -0.192472904920578
Batch 27/64 loss: -0.15216302871704102
Batch 28/64 loss: -0.15328210592269897
Batch 29/64 loss: -0.15649813413619995
Batch 30/64 loss: -0.14890927076339722
Batch 31/64 loss: -0.17951953411102295
Batch 32/64 loss: -0.16803473234176636
Batch 33/64 loss: -0.15957599878311157
Batch 34/64 loss: -0.14253449440002441
Batch 35/64 loss: -0.15249687433242798
Batch 36/64 loss: -0.12389498949050903
Batch 37/64 loss: -0.18057483434677124
Batch 38/64 loss: -0.15388906002044678
Batch 39/64 loss: -0.126428484916687
Batch 40/64 loss: -0.15600109100341797
Batch 41/64 loss: -0.18687385320663452
Batch 42/64 loss: -0.1385820508003235
Batch 43/64 loss: -0.17238765954971313
Batch 44/64 loss: -0.14105403423309326
Batch 45/64 loss: -0.1548784375190735
Batch 46/64 loss: -0.14803141355514526
Batch 47/64 loss: -0.17282837629318237
Batch 48/64 loss: -0.15016210079193115
Batch 49/64 loss: -0.16044652462005615
Batch 50/64 loss: -0.16625013947486877
Batch 51/64 loss: -0.1470910906791687
Batch 52/64 loss: -0.13642573356628418
Batch 53/64 loss: -0.15443360805511475
Batch 54/64 loss: -0.15795111656188965
Batch 55/64 loss: -0.1491560935974121
Batch 56/64 loss: -0.16814568638801575
Batch 57/64 loss: -0.13904404640197754
Batch 58/64 loss: -0.11287617683410645
Batch 59/64 loss: -0.15694284439086914
Batch 60/64 loss: -0.12301146984100342
Batch 61/64 loss: -0.14286822080612183
Batch 62/64 loss: -0.14953285455703735
Batch 63/64 loss: -0.17189118266105652
Batch 64/64 loss: -0.14366644620895386
Epoch 477  Train loss: -0.15498981686199412  Val loss: 0.05419350837923817
Epoch 478
-------------------------------
Batch 1/64 loss: -0.15518838167190552
Batch 2/64 loss: -0.15858036279678345
Batch 3/64 loss: -0.12863582372665405
Batch 4/64 loss: -0.15783238410949707
Batch 5/64 loss: -0.1795508861541748
Batch 6/64 loss: -0.131381094455719
Batch 7/64 loss: -0.1773281991481781
Batch 8/64 loss: -0.13528519868850708
Batch 9/64 loss: -0.13890230655670166
Batch 10/64 loss: -0.16701918840408325
Batch 11/64 loss: -0.1625608205795288
Batch 12/64 loss: -0.16846409440040588
Batch 13/64 loss: -0.13522297143936157
Batch 14/64 loss: -0.16041326522827148
Batch 15/64 loss: -0.18353283405303955
Batch 16/64 loss: -0.14541256427764893
Batch 17/64 loss: -0.12947314977645874
Batch 18/64 loss: -0.14356160163879395
Batch 19/64 loss: -0.13807862997055054
Batch 20/64 loss: -0.135939359664917
Batch 21/64 loss: -0.14661109447479248
Batch 22/64 loss: -0.15411555767059326
Batch 23/64 loss: -0.14447098970413208
Batch 24/64 loss: -0.1474807858467102
Batch 25/64 loss: -0.1665882170200348
Batch 26/64 loss: -0.12637358903884888
Batch 27/64 loss: -0.15618932247161865
Batch 28/64 loss: -0.14576303958892822
Batch 29/64 loss: -0.1580418348312378
Batch 30/64 loss: -0.15410470962524414
Batch 31/64 loss: -0.14848381280899048
Batch 32/64 loss: -0.14805501699447632
Batch 33/64 loss: -0.17381718754768372
Batch 34/64 loss: -0.1666567325592041
Batch 35/64 loss: -0.08781564235687256
Batch 36/64 loss: -0.14255458116531372
Batch 37/64 loss: -0.15728285908699036
Batch 38/64 loss: -0.16655564308166504
Batch 39/64 loss: -0.12193965911865234
Batch 40/64 loss: -0.16470593214035034
Batch 41/64 loss: -0.15013134479522705
Batch 42/64 loss: -0.171608567237854
Batch 43/64 loss: -0.13161444664001465
Batch 44/64 loss: -0.15969803929328918
Batch 45/64 loss: -0.13499438762664795
Batch 46/64 loss: -0.15608462691307068
Batch 47/64 loss: -0.16797131299972534
Batch 48/64 loss: -0.16493141651153564
Batch 49/64 loss: -0.17958307266235352
Batch 50/64 loss: -0.1486184000968933
Batch 51/64 loss: -0.13593167066574097
Batch 52/64 loss: -0.15628832578659058
Batch 53/64 loss: -0.15561360120773315
Batch 54/64 loss: -0.14441508054733276
Batch 55/64 loss: -0.17048966884613037
Batch 56/64 loss: -0.15724214911460876
Batch 57/64 loss: -0.15869951248168945
Batch 58/64 loss: -0.14551061391830444
Batch 59/64 loss: -0.13440310955047607
Batch 60/64 loss: -0.14600878953933716
Batch 61/64 loss: -0.13424354791641235
Batch 62/64 loss: -0.14737993478775024
Batch 63/64 loss: -0.15149974822998047
Batch 64/64 loss: -0.16430512070655823
Epoch 478  Train loss: -0.15115544363564137  Val loss: 0.05815156894860808
Epoch 479
-------------------------------
Batch 1/64 loss: -0.14208686351776123
Batch 2/64 loss: -0.11111068725585938
Batch 3/64 loss: -0.13693279027938843
Batch 4/64 loss: -0.15636223554611206
Batch 5/64 loss: -0.16123712062835693
Batch 6/64 loss: -0.17696887254714966
Batch 7/64 loss: -0.17109453678131104
Batch 8/64 loss: -0.15095096826553345
Batch 9/64 loss: -0.17343592643737793
Batch 10/64 loss: -0.1571056842803955
Batch 11/64 loss: -0.13730740547180176
Batch 12/64 loss: -0.17732036113739014
Batch 13/64 loss: -0.14754503965377808
Batch 14/64 loss: -0.16169536113739014
Batch 15/64 loss: -0.16876482963562012
Batch 16/64 loss: -0.12413644790649414
Batch 17/64 loss: -0.1288396716117859
Batch 18/64 loss: -0.13962608575820923
Batch 19/64 loss: -0.14955246448516846
Batch 20/64 loss: -0.1218879222869873
Batch 21/64 loss: -0.16075733304023743
Batch 22/64 loss: -0.1356636881828308
Batch 23/64 loss: -0.1414870023727417
Batch 24/64 loss: -0.14779257774353027
Batch 25/64 loss: -0.15145927667617798
Batch 26/64 loss: -0.1625954806804657
Batch 27/64 loss: -0.1488153338432312
Batch 28/64 loss: -0.14738720655441284
Batch 29/64 loss: -0.18018412590026855
Batch 30/64 loss: -0.16316545009613037
Batch 31/64 loss: -0.14454150199890137
Batch 32/64 loss: -0.1371747851371765
Batch 33/64 loss: -0.17058217525482178
Batch 34/64 loss: -0.11895668506622314
Batch 35/64 loss: -0.1536765694618225
Batch 36/64 loss: -0.154291570186615
Batch 37/64 loss: -0.1538042426109314
Batch 38/64 loss: -0.14645618200302124
Batch 39/64 loss: -0.17516937851905823
Batch 40/64 loss: -0.12734007835388184
Batch 41/64 loss: -0.15531527996063232
Batch 42/64 loss: -0.14399397373199463
Batch 43/64 loss: -0.15317189693450928
Batch 44/64 loss: -0.18460723757743835
Batch 45/64 loss: -0.17585515975952148
Batch 46/64 loss: -0.14486217498779297
Batch 47/64 loss: -0.1392163634300232
Batch 48/64 loss: -0.18605679273605347
Batch 49/64 loss: -0.1475604772567749
Batch 50/64 loss: -0.1364784836769104
Batch 51/64 loss: -0.1660444140434265
Batch 52/64 loss: -0.16600370407104492
Batch 53/64 loss: -0.16280966997146606
Batch 54/64 loss: -0.14860796928405762
Batch 55/64 loss: -0.16900894045829773
Batch 56/64 loss: -0.14084601402282715
Batch 57/64 loss: -0.17582443356513977
Batch 58/64 loss: -0.1833430528640747
Batch 59/64 loss: -0.16000795364379883
Batch 60/64 loss: -0.16927304863929749
Batch 61/64 loss: -0.16579878330230713
Batch 62/64 loss: -0.15935808420181274
Batch 63/64 loss: -0.12751364707946777
Batch 64/64 loss: -0.15250736474990845
Epoch 479  Train loss: -0.15358745093439138  Val loss: 0.05650239752739975
Epoch 480
-------------------------------
Batch 1/64 loss: -0.15501999855041504
Batch 2/64 loss: -0.13297945261001587
Batch 3/64 loss: -0.13545387983322144
Batch 4/64 loss: -0.12373578548431396
Batch 5/64 loss: -0.1750943660736084
Batch 6/64 loss: -0.1501113772392273
Batch 7/64 loss: -0.16858458518981934
Batch 8/64 loss: -0.1532994508743286
Batch 9/64 loss: -0.15264946222305298
Batch 10/64 loss: -0.1501699686050415
Batch 11/64 loss: -0.14675581455230713
Batch 12/64 loss: -0.17408281564712524
Batch 13/64 loss: -0.15751370787620544
Batch 14/64 loss: -0.11918580532073975
Batch 15/64 loss: -0.13797682523727417
Batch 16/64 loss: -0.18119925260543823
Batch 17/64 loss: -0.169470876455307
Batch 18/64 loss: -0.1538987159729004
Batch 19/64 loss: -0.16511595249176025
Batch 20/64 loss: -0.16437798738479614
Batch 21/64 loss: -0.10661637783050537
Batch 22/64 loss: -0.12967538833618164
Batch 23/64 loss: -0.15503543615341187
Batch 24/64 loss: -0.15578842163085938
Batch 25/64 loss: -0.15956789255142212
Batch 26/64 loss: -0.17403507232666016
Batch 27/64 loss: -0.1557483673095703
Batch 28/64 loss: -0.14967358112335205
Batch 29/64 loss: -0.16589680314064026
Batch 30/64 loss: -0.1268749237060547
Batch 31/64 loss: -0.1745605170726776
Batch 32/64 loss: -0.1697491705417633
Batch 33/64 loss: -0.1543751358985901
Batch 34/64 loss: -0.15552926063537598
Batch 35/64 loss: -0.14062035083770752
Batch 36/64 loss: -0.1455850601196289
Batch 37/64 loss: -0.1297503113746643
Batch 38/64 loss: -0.15372967720031738
Batch 39/64 loss: -0.15981179475784302
Batch 40/64 loss: -0.15038108825683594
Batch 41/64 loss: -0.13645035028457642
Batch 42/64 loss: -0.13754010200500488
Batch 43/64 loss: -0.18780910968780518
Batch 44/64 loss: -0.15836796164512634
Batch 45/64 loss: -0.1658916175365448
Batch 46/64 loss: -0.1716277301311493
Batch 47/64 loss: -0.15429240465164185
Batch 48/64 loss: -0.17311707139015198
Batch 49/64 loss: -0.1299142837524414
Batch 50/64 loss: -0.14569246768951416
Batch 51/64 loss: -0.1543562412261963
Batch 52/64 loss: -0.1559353470802307
Batch 53/64 loss: -0.11826485395431519
Batch 54/64 loss: -0.1317288875579834
Batch 55/64 loss: -0.13362503051757812
Batch 56/64 loss: -0.17296749353408813
Batch 57/64 loss: -0.18334460258483887
Batch 58/64 loss: -0.17877060174942017
Batch 59/64 loss: -0.17311137914657593
Batch 60/64 loss: -0.1760108768939972
Batch 61/64 loss: -0.17794224619865417
Batch 62/64 loss: -0.11385029554367065
Batch 63/64 loss: -0.1785820722579956
Batch 64/64 loss: -0.161918044090271
Epoch 480  Train loss: -0.15388725131165748  Val loss: 0.05479876528081206
Epoch 481
-------------------------------
Batch 1/64 loss: -0.1751387119293213
Batch 2/64 loss: -0.16269180178642273
Batch 3/64 loss: -0.1885715126991272
Batch 4/64 loss: -0.16899001598358154
Batch 5/64 loss: -0.15785309672355652
Batch 6/64 loss: -0.15080827474594116
Batch 7/64 loss: -0.1844058632850647
Batch 8/64 loss: -0.11932140588760376
Batch 9/64 loss: -0.16625618934631348
Batch 10/64 loss: -0.15119338035583496
Batch 11/64 loss: -0.16799038648605347
Batch 12/64 loss: -0.12894093990325928
Batch 13/64 loss: -0.13997256755828857
Batch 14/64 loss: -0.18023091554641724
Batch 15/64 loss: -0.17113590240478516
Batch 16/64 loss: -0.16031122207641602
Batch 17/64 loss: -0.17170476913452148
Batch 18/64 loss: -0.1871165633201599
Batch 19/64 loss: -0.16610261797904968
Batch 20/64 loss: -0.16223913431167603
Batch 21/64 loss: -0.15305358171463013
Batch 22/64 loss: -0.1600894331932068
Batch 23/64 loss: -0.17663979530334473
Batch 24/64 loss: -0.15440493822097778
Batch 25/64 loss: -0.14449459314346313
Batch 26/64 loss: -0.14293968677520752
Batch 27/64 loss: -0.14586150646209717
Batch 28/64 loss: -0.17141848802566528
Batch 29/64 loss: -0.15251249074935913
Batch 30/64 loss: -0.17522984743118286
Batch 31/64 loss: -0.13827955722808838
Batch 32/64 loss: -0.156620055437088
Batch 33/64 loss: -0.15835809707641602
Batch 34/64 loss: -0.15360093116760254
Batch 35/64 loss: -0.17474472522735596
Batch 36/64 loss: -0.17810815572738647
Batch 37/64 loss: -0.15791451930999756
Batch 38/64 loss: -0.15534168481826782
Batch 39/64 loss: -0.1488143801689148
Batch 40/64 loss: -0.15541011095046997
Batch 41/64 loss: -0.15042054653167725
Batch 42/64 loss: -0.17275750637054443
Batch 43/64 loss: -0.1602468490600586
Batch 44/64 loss: -0.151350736618042
Batch 45/64 loss: -0.14325636625289917
Batch 46/64 loss: -0.1188730001449585
Batch 47/64 loss: -0.14908671379089355
Batch 48/64 loss: -0.15488648414611816
Batch 49/64 loss: -0.11556297540664673
Batch 50/64 loss: -0.15939638018608093
Batch 51/64 loss: -0.14229726791381836
Batch 52/64 loss: -0.12049722671508789
Batch 53/64 loss: -0.16458934545516968
Batch 54/64 loss: -0.1160506010055542
Batch 55/64 loss: -0.1428728699684143
Batch 56/64 loss: -0.1447688341140747
Batch 57/64 loss: -0.16473045945167542
Batch 58/64 loss: -0.15622225403785706
Batch 59/64 loss: -0.14632970094680786
Batch 60/64 loss: -0.13191723823547363
Batch 61/64 loss: -0.14799857139587402
Batch 62/64 loss: -0.17023617029190063
Batch 63/64 loss: -0.1286529302597046
Batch 64/64 loss: -0.1391170620918274
Epoch 481  Train loss: -0.15485726548176185  Val loss: 0.06003133476394968
Epoch 482
-------------------------------
Batch 1/64 loss: -0.17387816309928894
Batch 2/64 loss: -0.15529173612594604
Batch 3/64 loss: -0.13683390617370605
Batch 4/64 loss: -0.14541178941726685
Batch 5/64 loss: -0.15741991996765137
Batch 6/64 loss: -0.1599733829498291
Batch 7/64 loss: -0.18524712324142456
Batch 8/64 loss: -0.16266712546348572
Batch 9/64 loss: -0.16452115774154663
Batch 10/64 loss: -0.1780892014503479
Batch 11/64 loss: -0.1456114649772644
Batch 12/64 loss: -0.12315511703491211
Batch 13/64 loss: -0.16701972484588623
Batch 14/64 loss: -0.12955576181411743
Batch 15/64 loss: -0.15670937299728394
Batch 16/64 loss: -0.12534868717193604
Batch 17/64 loss: -0.15278106927871704
Batch 18/64 loss: -0.15523993968963623
Batch 19/64 loss: -0.15228986740112305
Batch 20/64 loss: -0.16435623168945312
Batch 21/64 loss: -0.1532813310623169
Batch 22/64 loss: -0.14538240432739258
Batch 23/64 loss: -0.16672205924987793
Batch 24/64 loss: -0.14125508069992065
Batch 25/64 loss: -0.1343705654144287
Batch 26/64 loss: -0.13788127899169922
Batch 27/64 loss: -0.16366922855377197
Batch 28/64 loss: -0.14774668216705322
Batch 29/64 loss: -0.17389973998069763
Batch 30/64 loss: -0.15073728561401367
Batch 31/64 loss: -0.1624748706817627
Batch 32/64 loss: -0.16246479749679565
Batch 33/64 loss: -0.14470547437667847
Batch 34/64 loss: -0.13903576135635376
Batch 35/64 loss: -0.14989304542541504
Batch 36/64 loss: -0.1618959903717041
Batch 37/64 loss: -0.11996316909790039
Batch 38/64 loss: -0.18087875843048096
Batch 39/64 loss: -0.17318645119667053
Batch 40/64 loss: -0.17666888236999512
Batch 41/64 loss: -0.14111387729644775
Batch 42/64 loss: -0.13113456964492798
Batch 43/64 loss: -0.16334867477416992
Batch 44/64 loss: -0.14912331104278564
Batch 45/64 loss: -0.15130406618118286
Batch 46/64 loss: -0.1458185315132141
Batch 47/64 loss: -0.146226167678833
Batch 48/64 loss: -0.1269281506538391
Batch 49/64 loss: -0.1590091586112976
Batch 50/64 loss: -0.16919803619384766
Batch 51/64 loss: -0.1337568759918213
Batch 52/64 loss: -0.18131723999977112
Batch 53/64 loss: -0.14253294467926025
Batch 54/64 loss: -0.13688373565673828
Batch 55/64 loss: -0.18108969926834106
Batch 56/64 loss: -0.16624093055725098
Batch 57/64 loss: -0.15108871459960938
Batch 58/64 loss: -0.15178513526916504
Batch 59/64 loss: -0.18483245372772217
Batch 60/64 loss: -0.16404485702514648
Batch 61/64 loss: -0.17047125101089478
Batch 62/64 loss: -0.1456398367881775
Batch 63/64 loss: -0.14591753482818604
Batch 64/64 loss: -0.10252749919891357
Epoch 482  Train loss: -0.15361905845941282  Val loss: 0.05741858298016578
Epoch 483
-------------------------------
Batch 1/64 loss: -0.1374415159225464
Batch 2/64 loss: -0.17020893096923828
Batch 3/64 loss: -0.15903052687644958
Batch 4/64 loss: -0.15161502361297607
Batch 5/64 loss: -0.1484406590461731
Batch 6/64 loss: -0.1531665325164795
Batch 7/64 loss: -0.13960891962051392
Batch 8/64 loss: -0.14304471015930176
Batch 9/64 loss: -0.15114116668701172
Batch 10/64 loss: -0.16864436864852905
Batch 11/64 loss: -0.18993845582008362
Batch 12/64 loss: -0.14913612604141235
Batch 13/64 loss: -0.16744357347488403
Batch 14/64 loss: -0.15693563222885132
Batch 15/64 loss: -0.14714080095291138
Batch 16/64 loss: -0.15778660774230957
Batch 17/64 loss: -0.1408408284187317
Batch 18/64 loss: -0.16618698835372925
Batch 19/64 loss: -0.16869086027145386
Batch 20/64 loss: -0.1458643674850464
Batch 21/64 loss: -0.13296723365783691
Batch 22/64 loss: -0.1205713152885437
Batch 23/64 loss: -0.17559286952018738
Batch 24/64 loss: -0.1465740203857422
Batch 25/64 loss: -0.16957253217697144
Batch 26/64 loss: -0.1817454695701599
Batch 27/64 loss: -0.14123386144638062
Batch 28/64 loss: -0.09565186500549316
Batch 29/64 loss: -0.14852750301361084
Batch 30/64 loss: -0.1579936146736145
Batch 31/64 loss: -0.16723021864891052
Batch 32/64 loss: -0.15741097927093506
Batch 33/64 loss: -0.14328491687774658
Batch 34/64 loss: -0.1405544877052307
Batch 35/64 loss: -0.14411818981170654
Batch 36/64 loss: -0.14197683334350586
Batch 37/64 loss: -0.15807077288627625
Batch 38/64 loss: -0.1868453025817871
Batch 39/64 loss: -0.13893091678619385
Batch 40/64 loss: -0.16500923037528992
Batch 41/64 loss: -0.161088764667511
Batch 42/64 loss: -0.17575252056121826
Batch 43/64 loss: -0.1303461194038391
Batch 44/64 loss: -0.14674967527389526
Batch 45/64 loss: -0.12743765115737915
Batch 46/64 loss: -0.1589668095111847
Batch 47/64 loss: -0.16665905714035034
Batch 48/64 loss: -0.15794366598129272
Batch 49/64 loss: -0.14818567037582397
Batch 50/64 loss: -0.16767942905426025
Batch 51/64 loss: -0.12914854288101196
Batch 52/64 loss: -0.16294008493423462
Batch 53/64 loss: -0.14203691482543945
Batch 54/64 loss: -0.1484277844429016
Batch 55/64 loss: -0.1671764850616455
Batch 56/64 loss: -0.1675376296043396
Batch 57/64 loss: -0.1649233102798462
Batch 58/64 loss: -0.18768689036369324
Batch 59/64 loss: -0.12367266416549683
Batch 60/64 loss: -0.14222252368927002
Batch 61/64 loss: -0.16354727745056152
Batch 62/64 loss: -0.11791211366653442
Batch 63/64 loss: -0.1736646592617035
Batch 64/64 loss: -0.1427338719367981
Epoch 483  Train loss: -0.1531753000091104  Val loss: 0.05565087655975237
Epoch 484
-------------------------------
Batch 1/64 loss: -0.16512352228164673
Batch 2/64 loss: -0.1666950285434723
Batch 3/64 loss: -0.1682768166065216
Batch 4/64 loss: -0.15289008617401123
Batch 5/64 loss: -0.17399460077285767
Batch 6/64 loss: -0.182206392288208
Batch 7/64 loss: -0.1646147072315216
Batch 8/64 loss: -0.14724230766296387
Batch 9/64 loss: -0.16598141193389893
Batch 10/64 loss: -0.17396438121795654
Batch 11/64 loss: -0.14617860317230225
Batch 12/64 loss: -0.17128503322601318
Batch 13/64 loss: -0.13342392444610596
Batch 14/64 loss: -0.1421440839767456
Batch 15/64 loss: -0.1597103774547577
Batch 16/64 loss: -0.17920240759849548
Batch 17/64 loss: -0.15457475185394287
Batch 18/64 loss: -0.1538950800895691
Batch 19/64 loss: -0.14767807722091675
Batch 20/64 loss: -0.1678542196750641
Batch 21/64 loss: -0.17488598823547363
Batch 22/64 loss: -0.16541117429733276
Batch 23/64 loss: -0.1742405891418457
Batch 24/64 loss: -0.17993932962417603
Batch 25/64 loss: -0.1572442650794983
Batch 26/64 loss: -0.16190171241760254
Batch 27/64 loss: -0.1626213788986206
Batch 28/64 loss: -0.1613953709602356
Batch 29/64 loss: -0.15726622939109802
Batch 30/64 loss: -0.18068066239356995
Batch 31/64 loss: -0.14822471141815186
Batch 32/64 loss: -0.16089612245559692
Batch 33/64 loss: -0.13552188873291016
Batch 34/64 loss: -0.1587933897972107
Batch 35/64 loss: -0.14487528800964355
Batch 36/64 loss: -0.1531074047088623
Batch 37/64 loss: -0.17154794931411743
Batch 38/64 loss: -0.1652546226978302
Batch 39/64 loss: -0.17612487077713013
Batch 40/64 loss: -0.1657850444316864
Batch 41/64 loss: -0.14166373014450073
Batch 42/64 loss: -0.16202348470687866
Batch 43/64 loss: -0.14100468158721924
Batch 44/64 loss: -0.14475154876708984
Batch 45/64 loss: -0.13349926471710205
Batch 46/64 loss: -0.15920108556747437
Batch 47/64 loss: -0.13388311862945557
Batch 48/64 loss: -0.1498955488204956
Batch 49/64 loss: -0.14081788063049316
Batch 50/64 loss: -0.14909905195236206
Batch 51/64 loss: -0.16734188795089722
Batch 52/64 loss: -0.15664923191070557
Batch 53/64 loss: -0.13439369201660156
Batch 54/64 loss: -0.16708573698997498
Batch 55/64 loss: -0.16353583335876465
Batch 56/64 loss: -0.15641918778419495
Batch 57/64 loss: -0.1815859079360962
Batch 58/64 loss: -0.1479130983352661
Batch 59/64 loss: -0.1602165699005127
Batch 60/64 loss: -0.15230506658554077
Batch 61/64 loss: -0.1735466718673706
Batch 62/64 loss: -0.15326178073883057
Batch 63/64 loss: -0.14274460077285767
Batch 64/64 loss: -0.13649499416351318
Epoch 484  Train loss: -0.15824099942749623  Val loss: 0.05704765815505457
Epoch 485
-------------------------------
Batch 1/64 loss: -0.15027636289596558
Batch 2/64 loss: -0.13404929637908936
Batch 3/64 loss: -0.16578590869903564
Batch 4/64 loss: -0.13053691387176514
Batch 5/64 loss: -0.14484989643096924
Batch 6/64 loss: -0.16445156931877136
Batch 7/64 loss: -0.1575518250465393
Batch 8/64 loss: -0.15514373779296875
Batch 9/64 loss: -0.16259995102882385
Batch 10/64 loss: -0.1590040922164917
Batch 11/64 loss: -0.16965389251708984
Batch 12/64 loss: -0.16222742199897766
Batch 13/64 loss: -0.14453309774398804
Batch 14/64 loss: -0.17589205503463745
Batch 15/64 loss: -0.1446729302406311
Batch 16/64 loss: -0.15958166122436523
Batch 17/64 loss: -0.15399664640426636
Batch 18/64 loss: -0.10508596897125244
Batch 19/64 loss: -0.15416020154953003
Batch 20/64 loss: -0.17406314611434937
Batch 21/64 loss: -0.15136641263961792
Batch 22/64 loss: -0.1885738968849182
Batch 23/64 loss: -0.17795082926750183
Batch 24/64 loss: -0.13980209827423096
Batch 25/64 loss: -0.17098009586334229
Batch 26/64 loss: -0.17114198207855225
Batch 27/64 loss: -0.1454780101776123
Batch 28/64 loss: -0.17654234170913696
Batch 29/64 loss: -0.1696869134902954
Batch 30/64 loss: -0.1713162660598755
Batch 31/64 loss: -0.14835619926452637
Batch 32/64 loss: -0.14369451999664307
Batch 33/64 loss: -0.15132063627243042
Batch 34/64 loss: -0.1771184206008911
Batch 35/64 loss: -0.16424709558486938
Batch 36/64 loss: -0.14321744441986084
Batch 37/64 loss: -0.13430821895599365
Batch 38/64 loss: -0.15454816818237305
Batch 39/64 loss: -0.14487594366073608
Batch 40/64 loss: -0.1461271047592163
Batch 41/64 loss: -0.1391284465789795
Batch 42/64 loss: -0.1639847457408905
Batch 43/64 loss: -0.17259448766708374
Batch 44/64 loss: -0.15096336603164673
Batch 45/64 loss: -0.14024752378463745
Batch 46/64 loss: -0.12773174047470093
Batch 47/64 loss: -0.1605527400970459
Batch 48/64 loss: -0.13838696479797363
Batch 49/64 loss: -0.1658647656440735
Batch 50/64 loss: -0.13197433948516846
Batch 51/64 loss: -0.16471797227859497
Batch 52/64 loss: -0.16998213529586792
Batch 53/64 loss: -0.1657370924949646
Batch 54/64 loss: -0.17091384530067444
Batch 55/64 loss: -0.1358356475830078
Batch 56/64 loss: -0.16358619928359985
Batch 57/64 loss: -0.1353200078010559
Batch 58/64 loss: -0.12294161319732666
Batch 59/64 loss: -0.147446870803833
Batch 60/64 loss: -0.16585224866867065
Batch 61/64 loss: -0.16389203071594238
Batch 62/64 loss: -0.18209630250930786
Batch 63/64 loss: -0.12576067447662354
Batch 64/64 loss: -0.13418298959732056
Epoch 485  Train loss: -0.15443008904363595  Val loss: 0.05655102979686252
Epoch 486
-------------------------------
Batch 1/64 loss: -0.1592017412185669
Batch 2/64 loss: -0.17089951038360596
Batch 3/64 loss: -0.17308545112609863
Batch 4/64 loss: -0.17721211910247803
Batch 5/64 loss: -0.1722877323627472
Batch 6/64 loss: -0.140672504901886
Batch 7/64 loss: -0.16643494367599487
Batch 8/64 loss: -0.14282965660095215
Batch 9/64 loss: -0.16430914402008057
Batch 10/64 loss: -0.1659652590751648
Batch 11/64 loss: -0.18551450967788696
Batch 12/64 loss: -0.15847671031951904
Batch 13/64 loss: -0.1541275978088379
Batch 14/64 loss: -0.15287262201309204
Batch 15/64 loss: -0.15582576394081116
Batch 16/64 loss: -0.15022724866867065
Batch 17/64 loss: -0.1579653024673462
Batch 18/64 loss: -0.16863149404525757
Batch 19/64 loss: -0.16605499386787415
Batch 20/64 loss: -0.14361482858657837
Batch 21/64 loss: -0.13708370923995972
Batch 22/64 loss: -0.1559445559978485
Batch 23/64 loss: -0.10626822710037231
Batch 24/64 loss: -0.14361345767974854
Batch 25/64 loss: -0.17817610502243042
Batch 26/64 loss: -0.1578061878681183
Batch 27/64 loss: -0.14590799808502197
Batch 28/64 loss: -0.1527007818222046
Batch 29/64 loss: -0.1495605707168579
Batch 30/64 loss: -0.15874552726745605
Batch 31/64 loss: -0.17010000348091125
Batch 32/64 loss: -0.1663181483745575
Batch 33/64 loss: -0.17639082670211792
Batch 34/64 loss: -0.16079407930374146
Batch 35/64 loss: -0.17784667015075684
Batch 36/64 loss: -0.1550927758216858
Batch 37/64 loss: -0.14731574058532715
Batch 38/64 loss: -0.13869339227676392
Batch 39/64 loss: -0.14814919233322144
Batch 40/64 loss: -0.14344263076782227
Batch 41/64 loss: -0.147987961769104
Batch 42/64 loss: -0.15158581733703613
Batch 43/64 loss: -0.13577544689178467
Batch 44/64 loss: -0.165810227394104
Batch 45/64 loss: -0.1566872000694275
Batch 46/64 loss: -0.17612111568450928
Batch 47/64 loss: -0.15198582410812378
Batch 48/64 loss: -0.14443188905715942
Batch 49/64 loss: -0.17423999309539795
Batch 50/64 loss: -0.1488475203514099
Batch 51/64 loss: -0.13796627521514893
Batch 52/64 loss: -0.1553436517715454
Batch 53/64 loss: -0.1458127498626709
Batch 54/64 loss: -0.17524421215057373
Batch 55/64 loss: -0.1542527675628662
Batch 56/64 loss: -0.14258062839508057
Batch 57/64 loss: -0.18488043546676636
Batch 58/64 loss: -0.1768779754638672
Batch 59/64 loss: -0.1504061222076416
Batch 60/64 loss: -0.11430919170379639
Batch 61/64 loss: -0.13022714853286743
Batch 62/64 loss: -0.16299307346343994
Batch 63/64 loss: -0.1715877652168274
Batch 64/64 loss: -0.15262335538864136
Epoch 486  Train loss: -0.15633854468663533  Val loss: 0.05646116893316053
Epoch 487
-------------------------------
Batch 1/64 loss: -0.17674344778060913
Batch 2/64 loss: -0.13402575254440308
Batch 3/64 loss: -0.1507304310798645
Batch 4/64 loss: -0.12400740385055542
Batch 5/64 loss: -0.17629742622375488
Batch 6/64 loss: -0.13196635246276855
Batch 7/64 loss: -0.1773199439048767
Batch 8/64 loss: -0.16222146153450012
Batch 9/64 loss: -0.1478574275970459
Batch 10/64 loss: -0.14044779539108276
Batch 11/64 loss: -0.15629780292510986
Batch 12/64 loss: -0.15108346939086914
Batch 13/64 loss: -0.1553264856338501
Batch 14/64 loss: -0.16270965337753296
Batch 15/64 loss: -0.1639190912246704
Batch 16/64 loss: -0.1725032925605774
Batch 17/64 loss: -0.17039388418197632
Batch 18/64 loss: -0.14133501052856445
Batch 19/64 loss: -0.1571284532546997
Batch 20/64 loss: -0.18347245454788208
Batch 21/64 loss: -0.165338397026062
Batch 22/64 loss: -0.16365045309066772
Batch 23/64 loss: -0.1130443811416626
Batch 24/64 loss: -0.15171867609024048
Batch 25/64 loss: -0.16450142860412598
Batch 26/64 loss: -0.15973234176635742
Batch 27/64 loss: -0.1564481258392334
Batch 28/64 loss: -0.16869986057281494
Batch 29/64 loss: -0.14599168300628662
Batch 30/64 loss: -0.15444457530975342
Batch 31/64 loss: -0.18304181098937988
Batch 32/64 loss: -0.15163171291351318
Batch 33/64 loss: -0.1605396568775177
Batch 34/64 loss: -0.1337178349494934
Batch 35/64 loss: -0.11601793766021729
Batch 36/64 loss: -0.16934466361999512
Batch 37/64 loss: -0.17259079217910767
Batch 38/64 loss: -0.16437792778015137
Batch 39/64 loss: -0.17528676986694336
Batch 40/64 loss: -0.168182373046875
Batch 41/64 loss: -0.12342095375061035
Batch 42/64 loss: -0.1432090401649475
Batch 43/64 loss: -0.13246577978134155
Batch 44/64 loss: -0.17628109455108643
Batch 45/64 loss: -0.14650774002075195
Batch 46/64 loss: -0.17204418778419495
Batch 47/64 loss: -0.1488056182861328
Batch 48/64 loss: -0.15557348728179932
Batch 49/64 loss: -0.15798413753509521
Batch 50/64 loss: -0.16650539636611938
Batch 51/64 loss: -0.19088208675384521
Batch 52/64 loss: -0.154008150100708
Batch 53/64 loss: -0.17335790395736694
Batch 54/64 loss: -0.1559026837348938
Batch 55/64 loss: -0.15678861737251282
Batch 56/64 loss: -0.1672491431236267
Batch 57/64 loss: -0.1392536163330078
Batch 58/64 loss: -0.17104756832122803
Batch 59/64 loss: -0.13502037525177002
Batch 60/64 loss: -0.13008332252502441
Batch 61/64 loss: -0.12132531404495239
Batch 62/64 loss: -0.1556706428527832
Batch 63/64 loss: -0.1673811376094818
Batch 64/64 loss: -0.1542019248008728
Epoch 487  Train loss: -0.15577267268124748  Val loss: 0.056853749088405334
Epoch 488
-------------------------------
Batch 1/64 loss: -0.13460242748260498
Batch 2/64 loss: -0.15599915385246277
Batch 3/64 loss: -0.12025630474090576
Batch 4/64 loss: -0.1552138328552246
Batch 5/64 loss: -0.16522979736328125
Batch 6/64 loss: -0.1651376187801361
Batch 7/64 loss: -0.17787280678749084
Batch 8/64 loss: -0.14525997638702393
Batch 9/64 loss: -0.14834153652191162
Batch 10/64 loss: -0.16277176141738892
Batch 11/64 loss: -0.16789263486862183
Batch 12/64 loss: -0.13383221626281738
Batch 13/64 loss: -0.16508150100708008
Batch 14/64 loss: -0.14667075872421265
Batch 15/64 loss: -0.15169352293014526
Batch 16/64 loss: -0.14636319875717163
Batch 17/64 loss: -0.16469663381576538
Batch 18/64 loss: -0.16150858998298645
Batch 19/64 loss: -0.12006843090057373
Batch 20/64 loss: -0.152532696723938
Batch 21/64 loss: -0.15781450271606445
Batch 22/64 loss: -0.16446518898010254
Batch 23/64 loss: -0.1631203293800354
Batch 24/64 loss: -0.16131463646888733
Batch 25/64 loss: -0.12414640188217163
Batch 26/64 loss: -0.1620469093322754
Batch 27/64 loss: -0.16334661841392517
Batch 28/64 loss: -0.14531934261322021
Batch 29/64 loss: -0.1717473864555359
Batch 30/64 loss: -0.14565515518188477
Batch 31/64 loss: -0.1774860918521881
Batch 32/64 loss: -0.16645947098731995
Batch 33/64 loss: -0.15245747566223145
Batch 34/64 loss: -0.15289068222045898
Batch 35/64 loss: -0.15949058532714844
Batch 36/64 loss: -0.16964197158813477
Batch 37/64 loss: -0.1658746898174286
Batch 38/64 loss: -0.16860297322273254
Batch 39/64 loss: -0.1860409379005432
Batch 40/64 loss: -0.1534506380558014
Batch 41/64 loss: -0.16045242547988892
Batch 42/64 loss: -0.1525355577468872
Batch 43/64 loss: -0.1376786231994629
Batch 44/64 loss: -0.15356743335723877
Batch 45/64 loss: -0.16296452283859253
Batch 46/64 loss: -0.14219582080841064
Batch 47/64 loss: -0.1744992733001709
Batch 48/64 loss: -0.15508505702018738
Batch 49/64 loss: -0.15267908573150635
Batch 50/64 loss: -0.1540030837059021
Batch 51/64 loss: -0.1481882929801941
Batch 52/64 loss: -0.16655516624450684
Batch 53/64 loss: -0.15838080644607544
Batch 54/64 loss: -0.13927507400512695
Batch 55/64 loss: -0.15349262952804565
Batch 56/64 loss: -0.1654512584209442
Batch 57/64 loss: -0.15252655744552612
Batch 58/64 loss: -0.12717598676681519
Batch 59/64 loss: -0.17067760229110718
Batch 60/64 loss: -0.17522293329238892
Batch 61/64 loss: -0.16995173692703247
Batch 62/64 loss: -0.16818314790725708
Batch 63/64 loss: -0.15009945631027222
Batch 64/64 loss: -0.13630908727645874
Epoch 488  Train loss: -0.15600738408518772  Val loss: 0.05501538760883292
Epoch 489
-------------------------------
Batch 1/64 loss: -0.17258334159851074
Batch 2/64 loss: -0.1678737998008728
Batch 3/64 loss: -0.16664639115333557
Batch 4/64 loss: -0.17824923992156982
Batch 5/64 loss: -0.15577378869056702
Batch 6/64 loss: -0.1580776572227478
Batch 7/64 loss: -0.14729106426239014
Batch 8/64 loss: -0.15186858177185059
Batch 9/64 loss: -0.16333657503128052
Batch 10/64 loss: -0.1860859990119934
Batch 11/64 loss: -0.16009438037872314
Batch 12/64 loss: -0.1678484082221985
Batch 13/64 loss: -0.16659843921661377
Batch 14/64 loss: -0.16147202253341675
Batch 15/64 loss: -0.15456163883209229
Batch 16/64 loss: -0.14255160093307495
Batch 17/64 loss: -0.16260552406311035
Batch 18/64 loss: -0.13354676961898804
Batch 19/64 loss: -0.1399173140525818
Batch 20/64 loss: -0.13088631629943848
Batch 21/64 loss: -0.13736313581466675
Batch 22/64 loss: -0.1593051552772522
Batch 23/64 loss: -0.19128480553627014
Batch 24/64 loss: -0.13693541288375854
Batch 25/64 loss: -0.16482987999916077
Batch 26/64 loss: -0.15030944347381592
Batch 27/64 loss: -0.15948864817619324
Batch 28/64 loss: -0.13705909252166748
Batch 29/64 loss: -0.16571301221847534
Batch 30/64 loss: -0.13170886039733887
Batch 31/64 loss: -0.10582435131072998
Batch 32/64 loss: -0.15180903673171997
Batch 33/64 loss: -0.16359317302703857
Batch 34/64 loss: -0.13379991054534912
Batch 35/64 loss: -0.16477805376052856
Batch 36/64 loss: -0.1614646315574646
Batch 37/64 loss: -0.15218448638916016
Batch 38/64 loss: -0.16767892241477966
Batch 39/64 loss: -0.16155889630317688
Batch 40/64 loss: -0.1729656159877777
Batch 41/64 loss: -0.138616681098938
Batch 42/64 loss: -0.13725250959396362
Batch 43/64 loss: -0.1616358757019043
Batch 44/64 loss: -0.169855535030365
Batch 45/64 loss: -0.1631629467010498
Batch 46/64 loss: -0.17576318979263306
Batch 47/64 loss: -0.16935521364212036
Batch 48/64 loss: -0.1680794358253479
Batch 49/64 loss: -0.1602064073085785
Batch 50/64 loss: -0.13556957244873047
Batch 51/64 loss: -0.13567936420440674
Batch 52/64 loss: -0.1352723240852356
Batch 53/64 loss: -0.16516739130020142
Batch 54/64 loss: -0.17170068621635437
Batch 55/64 loss: -0.16105154156684875
Batch 56/64 loss: -0.1625065803527832
Batch 57/64 loss: -0.16377419233322144
Batch 58/64 loss: -0.1797228455543518
Batch 59/64 loss: -0.1649152636528015
Batch 60/64 loss: -0.1531069278717041
Batch 61/64 loss: -0.14566105604171753
Batch 62/64 loss: -0.14946705102920532
Batch 63/64 loss: -0.1393248438835144
Batch 64/64 loss: -0.11908966302871704
Epoch 489  Train loss: -0.1558225577952815  Val loss: 0.057224887752860686
Epoch 490
-------------------------------
Batch 1/64 loss: -0.1637668013572693
Batch 2/64 loss: -0.17107248306274414
Batch 3/64 loss: -0.15643739700317383
Batch 4/64 loss: -0.1743599772453308
Batch 5/64 loss: -0.1714504361152649
Batch 6/64 loss: -0.16803789138793945
Batch 7/64 loss: -0.15946650505065918
Batch 8/64 loss: -0.1562635898590088
Batch 9/64 loss: -0.17619997262954712
Batch 10/64 loss: -0.1763610541820526
Batch 11/64 loss: -0.16405287384986877
Batch 12/64 loss: -0.1398160457611084
Batch 13/64 loss: -0.13663476705551147
Batch 14/64 loss: -0.178200364112854
Batch 15/64 loss: -0.15660971403121948
Batch 16/64 loss: -0.1464795470237732
Batch 17/64 loss: -0.16326558589935303
Batch 18/64 loss: -0.12541323900222778
Batch 19/64 loss: -0.13367629051208496
Batch 20/64 loss: -0.15591230988502502
Batch 21/64 loss: -0.18391409516334534
Batch 22/64 loss: -0.18395233154296875
Batch 23/64 loss: -0.1608022153377533
Batch 24/64 loss: -0.15700775384902954
Batch 25/64 loss: -0.16031667590141296
Batch 26/64 loss: -0.16133180260658264
Batch 27/64 loss: -0.1575048565864563
Batch 28/64 loss: -0.13225221633911133
Batch 29/64 loss: -0.16245585680007935
Batch 30/64 loss: -0.13236004114151
Batch 31/64 loss: -0.1489095687866211
Batch 32/64 loss: -0.14457273483276367
Batch 33/64 loss: -0.17638719081878662
Batch 34/64 loss: -0.1479487419128418
Batch 35/64 loss: -0.16709205508232117
Batch 36/64 loss: -0.1644531488418579
Batch 37/64 loss: -0.1831241250038147
Batch 38/64 loss: -0.12377923727035522
Batch 39/64 loss: -0.14468061923980713
Batch 40/64 loss: -0.1692122220993042
Batch 41/64 loss: -0.1667342185974121
Batch 42/64 loss: -0.17934495210647583
Batch 43/64 loss: -0.14538264274597168
Batch 44/64 loss: -0.1624775230884552
Batch 45/64 loss: -0.12788939476013184
Batch 46/64 loss: -0.14390480518341064
Batch 47/64 loss: -0.185261070728302
Batch 48/64 loss: -0.1738230586051941
Batch 49/64 loss: -0.17959299683570862
Batch 50/64 loss: -0.17176514863967896
Batch 51/64 loss: -0.1305544376373291
Batch 52/64 loss: -0.18349921703338623
Batch 53/64 loss: -0.16504546999931335
Batch 54/64 loss: -0.15382760763168335
Batch 55/64 loss: -0.16444653272628784
Batch 56/64 loss: -0.13845086097717285
Batch 57/64 loss: -0.13015401363372803
Batch 58/64 loss: -0.1446472406387329
Batch 59/64 loss: -0.16685575246810913
Batch 60/64 loss: -0.12452864646911621
Batch 61/64 loss: -0.15425217151641846
Batch 62/64 loss: -0.1353958249092102
Batch 63/64 loss: -0.17398914694786072
Batch 64/64 loss: -0.13217079639434814
Epoch 490  Train loss: -0.15743506141737396  Val loss: 0.05647136132741712
Epoch 491
-------------------------------
Batch 1/64 loss: -0.16394361853599548
Batch 2/64 loss: -0.1678270399570465
Batch 3/64 loss: -0.14702510833740234
Batch 4/64 loss: -0.17178332805633545
Batch 5/64 loss: -0.17403331398963928
Batch 6/64 loss: -0.15489685535430908
Batch 7/64 loss: -0.13765090703964233
Batch 8/64 loss: -0.18823951482772827
Batch 9/64 loss: -0.1573563814163208
Batch 10/64 loss: -0.16604572534561157
Batch 11/64 loss: -0.1669767200946808
Batch 12/64 loss: -0.17898279428482056
Batch 13/64 loss: -0.1634473204612732
Batch 14/64 loss: -0.15110713243484497
Batch 15/64 loss: -0.17210888862609863
Batch 16/64 loss: -0.1520167589187622
Batch 17/64 loss: -0.1426522135734558
Batch 18/64 loss: -0.1583855152130127
Batch 19/64 loss: -0.1729430854320526
Batch 20/64 loss: -0.1547173261642456
Batch 21/64 loss: -0.1635838747024536
Batch 22/64 loss: -0.16520851850509644
Batch 23/64 loss: -0.1692746877670288
Batch 24/64 loss: -0.19259333610534668
Batch 25/64 loss: -0.15320909023284912
Batch 26/64 loss: -0.17983323335647583
Batch 27/64 loss: -0.16845077276229858
Batch 28/64 loss: -0.19009920954704285
Batch 29/64 loss: -0.18817219138145447
Batch 30/64 loss: -0.17617031931877136
Batch 31/64 loss: -0.13724911212921143
Batch 32/64 loss: -0.17562460899353027
Batch 33/64 loss: -0.14752542972564697
Batch 34/64 loss: -0.1688968539237976
Batch 35/64 loss: -0.1318354606628418
Batch 36/64 loss: -0.1571286916732788
Batch 37/64 loss: -0.13639456033706665
Batch 38/64 loss: -0.1725897192955017
Batch 39/64 loss: -0.15261971950531006
Batch 40/64 loss: -0.12925434112548828
Batch 41/64 loss: -0.16402658820152283
Batch 42/64 loss: -0.148159921169281
Batch 43/64 loss: -0.15568751096725464
Batch 44/64 loss: -0.15930980443954468
Batch 45/64 loss: -0.17830294370651245
Batch 46/64 loss: -0.13252860307693481
Batch 47/64 loss: -0.1769566535949707
Batch 48/64 loss: -0.15909042954444885
Batch 49/64 loss: -0.17376500368118286
Batch 50/64 loss: -0.17429423332214355
Batch 51/64 loss: -0.13953125476837158
Batch 52/64 loss: -0.17552915215492249
Batch 53/64 loss: -0.1744072437286377
Batch 54/64 loss: -0.14121830463409424
Batch 55/64 loss: -0.17064017057418823
Batch 56/64 loss: -0.11660850048065186
Batch 57/64 loss: -0.16742956638336182
Batch 58/64 loss: -0.12673228979110718
Batch 59/64 loss: -0.14796382188796997
Batch 60/64 loss: -0.15640968084335327
Batch 61/64 loss: -0.16982987523078918
Batch 62/64 loss: -0.13105320930480957
Batch 63/64 loss: -0.14986693859100342
Batch 64/64 loss: -0.1313883662223816
Epoch 491  Train loss: -0.15977625496247236  Val loss: 0.05802841448701944
Epoch 492
-------------------------------
Batch 1/64 loss: -0.16496849060058594
Batch 2/64 loss: -0.19109559059143066
Batch 3/64 loss: -0.18396884202957153
Batch 4/64 loss: -0.16606512665748596
Batch 5/64 loss: -0.17085635662078857
Batch 6/64 loss: -0.14976418018341064
Batch 7/64 loss: -0.12050390243530273
Batch 8/64 loss: -0.16807907819747925
Batch 9/64 loss: -0.19217580556869507
Batch 10/64 loss: -0.15520206093788147
Batch 11/64 loss: -0.18953955173492432
Batch 12/64 loss: -0.1251855492591858
Batch 13/64 loss: -0.17646783590316772
Batch 14/64 loss: -0.13088059425354004
Batch 15/64 loss: -0.13110345602035522
Batch 16/64 loss: -0.09321075677871704
Batch 17/64 loss: -0.14421826601028442
Batch 18/64 loss: -0.15248560905456543
Batch 19/64 loss: -0.11587977409362793
Batch 20/64 loss: -0.15327543020248413
Batch 21/64 loss: -0.15779539942741394
Batch 22/64 loss: -0.1634584367275238
Batch 23/64 loss: -0.16491591930389404
Batch 24/64 loss: -0.16289067268371582
Batch 25/64 loss: -0.1450653076171875
Batch 26/64 loss: -0.15758094191551208
Batch 27/64 loss: -0.15669089555740356
Batch 28/64 loss: -0.13812774419784546
Batch 29/64 loss: -0.15398043394088745
Batch 30/64 loss: -0.16916033625602722
Batch 31/64 loss: -0.1694677472114563
Batch 32/64 loss: -0.13663941621780396
Batch 33/64 loss: -0.1351701021194458
Batch 34/64 loss: -0.16634145379066467
Batch 35/64 loss: -0.14669549465179443
Batch 36/64 loss: -0.14416152238845825
Batch 37/64 loss: -0.1707354187965393
Batch 38/64 loss: -0.15962296724319458
Batch 39/64 loss: -0.17166391015052795
Batch 40/64 loss: -0.17644953727722168
Batch 41/64 loss: -0.1631457805633545
Batch 42/64 loss: -0.17771756649017334
Batch 43/64 loss: -0.15408223867416382
Batch 44/64 loss: -0.16067594289779663
Batch 45/64 loss: -0.1410534381866455
Batch 46/64 loss: -0.1525018811225891
Batch 47/64 loss: -0.16488763689994812
Batch 48/64 loss: -0.18409544229507446
Batch 49/64 loss: -0.18673372268676758
Batch 50/64 loss: -0.15005367994308472
Batch 51/64 loss: -0.15548175573349
Batch 52/64 loss: -0.1716342568397522
Batch 53/64 loss: -0.172873854637146
Batch 54/64 loss: -0.17106151580810547
Batch 55/64 loss: -0.15515965223312378
Batch 56/64 loss: -0.18977826833724976
Batch 57/64 loss: -0.17926400899887085
Batch 58/64 loss: -0.14534920454025269
Batch 59/64 loss: -0.1643584966659546
Batch 60/64 loss: -0.16392481327056885
Batch 61/64 loss: -0.1610925793647766
Batch 62/64 loss: -0.16764777898788452
Batch 63/64 loss: -0.16550269722938538
Batch 64/64 loss: -0.08716464042663574
Epoch 492  Train loss: -0.15819591540916295  Val loss: 0.05534598528314702
Epoch 493
-------------------------------
Batch 1/64 loss: -0.15367424488067627
Batch 2/64 loss: -0.16251516342163086
Batch 3/64 loss: -0.17485085129737854
Batch 4/64 loss: -0.18865108489990234
Batch 5/64 loss: -0.1491033434867859
Batch 6/64 loss: -0.18336760997772217
Batch 7/64 loss: -0.14497816562652588
Batch 8/64 loss: -0.16416865587234497
Batch 9/64 loss: -0.18495678901672363
Batch 10/64 loss: -0.17199847102165222
Batch 11/64 loss: -0.17448440194129944
Batch 12/64 loss: -0.1335216760635376
Batch 13/64 loss: -0.1656821072101593
Batch 14/64 loss: -0.14798307418823242
Batch 15/64 loss: -0.1573522686958313
Batch 16/64 loss: -0.18362587690353394
Batch 17/64 loss: -0.1442776322364807
Batch 18/64 loss: -0.15416842699050903
Batch 19/64 loss: -0.15321868658065796
Batch 20/64 loss: -0.16207996010780334
Batch 21/64 loss: -0.17248591780662537
Batch 22/64 loss: -0.15791264176368713
Batch 23/64 loss: -0.13961338996887207
Batch 24/64 loss: -0.15569663047790527
Batch 25/64 loss: -0.19165673851966858
Batch 26/64 loss: -0.1656252145767212
Batch 27/64 loss: -0.17360332608222961
Batch 28/64 loss: -0.18377846479415894
Batch 29/64 loss: -0.1732463836669922
Batch 30/64 loss: -0.14444243907928467
Batch 31/64 loss: -0.1773609220981598
Batch 32/64 loss: -0.15990370512008667
Batch 33/64 loss: -0.15732896327972412
Batch 34/64 loss: -0.12046456336975098
Batch 35/64 loss: -0.15013718605041504
Batch 36/64 loss: -0.1295527219772339
Batch 37/64 loss: -0.16687169671058655
Batch 38/64 loss: -0.13891595602035522
Batch 39/64 loss: -0.17589956521987915
Batch 40/64 loss: -0.13822388648986816
Batch 41/64 loss: -0.1574142575263977
Batch 42/64 loss: -0.15612685680389404
Batch 43/64 loss: -0.13809555768966675
Batch 44/64 loss: -0.14770734310150146
Batch 45/64 loss: -0.1438528299331665
Batch 46/64 loss: -0.175991028547287
Batch 47/64 loss: -0.1553790271282196
Batch 48/64 loss: -0.17624133825302124
Batch 49/64 loss: -0.16720643639564514
Batch 50/64 loss: -0.15892556309700012
Batch 51/64 loss: -0.16834253072738647
Batch 52/64 loss: -0.10937672853469849
Batch 53/64 loss: -0.17046326398849487
Batch 54/64 loss: -0.1394292712211609
Batch 55/64 loss: -0.15613389015197754
Batch 56/64 loss: -0.16483473777770996
Batch 57/64 loss: -0.17286598682403564
Batch 58/64 loss: -0.15541762113571167
Batch 59/64 loss: -0.14856135845184326
Batch 60/64 loss: -0.16986197233200073
Batch 61/64 loss: -0.17821991443634033
Batch 62/64 loss: -0.15387442708015442
Batch 63/64 loss: -0.13620656728744507
Batch 64/64 loss: -0.16883990168571472
Epoch 493  Train loss: -0.15928685863812764  Val loss: 0.05784947233101756
Epoch 494
-------------------------------
Batch 1/64 loss: -0.17454594373703003
Batch 2/64 loss: -0.18596506118774414
Batch 3/64 loss: -0.12611174583435059
Batch 4/64 loss: -0.16323745250701904
Batch 5/64 loss: -0.17144939303398132
Batch 6/64 loss: -0.1360531449317932
Batch 7/64 loss: -0.14347124099731445
Batch 8/64 loss: -0.16134217381477356
Batch 9/64 loss: -0.16650745272636414
Batch 10/64 loss: -0.17558729648590088
Batch 11/64 loss: -0.17635631561279297
Batch 12/64 loss: -0.14188134670257568
Batch 13/64 loss: -0.1649777889251709
Batch 14/64 loss: -0.18174654245376587
Batch 15/64 loss: -0.14061105251312256
Batch 16/64 loss: -0.15010720491409302
Batch 17/64 loss: -0.1791011095046997
Batch 18/64 loss: -0.16162559390068054
Batch 19/64 loss: -0.18552130460739136
Batch 20/64 loss: -0.18884694576263428
Batch 21/64 loss: -0.15480941534042358
Batch 22/64 loss: -0.16111350059509277
Batch 23/64 loss: -0.169219970703125
Batch 24/64 loss: -0.13809239864349365
Batch 25/64 loss: -0.1640196442604065
Batch 26/64 loss: -0.17042744159698486
Batch 27/64 loss: -0.14451229572296143
Batch 28/64 loss: -0.132490336894989
Batch 29/64 loss: -0.14035868644714355
Batch 30/64 loss: -0.15139013528823853
Batch 31/64 loss: -0.1484493613243103
Batch 32/64 loss: -0.1609511375427246
Batch 33/64 loss: -0.15040403604507446
Batch 34/64 loss: -0.15103864669799805
Batch 35/64 loss: -0.15441536903381348
Batch 36/64 loss: -0.1734323799610138
Batch 37/64 loss: -0.13005614280700684
Batch 38/64 loss: -0.1777496039867401
Batch 39/64 loss: -0.16566520929336548
Batch 40/64 loss: -0.1158454418182373
Batch 41/64 loss: -0.10809922218322754
Batch 42/64 loss: -0.13761895895004272
Batch 43/64 loss: -0.15226256847381592
Batch 44/64 loss: -0.1680474877357483
Batch 45/64 loss: -0.1648331880569458
Batch 46/64 loss: -0.17084771394729614
Batch 47/64 loss: -0.16679245233535767
Batch 48/64 loss: -0.16199246048927307
Batch 49/64 loss: -0.15631914138793945
Batch 50/64 loss: -0.15313947200775146
Batch 51/64 loss: -0.16589981317520142
Batch 52/64 loss: -0.16995304822921753
Batch 53/64 loss: -0.15568506717681885
Batch 54/64 loss: -0.16813170909881592
Batch 55/64 loss: -0.16851690411567688
Batch 56/64 loss: -0.14645957946777344
Batch 57/64 loss: -0.17076274752616882
Batch 58/64 loss: -0.1752694547176361
Batch 59/64 loss: -0.1436864733695984
Batch 60/64 loss: -0.08282345533370972
Batch 61/64 loss: -0.14423131942749023
Batch 62/64 loss: -0.11545473337173462
Batch 63/64 loss: -0.17991715669631958
Batch 64/64 loss: -0.153975248336792
Epoch 494  Train loss: -0.1564190247479607  Val loss: 0.06119626985792442
Epoch 495
-------------------------------
Batch 1/64 loss: -0.17254942655563354
Batch 2/64 loss: -0.15848520398139954
Batch 3/64 loss: -0.1530371904373169
Batch 4/64 loss: -0.18078678846359253
Batch 5/64 loss: -0.14619982242584229
Batch 6/64 loss: -0.17963576316833496
Batch 7/64 loss: -0.15504738688468933
Batch 8/64 loss: -0.1613791584968567
Batch 9/64 loss: -0.17924413084983826
Batch 10/64 loss: -0.15233582258224487
Batch 11/64 loss: -0.1622752845287323
Batch 12/64 loss: -0.15599721670150757
Batch 13/64 loss: -0.15721923112869263
Batch 14/64 loss: -0.17082184553146362
Batch 15/64 loss: -0.169207364320755
Batch 16/64 loss: -0.14443886280059814
Batch 17/64 loss: -0.1866673231124878
Batch 18/64 loss: -0.1675424575805664
Batch 19/64 loss: -0.12958747148513794
Batch 20/64 loss: -0.1587918996810913
Batch 21/64 loss: -0.17067283391952515
Batch 22/64 loss: -0.16769379377365112
Batch 23/64 loss: -0.15363681316375732
Batch 24/64 loss: -0.149358868598938
Batch 25/64 loss: -0.1661759614944458
Batch 26/64 loss: -0.1263599395751953
Batch 27/64 loss: -0.1653551459312439
Batch 28/64 loss: -0.1758885383605957
Batch 29/64 loss: -0.15358591079711914
Batch 30/64 loss: -0.1575225591659546
Batch 31/64 loss: -0.17458295822143555
Batch 32/64 loss: -0.1427949070930481
Batch 33/64 loss: -0.15585526823997498
Batch 34/64 loss: -0.158233642578125
Batch 35/64 loss: -0.16010937094688416
Batch 36/64 loss: -0.15587472915649414
Batch 37/64 loss: -0.15669530630111694
Batch 38/64 loss: -0.1680651605129242
Batch 39/64 loss: -0.16382649540901184
Batch 40/64 loss: -0.14427107572555542
Batch 41/64 loss: -0.1625581681728363
Batch 42/64 loss: -0.1556462049484253
Batch 43/64 loss: -0.14835768938064575
Batch 44/64 loss: -0.16795021295547485
Batch 45/64 loss: -0.13152116537094116
Batch 46/64 loss: -0.1587551236152649
Batch 47/64 loss: -0.11913025379180908
Batch 48/64 loss: -0.1673913300037384
Batch 49/64 loss: -0.17834526300430298
Batch 50/64 loss: -0.14233261346817017
Batch 51/64 loss: -0.16140016913414001
Batch 52/64 loss: -0.18049854040145874
Batch 53/64 loss: -0.15760374069213867
Batch 54/64 loss: -0.14193809032440186
Batch 55/64 loss: -0.15072381496429443
Batch 56/64 loss: -0.11066269874572754
Batch 57/64 loss: -0.13753211498260498
Batch 58/64 loss: -0.1515815258026123
Batch 59/64 loss: -0.1653968095779419
Batch 60/64 loss: -0.14179182052612305
Batch 61/64 loss: -0.13656914234161377
Batch 62/64 loss: -0.15180695056915283
Batch 63/64 loss: -0.15501433610916138
Batch 64/64 loss: -0.13318973779678345
Epoch 495  Train loss: -0.15658367077509563  Val loss: 0.05914508907246016
Epoch 496
-------------------------------
Batch 1/64 loss: -0.15768349170684814
Batch 2/64 loss: -0.1560955047607422
Batch 3/64 loss: -0.14650076627731323
Batch 4/64 loss: -0.1503063440322876
Batch 5/64 loss: -0.16947680711746216
Batch 6/64 loss: -0.18426859378814697
Batch 7/64 loss: -0.1336369514465332
Batch 8/64 loss: -0.15993133187294006
Batch 9/64 loss: -0.16425836086273193
Batch 10/64 loss: -0.1713026463985443
Batch 11/64 loss: -0.14578771591186523
Batch 12/64 loss: -0.1624499261379242
Batch 13/64 loss: -0.19134163856506348
Batch 14/64 loss: -0.19159311056137085
Batch 15/64 loss: -0.17366069555282593
Batch 16/64 loss: -0.13110172748565674
Batch 17/64 loss: -0.15455114841461182
Batch 18/64 loss: -0.1777915358543396
Batch 19/64 loss: -0.15431487560272217
Batch 20/64 loss: -0.1693049967288971
Batch 21/64 loss: -0.16011863946914673
Batch 22/64 loss: -0.13774096965789795
Batch 23/64 loss: -0.15354621410369873
Batch 24/64 loss: -0.2013506293296814
Batch 25/64 loss: -0.13250809907913208
Batch 26/64 loss: -0.1664242446422577
Batch 27/64 loss: -0.16853851079940796
Batch 28/64 loss: -0.15910625457763672
Batch 29/64 loss: -0.13785874843597412
Batch 30/64 loss: -0.18491652607917786
Batch 31/64 loss: -0.1671016812324524
Batch 32/64 loss: -0.16178342700004578
Batch 33/64 loss: -0.1722288429737091
Batch 34/64 loss: -0.1711598038673401
Batch 35/64 loss: -0.15719079971313477
Batch 36/64 loss: -0.1382375955581665
Batch 37/64 loss: -0.14214348793029785
Batch 38/64 loss: -0.1600944697856903
Batch 39/64 loss: -0.16252917051315308
Batch 40/64 loss: -0.16757449507713318
Batch 41/64 loss: -0.1608540117740631
Batch 42/64 loss: -0.14222371578216553
Batch 43/64 loss: -0.16902557015419006
Batch 44/64 loss: -0.1634393036365509
Batch 45/64 loss: -0.16903096437454224
Batch 46/64 loss: -0.14374876022338867
Batch 47/64 loss: -0.16707396507263184
Batch 48/64 loss: -0.16652148962020874
Batch 49/64 loss: -0.18085962533950806
Batch 50/64 loss: -0.1580224633216858
Batch 51/64 loss: -0.15988785028457642
Batch 52/64 loss: -0.15350693464279175
Batch 53/64 loss: -0.1610521674156189
Batch 54/64 loss: -0.12893891334533691
Batch 55/64 loss: -0.14875000715255737
Batch 56/64 loss: -0.15739375352859497
Batch 57/64 loss: -0.16207334399223328
Batch 58/64 loss: -0.13028693199157715
Batch 59/64 loss: -0.15589460730552673
Batch 60/64 loss: -0.09342765808105469
Batch 61/64 loss: -0.14725416898727417
Batch 62/64 loss: -0.15877214074134827
Batch 63/64 loss: -0.16193553805351257
Batch 64/64 loss: -0.14985454082489014
Epoch 496  Train loss: -0.15842942069558535  Val loss: 0.05603362276791707
Epoch 497
-------------------------------
Batch 1/64 loss: -0.14564204216003418
Batch 2/64 loss: -0.1283702254295349
Batch 3/64 loss: -0.15483909845352173
Batch 4/64 loss: -0.16904917359352112
Batch 5/64 loss: -0.15940257906913757
Batch 6/64 loss: -0.18683430552482605
Batch 7/64 loss: -0.1916179060935974
Batch 8/64 loss: -0.20851606130599976
Batch 9/64 loss: -0.18825674057006836
Batch 10/64 loss: -0.16006934642791748
Batch 11/64 loss: -0.174208402633667
Batch 12/64 loss: -0.135359525680542
Batch 13/64 loss: -0.1534746289253235
Batch 14/64 loss: -0.15546125173568726
Batch 15/64 loss: -0.16764283180236816
Batch 16/64 loss: -0.14631831645965576
Batch 17/64 loss: -0.1713579297065735
Batch 18/64 loss: -0.180504709482193
Batch 19/64 loss: -0.17630526423454285
Batch 20/64 loss: -0.17740866541862488
Batch 21/64 loss: -0.1724211573600769
Batch 22/64 loss: -0.18094384670257568
Batch 23/64 loss: -0.16632476449012756
Batch 24/64 loss: -0.17037400603294373
Batch 25/64 loss: -0.1550176739692688
Batch 26/64 loss: -0.15609103441238403
Batch 27/64 loss: -0.1413813829421997
Batch 28/64 loss: -0.1581333875656128
Batch 29/64 loss: -0.17315798997879028
Batch 30/64 loss: -0.17949292063713074
Batch 31/64 loss: -0.15799927711486816
Batch 32/64 loss: -0.16587281227111816
Batch 33/64 loss: -0.1864900290966034
Batch 34/64 loss: -0.16876131296157837
Batch 35/64 loss: -0.15357011556625366
Batch 36/64 loss: -0.18018794059753418
Batch 37/64 loss: -0.11714982986450195
Batch 38/64 loss: -0.15847152471542358
Batch 39/64 loss: -0.1617756187915802
Batch 40/64 loss: -0.16003909707069397
Batch 41/64 loss: -0.15024703741073608
Batch 42/64 loss: -0.09338796138763428
Batch 43/64 loss: -0.15448445081710815
Batch 44/64 loss: -0.1745869517326355
Batch 45/64 loss: -0.1540176272392273
Batch 46/64 loss: -0.15452146530151367
Batch 47/64 loss: -0.15940555930137634
Batch 48/64 loss: -0.16340243816375732
Batch 49/64 loss: -0.18561840057373047
Batch 50/64 loss: -0.13560497760772705
Batch 51/64 loss: -0.14680927991867065
Batch 52/64 loss: -0.16221073269844055
Batch 53/64 loss: -0.16300982236862183
Batch 54/64 loss: -0.14402347803115845
Batch 55/64 loss: -0.1647948920726776
Batch 56/64 loss: -0.1620023250579834
Batch 57/64 loss: -0.15670982003211975
Batch 58/64 loss: -0.17042452096939087
Batch 59/64 loss: -0.15451091527938843
Batch 60/64 loss: -0.16673117876052856
Batch 61/64 loss: -0.15039193630218506
Batch 62/64 loss: -0.14839649200439453
Batch 63/64 loss: -0.14947229623794556
Batch 64/64 loss: -0.16350284218788147
Epoch 497  Train loss: -0.1612813555726818  Val loss: 0.05833115589987371
Epoch 498
-------------------------------
Batch 1/64 loss: -0.1782304048538208
Batch 2/64 loss: -0.16152864694595337
Batch 3/64 loss: -0.18183940649032593
Batch 4/64 loss: -0.18199437856674194
Batch 5/64 loss: -0.14232748746871948
Batch 6/64 loss: -0.15987235307693481
Batch 7/64 loss: -0.1753407120704651
Batch 8/64 loss: -0.15652385354042053
Batch 9/64 loss: -0.17027002573013306
Batch 10/64 loss: -0.16302281618118286
Batch 11/64 loss: -0.17780321836471558
Batch 12/64 loss: -0.18126100301742554
Batch 13/64 loss: -0.17988252639770508
Batch 14/64 loss: -0.1795734167098999
Batch 15/64 loss: -0.18760329484939575
Batch 16/64 loss: -0.14385008811950684
Batch 17/64 loss: -0.15489715337753296
Batch 18/64 loss: -0.1663895845413208
Batch 19/64 loss: -0.1696285605430603
Batch 20/64 loss: -0.15834933519363403
Batch 21/64 loss: -0.15252327919006348
Batch 22/64 loss: -0.16419237852096558
Batch 23/64 loss: -0.14951801300048828
Batch 24/64 loss: -0.16096362471580505
Batch 25/64 loss: -0.1231011152267456
Batch 26/64 loss: -0.18771207332611084
Batch 27/64 loss: -0.15887528657913208
Batch 28/64 loss: -0.1695556640625
Batch 29/64 loss: -0.12146365642547607
Batch 30/64 loss: -0.1609412431716919
Batch 31/64 loss: -0.1630561649799347
Batch 32/64 loss: -0.14111077785491943
Batch 33/64 loss: -0.13179939985275269
Batch 34/64 loss: -0.17208054661750793
Batch 35/64 loss: -0.16305029392242432
Batch 36/64 loss: -0.16251829266548157
Batch 37/64 loss: -0.17038697004318237
Batch 38/64 loss: -0.15014296770095825
Batch 39/64 loss: -0.17464062571525574
Batch 40/64 loss: -0.142913818359375
Batch 41/64 loss: -0.1259326934814453
Batch 42/64 loss: -0.16106539964675903
Batch 43/64 loss: -0.12283587455749512
Batch 44/64 loss: -0.15643402934074402
Batch 45/64 loss: -0.168498694896698
Batch 46/64 loss: -0.156125009059906
Batch 47/64 loss: -0.1692379117012024
Batch 48/64 loss: -0.14848965406417847
Batch 49/64 loss: -0.18085533380508423
Batch 50/64 loss: -0.13189774751663208
Batch 51/64 loss: -0.1636030077934265
Batch 52/64 loss: -0.13493293523788452
Batch 53/64 loss: -0.1523265838623047
Batch 54/64 loss: -0.157143235206604
Batch 55/64 loss: -0.1680334210395813
Batch 56/64 loss: -0.16117572784423828
Batch 57/64 loss: -0.13547515869140625
Batch 58/64 loss: -0.1268560290336609
Batch 59/64 loss: -0.17890149354934692
Batch 60/64 loss: -0.16270959377288818
Batch 61/64 loss: -0.16198769211769104
Batch 62/64 loss: -0.14880895614624023
Batch 63/64 loss: -0.153629332780838
Batch 64/64 loss: -0.1677519679069519
Epoch 498  Train loss: -0.1591137874360178  Val loss: 0.05612324685165562
Epoch 499
-------------------------------
Batch 1/64 loss: -0.15156519412994385
Batch 2/64 loss: -0.16806840896606445
Batch 3/64 loss: -0.17624253034591675
Batch 4/64 loss: -0.18938636779785156
Batch 5/64 loss: -0.13667964935302734
Batch 6/64 loss: -0.17005380988121033
Batch 7/64 loss: -0.15724042057991028
Batch 8/64 loss: -0.17690545320510864
Batch 9/64 loss: -0.1597752571105957
Batch 10/64 loss: -0.19184339046478271
Batch 11/64 loss: -0.162065327167511
Batch 12/64 loss: -0.1579182744026184
Batch 13/64 loss: -0.16411226987838745
Batch 14/64 loss: -0.16725215315818787
Batch 15/64 loss: -0.14380216598510742
Batch 16/64 loss: -0.1528463363647461
Batch 17/64 loss: -0.15720701217651367
Batch 18/64 loss: -0.15481030941009521
Batch 19/64 loss: -0.1743016242980957
Batch 20/64 loss: -0.15834009647369385
Batch 21/64 loss: -0.188446044921875
Batch 22/64 loss: -0.1299242377281189
Batch 23/64 loss: -0.14639121294021606
Batch 24/64 loss: -0.16420108079910278
Batch 25/64 loss: -0.16542553901672363
Batch 26/64 loss: -0.1699247658252716
Batch 27/64 loss: -0.17931169271469116
Batch 28/64 loss: -0.14765208959579468
Batch 29/64 loss: -0.16415366530418396
Batch 30/64 loss: -0.186433345079422
Batch 31/64 loss: -0.13568127155303955
Batch 32/64 loss: -0.15236979722976685
Batch 33/64 loss: -0.1441437005996704
Batch 34/64 loss: -0.13314515352249146
Batch 35/64 loss: -0.1276341676712036
Batch 36/64 loss: -0.16790127754211426
Batch 37/64 loss: -0.15292948484420776
Batch 38/64 loss: -0.16371876001358032
Batch 39/64 loss: -0.1495949625968933
Batch 40/64 loss: -0.14683294296264648
Batch 41/64 loss: -0.19220444560050964
Batch 42/64 loss: -0.15547996759414673
Batch 43/64 loss: -0.18269646167755127
Batch 44/64 loss: -0.17878419160842896
Batch 45/64 loss: -0.14785009622573853
Batch 46/64 loss: -0.15496015548706055
Batch 47/64 loss: -0.13964039087295532
Batch 48/64 loss: -0.15026605129241943
Batch 49/64 loss: -0.16201114654541016
Batch 50/64 loss: -0.1689404845237732
Batch 51/64 loss: -0.16235965490341187
Batch 52/64 loss: -0.15664327144622803
Batch 53/64 loss: -0.15349507331848145
Batch 54/64 loss: -0.14886260032653809
Batch 55/64 loss: -0.15168553590774536
Batch 56/64 loss: -0.14534664154052734
Batch 57/64 loss: -0.17065617442131042
Batch 58/64 loss: -0.14965534210205078
Batch 59/64 loss: -0.16052213311195374
Batch 60/64 loss: -0.1649211347103119
Batch 61/64 loss: -0.1754712462425232
Batch 62/64 loss: -0.15223681926727295
Batch 63/64 loss: -0.16175365447998047
Batch 64/64 loss: -0.10313558578491211
Epoch 499  Train loss: -0.15921608793969247  Val loss: 0.05753811794457976
Epoch 500
-------------------------------
Batch 1/64 loss: -0.18341577053070068
Batch 2/64 loss: -0.15490999817848206
Batch 3/64 loss: -0.14421045780181885
Batch 4/64 loss: -0.14707541465759277
Batch 5/64 loss: -0.1488121747970581
Batch 6/64 loss: -0.1616939902305603
Batch 7/64 loss: -0.1453806757926941
Batch 8/64 loss: -0.1811462640762329
Batch 9/64 loss: -0.19062983989715576
Batch 10/64 loss: -0.15906351804733276
Batch 11/64 loss: -0.13649970293045044
Batch 12/64 loss: -0.16719111800193787
Batch 13/64 loss: -0.15654420852661133
Batch 14/64 loss: -0.16486722230911255
Batch 15/64 loss: -0.178175151348114
Batch 16/64 loss: -0.13810038566589355
Batch 17/64 loss: -0.16973352432250977
Batch 18/64 loss: -0.1391332745552063
Batch 19/64 loss: -0.15979686379432678
Batch 20/64 loss: -0.17346209287643433
Batch 21/64 loss: -0.1556941270828247
Batch 22/64 loss: -0.15631014108657837
Batch 23/64 loss: -0.16847896575927734
Batch 24/64 loss: -0.15752777457237244
Batch 25/64 loss: -0.1857435405254364
Batch 26/64 loss: -0.1523556113243103
Batch 27/64 loss: -0.1595458686351776
Batch 28/64 loss: -0.16511523723602295
Batch 29/64 loss: -0.12052816152572632
Batch 30/64 loss: -0.1681978702545166
Batch 31/64 loss: -0.17028823494911194
Batch 32/64 loss: -0.16849255561828613
Batch 33/64 loss: -0.1596686840057373
Batch 34/64 loss: -0.1317838430404663
Batch 35/64 loss: -0.1704246699810028
Batch 36/64 loss: -0.17483174800872803
Batch 37/64 loss: -0.15385913848876953
Batch 38/64 loss: -0.152515709400177
Batch 39/64 loss: -0.14193028211593628
Batch 40/64 loss: -0.15834376215934753
Batch 41/64 loss: -0.14967328310012817
Batch 42/64 loss: -0.11317074298858643
Batch 43/64 loss: -0.18507516384124756
Batch 44/64 loss: -0.1797688603401184
Batch 45/64 loss: -0.16390496492385864
Batch 46/64 loss: -0.1551036834716797
Batch 47/64 loss: -0.11065006256103516
Batch 48/64 loss: -0.1780490279197693
Batch 49/64 loss: -0.16119149327278137
Batch 50/64 loss: -0.15669941902160645
Batch 51/64 loss: -0.15996640920639038
Batch 52/64 loss: -0.15910691022872925
Batch 53/64 loss: -0.18769538402557373
Batch 54/64 loss: -0.17445459961891174
Batch 55/64 loss: -0.14937597513198853
Batch 56/64 loss: -0.14285361766815186
Batch 57/64 loss: -0.17674881219863892
Batch 58/64 loss: -0.14561843872070312
Batch 59/64 loss: -0.16262656450271606
Batch 60/64 loss: -0.1771250069141388
Batch 61/64 loss: -0.15590900182724
Batch 62/64 loss: -0.15077823400497437
Batch 63/64 loss: -0.1565799117088318
Batch 64/64 loss: -0.15818050503730774
Epoch 500  Train loss: -0.1590939375699735  Val loss: 0.058957176724660026
SLIC undersegmentation error: 0.12412920962199316
SLIC inter-cluster variation: 0.13904419774313004
SLIC number of superpixels: 21483
SLIC superpixels per image: 73.82474226804123
Model loaded
Test metrics:
0.03051103861471222 0.3392302405498282 21.74173806287874 tensor(0.2758, dtype=torch.float64) 0.894360117977124 4.638766995180203 27802
Inference time: 0.003139521248152166 seconds
Relabeled undersegmentation error: 0.08432714776632305
Relabeled inter-cluster variation: 0.03685776160413358
Relabeled mean superpixels count: 443.192439862543
Original mean superpixels count: 95.53951890034364
Done!
Job id: 422778
